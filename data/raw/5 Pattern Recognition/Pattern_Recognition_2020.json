[
  {
    "title": "Enhanced Grassmann discriminant analysis with randomized time warping for motion recognition",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107028",
    "abstract": "This paper proposes a framework for classifying motion sequences, by extending the framework of Grassmann discriminant analysis (GDA). A problem of GDA is that its discriminant space is not necessarily optimal. This limitation becomes even more prominent when utilizing the subspace representation of randomized time warping (RTW). RTW is a sequence representation that can effectively model a motion’s temporal information by a low-dimensional subspace, simplifying the problem of comparing two sequences to that of comparing two subspaces. The key idea of the proposed enhanced GDA is projecting class subspaces onto a generalized difference subspace before mapping them on a Grassmann manifold. The GDS projection can remove overlapping components of the subspaces in the vector space, nearly orthogonalizing them. Consequently, a dictionary of orthogonalized class subspaces produces a set of more discriminant data points in the Grassmann manifold, in comparison with the original set. This set of data points can further enhance the discriminant ability of GDA. We demonstrate the validity of the proposed framework, RTW+eGDA, through experiments on motion recognition using the publicly available Cambridge gesture, KTH action, and UCF sports datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303310",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Discriminant",
      "Dynamic time warping",
      "Grassmannian",
      "Law",
      "Linear discriminant analysis",
      "Linear subspace",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Programming language",
      "Projection (relational algebra)",
      "Pure mathematics",
      "Representation (politics)",
      "Set (abstract data type)",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Souza",
        "given_name": "Lincon S."
      },
      {
        "surname": "Gatto",
        "given_name": "Bernardo B."
      },
      {
        "surname": "Xue",
        "given_name": "Jing-Hao"
      },
      {
        "surname": "Fukui",
        "given_name": "Kazuhiro"
      }
    ]
  },
  {
    "title": "Segmentation strategies for the alpha-tree data structure",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.027",
    "abstract": "The alpha-tree is a versatile algorithm for color image segmentation employing attribute constraints to control the partitioning of the alpha-connected image space. Attribute constraints are enforced using a maximization strategy that returns the set of the largest connected components complying with these constraints assuming no prior violations from nested sub-components. This article presents two new strategies extending the way this set is defined. These are the non-target clustering and attribute maximization strategies, that give access to segments that could not be defined previously. Collectively they enable the handling of texture-rich regions that cannot be clustered into meaningful segments, and compute the unsupervised segmentation of images by seeking for extreme attribute values.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303496",
    "keywords": [
      "Alpha (finance)",
      "Artificial intelligence",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Construct validity",
      "Data mining",
      "Image (mathematics)",
      "Image segmentation",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Pattern recognition (psychology)",
      "Programming language",
      "Psychometrics",
      "Segmentation",
      "Set (abstract data type)",
      "Statistics",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Ouzounis",
        "given_name": "Georgios K."
      }
    ]
  },
  {
    "title": "Wasserstein GAN based on Autoencoder with back-translation for cross-lingual embedding mappings",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.033",
    "abstract": "Recent works about learning cross-lingual word mappings (CWMs) focus on relaxing the requirement of bilingual signals through generative adversarial networks (GANs). GANs based models intend to enforce source embedding space to align target embedding space. However, existing GANs based models cannot exploit the underlying information of target-side for an alignment standard in the training, which may lead to some suboptimal results of CWMs. To address this problem, we propose a novel method, named Wasserstein GAN based on autoencoder with back-translation (ABWGAN) that can effectively exploit the target-side information and improve the performance of GANs based models. ABWGAN is an innovative combination of preliminary mappings learning and back-translation with target-side (BT-TS). In the proposed BT-TS, we back-translate target-side embeddings with preliminary CWMs to learn the final cross-lingual mappings, which enables to improve the quality of the preliminary mappings by reusing the target-side samples. Experimental results on three language pairs demonstrate the effectiveness of the proposed ABWGAN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303538",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Autoencoder",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Deep learning",
      "Embedding",
      "Exploit",
      "Focus (optics)",
      "Gene",
      "Messenger RNA",
      "Optics",
      "Physics",
      "Theoretical computer science",
      "Translation (biology)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yuhong"
      },
      {
        "surname": "Li",
        "given_name": "Yuling"
      },
      {
        "surname": "Zhu",
        "given_name": "Yi"
      },
      {
        "surname": "Hu",
        "given_name": "Xuegang"
      }
    ]
  },
  {
    "title": "What is the minimum training data size to reliably identify writers in medieval manuscripts?",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.030",
    "abstract": "One of the most important research topics in the field of palaeography is the identification of the different scribes who participated in the writing process of a medieval book. Using traditional palaeographic tools, a palaeographer spends a lot of time reading, measuring and comparing thousands of letters or graphic signs. The aim is to evaluate different characteristics, such as height or width of letters, distance between characters, angles of inclination, number and type of abbreviations etc., which allow a reliable identification of the scribes who contributed to the production of a given manuscript. Despite the growing scientific interest that has been observed in recent years in the use of computer techniques applied to palaeographic research, a general agreement has not yet been reached among researchers, either about the effectiveness of automatic analysis tools, or on the features that should be considered to perform such an analysis. However, in the context of a highly standardized school, the use of some basic page layout features can be very useful for automatically identifying the presence of different hands. In this context, the aim of our study is to verify whether it is possible to strongly reduce the amount of data a palaeographer must analyse manually, in an attempt to answer the following question: what is the minimum size of the training set that allows a classification system to identify the different scribal hands reliably? To this purpose, we have considered two well-known and highly efficient classification techniques, progressively varying the size of the training set and comparing the corresponding classification results. To improve the classification reliability, we have also introduced a multi-expert classification architecture, enabling an easy implementation of a reject option. The experimental results, performed on two large sets of digital images extracted from two entire 12th-century Bibles, show that using only a few pages of these bibles as a training set, it is possible to identify automatically the scribal hands in the remaining pages with great reliability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303472",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Context (archaeology)",
      "Data science",
      "Field (mathematics)",
      "Identification (biology)",
      "Information retrieval",
      "Linguistics",
      "Mathematics",
      "Natural language processing",
      "Paleontology",
      "Philosophy",
      "Process (computing)",
      "Programming language",
      "Pure mathematics",
      "Reading (process)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Cilia",
        "given_name": "Nicole D."
      },
      {
        "surname": "De Stefano",
        "given_name": "Claudio"
      },
      {
        "surname": "Fontanella",
        "given_name": "Francesco"
      },
      {
        "surname": "Molinara",
        "given_name": "Mario"
      },
      {
        "surname": "Scotto di Freca",
        "given_name": "Alessandra"
      }
    ]
  },
  {
    "title": "DELP-DAR system for license plate detection and recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.007",
    "abstract": "Automatic License Plate detection and Recognition (ALPR) is a quite popular and active research topic in the field of computer vision, image processing and intelligent transport systems. ALPR is used to make detection and recognition processes more robust and efficient in highly complicated environments and backgrounds. Several research investigations are still necessary due to some constraints such as: completeness of numbering systems of countries, different colors, various languages, multiple sizes and varied fonts. For this, we present in this paper an automatic framework for License Plate (LP) detection and recognition from complex scenes. Our framework is based on mask region convolutional neural networks used for LP detection, segmentation and recognition. Although some studies have focused on LP detection, LP recognition, LP segmentation or just two of them, our study uses the maskr-cnn in the three stages. The evaluation of our framework is enhanced by four datasets for different countries and consequently with various languages. In fact, it tested on four datasets including images captured from multiple scenes under numerous conditions such as varied orientation, poor quality images, blurred images and complex environmental backgrounds. Extensive experiments show the robustness and efficiency of our suggested Extensive experiments show the robustness and efficiency of our suggested system that achieves in accuracy rate 99.3% on AOLP and 98.9% on Caltech dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303216",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Field (mathematics)",
      "Gene",
      "Image (mathematics)",
      "Image processing",
      "License",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Robustness (evolution)",
      "Segmentation",
      "Text detection"
    ],
    "authors": [
      {
        "surname": "Selmi",
        "given_name": "Zied"
      },
      {
        "surname": "Halima",
        "given_name": "Mohamed Ben"
      },
      {
        "surname": "Pal",
        "given_name": "Umapada"
      },
      {
        "surname": "Alimi",
        "given_name": "M. Adel"
      }
    ]
  },
  {
    "title": "A time-series prediction framework using sequential learning algorithms and dimensionality reduction within a sparsification approach",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.031",
    "abstract": "The adaptive kernel filters are sequential learning algorithms that operate in a particular functional space called a reproducing kernel Hilbert space. However, their performance depends on the selection of two hyper-parameters, i.e., kernel bandwidth and learning-rate. Besides, as these algorithms train the model using a sequence of input vectors, their computation scales with the number of samples. In this work, we propose to address the previous challenges of these sequential learning algorithms. The proposed framework, unlike similar methods, maximizes the correntropy function to optimize the bandwidth and learning-rate parameters. Further, we introduce a sparsification strategy based on dimensionality reduction to remove redundant samples. The framework is tested on both synthetic and real-world data sets, showing convergence to relatively low values of mean-square-error.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518308006",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Combinatorics",
      "Computation",
      "Computer science",
      "Computer security",
      "Curse of dimensionality",
      "Dimensionality reduction",
      "Genetics",
      "Geometry",
      "Hilbert space",
      "Kernel (algebra)",
      "Kernel method",
      "Key (lock)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Paleontology",
      "Rate of convergence",
      "Reduction (mathematics)",
      "Reproducing kernel Hilbert space",
      "Sequence (biology)",
      "Series (stratigraphy)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Garcia-Vega",
        "given_name": "S."
      },
      {
        "surname": "León-Gómez",
        "given_name": "E.A."
      },
      {
        "surname": "Castellanos-Dominguez",
        "given_name": "G."
      }
    ]
  },
  {
    "title": "An end-to-end deep learning system for medieval writer identification",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.025",
    "abstract": "This paper presents an end-to-end system to identify writers in medieval manuscripts. The proposed system consists in a three-step model for detection and classification of lines in the manuscript and page writer identification. The first two steps are based on deep neural networks trained with transfer learning techniques and specialized to solve the task in hand. The third stage is a weighted majority vote row-decision combiner that assigns to each page a writer. The main goal of this paper is to study the applicability of deep learning in this context when a relatively small training dataset is available. We tested our system with several state-of-the-art deep architectures on a digitized manuscript known as the Avila Bible, using only 9.6% of the total pages for training. Our approach proves to be very effective in identifying page writers, reaching a peak of 96.48% of accuracy and 96.56% of F1 score.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303460",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Botany",
      "Computer science",
      "Context (archaeology)",
      "Deep learning",
      "Economics",
      "End-to-end principle",
      "History",
      "Identification (biology)",
      "Machine learning",
      "Management",
      "Natural language processing",
      "Task (project management)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Cilia",
        "given_name": "N.D."
      },
      {
        "surname": "De Stefano",
        "given_name": "C."
      },
      {
        "surname": "Fontanella",
        "given_name": "F."
      },
      {
        "surname": "Marrocco",
        "given_name": "C."
      },
      {
        "surname": "Molinara",
        "given_name": "M."
      },
      {
        "surname": "Scotto Di Freca",
        "given_name": "A."
      }
    ]
  },
  {
    "title": "Combinatorial space of watershed hierarchies for image characterization",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.002",
    "abstract": "We propose a framework for image characterization using hierarchies of segmentations. For this purpose, we structure the space of hierarchies using the Gromov–Hausdorff distance. We propose different ways of combining hierarchies and study their properties thanks to the GH distance. We then expose how to leverage the combinatorial space of hierarchies to derive efficient image representations. This framework opens a path for a controlled exploration and use of the combinatorial space of hierarchies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303162",
    "keywords": [
      "Artificial intelligence",
      "Characterization (materials science)",
      "Computer science",
      "Image (mathematics)",
      "Leverage (statistics)",
      "Materials science",
      "Mathematics",
      "Nanotechnology",
      "Operating system",
      "Space (punctuation)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Fehri",
        "given_name": "Amin"
      },
      {
        "surname": "Velasco-Forero",
        "given_name": "Santiago"
      },
      {
        "surname": "Meyer",
        "given_name": "Fernand"
      }
    ]
  },
  {
    "title": "Depth estimation from stereo cameras through a curved transparent medium",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.012",
    "abstract": "In this paper, we propose a novel method for estimating depth values by stereo cameras through a curved transparent medium that causes refraction. Our method takes both the surface shape of the medium and the refraction into account. We model that the rays from the stereo cameras are refracted by a curved transparent medium whose inner surface is represented by a parametric model, assuming that the medium has constant thickness. The parameters of the model are estimated using a constrained optimization simply by attaching several markers on the inner surface. The depth value is then estimated by the triangulation considering the refraction based on the model. The experimental results show that our method yields consistently high error reduction rates with respect to the baseline method without considering the refraction caused by the medium. In addition, our method provides satisfactory estimates for various shapes of the medium.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303265",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Optics",
      "Parametric statistics",
      "Photometric stereo",
      "Physics",
      "Refraction",
      "Statistics",
      "Surface (topology)",
      "Triangulation"
    ],
    "authors": [
      {
        "surname": "Yoon",
        "given_name": "Seongwook"
      },
      {
        "surname": "Choi",
        "given_name": "Taehyeon"
      },
      {
        "surname": "Sull",
        "given_name": "Sanghoon"
      }
    ]
  },
  {
    "title": "A sparse structure for fast circle detection",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107022",
    "abstract": "In the paper, we present a circle detector that achieves the state-of-art performance in almost every type of image. The detector represents each circle instance by a set of equally distributed arcs and searches for the same number of edge points to cover these arcs. The new formulation leads to the voting in minimizing/maximizing way which is different from the typical accumulative way adopted by Hough transform. From the formulation, circle detection is then decomposed into radius-dependent and -independent part. The calculation of independent part is computationally expensive but shared by different radii. This decomposition gets rid of the redundant computation in handling multiple radii and therefore speeds up the detection process. Originated from the sparse nature of independent part, we design a sparse structure for its batch computation which is fulfilled in just one sweep of the edge points. Circle detector based on this sparse structure is then proposed which achieves the comparable time complexity as the algorithm based on Hough transform using 2D accumulator array. For testing, we created an information-rich dataset with images coming from multiple sources. It contains five categories and covers a wide spectrum of images, ranging from true color images to the binary ones. The experimental results demonstrate that the proposed approach outperforms the solutions based on accumulative voting.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303255",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computation",
      "Computer science",
      "Computer vision",
      "Detector",
      "Hough transform",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Su",
        "given_name": "Yuanqi"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaoning"
      },
      {
        "surname": "Cuan",
        "given_name": "Bonan"
      },
      {
        "surname": "Liu",
        "given_name": "Yuehu"
      },
      {
        "surname": "Wang",
        "given_name": "Zehao"
      }
    ]
  },
  {
    "title": "Scalable logo detection by self co-learning",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107003",
    "abstract": "Existing logo detection methods usually consider a small number of logo classes, limited images per class and assume fine-gained object bounding box annotations. This limits their scalability to real-world dynamic applications. In this work, we tackle these challenges by exploring a web data learning principle without the need for exhaustive manual labelling. Specifically, we propose a novel incremental learning approach, called Scalable Logo Self-co-Learning (SL2), capable of automatically self-discovering informative training images from noisy web data for progressively improving model capability in a cross-model co-learning manner. Moreover, we introduce a very large (2,190,757 images of 194 logo classes) logo dataset “WebLogo-2M” by designing an automatic data collection and processing method. Extensive comparative evaluations demonstrate the superiority of SL2 over the state-of-the-art strongly and weakly supervised detection models and contemporary web data learning approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303061",
    "keywords": [
      "Artificial intelligence",
      "Bounding overwatch",
      "Computer science",
      "Database",
      "Image (mathematics)",
      "Logo (programming language)",
      "Machine learning",
      "Minimum bounding box",
      "Object (grammar)",
      "Programming language",
      "Scalability"
    ],
    "authors": [
      {
        "surname": "Su",
        "given_name": "Hang"
      },
      {
        "surname": "Gong",
        "given_name": "Shaogang"
      },
      {
        "surname": "Zhu",
        "given_name": "Xiatian"
      }
    ]
  },
  {
    "title": "Component trees for image sequences and streams",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.038",
    "abstract": "Morphological hierarchies now form a well-established framework for (still) image modeling and processing. However, their extension to time-related data remains largely unexplored. In this paper, we address such a topic and show how to analyze image sequences with tree-based representations. To do so, we distinguish between three kinds of models, namely spatial, temporal and spatial-temporal hierarchies. For each of them, we review different strategies to build the hierarchy from an image sequence. We also propose some algorithms to update such trees when new images are appended to the series and we compared the time complexity with tree building from scratch. We illustrate our findings with the max and min-tree structures built on grayscale data provided by Satellite Image Time Series that are gathering a growing interest in Earth Observation. Besides, we provide a comparative study for different hierarchies with classification experiments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303587",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Binary tree",
      "Biology",
      "Component (thermodynamics)",
      "Computer science",
      "Data mining",
      "Economics",
      "Genetics",
      "Grayscale",
      "Hierarchy",
      "Image (mathematics)",
      "Market economy",
      "Mathematical analysis",
      "Mathematics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Physics",
      "Sequence (biology)",
      "Series (stratigraphy)",
      "Theoretical computer science",
      "Thermodynamics",
      "Tree (set theory)",
      "Tree structure"
    ],
    "authors": [
      {
        "surname": "Tuna",
        "given_name": "Caglayan"
      },
      {
        "surname": "Mirmahboub",
        "given_name": "Behzad"
      },
      {
        "surname": "Merciol",
        "given_name": "François"
      },
      {
        "surname": "Lefèvre",
        "given_name": "Sébastien"
      }
    ]
  },
  {
    "title": "Enhancing sentient embodied conversational agents with machine learning",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.035",
    "abstract": "Within the area of intelligent User Interfaces, we propose what we call Sentient Embodied Conversational Agents (SECAs): virtual characters able to engage users in complex conversations and to incorporate sentient capabilities similar to the ones humans have. This paper introduces SECAs together with their architecture and a publicly available software library that facilitates their inclusion in applications –such as educational and elder-care– requiring proactive and sensitive agent behaviours. In fact, we illustrate our proposal with a virtual tutor embedded in an educational application for children. The evaluation was performed in two stages: firstly, we tested a version with basic textual processing capabilities; and secondly, we evaluated a SECA with Machine-Learning-enhanced user understanding capabilities. The results show a significant improvement in users’ perception of the agent’s understanding capability. Indeed, the Response Error Rate decreased from 22.31% to 11.46% using ML techniques. Moreover, 99.33% of the participants consider the global experience of talking with the virtual tutor with sentient capabilities to be satisfactory.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303551",
    "keywords": [
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Dialog box",
      "Dialog system",
      "Embodied agent",
      "Embodied cognition",
      "Human–computer interaction",
      "Intelligent agent",
      "Neuroscience",
      "Perception",
      "Programming language",
      "Psychology",
      "Software agent",
      "TUTOR",
      "Virtual agent",
      "Visual arts",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Tellols",
        "given_name": "Dolça"
      },
      {
        "surname": "Lopez-Sanchez",
        "given_name": "Maite"
      },
      {
        "surname": "Rodríguez",
        "given_name": "Inmaculada"
      },
      {
        "surname": "Almajano",
        "given_name": "Pablo"
      },
      {
        "surname": "Puig",
        "given_name": "Anna"
      }
    ]
  },
  {
    "title": "Deep-learning framework to detect lung abnormality – A study with chest X-Ray and lung CT scan images",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.013",
    "abstract": "Lung abnormalities are highly risky conditions in humans. The early diagnosis of lung abnormalities is essential to reduce the risk by enabling quick and efficient treatment. This research work aims to propose a Deep-Learning (DL) framework to examine lung pneumonia and cancer. This work proposes two different DL techniques to assess the considered problem: (i) The initial DL method, named a modified AlexNet (MAN), is proposed to classify chest X-Ray images into normal and pneumonia class. In the MAN, the classification is implemented using with Support Vector Machine (SVM), and its performance is compared against Softmax. Further, its performance is validated with other pre-trained DL techniques, such as AlexNet, VGG16, VGG19 and ResNet50. (ii) The second DL work implements a fusion of handcrafted and learned features in the MAN to improve classification accuracy during lung cancer assessment. This work employs serial fusion and Principal Component Analysis (PCA) based features selection to enhance the feature vector. The performance of this DL frame work is tested using benchmark lung cancer CT images of LIDC-IDRI and classification accuracy (97.27%) is attained.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303277",
    "keywords": [
      "Abnormality",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Deep learning",
      "Geodesy",
      "Geography",
      "Internal medicine",
      "Lung",
      "Lung cancer",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)",
      "Principal component analysis",
      "Psychiatry",
      "Radiology",
      "Softmax function",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Bhandary",
        "given_name": "Abhir"
      },
      {
        "surname": "Prabhu",
        "given_name": "G. Ananth"
      },
      {
        "surname": "Rajinikanth",
        "given_name": "V."
      },
      {
        "surname": "Thanaraj",
        "given_name": "K. Palani"
      },
      {
        "surname": "Satapathy",
        "given_name": "Suresh Chandra"
      },
      {
        "surname": "Robbins",
        "given_name": "David E."
      },
      {
        "surname": "Shasky",
        "given_name": "Charles"
      },
      {
        "surname": "Zhang",
        "given_name": "Yu-Dong"
      },
      {
        "surname": "Tavares",
        "given_name": "João Manuel R.S."
      },
      {
        "surname": "Raja",
        "given_name": "N. Sri Madhava"
      }
    ]
  },
  {
    "title": "A bag of constrained informative deep visual words for image retrieval",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.011",
    "abstract": "In this paper, we propose a bag of constrained informative deep visual words (BoCIDVW) model for image retrieval. Informative patches from each image are first obtained using patch entropy values. Each such patch is represented by deep features extracted through VGG16-Net. Two sets of constraints, namely, the must-link (ML) and the cannot-link (CL), are obtained for each deep informative patch in an unsupervised manner from its mutual information values (with other patches). The patches are then quantized using the Linear-time Constrained Vector Quantization Error (LCVQE), a fast yet accurate constrained K-means algorithm. The resulting clusters, which we term constrained informative deep visual words, are employed to label each patch. Finally, a bag (histogram) of constrained informative visual words is developed for image retrieval. Experiments on three different publicly available datasets demonstrate the merit of the proposed formulation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303253",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Entropy (arrow of time)",
      "Histogram",
      "Image (mathematics)",
      "Image retrieval",
      "Mutual information",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantization (signal processing)",
      "Quantum mechanics",
      "Visual Word"
    ],
    "authors": [
      {
        "surname": "Mukherjee",
        "given_name": "Anindita"
      },
      {
        "surname": "Sil",
        "given_name": "Jaya"
      },
      {
        "surname": "Sahu",
        "given_name": "Abhimanyu"
      },
      {
        "surname": "Chowdhury",
        "given_name": "Ananda S."
      }
    ]
  },
  {
    "title": "Improved local search for graph edit distance",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.10.028",
    "abstract": "The graph edit distance (GED) measures the dissimilarity between two graphs as the minimal cost of a sequence of elementary operations transforming one graph into another. This measure is fundamental in many areas such as structural pattern recognition or classification. However, exactly computing GED is NP -hard. Among different classes of heuristic algorithms that were proposed to compute approximate solutions, local search based algorithms provide the tightest upper bounds for GED. In this paper, we present K-REFINE and RANDPOST. K-REFINE generalizes and improves an existing local search algorithm and performs particularly well on small graphs. RANDPOST is a general warm start framework that stochastically generates promising initial solutions to be used by any local search based GED algorithm. It is particularly efficient on large graphs. An extensive empirical evaluation demonstrates that both K-REFINE and RANDPOST perform excellently in practice.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786551930306X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Edit distance",
      "Graph",
      "Heuristic",
      "Local search (optimization)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Boria",
        "given_name": "Nicolas"
      },
      {
        "surname": "Blumenthal",
        "given_name": "David B."
      },
      {
        "surname": "Bougleux",
        "given_name": "Sébastien"
      },
      {
        "surname": "Brun",
        "given_name": "Luc"
      }
    ]
  },
  {
    "title": "Learning the Principles of Art History with convolutional neural networks",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.008",
    "abstract": "Understanding the historical transformation of artistic styles implies the recognition of different stylistic properties. From a computer vision perspective, stylistic properties represent complex image features. In our work we explore the use of convolutional neural networks for learning features that are relevant for understanding properties of artistic styles. We focus on stylistic properties described by Heinrich Wölfflin in his book Principles of Art History (1915). Wölfflin identified five key visual principles, each defined by two contrasting concepts. We refer to each principle as one high-level image feature that measures how much each of the contrasting concepts is present in an image. We introduce convolutional neural network regression models trained to predict values of the five Wölfflin’s features. We provide quantitative and qualitative evaluations of those predictions, as well as analyze how the predicted values relate to different styles and artists. The outcome of our analysis suggests that the models learn to discriminate meaningful features that correspond to the visual characteristics of concepts described by Wölfflin. This indicates that the presented approach can be used to enable new ways of exploring fine art collections based on image features relevant and well-known within art history.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303228",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Feature (linguistics)",
      "Focus (optics)",
      "Gene",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Natural language processing",
      "Optics",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Philosophy",
      "Physics",
      "Style (visual arts)",
      "Transformation (genetics)",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Cetinic",
        "given_name": "Eva"
      },
      {
        "surname": "Lipic",
        "given_name": "Tomislav"
      },
      {
        "surname": "Grgic",
        "given_name": "Sonja"
      }
    ]
  },
  {
    "title": "Integrating prediction and reconstruction for anomaly detection",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.024",
    "abstract": "Anomaly detection in videos refers to identifying events that rarely or shouldn’t happen in a certain context. Among all existing methods, the idea of reconstruction or future frame prediction is predominant for detecting anomalies. Reconstruction methods try to minimize the reconstruction errors of training data, but cannot guarantee large reconstruction errors for abnormal events. Future frame prediction methods follow the concept that normal events are predictable while abnormal ones are unpredictable. However, the results may drop rapidly since prediction is not robust to the noise in real-world surveillance videos. In this paper, we propose an approach that combines the advantages and balances the disadvantages of these two methods. An end-to-end network is designed to conduct future frame prediction and reconstruction sequentially. Future frame prediction makes the reconstruction errors large enough to facilitate the identification of abnormal events, while reconstruction helps enhance the predicted future frames from normal events. Specifically, we connect two U-Net blocks in the generator. One block works in the form of frame prediction, and the other tries to reconstruct the frames generated by the former block. Experiments over several benchmark datasets demonstrate the superiority of our method over previous state-of-the-art approaches, while running in real-time at 30 frames per second.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303447",
    "keywords": [
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Computer science",
      "Condensed matter physics",
      "Data mining",
      "Pattern recognition (psychology)",
      "Physics"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Yao"
      },
      {
        "surname": "Zhao",
        "given_name": "Lin"
      },
      {
        "surname": "Zhang",
        "given_name": "Shanshan"
      },
      {
        "surname": "Gong",
        "given_name": "Chen"
      },
      {
        "surname": "Li",
        "given_name": "Guangyu"
      },
      {
        "surname": "Yang",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "Explaining digital humanities by aligning images and textual descriptions",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.018",
    "abstract": "Replicating the human ability to connect Vision and Language has recently been gaining a lot of attention in the Computer Vision and the Natural Language Processing communities. This research effort has resulted in algorithms that can retrieve images from textual descriptions and vice versa, when realistic images and sentences with simple semantics are employed and when paired training data is provided. In this paper, we go beyond these limitations and tackle the design of visual-semantic algorithms in the domain of the Digital Humanities. This setting not only advertises more complex visual and semantic structures but also features a significant lack of training data which makes the use of fully-supervised approaches infeasible. With this aim, we propose a joint visual-semantic embedding that can automatically align illustrations and textual elements without paired supervision. This is achieved by transferring the knowledge learned on ordinary visual-semantic datasets to the artistic domain. Experiments, performed on two datasets specifically designed for this domain, validate the proposed strategies and quantify the domain shift between natural images and artworks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303381",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Domain (mathematical analysis)",
      "Embedding",
      "Epistemology",
      "Information retrieval",
      "Mathematical analysis",
      "Mathematics",
      "Natural language",
      "Natural language processing",
      "Philosophy",
      "Programming language",
      "Semantics (computer science)",
      "Simple (philosophy)"
    ],
    "authors": [
      {
        "surname": "Cornia",
        "given_name": "Marcella"
      },
      {
        "surname": "Stefanini",
        "given_name": "Matteo"
      },
      {
        "surname": "Baraldi",
        "given_name": "Lorenzo"
      },
      {
        "surname": "Corsini",
        "given_name": "Massimiliano"
      },
      {
        "surname": "Cucchiara",
        "given_name": "Rita"
      }
    ]
  },
  {
    "title": "In-classroom learning analytics based on student behavior, topic and teaching characteristic mining",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.023",
    "abstract": "The automatic analysis of students’ in-classroom behavior is valuable to evaluate the effect of teaching. Recent studies of in-classroom video analysis mainly focus on lecture content, positions and identities of students. In this paper, we propose to analyze the students’ concentration degree to the teacher or teaching content. Specifically, we detect students’ faces, track faces, and analyze the students’ behavior, i.e. raising or downing faces and corresponding head orientations to the teacher, teaching content or not. Besides, texts are obtained from the teacher’s speech and the course topics taught in the class are extracted. Audio features of the teacher’s speech are extracted and analyzed. Finally, the correlation of the students’ concentration degree with the course topics, audio features are analyzed. This analysis can help teachers find the effective teaching characteristic to better improve students’ concentration degree.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303435",
    "keywords": [
      "Analytics",
      "Artificial intelligence",
      "Computer science",
      "Data science",
      "Learning analytics",
      "Mathematics education",
      "Psychology"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Bohong"
      },
      {
        "surname": "Yao",
        "given_name": "Zeping"
      },
      {
        "surname": "Lu",
        "given_name": "Hong"
      },
      {
        "surname": "Zhou",
        "given_name": "Yaqian"
      },
      {
        "surname": "Xu",
        "given_name": "Jinkai"
      }
    ]
  },
  {
    "title": "Dynamic graph convolutional networks",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107000",
    "abstract": "In many different classification tasks it is required to manage structured data, which are usually modeled as graphs. Moreover, these graphs can be dynamic, meaning that the vertices/edges of each graph may change over time. The goal is to exploit existing neural network architectures to model datasets that are best represented with graph structures that change over time. To the best of the authors’ knowledge, this task has not been addressed using these kinds of architectures. Two novel approaches are proposed, which combine Long Short-Term Memory networks and Graph Convolutional Networks to learn long short-term dependencies together with graph structure. The advantage provided by the proposed methods is confirmed by the results achieved on four real world datasets: an increase of up to 12 percentage points in Accuracy and F1 scores for vertex-based semi-supervised classification and up to 2 percentage points in Accuracy and F1 scores for graph-based supervised classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303036",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Convolutional neural network",
      "Exploit",
      "Graph",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Manessi",
        "given_name": "Franco"
      },
      {
        "surname": "Rozza",
        "given_name": "Alessandro"
      },
      {
        "surname": "Manzo",
        "given_name": "Mario"
      }
    ]
  },
  {
    "title": "Reduced data set for multi-target recognition using compressed sensing frame",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.10.027",
    "abstract": "Oceanographic plankton classification for many images is time consuming, especially for low-contrast images obtained when the water is dirty and opaque due to impurities. A novel, overcomplete dictionary algorithm is studied by analyzing the sparse characteristics of the image matrix using pixel values. The features in hyperspace are mapped onto a specifically designed vector space. Thus, mathematically, the algorithm exhibits faster calculating convergence and has a strong expressivity for the selective signal in the vector space with a lower signal loss rate. The clustering method based on the dictionary can classify planktons for species counting, which can enable high-speed, multi-object recognition of planktons in turbid water. The experimental results demonstrate that if less data (up to 60%) is processed for each image, a recall rate and accuracy greater than 75% and a structural similarity for the reconstructed image greater than 0.9 can be achieved.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303071",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Data set",
      "Frame (networking)",
      "Frame rate",
      "Hyperspace",
      "Image (mathematics)",
      "Pattern recognition (psychology)",
      "Pixel",
      "Programming language",
      "SIGNAL (programming language)",
      "Set (abstract data type)",
      "Similarity (geometry)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Cheng",
        "given_name": "Xuemin"
      },
      {
        "surname": "Dong",
        "given_name": "Changqing"
      },
      {
        "surname": "Ren",
        "given_name": "Yong"
      },
      {
        "surname": "Cheng",
        "given_name": "Kaichang"
      },
      {
        "surname": "Yan",
        "given_name": "Lei"
      },
      {
        "surname": "Hu",
        "given_name": "Yao"
      },
      {
        "surname": "Hao",
        "given_name": "Qun"
      }
    ]
  },
  {
    "title": "Salient video object detection using a virtual border and guided filter",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.106998",
    "abstract": "In this paper, we present a novel method for salient object detection in videos. Salient object detection methods based on background prior may miss salient region when the salient object touches the frame borders. To solve this problem, we propose to detect the whole salient object via the adjunction of virtual borders. A guided filter is then applied on the temporal output to integrate the spatial edge information for a better detection of the salient object edges. At last, a global spatio-temporal saliency map is obtained by combining the spatial saliency map and the temporal saliency map together according to the entropy. The proposed method is assessed on three popular datasets (Fukuchi, FBMS and VOS) and compared to several state-of-the-art methods. The experimental results show that the proposed approach outperforms the tested methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303012",
    "keywords": [
      "Adjunction",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Entropy (arrow of time)",
      "Filter (signal processing)",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Physics",
      "Pure mathematics",
      "Quantum mechanics",
      "Saliency map",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Qiong"
      },
      {
        "surname": "Zhang",
        "given_name": "Lu"
      },
      {
        "surname": "Zou",
        "given_name": "Wenbin"
      },
      {
        "surname": "Kpalma",
        "given_name": "Kidiyo"
      }
    ]
  },
  {
    "title": "Sparsification of core set models in non-metric supervised learning",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.10.024",
    "abstract": "Supervised learning employing positive semi definite kernels has gained wide attraction and lead to a variety of successful machine learning approaches. The restriction to positive semi definite kernels and a hilbert space is common to simplify the mathematical derivations of the respective learning methods, but is also limiting because more recent research indicates that non-metric, and therefore non positive semi definite, data representations are often more effective. This challenge is addressed by multiple approaches and recently dedicated algorithms for so called indefinite learning have been proposed. Along this line, the Krĕin space Support Vector Machine (KSVM) and variants are very efficient classifiers for indefinite learning problems, but with a non-sparse decision function. This very dense decision function prevents practical applications due to a costly out of sample extension. We focus on this problem and provide two post processing techniques to sparsify models as obtained by a Krĕin space SVM approach. In particular we consider the indefinite Core Vector Machine and indefinite Core Vector Regression Machine which are both efficient for psd kernels, but suffer from the same dense decision function, if the Krĕin space approach is used. We evaluate the influence of different levels of sparsity and employ a Nyström approach to address large scale problems. Experiments show that our algorithm is similar efficient as the non-sparse Krĕin space Support Vector Machine but with substantially lower costs, such that also problems of larger scale can be processed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303034",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Economics",
      "Eigenvalues and eigenvectors",
      "Evolutionary biology",
      "Function (biology)",
      "Hilbert space",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Metric (unit)",
      "Operating system",
      "Operations management",
      "Physics",
      "Positive-definite matrix",
      "Quantum mechanics",
      "Reproducing kernel Hilbert space",
      "Space (punctuation)",
      "Supervised learning",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Schleif",
        "given_name": "Frank-Michael"
      },
      {
        "surname": "Raab",
        "given_name": "Christoph"
      },
      {
        "surname": "Tino",
        "given_name": "Peter"
      }
    ]
  },
  {
    "title": "Rough segmentation of coherent local intensity for bias induced 3-D MR brain images",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.106997",
    "abstract": "Segmentation of brain MR volumes into different meaningful tissue classes is an essential prerequisite for many clinical analyses. However, intensity inhomogeneity or bias field, present in MR volumes, considerably degrades the quality of segmentation. In this regard, the paper presents a new segmentation algorithm, termed as CoLoRS (Coherent Local Intensity Rough Segmentation), for brain MR volumes corrupted with bias field artifact. It judiciously integrates the merits of coherent local intensity clustering and the theory of rough sets for simultaneous segmentation and bias field correction of brain MR volumes. The proposed algorithm partitions the entire image space into a number of small overlapping neighborhood regions. The bias, in each neighborhood region, is assumed to be constant. For each individual region, an objective function is defined for coherent local intensity rough segmentation. The voxels near the center point have similar influences on local objective function. In addition, the smaller distance between center and neighboring voxels yields more contribution on the voxel of interest. The proposed algorithm uses the dual-region concept to represent the neighborhood structure more efficiently. It makes possible of separate modeling of the voxels within neighborhood, according to their locations. Each region is considered to have several tissue classes, where each tissue class consists of a core region and an overlapping region. The segmentation in fuzzy approximation spaces provides an effective mean for brain MR volume analysis, as it handles overlapping partitions and addresses vagueness in tissue class definition. The effectiveness of the proposed algorithm, along with a comparison with existing approaches, is demonstrated on several publicly available brain MR data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303000",
    "keywords": [
      "Algorithm",
      "Artifact (error)",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Image segmentation",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Scale-space segmentation",
      "Segmentation",
      "Segmentation-based object categorization",
      "Voxel"
    ],
    "authors": [
      {
        "surname": "Roy",
        "given_name": "Shaswati"
      },
      {
        "surname": "Maji",
        "given_name": "Pradipta"
      }
    ]
  },
  {
    "title": "Hyperspectral anomaly detection via density peak clustering",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.022",
    "abstract": "In the last few years, a density peak clustering algorithm (DP) has demonstrated its advantages in hyperspectral data analysis and processing. In this letter, we take the benefits of the DP algorithm to the hyperspectral anomaly detection, to circumvent two negative aspects which affect the detection performance: The untenable supposition of the Gaussian distribution and the contamination of the background statistics caused by anomalies. Specifically, the proposed DP-based hyperspectral anomaly detection method is implemented as follows: A hyperspectral image (HSI) is first divided into local windows to address computationally expensive density computations. In each local window, the DP is performed to calculate the density of each pixel. Finally, we detect anomalies using the obtained density map, based on that anomalies are generally with low probability of existence in the image and thus have low densities. Experimental results obtained on four real hyperspectral datasets demonstrate that the detection performance of the proposed method is superior to some widely used anomaly detection methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303423",
    "keywords": [
      "Algorithm",
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Cluster analysis",
      "Computation",
      "Computer science",
      "Condensed matter physics",
      "Gaussian",
      "Hyperspectral imaging",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Tu",
        "given_name": "Bing"
      },
      {
        "surname": "Yang",
        "given_name": "Xianchang"
      },
      {
        "surname": "Li",
        "given_name": "Nanying"
      },
      {
        "surname": "Zhou",
        "given_name": "Chengle"
      },
      {
        "surname": "He",
        "given_name": "Danbing"
      }
    ]
  },
  {
    "title": "In-air handwritten Chinese text recognition with temporal convolutional recurrent network",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107025",
    "abstract": "As a new human-computer interaction way, in-air handwriting allows users to perform gesture-based writing in the midair. However, most existing in-air handwriting systems mainly focus on recognizing either isolated characters/words or only a small number of texts, making those systems far from practical applications. Instead, here we present a 3D in-air handwritten Chinese text recognition (IAHCTR) system for the first time, and construct the first public large-scale IAHCT dataset. Moreover, a novel architecture, named the temporal convolutional recurrent network (TCRN), is proposed for online HCTR. Specifically, the TCRN first applies the 1-dimensional convolution to extract local contextual features from low-level trajectories, and then it utilizes the recurrent network to capture long-term dependencies of high-level outputs. Compared with the state-of-the-art architecture, the TCRN not only avoids the domain-specific knowledge for feature image extraction, but also attains higher training efficiency with a more compact model. Empirically, this TCRN also outperforms the single recurrent network with faster prediction and higher accuracy. Experiments on CASIA-OLHWDB2 & ICDAR-2013 demonstrate that the TCRN yields the best result in comparison to the state-of-the-art methods for online HCTR.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303280",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Gan",
        "given_name": "Ji"
      },
      {
        "surname": "Wang",
        "given_name": "Weiqiang"
      },
      {
        "surname": "Lu",
        "given_name": "Ke"
      }
    ]
  },
  {
    "title": "Exploring diverse and fine-grained caption for video by incorporating convolutional architecture into LSTM-based model",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.003",
    "abstract": "As a fundamental problem in visual understanding, video captioning has attracted much attention from both computer vision and natural language processing communities. Despite recent emergence of video captioning methods, how to generate diverse and fine-grained video description is far from being solved. To this end, this work makes the following contributions. First, a novel high-quality video captioning system featuring hierarchical long short-term memory structure and dual-stage loss is designed to translate videos to sentences. Second, we incorporate the convolutional architecture into our captioning system with the aim of generating diverse and fine-grained description. Third, we propose a novel performance evaluation metric named LTMS to assess the fine-grained captions. The experimental results on the benchmark datasets MSVD and MSR-VTT indicate the effectiveness of the proposed model, achieving superior performance over state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303174",
    "keywords": [
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Closed captioning",
      "Computer science",
      "Convolutional neural network",
      "Economics",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Metric (unit)",
      "Natural language processing",
      "Operations management",
      "Programming language",
      "Semantics (computer science)",
      "Speech recognition",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Xiao",
        "given_name": "Huanhou"
      },
      {
        "surname": "Xu",
        "given_name": "Junwei"
      },
      {
        "surname": "Shi",
        "given_name": "Jinglun"
      }
    ]
  },
  {
    "title": "Selective feature connection mechanism: Concatenating multi-layer CNN features with a feature selector",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.015",
    "abstract": "Different layers of deep convolutional neural networks(CNNs) can encode different-level information. High-layer features always contain more semantic information, and low-layer features contain more detail information. However, low-layer features suffer from the background clutter and semantic ambiguity. During visual recognition, the feature combination of the low-layer and high-level features plays an important role in context modulation. If directly combining the high-layer and low-layer features, the background clutter and semantic ambiguity may be caused due to the introduction of detailed information. In this paper, we propose a general network architecture to concatenate CNN features of different layers in a simple and effective way, called Selective Feature Connection Mechanism (SFCM). Low-level features are selectively linked to high-level features with a feature selector which is generated by high-level features. The proposed connection mechanism can effectively overcome the above-mentioned drawbacks. We demonstrate the effectiveness, superiority, and universal applicability of this method on multiple challenging computer vision tasks, including image classification, scene text detection, and image-to-image translation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303290",
    "keywords": [
      "Ambiguity",
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Clutter",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Convolutional neural network",
      "ENCODE",
      "Feature (linguistics)",
      "Feature extraction",
      "Gene",
      "Layer (electronics)",
      "Linguistics",
      "Organic chemistry",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Radar",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Du",
        "given_name": "Chen"
      },
      {
        "surname": "Wang",
        "given_name": "Yanna"
      },
      {
        "surname": "Wang",
        "given_name": "Chunheng"
      },
      {
        "surname": "Shi",
        "given_name": "Cunzhao"
      },
      {
        "surname": "Xiao",
        "given_name": "Baihua"
      }
    ]
  },
  {
    "title": "A torus model for optical flow",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.029",
    "abstract": "We propose a torus model for high-contrast patches of optical flow. Our model is derived from a database of ground-truth optical flow from the computer-generated video Sintel, collected by Butler et al. in A naturalistic open source movie for optical flow evaluation. Using persistent homology and zigzag persistence, popular tools from the field of computational topology, we show that the high-contrast 3 × 3 patches from this video are well-modeled by a torus, a nonlinear 2-dimensional manifold. Furthermore, we show that the optical flow torus model is naturally equipped with the structure of a fiber bundle, related to the statistics of range image patches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303514",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bundle",
      "Combinatorics",
      "Composite material",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Fiber bundle",
      "Flow (mathematics)",
      "Geometry",
      "Image (mathematics)",
      "Manifold (fluid mechanics)",
      "Materials science",
      "Mathematics",
      "Mechanical engineering",
      "Optical flow",
      "Persistent homology",
      "Segmentation",
      "Topology (electrical circuits)",
      "Torus"
    ],
    "authors": [
      {
        "surname": "Adams",
        "given_name": "Henry"
      },
      {
        "surname": "Bush",
        "given_name": "Johnathan"
      },
      {
        "surname": "Carr",
        "given_name": "Brittany"
      },
      {
        "surname": "Kassab",
        "given_name": "Lara"
      },
      {
        "surname": "Mirth",
        "given_name": "Joshua"
      }
    ]
  },
  {
    "title": "Moving object detection under different weather conditions using full-spectrum light sources",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.004",
    "abstract": "The moving object detection always remains an active field of research given the variety of challenges related to this topic. In fact, most of the challenges related to the low illumination and weather conditions (fog, snow, rain, etc.) remain unresolved and require more developments. In this paper, our intrinsic objective is to overcome these challenges using an effective moving object detection method. Unlike most works in the literature that use one of the two infrared or visible spectra independently, we proposed a Moving Object Detection method based on background modeling using the Full-Spectrum Light Sources (FSLS-MOD). To better ensure the adaptability and independence of the moving object speeds and sizes, the principle of the inter-frame differences’ methods is introduced in the background modeling stage. Furthermore, we applied a new strategy to switch between the spectra allowing us to benefit from the advantages of each spectrum and carry out a better moving object detection even in bad weather conditions. An experimental study by quantitative and qualitative evaluations proved the robustness and effectiveness of our proposed method of moving object detection using the switching strategy between full-spectrum light sources under different illuminations and weather conditions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303186",
    "keywords": [
      "Adaptability",
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Ecology",
      "Frame (networking)",
      "Gene",
      "Geography",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Remote sensing",
      "Robustness (evolution)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Rebai Boukhriss",
        "given_name": "Rania"
      },
      {
        "surname": "Fendri",
        "given_name": "Emna"
      },
      {
        "surname": "Hammami",
        "given_name": "Mohamed"
      }
    ]
  },
  {
    "title": "Attributes-aided part detection and refinement for person re-identification",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107016",
    "abstract": "Person attributes are often exploited as mid-level human semantic information to help promote the performance of person re-identification task. In this paper, unlike most existing methods simply taking the attribute learning as a classification problem, we perform it in a different way with the motivation that attributes are related to specific local regions, which refers to the perceptual ability of attributes. We utilize the process of attribute detection to generate corresponding attribute-part detectors, whose invariance to many influences like poses and camera views can be guaranteed. With detected local part regions, our model extracts local part features to handle the body part misalignment problem, which is another major challenge for person re-identification. The local descriptors are further refined by fused attribute information to eliminate interferences caused by detection deviation. Finally, the refined local feature works together with a holistic-level feature to constitute our final feature representation. Extensive experiments on two popular benchmarks with attribute annotations demonstrate the effectiveness of our model and competitive performance compared with state-of-the-art algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132031930319X",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Data mining",
      "Economics",
      "Feature (linguistics)",
      "Identification (biology)",
      "Law",
      "Linguistics",
      "Machine learning",
      "Management",
      "Object (grammar)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Process (computing)",
      "Representation (politics)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Shuzhao"
      },
      {
        "surname": "Yu",
        "given_name": "Huimin"
      },
      {
        "surname": "Hu",
        "given_name": "Roland"
      }
    ]
  },
  {
    "title": "Similarity learning with joint transfer constraints for person re-identification",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107014",
    "abstract": "The inconsistency of data distributions among multiple views is one of the most crucial issues which hinder the accuracy of person re-identification. To solve the problem, this paper presents a novel similarity learning model by combining the optimization of feature representation via multi-view visual words reconstruction and the optimization of metric learning via joint discriminative transfer learning. The starting point of the proposed model is to capture multiple groups of multi-view visual words (MvVW) through an unsupervised clustering method (i.e. K-means) from human parts (e.g. head, torso, legs). Then, we construct a joint feature matrix by combining multi-group feature matrices with different body parts. To solve the inconsistent distributions under different views, we propose a method of joint transfer constraint to learn the similarity function by combining multiple common subspaces, each in charge of a sub-region. In the common subspaces, the original samples can be reconstructed based on MvVW under low-rank and sparse representation constraints, which can enhance the structure robustness and noise resistance. During the process of objective function optimization, based on confinement fusion of multi-view and multiple sub-regions, a solution strategy is proposed to solve the objective function using joint matrix transform. Taking all of these into account, the issue of person re-identification under inconsistent data distributions can be transformed into a consistent iterative convex optimization problem, and solved via the inexact augmented Lagrange multiplier (IALM) algorithm. Extensive experiments are conducted on three challenging person re-identification datasets (i.e., VIPeR, CUHK01 and PRID450S), which shows that our model outperforms several state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303176",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Cluster analysis",
      "Computer science",
      "Discriminative model",
      "Feature learning",
      "Feature vector",
      "Gene",
      "Geometry",
      "Linear subspace",
      "Mathematics",
      "Optimization problem",
      "Pattern recognition (psychology)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Cairong"
      },
      {
        "surname": "Wang",
        "given_name": "Xuekuan"
      },
      {
        "surname": "Zuo",
        "given_name": "Wangmeng"
      },
      {
        "surname": "Shen",
        "given_name": "Fumin"
      },
      {
        "surname": "Shao",
        "given_name": "Ling"
      },
      {
        "surname": "Miao",
        "given_name": "Duoqian"
      }
    ]
  },
  {
    "title": "Multi-scale sequential network for semantic text segmentation and localization",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.001",
    "abstract": "We present a novel method for semantic text document analysis which in addition to localizing text it labels the text in user-defined semantic categories. More precisely, it consists of a fully-convolutional and sequential network that we apply to the particular case of slide analysis to detect title, bullets and standard text. Our contributions are twofold: (1) A multi-scale network consisting of a series of stages that sequentially refine the prediction of text and semantic labels (text, title, bullet); (2) A synthetic database of slide images with text and semantic annotation that is used to train the network with abundant data and wide variability in text appearance, slide layouts, and noise such as compression artifacts. We evaluate our method on a collection of real slide images collected from multiple conferences, and show that it is able to localize text with an accuracy of 95%, and to classify titles and bullets with accuracies of 94% and 85% respectively. In addition, we show that our method is competitive on scene and born-digital image datasets, such as ICDAR 2011, where it achieves an accuracy of 91.1%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303150",
    "keywords": [
      "Annotation",
      "Artificial intelligence",
      "Computer science",
      "Information retrieval",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Villamizar",
        "given_name": "Michael"
      },
      {
        "surname": "Canévet",
        "given_name": "Olivier"
      },
      {
        "surname": "Odobez",
        "given_name": "Jean-Marc"
      }
    ]
  },
  {
    "title": "Delaunay triangulation based text detection from multi-view images of natural scene",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.021",
    "abstract": "Text detection in the wild is still considered as a challenging issue to the researchers because of its several real time applications like forensic application, where CCTV camera captures images at different angles of the same scene. Unlike the existing methods that consider a single view captured orthogonally for text detection, this paper considers multi-view (view-1 and view-2 of the same spot) of the same scene captured at different angles or different height distances for text detection. For each pair of the same scene, the proposed method extracts features that describe characteristics of text components based on Delaunay Triangulation (DT), namely corner points, area and cavity of the DT. The features of corresponding DT in view-1 and view-2 are compared through cosine distance measure to estimate the similarity between two components of respective view-1 and view-2. If the pair satisfies the similarity condition, the components are considered as Candidate Text Components (CTC). In other words, these are the common components for view-1 and view-2 that satisfy the similarity condition. From each CTC of view-1 and view-2, the proposed method finds nearest neighbor components to restore the components of the same text line based on estimating degree of similarly between CTC and neighbor components using Chi-square and cosine distance measures. Furthermore, the proposed method uses a recognition step to detect correct texts by comparing recognition results of view-1 and view-2. The same recognition step is used for removing false positives to improve the performance of the proposed method. Experimental results on our own dataset, which contains pair of images of different situations, and the standard datasets, namely, ICDAR 2013, MSRATD-500, CTW1500, Total-text, ICDAR 2017 MLT and COCO-text, show that the proposed method outperforms the existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303393",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Connected component",
      "Cosine similarity",
      "Data mining",
      "Delaunay triangulation",
      "False positive paradox",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Measure (data warehouse)",
      "Pattern recognition (psychology)",
      "Similarity (geometry)",
      "Triangulation",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Roy",
        "given_name": "Soumyadip"
      },
      {
        "surname": "Shivakumara",
        "given_name": "Palaiahnakote"
      },
      {
        "surname": "Pal",
        "given_name": "Umapada"
      },
      {
        "surname": "Lu",
        "given_name": "Tong"
      },
      {
        "surname": "Kumar",
        "given_name": "Govindaraj Hemantha"
      }
    ]
  },
  {
    "title": "L 1-Subspace Tracking for Streaming Data",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.106992",
    "abstract": "High-dimensional data usually exhibit intrinsic low-rank structures. With tremendous amount of streaming data generated by ubiquitous sensors in the world of Internet-of-Things, fast detection of such low-rank pattern is of utmost importance to a wide range of applications. In this work, we present an L 1-subspace tracking method to capture the low-rank structure of streaming data. The method is based on the L 1-norm principal-component analysis (L 1-PCA) theory that offers outlier resistance in subspace calculation. The proposed method updates the L 1-subspace as new data are acquired by sensors. In each time slot, the conformity of each datum is measured by the L 1-subspace calculated in the previous time slot and used to weigh the datum. Iterative weighted L 1-PCA is then executed through a refining function. The superiority of the proposed L 1-subspace tracking method compared to existing approaches is demonstrated through experimental studies in various application fields.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132031930295X",
    "keywords": [
      "Artificial intelligence",
      "Cartography",
      "Combinatorics",
      "Composite material",
      "Computer science",
      "Data mining",
      "Geodetic datum",
      "Geography",
      "Materials science",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Principal component analysis",
      "Psychology",
      "Range (aeronautics)",
      "Rank (graph theory)",
      "Subspace topology",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Ying"
      },
      {
        "surname": "Tountas",
        "given_name": "Konstantinos"
      },
      {
        "surname": "Pados",
        "given_name": "Dimitris A."
      },
      {
        "surname": "Batalama",
        "given_name": "Stella N."
      },
      {
        "surname": "Medley",
        "given_name": "Michael J."
      }
    ]
  },
  {
    "title": "Minimum margin loss for deep face recognition",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107012",
    "abstract": "Face recognition has achieved great success owing to the fast development of deep neural networks in the past few years. Different loss functions can be used in a deep neural network resulting in different performance. Most recently some loss functions have been proposed, which have advanced the state of the art. However, they cannot solve the problem of margin bias which is present in class imbalanced datasets, having the so-called long-tailed distributions. In this paper, we propose to solve the margin bias problem by setting a minimum margin for all pairs of classes. We present a new loss function, Minimum Margin Loss (MML), which is aimed at enlarging the margin of those overclose class centre pairs so as to enhance the discriminative ability of the deep features. MML, together with Softmax Loss and Centre Loss, supervises the training process to balance the margins of all classes irrespective of their class distributions. We implemented MML in Inception-ResNet-v1 and conducted extensive experiments on seven face recognition benchmark datasets, MegaFace, FaceScrub, LFW, SLLFW, YTF, IJB-B and IJB-C. Experimental results show that the proposed MML loss function has led to new state of the art in face recognition, reducing the negative effect of margin bias.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303152",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Deep learning",
      "Deep neural networks",
      "Discriminative model",
      "Evolutionary biology",
      "Face (sociological concept)",
      "Facial recognition system",
      "Function (biology)",
      "Geodesy",
      "Geology",
      "Machine learning",
      "Margin (machine learning)",
      "Pattern recognition (psychology)",
      "Social science",
      "Sociology",
      "Softmax function"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Xin"
      },
      {
        "surname": "Wang",
        "given_name": "Hui"
      },
      {
        "surname": "Scotney",
        "given_name": "Bryan"
      },
      {
        "surname": "Wan",
        "given_name": "Huan"
      }
    ]
  },
  {
    "title": "Incremental bit-quads count in component trees: Theory, algorithms, and optimization",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.10.036",
    "abstract": "Component tree is a full image representation which encodes all connected components from upper (resp. lower) level sets of a given image through the inclusion relation. Information from this representation can be used in many image processing and computational vision applications, e.g. connected filtering, image segmentation, feature extraction, among others. In general, each node of a component tree represents a connected component of a level set and stores attributes which describes features of this connected component. This paper presents a review of a previously published method to compute attributes such as area, perimeter, and number of Euler by incrementally counting patterns while traversing nodes of a component tree. This method foundation is further detailed in this paper by presenting a novel theoretical background and algorithm correctness intuition. We also present a novel approach for this algorithm showing improvements for run-time execution and precision analysis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303149",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Bit (key)",
      "Component (thermodynamics)",
      "Computer science",
      "Computer security",
      "Mathematics",
      "Physics",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Silva",
        "given_name": "Dennis J."
      },
      {
        "surname": "Alves",
        "given_name": "Wonder A.L."
      },
      {
        "surname": "Hashimoto",
        "given_name": "Ronaldo Fumio"
      }
    ]
  },
  {
    "title": "Data security based on homographic function",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.10.032",
    "abstract": "In this paper, we adapt a homographic function to encrypt digital images, which takes into account the value of the pixel as well as its position in the image. The homographic function is a function represented as a quotient of two affine functions in a switching field K; it is represented by four different coefficients of zeros, which must verify certain conditions. One take the field K equal Z/257Z switching field (257 is a prime number), (all its non-zero elements are invertible, we take into account the following condition if we have 0 as the pixel value we put 256), 256 is the inverse of 256 in Z/257Z. We will show that this a homophonic substitution, which can represent the confusion part in a modern cryptographic system. We strengthened this proposal by a diffusion method and, we have used the Hénon attractor of is 2-dimensional; each pixel will be permuted to another position of the encrypted image. We managed to build a cryptosystem who passed all the statistic tests, and that has a secret key of 253 bits in length.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303101",
    "keywords": [
      "Affine transformation",
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Combinatorics",
      "Computer science",
      "Cryptography",
      "Cryptosystem",
      "Discrete mathematics",
      "Economics",
      "Encryption",
      "Evolutionary biology",
      "Field (mathematics)",
      "Finance",
      "Function (biology)",
      "Image (mathematics)",
      "Mathematics",
      "Operating system",
      "Pixel",
      "Position (finance)",
      "Prime (order theory)",
      "Pure mathematics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "ALI-PACHA",
        "given_name": "Hana"
      },
      {
        "surname": "HADJ-SAID",
        "given_name": "Naima"
      },
      {
        "surname": "ALI-PACHA",
        "given_name": "Adda"
      }
    ]
  },
  {
    "title": "Outer-Points shaver: Robust graph-based clustering via node cutting",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107001",
    "abstract": "Graph-based clustering is an efficient method for identifying clusters in local and nonlinear data patterns. Among the existing methods, spectral clustering is one of the most prominent algorithms. However, this method is vulnerable to noise and outliers. This study proposes a robust graph-based clustering method that removes the data nodes of relatively low density. The proposed method calculates the pseudo-density from a similarity matrix, and reconstructs it using a sparse regularization model. In this process, noise and the outer points are determined and removed. Unlike previous edge cutting-based methods, the proposed method is robust to noise while detecting clusters because it cuts out irrelevant nodes. We use a simulation and real-world data to demonstrate the usefulness of the proposed method by comparing it to existing methods in terms of clustering accuracy and robustness to noisy data. The comparison results confirm that the proposed method outperforms the alternatives.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303048",
    "keywords": [
      "1-planar graph",
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "CURE data clustering algorithm",
      "Chemistry",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Data point",
      "Data stream clustering",
      "Dense graph",
      "Gene",
      "Graph",
      "Image (mathematics)",
      "Line graph",
      "Noise (video)",
      "Noisy data",
      "Outlier",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)",
      "Robustness (evolution)",
      "Spectral clustering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Younghoon"
      },
      {
        "surname": "Do",
        "given_name": "Hyungrok"
      },
      {
        "surname": "Kim",
        "given_name": "Seoung Bum"
      }
    ]
  },
  {
    "title": "Decomposed slice sampling for factorized distributions",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107021",
    "abstract": "Slice sampling provides an automatical adjustment to match the characteristics of the distribution. Although this method has made great success in many situations, it becomes limited when the distribution is complex. Inspired by Higdon [1], in this paper, we present a decomposed sampling framework based on slice sampling called decomposed slice sampling (DSS). We suppose that the target distribution can be divided into two multipliers so that information in each term can be used, respectively. The first multiplier is used in the first step of DSS to obtain horizontal slices and the last term is used in the second step. Simulations on four simple distributions indicate the effectiveness of our method. Compared with slice sampling and Hamiltonian Monte Carlo on Gaussian distributions in different dimensions and ten real-world datasets, the proposed method achieves better performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303243",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Computer vision",
      "Filter (signal processing)",
      "Gaussian",
      "Hybrid Monte Carlo",
      "Importance sampling",
      "Markov chain Monte Carlo",
      "Mathematics",
      "Monte Carlo method",
      "Physics",
      "Quantum mechanics",
      "Rejection sampling",
      "Sampling (signal processing)",
      "Slice sampling",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jiachun"
      },
      {
        "surname": "Sun",
        "given_name": "Shiliang"
      }
    ]
  },
  {
    "title": "Voronoi tree models for distribution-preserving sampling and generation",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107002",
    "abstract": "We propose a method based on recursive binary Voronoi trees to learn a nonparametric model of the distribution underlying a given dataset. The obtained model can be used as a general tool both to extract good samples from the original dataset (e.g., for batch selection, bagging, or sample size reduction) or to generate new synthetic ones, also in a conditional fashion (e.g., to deal with imbalanced sets or to reconstruct corrupted points). In order to ensure that the distribution of the new sets, either sampled or generated, follows closely that of the original dataset, we design all the procedures according to a specific measure of distance between distributions. The use of binary recursive Voronoi structures enables the proposed algorithms to be simple, efficient and able to adapt to the shape of the original dataset. Simulation tests showcase the good performance and flexibility of the approach in various learning contexts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132031930305X",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Filter (signal processing)",
      "Flexibility (engineering)",
      "Geometry",
      "Mathematical analysis",
      "Mathematics",
      "Measure (data warehouse)",
      "Nonparametric statistics",
      "Pattern recognition (psychology)",
      "Sampling (signal processing)",
      "Statistics",
      "Tree (set theory)",
      "Voronoi diagram"
    ],
    "authors": [
      {
        "surname": "Cervellera",
        "given_name": "Cristiano"
      },
      {
        "surname": "Macciò",
        "given_name": "Danilo"
      }
    ]
  },
  {
    "title": "Ethnicity classification by the 3D Discrete Landmarks Model measure in Kendall shape space",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.10.035",
    "abstract": "Knowledge of human ethnicity constitutes important biometric information. An automated ethnicity classification is a good first step in facial analysis. However, most ethnicity classification methods require a complex feature extraction and model training process. We propose a novel ethnicity classification method based on the analysis of facial landmarks in Kendall shape space. Facial features with different relative positions have a close relationship with ethnicity. Facial landmarks can represent positions of facial features. We build a Discrete Landmarks Model (DLM) based on facial landmarks and construct an ethnicity classification model based on the DLM analysis. The clear advantages of our method are that it is fully automated; requires no complex data preprocessing, feature extraction or a complex training process; results in a fast and accurate classification process. We estimate the effectiveness of our method experimentally, using public databases such as Texas3D, FRGC2.0, BU-3DFE and BU-4DFE. On average, our method can achieve a 95% ethnicity classification rate with each classification attempt in 2.0 s.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303137",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Face (sociological concept)",
      "Facial recognition system",
      "Feature (linguistics)",
      "Feature extraction",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Preprocessor",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Lv",
        "given_name": "Chenlei"
      },
      {
        "surname": "Wu",
        "given_name": "Zhongke"
      },
      {
        "surname": "Wang",
        "given_name": "Xingce"
      },
      {
        "surname": "Dan",
        "given_name": "Zhang"
      },
      {
        "surname": "Zhou",
        "given_name": "Mingquan"
      }
    ]
  },
  {
    "title": "Deeply learned pore-scale facial features with a large pore-to-pore correspondences dataset",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.10.021",
    "abstract": "Similar to fingerprints and irises, pore-scale facial features can be used to distinguish human identities effectively. However, without pore-to-pore correspondences dataset, there are no deep learning based methods for pore-scale facial features. Actually, it is hard to establish a large pore-to-pore correspondences dataset due to the existing high-resolution face databases are uncalibrated and nonsynchronous. In this paper, we employ a constraint based on 3D facial model and construct a large pore-to-pore correspondences dataset. This dataset is then used to train a Convolutional Neural Network (CNN) to generate the novel pore-scale facial features - Deeply Learned Pore-scale Facial Features (DLPFF). The experiments show that our learning based method achieves the state-of-the-art matching performance on the Bosphorus facial database and has good generalization.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303010",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Face (sociological concept)",
      "Feature (linguistics)",
      "Generalization",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Zeng",
        "given_name": "Xianxian"
      },
      {
        "surname": "Wang",
        "given_name": "Xiaodong"
      },
      {
        "surname": "Chen",
        "given_name": "Kairui"
      },
      {
        "surname": "Li",
        "given_name": "Dong"
      },
      {
        "surname": "Zhang",
        "given_name": "Yun"
      },
      {
        "surname": "Lam",
        "given_name": "Kin-Man"
      }
    ]
  },
  {
    "title": "Compressing the CNN architecture for in-air handwritten Chinese character recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.028",
    "abstract": "Since the convolutional neural network (CNN) has brought great breakthroughs in the field of computer vision, it recently has been introduced to the in-air handwritten Chinese character recognition (IAHCCR) to achieve better recognition performance. However, the CNN is typically over-parameterized and contains lots of redundant filters or parameters. This leads the CNN to suffer from huge computation cost and considerable storage usage, limiting its deployments to resource-constrained devices like mobile phones and intelligent TVs. In this paper, we propose a unified algorithm to effectively compress the CNN for IAHCCR with little accuracy loss. Specifically, we first utilize the channel pruning strategy to simplify the network structure, and then adopt the network quantization technique to represent parameters with lower precision. We conduct experiments on the in-air handwriting dataset IAHCC-UCAS2016, where the baseline CNN achieves the state-of-the-art accuracy of 95.33% with 15.5 MB of storage. After the compression, we achieve 12.4 × storage saving and 1.7 × theoretical acceleration with only 0.17% accuracy loss. Moreover, evaluations on other benchmark datasets including ICDAR-2013 and MNIST further demonstrate the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303502",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Biology",
      "Computation",
      "Computer science",
      "Convolutional neural network",
      "Geodesy",
      "Geography",
      "MNIST database",
      "Pattern recognition (psychology)",
      "Pruning",
      "Quantization (signal processing)"
    ],
    "authors": [
      {
        "surname": "Gan",
        "given_name": "Ji"
      },
      {
        "surname": "Wang",
        "given_name": "Weiqiang"
      },
      {
        "surname": "Lu",
        "given_name": "Ke"
      }
    ]
  },
  {
    "title": "Active deep neural network features selection for segmentation and recognition of brain tumors using MRI images",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.019",
    "abstract": "Glioma is a kind of brain tumor that can arise at a distinct location along with dissimilar appearance and size. The high-grade glioma (HGG) is a serious kind of cancer when compare to low-graded glioma (LGG). The manual diagnosis process of these tumors is tiring and time consuming. Therefore, in clinical practices, MRI is useful to assess gliomas as it provides essential information of tumor regions. In this manuscript, an active deep learning-based feature selection approach is suggested to segment and recognize brain tumors. Contrast enhancement is made in the primary step and supplied to SbDL for saliency map construction, which later converts into binarized form by applying simple thresholding. In the classification phase, the Inception V3 pre-trained CNN model is employed for deep feature extraction. These features are simply concatenated along with dominant rotated LBP (DRLBP) for better texture analysis. Later, the concatenated vector is optimized through particle swarm optimization (PSO), so as to classify using softmax classifier. The experiments are conducted in two phases. At first, the segmentation approach SbDL is validated on BRATS2017 and BRATS2018 datasets. The achieved dice score for the BRAST2017 dataset is 83.73% for core tumor, 93.7% for the whole tumor and 79.94% for enhanced tumor. For BRATS2018 dataset, dice score obtained is 88.34% (core), 91.2% (whole) and 81.84% (enhanced). At the second, the classification strategy is applied on BRATS2013, 2014, 2017 and 2018 with an average accuracy of more than 92%. The overall results show that the presented method outperforms for both segmentation and classification of brain tumors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303411",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Cancer research",
      "Classifier (UML)",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Dice",
      "Feature extraction",
      "Feature selection",
      "Geometry",
      "Glioma",
      "Image (mathematics)",
      "Mathematics",
      "Medicine",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Softmax function",
      "Thresholding"
    ],
    "authors": [
      {
        "surname": "Sharif",
        "given_name": "Muhammad Irfan"
      },
      {
        "surname": "Li",
        "given_name": "Jian Ping"
      },
      {
        "surname": "Khan",
        "given_name": "Muhammad Attique"
      },
      {
        "surname": "Saleem",
        "given_name": "Muhammad Asim"
      }
    ]
  },
  {
    "title": "Robust ECG biometrics using GNMF and sparse representation",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.005",
    "abstract": "As a vital sign, Electrocardiogram (ECG) has highly discriminative characteristics in the field of biometrics. This paper aims to propose a novel robust ECG biometric method based on graph regularized non-negative matrix factorization (GNMF) and sparse representation. First, after ECG signal pre-processing and heartbeat segmentation, GNMF is used to reduce the dimensions of each heartbeat. In GNMF, an affinity graph is constructed to encode the geometrical information and label information in order to obtain more discriminative features. Second, in order to seek highly discriminability of ECG, the sparse representation is utilized to perform final feature extraction. We evaluate the method on two public datasets: ECG-ID and MIT-BIH Arrhythmia (MITDB). When fusing three heartbeats as a test sample, the accuracy achieves 98.03% and 100% on the ECG-ID dataset and the MITDB dataset, respectively. Experimental results show that the proposed method is robust for within-session and across-session of the ECG signal.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303198",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Computer security",
      "Discriminative model",
      "Feature extraction",
      "Heartbeat",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Sparse approximation",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Rui"
      },
      {
        "surname": "Yang",
        "given_name": "Gongping"
      },
      {
        "surname": "Wang",
        "given_name": "Kuikui"
      },
      {
        "surname": "Huang",
        "given_name": "Yuwen"
      },
      {
        "surname": "Yuan",
        "given_name": "Feng"
      },
      {
        "surname": "Yin",
        "given_name": "Yilong"
      }
    ]
  },
  {
    "title": "Lungs cancer classification from CT images: An integrated design of contrast based classical features fusion and selection",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.014",
    "abstract": "Lung cancer is a fatal type of cancer and it causes of severe deaths of approximately 422 people every day, worldwide. However, an early diagnosis is an expedient requirement for increasing the chances of human survival. In this regard the existing techniques of tumor detection, CT scans are mostly utilized to recognize the infected regions, nevertheless, the major challenges of CT images are low visibility of tumor regions, negative rates, to name but a few. In this work, we propose a novel design of contrast stretching based classical features fusion process for localizing the of lungs cancer classification. The proposed method encompasses three significant steps: firstly, contrast of original CT images is improved by gamma correction max intensity weights approach. Secondly, multiple texture, point, and geometric features are extracted from contrast images, which are later subjected to a serial canonical correlation-based fusion. Thirdly, zero values and negative features are replaced by an entropy-based approach, followed by weighted NCA for selection. Most discriminate features are fed into ensemble classifier for final classification. The validation of the proposed method is conducted on publicly available dataset: Lungs Data Science Bowl 2017 to achieving maximum accuracy of 99.4%. The numerical results clearly show that the performance of our proposed method outperforms in comparison with several existing methods with greater accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303289",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Contrast (vision)",
      "Entropy (arrow of time)",
      "Feature selection",
      "Fusion",
      "Grey level",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Khan",
        "given_name": "M. Attique"
      },
      {
        "surname": "Rubab",
        "given_name": "S."
      },
      {
        "surname": "Kashif",
        "given_name": "Asifa"
      },
      {
        "surname": "Sharif",
        "given_name": "Muhammad Imran"
      },
      {
        "surname": "Muhammad",
        "given_name": "Nazeer"
      },
      {
        "surname": "Shah",
        "given_name": "Jamal Hussain"
      },
      {
        "surname": "Zhang",
        "given_name": "Yu-Dong"
      },
      {
        "surname": "Satapathy",
        "given_name": "Suresh Chandra"
      }
    ]
  },
  {
    "title": "Developed Newton-Raphson based deep features selection framework for skin lesion recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.034",
    "abstract": "Melanoma is the fatal form of skin cancer; however, its diagnosis at the primary stages significantly reduces the mortality rate. These days, the increasing numbers of skin cancer patients have boosted the requirement for a care decision support system - capable of detecting the lesions with high accuracy. In this work, a method is proposed for skin cancer localization and recognition by implementing a novel combination of a deep learning model and iteration-controlled Newton-Raphson (IcNR) based feature selection method. The proposed framework follows three primary steps - lesion localization through faster region based convolutional neural network (RCNN), deep feature extraction, and feature selection by IcNR approach. In the localization step, a new contrast stretching approach based on bee colony method (ABC) is being followed. The enhanced images along with their ground truths are later plugged into Fast-RCNN to get segmented images. A pre-trained model, DenseNet201, is utilized to extract deep features via transfer learning, which are later subjected to selection step using proposed IcNR approach. The selected most discriminant features are finally utilized for classification using multilayered feed forward neural networks. Tests are performed on ISBI2016 and ISBI2017 datasets to achieving an accuracy of 94.5% and 93.4%, respectively. Simulation results reveal that the proposed technique outperforms existing methods with greater accuracy, and time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786551930354X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Cancer",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Feature extraction",
      "Feature selection",
      "Internal medicine",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)",
      "Selection (genetic algorithm)",
      "Skin cancer",
      "Skin lesion",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Khan",
        "given_name": "Muhammad Attique"
      },
      {
        "surname": "Sharif",
        "given_name": "Muhammad"
      },
      {
        "surname": "Akram",
        "given_name": "Tallha"
      },
      {
        "surname": "Bukhari",
        "given_name": "Syed Ahmad Chan"
      },
      {
        "surname": "Nayak",
        "given_name": "Ramesh Sunder"
      }
    ]
  },
  {
    "title": "Brain tumor classification based on DWT fusion of MRI sequences using convolutional neural network",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.016",
    "abstract": "Tumor in brain is an anthology of anomalous cells. It leads to increase in death rate among humans. Therefore, in this manuscript, a fusion process is proposed to combine structural and texture information of four MRI sequences (T1C, T1, Flair and T2) for the detection of brain tumor. A discrete wavelet transform (DWT) along with Daubechies wavelet kernel is utilized for fusion process which provides a more informative tumor region as compared to an individual single sequence of MRI. After the fusion process, a partial differential diffusion filter (PDDF) is applied to remove noise. A global thresholding method is used for segmenting tumor region which is then fed to proposed convolutional neural network (CNN) model for finally differentiating tumor and non-tumor regions. Five publicly available datasets i.e., BRATS 2012, BRATS 2013, BRATS 2015, BRATS 2013 Leader board and BRATS 2018 are used for proposed method evaluation. The results show that fused images provide better results as compared to individual sequences on benchmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303307",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Convolutional neural network",
      "Discrete wavelet transform",
      "Fluid-attenuated inversion recovery",
      "Haar",
      "Image (mathematics)",
      "Kernel (algebra)",
      "Magnetic resonance imaging",
      "Mathematics",
      "Medicine",
      "Pattern recognition (psychology)",
      "Radiology",
      "Thresholding",
      "Wavelet",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Amin",
        "given_name": "Javaria"
      },
      {
        "surname": "Sharif",
        "given_name": "Muhammad"
      },
      {
        "surname": "Gul",
        "given_name": "Nadia"
      },
      {
        "surname": "Yasmin",
        "given_name": "Mussarat"
      },
      {
        "surname": "Shad",
        "given_name": "Shafqat Ali"
      }
    ]
  },
  {
    "title": "Auto-weighted multi-view clustering via deep matrix decomposition",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107015",
    "abstract": "Real data are often collected from multiple channels or comprised of different representations (i.e., views). Multi-view learning provides an elegant way to analyze the multi-view data for low-dimensional representation. In recent years, several multi-view learning methods have been designed and successfully applied in various tasks. However, existing multi-view learning methods usually work in a single layer formulation. Since the mapping between the obtained representation and the original data contains rather complex hierarchical information with implicit lower-level hidden attributes, it is desirable to fully explore the hidden structures hierarchically. In this paper, a novel deep multi-view clustering model is proposed by uncovering the hierarchical semantics of the input data in a layer-wise way. By utilizing a novel collaborative deep matrix decomposition framework, the hidden representations are learned with respect to different attributes. The proposed model is able to collaboratively learn the hierarchical semantics obtained by each layer. The instances from the same class are forced to be closer layer by layer in the low-dimensional space, which is beneficial for the subsequent clustering task. Furthermore, an idea weight is automatically assigned to each view without introducing extra hyperparameter as previous methods do. To solve the optimization problem of our model, an efficient iterative updating algorithm is proposed and its convergence is also guaranteed theoretically. Our empirical study on multi-view clustering task shows encouraging results of our model in comparison to the state-of-the-art algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303188",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Cluster analysis",
      "Computer science",
      "Convergence (economics)",
      "Data mining",
      "Economic growth",
      "Economics",
      "Hyperparameter",
      "Law",
      "Layer (electronics)",
      "Machine learning",
      "Management",
      "Organic chemistry",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Semantics (computer science)",
      "Task (project management)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Shudong"
      },
      {
        "surname": "Kang",
        "given_name": "Zhao"
      },
      {
        "surname": "Xu",
        "given_name": "Zenglin"
      }
    ]
  },
  {
    "title": "Merge-and-simplify operation for compact combinatorial pyramid definition",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.009",
    "abstract": "Image pyramids are employed for years in digital image processing. They permit to store and use different scales/levels of details of an image. To represent all the topological information of the different levels, combinatorial pyramids have proved having many interests. But, when using an explicit representation, one drawback of this structure is the memory space required to store such a pyramid. In this paper, this drawback is solved by defining a compact version of combinatorial pyramids. This definition is based on the definition of a new operation, called “merge-and-simplify”, which simultaneously merges regions and simplifies their boundaries. Our experiments show that the memory space of our solution is much smaller than the one of the original version. Moreover, the computation time of our solution is faster, because there are less levels in our pyramid than in the original one.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786551930323X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computation",
      "Computer science",
      "Digital image",
      "Geometry",
      "Image (mathematics)",
      "Image processing",
      "Law",
      "Mathematics",
      "Merge (version control)",
      "Parallel computing",
      "Political science",
      "Politics",
      "Pyramid (geometry)",
      "Representation (politics)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Damiand",
        "given_name": "Guillaume"
      },
      {
        "surname": "Zara",
        "given_name": "Florence"
      }
    ]
  },
  {
    "title": "Securing biometric user template using modified minutiae attributes",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.037",
    "abstract": "The minutiae points information of a fingerprint is generally saved directly in the database as a template for the user. It has been deduced through numerous research works that the original fingerprint of a user can be obtained from the minutiae points information. As the databases are prone to various attacks, their security becomes a huge concern in fingerprint based authentication systems. Hereby, a novel technique has been introduced which is based on the modification of the minutiae attributes. The user template generated through the proposed technique is extremely secure and robust. The proposed technique achieved 1.63%, 1%, and 2.43% EER under stolen-key attack scenario for FVC2002 DB1, FVC2002 DB2, and FVC2002 DB3 fingerprint databases respectively. The proposed technique achieved 0% EER under different-key scenario. Highly encouraging results are obtained that show the viability and effectiveness of the proposed technique.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303575",
    "keywords": [
      "Artificial intelligence",
      "Authentication (law)",
      "Biometrics",
      "Computer science",
      "Computer security",
      "Data mining",
      "Fingerprint (computing)",
      "Fingerprint Verification Competition",
      "Fingerprint recognition",
      "Key (lock)",
      "Minutiae",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Ali",
        "given_name": "Syed Sadaf"
      },
      {
        "surname": "Ganapathi",
        "given_name": "Iyyakutti Iyappan"
      },
      {
        "surname": "Prakash",
        "given_name": "Surya"
      },
      {
        "surname": "Consul",
        "given_name": "Pooja"
      },
      {
        "surname": "Mahyo",
        "given_name": "Sajid"
      }
    ]
  },
  {
    "title": "An integrated design of particle swarm optimization (PSO) with fusion of features for detection of brain tumor",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.017",
    "abstract": "Tumor in brain is a major cause of death in human beings. If not treated properly and timely, there is a high chance of it to become malignant. Therefore, brain tumor detection at an initial stage is a significant requirement. In this work, initially the skull is removed through brain surface extraction (BSE) method. The skull removed image is then fed to particle swarm optimization (PSO) to achieve better segmentation. In the next step, Local binary patterns (LBP) and deep features of segmented images are extracted and genetic algorithm (GA) is applied for best features selection. Finally, artificial neural network (ANN) and other classifiers are utilized to classify the tumor grades. The publicly available complex brain datasets such as RIDER and BRATS 2018 Challenge are utilized for evaluation of method and attained 99% maximum accuracy. The results are also compared with existing methods which evident that the presented technique provided improved outcomes which are clear proof of its effectiveness and novelty.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786551930337X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Brain tumor",
      "Computer science",
      "Histogram",
      "Image (mathematics)",
      "Local binary patterns",
      "Machine learning",
      "Medicine",
      "Novelty",
      "Particle swarm optimization",
      "Pathology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Segmentation",
      "Theology"
    ],
    "authors": [
      {
        "surname": "Sharif",
        "given_name": "Muhammad"
      },
      {
        "surname": "Amin",
        "given_name": "Javaria"
      },
      {
        "surname": "Raza",
        "given_name": "Mudassar"
      },
      {
        "surname": "Yasmin",
        "given_name": "Mussarat"
      },
      {
        "surname": "Satapathy",
        "given_name": "Suresh Chandra"
      }
    ]
  },
  {
    "title": "Time series classification using local distance-based features in multi-modal fusion networks",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107024",
    "abstract": "We propose the use of a novel feature, called local distance features, for time series classification. The local distance features are extracted using Dynamic Time Warping (DTW) and classified using Convolutional Neural Networks (CNN). DTW is classically as a robust distance measure for distance-based time series recognition methods. However, by using DTW strictly as a global distance measure, information about the matching is discarded. We show that this information can further be used as supplementary input information in temporal CNNs. This is done by using both the raw data and the features extracted from DTW in multi-modal fusion CNNs. Furthermore, we explore the effects of different prototype selection methods, prototype numbers, and data fusion schemes induce on the accuracy. We perform experiments on a wide range of time series datasets including three Unipen handwriting datasets, four UCI Machine Learning Repository datasets, and 85 UCR Time Series Classification Archive datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303279",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Chemistry",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Distance measures",
      "Dynamic time warping",
      "Feature (linguistics)",
      "Linguistics",
      "Machine learning",
      "Matching (statistics)",
      "Mathematics",
      "Measure (data warehouse)",
      "Modal",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Polymer chemistry",
      "Series (stratigraphy)",
      "Statistics",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Kenji Iwana",
        "given_name": "Brian"
      },
      {
        "surname": "Uchida",
        "given_name": "Seiichi"
      }
    ]
  },
  {
    "title": "A new approach for reduction of attributes based on stripped quotient sets",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.106999",
    "abstract": "Attribute reduction is a key problem in many areas such as data mining, pattern recognition, machine learning. The problems of finding all reducts as well as finding a minimal reduct in a given data table have been proved to be NP-hard. Therefore, to overcome this difficulty, many heuristic attribute reduction methods have been developed in recent years. In the process of heuristic attribute reduction, accelerating calculation of attribute significance is very important, especially for big data cases. In this paper, we firstly propose attribute significance measures based on stripped quotient sets. Then, by using these measures, we design efficient algorithms for calculating core and reduct, in which the time complexity will be considered in detail. Additionally, we will also give properties directly related to efficiently computing the attribute significance and significantly reducing the data size in the process of calculation. By theoretical and experimental views, we will show that our method can perform efficiently for large-scale data sets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303024",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Attribute domain",
      "Computer science",
      "Computer security",
      "Data mining",
      "Decision table",
      "Geometry",
      "Heuristic",
      "Key (lock)",
      "Mathematics",
      "Operating system",
      "Process (computing)",
      "Pure mathematics",
      "Quotient",
      "Reduct",
      "Reduction (mathematics)",
      "Rough set",
      "Table (database)"
    ],
    "authors": [
      {
        "surname": "Thuy",
        "given_name": "Nguyen Ngoc"
      },
      {
        "surname": "Wongthanavasu",
        "given_name": "Sartra"
      }
    ]
  },
  {
    "title": "Robust Visual Tracking based on Adversarial Unlabeled Instance Generation with Label Smoothing Loss Regularization",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107027",
    "abstract": "Recent studies have shown that deep neural networks have pushed visual tracking accuracy to new heights, but finding more robust long-term tracking is still challenging because of the dynamic foreground and background changes. This phenomenon affects the overall performance via online training sample generation. The dense sampling strategy has been widely used for its convenience, the appearance variation is severely limited by its highly spatial overlapping mechanism. The sample candidate evaluation with a classification score metric is not always reliable throughout the entire process, therefore, tracking failure is inevitable. As an effective solution, this paper proposes a novel sample-level generative adversarial network (GAN) to enrich the training data by generating massive amounts of sample-level GAN samples. These samples are not only similar to the real-life scenarios, but also could carry more diversity of deformation and motion blur to a certain degree. For occlusion invariance, a feature-level GAN is incorporated to generate more challenging feature-level GAN data by creating random occlusion masks in deep feature space. To facilitate the online learning process, a label smoothing loss regularization is introduced to achieve model regularization and over-fitting reduction by integrating the unlabeled GAN-generated training data with the realistically labeled ones. In addition, a re-detection correlation filter conservatively trained with reliable training data is employed to integrate a classification score metric to perform reliable model updates and avoid heavy degradation. Furthermore, we also carry out the re-detection correlation filter on the candidate region proposals to handle the tracking failures. The proposed tracker has shown superior performance in comparison to the other state-of-the-art tracking approaches on the OTB-2013, OTB-100, UAV123, UAV20L, and VOT2016 benchmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303309",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)",
      "Smoothing"
    ],
    "authors": [
      {
        "surname": "Han",
        "given_name": "Yamin"
      },
      {
        "surname": "Zhang",
        "given_name": "Peng"
      },
      {
        "surname": "Huang",
        "given_name": "Wei"
      },
      {
        "surname": "Zha",
        "given_name": "Yufei"
      },
      {
        "surname": "Cooper",
        "given_name": "Garth Douglas"
      },
      {
        "surname": "Zhang",
        "given_name": "Yanning"
      }
    ]
  },
  {
    "title": "MASAT: A fast and robust algorithm for pose-graph initialization",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.010",
    "abstract": "In this paper, we propose a novel algorithm to compute the initial structure of pose-graph based Simultaneous Localization and Mapping (SLAM) systems. We perform a Breadth-First Search (BFS) on the graph in order to obtain multiple votes regarding the location of a certain robot position from all of its previously processed neighbors. Next, we define the initial location of a pose as the average of the multiple alternatives. By adopting the proposed initialization approach, the number of iterations needed for optimization is significantly reduced while the computational complexity remains lightweight. We perform quantitative evaluation on various 2D and 3D benchmark datasets to demonstrate the advantages of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303241",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Graph",
      "Initialization",
      "Pattern recognition (psychology)",
      "Programming language",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Harsányi",
        "given_name": "Károly"
      },
      {
        "surname": "Kiss",
        "given_name": "Attila"
      },
      {
        "surname": "Szirányi",
        "given_name": "Tamás"
      },
      {
        "surname": "Majdik",
        "given_name": "András"
      }
    ]
  },
  {
    "title": "Population-guided large margin classifier for high-dimension low-sample-size problems",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107030",
    "abstract": "In this paper, we propose a novel linear binary classifier, denoted by population-guided large margin classifier (PGLMC), applicable to any sorts of data, including high-dimensional low-sample-size (HDLSS). PGLMC is conceived with a projecting direction w given by the comprehensive consideration of local structural information of the hyperplane and the statistics of the training samples. Our proposed model has several advantages compared to those widely used approaches. First, it isn't sensitive to the intercept term b. Second, it operates well with imbalanced data. Third, it is relatively simple to be implemented based on Quadratic Programming. Fourth, it is robust to the model specification for various real applications. The theoretical properties of PGLMC are proven. We conduct a series of evaluations on the simulated and five realworld benchmark data sets, including DNA classification, medical image analysis and face recognition. PGLMC outperforms the state-of-theart classification methods in most cases, or obtains comparable results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303334",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Binary number",
      "Classifier (UML)",
      "Computer science",
      "Demography",
      "Facial recognition system",
      "Geodesy",
      "Geography",
      "Geometry",
      "Hyperplane",
      "Machine learning",
      "Margin (machine learning)",
      "Margin classifier",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Population",
      "Quadratic classifier",
      "Quadratic equation",
      "Sample size determination",
      "Sociology",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Yin",
        "given_name": "Qingbo"
      },
      {
        "surname": "Adeli",
        "given_name": "Ehsan"
      },
      {
        "surname": "Shen",
        "given_name": "Liran"
      },
      {
        "surname": "Shen",
        "given_name": "Dinggang"
      }
    ]
  },
  {
    "title": "Online learning using projections onto shrinkage closed balls for adaptive brain-computer interface",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107017",
    "abstract": "Wearable/portable brain-computer interfaces (BCIs) for the long-term end use are a focus of recent BCI research. A challenge is how to update the BCI to meet changes in electroencephalography (EEG) signals, since the resource are so limited that retraining of traditional well-performed models, such as a support vector machine, is nearly impossible. To cope with this challenge, less-demanding adaptive online learning can be considered. We investigated an adaptive projected sub-gradient method (APSM) that is originated from the set theoretic estimation formulation and the projections onto convex sets theory. APSM provides a unifying framework for both adaptive classification and regression tasks. Coefficients of APSM are adjusted online as data arrive sequentially, with a regularization constraint made by projections onto a fixed closed ball. We extended the general APSM to a shrinkage form, where shrinkage closed balls were used instead of the original fixed one, expecting a more controllable fading effect and better adaptability. The convergence of shrinkage APSM was proved. It was also demonstrated that as shrinkage factor approached to 1, the limit point of shrinkage APSM would approach to the optimal solution with the least norm, which could be especially beneficial for generalization of the classifier. The performance of the proposed method was evaluated, and compared with those of the general APSM, the incremental support vector machine, and the passive aggressive algorithm, through an event-related potential-based BCI experiment. Results showed the advantage of the proposed method over the others on both the online classification performance and the easiness of tuning. Our study revealed the effectiveness of the proposed method for adaptive EEG classification, making it a promising tool for on-device training and updating of wearable/portable BCIs, as well as for application in other related fields, such as EEG-based biometrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303206",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Brain–computer interface",
      "Computer science",
      "Electroencephalography",
      "Machine learning",
      "Psychiatry",
      "Psychology",
      "Regularization (linguistics)",
      "Shrinkage",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Zheng"
      },
      {
        "surname": "Cheng",
        "given_name": "Jun"
      },
      {
        "surname": "Tao",
        "given_name": "Dapeng"
      }
    ]
  },
  {
    "title": "GF-Net: Improving machine reading comprehension with feature gates",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.10.030",
    "abstract": "Machine reading comprehension (MRC) is a field of question-answering in which computers understand given passages and answer related questions. Several previous models have tried to combine the use of linguistic and word embedding features to improve the performance of MRC; however, they could not obtain successful results because of feature interference problems caused by simple concatenation of the two. To resolve these problems, a machine reading comprehension model called gated feature network (GF-Net) is proposed in which linguistic features are selectively used according to their roles in the process of answer selection. In the GF-Net, the weights of the linguistic features are automatically controlled through gate mechanisms called feature gates. In the experiments with Stanford Question Answering Dataset SQuAD, the MRC models with feature gates showed a 0.67%p higher average of exact match (EM) and 0.64%p higher average of F1-score than models without feature gates. In addition, the GF-Net outperformed the previous MRC models to which feature gates were added. Based on these experimental results, it is concluded that the gate mechanism can contribute to an improvement in the performance of MRC models and the architecture of the GF-Net is suitable for the task of MRC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518306548",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Computer science",
      "Concatenation (mathematics)",
      "Feature (linguistics)",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Natural language processing",
      "Net (polyhedron)",
      "Operating system",
      "Philosophy",
      "Process (computing)"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Hyeon-gu"
      },
      {
        "surname": "Kim",
        "given_name": "Harksoo"
      }
    ]
  },
  {
    "title": "A commentary on “Learning error-correcting graph matching with a multiclass neural network”, Pattern Recognition Letters, 2018",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.10.033",
    "abstract": "We analyze the learning graph-matching algorithm presented in M. Martineau, R. Raveaux, D. Conte, G.Venturini, Learning error-correcting graph matching with a multiclass neural network, Pattern Recognition Letters (2018). Authors propose a new definition of the graph edit distance and also a learning algorithm that deduces some weights on this new graph edit distance. In this commentary, we first show that this new definition of the graph edit distance cannot be considered a distance and then, we discuss how this fact influences on the application of their learning methodology.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303113",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Edit distance",
      "Graph",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Serratosa",
        "given_name": "Francesc"
      }
    ]
  },
  {
    "title": "Visual saliency guided complex image retrieval",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.08.010",
    "abstract": "Compared with the traditional text data, multimedia data are concise and contains rich meanings, so people are more willing to use the multimedia data to store information. How to effectively retrieve information is essential. This paper proposes a novel visual saliency guided complex image retrieval model. Initially, Itti visual saliency model is presented. In this model, the overall saliency map is generated by the integration of direction, intensity and color saliency map, respectively. Then, to help describe the image pattern more clearly, we present the multi-feature fusion paradigm of images. To address the complexity of the images, we propose a two-stage definition: (1) Cognitive load based complexity; (2) Cognitive level of complexity classification. The group sparse logistic regression model is integrated to finalize the image retrieval system. The performance of the proposed system is tested on different databases compared with the other state-of-the-art models which overcome the baselines in complex scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518304045",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Image retrieval",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Saliency map",
      "Visual Word",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Haoxiang"
      },
      {
        "surname": "Li",
        "given_name": "Zhihui"
      },
      {
        "surname": "Li",
        "given_name": "Yang"
      },
      {
        "surname": "Gupta",
        "given_name": "B.B."
      },
      {
        "surname": "Choi",
        "given_name": "Chang"
      }
    ]
  },
  {
    "title": "A hierarchical autoencoder learning model for path prediction and abnormality detection",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.06.030",
    "abstract": "In this paper, we introduce an unsupervised hierarchical framework for modeling trajectories in surveillance scenarios. Inspired by the object recognition field, a novel feature representation optimized for a neural network learning architecture is proposed. Low levels of the hierarchy capture local spatio-temporal motion attributes such as spatial orientation and speed, while higher levels contribute to obtaining richer semantic information. The bottom-up construction of the hierarchical framework exploits the inherent statistical correlations between neighboring elements using an increasing spatio-temporal grid. Cross-entropy based optimization in combination with autoencoders is used to learn weights for subsequent hierarchical layers. Finally, the Bayesian probabilistic framework built on top of the hierarchical model is proposed for applications such as long-term path prediction and abnormality detection. We demonstrate the efficiency of the proposed model on both indoor and outdoor datasets, achieving results comparable with state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519301916",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Computer science",
      "Computer security",
      "Cross entropy",
      "Data mining",
      "Deep learning",
      "Economics",
      "Entropy (arrow of time)",
      "Exploit",
      "Feature (linguistics)",
      "Feature learning",
      "Geometry",
      "Grid",
      "Hierarchical database model",
      "Hierarchy",
      "Law",
      "Linguistics",
      "Machine learning",
      "Market economy",
      "Mathematics",
      "Pairwise comparison",
      "Path (computing)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Probabilistic logic",
      "Programming language",
      "Quantum mechanics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Dotti",
        "given_name": "Dario"
      },
      {
        "surname": "Popa",
        "given_name": "Mirela"
      },
      {
        "surname": "Asteriadis",
        "given_name": "Stylianos"
      }
    ]
  },
  {
    "title": "Multi-label chest X-ray image classification via category-wise residual attention learning",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.10.027",
    "abstract": "This paper considers the problem of multi-label thorax disease classification on chest X-ray images. Identifying one or more pathologies from a chest X-ray image is often hindered by the pathologies unrelated to the targets. In this paper, we address the above problem by proposing a category-wise residual attention learning (CRAL) framework. CRAL predicts the presence of multiple pathologies in a class-specific attentive view. It aims to suppress the obstacles of irrelevant classes by endowing small weights to the corresponding feature representation. Meanwhile, the relevant features would be strengthened by assigning larger weights. Specifically, the proposed framework consists of two modules: feature embedding module and attention learning module. The feature embedding module learns high-level features with a convolutional neural network (CNN) while the attention learning module focuses on exploring the assignment scheme of different categories. The attention module can be flexibly integrated into any feature embedding networks with end-to-end training. The comprehensive experiments are conducted on the Chest X-ray14 dataset. CRAL yields the average AUC score of 0.816 which is a new state of the art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518308559",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Embedding",
      "Feature (linguistics)",
      "Feature learning",
      "Image (mathematics)",
      "Law",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Residual"
    ],
    "authors": [
      {
        "surname": "Guan",
        "given_name": "Qingji"
      },
      {
        "surname": "Huang",
        "given_name": "Yaping"
      }
    ]
  },
  {
    "title": "Multi-task learning for object keypoints detection and classification",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.08.013",
    "abstract": "Object keypoints detection and classification are both central research topics in computer vision. Due to their wide range potential applications in the real world, substantial efforts have been taken to advance their performance. However, these two related tasks are mainly treated separately in previous works. We argue that keypoints detection and classification can be complementary tasks and beneficial to each other. Knowing the category of a object is able to reduce the searching space of keypoints detection models and facilitate more precise localization. On the other hand, having the knowledge of object keypoints can make classification models pay more attention on areas that are more associated with the object, which will inevitably promote classification accuracy. Embracing this observation, we propose to model keypoints detection and classification in a multi-task learning framework. Specifically, a multi-task deep network is designed and trained to conduct both tasks, where we devise the model structure delicately to carry out sufficient training of both tasks. Extensive experiments are set up on the AIFASHION DATASET and Human3.6M DATASET to validate our proposal, we show that our algorithm outperforms separate models trained individually on each task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518304288",
    "keywords": [
      "Artificial intelligence",
      "Carry (investment)",
      "Cognitive neuroscience of visual object recognition",
      "Composite material",
      "Computer science",
      "Economics",
      "Finance",
      "Machine learning",
      "Management",
      "Materials science",
      "Object (grammar)",
      "Object detection",
      "Operating system",
      "Pattern recognition (psychology)",
      "Programming language",
      "Range (aeronautics)",
      "Set (abstract data type)",
      "Space (punctuation)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Jie"
      },
      {
        "surname": "Zhao",
        "given_name": "Lin"
      },
      {
        "surname": "Zhang",
        "given_name": "Shanshan"
      },
      {
        "surname": "Gong",
        "given_name": "Chen"
      },
      {
        "surname": "Yang",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "A survey on 3D mask presentation attack detection and countermeasures",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107032",
    "abstract": "Despite the impressive progress in face recognition, current systems are vulnerable to presentation attacks, which subvert the face recognition systems by presenting a face artifact. Several techniques have been developed to automatically detect different presentation attacks, mostly for 2D photo print and video replay attacks. However, with the development of 3D modeling and printing technologies, 3D mask has become a more effective way to attack the face recognition systems. Over the last decade, various detection methods for 3D mask attacks have been proposed, but there is no survey yet to summarize the advances. We present a comprehensive overview of the state-of-the-art approaches in 3D mask spoofing and anti-spoofing, including existing databases and countermeasures. In addition, we quantitatively compare the performance of different mask spoofing detection methods on a common ground (i.e., using the same database and evaluation metric). The effectiveness of several 2D presentation attack detection methods is also evaluated on two 3D mask spoofing databases to show whether they are applicable or not for 3D mask attacks. Finally, we present some insights and summarize open issues to address in the future.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303358",
    "keywords": [
      "Artifact (error)",
      "Artificial intelligence",
      "Authentication (law)",
      "Computer science",
      "Computer security",
      "Face (sociological concept)",
      "Facial recognition system",
      "Medicine",
      "Pattern recognition (psychology)",
      "Presentation (obstetrics)",
      "Radiology",
      "Replay attack",
      "Social science",
      "Sociology",
      "Spoofing attack"
    ],
    "authors": [
      {
        "surname": "Jia",
        "given_name": "Shan"
      },
      {
        "surname": "Guo",
        "given_name": "Guodong"
      },
      {
        "surname": "Xu",
        "given_name": "Zhengquan"
      }
    ]
  },
  {
    "title": "Learning visual relationship and context-aware attention for image captioning",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107075",
    "abstract": "Image captioning which automatically generates natural language descriptions for images has attracted lots of research attentions and there have been substantial progresses with attention based captioning methods. However, most attention-based image captioning methods focus on extracting visual information in regions of interest for sentence generation and usually ignore the relational reasoning among those regions of interest in an image. Moreover, these methods do not take into account previously attended regions which can be used to guide the subsequent attention selection. In this paper, we propose a novel method to implicitly model the relationship among regions of interest in an image with a graph neural network, as well as a novel context-aware attention mechanism to guide attention selection by fully memorizing previously attended visual content. Compared with the existing attention-based image captioning methods, ours can not only learn relation-aware visual representations for image captioning, but also consider historical context information on previous attention. We perform extensive experiments on two public benchmark datasets: MS COCO and Flickr30K, and the experimental results indicate that our proposed method is able to outperform various state-of-the-art methods in terms of the widely used evaluation metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303760",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Closed captioning",
      "Cognitive psychology",
      "Computer science",
      "Context (archaeology)",
      "Focus (optics)",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Machine learning",
      "Memorization",
      "Natural language processing",
      "Optics",
      "Paleontology",
      "Physics",
      "Psychology",
      "Selection (genetic algorithm)",
      "Sentence",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Junbo"
      },
      {
        "surname": "Wang",
        "given_name": "Wei"
      },
      {
        "surname": "Wang",
        "given_name": "Liang"
      },
      {
        "surname": "Wang",
        "given_name": "Zhiyong"
      },
      {
        "surname": "Feng",
        "given_name": "David Dagan"
      },
      {
        "surname": "Tan",
        "given_name": "Tieniu"
      }
    ]
  },
  {
    "title": "Learning binary code for fast nearest subspace search",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107040",
    "abstract": "Subspace is widely used to represent objects under different viewpoints, illuminations, identities, and more. Due to the growing amount and dimensionality of visual contents, fast search in a large-scale database with high-dimensional subspaces is an important task in many applications, such as image retrieval, clustering, video retrieval, and visual recognition. This can be facilitated by approximate nearest subspace (ANS) search which requires effective subspace representation. All existing methods for this problem represent a subspace by a point in the Euclidean or the Grassmannian space before applying the approximate nearest neighbor (ANN) search. However, the efficiency of these methods is not guaranteed because the subspace representation step can be very time consuming when coping with high-dimensional data. Moreover, the subspace to point transforming process may cause subspace structural information loss which influences the search accuracy. In this paper, we present a new approach for hashing-based ANS search which can directly binarize a subspace without transforming it into a vector. The proposed method learns the binary codes for subspaces following a similarity preserving criterion, and simultaneously leverages the learned binary codes to train matrix classifiers as hash functions. Experiments on face and action recognition and video retrieval applications show that our method outperforms several state-of-the-art methods in both efficiency and accuracy. Moreover, we also compare our method with vector-based hashing methods. Results also show the superiority of our subspace matrix based search scheme.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303425",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary code",
      "Binary number",
      "Computer science",
      "Computer security",
      "Geometry",
      "Hash function",
      "Hash table",
      "Linear subspace",
      "Locality-sensitive hashing",
      "Mathematics",
      "Nearest neighbor search",
      "Pattern recognition (psychology)",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Lei"
      },
      {
        "surname": "Bai",
        "given_name": "Xiao"
      },
      {
        "surname": "Liu",
        "given_name": "Xianglong"
      },
      {
        "surname": "Zhou",
        "given_name": "Jun"
      },
      {
        "surname": "Hancock",
        "given_name": "Edwin R."
      }
    ]
  },
  {
    "title": "Weakly-paired deep dictionary learning for cross-modal retrieval",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.06.021",
    "abstract": "Many multi-modal data suffers from significant weak-pairing characteristics, i.e., there is no sample-to-sample correspondence between modalities, rather classes of samples in one modality correspond to classes of samples in the other modality. This provides great challenges for the cross-modal learning for retrieval. In this work, our focus is learning cross-modal representations with minimal class label supervision and without correspondences between samples. To tackle this challenging problem, we establish a scalable hierarchical learning architecture to deal with the extensive weakly-paired heterogeneous multi-modal data. A shared classifier across different modalities is used to effectively deal with label supervision information, and a multi-modal low-rank model is introduced to encourage the modal-invariant representation. Finally, some cross-modal validations on publicly available datasets are performed to show the advantages of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786551830268X",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Classifier (UML)",
      "Combinatorics",
      "Computer science",
      "Deep learning",
      "Machine learning",
      "Mathematics",
      "Modal",
      "Modalities",
      "Modality (human–computer interaction)",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Polymer chemistry",
      "Rank (graph theory)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Huaping"
      },
      {
        "surname": "Wang",
        "given_name": "Feng"
      },
      {
        "surname": "Zhang",
        "given_name": "Xinyu"
      },
      {
        "surname": "Sun",
        "given_name": "Fuchun"
      }
    ]
  },
  {
    "title": "A general non-parametric active learning framework for classification on multiple manifolds",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.01.013",
    "abstract": "Active learning is an important paradigm for investigating learners’ behavior and reducing costs on labeling. We propose a novel non-parametric active learning framework which utilizes label propagation to sense the potential data clusters/manifolds in the feature space and minimizes global uncertainty to investigate the unexplored clusters/manifolds for querying examples. Based on this framework, it is convenient to design new active learning algorithms for targeted problems. Furthermore, we analyze the sample selection mechanism of our proposed method and provide a formal proof. While selecting informative examples, our method has the following characteristics: (1) in each iteration, examples are primarily chosen from the cluster which contains unlabeled samples; (2) if there is more than one cluster with unlabeled samples, it will choose from the one containing the most samples; (3) the example which has the closest connection with the others will be preferentially selected for the same cluster. The designed algorithms achieve empirical success in multi-class classification and dramatically reduce the label costs on several real world datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519300108",
    "keywords": [
      "Active learning (machine learning)",
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Class (philosophy)",
      "Cluster (spacecraft)",
      "Computer science",
      "Data mining",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Parametric statistics",
      "Programming language",
      "Sample (material)",
      "Selection (genetic algorithm)",
      "Space (punctuation)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Lei"
      },
      {
        "surname": "Ma",
        "given_name": "Yuqing"
      },
      {
        "surname": "Liu",
        "given_name": "Xianglong"
      }
    ]
  },
  {
    "title": "Cluster-wise learning network for multi-person pose estimation",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107074",
    "abstract": "In this paper, we propose a cluster-wise feature aggregation network that exploits multi-level contextual association for multi-person pose estimation. The recent popular approach for pose estimation is extracting the local maximum response from each detection heatmap that trained for a specific keypoint type. To exploit more contextual information, our network simultaneously learns complementary semantic information to encourage the detected keypoints subject to a certain contextual constraint. Specifically, our network uses dense and sparse branches to generate paired multi-peak detection heatmaps for clusters of keypoints. To enhance the feature passing through the network, we aggregate information from different branches. The in-branch aggregation enriches the detection features in each branch by absorbing the holistic human region attention. The cross-branch aggregation further strengthens the detection features by fusing global and local context information between dense and sparse branches. We demonstrate competitive performance of our network on the benchmark dataset for multi-person pose estimation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303759",
    "keywords": [
      "Aggregate (composite)",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Composite material",
      "Computer science",
      "Computer security",
      "Constraint (computer-aided design)",
      "Context (archaeology)",
      "Exploit",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Geometry",
      "Linguistics",
      "Machine learning",
      "Materials science",
      "Mathematics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pose"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Ying"
      },
      {
        "surname": "Luo",
        "given_name": "Zhiwei"
      },
      {
        "surname": "Quan",
        "given_name": "Changqin"
      },
      {
        "surname": "Liu",
        "given_name": "Dianchao"
      },
      {
        "surname": "Wang",
        "given_name": "Gang"
      }
    ]
  },
  {
    "title": "Modality-correlation-aware sparse representation for RGB-infrared object tracking",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.10.002",
    "abstract": "To intelligently analyze and understand video content, a key step is to accurately perceive the motion of the interested objects in videos. To this end, the task of object tracking, which aims to determine the position and status of the interested object in consecutive video frames, is very important, and has received great research interest in the last decade. Although numerous algorithms have been proposed for object tracking in RGB videos, most of them may fail to track the object when the information from the RGB video is not reliable (e.g. in dim environment or large illumination change). To address this issue, with the popularity of dual-camera systems for capturing RGB and infrared videos, this paper presents a feature representation and fusion model to combine the feature representation of the object in RGB and infrared modalities for object tracking. Specifically, this proposed model is able to (1) perform feature representation of objects in different modalities by employing the robustness of sparse representation, and (2) combine the representation by exploiting the modality correlation. Extensive experiments demonstrate the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518307633",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Gene",
      "Law",
      "Linguistics",
      "Modality (human–computer interaction)",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "RGB color model",
      "Representation (politics)",
      "Robustness (evolution)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Lan",
        "given_name": "Xiangyuan"
      },
      {
        "surname": "Ye",
        "given_name": "Mang"
      },
      {
        "surname": "Zhang",
        "given_name": "Shengping"
      },
      {
        "surname": "Zhou",
        "given_name": "Huiyu"
      },
      {
        "surname": "Yuen",
        "given_name": "Pong C."
      }
    ]
  },
  {
    "title": "Semi-supervised cross-modal common representation learning with vector-valued manifold regularization",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.01.002",
    "abstract": "While cross-media data, like text, image, audio, video and 3D model, has been the main form of big data, there is a current dearth of research on cross-media retrieval. In this paper, we focus on how to learn the common representation of heterogeneous data which is a key challenge for cross-media retrieval. Most existing approaches linearly project original low-level feature into a joint feature space for isomorphic data representation. However, linear projection cannot capture most complex cross-modal correlation with high nonlinearity. In this paper, we propose a novel feature learning algorithm, which is semi-supervised cross-modal vector-valued manifold regularization (SCVM), to explore common representation of heterogeneous data. SCVM jointly explores low-level feature correlation and semantic information in a unified framework. Based on manifold regularization, we learn cross-media features from vector-valued reproducing kernel Hilbert spaces (RKHS) by kernel transformation on both labeled and unlabeled samples; moreover, we impose smoothness constraints of possible solutions to improve retrieval accuracy. Comparing with the current state-of-the-art approaches on two public datasets, comprehensive experimental results show superior performance of our SCVM. The method is more robust and stable when extended from two media types to five media types, which is very attractive in practical application.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519300029",
    "keywords": [
      "Applied mathematics",
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Dimensionality reduction",
      "Engineering",
      "Law",
      "Manifold (fluid mechanics)",
      "Manifold alignment",
      "Mathematics",
      "Mechanical engineering",
      "Modal",
      "Nonlinear dimensionality reduction",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Polymer chemistry",
      "Regularization (linguistics)",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Hong"
      },
      {
        "surname": "Wang",
        "given_name": "Ting"
      },
      {
        "surname": "Dai",
        "given_name": "Gang"
      }
    ]
  },
  {
    "title": "Weighted tensor nuclear norm minimization for tensor completion using tensor-SVD",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.12.012",
    "abstract": "In this paper, we consider the tensor completion problem, which aims to estimate missing values from limited information. Our model is based on the recently proposed tensor-SVD, which uses the relationships among the color channels in an image or video recovery problem. To improve the availability of the model, we propose the weighted tensor nuclear norm whose weights are fixed in the algorithm, study its properties and prove the Karush-Kuhn-Tucker (KKT) conditions of the proposed algorithm. We conduct extensive experiments to verify the recovery capability of the proposed algorithm. The experimental results demonstrate improvements in computation time and recovery effect compared with related methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518309231",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Computation",
      "Computer science",
      "Eigenvalues and eigenvectors",
      "Karush–Kuhn–Tucker conditions",
      "Law",
      "Mathematical optimization",
      "Mathematics",
      "Matrix norm",
      "Minification",
      "Norm (philosophy)",
      "Physics",
      "Political science",
      "Pure mathematics",
      "Quantum mechanics",
      "Singular value decomposition",
      "Tensor (intrinsic definition)"
    ],
    "authors": [
      {
        "surname": "Mu",
        "given_name": "Yang"
      },
      {
        "surname": "Wang",
        "given_name": "Ping"
      },
      {
        "surname": "Lu",
        "given_name": "Liangfu"
      },
      {
        "surname": "Zhang",
        "given_name": "Xuyun"
      },
      {
        "surname": "Qi",
        "given_name": "Lianyong"
      }
    ]
  },
  {
    "title": "Land contained sea area ship detection using spaceborne image",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.01.015",
    "abstract": "Using spaceborne remote sensing images for ship detection is a significant application for maritime monitoring, especially the synthetic aperture radar (SAR) image which can provide data all days and all weather. Limited attention to the problem of detection in land contained sea area, which is the more difficult than detection on pure sea surface. In this letter, we propose a ship detection for the land contained sea area. In order to decrease the possibly existing false alarms form the island, an island filter is used as the first step. An automatically segmentation method called constant false alarm rate (CFAR) is used to get candidates from the big map then. In the third step, we use a CNN based classifier to separate false alarms from ship object. In order to fit for the SAR slice, a pooling utilization type called “max-mean pooling” is proposed. Finally, some experiments are given to demonstrate the effectiveness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519300212",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Constant false alarm rate",
      "False alarm",
      "Geology",
      "Image (mathematics)",
      "Image processing",
      "Mathematical morphology",
      "Object detection",
      "Pattern recognition (psychology)",
      "Pooling",
      "Remote sensing",
      "Segmentation",
      "Synthetic aperture radar"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Ziwei"
      },
      {
        "surname": "Yang",
        "given_name": "Ting"
      },
      {
        "surname": "Zhang",
        "given_name": "Hong"
      }
    ]
  },
  {
    "title": "Improving retinal vessel segmentation with joint local loss by matting",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107068",
    "abstract": "Besides the binary segmentation, many retinal image segmentation methods also produce a score map, where a nonnegative score is assigned for each pixel to indicate the likelihood of being a vessel. This observation inspires us to propose a new approach as a post-processing step to improve existing methods by formulating segmentation as a matting problem. A trimap is obtained via a bi-level thresholding of the score map from existing methods, which is instrumental in focusing the attention to pixels of these unknown areas. A dedicated end-to-end matting algorithm is further developed to retrieve those vessel pixels in the unknown areas, and to produce the final vessel segmentation by minimizing global pixel loss and local matting loss. Our approach is shown to be particularly effective in rescuing thin and tiny vessels that may lead to disconnections of vessel fragments. Moreover, it is observed that our approach is capable of improving the overall segmentation performance across a broad range of existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303693",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Image segmentation",
      "Pattern recognition (psychology)",
      "Pixel",
      "Scale-space segmentation",
      "Segmentation",
      "Thresholding"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "He"
      },
      {
        "surname": "Li",
        "given_name": "Huiqi"
      },
      {
        "surname": "Cheng",
        "given_name": "Li"
      }
    ]
  },
  {
    "title": "Learning image compressed sensing with sub-pixel convolutional generative adversarial network",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107051",
    "abstract": "Compressed sensing (CS) is a new technology to reconstruct image from randomized measurements, but the reconstruction procedure involves a time-consuming iterative optimization. In addition, the reconstruction quality becomes poor in low sampling rate. In order to alleviate these issues of the conventional CS image reconstruction, we propose a novel sub-pixel convolutional generative adversarial network (GAN) to learn compressed sensing reconstruction of images. The generator constructs the sub-pixel convolutional network to learn the explicit mapping from the low-dimensional measurement vector to the high-dimensional reconstruction, in which a compound loss, including reconstruction loss, measurement loss and adversarial loss, is designed to guide the network learning. By means of the adversarial training with discriminator, the generator can learn the inherent image distribution and improve the reconstruction quality. Moreover, the test image can be fast reconstructed by simply passing the low-dimensional measurement vector through the generator network. The proposed algorithm is tested on MNIST, F-MNIST and CelebA datasets, and the experimental results show that it is superior to some state-of-the-art deep learning based and iterative optimization based algorithms, in terms of both time complexity and reconstruction quality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132031930353X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Compressed sensing",
      "Computer science",
      "Deep learning",
      "Detector",
      "Discriminator",
      "Generator (circuit theory)",
      "Image (mathematics)",
      "Iterative reconstruction",
      "MNIST database",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel",
      "Power (physics)",
      "Quantum mechanics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Yubao"
      },
      {
        "surname": "Chen",
        "given_name": "Jiwei"
      },
      {
        "surname": "Liu",
        "given_name": "Qingshan"
      },
      {
        "surname": "Liu",
        "given_name": "Guangcan"
      }
    ]
  },
  {
    "title": "Tensor-based sparse representations of multi-phase medical images for classification of focal liver lesions",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.01.001",
    "abstract": "Medical images play an important role in clinics. In most clinic sites, the diagnosis of diseases and the comprehending of disease progression need firstly accurate interpretation of the available medical images, which would be time-consuming in manual interpretation of the accumulated large amount of medical images. Thus automatical analysis and understanding of the available medical images become an active research topic, and therein, feature extraction of medical images plays an important role for achieving diagnosis performance. Natural images hold two-dimensional structures and linear statistical methods, such as k-means, GMM, and sparse coding, are widely applied for extracting compact and inherent representations. In contrast, medical images themselves have three-dimensional structures and possibly consist of multi-phase extension. Directly applying linear methods on the available medical data would lead to high dimensional vectors by reshaping the multi-dimensional data and would destroy the correlated relation among different dimensions of the raw medical data domain. Therefore, this study proposes a multilinear extension of the linear sparse coding for extracting compact and effective intermediate representations of the multi-dimensional local structures in multi-phase CT images, and aggregating the intermediate representations in the Bag-of-Visual-Words (BoVW) manner for classification of focal liver lesions (FLLs). In the proposed approach, three-layer volumes from the corresponding slices of multi-phase CT images are formed and spatiotemporal local structures from the volumes are extracted as 3rd-order tensors. Regarding the high dimensional local structures as tensors, we propose a K-CP (CANDECOMP/PARAFAC) algorithm to learn a tensor dictionary in an iterative way and extract the sparse representation with a multilinear OMP method. The aggregation of the sparse representation is implemented in the BoVW manner, which has been proved to be an effective method for extracting features from natural images and medical images. The proposed strategy is evaluated in classification of focal liver lesions and achieved better results than conventional BoVW with linear statistical methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519300017",
    "keywords": [
      "Algebra over a field",
      "Artificial intelligence",
      "Coding (social sciences)",
      "Computer science",
      "Division algebra",
      "Filtered algebra",
      "Image (mathematics)",
      "Interpretation (philosophy)",
      "Mathematics",
      "Medical diagnosis",
      "Medicine",
      "Multilinear algebra",
      "Multilinear map",
      "Pattern recognition (psychology)",
      "Programming language",
      "Pure mathematics",
      "Radiology",
      "Statistics",
      "Structure tensor",
      "Tensor (intrinsic definition)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jian"
      },
      {
        "surname": "Li",
        "given_name": "Jing"
      },
      {
        "surname": "Han",
        "given_name": "Xian-Hua"
      },
      {
        "surname": "Lin",
        "given_name": "Lanfen"
      },
      {
        "surname": "Hu",
        "given_name": "Hongjie"
      },
      {
        "surname": "Xu",
        "given_name": "Yingying"
      },
      {
        "surname": "Chen",
        "given_name": "Qingqing"
      },
      {
        "surname": "Iwamoto",
        "given_name": "Yutaro"
      },
      {
        "surname": "Chen",
        "given_name": "Yen-Wei"
      }
    ]
  },
  {
    "title": "Spatio-temporal deformable 3D ConvNets with attention for action recognition",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107037",
    "abstract": "The irregularity of human actions poses great challenges in video action recognition. Recently, 3D ConvNet methods have shown promising performance at modelling the motion and appearance information. However, the fixed geometric structure of 3D convolution filters largely limits the learning capacity for video action recognition. To address this problem, this paper proposes a spatio-temporal deformable ConvNet module with an attention mechanism, which takes into consideration the mutual correlations in both temporal and spatial domains, to effectively capture the long-range and long-distance dependencies in the video actions. Our attention based deformable module, as a generic module for 3D ConvNets, can adaptively learn more accurate spatio-temporal offsets to model the action irregularity. The experiments on two popular datasets (UCF-101 and HMDB-51) demonstrate that our module significantly outperforms the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303383",
    "keywords": [
      "Action (physics)",
      "Action recognition",
      "Artificial intelligence",
      "Artificial neural network",
      "Class (philosophy)",
      "Composite material",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Materials science",
      "Motion (physics)",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Range (aeronautics)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Jun"
      },
      {
        "surname": "Liu",
        "given_name": "Xianglong"
      },
      {
        "surname": "Zhang",
        "given_name": "Mingyuan"
      },
      {
        "surname": "Wang",
        "given_name": "Deqing"
      }
    ]
  },
  {
    "title": "Robust one-dimensional calibration and localisation of a distributed camera sensor network",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107058",
    "abstract": "Calibration and localisation of a camera sensor network is an essential requirement for higher-level computer vision tasks, such as mapping or tracking. Additionally, distributed algorithms are being increasingly used to create scalable networks robust to node failure. We propose a distributed calibration and localisation algorithm based on multi-view one-dimensional calibration, alternating direction method of multipliers, and Gaussian belief propagation. Our algorithm builds upon an existing calibration algorithm by improving the numerical conditioning and non-linear refinement. We adapt this to a distributed network, bringing local estimates at each camera node to global consensus. Simulation and experimental results show that our algorithm performs with high accuracy compared to other calibration techniques, in centralised and distributed networks, and is well suited for practical applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303607",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Brooks–Iyengar algorithm",
      "Calibration",
      "Camera resectioning",
      "Computer science",
      "Computer vision",
      "Database",
      "Distributed algorithm",
      "Distributed computing",
      "Engineering",
      "Gaussian",
      "Key distribution in wireless sensor networks",
      "Mathematics",
      "Node (physics)",
      "Pedagogy",
      "Physics",
      "Psychology",
      "Quantum mechanics",
      "Real-time computing",
      "Scalability",
      "Statistics",
      "Structural engineering",
      "Telecommunications",
      "Tracking (education)",
      "Wireless",
      "Wireless network"
    ],
    "authors": [
      {
        "surname": "Halloran",
        "given_name": "Brendan"
      },
      {
        "surname": "Premaratne",
        "given_name": "Prashan"
      },
      {
        "surname": "Vial",
        "given_name": "Peter James"
      }
    ]
  },
  {
    "title": "Unsupervised object-level video summarization with online motion auto-encoder",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.07.030",
    "abstract": "Unsupervised video summarization plays an important role on digesting, browsing, and searching the ever-growing videos every day, and the underlying fine-grained semantic and motion information (i.e., objects of interest and their key motions) in online videos has been barely touched. In this paper, we investigate a pioneer research direction towards the fine-grained unsupervised object-level video summarization. It can be distinguished from existing pipelines in two aspects: extracting key motions of participated objects, and learning to summarize in an unsupervised and online manner. To achieve this goal, we propose a novel online motion Auto-Encoder (online motion-AE) framework that functions on the super-segmented object motion clips. Comprehensive experiments on a newly-collected surveillance dataset and public datasets have demonstrated the effectiveness of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518303404",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Automatic summarization",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Deep learning",
      "Encoder",
      "Information retrieval",
      "Key (lock)",
      "Motion (physics)",
      "Object (grammar)",
      "Operating system",
      "Programming language",
      "Semantics (computer science)",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yujia"
      },
      {
        "surname": "Liang",
        "given_name": "Xiaodan"
      },
      {
        "surname": "Zhang",
        "given_name": "Dingwen"
      },
      {
        "surname": "Tan",
        "given_name": "Min"
      },
      {
        "surname": "Xing",
        "given_name": "Eric P."
      }
    ]
  },
  {
    "title": "High-dimensional unsupervised classification via parsimonious contaminated mixtures",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107031",
    "abstract": "The contaminated Gaussian distribution represents a simple heavy-tailed elliptical generalization of the Gaussian distribution; unlike the often-considered t-distribution, it also allows for automatic detection of mild outlying or “bad” points in the same way that observations are typically assigned to the groups in the finite mixture model context. Starting from this distribution, we propose the contaminated factor analysis model as a method for dimensionality reduction and detection of bad points in higher dimensions. A mixture of contaminated Gaussian factor analyzers (MCGFA) model follows therefrom, and extends the recently proposed mixture of contaminated Gaussian distributions to high-dimensional data. We introduce a family of 32 parsimonious models formed by introducing constraints on the covariance and contamination structures of the general MCGFA model. We outline a variant of the expectation-maximization algorithm for parameter estimation. Various implementation issues are discussed, and the novel family of models is compared to well-established approaches on both simulated and real data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303346",
    "keywords": [],
    "authors": [
      {
        "surname": "Punzo",
        "given_name": "Antonio"
      },
      {
        "surname": "Blostein",
        "given_name": "Martin"
      },
      {
        "surname": "McNicholas",
        "given_name": "Paul D."
      }
    ]
  },
  {
    "title": "Fast semi-supervised learning with anchor graph for large hyperspectral images",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.08.008",
    "abstract": "As the labeled samples of hyperspectral image (HSI) are very scarce and labeling sample costs too much time and is expensive, semi-supervised learning (SSL) has an important application in hyperspectral image (HSI) classification. Among SSL approaches, graph-based SSL (GSSL) model has recently attracted much attention. However, most GSSL methods still can not deal with the large HSI as their high computational complexity. In this letter, we propose a novel approach, called fast semi-supervised learning with anchor graph (FSSLAG) to solve the large HSI classification problem. In the proposed FSSLAG algorithm, the anchor graph, which is parameter-free, naturally sparse and scale invariant, is first constructed. Then the label of samples can be inferred through the graph. The computational complexity of FSSLAG can be reduced to O(ndm), which is a significant improvement compared with traditional graph-based SSL methods that need O(n 3), where n, d and m are the number of samples, features and anchors, respectively. Several experiments have demonstrated the effectiveness and efficiency of FSSLAG in terms of computational speed and classification accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518304021",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computational complexity theory",
      "Computer science",
      "Graph",
      "Hyperspectral imaging",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Semi-supervised learning",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Fang"
      },
      {
        "surname": "Wang",
        "given_name": "Rong"
      },
      {
        "surname": "Jia",
        "given_name": "Weimin"
      }
    ]
  },
  {
    "title": "Discriminant component analysis via distance correlation maximization",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107052",
    "abstract": "In the following study, an innovative supervised dimensionality reduction technique is proposed. dCor-based Dimensionality Reduction or dDR technique is based on distance correlation; a powerful correlation measure which is applicable to arbitrary-dimensional random variables. By projecting the samples to a lower dimensional space, dDR maximizes the correlation between explanatory and response variables. The proposed dDR algorithm can be easily implemented and it is computationally efficient. Moreover, it has a closed-form and a simple solution which makes it significantly effective in many different applications. In order to apply the proposed technique on non-linear problems, the kernel version of the dDR is also derived. Extensive analyses and empirical experiments across various visualization, classification, and regression tasks indicate that our algorithm is the method of choice; as it offers statistically superior results in comparison with other state-of-the-art approaches in the literature.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303541",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Canonical correlation",
      "Combinatorics",
      "Computer science",
      "Correlation",
      "Curse of dimensionality",
      "Data mining",
      "Dimensionality reduction",
      "Distance correlation",
      "Geometry",
      "Kernel (algebra)",
      "Kernel method",
      "Linear discriminant analysis",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Measure (data warehouse)",
      "Pattern recognition (psychology)",
      "Principal component analysis",
      "Reduction (mathematics)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Abdi",
        "given_name": "Lida"
      },
      {
        "surname": "Ghodsi",
        "given_name": "Ali"
      }
    ]
  },
  {
    "title": "Guest editorial: Image/video understanding and analysis",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.07.003",
    "abstract": "The explosive increase of multimedia data (i.e., text, image, and video) on the Internet has brought the great challenge of how to effectively index, retrieve and organize these resources. Much research attention has been paid to understand and analyze the content in multimedia data. One can only think to the amount of image/video data downloaded every minute in social media or to the number of surveillance cameras installed in our cities nowadays. Both scenarios require the researchers to develop automatic or semi-automatic approaches which are able to mine discriminate information from a large quantity of raw data. This special issue aims to collect recent state-of-the-art achievement on image/video understanding and analysis, especially the work devoted to several new challenges in the field. This special issue mainly focuses on image/video understanding and analysis (IVUA). We solicited original contributions, of leading researchers and practitioners from academia as well as industry, which address a wide range of theoretical and application issues in image and video understanding and analysis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519301941",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data science",
      "Field (mathematics)",
      "Image (mathematics)",
      "Mathematics",
      "Multimedia",
      "Pure mathematics",
      "The Internet",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Chang",
        "given_name": "Xiaojun"
      },
      {
        "surname": "Liang",
        "given_name": "Xiaodan"
      },
      {
        "surname": "Yan",
        "given_name": "Yan"
      },
      {
        "surname": "Nie",
        "given_name": "Liqiang"
      }
    ]
  },
  {
    "title": "Fast spectral clustering learning with hierarchical bipartite graph for large-scale data",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.06.024",
    "abstract": "Spectral clustering (SC) is drawing more and more attention due to its effectiveness in unsupervised learning. However, all of these methods still have limitations. First, the method is not suitable for large-scale problems due to its high computational complexity. Second, the neighborhood weighted graph is constructed by the Gaussian kernel, meaning that more work is required to tune the heat-kernel parameter. In order to overcome these issues, we propose a novel spectral clustering based on hierarchical bipartite graph (SCHBG) approach by exploring multiple-layer anchors with a pyramid-style structure. First, the proposed algorithm constructs a hierarchical bipartite graph, and then performs spectral analysis on the graph. As a result, the computational complexity can be largely reduced. Furthermore, we adopt a parameter-free yet effective neighbor assignment strategy to construct the similarity matrix, which avoids the need to tune the heat-kernel parameter. Finally, the algorithm is able to deal with the out-of-sample problem for large-scale data and its computational complexity is significantly reduced. Experiments demonstrate the efficiency and effectiveness of the proposed SCHBG algorithm. Results show that the SCHBG approach can achieve good clustering accuracy (76%) on an 8-million datasets. Furthermore, owing to the use of the bipartite graph, the algorithm can reduce the time cost for out-of-sample situations with almost the same clustering accuracy as for large sizes of data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786551830271X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bipartite graph",
      "Cluster analysis",
      "Computational complexity theory",
      "Computer science",
      "Correlation clustering",
      "Graph",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Spectral clustering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Xiaojun"
      },
      {
        "surname": "Yu",
        "given_name": "Weizhong"
      },
      {
        "surname": "Wang",
        "given_name": "Rong"
      },
      {
        "surname": "Zhang",
        "given_name": "Guohao"
      },
      {
        "surname": "Nie",
        "given_name": "Feiping"
      }
    ]
  },
  {
    "title": "Multi-class joint subspace learning for cross-modal retrieval",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.08.012",
    "abstract": "Most existing supervised subspace learning methods use the label information for high-level semantic exploration and learn one couple of common mapping matrices for all classes in the retrieval task. However, there are different semantic distributions among different classes and thus we propose to learn different mapping matrices for different classes in this paper, which facilitates learning more discriminative subspace. In addition, semantic overlap usually exists among different classes, which is reflected through common samples in different classes. Therefore, the multi-class joint subspace learning algorithm (MJSL) is proposed to distinct the different classes and mine the potential shared information of semantic overlap as much as possible. Specifically, the MJSL method considers exploring high-level semantic, keeping pair-wised closeness and selecting optimal features to obtain the most discriminative subspace for each class. Meanwhile, the trace-norm based joint learning is used for exploring the potential shared information of semantic overlap among different classes. Since the optimal mapping matrices have been learned via an iterative joint optimization algorithm with fast convergence, a linear SVM classifier is trained to establish the mapping relationship between multi-modal data and their potential semantic classes. Thus, the most related mapping matrices can be identified for each query adaptively and the retrieval performance can be promoted. Extensive experiments on two popular public datasets demonstrate that our algorithm outperforms several state-of-the-art cross-modal retrieval algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518304264",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Class (philosophy)",
      "Computer science",
      "Engineering",
      "Joint (building)",
      "Mathematics",
      "Modal",
      "Pattern recognition (psychology)",
      "Polymer chemistry",
      "Structural engineering",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "En"
      },
      {
        "surname": "Li",
        "given_name": "Jing"
      },
      {
        "surname": "Wang",
        "given_name": "Li"
      },
      {
        "surname": "Zhang",
        "given_name": "Jia"
      },
      {
        "surname": "Wan",
        "given_name": "Wenbo"
      },
      {
        "surname": "Sun",
        "given_name": "Jiande"
      }
    ]
  },
  {
    "title": "Spatio-temporal fall event detection in complex scenes using attention guided LSTM",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.08.031",
    "abstract": "Fall events are one of the greatest risks for public safety, especially in some complex scenes with large number of people. Nevertheless, there are few researches on fall detection in complex scenes, and even no public datasets. A fall event dataset in crowded and complex scenes is constructed. Aiming at detecting fall events in complex scenes, we further propose an attention guided LSTM model. Our method provides the spatial and temporal locations of fall events, which are indispensable information for danger alarm in complex public scenes. Specifically, the effective YOLO v3 is employed to detect pedestrian in videos, and followed by a tracking module. CNN features are extracted for each tracked bounding boxes. Fall events are detected by the attention guided LSTM. Experimental results show that our method achieves good performance, outperforming the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786551830504X",
    "keywords": [
      "Artificial intelligence",
      "Bounding overwatch",
      "Computer science",
      "Computer vision",
      "Event (particle physics)",
      "False alarm",
      "Machine learning",
      "Object detection",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Feng",
        "given_name": "Qi"
      },
      {
        "surname": "Gao",
        "given_name": "Chenqiang"
      },
      {
        "surname": "Wang",
        "given_name": "Lan"
      },
      {
        "surname": "Zhao",
        "given_name": "Yue"
      },
      {
        "surname": "Song",
        "given_name": "Tiecheng"
      },
      {
        "surname": "Li",
        "given_name": "Qiang"
      }
    ]
  },
  {
    "title": "3D shape reconstruction from multifocus image fusion using a multidirectional modified Laplacian operator",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107065",
    "abstract": "Multifocus image fusion techniques primarily emphasize human vision and machine perception to evaluate an image, which often ignore depth information contained in the focus regions. In this paper, a novel 3D shape reconstruction algorithm based on nonsubsampled shearlet transform (NSST) microscopic multifocus image fusion method is proposed to mine 3D depth information from the fusion process. The shift-invariant property of NSST guarantees the spatial corresponding relationship between the image sequence and its high-frequency subbands. Since the high-frequency components of an image represent the focus level of the image, a new multidirectional modified Laplacian (MDML) as the focus measure maps the high-frequency subbands to images of various levels of depth. Next, the initial 3D reconstruction result is obtained by using an optimal level selection strategy based on the summation of the multiscale Laplace responses to exploit these depth maps. Finally, an iterative edge repair method is implemented to refine the reconstruction result. The experimental results show that the proposed method has better performance, especially when the source images have low-contrast regions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132031930367X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Focus (optics)",
      "Fusion",
      "Fusion rules",
      "Image (mathematics)",
      "Image fusion",
      "Invariant (physics)",
      "Iterative reconstruction",
      "Laplace operator",
      "Linguistics",
      "Mathematical analysis",
      "Mathematical physics",
      "Mathematics",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Process (computing)"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Tao"
      },
      {
        "surname": "Hu",
        "given_name": "Zhiguo"
      },
      {
        "surname": "Qian",
        "given_name": "Yuhua"
      },
      {
        "surname": "Qiao",
        "given_name": "Zhiwei"
      },
      {
        "surname": "Zhang",
        "given_name": "Linyuan"
      }
    ]
  },
  {
    "title": "General model for linear information extraction based on the shear transformation",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.10.010",
    "abstract": "Most images have lots of linear information, which plays an important role in the tasks of image processing and pattern recognition. However, due to the interference of complex background in the images, the multiple directional characteristics of the linear information, and the problems of directional limitations in traditional methods, all these lead to the incomplete and discontinuities existed in the extracted linear information. To solve these problems, this paper puts forward a general model for improving the performance of the linear information extraction methods (LIEM) by utilizing the shear transformation. In this model, the shear transformation can transform an object (a filter or an image) in multiple directions, which directly or indirectly increases the directional characteristics of the traditional LIEM, and improves their performances to extract directional linear information and weak linear information. This paper elaborates on the basic principles of the model and its two implementations for the specific tasks. Furthermore, a variety of experiments are made to verify the versatility and effectiveness of the proposed model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518308134",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Classification of discontinuities",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Filter (signal processing)",
      "Gene",
      "Image (mathematics)",
      "Image processing",
      "Linear filter",
      "Linear map",
      "Linear model",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Pengfei"
      },
      {
        "surname": "Guo",
        "given_name": "Jun"
      },
      {
        "surname": "Chen",
        "given_name": "Feng"
      },
      {
        "surname": "Xiao",
        "given_name": "Yun"
      },
      {
        "surname": "Xia",
        "given_name": "Qishou"
      },
      {
        "surname": "Liu",
        "given_name": "Baoying"
      }
    ]
  },
  {
    "title": "Polycrystalline silicon wafer defect segmentation based on deep convolutional neural networks",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.12.013",
    "abstract": "Defect segmentation is an important way for defect detection in machine vision. For polycrystalline silicon wafer production, it is difficult to automatically segment defects due to its inhomogeneous background and unpredictable defect shapes. The conventional methods based on handcrafted models or features not only heavily rely on the expertise, but also are not flexible from one application case to another. In this paper, we propose a defect segmentation method for polycrystalline silicon wafer based on the deep convolutional networks. Firstly, we apply Region Proposal Network (RPN) to generate underlying defect regions. Then, these generated image patches are processed to suitable sizes for feeding into a improved segmentation network which is modified based on U-net and a dilation convolution. Real defect images are used to test the proposed method and experimental results show that proposed method achieves better performance compared with the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518309243",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Deep learning",
      "Dilation (metric space)",
      "Image segmentation",
      "Layer (electronics)",
      "Materials science",
      "Mathematics",
      "Nanotechnology",
      "Optoelectronics",
      "Pattern recognition (psychology)",
      "Polycrystalline silicon",
      "Segmentation",
      "Silicon",
      "Thin-film transistor",
      "Wafer"
    ],
    "authors": [
      {
        "surname": "Han",
        "given_name": "Hui"
      },
      {
        "surname": "Gao",
        "given_name": "Chenqiang"
      },
      {
        "surname": "Zhao",
        "given_name": "Yue"
      },
      {
        "surname": "Liao",
        "given_name": "Shisha"
      },
      {
        "surname": "Tang",
        "given_name": "Lin"
      },
      {
        "surname": "Li",
        "given_name": "Xindou"
      }
    ]
  },
  {
    "title": "Joint deep semantic embedding and metric learning for person re-identification",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.08.011",
    "abstract": "We focus on the person re-identification (re-id) task, whose goal is to automatically re-identify individual persons from multiple non-overlapping cameras or the same camera across time. While most existing works rely on exploring properties of the visual data, we consider taking advantage of both visual and textual representations. Given images and natural language descriptions of the persons in the probe, the re-id system is required to rank all the samples in the gallery set. We embed the visual representations and textual descriptions in a unified space, in which we map similar examples close to each other and map dissimilar examples farther apart. Our premise is that, in general, strong semantic correlations exist between different persons. The space casts a person in gallery set as a combination of the persons in probe set. The model is trained in an end-to-end fashion. We conduct extensive experiments on the challenging i-LIDS, PRID-2011, CUHK03 and Market-1501 datasets, and confirm that the proposed model achieves state-of-the-art performances.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518304057",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Economics",
      "Embedding",
      "Focus (optics)",
      "Identification (biology)",
      "Linguistics",
      "Management",
      "Mathematics",
      "Metric (unit)",
      "Natural language processing",
      "Operating system",
      "Operations management",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Premise",
      "Programming language",
      "Rank (graph theory)",
      "Set (abstract data type)",
      "Space (punctuation)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Chang",
        "given_name": "Yan-Shuo"
      },
      {
        "surname": "Wang",
        "given_name": "Ming-Yu"
      },
      {
        "surname": "He",
        "given_name": "Lang"
      },
      {
        "surname": "Lu",
        "given_name": "Wei"
      },
      {
        "surname": "Su",
        "given_name": "Hui"
      },
      {
        "surname": "Gao",
        "given_name": "Ni"
      },
      {
        "surname": "Yang",
        "given_name": "Xin-An"
      }
    ]
  },
  {
    "title": "A generalized least-squares approach regularized with graph embedding for dimensionality reduction",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107023",
    "abstract": "In current graph embedding methods, low dimensional projections are obtained by preserving either global geometrical structure of data or local geometrical structure of data. In this paper, the PCA (Principal Component Analysis) idea of minimizing least-squares reconstruction errors is regularized with graph embedding, to unify various local manifold embedding methods within a generalized framework to keep global and local low dimensional subspace. Different from the well-known PCA method, our proposed generalized least-squares approach considers data distributions together with an instance penalty in each data point. In this way, PCA is viewed as a special instance of our proposed generalized least squares framework for preserving global projections. Applying a regulation of graph embedding, we can obtain projection that preserves both intrinsic geometrical structure and global structure of data. From the experimental results on a variety of face and handwritten digit recognition, our proposed method has advantage of superior performances in keeping lower dimensional subspaces and higher classification results than state-of-the-art graph embedding methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303267",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Data point",
      "Dimensionality reduction",
      "Embedding",
      "Geometry",
      "Graph",
      "Graph embedding",
      "Linear subspace",
      "Mathematics",
      "Nonlinear dimensionality reduction",
      "Pattern recognition (psychology)",
      "Principal component analysis",
      "Projection (relational algebra)",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Shen",
        "given_name": "Xiang-Jun"
      },
      {
        "surname": "Liu",
        "given_name": "Si-Xing"
      },
      {
        "surname": "Bao",
        "given_name": "Bing-Kun"
      },
      {
        "surname": "Pan",
        "given_name": "Chun-Hong"
      },
      {
        "surname": "Zha",
        "given_name": "Zheng-Jun"
      },
      {
        "surname": "Fan",
        "given_name": "Jianping"
      }
    ]
  },
  {
    "title": "A Hybrid convolutional neural network for sketch recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.01.006",
    "abstract": "With the popularity of touch-screen devices, it is becoming increasingly important to understand users’ free-hand sketches in computer vision and human-computer interaction. Most of existing sketch recognition methods employ the similar strategies used in image recognition, relying on appearance information represented by hand-crafted features or deep features from convolutional neural networks. We believe that sketch recognition can benefit from learning both appearance and shape representation. In this paper, we propose a novel architecture, named Hybrid CNN, which is composed of A-Net and S-Net. They describe appearance information and shape information, respectively. Hybrid CNN is then comprehensively evaluated in the sketch classification and retrieval tasks on different datasets, including TU-Berlin, Sketchy and Flickr15k. Experimental results demonstrate that the Hybrid CNN achieves competitive accuracy compared with the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519300078",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Gesture",
      "Gesture recognition",
      "Law",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Popularity",
      "Psychology",
      "Representation (politics)",
      "Sketch",
      "Sketch recognition",
      "Social psychology"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Xingyuan"
      },
      {
        "surname": "Huang",
        "given_name": "Yaping"
      },
      {
        "surname": "Zou",
        "given_name": "Qi"
      },
      {
        "surname": "Pei",
        "given_name": "Yanting"
      },
      {
        "surname": "Zhang",
        "given_name": "Runsheng"
      },
      {
        "surname": "Wang",
        "given_name": "Song"
      }
    ]
  },
  {
    "title": "Multi-stage adaptive regression for online activity recognition",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107053",
    "abstract": "Online activity recognition which aims to detect and recognize activity instantly from a continuous video stream is a key technology in human-robot interaction. However, the partial activity observation problem, mainly due to the incomplete sequence acquisition, makes it greatly challenging. This paper proposes a novel approach, named Multi-stage Adaptive Regression (MAR), for online activity recognition with the main focus on addressing the partial observation problem. Specifically, the MAR framework delicately assembles overlapped activity observations to improve its robustness against arbitrary activity segments. Then multiple score functions corresponding to each specific performance stage are collaboratively learned via a adaptive label strategy to enhance its power of discriminating similar partial activities. Moreover, the Online Human Interaction (OHI) database is constructed to evaluate the online activity recognition in human interaction scenarios. Extensive experimental evaluations on the Multi-Modal Action Detection (MAD) database and the OHI database show that the MAR method achieves an outstanding performance over the state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303553",
    "keywords": [
      "Activity recognition",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Gene",
      "Human–robot interaction",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Regression",
      "Robot",
      "Robustness (evolution)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Bangli"
      },
      {
        "surname": "Cai",
        "given_name": "Haibin"
      },
      {
        "surname": "Ju",
        "given_name": "Zhaojie"
      },
      {
        "surname": "Liu",
        "given_name": "Honghai"
      }
    ]
  },
  {
    "title": "Wavelet energy feature based source camera identification for ear biometric images",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.10.009",
    "abstract": "In this paper a source camera identification algorithm for ear biometric images has been proposed based on tunable filter bank as a feature extractor. Maintaining the frequency selectivity property, distinct features are extracted by this filter bank, based on a half-band polynomial of 14th order. With the help of four ear databases, it is demonstrated that tunable filter bank based features correctly identify the sources of ear images with an average accuracy of 99.25% when there are limited number of camera sources available. It is also shown that accuracy would fall when significantly large number of cameras are introduced to acquire ear images. Depending on the experimental results, it can be well concluded that tunable filter bank based feature, apart from its recognition performance, is also a promising candidate to support forensic validation of camera source.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518308080",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Biometrics",
      "Botany",
      "Computer science",
      "Computer vision",
      "Energy (signal processing)",
      "Engineering",
      "Epistemology",
      "Extractor",
      "Feature (linguistics)",
      "Feature extraction",
      "Filter (signal processing)",
      "Filter bank",
      "Identification (biology)",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process engineering",
      "Property (philosophy)",
      "Statistics",
      "Wavelet"
    ],
    "authors": [
      {
        "surname": "Chowdhury",
        "given_name": "Debbrota Paul"
      },
      {
        "surname": "Bakshi",
        "given_name": "Sambit"
      },
      {
        "surname": "Sa",
        "given_name": "Pankaj Kumar"
      },
      {
        "surname": "Majhi",
        "given_name": "Banshidhar"
      }
    ]
  },
  {
    "title": "Depth image super-resolution based on joint sparse coding",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.07.023",
    "abstract": "This paper proposes a new approach to single depth image super-resolution (SR), based upon a novel joint sparse coding model. A low-resolution color is used as a guide in the SR process. Firstly, we introduce synthetic characteristic image patch to learn a joint dictionary from the low-resolution depth map as well as its corresponding low-resolution intensity image. Then, we derive the joint nonlocal center sparse representation model based on sparse coding and theoretical analysis. In reconstruction process, we use Bayesian interpretation approach to estimation the sparse code coefficients for each unknown HR image patch. Meanwhile, we use an iterative algorithm to solve the JSC model. In addition, we exploit image patch redundancy within and across different scales, produce visually pleasing results without extensive training on external database. Experimental results demonstrate that the proposed method outperforms favorably many current state-of-the-art depth map super-resolution approaches on both visual effects and objective image quality and underpin the validity of our proposed model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518303222",
    "keywords": [
      "Artificial intelligence",
      "Coding (social sciences)",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Image resolution",
      "Iterative reconstruction",
      "Mathematics",
      "Neural coding",
      "Operating system",
      "Pattern recognition (psychology)",
      "Redundancy (engineering)",
      "Sparse approximation",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Beichen"
      },
      {
        "surname": "Zhou",
        "given_name": "Yuan"
      },
      {
        "surname": "Zhang",
        "given_name": "Yeda"
      },
      {
        "surname": "Wang",
        "given_name": "Aihua"
      }
    ]
  },
  {
    "title": "Adaptive appearance modeling via hierarchical entropy analysis over multi-type features",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107059",
    "abstract": "The descriptiveness of visual models is crucial for many image processing applications, however, it is still challenging to adaptively formulate such models. This paper systematically advocates a generic and adaptive appearance modeling method. For object-specific instances in images, it can adaptively generate a descriptive codebook by exploring the maximum discriminability of multi-type features. The key idea is to define feature-independent information entropy as a unified criterion to measure different features in a common entropy space. Towards this goal, a hierarchical maximum entropy (HME) model is proposed to conduct multi-feature selection based on the random forest. Specifically, the improved random forest replaces space-specific expression “distance similarity” with the statistical concept “entropy”. Thus, the random forest could integrate the subspace clustering results from different feature spaces. Such integration can not only afford adaptive feature selection and cross-feature error control but also be robust to possible weak/inconsistent feature expressions. To effectively construct a class-specific appearance model, a sparse codebook model, consisting of a series of weak learners, is proposed to further explore the maximum discriminative subspaces of each object class. Finally, a maximum entropy model is proposed to formulate appearance model by optimizing the probabilistic distributions of all the codebook words’ response parameters. To verify the efficacy and effectiveness of the proposed model, it is applied to multi-class image classification. We conduct extensive experiments and make comprehensive evaluations w.r.t several state-of-the-art methods over PASCAL VOC 2007, VOC 2012, Caltech 101 and Caltech 256 datasets. All the results demonstrate the advantages of the our method in terms of precision, robustness, flexibility, and versatility.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303619",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Entropy (arrow of time)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Jizhou"
      },
      {
        "surname": "Li",
        "given_name": "Shuai"
      },
      {
        "surname": "Qin",
        "given_name": "Hong"
      },
      {
        "surname": "Hao",
        "given_name": "Aimin"
      }
    ]
  },
  {
    "title": "Realtime multi-scale scene text detection with scale-based region proposal network",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107026",
    "abstract": "Multi-scale approaches have been widely used for achieving high accuracy for scene text detection, but they usually slow down the speed of the whole system. In this paper, we propose a two-stage framework for realtime multi-scale scene text detection. The first stage employs a novel Scale-based Region Proposal Network (SRPN) which can localize text of wide scale range and estimate text scale efficiently. Based on SRPN, non-text regions are filtered out, and text region proposals are generated. Moreover, based on text scale estimation by SRPN, small or big texts in region proposals are resized into a unified normal scale range. The second stage then adopts a Fully Convolutional Network based scene text detector to localize text words from proposals of the first stage. Text detector in the second stage detects texts of narrow scale range but accurately. Since most non-text regions are eliminated through SRPN efficiently, and texts in proposals are properly scaled to avoid multi-scale pyramid processing, the whole system is quite fast. We evaluate both performance and speed of the proposed method on datasets ICDAR2015, ICDAR2013, and MSRA-TD500. On ICDAR2015, our system can reach the state-of-the-art F-measure score of 85.40% at 16.5 fps (frame per second), and competitive performance of 79.66% at 35.1 fps, either of which is more than 5 times faster than previous best methods. On ICDAR2013 and MSRA-TD500, we also achieve remarkable speedup by keeping competitive performance. Ablation experiments are also provided to demonstrate the reasonableness of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303292",
    "keywords": [
      "Artificial intelligence",
      "Cartography",
      "Composite material",
      "Computer science",
      "Computer vision",
      "Detector",
      "Frame (networking)",
      "Frame rate",
      "Geography",
      "Image (mathematics)",
      "Materials science",
      "Optics",
      "Parallel computing",
      "Pattern recognition (psychology)",
      "Physics",
      "Pyramid (geometry)",
      "Range (aeronautics)",
      "Scale (ratio)",
      "Speedup",
      "Telecommunications",
      "Text detection"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Wenhao"
      },
      {
        "surname": "Zhang",
        "given_name": "Xu-Yao"
      },
      {
        "surname": "Yin",
        "given_name": "Fei"
      },
      {
        "surname": "Luo",
        "given_name": "Zhenbo"
      },
      {
        "surname": "Ogier",
        "given_name": "Jean-Marc"
      },
      {
        "surname": "Liu",
        "given_name": "Cheng-Lin"
      }
    ]
  },
  {
    "title": "Clustering by connection center evolution",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107063",
    "abstract": "The determination of clustering centers generally depends on the observation scale that we use to analyze the data to be clustered. An inappropriate scale usually leads to unreasonable clustering centers and thus unreasonable results. In this study, we first consider the similarity of elements in the data as the connectivity of vertices in an undirected graph, then present the concept of connection center and regard it as the clustering center of the data. Based on this definition, the determination of clustering centers and the assignment of class become very simple, natural and effective. One more crucial finding is that the clustering centers of different scales can be obtained easily by different powers of a similarity matrix, and the change of power from small to large leads to the dynamic evolution of clustering centers from local (microscopic) to global (macroscopic). Further, in this process of evolution, the number of clusters changes discontinuously, which means that the presented method can automatically skip the unreasonable number of clusters, suggest appropriate observation scales and provide corresponding clustering results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303656",
    "keywords": [
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Complete-linkage clustering",
      "Computer science",
      "Connection (principal bundle)",
      "Correlation clustering",
      "Data mining",
      "Fuzzy clustering",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Similarity (geometry)",
      "Single-linkage clustering"
    ],
    "authors": [
      {
        "surname": "Geng",
        "given_name": "Xiurui"
      },
      {
        "surname": "Tang",
        "given_name": "Hairong"
      }
    ]
  },
  {
    "title": "A model-based gait recognition method with body pose and human prior knowledge",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107069",
    "abstract": "We propose in this paper a novel model-based gait recognition method, PoseGait. Gait recognition is a challenging and attractive task in biometrics. Early approaches to gait recognition were mainly appearance-based. The appearance-based features are usually extracted from human body silhouettes, which are easy to compute and have shown to be efficient for recognition tasks. Nevertheless silhouettes shape is not invariant to changes in clothing, and can be subject to drastic variations, due to illumination changes or other external factors. An alternative to silhouette-based features are model-based features. However, they are very challenging to acquire especially for low image resolution. In contrast to previous approaches, our model PoseGait exploits human 3D pose estimated from images by Convolutional Neural Network as the input feature for gait recognition. The 3D pose, defined by the 3D coordinates of joints of the human body, is invariant to view changes and other external factors of variation. We design spatio-temporal features from the 3D pose to improve the recognition rate. Our method is evaluated on two large datasets, CASIA B and CASIA E. The experimental results show that the proposed method can achieve state-of-the-art performance and is robust to view and clothing variations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132031930370X",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Biometrics",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Feature extraction",
      "Gait",
      "Invariant (physics)",
      "Linguistics",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physiology",
      "Pose",
      "Silhouette"
    ],
    "authors": [
      {
        "surname": "Liao",
        "given_name": "Rijun"
      },
      {
        "surname": "Yu",
        "given_name": "Shiqi"
      },
      {
        "surname": "An",
        "given_name": "Weizhi"
      },
      {
        "surname": "Huang",
        "given_name": "Yongzhen"
      }
    ]
  },
  {
    "title": "Learning cross-modal correlations by exploring inter-word semantics and stacked co-attention",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.08.017",
    "abstract": "Cross-modal information retrieval aims to find heterogeneous data of various modalities from a given query of one modality. The main challenge is to learn the semantic correlations between different modalities and measure the distance across modalities. For text-image retrieval, existing work mostly uses off-the-shelf Convolutional Neural Network (CNN) for image feature extraction. For texts, word-level features such as bag-of-words or word2vec are employed to build deep learning models to represent texts. Besides word-level semantics, the semantic relations between words are also informative but less explored. In this paper, we explore the inter-word semantics by modelling texts by graphs using similarity measure based on word2vec. Besides feature presentations, we further study the problem of information imbalance between different modalities when describing the same semantics. For example textual descriptions often contain more background information that cannot be conveyed by images and vice versa. We propose a stacked co-attention network to progressively learn the mutually attended features of different modalities and enhance their fine-grained correlations. A dual-path neural network is proposed for cross-modal information retrieval. The model is trained by a pairwise similarity loss function to maximize the similarity of relevant text-image pairs and minimize the similarity of irrelevant pairs. Experimental results show that the proposed model outperforms the state-of-the-art methods significantly, with 19% improvement on accuracy for the best case.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518304380",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Embedding",
      "Feature (linguistics)",
      "Feature extraction",
      "Geometry",
      "Image (mathematics)",
      "Information retrieval",
      "Linguistics",
      "Mathematics",
      "Modalities",
      "Natural language processing",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Semantic similarity",
      "Semantics (computer science)",
      "Similarity (geometry)",
      "Social science",
      "Sociology",
      "Word (group theory)",
      "Word2vec"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Jing"
      },
      {
        "surname": "Lu",
        "given_name": "Yuhang"
      },
      {
        "surname": "Zhang",
        "given_name": "Weifeng"
      },
      {
        "surname": "Qin",
        "given_name": "Zengchang"
      },
      {
        "surname": "Liu",
        "given_name": "Yanbing"
      },
      {
        "surname": "Hu",
        "given_name": "Yue"
      }
    ]
  },
  {
    "title": "Quick automatic head image matting method based on segmentation and propagation",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.02.012",
    "abstract": "Image matting is a process of extracting objects from background in an image, which is an important work in digital image processing and video editing. Previous methods have poor performance and most methods need trimap or scribble to compute to get accurate image matting results. In this paper a quick automatic head image matting method for certificate photo production is put forward, which could rapidly extract satisfied head photo from images that are shoot by portable camera. First a new training data set about hair matting is created, and the image is segmented into different regions according to different gray level; then we detect and locate the face and eyes to adjust the correct head position; Finally, the accurate hair pixels are extracted from the edge region around head by multimodal Gaussian process regression. It's the advantage that the regions with clear foreground and background could be quickly extracted by segmented method, and the regions with similar foreground and background colors or complicated textures were labeled (the edge regions with hair around head), thus the hair could be extracted in smaller area. In our method the quick and accurate extracting of head photo needn't the trimap or scribbling or lots of training. Experimental results clearly demonstrate the superiority of our algorithm over previous methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519300522",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Face (sociological concept)",
      "Geology",
      "Geomorphology",
      "Head (geology)",
      "Image (mathematics)",
      "Image processing",
      "Image segmentation",
      "Operating system",
      "Pattern recognition (psychology)",
      "Pixel",
      "Process (computing)",
      "Segmentation",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xiaofan"
      },
      {
        "surname": "Li",
        "given_name": "Shengjie"
      },
      {
        "surname": "Sui",
        "given_name": "Liansheng"
      },
      {
        "surname": "Wang",
        "given_name": "Jiahao"
      }
    ]
  },
  {
    "title": "Informative variable identifier: Expanding interpretability in feature selection",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107077",
    "abstract": "There is nowadays an increasing interest in discovering relationships among input variables (also called features) from data to provide better interpretability, which yield more confidence in the solution and provide novel insights about the nature of the problem at hand. We propose a novel feature selection method, called Informative Variable Identifier (IVI), capable of identifying the informative variables and their relationships. It transforms the input-variable space distribution into a coefficient-feature space using existing linear classifiers or a more efficient weight generator that we also propose, Covariance Multiplication Estimator (CME). Informative features and their relationships are determined analyzing the joint distribution of these coefficients with resampling techniques. IVI and CME select the informative variables and then pass them on to any linear or nonlinear classifier. Experiments show that the proposed approach can outperform state-of-art algorithms in terms of feature identification capabilities, and even in classification performance when subsequent classifiers are used.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303784",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Discriminative model",
      "Estimator",
      "Feature (linguistics)",
      "Feature selection",
      "Feature vector",
      "Identifier",
      "Interpretability",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Random subspace method",
      "Resampling",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Muñoz-Romero",
        "given_name": "Sergio"
      },
      {
        "surname": "Gorostiaga",
        "given_name": "Arantza"
      },
      {
        "surname": "Soguero-Ruiz",
        "given_name": "Cristina"
      },
      {
        "surname": "Mora-Jiménez",
        "given_name": "Inmaculada"
      },
      {
        "surname": "Rojo-Álvarez",
        "given_name": "José Luis"
      }
    ]
  },
  {
    "title": "Separating Structure from Noise in Large Graphs Using the Regularity Lemma",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107070",
    "abstract": "How can we separate structural information from noise in large graphs? To address this fundamental question, we propose a graph summarization approach based on Szemerédi’s Regularity Lemma, a well-known result in graph theory, which roughly states that every graph can be approximated by the union of a small number of random-like bipartite graphs called “regular pairs”. Hence, the Regularity Lemma provides us with a principled way to describe the essential structure of large graphs using a small amount of data. Our paper has several contributions: (i) We present our summarization algorithm which is able to reveal the main structural patterns in large graphs. (ii) We discuss how to use our summarization framework to efficiently retrieve from a database the top-k graphs that are most similar to a query graph. (iii) Finally, we evaluate the noise robustness of our approach in terms of the reconstruction error and the usefulness of the summaries in addressing the graph search task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303711",
    "keywords": [],
    "authors": [
      {
        "surname": "Fiorucci",
        "given_name": "Marco"
      },
      {
        "surname": "Pelosin",
        "given_name": "Francesco"
      },
      {
        "surname": "Pelillo",
        "given_name": "Marcello"
      }
    ]
  },
  {
    "title": "Efficient CNN based summarization of surveillance videos for resource-constrained devices",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.08.003",
    "abstract": "The widespread usage of surveillance cameras in smart cities has resulted in a gigantic volume of video data whose indexing, retrieval and management is a challenging issue. Video summarization tends to detect important visual data from the surveillance stream and can help in efficient indexing and retrieval of required data from huge surveillance datasets. In this research article, we propose an efficient convolutional neural network based summarization method for surveillance videos of resource-constrained devices. Shot segmentation is considered as a backbone of video summarization methods and it affects the overall quality of the generated summary. Thus, we propose an effective shot segmentation method using deep features. Furthermore, our framework maintains the interestingness of the generated summary using image memorability and entropy. Within each shot, the frame with highest memorability and entropy score is considered as a keyframe. The proposed method is evaluated on two benchmark video datasets and the results are encouraging compared to state-of-the-art video summarization methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518303842",
    "keywords": [
      "Artificial intelligence",
      "Automatic summarization",
      "Benchmark (surveying)",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Entropy (arrow of time)",
      "Geodesy",
      "Geography",
      "Information retrieval",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Search engine indexing",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Muhammad",
        "given_name": "Khan"
      },
      {
        "surname": "Hussain",
        "given_name": "Tanveer"
      },
      {
        "surname": "Baik",
        "given_name": "Sung Wook"
      }
    ]
  },
  {
    "title": "Workflow recognition with structured two-stream convolutional networks",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.10.011",
    "abstract": "Intelligent monitoring plays an important role in the context of Industry 4.0”, and behavior recognition is one of the research points in computer vision. However, monitoring the workflow of human beings and machines in production process is very difficult in the real-world complex factory environment. In this paper, we propose a novel workflow recognition framework based on the structured two-stream convolutional neural networks (CNNs) to recognize the behavior of both workers and machines. To improve the accuracy of workflow recognition, we use the CNNs to extract the spatial-temporal features and integrate an attention mechanism to detect the valuable behavior. Then, a Video Triple model is introduced to gain extra timestamp information, which can extend the behavior recognition to workflow recognition. Extensive simulation experiments are conducted on THUMOS’14 dataset and a real-world workflow dataset that show the significant performance improvement in video activity recognition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518308146",
    "keywords": [
      "Activity recognition",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Convolutional neural network",
      "Data mining",
      "Database",
      "Factory (object-oriented programming)",
      "Machine learning",
      "Operating system",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Programming language",
      "Real-time computing",
      "Timestamp",
      "Workflow",
      "Workflow technology"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Haiyang"
      },
      {
        "surname": "Cheng",
        "given_name": "Kaiming"
      },
      {
        "surname": "Li",
        "given_name": "Zhongjin"
      },
      {
        "surname": "Chen",
        "given_name": "Jie"
      },
      {
        "surname": "Hu",
        "given_name": "Hua"
      }
    ]
  },
  {
    "title": "Improved image clustering with deep semantic embedding",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.10.022",
    "abstract": "Dimensionality reduction has found extensive use in the area of high dimensional data clustering. But unfortunately some valuable discriminative information might be lost in the process of dimensionality reduction, including feature extraction and feature selection. This issue inevitably degrades the performance of clustering algorithms. Recently, the semantic space embedding is emerging as a promising technique for state-of-the-art clustering methods, which can provide extra discriminative information and reasonably improve the clustering performance. In this paper, we plan to improve the performance of high dimensional image clustering by embedding semantic information into the original visual space. Inspired by the great success of deep learning, we employed a multi-layer autoencoder based on deep neural networks (DNNs) to undertake the semantic feature embedding and dimensionality reduction. By this way, the final image clustering task is carried out in the lower-dimensional feature space with deep semantic embedding. A series of experiments on acknowledged benchmark image datasets demonstrate that the proposed approaches can achieve superior performance over several existing clustering methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786551830850X",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Dimensionality reduction",
      "Discriminative model",
      "Embedding",
      "Feature (linguistics)",
      "Feature extraction",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Jun"
      },
      {
        "surname": "Yuan",
        "given_name": "Xuan"
      },
      {
        "surname": "Xu",
        "given_name": "Pengfei"
      },
      {
        "surname": "Bai",
        "given_name": "Hao"
      },
      {
        "surname": "Liu",
        "given_name": "Baoying"
      }
    ]
  },
  {
    "title": "Bayesian query expansion for multi-camera person re-identification",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.06.009",
    "abstract": "Person re-identification (re-ID) is challenging because pedestrians may exhibit distinct appearance under different cameras. Given a query image, previous methods usually output the person retrieval results directly, which may perform badly due to the limited information provided by the single query image. To mine more query information, we add an expansion step to post-process the initial ranking list. The intuition is that a true match in the gallery may be difficult to be found by the query alone, but it can be easily retrieved by other true matches in the initial ranking list. In this paper, we propose the Bayesian Query Expansion (BQE) method to generate a new query with information from the initial ranking list. The Bayesian model is used to predict true matches in the gallery. We apply pooling on the features of these “true matches” to get a single vector, i.e., the expanded new query, with which the retrieval process is performed again to obtain the final results. We evaluate BQE with various feature extraction methods and distance metric learning methods on four large-scale re-ID datasets. We observe consistent improvement over all the baselines and report competitive performances compared with the state-of-the-art results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518302344",
    "keywords": [
      "Artificial intelligence",
      "Bayesian probability",
      "Biology",
      "Botany",
      "Computer science",
      "Computer vision",
      "Identification (biology)",
      "Information retrieval",
      "Pattern recognition (psychology)",
      "Query expansion"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Yutian"
      },
      {
        "surname": "Zheng",
        "given_name": "Zhedong"
      },
      {
        "surname": "Zhang",
        "given_name": "Hong"
      },
      {
        "surname": "Gao",
        "given_name": "Chenqiang"
      },
      {
        "surname": "Yang",
        "given_name": "Yi"
      }
    ]
  },
  {
    "title": "LPR-Net: Recognizing Chinese license plate in complex environments",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.09.026",
    "abstract": "License plate recognition (LPR) technology has been attracting increasing interest during recent years for its exclusive role in real world intelligent traffic management systems. Owing to its importance, numerous LPR methods have been developed. These methods are generally composed of three processing steps, i.e. license plate location, character segmentation and character recognition. However, the three-step scheme always yields unsatisfactory recognition performance in challenging complex environment like uneven illumination, adverse atmospheric conditions, complex backgrounds, unclear vehicle plates, low-quality surveillance camera, etc. In such scenes, the obtained license plates are usually not clear, which will cause imprecise results of localization and segmentation. Consequently, the recognition capacity is inadequate as its performance highly depends on the effects of localization and segmentation. To address these challenges, we propose a novel Chinese vehicle license plate recognition method to directly recognize license plate through an end-to-end deep learning architecture named license plate recognition net (LPR-Net). The LPR-Net is a hybrid deep architecture that consists of a residual error network for extracting basic features, a multi-scale net for extracting multi-scale features, a regression net for locating plate and characters, and a classification net for recognition. Moreover, an effective scheme based on batch normalization is used to accelerate training speed in the learning procedure. Extensive experiments demonstrate that the proposed method achieves excellent recognition accuracy and works more robustly and efficiently compared with the state-of-the-art methods in complex environments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518306998",
    "keywords": [
      "Algorithm",
      "Anthropology",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "License",
      "Normalization (sociology)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Residual",
      "Segmentation",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Di"
      },
      {
        "surname": "Tian",
        "given_name": "Yumin"
      },
      {
        "surname": "Geng",
        "given_name": "Wenhui"
      },
      {
        "surname": "Zhao",
        "given_name": "Lin"
      },
      {
        "surname": "Gong",
        "given_name": "Chen"
      }
    ]
  },
  {
    "title": "Deep discriminative representation for generic palmprint recognition",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107071",
    "abstract": "State-of-the-art palmprint recognition methods have achieved significant performances. However, most of the existing methods are focused on particular scenarios such as a specific illumination or being captured using a contact-based or contactless device. Therefore, these algorithms cannot meet the ever-changing complex application requirements. To resolve this issue, this paper proposes a generic framework to represent high-level discriminative features for multiple scenarios in palmprint recognition with learned discriminative deep convolutional networks named deep discriminative representation (DDR). We propose to learn discriminative deep convolutional networks with limited palmprint training data, which is utilized to extract deep discriminative features. Then, the collaborative representation based classifier is implemented for palmprint recognition, which is flexible and practical in numerous scenarios. The experimental results demonstrate that DDR produces the best recognition performance in generic palmprint recognition compared to other state-of-the-art methods. For contact-based palmprint recognition under different lighting sources, DDR achieved the best performance on the PolyU Multi-spectral database with M_R, M_B, M_G and M_NIR, respectively. As for contactless palmprint recognition, DDR obtained the highest results on the IITD and CASIA databases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303723",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Discriminative model",
      "Law",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Shuping"
      },
      {
        "surname": "Zhang",
        "given_name": "Bob"
      }
    ]
  },
  {
    "title": "Progressive generative adversarial networks with reliable sample identification",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.01.007",
    "abstract": "Generative Adversarial Networks (GANs) are deep neural network architectures comprising of two neural networks, namely discriminator and generator, which contest with each other in a zero-sum game. In the past years, although original GANs and their variations have achieved impressive success, there are some challenges still remain, especially unstable training progress leading to gradient vanishing or saturation. We can show by inspection that the reliable samples with smaller errors are beneficial to achieve a better generator, while the unreliable one might disturb the training procedure. Enlightened from this observation, we introduce an indicator for each sample to indicate its reliability in this paper. Based on this, we exploit a new objective function to learn the generator/discriminator and infer the indicator for each sample simultaneously. In such a way, the unreliable samples that might result in the opposite side are discarded in training stage. Meanwhile, when the training errors become smaller, more and more samples are included in the reliable set of samples, until no more reliable one are produced. It is noteworthy that the proposed method is adapted to both the original GANs and its variations. Experiments on CIFAR-10, STL-10 and LSUN datasets demonstrate the state-of-the-art performance of the proposed framework with respect to GANs and its variations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519300066",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Artificial neural network",
      "CONTEST",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Computer security",
      "Deep neural networks",
      "Detector",
      "Discriminator",
      "Exploit",
      "Generative grammar",
      "Generator (circuit theory)",
      "Law",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Power (physics)",
      "Programming language",
      "Quantum mechanics",
      "Reliability (semiconductor)",
      "Sample (material)",
      "Set (abstract data type)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Gang"
      },
      {
        "surname": "Luo",
        "given_name": "Minnan"
      },
      {
        "surname": "Liu",
        "given_name": "Huan"
      },
      {
        "surname": "Zhang",
        "given_name": "Donghui"
      },
      {
        "surname": "Zheng",
        "given_name": "Qinghua"
      }
    ]
  },
  {
    "title": "Keyword-based approach for recognizing fraudulent messages by keystroke dynamics",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107067",
    "abstract": "In recent years, many approaches that use keystroke dynamics in free text authentication have been proposed. The major drawback of the proposed approaches is that training generally requires several months, thereby resulting in low practicality. In this study, a method to detect U.S. English fraudulent messages by analyzing keyboard users' keystroke dynamics is proposed. To the best of our knowledge, this is the first study to apply keystroke dynamics to detect fraudulent instant messages. In the proposed system, each user requires only approximately 20 min of training in U.S. English keystroke dynamics. Furthermore, a voting-based statistical classifier is presented to improve the recognition accuracy of instant messages and prevent phishing messages. Experimental results indicate that the proposed approach outperforms other relevant published methods in terms of shorter training time, fewer false alarms, and comparable recognition accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303681",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Authentication (law)",
      "Biometrics",
      "Classifier (UML)",
      "Computer science",
      "Computer security",
      "Dynamics (music)",
      "Keystroke dynamics",
      "Keystroke logging",
      "Machine learning",
      "Password",
      "Physics",
      "S/KEY",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Tsai",
        "given_name": "Cheng-Jung"
      },
      {
        "surname": "Huang",
        "given_name": "Po-Hao"
      }
    ]
  },
  {
    "title": "A baseline regularization scheme for transfer learning with convolutional neural networks",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107049",
    "abstract": "In inductive transfer learning, fine-tuning pre-trained convolutional networks substantially outperforms training from scratch. When using fine-tuning, the underlying assumption is that the pre-trained model extracts generic features, which are at least partially relevant for solving the target task, but would be difficult to extract from the limited amount of data available on the target task. However, besides the initialization with the pre-trained model and the early stopping, there is no mechanism in fine-tuning for retaining the features learned on the source task. In this paper, we investigate several regularization schemes that explicitly promote the similarity of the final solution with the initial model. We show the benefit of having an explicit inductive bias towards the initial model. We eventually recommend that the baseline protocol for transfer learning should rely on a simple L 2 penalty using the pre-trained model as a reference.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303516",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Baseline (sea)",
      "Computer science",
      "Convolutional neural network",
      "Geology",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Oceanography",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)",
      "Scheme (mathematics)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xuhong"
      },
      {
        "surname": "Grandvalet",
        "given_name": "Yves"
      },
      {
        "surname": "Davoine",
        "given_name": "Franck"
      }
    ]
  },
  {
    "title": "Fast and robust template matching with majority neighbour similarity and annulus projection transformation",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107029",
    "abstract": "In the paper, a novel fast and robust template matching method named A-MNS based on Majority Neighbour Similarity (MNS) and the annulus projection transformation (APT) is proposed. Its essence is the MNS, a useful, rotation-invariant, low-computational-cost and robust similarity measurement. The proposed method is theoretically demonstrated and experimentally evaluated as being able to estimate the rotation angle of the target object, overcome challenges such as background clutter, occlusion, arbitrary rotation transformation, and non-rigid deformation, while performing fast matching. Empirical results evaluated on the up-to-date benchmark show that A-MNS is 4.419 times faster than DDIS (the state-of-the-art) and is also competitive in terms of its matching accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303322",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Computer science",
      "Computer vision",
      "Gene",
      "Image (mathematics)",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Projection (relational algebra)",
      "Similarity (geometry)",
      "Statistics",
      "Template matching",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Lai",
        "given_name": "Jinxiang"
      },
      {
        "surname": "Lei",
        "given_name": "Liang"
      },
      {
        "surname": "Deng",
        "given_name": "Kaiyuan"
      },
      {
        "surname": "Yan",
        "given_name": "Runming"
      },
      {
        "surname": "Ruan",
        "given_name": "Yang"
      },
      {
        "surname": "Jinyun",
        "given_name": "Zhou"
      }
    ]
  },
  {
    "title": "Fine-grained action segmentation using the semi-supervised action GAN",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107039",
    "abstract": "In this paper we address the problem of continuous fine-grained action segmentation, in which multiple actions are present in an unsegmented video stream. The challenge for this task lies in the need to represent the hierarchical nature of the actions and to detect the transitions between actions, allowing us to localise the actions within the video effectively. We propose a novel recurrent semi-supervised Generative Adversarial Network (GAN) model for continuous fine-grained human action segmentation. Temporal context information is captured via a novel Gated Context Extractor (GCE) module, composed of gated attention units, that directs the queued context information through the generator model, for enhanced action segmentation. The GAN is made to learn features in a semi-supervised manner, enabling the model to perform action classification jointly with the standard, unsupervised, GAN learning procedure. We perform extensive evaluations on different architectural variants to demonstrate the importance of the proposed network architecture, and show that it is capable of outperforming current state-of-the-art on three challenging datasets: 50 Salads, MERL Shopping and Georgia Tech Egocentric Activities dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303413",
    "keywords": [
      "Action (physics)",
      "Action recognition",
      "Artificial intelligence",
      "Biology",
      "Business",
      "Class (philosophy)",
      "Computer science",
      "Context (archaeology)",
      "Economics",
      "Engineering",
      "Extractor",
      "Generative grammar",
      "Generator (circuit theory)",
      "Machine learning",
      "Management",
      "Market segmentation",
      "Marketing",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Physics",
      "Power (physics)",
      "Process engineering",
      "Quantum mechanics",
      "Segmentation",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Gammulle",
        "given_name": "Harshala"
      },
      {
        "surname": "Denman",
        "given_name": "Simon"
      },
      {
        "surname": "Sridharan",
        "given_name": "Sridha"
      },
      {
        "surname": "Fookes",
        "given_name": "Clinton"
      }
    ]
  },
  {
    "title": "Pattern complexity-based JND estimation for quantization watermarking",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.08.009",
    "abstract": "The perceptual just noticeable distortion (JND), which plays an important role in perceptual image watermarking and can refers to the optimal tradeoff between imperceptibility and robustness. Major challenges of the perceptual quantization watermarking approach are two-fold: (1) Most DCT-based JND models cannot accurately estimate the contrast masking effect due to the complicated interaction among visual contents. Research on cognitive science shows that the HVS is adaptive to extract the visual pattern for image understanding, and we can formulate the pattern complexity as another factor to determine the total watermarking strength. (2) Moreover, the calculated JND values will change as watermark embedding can affect the pixels of the image, i.e. the operation in cross-domain reduce the watermarking robustness. Therefore, in this regard, the maximum directional energy is calculated by three AC DCT coefficients, which can measure the different direction energy and keep the pattern complexity measurement insensitive to the changes caused by watermarking procedure. The luminance contrast is also calculated in the DCT domain. So a pattern complexity-based perceptual JND estimation model is designed by takeing DCT-based pattern complexity and luminance contrast into account. Furthermore, a new logarithmic quantization watermarking scheme is presented based on the proposed model to verify the feasibility and effectiveness of our proposed JND model. Experimental results show that the new built JND model can effectively enhance the robustness of the quantization watermarking scheme.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518304033",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Digital watermarking",
      "Discrete cosine transform",
      "Embedding",
      "Gene",
      "Human visual system model",
      "Image (mathematics)",
      "Just-noticeable difference",
      "Logarithm",
      "Luminance",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Quantization (signal processing)",
      "Robustness (evolution)",
      "Watermark"
    ],
    "authors": [
      {
        "surname": "Wan",
        "given_name": "Wenbo"
      },
      {
        "surname": "Wang",
        "given_name": "Jun"
      },
      {
        "surname": "Li",
        "given_name": "Jing"
      },
      {
        "surname": "Meng",
        "given_name": "Lili"
      },
      {
        "surname": "Sun",
        "given_name": "Jiande"
      },
      {
        "surname": "Zhang",
        "given_name": "Huaxiang"
      },
      {
        "surname": "Liu",
        "given_name": "Ju"
      }
    ]
  },
  {
    "title": "Diverse fuzzy c-means for image clustering",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.07.004",
    "abstract": "Image clustering is a key technique for better accomplishing image annotation and searching in large image repositories. Fuzzy c-means and its variations have achieved excellent performance on image clustering because they allow each image to belong to more than one cluster. However, these methods neglect the relations between different image clusters, and hence often suffer from the “cluster one-sidedness” problem that redundant centers are learned to characterize the same or similar image clusters. To this issue, we propose a diverse fuzzy c-means for image clustering via introducing a novel diversity regularization into the traditional fuzzy c-means objective. This diversity regularization guarantees the learned image cluster centers to be different from each other and to fill the image data space as much as possible. An efficient optimization algorithm is exploited to address the diverse fuzzy c-means objective, which is proved to converge to local optimal solutions and has a satisfactory time complexity. Experiments on synthetic and six image datasets demonstrate the effectiveness of the proposed method as well as the necessity of the diversity regularization.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518302903",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Fuzzy clustering",
      "Fuzzy logic",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Lingling"
      },
      {
        "surname": "Luo",
        "given_name": "Minnan"
      },
      {
        "surname": "Liu",
        "given_name": "Jun"
      },
      {
        "surname": "Li",
        "given_name": "Zhihui"
      },
      {
        "surname": "Zheng",
        "given_name": "Qinghua"
      }
    ]
  },
  {
    "title": "Supervised discrete cross-modal hashing based on kernel discriminant analysis",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107062",
    "abstract": "Cross-modal hashing methods have drawn considerable attention due to the rapid growth of multi-modal data. To obtain efficient binary codes in a low-dimensional Hamming space, most existing approaches relaxed the discrete constraint, which could cause quantization loss and even result in performance degradation. In order to avoid this bottleneck, some scholars employed iterative discrete cyclic coordinate descent (DCC) to learn hash codes bit by bit, but this was very time-consuming. To counter this problem, a simple yet novel supervised discrete cross-modal hashing framework is represented to directly learn the unified discrete binary codes with a close-form, rather than bit by bit. Furthermore, to preserve label separability, the kernel discriminant analysis is fused into the proposed framework to enrich the discrimination ability of the learned binary codes. The goal of the proposed method is to obtain the common discrete binary codes of different modalities in a shared latent Hamming space so that the different modalities of a sample can be effectively connected. Experimental study shows the encouraging results of the proposed algorithm in comparisons to the state-of-the-art baseline approaches on four real-world datasets. Especially on the LabelMe dataset, the superiority of the proposed method is obvious, with an average improvement of 9% over the best available results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303644",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Binary code",
      "Binary number",
      "Block code",
      "Combinatorics",
      "Computer science",
      "Computer security",
      "Decoding methods",
      "Double hashing",
      "Facial recognition system",
      "Feature hashing",
      "Hamming code",
      "Hamming distance",
      "Hamming space",
      "Hash function",
      "Hash table",
      "Kernel (algebra)",
      "Kernel Fisher discriminant analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Quantization (signal processing)",
      "Universal hashing"
    ],
    "authors": [
      {
        "surname": "Fang",
        "given_name": "Yixian"
      },
      {
        "surname": "Ren",
        "given_name": "Yuwei"
      }
    ]
  },
  {
    "title": "Object detection with class aware region proposal network and focused attention objective",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.09.025",
    "abstract": "In this paper, we propose a novel deep CNN-based framework to improve object detection performance. First, we introduce the Class Aware Region Proposal Network (CARPN) to produce high quality region proposals by using a new strategy for anchor generation, and by training the network with both bounding boxes and category labels of the objects. Instead of learning a binary object/non-object classifier for generating region proposals, we assign the class label to each anchor, and train the region proposal network with a multi-class loss. Second, we introduce the Focused Attention (FA) objective to encourage the network to learn features mainly from objects of interest while suppressing those features from the background region. As a result, false positive proposals caused by strong background features can be reduced to a large extent. Comprehensive experimental evaluations reveal that the proposed CARPN & FA framework remarkably outperforms the baseline Faster R-CNN method up to 4.1% mAP with a shallower network and 2.8% mAP with a deeper network, and achieves a better mAP than most of the latest state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518306986",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Bounding overwatch",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Image (mathematics)",
      "Machine learning",
      "Mathematics",
      "Minimum bounding box",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Saliency map"
    ],
    "authors": [
      {
        "surname": "Tao",
        "given_name": "Xiaoyu"
      },
      {
        "surname": "Gong",
        "given_name": "Yihong"
      },
      {
        "surname": "Shi",
        "given_name": "Weiwei"
      },
      {
        "surname": "Cheng",
        "given_name": "De"
      }
    ]
  },
  {
    "title": "Eye landmarks detection via weakly supervised learning",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107076",
    "abstract": "Extensive eye researches provide good results when images are captured under constrained environment. However, the accuracy of eye landmarks detection depends on explicit bounding-box of eye regions and drops severely in non-ideal conditions. This paper has proposed a novel weakly supervised eye landmarks detection algorithm with object detection and recurrent learning modules. The former is combined with faster R-CNN and is competent to detect bounding-box of facial components and initial positions of the eye. The recurrent module is employed for eye landmarks refinement using the initial eye shape. The proposed algorithm can augment training data effectively and our specific format data consist of supervised and weakly supervised samples. Supervised samples have ground truth of bounding-boxes, corresponding classification labels and eye landmarks coordinates while weakly supervised data does not have eye landmarks information. Despite trained on facial images, the proposed method can detect eyes in severely occluded or local view of facial images without prerequisites of face alignment. Further experiments are performed on our supervised testing set and some public datasets. Their results demonstrate the robustness and effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303772",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Bounding overwatch",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Face (sociological concept)",
      "Gene",
      "Image (mathematics)",
      "Minimum bounding box",
      "Object detection",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Social science",
      "Sociology",
      "Supervised learning"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Bin"
      },
      {
        "surname": "Chen",
        "given_name": "Renwen"
      },
      {
        "surname": "Zhou",
        "given_name": "Qinbang"
      },
      {
        "surname": "Xu",
        "given_name": "Wang"
      }
    ]
  },
  {
    "title": "Deep conditional adaptation networks and label correlation transfer for unsupervised domain adaptation",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107072",
    "abstract": "Unsupervised domain adaptation aims to improve the performance of an unknown target domain by utilizing the knowledge learned from a related source domain. Given that the target label information is unavailable in the unsupervised situation, it is challenging to match the domain distributions and to transfer the source model to target applications. In this paper, a Deep Conditional Adaptation Networks (DCAN) is proposed to address the unsupervised domain adaptation problem. DCAN is implemented based on a deep neural network and attempts to learn domain invariant features based on the Wasserstein distance. A conditional adaptation strategy is presented to reduce the domain distribution discrepancy and to address category mismatch and class prior bias, which are usually ignored in marginal adaptation approaches. Furthermore, we propose a label correlation transfer algorithm to address the unsupervised issues, by generating more effective pseudo target labels based on the underlying cross-domain relationship. A set of comparative experiments were performed on standard domain adaptation benchmarks and the results demonstrate that the proposed DCAN outperforms previous adaptation methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303735",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Conditional probability distribution",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Machine learning",
      "Marginal distribution",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Random variable",
      "Set (abstract data type)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yu"
      },
      {
        "surname": "Yang",
        "given_name": "Chunling"
      },
      {
        "surname": "Zhang",
        "given_name": "Yan"
      },
      {
        "surname": "Li",
        "given_name": "Yuze"
      }
    ]
  },
  {
    "title": "Joint spatial-spectral hyperspectral image classification based on convolutional neural network",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.10.003",
    "abstract": "Hyperspectral image (HSI) classification technology has been widely used in many earth observation tasks, such as detection, recognition, and surveillance. The traditional hyperspectral image classification methods mainly utilize hand-crafted features, such as edge and texture descriptors, which are not robust for different input data. By contrast, deep learning based methods exploit high-level features for hyperspectral image classification, but they usually degenerate the spatial-spectral structure, depend on a large number of training samples, and ignore a large amount of implicitly useful information. To address these problems, a new joint spatial-spectral hyperspectral image classification method based on different-scale two-stream convolutional network and spatial enhancement strategy is proposed in this paper. First, the pixel blocks at different scales around the center pixel are selected as the basic units to be processed. Then, a spatial enhancement strategy is designed to obtain various spatial location information under the limited training samples by the spatial rotation and row-column transformation. Finally, the spatial-spectral feature is learned by a different-scale two-stream convolutional network, and the classification result of the center pixel is obtained by a softmax layer. Experimental results on two datasets demonstrate that the proposed method outperforms other state-of-the-art methods qualitatively and quantitatively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518307748",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Feature extraction",
      "Geography",
      "Hyperspectral imaging",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Remote sensing",
      "Softmax function",
      "Spatial analysis"
    ],
    "authors": [
      {
        "surname": "Han",
        "given_name": "Mengxin"
      },
      {
        "surname": "Cong",
        "given_name": "Runmin"
      },
      {
        "surname": "Li",
        "given_name": "Xinyu"
      },
      {
        "surname": "Fu",
        "given_name": "Huazhu"
      },
      {
        "surname": "Lei",
        "given_name": "Jianjun"
      }
    ]
  },
  {
    "title": "Efficient weakly-supervised discrete hashing for large-scale social image retrieval",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.08.033",
    "abstract": "Hashing has been widely exploited for information retrieval recently, because of its high computation efficiency and low storage cost. However, many existing hashing methods cannot perform well on large-scale social image retrieval, due to the relaxed hash optimization and the lack of supervised semantic labels. In this paper, we propose an efficient Weakly-supervised Discrete Hashing (WDH) to solve the limitations. We formulate a unified weakly-supervised hash learning framework. It could effectively enrich the semantics of image hash codes with the freely obtained user-provided social tags and simultaneously remove their involved adverse noises. Furthermore, instead of relaxed hash optimization, we propose an efficient discrete hash optimization method based on Augmented Lagrangian Multiplier (ALM) to directly solve the hash codes without quantization information loss. Experiments on two standard social image datasets demonstrate the superior performance of the proposed method compared with several state-of-the-art hashing techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518305129",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Double hashing",
      "Dynamic perfect hashing",
      "Feature hashing",
      "Hash function",
      "Hash table",
      "Image (mathematics)",
      "Image retrieval",
      "Locality-sensitive hashing",
      "Pattern recognition (psychology)",
      "Quantization (signal processing)",
      "Theoretical computer science",
      "Universal hashing"
    ],
    "authors": [
      {
        "surname": "Cui",
        "given_name": "Hui"
      },
      {
        "surname": "Zhu",
        "given_name": "Lei"
      },
      {
        "surname": "Cui",
        "given_name": "Chaoran"
      },
      {
        "surname": "Nie",
        "given_name": "Xiushan"
      },
      {
        "surname": "Zhang",
        "given_name": "Huaxiang"
      }
    ]
  },
  {
    "title": "Adaptive multi-view subspace clustering for high-dimensional data",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.01.016",
    "abstract": "With the rapid development of multimedia technologies, we frequently confront with high-dimensional data and multi-view data, which usually contain redundant features and distinct types of features. How to efficiently cluster such kinds of data is still a great challenge. Traditional multi-view subspace clustering aims to determine the distribution of views by extra empirical parameters and search the optimal projection matrix by eigenvalue decomposition, which is impractical for real-world applications. In this paper, we propose a new adaptive multi-view subspace clustering method to integrate heterogenous data in the low-dimensional feature space. Concretely, we extend K-means clustering with feature learning to handle high-dimensional data. Besides, for multi-view data, we evaluate the weights of distinct views according to their compactness of the cluster structure in the low-dimensional subspace. We apply the proposed method to four benchmark datasets and compare it with several widely used clustering algorithms. Experimental results demonstrate the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519300121",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Data point",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Projection (relational algebra)",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Fei"
      },
      {
        "surname": "Wang",
        "given_name": "Xiao-dong"
      },
      {
        "surname": "Zeng",
        "given_name": "Zhi-qiang"
      },
      {
        "surname": "Hong",
        "given_name": "Chao-qun"
      }
    ]
  },
  {
    "title": "A Möbius transformation based model for fingerprint minutiae variations",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107054",
    "abstract": "When an individual’s fingerprint is scanned, although the global fingerprint pattern is unchanged, at the local level, between different scans the minutiae pattern may vary. Minutiae translation and rotation are caused by changing finger orientation and position shift during fingerprint acquisition. Minutiae patterns may also suffer non-linear distortion due to finger skin elasticity. Despite a variety of approaches to detecting deformations in fingerprint images, there has been no method available for capturing minutiae variations between two impressions of the same finger in a unified model. In this paper we address this issue by proposing a unified model to represent minutiae variations between fingerprint scans and formulate the changes to minutiae feature patterns. We identify the Möbius transformation as a good candidate for modelling minutiae translation, rotation and non-linear distortion, that is, different types of minutiae variations are described in a single model. Not only do we mathematically prove that the Möbius transformation based model is a unified model for capturing minutiae variations, but we also experimentally verify the effectiveness of this model using a public database.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303565",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Biochemistry",
      "Chemistry",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Distortion (music)",
      "Fingerprint (computing)",
      "Fingerprint recognition",
      "Gene",
      "Geometry",
      "Mathematics",
      "Messenger RNA",
      "Minutiae",
      "Orientation (vector space)",
      "Pattern recognition (psychology)",
      "Rotation (mathematics)",
      "Transformation (genetics)",
      "Translation (biology)"
    ],
    "authors": [
      {
        "surname": "Moorfield",
        "given_name": "James"
      },
      {
        "surname": "Wang",
        "given_name": "Song"
      },
      {
        "surname": "Yang",
        "given_name": "Wencheng"
      },
      {
        "surname": "Bedari",
        "given_name": "Aseel"
      },
      {
        "surname": "Kamp",
        "given_name": "Peter Van Der"
      }
    ]
  },
  {
    "title": "Deep-Person: Learning discriminative deep features for person Re-Identification",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107036",
    "abstract": "Person re-identification (Re-ID) requires discriminative features focusing on the full person to cope with inaccurate person bounding box detection, background clutter, and occlusion. Many recent person Re-ID methods attempt to learn such features describing full person details via part-based feature representation. However, the spatial context between these parts is ignored for the independent extractor on each separate part. In this paper, we propose to apply Long Short-Term Memory (LSTM) in an end-to-end way to model the pedestrian, seen as a sequence of body parts from head to foot. Integrating the contextual information strengthens the discriminative ability of local feature aligning better to full person. We also leverage the complementary information between local and global feature. Furthermore, we integrate both identification task and ranking task in one network, where a discriminative embedding and a similarity measurement are learned concurrently. This results in a novel three-branch framework named Deep-Person, which learns highly discriminative features for person Re-ID. Experimental results demonstrate that Deep-Person outperforms the state-of-the-art methods by a large margin on three challenging datasets including Market-1501, CUHK03, and DukeMTMC-reID.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303395",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature learning",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Bai",
        "given_name": "Xiang"
      },
      {
        "surname": "Yang",
        "given_name": "Mingkun"
      },
      {
        "surname": "Huang",
        "given_name": "Tengteng"
      },
      {
        "surname": "Dou",
        "given_name": "Zhiyong"
      },
      {
        "surname": "Yu",
        "given_name": "Rui"
      },
      {
        "surname": "Xu",
        "given_name": "Yongchao"
      }
    ]
  },
  {
    "title": "Fast large scale deep face search",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.01.012",
    "abstract": "Towards the large scale face search problem, this paper proposes a fast deep face search method which is realized by combination of deep convolution neural network (CNN), semantic hashing, and hash-based similarity search. First of all, to boost the performance in accuracy of face search, the residual network (Resnet) is exploited to construct a deep face feature model and then train it over the cleaned MS-Celeb-1M, which is used to extract real-valued face feature. Next, by imposing PCA and binarization operations, we convert the real-valued feature into compact hash code used for speeding up the face search. Based on the extracted dual features, the face search can be efficiently performed by adopting two-stage matching (i.e., coarse matching and fine matching) strategy. The coarse matching is implemented under the support of efficient hash indexing technique for yielding a small number of candidates while the fine stage is to filter out the unrelated images by cosine distance comparison of real-valued features. It is worth noting that we offer two coarse matching methods, such as GPU-hash and M-index-hash based matching, which are suitable for tens-of million and billion scale scenarios respectively. The experimental results demonstrate that the proposed method is very effective for large scale face search in both aspects of accuracy and real time property.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519300091",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Convolutional neural network",
      "Cosine similarity",
      "Face (sociological concept)",
      "Feature (linguistics)",
      "Hash function",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Search engine indexing",
      "Social science",
      "Sociology",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Zou",
        "given_name": "Fuhao"
      },
      {
        "surname": "Yang",
        "given_name": "Fan"
      },
      {
        "surname": "Chen",
        "given_name": "Wei"
      },
      {
        "surname": "Li",
        "given_name": "Kai"
      },
      {
        "surname": "Song",
        "given_name": "Jingkuan"
      },
      {
        "surname": "Chen",
        "given_name": "Jingcai"
      },
      {
        "surname": "Ling",
        "given_name": "Hefei"
      }
    ]
  },
  {
    "title": "3D Convolutional Neural Network based on memristor for video recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.12.005",
    "abstract": "Memristors have emerged as a potential tool to implement the training and operation of an integrated neural network, because of its current-voltage curve of the hysteresis loop and unique pulse regulation resistance method. However, most of the existing neural networks implemented on memristors are relatively basic architecture, and the processing functions are limited to the recognition of the simple signal and image models. In this paper, we propose a 3D Convolutional Neural Network based on memristor to recognize and classify the behaviors of human in the video with 6 main actions. As an extension of 2D Convolutional Neural Networks, 3D Convolutional Neural Networks have attracted attention for video information processing, since it introduces the time dimension innovatively on the basis of spatial dimensions to capture the contextual information between the different frames in the video. Accordingly, we use the 3D Convolution to construct our proposed neural network based on memristors. Besides, we use the basic 3 × 3 memristor arrays to construct the larger functional memristor arrays and form the 3D convolutional layers of our network by considering that the 3 × 3 basic memristor array has excellent flexibility and anti-jamming capability. With this strategy, we can make full use of the hardware structure to improve accuracy while reducing hardware noise. Finally, we implemented network obtain more than 70% accuracy on the Weizmann video dataset. This demonstration is an important step that memristors can implement the much larger and more complex neural networks for processing the more complex applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518309164",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Electrical engineering",
      "Electronic engineering",
      "Engineering",
      "Memistor",
      "Memristor",
      "Pattern recognition (psychology)",
      "Resistive random-access memory",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Jiaqi"
      },
      {
        "surname": "Li",
        "given_name": "Zhenghao"
      },
      {
        "surname": "Tang",
        "given_name": "Yongliang"
      },
      {
        "surname": "Hu",
        "given_name": "Wei"
      },
      {
        "surname": "Wu",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Sequence in sequence for video captioning",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.07.024",
    "abstract": "For video captioning, the words in the caption are closely related to an overall understanding of the video. Thus, a suitable representation for the video is rather important for the description. For more precise words in the task of video captioning, we aim to encode the video feature for current word at each time-stamp of the generation process. This paper proposes a new framework of ‘Sequence in Sequence’ to encode the sequential frames into a spatio-temporal representation at each time-stamp to utter a word and further distill most related visual content by an extra semantic loss. First, we aggregate the sequential frames to extract related visual content guided by last word, and get a representation with rich spatio-temporal information. Then, to decode the aggregated representation for a precise word, we leverage two layers of GRU structure, where the first layer further distills useful visual content based on an extra semantic loss and the second layer selects the correct word according to the distilled features. Experiments on two benchmark datasets demonstrate that our method outperforms the current state-of-the-art methods on Bleu@4, METEOR and CIDEr metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518303234",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Closed captioning",
      "Computer science",
      "ENCODE",
      "Feature (linguistics)",
      "Gene",
      "Image (mathematics)",
      "Information retrieval",
      "Law",
      "Leverage (statistics)",
      "Linguistics",
      "Natural language processing",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Speech recognition",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Huiyun"
      },
      {
        "surname": "Gao",
        "given_name": "Chongyang"
      },
      {
        "surname": "Han",
        "given_name": "Yahong"
      }
    ]
  },
  {
    "title": "Subgraph learning for graph matching",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.07.005",
    "abstract": "Graph matching is a powerful tool for computer vision, distance measure and machine learning. However, many factors influences the accuracy of matching. The outliers is a key problem in the process of matching. In this paper, a novel approach is proposed to handle graph matching problem based on Markov Chain Monte Carlo framework. By constructing a target distribution, the proposed can perform a process of sampling to maximize the graph matching objective. In this process, our method can effectively save matching pairwise under one-to-one matching constraints and also avoid the effect of outliers and deformation. The corresponding experiments on synthetic graphs, real images and view-based 3D model retrieval demonstrate the superiority of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518302897",
    "keywords": [
      "3-dimensional matching",
      "Algorithm",
      "Artificial intelligence",
      "Bayesian probability",
      "Bipartite graph",
      "Computer science",
      "Factor-critical graph",
      "Graph",
      "Line graph",
      "Machine learning",
      "Markov chain",
      "Markov chain Monte Carlo",
      "Matching (statistics)",
      "Mathematics",
      "Operating system",
      "Outlier",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Statistics",
      "Theoretical computer science",
      "Voltage graph"
    ],
    "authors": [
      {
        "surname": "Nie",
        "given_name": "Weizhi"
      },
      {
        "surname": "Ding",
        "given_name": "Hai"
      },
      {
        "surname": "Liu",
        "given_name": "Anan"
      },
      {
        "surname": "Deng",
        "given_name": "Zonghui"
      },
      {
        "surname": "Su",
        "given_name": "Yuting"
      }
    ]
  },
  {
    "title": "Weighted constraint based dictionary learning for image classification",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.09.008",
    "abstract": "Dictionary learning (DL) is a popular approach of image classification. Most DL methods ignore the information hidden in training samples or atoms, and thus cannot enhance the discrimination performance of a dictionary learning algorithm effectively. In addition, the training samples are prone to a wide range of variances such as sample noise and illumination change, which results in the degraded classification performance. Hence, in this paper, we propose a weighted constraint based dictionary learning algorithm to improve the classification performance of dictionary learning. More specifically, the proposed algorithm uses a diagonal weighted matrix to construct a constraint item for reducing the auto-correlation between atoms. Meanwhile, the training samples of the same class enjoy similar coding coefficients such that the reconfiguration and discrimination performance of dictionary is enhanced. Furthermore, in order to avoid over-fitting, we convert a strict two valued label matrix into a flexible matrix in the classification procedure allowing more degrees of freedom to fit the class labels. Experimental results show that the proposed algorithm outperforms massive state-of-the-art dictionary learning and sparse representation algorithms in image classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518305713",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Constraint (computer-aided design)",
      "Contextual image classification",
      "Diagonal",
      "Geometry",
      "Image (mathematics)",
      "K-SVD",
      "Machine learning",
      "Mathematics",
      "Neural coding",
      "Pattern recognition (psychology)",
      "Sparse approximation"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Yali"
      },
      {
        "surname": "Li",
        "given_name": "Lingjun"
      },
      {
        "surname": "Liu",
        "given_name": "Shigang"
      },
      {
        "surname": "Wang",
        "given_name": "Xili"
      },
      {
        "surname": "Li",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Accumulated and aggregated shifting of intensity for defect detection on micro 3D textured surfaces",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107057",
    "abstract": "Micro three-dimensional (3D) textured surfaces are being designed for a lot of electronic products to improve appearance and user experience. Defects are, however, inevitably caused during industrial manufacture. They are difficult to be detected due to low contrast and unclear boundary between defect and irregular textured defect-free region. To achieve robust defect detection on micro 3D textured surfaces of industrial products, this paper proposes a probabilistic saliency framework with a novel feature enhancement mechanism. Two saliency features, absolute intensity deviation and local intensity aggregation, are designed to represent the pixel-level initial saliency. Based on these two features, an iterative framework, named accumulated and aggregated shifting of intensity (AASI), is proposed to shift the intensity of each pixel according to its saliency. Finally, all the pixels are classified as defective or defect-free by fitting the AASI iteration results to two statistical models, an exponential model and a linear model. Importantly, AASI procedure is unsupervised and training-free, so it does not rely on huge training data with time-consuming manual labels. Experimental results on a large-scale image dataset taken from real-world industrial product surfaces demonstrate that the proposed approach achieves state-of-the-art accuracy in industrial applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303590",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Intensity (physics)",
      "Linguistics",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pixel",
      "Probabilistic logic"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Yaping"
      },
      {
        "surname": "Kaneko",
        "given_name": "Shun’ichi"
      },
      {
        "surname": "Asano",
        "given_name": "Hirokazu"
      }
    ]
  },
  {
    "title": "FC-RCCN: Fully convolutional residual continuous CRF network for semantic segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.08.030",
    "abstract": "Enlarging the spatial resolution of features generated by fully convolutional networks (FCNs) can improve the performance of semantic segmentation. To achieve this goal, deeper network with deconvolutional structure can be applied. However, when the network architecture becomes more complex, the training efficiency may degrade. To address the joint optimization problem of improving spatial resolution through deeper networks and training deeper networks more effectively, we propose a Fully Convolutional Residual Continuous CRF Network (FC-RCCN) for semantic segmentation. FC-RCCN is composed of three subnetworks: a unary network, a pairwise network, and a superpixel based continuous conditional random filed (C-CRF) network. In order to generate full spatial resolution predictions with high-quality, a residual block based unary network with multi-scale features fusion is proposed. Even though the unary network is a deeper network, the whole framework can be trained effectively in an end-to-end way using the joint pixel-level and superpixel-level supervised learning strategy which is optimized by a pixel-level softmax cross entropy loss and a superpixel-level log-likelihood loss. Besides, C-CRF inference is fused with pixel-level prediction during the test procedure, which guarantees the method’s robustness to the superpxiel errors. In the experiments, we evaluatee the power of the three subnetworks and the learning strategy comprehensively. Experiments on three benchmark datasets demonstrate that the proposed FC-RCCN outperforms previous segmentation methods and obtains the state-of-the-art performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518304999",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Computer security",
      "Conditional random field",
      "Convolutional neural network",
      "Gene",
      "Inference",
      "Mathematics",
      "Network architecture",
      "Pattern recognition (psychology)",
      "Residual",
      "Robustness (evolution)",
      "Segmentation",
      "Softmax function",
      "Unary operation"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Lei"
      },
      {
        "surname": "Kong",
        "given_name": "Xiangyong"
      },
      {
        "surname": "Gong",
        "given_name": "Chen"
      },
      {
        "surname": "Zhang",
        "given_name": "Fan"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaoguo"
      }
    ]
  },
  {
    "title": "Software expert discovery via knowledge domain embeddings in a collaborative network",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.10.030",
    "abstract": "Community Question Answering (CQA) websites can be claimed as the most major venues for knowledge sharing, and the most effective way of exchanging knowledge at present. Considering that massive amount of users are participating online and generating huge amount data, management of knowledge here systematically can be challenging. Expert recommendation is one of the major challenges, as it highlights users in CQA with potential expertise, which may help match unresolved questions with existing high quality answers while at the same time may help external services like human resource systems as another reference to evaluate their candidates. In this paper, we in this work we propose to exploring experts in CQA websites. We take advantage of recent distributed word representation technology to help summarize text chunks, and in a semantic view exploiting the relationships between natural language phrases to extract latent knowledge domains. By domains, the users’ expertise is determined on their historical performance, and a rank can be compute to given recommendation accordingly. In particular, Stack Overflow is chosen as our dataset to test and evaluate our work, where inclusive experiment shows our competence.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518308596",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Data science",
      "Domain (mathematical analysis)",
      "Domain knowledge",
      "Information retrieval",
      "Knowledge extraction",
      "Knowledge management",
      "Mathematical analysis",
      "Mathematics",
      "Question answering",
      "Rank (graph theory)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Chaoran"
      },
      {
        "surname": "Yao",
        "given_name": "Lina"
      },
      {
        "surname": "Wang",
        "given_name": "Xianzhi"
      },
      {
        "surname": "Benatallah",
        "given_name": "Boualem"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiang"
      }
    ]
  },
  {
    "title": "Underwater scene prior inspired deep underwater image and video enhancement",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107038",
    "abstract": "In underwater scenes, wavelength-dependent light absorption and scattering degrade the visibility of images and videos. The degraded underwater images and videos affect the accuracy of pattern recognition, visual understanding, and key feature extraction in underwater scenes. In this paper, we propose an underwater image enhancement convolutional neural network (CNN) model based on underwater scene prior, called UWCNN. Instead of estimating the parameters of underwater imaging model, the proposed UWCNN model directly reconstructs the clear latent underwater image, which benefits from the underwater scene prior which can be used to synthesize underwater image training data. Besides, based on the light-weight network structure and effective training data, our UWCNN model can be easily extended to underwater videos for frame-by-frame enhancement. Specifically, combining an underwater imaging physical model with optical properties of underwater scenes, we first synthesize underwater image degradation datasets which cover a diverse set of water types and degradation levels. Then, a light-weight CNN model is designed for enhancing each underwater scene type, which is trained by the corresponding training data. At last, this UWCNN model is directly extended to underwater video enhancement. Experiments on real-world and synthetic underwater images and videos demonstrate that our method generalizes well to different underwater scenes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303401",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Geology",
      "Linguistics",
      "Oceanography",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Underwater",
      "Visibility"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Chongyi"
      },
      {
        "surname": "Anwar",
        "given_name": "Saeed"
      },
      {
        "surname": "Porikli",
        "given_name": "Fatih"
      }
    ]
  },
  {
    "title": "Parallel connected-Component-Labeling based on homotopy trees",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.039",
    "abstract": "Taking advantage of the topological and isotopic properties of binary digital images, we present here a new algorithm for connected component labeling (CLL). A local-to-global treatment of the topological information within the image, allows us to develop an inherent parallel approach. The time complexity order for an image of m × n pixels, under the assumption that a processing element exists for each pixel, is near O ( l o g ( m + n ) ) . Additionally, our method computes both the foreground and background CCL, and allows a straightforward computation of topological features like Adjacency Trees. Experiments show that our method obtains better performance metrics than other approaches. Our work aims at generating a new class of labeling algorithms: those centered in fully parallel approaches based on computational topology, thus allowing a perfect concurrent execution in multiple threads and preventing the use of critical sections and atomic instructions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303599",
    "keywords": [
      "Adjacency list",
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Binary image",
      "Binary number",
      "Combinatorics",
      "Component (thermodynamics)",
      "Computation",
      "Computer science",
      "Connected component",
      "Connected-component labeling",
      "Digital image",
      "Digital topology",
      "Discrete mathematics",
      "Extension topology",
      "General topology",
      "Homotopy",
      "Image (mathematics)",
      "Image processing",
      "Image segmentation",
      "Mathematics",
      "Physics",
      "Pixel",
      "Pure mathematics",
      "Scale-space segmentation",
      "Theoretical computer science",
      "Thermodynamics",
      "Topological space",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Diaz-del-Rio",
        "given_name": "Fernando"
      },
      {
        "surname": "Sanchez-Cuevas",
        "given_name": "Pablo"
      },
      {
        "surname": "Molina-Abril",
        "given_name": "Helena"
      },
      {
        "surname": "Real",
        "given_name": "Pedro"
      }
    ]
  },
  {
    "title": "Tooth morphometry using quasi-conformal theory",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107064",
    "abstract": "Shape analysis is important in anthropology, bioarchaeology and forensic science for interpreting useful information from human remains. In particular, teeth are morphologically stable and hence well-suited for shape analysis. In this work, we propose a framework for tooth morphometry using quasi-conformal theory. Landmark-matching Teichmüller maps are used for establishing a 1-1 correspondence between tooth surfaces with prescribed anatomical landmarks. Then, a quasi-conformal statistical shape analysis model based on the Teichmüller mapping results is proposed for building a tooth classification scheme. We deploy our framework on a dataset of human premolars to analyze the tooth shape variation among genders and ancestries. Experimental results show that our method achieves much higher classification accuracy with respect to both gender and ancestry when compared to the existing methods. Furthermore, our model reveals the underlying tooth shape difference between different genders and ancestries in terms of the local geometric distortion and curvatures. In particular, our experiment suggests that the shape difference between genders is mostly captured by the conformal distortion but not the curvatures, while that between ancestries is captured by both of them.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303668",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Computer network",
      "Computer science",
      "Conformal map",
      "Distortion (music)",
      "Geometry",
      "Landmark",
      "Matching (statistics)",
      "Mathematics",
      "Programming language",
      "Shape analysis (program analysis)",
      "Static analysis",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Choi",
        "given_name": "Gary P.T."
      },
      {
        "surname": "Chan",
        "given_name": "Hei Long"
      },
      {
        "surname": "Yong",
        "given_name": "Robin"
      },
      {
        "surname": "Ranjitkar",
        "given_name": "Sarbin"
      },
      {
        "surname": "Brook",
        "given_name": "Alan"
      },
      {
        "surname": "Townsend",
        "given_name": "Grant"
      },
      {
        "surname": "Chen",
        "given_name": "Ke"
      },
      {
        "surname": "Lui",
        "given_name": "Lok Ming"
      }
    ]
  },
  {
    "title": "Deterministic dropout for deep neural networks using composite random forest",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.12.023",
    "abstract": "Dropout prevents overfitting in deep neural networks. Typical strategy of dropout involves random termination of connections irrespective of their importance. Termination blocks the propagation of class discriminative information across the network. As a result, dropout may lead to inferior performance. We propose a deterministic dropout where only unimportant connections are dropped ensuring propagation of class discriminative information. We identify the unimportant connections using a novel composite random forest, integrated into the network. We prove that better generalization is achieved by terminating these unimportant connections. The proposed algorithm is useful in preventing overfitting in noisy datasets. The proposal is equally good for datasets with smaller number of training examples. Experiments on several benchmark datasets show up to 8% improvement in classification accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303988",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Backpropagation",
      "Benchmark (surveying)",
      "Class (philosophy)",
      "Computer science",
      "Discriminative model",
      "Dropout (neural networks)",
      "Generalization",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Random forest"
    ],
    "authors": [
      {
        "surname": "Santra",
        "given_name": "Bikash"
      },
      {
        "surname": "Paul",
        "given_name": "Angshuman"
      },
      {
        "surname": "Mukherjee",
        "given_name": "Dipti Prasad"
      }
    ]
  },
  {
    "title": "Facial expression recognition based on deep convolution long short-term memory networks of double-channel weighted mixture",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.12.013",
    "abstract": "With the aging population and the increasing number of empty nest elderly, more and more researches focus on home service robots. Autonomous analysis of human emotions by robots is helpful to provide better services for human beings. Facial expression, as an important modality in emotional recognition, is helpful to improve emotional recognition. In order to explore a new method that can effectively improve the recognition rate of expression two facial expression recognition(FER) methods are proposed in this paper. They are double-channel weighted mixture deep convolution neural networks (WMDCNN) based on static images and deep cnn long short-term memory networks of double-channel weighted mixture(WMCNN-LSTM) based on image sequences. WMDCNN network can quickly recognize facial expressions and provide static image features for WMCNN-LSTM network. WMCNN-LSTM network utilizes the static image features to further acquire the temporal features of image sequence, which can realize the accurate recognition of facial expressions. The experimental results show that the average recognition rate of the WMDCNN network on the four datasets of CK+, JAFFE, Oulu-CASIA and MMI are 0.985, 0.923,0.86,0.78 respectively. The WMCNN-LSTM method has an average recognition rate of 0.975, 0.88, and 0.87 on the three datasets CK+, Oulu-CASIA and MMI respectively. By comparing with the existing FER method, our method further improves the recognition rate in the above four expression data sets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303769",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Deep learning",
      "Expression (computer science)",
      "Facial expression",
      "Facial expression recognition",
      "Facial recognition system",
      "Image (mathematics)",
      "Pattern recognition (psychology)",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Hepeng"
      },
      {
        "surname": "Huang",
        "given_name": "Bin"
      },
      {
        "surname": "Tian",
        "given_name": "Guohui"
      }
    ]
  },
  {
    "title": "An agent-based approach for recommending cultural tours",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.01.005",
    "abstract": "In the Cultural Heritage domain, the introduction of intelligent systems for planning of cultural visit was very appealing. Indeed when people decide to visit museums or archaeological sites, they usually would like to organize and schedule their time in order to fulfill some requirements and to match, as far as possible, their preferences and needs. In this work, we propose a novel methodology integrating recommendation facilities with agent-based planning techniques in order to implement a planner of routes within cultural sites as museums. In particular, the introduced methodology exploits from one hand a user-centered recommendation strategy to suggest the most suitable cultural items with respect to user’s preferences and, from the other one, it leverages multi-agents planning methods to generate the related routes consisting of the sequence of steps necessary to reach precise cultural goals depending on the context, by means of a state space exploration. We present a case of study for the proposed methodology and we describe some experimental results on system efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300027",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Context (archaeology)",
      "Cultural heritage",
      "Domain (mathematical analysis)",
      "Economics",
      "Engineering",
      "Exploit",
      "Finance",
      "Geography",
      "Human–computer interaction",
      "Knowledge management",
      "Mathematical analysis",
      "Mathematics",
      "Mechanical engineering",
      "Operating system",
      "Operations research",
      "Order (exchange)",
      "Planner",
      "Recommender system",
      "Schedule",
      "Space (punctuation)",
      "Work (physics)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Amato",
        "given_name": "Flora"
      },
      {
        "surname": "Moscato",
        "given_name": "Francesco"
      },
      {
        "surname": "Moscato",
        "given_name": "Vincenzo"
      },
      {
        "surname": "Pascale",
        "given_name": "Francesco"
      },
      {
        "surname": "Picariello",
        "given_name": "Antonio"
      }
    ]
  },
  {
    "title": "Learning disentangling and fusing networks for face completion under structured occlusions",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107073",
    "abstract": "Face completion aims to generate semantically new pixels for missing facial components. It is a challenging generative task due to large variations of face appearance. This paper studies generative face completion under structured occlusions. We treat the face completion and corruption as disentangling and fusing processes of clean faces and occlusions, and propose a jointly disentangling and fusing Generative Adversarial Network (DF-GAN). First, three domains are constructed, corresponding to the distributions of occluded faces, clean faces and structured occlusions. The disentangling and fusing processes are formulated as the transformations between the three domains. Then the disentangling and fusing networks are built to learn the transformations from unpaired data, where the encoder-decoder structure is adopted and allows DF-GAN to simulate structure occlusions by modifying the latent representations. Finally, the disentangling and fusing processes are unified into a dual learning framework along with an adversarial strategy. The proposed method is evaluated on Meshface verification problem. Experimental results on four Meshface databases demonstrate the effectiveness of our proposed method for the face completion under structured occlusions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303747",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Face (sociological concept)",
      "Facial recognition system",
      "Pattern recognition (psychology)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Zhihang"
      },
      {
        "surname": "Hu",
        "given_name": "Yibo"
      },
      {
        "surname": "He",
        "given_name": "Ran"
      },
      {
        "surname": "Sun",
        "given_name": "Zhenan"
      }
    ]
  },
  {
    "title": "Attention guided neural network models for occluded pedestrian detection",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.12.010",
    "abstract": "Occluded pedestrian detection has been a research difficulty in computer vision for a long time. The conventional approach to solve this problem is to learn partial detectors, which can be properly integrated for occlusion handling. However, the efficiency of this type of methods is limited in practical applications since the partial detectors can not cover all occlusion patterns. In this paper, an attention guided neural network model (AGNN) is proposed for the occlusion handling of pedestrian detections, which is inspired by the approaches of sentiment classification. Firstly, a fixed-size window slides on a still image without overlapping to generate a set of sub-images. Secondly, a convolutional neural network is employed to extract the high-level features from resulting sub-images. Then, the attention network performs local feature weighting, from which the features representing the body parts of pedestrians are selected. Finally, the feature sequences are classified by recurrent neural network in proper order based on the weighted results. In addition, we explore different mechanisms of attention guidance on the detector for the detections. Compared with the state-of-the-art methods on two standard pedestrian datasets, experimental results demonstrate the comparable performance of our approach in terms of miss rates.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303733",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Detector",
      "Engineering",
      "Feature (linguistics)",
      "Linguistics",
      "Medicine",
      "Pattern recognition (psychology)",
      "Pedestrian",
      "Pedestrian detection",
      "Philosophy",
      "Programming language",
      "Radiology",
      "Set (abstract data type)",
      "Telecommunications",
      "Transport engineering",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Zou",
        "given_name": "Tengtao"
      },
      {
        "surname": "Yang",
        "given_name": "Shangming"
      },
      {
        "surname": "Zhang",
        "given_name": "Yun"
      },
      {
        "surname": "Ye",
        "given_name": "Mao"
      }
    ]
  },
  {
    "title": "Single-image raindrop removal using concurrent channel-spatial attention and long-short skip connections",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.12.012",
    "abstract": "Image raindrop removal refers to removing raindrops from the images taken through glass under rainy weather. It is a challenging problem due to the large variations of raindrop appearances as well as the nonlinear visual distortions caused by raindrops. This problem becomes much more difficult when only a single image is provided. This paper aims at tackling the single-image raindrop removal problem by leveraging the power of deep learning. We proposed an end-to-end approach using the deep neural network. To address the challenges of raindrop removal, we introduce a concurrent channel and spatial attention mechanism implemented by Squeeze & Excitation into the network. The channel attention mechanism allows individually selecting useful features in the network for handling the raindrops of different appearances and restoring different image patterns. The spatial attention enables different treatments to the image regions with different degrees of distortion and with different local image patterns. In addition, long-short skip connections are added for utilizing the intermediate features of the network for better restoration. The experimental results show that the proposed approach outperformed the existing ones.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303757",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Artificial neural network",
      "Bandwidth (computing)",
      "Channel (broadcasting)",
      "Computer science",
      "Computer vision",
      "Distortion (music)",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Jiayi"
      },
      {
        "surname": "Xu",
        "given_name": "Yong"
      },
      {
        "surname": "Chen",
        "given_name": "Tianyi"
      },
      {
        "surname": "Huang",
        "given_name": "Yan"
      }
    ]
  },
  {
    "title": "Estimation of Gaussian mixture models via tensor moments with application to online learning",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.01.001",
    "abstract": "In this paper, we present an alternating gradient descent algorithm for estimating parameters of a spherical Gaussian mixture model by the method of moments (AGD-MoM). We formulate the problem as a constrained optimisation problem which simultaneously matches the third order moments from the data, represented as a tensor, and the second order moment, which is the empirical covariance matrix. We derive the necessary gradients (and second derivatives), and use them to implement alternating gradient search to estimate the parameters of the model. We show that the proposed method is applicable in both a batch as well as in a streaming (online) setting. Using synthetic and benchmark datasets, we demonstrate empirically that the proposed algorithm outperforms the more classical algorithms like Expectation Maximisation and variational Bayes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518305348",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Bayes' theorem",
      "Bayesian probability",
      "Benchmark (surveying)",
      "Classical mechanics",
      "Computer science",
      "Covariance",
      "Covariance matrix",
      "Estimator",
      "Gaussian",
      "Geodesy",
      "Geography",
      "Gradient descent",
      "Mathematical optimization",
      "Mathematics",
      "Method of moments (probability theory)",
      "Moment (physics)",
      "Physics",
      "Pure mathematics",
      "Quantum mechanics",
      "Statistics",
      "Tensor (intrinsic definition)"
    ],
    "authors": [
      {
        "surname": "Rahmani",
        "given_name": "Donya"
      },
      {
        "surname": "Niranjan",
        "given_name": "Mahesan"
      },
      {
        "surname": "Fay",
        "given_name": "Damien"
      },
      {
        "surname": "Takeda",
        "given_name": "Akiko"
      },
      {
        "surname": "Brodzki",
        "given_name": "Jacek"
      }
    ]
  },
  {
    "title": "Speckle noise suppression in 2D ultrasound kidney images using local pattern based topological derivative",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.12.005",
    "abstract": "One of the affordable and least harmful modalities used to efficiently detect and diagnose the kidney diseases is the Ultrasound scans. The main drawback of the ultrasound images is the presence of speckle noise that reduces the efficiency of image processing and hinders the interpretation. This paper proposes a novel technique named Local Binary Pattern based Discrete Topological Derivative and its variants to address speckle noise reduction problem in 2D ultrasound kidney images. In typical Discrete Topological Derivative, the execution time is higher and as a result an optimizer is incorporated based on local pattern and gradient tolerance value resulting in 20 times reduction in execution time with improved results. The experimentation is carried out on 100 clinical 2D ultrasound images and moreover, proposed methods are compared with the competing Discrete Topological Derivative and some commonly used speckle noise removal filters. The results are promising and thereby confirms that the proposed Local Binary Pattern based Discrete Topological Derivative can produce a better speckle noise reduction that enables the doctors to detect and diagnose kidney diseases in 2D ultrasound images with lesser strain.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786551930368X",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Medicine",
      "Noise (video)",
      "Noise reduction",
      "Pattern recognition (psychology)",
      "Radiology",
      "Reduction (mathematics)",
      "Speckle noise",
      "Speckle pattern",
      "Topology (electrical circuits)",
      "Ultrasound"
    ],
    "authors": [
      {
        "surname": "Deepthy Mary Alex",
        "given_name": ""
      },
      {
        "surname": "Hepzibah Christinal",
        "given_name": "A."
      },
      {
        "surname": "Abraham Chandy",
        "given_name": "D."
      },
      {
        "surname": "Singh",
        "given_name": "Arvinder"
      },
      {
        "surname": "Pushkaran",
        "given_name": "M"
      }
    ]
  },
  {
    "title": "Efficient hierarchical graph partitioning for image segmentation by optimum oriented cuts",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.01.008",
    "abstract": "In this work, a hierarchical graph partitioning based on optimum cuts in graphs is proposed for unsupervised image segmentation, that can be tailored to the target group of objects, according to their boundary polarity, by extending Oriented Image Foresting Transform (OIFT). The proposed method, named UOIFT, theoretically encompasses as a particular case the single-linkage algorithm by minimum spanning tree (MST) and gives superior segmentation results compared to other approaches commonly used in the literature, usually requiring a lower number of image partitions to accurately isolate the desired regions of interest with known polarity. The method is supported by new theoretical results involving the usage of non-monotonic-incremental cost functions in directed graphs and exploits the local contrast of image regions, being robust in relation to illumination variations and inhomogeneity effects. UOIFT is demonstrated using a region adjacency graph of superpixels in medical and natural images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300088",
    "keywords": [
      "Adjacency list",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Graph",
      "Graph partition",
      "Image (mathematics)",
      "Image segmentation",
      "Mathematical analysis",
      "Mathematics",
      "Minimum spanning tree-based segmentation",
      "Monotonic function",
      "Pattern recognition (psychology)",
      "Scale-space segmentation",
      "Segmentation",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Bejar",
        "given_name": "Hans H.C."
      },
      {
        "surname": "Ferzoli Guimaraes",
        "given_name": "Silvio Jamil"
      },
      {
        "surname": "Miranda",
        "given_name": "Paulo A.V."
      }
    ]
  },
  {
    "title": "Improving smart interactive experiences in cultural heritage through pattern recognition techniques",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.12.011",
    "abstract": "New Information and Communication Technologies have a large potential to improve general public awareness of the importance of Cultural Heritage (CH) and to provide tools that can make visits to historical sites more interesting and enjoyable. The Internet of Things (IoT) technology can further contribute to these goals, by allowing visitors to museum and CH sites to manipulate smart objects by receiving information that stimulates emotions, understanding and appropriation of the contents. In our research, interaction paradigms and innovative methods are developed to allow curators and guides of cultural sites (i.e., domain experts) to manage interactive IoT-based environments, in order to create Smart Interactive Experiences, which are usage situations created by synchronizing many available smart objects to specific situations that might better satisfy the needs of the visitors. This article illustrates a system that, by means of a tangible user interface, integrated by pattern recognition and computer vision techniques, supports CH experts in creating Smart Interactive Experiences by properly tailoring the behavior of the involved smart objects. An experimental evaluation of the used techniques has been performed and it is presented and discussed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303745",
    "keywords": [
      "Appropriation",
      "Archaeology",
      "Bubble",
      "Computer science",
      "Cultural heritage",
      "Data science",
      "Domain (mathematical analysis)",
      "History",
      "Human–computer interaction",
      "Interaction design",
      "Interactivity",
      "Interface (matter)",
      "Internet of Things",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Maximum bubble pressure method",
      "Multimedia",
      "Parallel computing",
      "Philosophy",
      "Smart environment",
      "Smart objects",
      "Synchronizing",
      "Telecommunications",
      "Transmission (telecommunications)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Balducci",
        "given_name": "Fabrizio"
      },
      {
        "surname": "Buono",
        "given_name": "Paolo"
      },
      {
        "surname": "Desolda",
        "given_name": "Giuseppe"
      },
      {
        "surname": "Impedovo",
        "given_name": "Donato"
      },
      {
        "surname": "Piccinno",
        "given_name": "Antonio"
      }
    ]
  },
  {
    "title": "Using keyword spotting systems as tools for the transcription of historical handwritten documents: Models and procedures for performance evaluation",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.01.007",
    "abstract": "The paper proposes a performance model for estimating the improvement of the time needed to transcribe small collections of handwritten documents by using a keyword spotting system (KWS) with respect to the time for manually achieving the transcription. The proposed model assumes that no other information than those obtained from the samples and the KWS system performance on the training set are available, and, depending on them, establishes analytically the condition the performance measures must satisfy to make it profitable to use the system, and, in the affirmative case, estimates the gain and the accuracy of such estimation. The model is complemented by a step-by-step procedure for building the training set, running the KWS on it, estimating the performance parameters on the data set, and eventually estimating the overall improvement and the accuracy of this estimate.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300040",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Economics",
      "Keyword spotting",
      "Linguistics",
      "Machine learning",
      "Operations management",
      "Performance improvement",
      "Philosophy",
      "Programming language",
      "Set (abstract data type)",
      "Spotting",
      "Training set",
      "Transcription (linguistics)"
    ],
    "authors": [
      {
        "surname": "Santoro",
        "given_name": "Adolfo"
      },
      {
        "surname": "Marcelli",
        "given_name": "Angelo"
      }
    ]
  },
  {
    "title": "Brain tumor segmentation and classification from magnetic resonance images: Review of selected methods from 2014 to 2019",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.020",
    "abstract": "The past few years have witnessed a significant increase in medical cases related to brain tumors, making it the 10th most common form of tumor affecting children and adults alike. However, it is also one of the most curable forms of tumors if detected well on time. Consequently scientists and researchers have been working towards developing sophisticated techniques and methods for identifying the form and stage of tumor. Magnetic Resonance Imaging (MRI) and Computer Tomography (CT) are two methods widely used for resectioning and examining the abnormalities in terms of shape, size or location of brain tissues which in turn help in detecting the tumors. MRI, due to its advantages over CT scan, discussed later in the paper, is preferred more by the doctors. The way towards sectioning tumor from MRI picture of a brain cerebrum is one of the profoundly engaged regions in the network of medical science as MRI is non-invasive imaging. This paper provides a systematic literature survey of techniques for brain tumor segmentation and classification of abnormality and normality from MRI images based on different methods including deep learning techniques, metaheuristic techniques and hybridization of these two. It includes presentation and quantitative investigation used in conventional segmentation and classification techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786551930340X",
    "keywords": [
      "Abnormality",
      "Artificial intelligence",
      "Brain morphometry",
      "Brain tumor",
      "Computer science",
      "Magnetic resonance imaging",
      "Medical physics",
      "Medicine",
      "Neuroimaging",
      "Neuroscience",
      "Normality",
      "Pathology",
      "Pattern recognition (psychology)",
      "Psychiatry",
      "Psychology",
      "Radiology",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Tiwari",
        "given_name": "Arti"
      },
      {
        "surname": "Srivastava",
        "given_name": "Shilpa"
      },
      {
        "surname": "Pant",
        "given_name": "Millie"
      }
    ]
  },
  {
    "title": "Long short term memory based patient-dependent model for FOG detection in Parkinson's disease",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.036",
    "abstract": "Deep learning has a great impact on healthcare for discovering hidden patterns in the clinical data to detect or predict the different diseases. This work proposed a monitoring procedure for Parkinson's disease (PD) using a recorded signals’ dataset from multiple wearable on body sensors placed at different positions on the leg, namely on knee, hip and ankle. Different symptoms of PD patients can be detected from the acceleration signals, where the Freezing of Gait (FOG) is considered the main sign. Typically, FOG is patient-dependent that varies in severity and incidence from patient to another. In this work, a deep learning model, namely the Long Short Term Memory (LSTM) network-based patient-dependent model was adopted for FOG detection. A comparison between the proposed model and the traditional machine learning methods, including the linear support vector machine (SVM) was conducted using the signals of the three sensors. The results established the superiority of the LSTM model, which achieved 83.38% in terms of the average accuracy in comparison with the SVM which achieved 79.48%. For example, in patient 2, the maximum accuracy achieved using the LSTM is 98.89%, while the corresponding maximum accuracy is 80% using the linear SVM.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303563",
    "keywords": [
      "Acceleration",
      "Ankle",
      "Artificial intelligence",
      "Classical mechanics",
      "Computer science",
      "Deep learning",
      "Disease",
      "Embedded system",
      "Gait",
      "Machine learning",
      "Medicine",
      "Parkinson's disease",
      "Pathology",
      "Pattern recognition (psychology)",
      "Physical medicine and rehabilitation",
      "Physics",
      "Quantum mechanics",
      "Support vector machine",
      "Term (time)",
      "Wearable computer"
    ],
    "authors": [
      {
        "surname": "Ashour",
        "given_name": "Amira S."
      },
      {
        "surname": "El-Attar",
        "given_name": "Amira"
      },
      {
        "surname": "Dey",
        "given_name": "Nilanjan"
      },
      {
        "surname": "El-Kader",
        "given_name": "Hatem Abd"
      },
      {
        "surname": "Abd El-Naby",
        "given_name": "Mostafa M."
      }
    ]
  },
  {
    "title": "A bio-inspired quaternion local phase CNN layer with contrast invariance and linear sensitivity to rotation angles",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.12.001",
    "abstract": "Deep learning models have been particularly successful with image recognition using Convolutional Neural Networks (CNN). However, the learning of a contrast invariance and rotation equivariance response may fail even with very deep CNNs or by large data augmentations in training. We were inspired by the V1 visual features of the mammalian visual system to emulate as much as possible the early visual system and add more invariant capacities to the CNN. We present a new quaternion local phase convolutional neural network layer encoding three local phases. We present two experimental setups: An image classification task with four contrast levels, and a linear regression task that predicts the rotation angle of an image. In sum, we obtain new patterns and feature representations for deep learning, which capture illumination invariance and a linear response to rotation angles.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303642",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Convolutional neural network",
      "Deep learning",
      "Feature (linguistics)",
      "Geometry",
      "Invariant (physics)",
      "Linguistics",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Quaternion",
      "Rotation (mathematics)",
      "Rotational invariance"
    ],
    "authors": [
      {
        "surname": "Moya-Sánchez",
        "given_name": "E. Ulises"
      },
      {
        "surname": "Xambó-Descamps",
        "given_name": "Sebastiá"
      },
      {
        "surname": "Sánchez Pérez",
        "given_name": "Abraham"
      },
      {
        "surname": "Salazar-Colores",
        "given_name": "Sebastián"
      },
      {
        "surname": "Martínez-Ortega",
        "given_name": "Jorge"
      },
      {
        "surname": "Cortés",
        "given_name": "Ulises"
      }
    ]
  },
  {
    "title": "Deep skin detection on low resolution grayscale images",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.12.021",
    "abstract": "In this work we present a facial skin detection method, based on a deep learning architecture, that is able to precisely associate a skin label to each pixel of a given image depicting a face. This is an important preliminary step in many applications, such as remote photoplethysmography (rPPG) in which the hearth rate of a subject needs to be estimated analyzing a video of his/her face. The proposed method can detect skin pixels even in low resolution grayscale face images (64 × 32 pixel). A dataset is also described and proposed in order to train the deep learning model. Given the small amount of data available, a transfer learning approach is adopted and validated in order to learn to solve the skin detection problem exploiting a colorization network. Qualitative and quantitative results are reported testing the method on different datasets and in presence of general illumination, facial expressions, object occlusions and it is able to work regardless of the gender, age and ethnicity of the subject.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303964",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Face (sociological concept)",
      "Face detection",
      "Facial recognition system",
      "Grayscale",
      "Image resolution",
      "Object detection",
      "Pattern recognition (psychology)",
      "Pixel",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Paracchini",
        "given_name": "Marco"
      },
      {
        "surname": "Marcon",
        "given_name": "Marco"
      },
      {
        "surname": "Villa",
        "given_name": "Federica"
      },
      {
        "surname": "Tubaro",
        "given_name": "Stefano"
      }
    ]
  },
  {
    "title": "Large-scale multi-label classification using unknown streaming images",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107100",
    "abstract": "In this paper, we investigate the large-scale multi-label image classification problem when images with unknown novel classes come in stream during the training stage. It coincides with the practical requirement that usually novel classes are detected and used to update an existing image recognition system. Most existing multi-label image classification methods cannot be directly applied in this scenario, where the training and testing stages must have the same label set. In this paper, we proposed to learn a multi-label classifier and a novel-class detector alternately to solve this problem. The multi-label classifier is learned using a convolutional neural network (CNN) from the images in the known classes. We proposed a recurrent novel-class detector which is learned in the supervised manner to detect the novel class by encoding image features with the multi-label information. In the experiment, our method is evaluated on several large-scale multi-label benchmarks including MS COCO. The results show the proposed method is comparable to most existing multi-label image classification methods, which validate its efficacy when encountering streaming images with unknown classes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304017",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Contextual image classification",
      "Convolutional neural network",
      "Detector",
      "Image (mathematics)",
      "Machine learning",
      "Multi-label classification",
      "Multiclass classification",
      "One-class classification",
      "Pattern recognition (psychology)",
      "Support vector machine",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yu"
      },
      {
        "surname": "Wang",
        "given_name": "Yin"
      },
      {
        "surname": "Liu",
        "given_name": "Xu-Ying"
      },
      {
        "surname": "Mi",
        "given_name": "Siya"
      },
      {
        "surname": "Zhang",
        "given_name": "Min-Ling"
      }
    ]
  },
  {
    "title": "SVD-based redundancy removal in 1-D CNNs for acoustic scene classification",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.02.004",
    "abstract": "In this letter, we propose a concise feature representation framework for acoustic scene classification by pruning embeddings obtained from SoundNet, a deep convolutional neural network. We demonstrate that the feature maps generated at various layers of SoundNet have redundancy. The proposed singular value decomposition based method reduces the redundancy while relying on the assumption that useful feature maps produced by different classes lie along different directions. This leads to ignoring the feature maps that produce similar embeddings for different classes. In the context of using an ensemble of classifiers on the various layers of SoundNet, pruning the redundant feature maps leads to reduction in dimensionality and computational complexity. Our experiments on acoustic scene classification demonstrate that ignoring 73% of feature maps reduces the performance by less than 1% with 12.67% reduction in computational complexity. In addition to this, we also show that the proposed pruning framework can be utilized to remove filters in the SoundNet network architecture, with 13x lesser model storage requirement. Also, the number of parameters reduce from 28 million to 2 million with marginal degradation in performance. This small model obtained after applying the proposed pruning procedure is evaluated on different acoustic scene classification datasets, and shows excellent generalization ability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300428",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computational complexity theory",
      "Computer science",
      "Convolutional neural network",
      "Dimensionality reduction",
      "Feature (linguistics)",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pruning",
      "Reduction (mathematics)",
      "Redundancy (engineering)",
      "Singular value decomposition"
    ],
    "authors": [
      {
        "surname": "Singh",
        "given_name": "Arshdeep"
      },
      {
        "surname": "Rajan",
        "given_name": "Padmanabhan"
      },
      {
        "surname": "Bhavsar",
        "given_name": "Arnav"
      }
    ]
  },
  {
    "title": "Conditional GAN based individual and global motion fusion for multiple object tracking in UAV videos",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.12.018",
    "abstract": "Multiple Object Tracking (MOT) meets great challenges in videos captured by Unmanned Aerial Vehicles (UAVs). Different from traditional videos, due to high altitude and abrupt motion changes of UAVs, the sizes of target objects in UAVs videos are usually very small and the appearance information of target objects is unreliable. The motion analysis is meaningful to associate multiple objects in UAV videos. However, the traditional motion analysis models inevitably suffer from the autonomous motion of UAVs. In this paper, we proposed a Conditional Generative Adversarial Networks (GAN) based model to predict complex motions in UAV videos. We regard the objects motions and the UAV movement as the individual motions and global motions respectively. They are complementary with each other and are employed jointly to facilitate accurate motion prediction. Specifically, a social Long Short Term Memory network is exploited to estimate the individual motion of objects, and a Siamese network is constructed to generate the global motion to reflect the view changes from UAVs, and a conditional GAN is developed to generate the final motion affinity. Extensive experimental results are conducted on public UAV datasets contained various types of objects and 4 different kinds of object detection inputs. Robust motion prediction and improved MOT performance are achieved compared with state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303915",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Match moving",
      "Motion (physics)",
      "Motion estimation",
      "Object (grammar)",
      "Pedagogy",
      "Psychology",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Hongyang"
      },
      {
        "surname": "Li",
        "given_name": "Guorong"
      },
      {
        "surname": "Su",
        "given_name": "Li"
      },
      {
        "surname": "Zhong",
        "given_name": "Bineng"
      },
      {
        "surname": "Yao",
        "given_name": "Hongxun"
      },
      {
        "surname": "Huang",
        "given_name": "Qingming"
      }
    ]
  },
  {
    "title": "An efficient unsupervised feature selection procedure through feature clustering",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.12.022",
    "abstract": "Due to the scarcity of readily available labels, unsupervised feature selection (UFS) methods are widely adopted in the analysis of high-dimensional data. However, most of the existing UFS methods primarily focus on the significance of features in maintaining the data structure while ignoring the redundancy among features. Moreover, the determination of the proper number of features is another challenge. In this paper, an efficient unsupervised feature selection method through feature clustering (EUFSFC) is proposed to address the redundancy among features, and to determine the size of the final feature subset. The proposed methodology is comprised of two steps: (a) feature cluster analysis, and (b) the selection of the representative features. An extended density-based clustering algorithm is proposed to separate features into an appropriate number of disjoint clusters with no requirement for predefined cluster numbers or radii. The selection of features is performed by choosing the most representative features from those feature clusters. Experiments are conducted to show the effectiveness of the proposed feature selection method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303976",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Disjoint sets",
      "Feature (linguistics)",
      "Feature selection",
      "Linguistics",
      "Mathematics",
      "Minimum redundancy feature selection",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Redundancy (engineering)",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Xuyang"
      },
      {
        "surname": "Nazmi",
        "given_name": "Shabnam"
      },
      {
        "surname": "Erol",
        "given_name": "Berat A."
      },
      {
        "surname": "Homaifar",
        "given_name": "Abdollah"
      },
      {
        "surname": "Gebru",
        "given_name": "Biniam"
      },
      {
        "surname": "Tunstel",
        "given_name": "Edward"
      }
    ]
  },
  {
    "title": "A multimodal approach for human activity recognition based on skeleton and RGB data",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.01.010",
    "abstract": "Human action recognition plays a fundamental role in the design of smart solution for home environments, particularly in relation to ambient assisted living applications, where the support of an automated system could improve the quality of life of humans trying to interpret and anticipate user needs, recognizing unusual behaviors or preventing dangerous situations (e.g. falls). In this work the potentialities of the Kinect sensor are fully exploited to design a robust approach for activity recognition combining the analysis of skeleton and RGB data streams. The skeleton representation is designed to capture the most representative body postures, while the temporal evolution of actions is better highlighted by the representation obtained from RGB images. The experimental results confirm that the combination of these two data sources allow to capture highly discriminative features resulting in an approach able to achieve state-of-the-art performance on public benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300106",
    "keywords": [
      "Action recognition",
      "Activity recognition",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Discriminative model",
      "Human–computer interaction",
      "Law",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Programming language",
      "RGB color model",
      "Relation (database)",
      "Representation (politics)",
      "Skeleton (computer programming)"
    ],
    "authors": [
      {
        "surname": "Franco",
        "given_name": "Annalisa"
      },
      {
        "surname": "Magnani",
        "given_name": "Antonio"
      },
      {
        "surname": "Maio",
        "given_name": "Dario"
      }
    ]
  },
  {
    "title": "CMIR-NET : A deep learning based model for cross-modal retrieval in remote sensing",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.02.006",
    "abstract": "We address the problem of cross-modal information retrieval in the domain of remote sensing. In particular, we are interested in two application scenarios: i) cross-modal retrieval between panchromatic (PAN) and multispectral imagery, and ii) multi-label image retrieval between very high resolution (VHR) images and speech-based label annotations. These multi-modal retrieval scenarios are more challenging than the traditional uni-modal retrieval approaches given the inherent differences in distributions between the modalities. However, with the increasing availability of multi-source remote sensing data and the scarcity of enough semantic annotations, the task of multi-modal retrieval has recently become extremely important. In this regard, we propose a novel deep neural network-based architecture that is considered to learn a discriminative shared feature space for all the input modalities, suitable for semantically coherent information retrieval. Extensive experiments are carried out on the benchmark large-scale PAN - multispectral DSRSID dataset and the multi-label UC-Merced dataset. Together with the Merced dataset, we generate a corpus of speech signals corresponding to the labels. Superior performance with respect to the current state-of-the-art is observed in all the cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300453",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Chemistry",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Discriminative model",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Information retrieval",
      "Linguistics",
      "Modal",
      "Modalities",
      "Multispectral image",
      "Panchromatic film",
      "Philosophy",
      "Polymer chemistry",
      "Remote sensing",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Chaudhuri",
        "given_name": "Ushasi"
      },
      {
        "surname": "Banerjee",
        "given_name": "Biplab"
      },
      {
        "surname": "Bhattacharya",
        "given_name": "Avik"
      },
      {
        "surname": "Datcu",
        "given_name": "Mihai"
      }
    ]
  },
  {
    "title": "Fault recognition using an ensemble classifier based on Dempster–Shafer Theory",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107079",
    "abstract": "Aiming at the poor performance of individual classifier in the field of fault recognition, in this paper, a new ensemble classifier is constructed to improve the classification accuracy by combining multiple classifiers based on Dempster–Shafer Theory (DST). However, in some specific cases, especially when dealing with the combination of conflicting evidences, the DST may produce counter-intuitive results and loss its advantages in combining classifiers. To solve this problem, a new improved combination method is proposed to alleviate the conflicts between evidences and a new ensemble technique is developed for the combination of individual classifiers, which can be well used in the design of accurate classifier ensembles. The main advantage of the proposed combination method is that of making the combination process more efficient and accurate by defining the objective weights and subjective weights of member classifiers’ outputs. To verify the effectiveness of the proposed combination method, four individual classifiers are selected for constructing ensemble classifier and tested on Tennessee-Eastman Process (TEP) datasets and UCI machine learning datasets. The experimental results show that the ensemble classifier can significantly improve the classification accuracy and outperforms all the selected individual classifiers. By comparison with other combination methods based on DST and some state-of-the-art ensemble methods, the proposed combination method shows better abilities in dealing with the combination of individual classifiers and outperforms the others in multiple performance measurements. Finally, the proposed ensemble classifier is applied to the fault recognition in real chemical plant.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303802",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Dempster–Shafer theory",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zhen"
      },
      {
        "surname": "Wang",
        "given_name": "Rongxi"
      },
      {
        "surname": "Gao",
        "given_name": "Jianmin"
      },
      {
        "surname": "Gao",
        "given_name": "Zhiyong"
      },
      {
        "surname": "Liang",
        "given_name": "Yanjie"
      }
    ]
  },
  {
    "title": "Matching ostraca fragments using a siamese neural network",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.01.012",
    "abstract": "As part of sociological studies, artifacts such as pottery ostraca from Upper Egypt are studied by egyptologists. These pottery pieces are covered with textual inscriptions many of which concern accounting and other economic and administrative matters. Thus, these writings contain pertinent information for understanding ancient Egyptian civilizations. The ostraca fragments retrieved from excavations are available in large quantities, but most of them are not yet analyzed, sorted, and reassembled by the egyptologists. We present a fragment matching approach based on pairwise local assemblies, using a 2D Siamese Neural Network to evaluate matching probabilities. This network is designed to predict simultaneously the existence or absence of a match, and the spatial relationship of one fragment in relation to the other (up, down, left, or right). We trained our deep learning model on a dataset of 6000 patches extracted from ostraca images, and achieved 96% accuracy on a validation dataset of 1000 patches. Then we propose a pipeline to reconstruct larger ostraca images using step by step pairwise matching, and able to produce a series of image reconstruction proposals. This method is based on the construction of a graph through iterative addition of small fragments. This work is intended as a proof of concept that archaeologists can benefit from automatic processing of their ostraca dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300118",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Data mining",
      "Fragment (logic)",
      "Graph",
      "Matching (statistics)",
      "Mathematics",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Pipeline (software)",
      "Programming language",
      "Relation (database)",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Ostertag",
        "given_name": "Cecilia"
      },
      {
        "surname": "Beurton-Aimar",
        "given_name": "Marie"
      }
    ]
  },
  {
    "title": "Image fusion via sparse regularization with non-convex penalties",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.01.020",
    "abstract": "The L 1 norm regularized least squares method is often used for finding sparse approximate solutions and is widely used in signal restoration. Basis pursuit denoising (BPD) performs noise reduction in this way. However, the shortcoming of using L 1 norm regularization is the underestimation of the true solution. Recently, a class of non-convex penalties have been proposed to improve this situation. This kind of penalty function is non-convex itself, but preserves the convexity property of the whole cost function. This approach has been confirmed to offer good performance in 1-D signal denoising. This paper demonstrates the aforementioned method to 2-D signals (images) and applies it to multisensor image fusion. The problem is posed as an inverse one and a corresponding cost function is judiciously designed to include two data attachment terms. The whole cost function is proved to be convex upon suitably choosing the non-convex penalty, so that the cost function minimization can be tackled by convex optimization approaches, which comprise simple computations. The performance of the proposed method is benchmarked against a number of state-of-the-art image fusion techniques and superior performance is demonstrated both visually and in terms of various assessment measures.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300325",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Convex combination",
      "Convex function",
      "Convex optimization",
      "Convexity",
      "Economics",
      "Financial economics",
      "Geometry",
      "Law",
      "Mathematical optimization",
      "Mathematics",
      "Minification",
      "Noise reduction",
      "Norm (philosophy)",
      "Penalty method",
      "Political science",
      "Proper convex function",
      "Regular polygon",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "Anantrasirichai",
        "given_name": "Nantheera"
      },
      {
        "surname": "Zheng",
        "given_name": "Rencheng"
      },
      {
        "surname": "Selesnick",
        "given_name": "Ivan"
      },
      {
        "surname": "Achim",
        "given_name": "Alin"
      }
    ]
  },
  {
    "title": "Feature-extraction methods for historical manuscript dating based on writing style development",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.01.027",
    "abstract": "Paleographers and philologists perform significant research in finding the dates of ancient manuscripts to understand the historical contexts. To estimate these dates, the traditional process of using classical paleography is subjective, tedious, and often time-consuming. An automatic system based on pattern recognition techniques that infers these dates would be a valuable tool for scholars. In this study, the development of handwriting styles over time in the Dead Sea Scrolls, a collection of ancient manuscripts, is used to create a model that predicts the date of a query manuscript. In order to extract the handwriting styles, several dedicated feature-extraction techniques have been explored. Additionally, a self-organizing time map is used as a codebook. Support vector regression is used to estimate a date based on the feature vector of a manuscript. The date estimation from grapheme-based technique outperforms other feature-extraction techniques in identifying the chronological style development of handwriting in this study of the Dead Sea Scrolls.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300386",
    "keywords": [
      "Ancient history",
      "Archaeology",
      "Artificial intelligence",
      "Codebook",
      "Computer science",
      "Feature (linguistics)",
      "Feature extraction",
      "Geography",
      "Handwriting",
      "Handwriting recognition",
      "History",
      "Linguistics",
      "Natural language processing",
      "Palaeography",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Style (visual arts)",
      "Writing style"
    ],
    "authors": [
      {
        "surname": "Dhali",
        "given_name": "Maruf A."
      },
      {
        "surname": "Jansen",
        "given_name": "Camilo Nathan"
      },
      {
        "surname": "de Wit",
        "given_name": "Jan Willem"
      },
      {
        "surname": "Schomaker",
        "given_name": "Lambert"
      }
    ]
  },
  {
    "title": "Special issue on recent advances in statistical, structural and syntactic pattern recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.12.004",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303678",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Natural language processing",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Bai",
        "given_name": "Xiao"
      },
      {
        "surname": "Hancock",
        "given_name": "Edwin R."
      },
      {
        "surname": "Wilson",
        "given_name": "Richard C."
      },
      {
        "surname": "Ho",
        "given_name": "Tin Kam"
      }
    ]
  },
  {
    "title": "Two sides of the same coin: Improved ancient coin classification using Graph Transduction Games",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.12.007",
    "abstract": "In this work we tackle the problem of automatic recognition of ancient coin types using a semisupervised learning method, namely Graph Transduction Games. Such problem is complex, mainly due to the low inter-class and large intra-class variations and the task becomes even more complex due to lack of labeled large datasets from certain ancient ages. In this paper we propose a new dataset which is chiefly the extension of a previous one both in terms of quantity and diversity. Moreover, we propose a game-theoretic model that exploits both sides of a coin to achieve higher classification accuracy. We experimentally demonstrate that proposed approach brings performance improvement in this complex task even when few number of labelled images are available.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303708",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Class (philosophy)",
      "Computer science",
      "Computer security",
      "Economics",
      "Exploit",
      "Extension (predicate logic)",
      "Graph",
      "Machine learning",
      "Management",
      "Programming language",
      "Task (project management)",
      "Theoretical computer science",
      "Transduction (biophysics)"
    ],
    "authors": [
      {
        "surname": "Aslan",
        "given_name": "Sinem"
      },
      {
        "surname": "Vascon",
        "given_name": "Sebastiano"
      },
      {
        "surname": "Pelillo",
        "given_name": "Marcello"
      }
    ]
  },
  {
    "title": "A cockpit of multiple measures for assessing film restoration quality",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.01.009",
    "abstract": "In machine vision, the idea of expressing the quality of a films by a single value is very popular. Usually this value is computed by processing a set of image features with the aim of resembling as much as possible a kind of human judgment of the film quality. Since human quality assessment is a complex mechanism involving many different perceptual aspects, we believe that such approach may scarcely provide a comprehensive analysis. Especially in the field of digital movie restoration, a single score can hardly provide reliable information about the effects of the various restoring operations. For this reason we introduce an alternative approach, where a set of measures, describing over time basic global and local visual properties of the film frames, is computed in an unsupervised way and delivered to expert evaluators for checking the restoration pipeline and results. The proposed framework can be viewed as a car or airplane cockpit, whose parameters (i.e. the computed measures) are necessary to control the machine status and performance. This cockpit, which is publicly available online, would like to support the digital restoration process and its assessment.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300076",
    "keywords": [
      "Aeronautics",
      "Artificial intelligence",
      "Biology",
      "Cockpit",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Epistemology",
      "Field (mathematics)",
      "Human–computer interaction",
      "Machine learning",
      "Mathematics",
      "Neuroscience",
      "Operating system",
      "Perception",
      "Philosophy",
      "Pipeline (software)",
      "Process (computing)",
      "Programming language",
      "Pure mathematics",
      "Quality (philosophy)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Barricelli",
        "given_name": "Barbara Rita"
      },
      {
        "surname": "Casiraghi",
        "given_name": "Elena"
      },
      {
        "surname": "Lecca",
        "given_name": "Michela"
      },
      {
        "surname": "Plutino",
        "given_name": "Alice"
      },
      {
        "surname": "Rizzi",
        "given_name": "Alessandro"
      }
    ]
  },
  {
    "title": "A multi-feature bipartite graph ensemble for image segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.12.017",
    "abstract": "Image segmentation benefits from using multi-feature ensembles. In this paper, we propose a novel multi-layer bipartite graph model for more effective feature fusion. This model employs multiple graph layers, each representing a feature space. They share common vertices but have individual edge sets that are obtained from different feature spaces. The features are fused by the regularization defined on a Grassmann manifold, which compresses the graph layers into subspace representations and merges them into one. The merged graph is then fed into a graph-cut algorithm to generate the final segmentation. Experiments carried out using the Berkeley segmentation benchmark show that our model is effective in feature fusion and image segmentation, outperforming the state-of-the-art superpixel-based methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303800",
    "keywords": [
      "Artificial intelligence",
      "Bipartite graph",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Graph",
      "Image segmentation",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Segmentation",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Gu",
        "given_name": "Xianbin"
      },
      {
        "surname": "Deng",
        "given_name": "Jeremiah D."
      }
    ]
  },
  {
    "title": "Flexible character accuracy measure for reading-order-independent evaluation",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.02.003",
    "abstract": "The extraction of textual information from scanned document pages is a fundamental stage in any digitisation effort and directly determines the success of the overall document analysis and understanding application scenarios. To evaluate and improve the performance of optical character recognition (OCR), it is necessary to measure the accuracy of that step alone, without the influence of the processing steps that precede it (e.g. text block segmentation and ordering). Current OCR performance evaluation measures (based on edit distance) are strongly subjective as they need to first serialise the entire text in the documents – a process influenced heavily by the specific reading order determined (often wrongly, especially in cases of multicolumn and complex layouts) by processing steps prior to OCR. This paper presents a new objective and practical edit-distance-based character recognition accuracy measure which overcomes those limitations. It achieves its independence from the reading order by comparing sub-strings of text in a flexible way (i.e. allowing for ordering variations). The precision of the flexible character accuracy measure enables the effective tuning of complete digitisation workflows (as OCR errors are isolated and other steps can be evaluated and optimised separately). For the same reason, it also enables a better estimation of post-OCR (manual) correction effort required. The proposed character accuracy measure has been systematically analysed and validated under lab conditions as well as successfully used in practice in a number of high-profile international competitions since 2017.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300416",
    "keywords": [
      "Artificial intelligence",
      "Character (mathematics)",
      "Computer science",
      "Data mining",
      "Database",
      "Document processing",
      "Geometry",
      "Image (mathematics)",
      "Information retrieval",
      "Law",
      "Matching (statistics)",
      "Mathematics",
      "Measure (data warehouse)",
      "Natural language processing",
      "Operating system",
      "Optical character recognition",
      "Pattern recognition (psychology)",
      "Political science",
      "Process (computing)",
      "Reading (process)",
      "Segmentation",
      "Statistics",
      "Workflow"
    ],
    "authors": [
      {
        "surname": "Clausner",
        "given_name": "Christian"
      },
      {
        "surname": "Pletschacher",
        "given_name": "Stefan"
      },
      {
        "surname": "Antonacopoulos",
        "given_name": "Apostolos"
      }
    ]
  },
  {
    "title": "Shaping for PET image analysis",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.01.017",
    "abstract": "Component-trees constitute an efficient data structure for hierarchical image modeling. In particular they are relevant for processing and analyzing images where the structures of interest correspond either to local maxima or local minima of intensity. This is indeed the case of functional data in medical imaging. This motivates the use of component-tree-based approaches for analyzing Positron Emission Tomography (PET) images in the context of oncology. In this article, we present a simple, yet efficient, methodological framework for PET image analysis based on component-trees. More precisely, we show that the second-order paradigm of shaping, that broadly consists of computing the component-tree of a component-tree, provides a relevant way of generalizing the threshold-based strategies classically used by medical practitioners for handling PET images. In addition, it also allows to embed relevant priors regarding the sought cancer lesions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300167",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Component (thermodynamics)",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Data mining",
      "Geography",
      "Hierarchical database model",
      "Image (mathematics)",
      "Image processing",
      "Mathematical analysis",
      "Mathematics",
      "Maxima and minima",
      "Medical imaging",
      "Medicine",
      "Nuclear medicine",
      "Pattern recognition (psychology)",
      "Physics",
      "Positron emission tomography",
      "Thermodynamics",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Grossiord",
        "given_name": "Éloïse"
      },
      {
        "surname": "Passat",
        "given_name": "Nicolas"
      },
      {
        "surname": "Talbot",
        "given_name": "Hugues"
      },
      {
        "surname": "Naegel",
        "given_name": "Benoît"
      },
      {
        "surname": "Kanoun",
        "given_name": "Salim"
      },
      {
        "surname": "Tal",
        "given_name": "Ilan"
      },
      {
        "surname": "Tervé",
        "given_name": "Pierre"
      },
      {
        "surname": "Ken",
        "given_name": "Soléakhéna"
      },
      {
        "surname": "Casasnovas",
        "given_name": "Olivier"
      },
      {
        "surname": "Meignan",
        "given_name": "Michel"
      },
      {
        "surname": "Najman",
        "given_name": "Laurent"
      }
    ]
  },
  {
    "title": "Video classification and retrieval through spatio-temporal Radon features",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107099",
    "abstract": "The rise in the availability of video content for access via the Internet and the medium of television has resulted in the development of automatic search procedures to retrieve the desired video. Searches can be simplified and hastened by employing automatic classification of videos. This paper proposes a descriptor called the Spatio-Temporal Histogram of Radon Projections (STHRP) for representing the temporal pattern of the contents of a video and demonstrates its application to video classification and retrieval. The first step in STHRP pattern computation is to represent any video as Three Orthogonal Planes (TOPs), i.e., XY, XT and YT, signifying the spatial and temporal contents. Frames corresponding to each plane are partitioned into overlapping blocks. Radon projections are obtained over these blocks at different orientations, resulting in weighted transform coefficients that are normalized and grouped into bins. Linear Discriminant Analysis (LDA) is performed over these coefficients of the TOPs to arrive at a compact description of STHRP pattern. Compared to existing classification and retrieval approaches, the proposed descriptor is highly robust to translation, rotation and illumination variations in videos. To evaluate the capabilities of the invariant STHRP pattern, we analyse the performance by conducting experiments on the UCF-101, HMDB51, 10contexts and TRECVID data sets for classification and retrieval using a bagged tree model. Experimental evaluation of video classification reveals that STHRP pattern can achieve classification rates of 96.15%, 71.7%, 93.24% and 97.3% for the UCF-101, HMDB51,10contexts and TRECVID 2005 data sets respectively. We conducted retrieval experiments on the TRECVID 2005, JHMDB and 10contexts data sets and the results revealed that STHRP pattern is able to provide the videos relevant to the user's query in minimal time (0.05s) with a good precision rate.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304005",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computation",
      "Computer science",
      "Gene",
      "Histogram",
      "Image (mathematics)",
      "Invariant (physics)",
      "Linear discriminant analysis",
      "Mathematical physics",
      "Mathematics",
      "Messenger RNA",
      "Pattern recognition (psychology)",
      "Translation (biology)"
    ],
    "authors": [
      {
        "surname": "Sasithradevi",
        "given_name": "A."
      },
      {
        "surname": "Roomi",
        "given_name": "S. Mohamed Mansoor"
      }
    ]
  },
  {
    "title": "Persistence-based resolution-independent meshes of superpixels",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.01.014",
    "abstract": "The over-segmentation problem is to split a pixel-based image into a smaller number of superpixels that can be treated as indecompasable regions to speed up higher level image processing such as segmentation or object detection. A traditional superpixel is a potentially disconnected union of square pixels, which can have complicated topology (with holes) and geometry (highly zigzag boundaries). This paper contributes to new resolution-independent superpixels modeled as convex polygons with straight-line edges and vertices with real coordinates not restricted to a fixed pixel grid. Any such convex polygon can be rendered at any resolution higher than in original images, hence superpixels are resolution-independent. The key difficulty in obtaining resolution-independent superpixels is to find continuous straight-line edges, while classical edge detection focuses on extracting only discrete edge pixels. The recent Persistent Line Segment Detector (PLSD) avoids intersections and small angles between line segments, which are hard to fix before a proper polygonal mesh can be constructed. The key novelty is an automatic selection of strongest straight-line segments by using the concept of persistence from Topological Data Analysis, which allows to rank segments by their strength. The PLSD performed well in comparison with the only past Line Segment Detector Algorithm (LSDA) on the Berkeley Segmentation Database of 500 real-life images. The PLSD is now extended to the Persistent Resolution-Independent Mesh (PRIM).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300131",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Frame (networking)",
      "Geometry",
      "Image segmentation",
      "Line (geometry)",
      "Line segment",
      "Mathematics",
      "Pixel",
      "Polygon (computer graphics)",
      "Polygon mesh",
      "Regular polygon",
      "Segmentation",
      "Telecommunications",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Kurlin",
        "given_name": "Vitaliy"
      },
      {
        "surname": "Muszynski",
        "given_name": "Grzegorz"
      }
    ]
  },
  {
    "title": "Gastrointestinal diseases segmentation and classification based on duo-deep architectures",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.12.024",
    "abstract": "Nowadays, almost one million gastrointestinal patients are successfully treated by Wireless Capsule Endoscopy (WCE). It is the latest technology in the area of medical imaging for the diagnosis of gastrointestinal diseases such as ulcer, polyp, bleeding, etc. Manual diagnosis process is time-consuming and hard for doctors; therefore, researchers have proposed computerized techniques for detection and classification of these diseases. In this article, a deep learning-based method is presented for ulcer detection and gastrointestinal diseases (ulcer, polyp, bleeding) classification. Modified mask Recurrent Convolutional Neural Network (RCNN) based ulcer segmentation is proposed. The ulcer annotated images are utilized to train the Mask RCNN model to obtain output in the form of bounding box ulcer detected area and mask segmented region. In the classification phase, the ResNet101 pre-trained CNN model is fine-tuned through transfer learning to derive deep features. The acquired deep features are optimized through grasshopper optimization along with minimum distance fitness function. The best-selected features are finally supplied to a Multi-class Support Vector Machine (MSVM) of cubic kernel function for final classification. Experiments have been performed in two-steps; first, the ulcer segmentation results are computed through recall, precision, and Mean Overlap Coefficient (MOC). The ResNet50+FPN as backbone and training all the layers of Mask-RCNN gives best results in terms of MOC = 0.8807 and average precision = 1.0. Second, the best classification accuracy of 99.13% is achieved on the cubic SVM for K = 10 . It is clearly perceived that the proposed method outperforms when compared and analyzed with the existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786551930399X",
    "keywords": [
      "Artificial intelligence",
      "Capsule endoscopy",
      "Combinatorics",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Image (mathematics)",
      "Kernel (algebra)",
      "Mathematics",
      "Medicine",
      "Minimum bounding box",
      "Pattern recognition (psychology)",
      "Radiology",
      "Segmentation",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Khan",
        "given_name": "Mehshan Ahmed"
      },
      {
        "surname": "Khan",
        "given_name": "Muhammad Attique"
      },
      {
        "surname": "Ahmed",
        "given_name": "Fawad"
      },
      {
        "surname": "Mittal",
        "given_name": "Mamta"
      },
      {
        "surname": "Goyal",
        "given_name": "Lalit Mohan"
      },
      {
        "surname": "Jude Hemanth",
        "given_name": "D."
      },
      {
        "surname": "Satapathy",
        "given_name": "Suresh Chandra"
      }
    ]
  },
  {
    "title": "CHAT-Bot: A cultural heritage aware teller-bot for supporting touristic experiences",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.01.003",
    "abstract": "Cultural heritage is an important resource that allows us to know and promote a territory. In this respect, it is important to experiment with the enhancement of cultural heritage by adopting approaches that meet the dynamic needs of various types of users. The aim of this paper is to introduce a recommender system capable of developing adaptive tourist routes. In fact, the proposed system suggests points of interest and related services according to both the profile of the tourist and contextual aspects. In particular, the interaction of the user with the system occurs through a chatbot that allows to build a real dialog. In order to show the potential of the proposed approach, a prototype was developed to support the user in building a customized tourist route related to some of the most important cultural sites in Campania (a region in Southern Italy): Herculaneum, Paestum and Pompeii.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300052",
    "keywords": [
      "Archaeology",
      "Business",
      "Chatbot",
      "Computer network",
      "Computer science",
      "Cultural heritage",
      "Dialog box",
      "Finance",
      "Geography",
      "Human–computer interaction",
      "Order (exchange)",
      "Resource (disambiguation)",
      "Tourism",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Casillo",
        "given_name": "Mario"
      },
      {
        "surname": "Clarizia",
        "given_name": "Fabio"
      },
      {
        "surname": "D'Aniello",
        "given_name": "Giuseppe"
      },
      {
        "surname": "De Santo",
        "given_name": "Massimo"
      },
      {
        "surname": "Lombardi",
        "given_name": "Marco"
      },
      {
        "surname": "Santaniello",
        "given_name": "Domenico"
      }
    ]
  },
  {
    "title": "A group lasso based sparse KNN classifier",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.12.020",
    "abstract": "Sparse features have been shown effective in applications of computer vision, machine learning, signal processing, etc. Group sparsity was proposed by considering that there exist natural group structures in many problems. Previous research mainly focuses on improving the way to extract sparse features, such as lasso, group lasso, overlapped group lasso, sparse group lasso. In existing work, sparse features are usually taken as input for classifiers, such as SVM, KNN, or SRC (Sparse Representation based Classification). In this paper, we find that, instead of using sparse group features as input for classifiers, sparse group features are good candidates for selection of most relevant classes/groups. We design a new classifier to improve classification accuracy: (1) we use sparse group lasso to select K most relevant classes/groups, which makes this approach robust, because it filters out unrelated classes/groups in group level, instead of individual sample level; (2) KSVD is used to get exact desired sparsity (k nonzero entries) and thus eliminates the difficulty of hyperparameter tuning; (3) simple summation of regression weights within each class/group contains sufficient class discriminant information, and the chance of a sample belonging to a specific class is denoted simply by the summation of corresponding regression weights within each class, which is in line with the need of Explainable AI (XAI). The K most relevant groups/classes can be considered as K neighbors of the correct class. Thus, we call this classifier Group Lasso based Sparse KNN (GLSKNN). Compared to 8 other approaches, GLSKNN classifier outperforms other methods in term of classification accuracy for two public image datasets and images with different occlusion/noise levels.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303940",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Discriminant",
      "Elastic net regularization",
      "Feature selection",
      "Hyperparameter",
      "Lasso (programming language)",
      "Linear discriminant analysis",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Sparse approximation",
      "Support vector machine",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Shuai"
      },
      {
        "surname": "Ding",
        "given_name": "Chris"
      }
    ]
  },
  {
    "title": "Swapping trajectories with a sufficient sanitizer",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.02.011",
    "abstract": "Real-time mobility data is useful for several applications such as planning transports in metropolitan areas or localizing services in towns. However, if such data is collected without any privacy protection it may reveal sensible locations and pose safety risks to an individual associated to it. Thus, mobility data must be anonymized preferably at the time of collection. In this paper, we consider the SwapMob algorithm that mitigates privacy risks by swapping partial trajectories. We formalize the concept of sufficient sanitizer and show that the SwapMob algorithm is a sufficient sanitizer for various statistical decision problems. That is, it preserves the aggregate information of the spatial database in the form of sufficient statistics and also provides privacy to the individuals. This may be used for personalized assistants taking advantage of users’ locations, so they can ensure user privacy while providing accurate response to the user requirements. We measure the privacy provided by SwapMob as the Adversary Information Gain, which measures the capability of an adversary to leverage his knowledge of exact data points to infer a larger segment of the sanitized trajectory. We test the utility of the data obtained after applying SwapMob sanitization in terms of Origin-Destination matrices, a fundamental tool in transportation modelling.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300489",
    "keywords": [
      "Adversary",
      "Aggregate (composite)",
      "Biology",
      "Composite material",
      "Computer network",
      "Computer science",
      "Computer security",
      "Data mining",
      "Genetics",
      "Hand sanitizer",
      "Information sensitivity",
      "Leverage (statistics)",
      "Machine learning",
      "Materials science",
      "Medicine",
      "Metropolitan area",
      "Path (computing)",
      "Pathology"
    ],
    "authors": [
      {
        "surname": "Salas",
        "given_name": "Julián"
      },
      {
        "surname": "Megías",
        "given_name": "David"
      },
      {
        "surname": "Torra",
        "given_name": "Vicenç"
      },
      {
        "surname": "Toger",
        "given_name": "Marina"
      },
      {
        "surname": "Dahne",
        "given_name": "Joel"
      },
      {
        "surname": "Sainudiin",
        "given_name": "Raazesh"
      }
    ]
  },
  {
    "title": "Efficient nearest neighbor search in high dimensional hamming space",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107082",
    "abstract": "Fast approximate nearest neighbor search has been well studied for real-valued vectors, however, the methods for binary descriptors are less developed. The paper addresses this problem by resorting to the well established techniques in Euclidean space. To this end, the binary descriptors are firstly mapped into low dimensional float vectors under the condition that the neighborhood information in the original Hamming space could be preserved in the mapped Euclidean space as much as possible. Then, KD-Tree is used to partitioning the mapped Euclidean space in order to quickly find approximate nearest neighbors for a given query point. This is identical to filter out a subset of nearest neighbor candidates in the original Hamming space due to the property of neighborhood preserving. Finally, Hamming ranking is applied to the small number of candidates to find out the approximate nearest neighbor in the original Hamming space, with only a fraction of running time compared to the bruteforce linear scan. Our experiments demonstrate that the proposed method significantly outperforms the state of the arts, obtaining improved search accuracy at various speed up factors, e.g., at least 16% improvement of search accuracy over previous methods (from 67.7% to 83.7%) when the search speed is 200 times faster than the linear scan for a one million database.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303838",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Best bin first",
      "Block code",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Correlation clustering",
      "Cover tree",
      "Decoding methods",
      "Euclidean distance",
      "Euclidean space",
      "Filter (signal processing)",
      "Hamming code",
      "Hamming distance",
      "Hamming space",
      "Mathematics",
      "Nearest neighbor search",
      "Pattern recognition (psychology)",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "Bin"
      },
      {
        "surname": "Kong",
        "given_name": "Qingqun"
      },
      {
        "surname": "Zhang",
        "given_name": "Baoqian"
      },
      {
        "surname": "Liu",
        "given_name": "Hongmin"
      },
      {
        "surname": "Pan",
        "given_name": "Chunhong"
      },
      {
        "surname": "Lu",
        "given_name": "Jiwen"
      }
    ]
  },
  {
    "title": "Attacking NIST biometric image software using nonlinear optimization",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.12.003",
    "abstract": "Automated fingerprint identification systems are deployed by law enforcement agencies all over the world for authentication. In the US, the NIST biometric image software (NBIS) is used by the Department of Homeland Security and the Federal Bureau of Investigation for fingerprint matching. NBIS uses MINDTCT as the minutia extractor and BOZORTH3 as the fingerprint matcher. We use nonlinear optimization to attack the BOZORTH3 fingerprint matching system. We use FVC2002, ATVS and CASIA datasets to validate the performance of our attack. We show that the average match score of attack fingerprints is 111.2 for FVC2002, 97.17 for ATVS and 111.07 for the CASIA dataset. We show that for all three datasets, changing only 14 minutia features allows us to attack the BOZORTH3 fingerprint matcher with more than 75% probability of successful attack.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303666",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Biometrics",
      "Botany",
      "Computer science",
      "Computer security",
      "Data mining",
      "Fingerprint (computing)",
      "Fingerprint Verification Competition",
      "Fingerprint recognition",
      "Homeland security",
      "Identification (biology)",
      "Law",
      "Law enforcement",
      "Matching (statistics)",
      "Mathematics",
      "Minutiae",
      "NIST",
      "Pattern recognition (psychology)",
      "Political science",
      "Programming language",
      "Software",
      "Speech recognition",
      "Statistics",
      "Terrorism"
    ],
    "authors": [
      {
        "surname": "Raj",
        "given_name": "Sunny"
      },
      {
        "surname": "Pannu",
        "given_name": "Jodh S."
      },
      {
        "surname": "Fernandes",
        "given_name": "Steven L."
      },
      {
        "surname": "Ramanathan",
        "given_name": "Arvind"
      },
      {
        "surname": "Pullum",
        "given_name": "Laura L."
      },
      {
        "surname": "Jha",
        "given_name": "Sumit K."
      }
    ]
  },
  {
    "title": "Facial expression recognition based on a multi-task global-local network",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.01.016",
    "abstract": "Facial expression recognition plays an important role in intelligent human-computer interaction. The clues for understanding facial expressions lie not in global facial appearance, but also in local informative dynamics among different but confusing expressions. In this paper, we design a multi-task learning framework for global-local representation of facial expressions. First, a shared shallow module is designed to learn information from local regions and the global image. Then we construct a part-based module, which processes critical local regions including the eyes, the nose, and the mouth to extract local informative dynamics related to facial expressions. A global face module is proposed to extract global appearance features related to expressions. The proposed network extracts both local-global and spatio-temporal information for a discriminative and robust representation of facial expressions. Through properly fusing these modules into a system, we have achieved competitive results on the CK+ and Oulu-CASIA databases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300155",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Construct (python library)",
      "Discriminative model",
      "Economics",
      "Expression (computer science)",
      "Face (sociological concept)",
      "Facial expression",
      "Law",
      "Management",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Social science",
      "Sociology",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Mingjing"
      },
      {
        "surname": "Zheng",
        "given_name": "Huicheng"
      },
      {
        "surname": "Peng",
        "given_name": "Zhifeng"
      },
      {
        "surname": "Dong",
        "given_name": "Jiayu"
      },
      {
        "surname": "Du",
        "given_name": "Heran"
      }
    ]
  },
  {
    "title": "Depth-map completion for large indoor scene reconstruction",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107112",
    "abstract": "Traditional Multi View Stereo (MVS) algorithms are often difficult to deal with large-scale indoor scene reconstruction, due to the photo-consistency measurement errors in weak textured regions, which are commonly exist in indoor scenes. To solve this limitation, in this paper we proposed a point cloud completion strategy that combines learning-based depth-map completion and geometry-based consistency filtering to fill large-area missing in depth-maps. The proposed method takes nonuniform and noisy MVS depth-map as input, and completes each depth-map individually. In the completion process, we first complete depth-maps using learning based method, and then filter each depth-map using depth consistency validation with its neighboring depth-maps. This depth-map completion and geometric filtering steps are performed iteratively until the number of depth points is converged. Experiments on large-scale indoor scenes and benchmark MVS datasets demonstrate the effectiveness of the proposed methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304133",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Depth map",
      "Filter (signal processing)",
      "Geography",
      "Image (mathematics)",
      "Operating system",
      "Point cloud",
      "Process (computing)",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Hongmin"
      },
      {
        "surname": "Tang",
        "given_name": "Xincheng"
      },
      {
        "surname": "Shen",
        "given_name": "Shuhan"
      }
    ]
  },
  {
    "title": "Multi-Scale Weight Sharing Network for Image Recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.01.011",
    "abstract": "In this paper, we explore the idea of weight sharing over multiple scales in convolutional networks. Inspired by traditional computer vision approaches, we share the weights of convolution kernels over different scales in the same layers of the network. Although multi-scale feature aggregation and sharing inside convolutional networks are common in practice, none of the previous works address the issue of convolutional weight sharing. We evaluate our weight sharing scheme on two heterogeneous image recognition datasets – ImageNet (object recognition) and Places365-Standard (scene classification). With approximately 25% fewer parameters, our shared weight ResNet model provides similar performance compared to baseline ResNets. Shared-weight models are further validated via transfer learning experiments on four additional image recognition datasets – Caltech256 and Stanford 40 Actions (object-centric) and SUN397 and MIT Inddor67 (scene-centric). Experimental results demonstrate significant redundancy in the vanilla implementations of the deeper networks, and also indicate that a shift towards increasing the receptive field per parameter may improve future convolutional network architectures.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552030009X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Cognitive neuroscience of visual object recognition",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Object (grammar)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Redundancy (engineering)"
    ],
    "authors": [
      {
        "surname": "Aich",
        "given_name": "Shubhra"
      },
      {
        "surname": "Yamazaki",
        "given_name": "Masaki"
      },
      {
        "surname": "Taniguchi",
        "given_name": "Yasuhiro"
      },
      {
        "surname": "Stavness",
        "given_name": "Ian"
      }
    ]
  },
  {
    "title": "Heterogeneous oblique random forest",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107078",
    "abstract": "Decision trees in random forests use a single feature in non-leaf nodes to split the data. Such splitting results in axis-parallel decision boundaries which may fail to exploit the geometric structure in the data. In oblique decision trees, an oblique hyperplane is employed instead of an axis-parallel hyperplane. Trees with such hyperplanes can better exploit the geometric structure to increase the accuracy of the trees and reduce the depth. The present realizations of oblique decision trees do not evaluate many promising oblique splits to select the best. In this paper, we propose a random forest of heterogeneous oblique decision trees that employ several linear classifiers at each non-leaf node on some top ranked partitions which are obtained via one-vs-all and two-hyperclasses based approaches and ranked based on ideal Gini scores and cluster separability. The oblique hyperplane that optimizes the impurity criterion is then selected as the splitting hyperplane for that node. We benchmark 190 classifiers on 121 UCI datasets. The results show that the oblique random forests proposed in this paper are the top 3 ranked classifiers with the heterogeneous oblique random forest being statistically better than all 189 classifiers in the literature.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303796",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Combinatorics",
      "Computer science",
      "Computer security",
      "Decision tree",
      "Exploit",
      "Geodesy",
      "Geography",
      "Hyperplane",
      "Linguistics",
      "Mathematics",
      "Oblique case",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Random forest"
    ],
    "authors": [
      {
        "surname": "Katuwal",
        "given_name": "Rakesh"
      },
      {
        "surname": "Suganthan",
        "given_name": "P.N."
      },
      {
        "surname": "Zhang",
        "given_name": "Le"
      }
    ]
  },
  {
    "title": "Metric learning-based kernel transformer with triplets and label constraints for feature fusion",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107086",
    "abstract": "Feature fusion is an important skill to improve the performance in computer vision, the difficult problem of feature fusion is how to learn the complementary properties of different features. We recognize that feature fusion can benefit from kernel metric learning. Thus, a metric learning-based kernel transformer method for feature fusion is proposed in this paper. First, we propose a kernel transformer to convert data from data space to kernel space, which makes feature fusion and metric learning can be performed in the transformed kernel space. Second, in order to realize supervised learning, both triplets and label constraints are embedded into our model. Third, in order to solve the unknown kernel matrices, LogDet divergence is also introduced into our model. Finally, a complete optimization objective function is formed. Based on an alternating direction method of multipliers (ADMM) solver and the Karush-Kuhn-Tucker (KKT) theorem, the proposed optimization problem is solved with the rigorous theoretical analysis. Experimental results on image retrieval demonstrate the effectiveness of the proposed methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303875",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Engineering",
      "Feature vector",
      "Karush–Kuhn–Tucker conditions",
      "Kernel (algebra)",
      "Kernel embedding of distributions",
      "Kernel method",
      "Kernel principal component analysis",
      "Mathematical optimization",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Radial basis function kernel",
      "Solver",
      "Support vector machine",
      "Variable kernel density estimation"
    ],
    "authors": [
      {
        "surname": "Kan",
        "given_name": "Shichao"
      },
      {
        "surname": "Zhang",
        "given_name": "Linna"
      },
      {
        "surname": "He",
        "given_name": "Zhihai"
      },
      {
        "surname": "Cen",
        "given_name": "Yigang"
      },
      {
        "surname": "Chen",
        "given_name": "Shiming"
      },
      {
        "surname": "Zhou",
        "given_name": "Jikun"
      }
    ]
  },
  {
    "title": "CNN based spatial classification features for clustering offline handwritten mathematical expressions",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.12.015",
    "abstract": "To help human markers mark a large number of answers of handwritten mathematical expressions (HMEs), clustering them makes marking more efficient and reliable. Clustering HMEs, however, faces the problem of extracting both localization and classification representation of mathematical symbols for an HME image and defining the distance between two HME images. First, we propose a method based on Convolutional Neural Networks (CNN) to extract the representations for an HME. Symbols in various scales are located and classified by a combination of features from a multi-scale CNN. We use weakly supervised training combined with symbols attention to enhance localization and classification predictions. Second, we propose a multi-level spatial distance between two representations for clustering HMEs. Experiments on CROHME 2016 and CROHME 2019 dataset show the promising results of 0.99 and 0.96 in purity, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303782",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Nguyen",
        "given_name": "Cuong Tuan"
      },
      {
        "surname": "Khuong",
        "given_name": "Vu Tran Minh"
      },
      {
        "surname": "Nguyen",
        "given_name": "Hung Tuan"
      },
      {
        "surname": "Nakagawa",
        "given_name": "Masaki"
      }
    ]
  },
  {
    "title": "Creating speaker independent ASR system through prosody modification based data augmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.12.019",
    "abstract": "In this paper, the effect of prosody-modification-based data augmentation is explored in the context of automatic speech recognition (ASR). The primary motive is to develop ASR systems that are less affected by speaker-dependent acoustic variations. Two factors contributing towards inter-speaker variability that are focused on in this paper are pitch and speaking-rate variations. In order to simulate such an ASR task, we have trained an ASR system on adults’ speech and tested it using speech data from adult as well as child speakers. Compared to adults’ speech test case, the recognition rates are noted to be extremely degraded when the test speech is from child speakers. The observed degradation is basically due to large differences in pitch and speaking-rate between adults’ and children’s speech. To overcome this problem, pitch and speaking-rate of the training speech are modified to create new versions of the data. The original and the modified versions are then pooled together in order to capture greater acoustic variability. The ASR system trained on augmented data is noted to be more robust towards speaker-dependent variations. Relative improvements of 11.5% and 27.0% over the baseline are obtained on decoding adults’ and children’s speech test sets, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303952",
    "keywords": [
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Engineering",
      "Paleontology",
      "Prosody",
      "Speaker diarisation",
      "Speaker recognition",
      "Speech recognition",
      "Systems engineering",
      "Task (project management)",
      "Word error rate"
    ],
    "authors": [
      {
        "surname": "Shahnawazuddin",
        "given_name": "S"
      },
      {
        "surname": "Adiga",
        "given_name": "Nagaraj"
      },
      {
        "surname": "Kathania",
        "given_name": "Hemant Kumar"
      },
      {
        "surname": "Sai",
        "given_name": "B Tarun"
      }
    ]
  },
  {
    "title": "Classification of engraved pottery sherds mixing deep-learning features by compact bilinear pooling",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.12.009",
    "abstract": "The ARCADIA project aims at using pattern recognition and machine learning to promote a systematic analysis of the large corpus of archaeological pottery fragments excavated in Saran (France). Dating from the High Middle Ages, these sherds have been engraved with repeated patterns using a carved wooden wheel. The study of these engraved patterns allows archaeologists to better understand the diffusion of ceramic productions. In this paper, we present a method that classifies patterns of ceramic sherds by combining deep learning-based features extracted from some pre-trained Convolutional Neural Network (CNN) models. A dataset composed of 888 digital patterns extracted from 3D scans of pottery sherds was used to evaluate our approach. The classification capacity of each CNN model was first assessed individually. Then, several combinations of common pooling methods using different classifiers were tested. The best result was obtained when features of the VGG19 and ResNet50 models were combined using Compact Bilinear Pooling (CBP) with a high classification rate of 95.23%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303721",
    "keywords": [
      "Archaeology",
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Engraving",
      "Geography",
      "Pattern recognition (psychology)",
      "Pooling",
      "Pottery",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Chetouani",
        "given_name": "Aladine"
      },
      {
        "surname": "Treuillet",
        "given_name": "Sylvie"
      },
      {
        "surname": "Exbrayat",
        "given_name": "Matthieu"
      },
      {
        "surname": "Jesset",
        "given_name": "Sébastien"
      }
    ]
  },
  {
    "title": "Recognising decorations in archaeological finds through the analysis of characteristic curves on 3D models",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.01.025",
    "abstract": "In the analysis of archaeological finds, it is important for archaeologists to identify their style, origin, period, etc. to allow their correct classification. In the digital era, the development of automatic techniques to measure the peculiar characteristics of archaeological finds would be of great help in this activity. Considering that ancient artefacts are very often incomplete, consumed, degraded, if not consisting of simple fragments, geometric details, such as decorations, visual motifs, patterns, are more useful for their analysis than global characteristics. These patterns are usually composed by characteristic curves arranged in a regular way, as in a Greek fret or a floral band. Here we propose the recognition of characteristic curves on 3D models of archaeological artefacts, identified by a set of characteristic points. We approximate these curves with known curves to provide localisation and quantitative measurement of the characteristic features used as decorations or patterns of the digital models of ancient objects. To solve this problem, we adopt a generalised version of the Hough Transform (HT). In addition, we introduce new rules of composition and automatic aggregation of the characteristic curves, not limiting the recognition to a single curve at a time and supporting an automatic annotation of the fragment digital model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300362",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Geography",
      "Hough transform",
      "Image (mathematics)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Romanengo",
        "given_name": "Chiara"
      },
      {
        "surname": "Biasotti",
        "given_name": "Silvia"
      },
      {
        "surname": "Falcidieno",
        "given_name": "Bianca"
      }
    ]
  },
  {
    "title": "I-PETER (Interactive platform to experience tours and education on the rocks): A virtual system for the understanding and dissemination of mineralogical-petrographic science",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.12.002",
    "abstract": "Nowadays mineralogical and petrographic sciences represent important resources for the understanding of natural and cultural heritage. Indeed, the implication in economic and social scenario, from the exploitation of geo-resources to the natural stone application in architecture, involves the whole society. In this context, scientific museums are called to enhance society awareness about the importance of minerals and rocks, while facilitating their understanding. It is well-known that the majority of the population is still left out from the museum fruition, usually because of problems of cultural accessibility. In order to find a solution, the Museum of Mineralogy, Petrography and Volcanology of the University of Catania implemented a new communication system based on the visitor’s personal experience. The idea is to give to all users, regardless of their educational level, the possibility to learn by playing. The collaboration among geologists, conservators and computer scientists led to the implementation of a web application called I-PETER: Interactive Platform to Experience Tours and Education on the Rocks. The application, interacting with a database, offers to the public two different modes of exploring the museum’s collections. The visitor can decide to observe the rocks or minerals from the sample exposed to the internal structure and then explore their external application on monuments, or he/she can choose to make a tour in the interactive map, focusing on a particular monument and from there make the virtual reverse path from the macro to the micro scale. Furthermore, thanks to the effort made to construct the knowledge base at the base of I-PETER web app, a labelled dataset of images of rocks and minerals can be easily released for future petrological investigations based on machine learning. Thanks to this pilot project, the museum’s scientific knowledge can promote cultural tourism by making it more accessible to a wide public, as well as support scientific research on petrography and mineralogy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303654",
    "keywords": [
      "Archaeology",
      "Architecture",
      "Computer science",
      "Context (archaeology)",
      "Cultural heritage",
      "Demography",
      "Earth science",
      "Geography",
      "Geology",
      "Natural (archaeology)",
      "Paleontology",
      "Petrography",
      "Population",
      "Programming language",
      "Sociology",
      "Visitor pattern",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Sinitò",
        "given_name": "Diego"
      },
      {
        "surname": "Fugazzotto",
        "given_name": "Maura"
      },
      {
        "surname": "Stroscio",
        "given_name": "Antonio"
      },
      {
        "surname": "Coccato",
        "given_name": "Alessia"
      },
      {
        "surname": "Allegra",
        "given_name": "Dario"
      },
      {
        "surname": "Barone",
        "given_name": "Germana"
      },
      {
        "surname": "Mazzoleni",
        "given_name": "Paolo"
      },
      {
        "surname": "Stanco",
        "given_name": "Filippo"
      }
    ]
  },
  {
    "title": "Robust geodesic based outlier detection for class imbalance problem",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.01.028",
    "abstract": "Outlier detection is very useful in many applications, such as fraud detection and network intrusion detection. However, some existing methods often generate incorrect identification results due to the imbalanced distribution of data points. In this paper, we present a robust geodesic-based outlier detection algorithm which simultaneously considers both global disconnectivity score and local real degree as measures of outlierness. We first construct the global disconnectivity score to incorporate suitable global characteristics of data, then we provide the local real degree to effectively consider the local characteristics of points. Thus, we can identify local outliers with higher overall connectivity but in a smaller cluster with fewer points. Experimental results obtained for a number of synthetic and real-world data sets demonstrate the effectiveness and robustness of our method. In particular, we estimate an increase in average area under curve (AUC) on ten datasets of approximately 15%, with smaller RMSD than any of the competing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300441",
    "keywords": [
      "Acoustics",
      "Anomaly detection",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Degree (music)",
      "Gene",
      "Geodesic",
      "Intrusion detection system",
      "Local outlier factor",
      "Mathematical analysis",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Physics",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Canghong"
      },
      {
        "surname": "Li",
        "given_name": "Xiaojie"
      },
      {
        "surname": "Lv",
        "given_name": "Jiancheng"
      },
      {
        "surname": "Yin",
        "given_name": "Jing"
      },
      {
        "surname": "Mumtaz",
        "given_name": "Imran"
      }
    ]
  },
  {
    "title": "BshapeNet: Object detection and instance segmentation with bounding shape masks",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.01.024",
    "abstract": "We propose a modularizable component that can predict the boundary shapes and boxes of an image, along with a new masking scheme for improving object detection and instance segmentation. Specifically, we introduce two types of novel masks: a bounding box (bbox) mask and a bounding shape (bshape) mask. For each of these types, we consider two variants—the “Thick” model and the “Scored” model—both of which have the same morphology but differ in ways that make their boundaries thicker. To evaluate our masks, we design extended frameworks by adding a bshape mask (or a bbox mask) branch to a Faster R-CNN, and call this BshapeNet (or BboxNet). Furthermore, we propose BshapeNet+, a network that combines a bshape mask branch with a Mask R-CNN. Among our various models, BshapeNet+ demonstrates the best performance in both tasks. In addition, BshapeNet+ markedly outperforms the baseline models on MS COCO and Cityscapes and achieves highly competitive results with state-of-the-art models. In particular, the experimental results show that our branch works well on small objects and is easily applicable to various models, such as PANet as well as Faster R-CNN and Mask R-CNN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300350",
    "keywords": [
      "Artificial intelligence",
      "Boundary (topology)",
      "Bounding overwatch",
      "Computer science",
      "Computer vision",
      "Detector",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Minimum bounding box",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Kang",
        "given_name": "Ba Rom"
      },
      {
        "surname": "Lee",
        "given_name": "Hyunku"
      },
      {
        "surname": "Park",
        "given_name": "Keunju"
      },
      {
        "surname": "Ryu",
        "given_name": "Hyunsurk"
      },
      {
        "surname": "Kim",
        "given_name": "Ha Young"
      }
    ]
  },
  {
    "title": "Improving pattern spotting in historical documents using feature pyramid networks",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.02.002",
    "abstract": "Pattern spotting consists of locating different instances of a given object (i.e. an image query) in a collection of historical document images. These patterns may vary in shape, size, color, context and even style because they are hand-drawn, which makes pattern spotting a difficult task. To tackle this problem, we propose a Convolutional Neural Network (CNN) approach based on Feature Pyramid Networks (FPN) as the feature extractor of our system. Using FPN allows to extract descriptors of local regions of the documents to be indexed and queries, at multiple scales with just a single forward pass. Experiments conducted on DocExplore dataset show that the proposed system improves mAP by 73% (from 0.157 to 0.272) in pattern localization compared with state-of-the-art results, even when the feature extractor is not trained with domain-specific data. Memory requirement and computation time are also decreased since the descriptor dimension used for distance computation is reduced by a factor of 16.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300404",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computation",
      "Computer science",
      "Context (archaeology)",
      "Convolutional neural network",
      "Dimension (graph theory)",
      "Engineering",
      "Extractor",
      "Feature (linguistics)",
      "Feature extraction",
      "Geometry",
      "Keyword spotting",
      "Linguistics",
      "Mathematics",
      "Object (grammar)",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process engineering",
      "Pure mathematics",
      "Pyramid (geometry)",
      "Spotting"
    ],
    "authors": [
      {
        "surname": "Úbeda",
        "given_name": "Ignacio"
      },
      {
        "surname": "Saavedra",
        "given_name": "Jose M."
      },
      {
        "surname": "Nicolas",
        "given_name": "Stéphane"
      },
      {
        "surname": "Petitjean",
        "given_name": "Caroline"
      },
      {
        "surname": "Heutte",
        "given_name": "Laurent"
      }
    ]
  },
  {
    "title": "Hierarchical Attention Network for Action Segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.01.023",
    "abstract": "Temporal segmentation of events is an essential task and a precursor for the automatic recognition of human actions in the video. Several attempts have been made to capture frame-level salient aspects through attention but they lack the capacity to effectively map the temporal relationships in between the frames as they only capture a limited span of temporal dependencies. To this end we propose a complete end-to-end supervised learning approach that can better learn relationships between actions over time, thus improving the overall segmentation performance. The proposed hierarchical recurrent attention framework analyses the input video at multiple temporal scales, to form embeddings at frame level and segment level, and perform fine-grained action segmentation. This generates a simple, lightweight, yet extremely effective architecture for segmenting continuous video streams and has multiple application domains. We evaluate our system on multiple challenging public benchmark datasets, including MERL Shopping, 50 salads, and Georgia Tech Egocentric datasets and achieves state-of-the-art performance. The evaluated datasets encompass numerous video capture settings which are inclusive of static overhead camera views and dynamic, ego-centric head-mounted camera views, demonstrating the direct applicability of the proposed framework in a variety of settings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518307104",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Business",
      "Computer science",
      "Computer vision",
      "Economics",
      "Frame (networking)",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Management",
      "Market segmentation",
      "Marketing",
      "Operating system",
      "Overhead (engineering)",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Task (project management)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Gammulle",
        "given_name": "Harshala"
      },
      {
        "surname": "Denman",
        "given_name": "Simon"
      },
      {
        "surname": "Sridharan",
        "given_name": "Sridha"
      },
      {
        "surname": "Fookes",
        "given_name": "Clinton"
      }
    ]
  },
  {
    "title": "DV-Net: Dual-view network for 3D reconstruction by fusing multiple sets of gated control point clouds",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.02.001",
    "abstract": "Deep learning for 3D reconstruction have just shown some promising advantanges, where 3D shapes can be predicted from a single RGB image. However, such works are often limited by single feature cue, which does not capture the 3D shape of objects well. To address this problem, an end-to-end 3D reconstruction approach that predicts 3D point cloud from dual-view RGB images is proposed in this paper. It consists of several processing parts. A dual-view 3D reconstruction network is proposed for 3D reconstruction, which predicts object’s point clouds by exploiting two RGB images with different views, and avoids the limitation of single feature cue. Another structure feature learning network is performed to extract the structure features with stronger representation ability from point clouds. A gated control network for data fusion is proposed to gather point clouds. It takes two sets of point clouds with different views as input and fuses them. The proposed approach is thoroughly evaluated with extensive experiments on the widely-used ShapeNet dataset. Both the qualitative results and quantitative analysis demonstrate that this method not only captures the detailed geometric structures of 3D shapes for different object categories with complex topologies, but also achieves state-of-the-art performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300398",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Dual (grammatical number)",
      "Feature (linguistics)",
      "Geometry",
      "Law",
      "Linguistics",
      "Literature",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point (geometry)",
      "Point cloud",
      "Political science",
      "Politics",
      "RGB color model",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Jia",
        "given_name": "Xin"
      },
      {
        "surname": "Yang",
        "given_name": "Shourui"
      },
      {
        "surname": "Peng",
        "given_name": "Yuxin"
      },
      {
        "surname": "Zhang",
        "given_name": "Junchao"
      },
      {
        "surname": "Chen",
        "given_name": "Shengyong"
      }
    ]
  },
  {
    "title": "Thorax disease classification with attention guided convolutional neural network",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.040",
    "abstract": "This paper considers the task of thorax disease diagnosis on chest X-ray (CXR) images. Most existing methods generally learn a network with global images as input. However, thorax diseases usually happen in (small) localized areas which are disease specific. Thus training CNNs using global images may be affected by the (excessive) irrelevant noisy areas. Besides, due to the poor alignment of some CXR images, the existence of irregular borders hinders the network performance. For addressing the above problems, we propose to integrate the global and local cues into a three-branch attention guided convolution neural network (AG-CNN) to identify thorax diseases. An attention guided mask inference based cropping strategy is proposed to avoid noise and improve alignment in the global branch. AG-CNN also integrates the global cues to compensate the lost discriminative cues by the local branch. Specifically, we first learn a global CNN branch using global images. Then, guided by the attention heatmap generated from the global branch, we infer a mask to crop a discriminative region from the global image. The local region is used for training a local CNN branch. Lastly, we concatenate the last pooling layers of both the global and local branches for fine-tuning the fusion branch. Experiments on the ChestX-ray14 dataset demonstrate that after integrating the local cues with the global information, the average AUC scores are improved by AG-CNN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303617",
    "keywords": [
      "Anatomy",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Discriminative model",
      "Inference",
      "Machine learning",
      "Medicine",
      "Pattern recognition (psychology)",
      "Pooling",
      "Thorax (insect anatomy)"
    ],
    "authors": [
      {
        "surname": "Guan",
        "given_name": "Qingji"
      },
      {
        "surname": "Huang",
        "given_name": "Yaping"
      },
      {
        "surname": "Zhong",
        "given_name": "Zhun"
      },
      {
        "surname": "Zheng",
        "given_name": "Zhedong"
      },
      {
        "surname": "Zheng",
        "given_name": "Liang"
      },
      {
        "surname": "Yang",
        "given_name": "Yi"
      }
    ]
  },
  {
    "title": "Integrated design of deep features fusion for localization and classification of skin cancer",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.042",
    "abstract": "The common fatal type of skin cancer is melanoma. Recently, numerous intelligent systems are used to detect skin cancer at an early stage. These systems are helpful for a dermatologist as a preliminary judgment to diagnose skin cancer. However, accurate skin lesion detection is an intricate task. This work comprises three main phases, firstly perform preprocessing to resize the images to 240 × 240 × 3 and convert RGB into L^* a^* b^* in which the luminance channel is selected. Secondly, Biorthogonal 2-D wavelet transform, Otsu algorithm are used to segment the skin lesion. Thirdly, deep features extracted from pre-trained Alex net and VGG16 and serially fused. The applied PCA for optimal features selection for classification into benign and malignant. The publically available datasets (PH2, ISBI 2016- 2017) are merged to form a single large dataset for the validated of proposed method. The results comparison is performed with the existing work which confirms that the proposed method classifies the skin lesion more accurately.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303630",
    "keywords": [
      "Artificial intelligence",
      "Cancer",
      "Computer science",
      "Deep learning",
      "Dermatology",
      "Internal medicine",
      "Medicine",
      "Pattern recognition (psychology)",
      "Preprocessor",
      "RGB color model",
      "Skin cancer",
      "Skin lesion"
    ],
    "authors": [
      {
        "surname": "Amin",
        "given_name": "Javeria"
      },
      {
        "surname": "Sharif",
        "given_name": "Abida"
      },
      {
        "surname": "Gul",
        "given_name": "Nadia"
      },
      {
        "surname": "Anjum",
        "given_name": "Muhammad Almas"
      },
      {
        "surname": "Nisar",
        "given_name": "Muhammad Wasif"
      },
      {
        "surname": "Azam",
        "given_name": "Faisal"
      },
      {
        "surname": "Bukhari",
        "given_name": "Syed Ahmad Chan"
      }
    ]
  },
  {
    "title": "Estimation of ergodicity limits of bag-of-words modeling for guaranteed stochastic convergence",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107094",
    "abstract": "This paper suggests an efficient dual ergodicity limits-based bag-of-words (DEL-BoW) modeling technique. The suggested DEL-BoW technique estimates two limits of ergodicity of a discrete random variable (drv) that is formed from the BoW classification performance of multiple runs. The first limit of ergodicity is estimated with a relatively larger ball of convergence to keep the drv shorter. Hence both robustness against random initialization and estimation of the optimal model-order are realized with a reduced number of iterations. Once the optimal model-order is estimated, the radius of ball of convergence is reduced and a second limit of ergodicity is estimated. Reducing the ball of convergence enlarges the size of the considered performance drv that enhances the classification performance. Experiments conducted on Caltech-101, Caltech-256, 15-Scenes, and Flower-102 datasets resulted in classification accuracy of 86.91%, 72.57%, 90.57%, and 90.86%, respectively. Comparison with state-of-the-art techniques shows the excellent performance of the DEL-BoW modeling process.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303954",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Ball (mathematics)",
      "Biochemistry",
      "Channel (broadcasting)",
      "Chemistry",
      "Computer network",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Ergodicity",
      "Gene",
      "Initialization",
      "Mathematical analysis",
      "Mathematics",
      "Physics",
      "Programming language",
      "Rate of convergence",
      "Robustness (evolution)",
      "Statistical physics",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Ghalyan",
        "given_name": "Ibrahim F. Jasim"
      }
    ]
  },
  {
    "title": "Challenges in automatic Munsell color profiling for cultural heritage",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.12.008",
    "abstract": "Color specification is the process of measuring the color of a sample in a given color space. We focused onto the Munsell color space as archaeologists are used to employ the so called Munsell Soil Color Charts (MSCCs) directly in the excavation sites. For these scholars and researchers, being enabled to perform Munsell color specification in an automatic way is crucial, as they spend a lot of time to subjectively specify colors in the Munsell system. We extended the dataset ARCA328, which was specifically thought for the automatic Munsell color specification issue, increasing the number of images from 328 to 1,488, and the number of samples from 56,160 to 315,333. Then, we conducted generalization-tests of color conversion for color specification, adopting a classification approach instead of a regression one. This choice was motivated by the fact that the set of all the possible HVC coordinates in the MSCCs is a discrete one. Hence, we decided to consider each chip in the MSCCs as a class to be learnt and recognized by the SVC. With these tests we highligthed the limits of automatic Munsell color specification without any reference-system or calibration phase. Finally, we gave insights for future works aimed to design automatic illuminant calibration phase and to investigate deep learning approaches, leveraging a synthetic images rendering procedure we also present in this work.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786551930371X",
    "keywords": [
      "Artificial intelligence",
      "Color image",
      "Color model",
      "Color quantization",
      "Color rendering index",
      "Color space",
      "Computer science",
      "Computer vision",
      "ICC profile",
      "Image (mathematics)",
      "Image processing",
      "Optoelectronics",
      "Pattern recognition (psychology)",
      "Phosphor",
      "Physics",
      "Rendering (computer graphics)",
      "Standard illuminant"
    ],
    "authors": [
      {
        "surname": "Milotta",
        "given_name": "Filippo Luigi Maria"
      },
      {
        "surname": "Furnari",
        "given_name": "Giuseppe"
      },
      {
        "surname": "Quattrocchi",
        "given_name": "Camillo"
      },
      {
        "surname": "Pasquale",
        "given_name": "Stefania"
      },
      {
        "surname": "Allegra",
        "given_name": "Dario"
      },
      {
        "surname": "Gueli",
        "given_name": "Anna Maria"
      },
      {
        "surname": "Stanco",
        "given_name": "Filippo"
      },
      {
        "surname": "Tanasi",
        "given_name": "Davide"
      }
    ]
  },
  {
    "title": "Approximating lower-star persistence via 2D combinatorial map simplification",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.01.018",
    "abstract": "Filtration simplification consists of simplifying a given filtration while simultaneously controlling the perturbation in the associated persistence diagrams. In this paper, we propose a filtration simplification algorithm for orientable 2-dimensional (2D) manifolds with or without boundary (meshes) represented by 2D combinatorial maps. Given a lower-star filtration of the mesh, faces are added into contiguous clusters according to a “height” function and a parameter ϵ. Faces in the same cluster are merged into a single face, resulting in a lower resolution mesh and a simpler filtration. We prove that the parameter ϵ bounds the perturbation in the original persistence diagrams, and we provide experiments demonstrating the computational advantages of the simplification process.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300179",
    "keywords": [
      "Algorithm",
      "Biology",
      "Boundary (topology)",
      "Cluster (spacecraft)",
      "Combinatorics",
      "Computer science",
      "Evolutionary biology",
      "Filtration (mathematics)",
      "Function (biology)",
      "Geometry",
      "Mathematical analysis",
      "Mathematics",
      "Perturbation (astronomy)",
      "Physics",
      "Polygon mesh",
      "Programming language",
      "Pure mathematics",
      "Quantum mechanics",
      "Star (game theory)"
    ],
    "authors": [
      {
        "surname": "Damiand",
        "given_name": "Guillaume"
      },
      {
        "surname": "Paluzo-Hidalgo",
        "given_name": "Eduardo"
      },
      {
        "surname": "Slechta",
        "given_name": "Ryan"
      },
      {
        "surname": "Gonzalez-Diaz",
        "given_name": "Rocio"
      }
    ]
  },
  {
    "title": "Multi-model ensemble with rich spatial information for object detection",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107098",
    "abstract": "Due to the development of deep learning networks and big data dimensionality, research on ensemble deep learning is receiving an increasing amount of attention. This paper takes the object detection task as the research domain and proposes an object detection framework based on ensemble deep learning. To guarantee the accuracy as well as real-time detection, the detector uses a Single Shot MultiBox Detector (SSD) as the backbone and combines ensemble learning with context modeling and multi-scale feature representation. Two modes were designed in order to achieve ensemble learning: NMS Ensembling and Feature Ensembling. In addition, to obtain contextual information, we used dilated convolution to expand the receptive field of the network. Compared with state-of-the-art detectors, our detector achieves superior performance on the PASCAL VOC set and the MS COCO set.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303991",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Curse of dimensionality",
      "Deep learning",
      "Detector",
      "Ensemble learning",
      "Feature learning",
      "Machine learning",
      "Object detection",
      "Paleontology",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Jie"
      },
      {
        "surname": "Wang",
        "given_name": "Wei"
      },
      {
        "surname": "Wang",
        "given_name": "Hanyuan"
      },
      {
        "surname": "Guo",
        "given_name": "Jinhong"
      }
    ]
  },
  {
    "title": "Collaborative and geometric multi-kernel learning for multi-class classification",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107050",
    "abstract": "The multi-class classification is the problem of classifying the sample into one of three or more classes. In this paper, we propose an algorithm named collaborative and geometric multi-kernel learning (CGMKL) to classify multi-class data into corresponding class directly. The CGMKL uses the Multiple Empirical Kernel Learning (MEKL) to map the sample into multiple kernel spaces, and then trains the softmax function in each kernel space. To realize the collaborative learning, one regularization term, which controls the consistent outputs of samples in different kernel spaces, provides the complementary information. Moreover, another regularization term exhibits the classification result with a geometric feature by reducing the within-class distance of the outputs of samples. Extensive Experiments on the multi-class data sets validate the effectiveness of the CGMKL.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303528",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Class (philosophy)",
      "Computer science",
      "Discrete mathematics",
      "Graph kernel",
      "Kernel (algebra)",
      "Kernel embedding of distributions",
      "Kernel method",
      "Mathematics",
      "Multiple kernel learning",
      "Pattern recognition (psychology)",
      "Polynomial kernel",
      "Radial basis function kernel",
      "Softmax function",
      "Support vector machine",
      "Tree kernel",
      "Variable kernel density estimation"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zhe"
      },
      {
        "surname": "Zhu",
        "given_name": "Zonghai"
      },
      {
        "surname": "Li",
        "given_name": "Dongdong"
      }
    ]
  },
  {
    "title": "A dimension-reduction based multilayer perception method for supporting the medical decision making",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.026",
    "abstract": "Due to the rapid development of Medical IoT recently, how to effectively apply these huge amounts of IoT data to enhance the reliability of the clinical decision making has become an increasing issue in the medical field. These data usually comprise high-complicated features with tremendous volume, and it implies that the simple inference models may less powerful to be practiced. In deep learning, multilayer perceptron (MLP) is a kind of feed-forward artificial neural network, and it is one of the high-performance methods about stochastic scheme, fitness approximation, and regression analysis. To process these high uncertain data, the proposed work based on MLP structure in particular integrates the boosting scheme and dimension-reduction process. In this proposed work, the advanced ReLU-based activation function is used. Also, the weight initialization is applied to improve the stable prediction and convergence. After the improved dimension-reduction process is introduced, the proposed method can effectively learn the hidden information from the reformative data and the precise labels also can be recognized by stacking a small amount of neural network layers with paying few extra cost. The proposed work shows a possible path of embedding dimension reduction in deep learning structure with minor price. In addition to the prediction issue, the proposed method can also be applied to assess risk and forecast trend among different information systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303484",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Dimension (graph theory)",
      "Dimensionality reduction",
      "Geometry",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Pure mathematics",
      "Reduction (mathematics)"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Shin-Jye"
      },
      {
        "surname": "Tseng",
        "given_name": "Ching-Hsun"
      },
      {
        "surname": "Lin",
        "given_name": "G.T.–R."
      },
      {
        "surname": "Yang",
        "given_name": "Yun"
      },
      {
        "surname": "Yang",
        "given_name": "Po"
      },
      {
        "surname": "Muhammad",
        "given_name": "Khan"
      },
      {
        "surname": "Pandey",
        "given_name": "Hari Mohan"
      }
    ]
  },
  {
    "title": "Discovering Leonardo with artificial intelligence and holograms: A user study",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.01.006",
    "abstract": "Cutting-edge visualization and interaction technologies are increasingly used in museum exhibitions, providing novel ways to engage visitors and enhance their cultural experience. Existing applications are commonly built upon a single technology, focusing on visualization, motion or verbal interaction (e.g., high-resolution projections, gesture interfaces, chatbots). This aspect limits their potential, since museums are highly heterogeneous in terms of visitors profiles and interests, requiring multi-channel, customizable interaction modalities. To this aim, this work describes and evaluates an artificial intelligence powered, interactive holographic stand aimed at describing Leonardo Da Vinci’s art. This system provides the users with accurate 3D representations of Leonardo’s machines, which can be interactively manipulated through a touchless user interface. It is also able to dialog with the users in natural language about Leonardo’s art, while keeping the context of conversation and interactions. Furthermore, the results of a large user study, carried out during art and tech exhibitions, are presented and discussed. The goal was to assess how users of different ages and interests perceive, understand and explore cultural objects when holograms and artificial intelligence are used as instruments of knowledge and analysis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300039",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Conversation",
      "Dialog box",
      "Exhibition",
      "Gesture",
      "Human–computer interaction",
      "Linguistics",
      "Modalities",
      "Multimedia",
      "Operating system",
      "Paleontology",
      "Philosophy",
      "Social science",
      "Sociology",
      "User interface",
      "Visual arts",
      "Visualization",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Caggianese",
        "given_name": "Giuseppe"
      },
      {
        "surname": "De Pietro",
        "given_name": "Giuseppe"
      },
      {
        "surname": "Esposito",
        "given_name": "Massimo"
      },
      {
        "surname": "Gallo",
        "given_name": "Luigi"
      },
      {
        "surname": "Minutolo",
        "given_name": "Aniello"
      },
      {
        "surname": "Neroni",
        "given_name": "Pietro"
      }
    ]
  },
  {
    "title": "Hierarchical segmentation from a non-increasing edge observation attribute",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.12.014",
    "abstract": "Hierarchical image segmentation provides region-oriented scale-spaces: sets of image segmentations at different detail levels in which the segmentations at finer levels are nested with respect to those at coarser levels. Guimarães et al. proposed a hierarchical graph-based image segmentation (HGB) method based on the Felzenszwalb-Huttenlocher dissimilarity. It computes, for each edge of a graph, the minimum scale in a hierarchy at which two regions linked by this edge should be merged according to the dissimilarity. We provide an explicit definition of the (edge-) observation attribute and Boolean criterion which are at the basis of this method and show that they are not increasing. Then, we propose an algorithm to compute all the scales for which the criterion holds true. Finally, we propose new methods to regularize the observation attribute and criterion and to set up the observation scale value of each edge of a graph, following the current trend in mathematical morphology to study criteria which are not increasing on a hierarchy. Assessments on Pascal VOC 2010 and 2012 show that these strategies lead to better segmentation results than the ones obtained with the original HGB method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303770",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Economics",
      "Enhanced Data Rates for GSM Evolution",
      "Graph",
      "Hierarchy",
      "Image segmentation",
      "Market economy",
      "Mathematics",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Scale (ratio)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Cayllahua-Cahuina",
        "given_name": "Edward"
      },
      {
        "surname": "Cousty",
        "given_name": "Jean"
      },
      {
        "surname": "Guimarães",
        "given_name": "Silvio Jamil F."
      },
      {
        "surname": "Kenmochi",
        "given_name": "Yukiko"
      },
      {
        "surname": "Cámara-Chávez",
        "given_name": "Guillermo"
      },
      {
        "surname": "de Albuquerque Araújo",
        "given_name": "Arnaldo"
      }
    ]
  },
  {
    "title": "Multi-scale Gated Fully Convolutional DenseNets for semantic labeling of historical newspaper images",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.01.026",
    "abstract": "Historical newspaper image analysis is a challenging task due to the complex layout of newspapers and its variability among collections. While traditional approaches are rule-based methods with many successive steps, recent works show that deep learning approaches can be successfully used to provide a pixel labeling of the various fields occurring in a page. This allows the automatic extraction of the document structure and accessing the different semantic entities. Recent improvements proposed to strengthen convolutional neural network capacities such as gated mechanism may also apply well to to task at end. In this respect, we propose a fully convolutional neural network architecture (FCN) that outputs a pixel-labeling of the various semantic entities that occur in historical newspaper images. Our model is based on a novel Multi-Scale Gated Block architecture (MSGB), made of dense connections and gating mechanisms that handle a multi-scale analysis of the input image with self-attention. Evaluations conducted on 4 historical newspaper datasets including up to 11 semantic classes show that our proposition outperforms standard FCN architectures.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300374",
    "keywords": [
      "Advertising",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Business",
      "Cartography",
      "Computer science",
      "Convolutional neural network",
      "Economics",
      "Geography",
      "Geometry",
      "Information retrieval",
      "Management",
      "Mathematics",
      "Natural language processing",
      "Newspaper",
      "Pattern recognition (psychology)",
      "Pixel",
      "Scale (ratio)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Soullard",
        "given_name": "Yann"
      },
      {
        "surname": "Tranouez",
        "given_name": "Pierrick"
      },
      {
        "surname": "Chatelain",
        "given_name": "Clément"
      },
      {
        "surname": "Nicolas",
        "given_name": "Stéphane"
      },
      {
        "surname": "Paquet",
        "given_name": "Thierry"
      }
    ]
  },
  {
    "title": "Variance-preserving deep metric learning for content-based image retrieval",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.041",
    "abstract": "Supervised deep metric learning led to spectacular results for several Content-based Information Retrieval (CBIR) applications. The success of these approaches slowly led to the belief that image retrieval and classification are just slightly different variations of the same problem. However, recent evidence suggests that learning highly discriminative representation for a (limited) set of training classes removes valuable information from the representation, potentially harming both the in-domain, as well as the out-of-domain retrieval precision. In this paper, we propose a regularized discriminative deep metric learning method that aims to not only learn a representation that allows for discriminating between different classes, but it is also capable of encoding the latent generative factors separately for each class, overcoming this limitation. This allows for modeling the in-class variance and, as a result, maintaining the ability to represent both sub-classes of the in-domain data, as well as objects that belong to classes outside the training domain. The effectiveness of the proposed method, over existing supervised and unsupervised representation/metric learning approaches, is demonstrated under different in-domain and out-of-domain setups and three challenging image datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303629",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Deep learning",
      "Discriminative model",
      "Domain (mathematical analysis)",
      "Economics",
      "Feature learning",
      "Image (mathematics)",
      "Image retrieval",
      "Law",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Semi-supervised learning",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Passalis",
        "given_name": "Nikolaos"
      },
      {
        "surname": "Iosifidis",
        "given_name": "Alexandros"
      },
      {
        "surname": "Gabbouj",
        "given_name": "Moncef"
      },
      {
        "surname": "Tefas",
        "given_name": "Anastasios"
      }
    ]
  },
  {
    "title": "End-to-end video subtitle recognition via a deep Residual Neural Network",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.01.019",
    "abstract": "Video subtitle recognition can substantially facilitate a wide range of applications like automatic video retrieval and summarization. However, current methods face great challenges in video subtitle recognition, due to complex backgrounds, diverse fonts, and low contrast between texts and backgrounds. In this paper, we propose an end-to-end pipeline for recognizing subtitles of video. Connectionist Text Proposal Network (CTPN) is utilized for video subtitle detection, while Residual Network (ResNet), Gated Recurrent Unit (GRU) and Connectionist Temporal Classification (CTC) are used to recognize Chinese and English subtitles in video images. Specifically, the subtitle area in video images is firstly located via the CTPN method. Afterwards, the detected subtitle area is inputted into the ResNet for extracting the feature sequences. Next, a bidirectional GRU layer is employed to model the feature sequences. Finally, CTC is adopted to calculate the loss and output the final result. On two public datasets ICDAR2003 and ICDAR2013, the proposed method can get better performance with 92.3% and 89.2% recognition accuracy in all of the current methods. In addition, experiments on real video subtitle have also proved that the proposed method achieves the best performance in the state-of-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300313",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Automatic summarization",
      "Computer science",
      "Computer vision",
      "Connectionism",
      "Feature (linguistics)",
      "Linguistics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Residual",
      "Speech recognition",
      "Subtitle"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Hongyu"
      },
      {
        "surname": "Xu",
        "given_name": "Xin"
      }
    ]
  },
  {
    "title": "Deep reinforcement hashing with redundancy elimination for effective image retrieval",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107116",
    "abstract": "Hashing is one of the most promising techniques in approximate nearest neighbor search due to its time efficiency and low cost in memory. Recently, with the help of deep learning, deep supervised hashing can perform representation learning and compact hash code learning jointly in an end-to-end style, and obtains better retrieval accuracy compared to non-deep methods. However, most deep hashing methods are trained with a pair-wise loss or triplet loss in a mini-batch style, which makes them inefficient at data sampling and cannot preserve the global similarity information. Besides that, many existing methods generate hash codes with redundant or even harmful bits, which is a waste of space and may lower the retrieval accuracy. In this paper, we propose a novel deep reinforcement hashing model with redundancy elimination called Deep Reinforcement De-Redundancy Hashing (DRDH), which can fully exploit large-scale similarity information and eliminate redundant hash bits with deep reinforcement learning. DRDH conducts hash code inference in a block-wise style, and uses Deep Q Network (DQN) to eliminate redundant bits. Very promising results have been achieved on four public datasets, i.e., CIFAR-10, NUS-WIDE, MS-COCO, and Open-Images-V4, which demonstrate that our method can generate highly compact hash codes and yield better retrieval performance than those of state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304170",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Deep learning",
      "Double hashing",
      "Dynamic perfect hashing",
      "Feature hashing",
      "Hash function",
      "Hash table",
      "Image (mathematics)",
      "Image retrieval",
      "Operating system",
      "Pattern recognition (psychology)",
      "Redundancy (engineering)",
      "Reinforcement learning"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Juexu"
      },
      {
        "surname": "Zhang",
        "given_name": "Yuejie"
      },
      {
        "surname": "Feng",
        "given_name": "Rui"
      },
      {
        "surname": "Zhang",
        "given_name": "Tao"
      },
      {
        "surname": "Fan",
        "given_name": "Weiguo"
      }
    ]
  },
  {
    "title": "Connectivity-based cylinder detection in unorganized point clouds",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107161",
    "abstract": "Cylinder detection is an important step in reverse engineering of industrial sites, as such environments often contain a large number of cylindrical pipes and tanks. However, existing techniques for cylinder detection require the specification of several parameters which are difficult to adjust because their values depend on the noise level of the input point cloud. Also, these solutions often expect the cylinders to be either parallel or perpendicular to the ground. We present a cylinder-detection technique that is robust to noise, contains parameters which require little to no fine-tuning, and can handle cylinders with arbitrary orientations. Our approach is based on a robust linear-time circle-detection algorithm that naturally discards outliers, allowing our technique to handle datasets with various density and noise levels while using a set of default parameter values. It works by projecting the point cloud onto a set of directions over the unit hemisphere and detecting circular projections formed by samples defining connected components in 3D. The extracted cylindrical surfaces are obtained by fitting a cylinder to each connected component. We compared our technique against the state-of-the-art methods on both synthetic and real datasets containing various densities and noise levels, and show that it outperforms existing techniques in terms of accuracy and robustness to noise, while still maintaining a competitive running time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304613",
    "keywords": [
      "Algorithm",
      "Anomaly detection",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Cylinder",
      "Gene",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Noise (video)",
      "Outlier",
      "Perpendicular",
      "Point (geometry)",
      "Point cloud",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Araújo",
        "given_name": "Abner M.C."
      },
      {
        "surname": "Oliveira",
        "given_name": "Manuel M."
      }
    ]
  },
  {
    "title": "Deformable face net for pose invariant face recognition",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107113",
    "abstract": "Unconstrained face recognition still remains a challenging task due to various factors such as pose, expression, illumination, partial occlusion, etc. In particular, the most significant appearance variations are stemmed from poses which leads to severe performance degeneration. In this paper, we propose a novel Deformable Face Net (DFN) to handle the pose variations for face recognition. The deformable convolution module attempts to simultaneously learn face recognition oriented alignment and identity-preserving feature extraction. The displacement consistency loss (DCL) is proposed as a regularization term to enforce the learnt displacement fields for aligning faces to be locally consistent both in the orientation and amplitude since faces possess strong structure. Moreover, the identity consistency loss (ICL) and the pose-triplet loss (PTL) are designed to minimize the intra-class feature variation caused by different poses and maximize the inter-class feature distance under the same poses. The proposed DFN can effectively handle pose invariant face recognition (PIFR). Extensive experiments show that the proposed DFN outperforms the state-of-the-art methods, especially on the datasets with large poses.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304145",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Face (sociological concept)",
      "Face detection",
      "Facial recognition system",
      "Geometry",
      "Invariant (physics)",
      "Mathematical physics",
      "Mathematics",
      "Net (polyhedron)",
      "Pattern recognition (psychology)",
      "Social science",
      "Sociology",
      "Three-dimensional face recognition"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Mingjie"
      },
      {
        "surname": "Zhang",
        "given_name": "Jie"
      },
      {
        "surname": "Shan",
        "given_name": "Shiguang"
      },
      {
        "surname": "Kan",
        "given_name": "Meina"
      },
      {
        "surname": "Chen",
        "given_name": "Xilin"
      }
    ]
  },
  {
    "title": "Movie fill in the blank by joint learning from video and text with adaptive temporal attention",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.06.030",
    "abstract": "Video understanding is a challenging problem and it attracts a lot of research attention. Lately, a new task called movie fill in the blank (MovieFIB) is proposed. In this task, given a movie clip and a description which has one blank, we need to predict the word in the blank accurately. Previous studies make many contributions to tackling this problem. However, some of them do not utilize the relationship between words and video frames, and some others treat visual information as essential elements for blank word prediction, which fail to distinguish the effects of texts before and after the blank. To overcome the limitations, in this paper we propose to use adaptive temporal attention and fuse text information with attention. We first extract video and word features. Then, adaptive temporal attention is used to update original description. For the updated description, we extract its text information. Attention mechanism is applied to fuse text information. Finally, we use adaptive temporal attention to predict the blank word. Extensive experiments demonstrate that our model achieves satisfactory performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518302794",
    "keywords": [
      "Artificial intelligence",
      "Blank",
      "Computer science",
      "Economics",
      "Electrical engineering",
      "Engineering",
      "Fuse (electrical)",
      "Linguistics",
      "Management",
      "Mechanical engineering",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Task (project management)",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Jie"
      },
      {
        "surname": "Shao",
        "given_name": "Jie"
      },
      {
        "surname": "He",
        "given_name": "Chengkun"
      }
    ]
  },
  {
    "title": "Dynamic imposter based online instance matching for person search",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107120",
    "abstract": "Person search aims to locate the target person matching a given query from a list of unconstrained whole images. It is a challenging task due to the unavailable bounding boxes of pedestrians, limited samples for each labeled identity and large amount of unlabeled persons in existing datasets. To address these issues, we propose a novel end-to-end learning framework for person search. The proposed framework settles pedestrian detection and person re-identification concurrently. To achieve the goal of co-learning and utilize the information of unlabeled persons, a novel yet extremely efficient Dynamic Imposter based Online Instance Matching (DI-OIM) loss is formulated. The DI-OIM loss is inspired by the observation that pedestrians appearing in the same image obviously have different identities. Thus we assign the unlabeled persons with dynamic pseudo-labels. The pseudo-labeled persons along with the labeled persons can be used to learn powerful feature representations. Experiments on CUHK-SYSU and PRW datasets demonstrate that our method outperforms other state-of-the-art algorithms. Moreover, it is superior and efficient in terms of memory capacity comparing with existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304212",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Bounding overwatch",
      "Computer science",
      "Economics",
      "Feature (linguistics)",
      "Feature matching",
      "Identification (biology)",
      "Identity (music)",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Management",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Statistics",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Dai",
        "given_name": "Ju"
      },
      {
        "surname": "Zhang",
        "given_name": "Pingping"
      },
      {
        "surname": "Lu",
        "given_name": "Huchuan"
      },
      {
        "surname": "Wang",
        "given_name": "Hongyu"
      }
    ]
  },
  {
    "title": "UNIC: A fast nonparametric clustering",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107117",
    "abstract": "Clustering is among the tools for exploring, analyzing, and deriving information from data. In the case of large data sets, the real burden to the application of clustering algorithms can be their complexity and demand of control parameters. We present a new fast nonparametric clustering algorithm, UNIC, to address these challenges. To identify clusters, the algorithm evaluates the distances between selected points and other points in the set. While assessing these distances, it employs methods of robust statistics to identify the cluster borders. The performance of the proposed algorithm is assessed in an experimental study and compared with several existing clustering methods over a variety of benchmark data sets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304182",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "CURE data clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Data set",
      "Geodesy",
      "Geography",
      "Mathematics",
      "Nonparametric statistics",
      "Programming language",
      "Set (abstract data type)",
      "Single-linkage clustering",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Leopold",
        "given_name": "Nadiia"
      },
      {
        "surname": "Rose",
        "given_name": "Oliver"
      }
    ]
  },
  {
    "title": "Complex Contourlet-CNN for polarimetric SAR image classification",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107110",
    "abstract": "A complex multiscale network named complex Contourlet convolutional neural network (complex Contourlet-CNN) is proposed for polarimetric synthetic aperture radar (PolSAR) image classification in this paper. In order to make full use of the phase information of PolSAR image, we redefine the conventional operations of CNN in complex field, and the data sets and parameters are always expressed through the complex matrixes in complex CNN. Moreover, the multiscale deep Contourlet filter banks are constructed to extract more robust discriminative features with multidirection, multiscale, and multiresolution properties, which can improve the performance of complex CNN by replacing filters of the first complex convolutional layer with the multiscale deep Contourlet filter banks. The Contourlet transform is used for helping the complex CNN network to capture abstract features in a certain direction and frequency band. Furthermore, the proposed network based on the multiscale geometric properties of Contourlet transform can retrieve the information in the region and direction corresponding to the extracted features. Experiments on different spatial resolutions and land coverings of Flevoland, San Francisco Bay, and Germany PolSAR images show that less training data is required and the performance of the proposed explainable deep learning method is comparable to that of the existing state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132031930411X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Contourlet",
      "Convolutional neural network",
      "Deep learning",
      "Discriminative model",
      "Filter (signal processing)",
      "Pattern recognition (psychology)",
      "Softmax function",
      "Synthetic aperture radar",
      "Wavelet",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Lingling"
      },
      {
        "surname": "Ma",
        "given_name": "Liyuan"
      },
      {
        "surname": "Jiao",
        "given_name": "Licheng"
      },
      {
        "surname": "Liu",
        "given_name": "Fang"
      },
      {
        "surname": "Sun",
        "given_name": "Qigong"
      },
      {
        "surname": "Zhao",
        "given_name": "Jin"
      }
    ]
  },
  {
    "title": "ExprADA: Adversarial domain adaptation for facial expression analysis",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107111",
    "abstract": "We propose a deep neural network based image-to-image translation for domain adaptation, which aims at finding translations between image domains. Despite recent GAN based methods showing promising results in image-to-image translation, they are prone to fail at preserving semantic information and maintaining image details during translation, which reduces their practicality on tasks such as facial expression synthesis. In this paper, we learn a framework with two training objectives: first, we propose a multi-domain image synthesis model, yielding a better recognition performance compared to other GAN based methods, with a focus on the data augmentation process; second, we explore the use of domain adaptation to transform the visual appearance of the images from different domains, with the detail of face characteristics (e.g., identity) well preserved. Doing so, the expression recognition model learned from the source domain can be generalized to the translated images from target domain, without the need for re-training a model for new target domain. Extensive experiments demonstrate that ExprADA shows significant improvements in facial expression recognition accuracy compared to state-of-the-art domain adaptation methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304121",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Expression (computer science)",
      "Face (sociological concept)",
      "Facial expression",
      "Facial recognition system",
      "Focus (optics)",
      "Gene",
      "Image (mathematics)",
      "Image translation",
      "Mathematical analysis",
      "Mathematics",
      "Messenger RNA",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Process (computing)",
      "Programming language",
      "Social science",
      "Sociology",
      "Translation (biology)"
    ],
    "authors": [
      {
        "surname": "Bozorgtabar",
        "given_name": "Behzad"
      },
      {
        "surname": "Mahapatra",
        "given_name": "Dwarikanath"
      },
      {
        "surname": "Thiran",
        "given_name": "Jean-Philippe"
      }
    ]
  },
  {
    "title": "Multi-task learning using variational auto-encoder for sentiment classification",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.06.027",
    "abstract": "With the rapid growth of the big data, many approaches in the representation of text for sentiment classification have been successfully proposed in natural language processing. However, these approaches remedy this problem based on single-task supervised objectives learning and do not consider their relative of multiple tasks. Based on these defects, in this work, we consider these tasks are relative, and use weight-shared parameters for learning the representation of text in neural network model, we introduce and study a multi-task approach with variational auto-encoder generative model (MTVAE) by jointly learning them. Experimental results on six subsets of Amazon review data show that the proposed approach can effectively improve the sentiment classification accuracy by other relative tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518302769",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Computer science",
      "Deep learning",
      "Economics",
      "Encoder",
      "External Data Representation",
      "Generative grammar",
      "Generative model",
      "Law",
      "Machine learning",
      "Management",
      "Multi-task learning",
      "Operating system",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Sentiment analysis",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Guangquan"
      },
      {
        "surname": "Zhao",
        "given_name": "Xishun"
      },
      {
        "surname": "Yin",
        "given_name": "Jian"
      },
      {
        "surname": "Yang",
        "given_name": "Weiwei"
      },
      {
        "surname": "Li",
        "given_name": "Bo"
      }
    ]
  },
  {
    "title": "Robust rigid registration algorithm based on pointwise correspondence and correntropy",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.06.028",
    "abstract": "The iterative closest point (ICP) algorithm is fast and accurate for rigid point set registration, but it works badly when handling noisy data or point clouds with outliers. This paper instead proposes a novel method based on the ICP algorithm to deal with this problem. Firstly, correntropy is introduced into the rigid registration problem which could handle noises and outliers well, and then a new energy function based on maximum correntropy criterion is proposed. After that, a new ICP algorithm based on correntropy is proposed, which performs well in dealing with rigid registration with noises and outliers. This new algorithm converges monotonically from any given parameters, which is similar to the ICP algorithm. Experimental results demonstrate its accuracy and robustness compared with the traditional ICP algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518302770",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Gene",
      "Iterative closest point",
      "Mathematical analysis",
      "Mathematics",
      "Monotonic function",
      "Outlier",
      "Point cloud",
      "Pointwise",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Du",
        "given_name": "Shaoyi"
      },
      {
        "surname": "Xu",
        "given_name": "Guanglin"
      },
      {
        "surname": "Zhang",
        "given_name": "Sirui"
      },
      {
        "surname": "Zhang",
        "given_name": "Xuetao"
      },
      {
        "surname": "Gao",
        "given_name": "Yue"
      },
      {
        "surname": "Chen",
        "given_name": "Badong"
      }
    ]
  },
  {
    "title": "Locality-constrained affine subspace coding for image classification and retrieval",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107167",
    "abstract": "Feature coding is a key component of the bag of visual words (BoVW) model, which is designed to improve image classification and retrieval performance. In the feature coding process, each feature of an image is nonlinearly mapped via a dictionary of visual words to form a high-dimensional sparse vector. Inspired by the well-known locality-constrained linear coding (LLC), we present a locality-constrained affine subspace coding (LASC) method to address the limitation whereby LLC fails to consider the local geometric structure around visual words. LASC is distinguished from all the other coding methods since it constructs a dictionary consisting of an ensemble of affine subspaces. As such, the local geometric structure of a manifold is explicitly modeled by such a dictionary. In the process of coding, each feature is linearly decomposed and weighted to form the first-order LASC vector with respect to its top-k neighboring subspaces. To further boost performance, we propose the second-order LASC vector based on information geometry. We use the proposed coding method to perform both image classification and image retrieval tasks and the experimental results show that the method achieves superior or competitive performance in comparison to state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304674",
    "keywords": [
      "Affine transformation",
      "Artificial intelligence",
      "Coding (social sciences)",
      "Computer science",
      "Feature vector",
      "Geometry",
      "Image (mathematics)",
      "Image retrieval",
      "Linear subspace",
      "Linguistics",
      "Locality",
      "Mathematics",
      "Neural coding",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pure mathematics",
      "Statistics",
      "Subspace topology",
      "Visual Word"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Bingbing"
      },
      {
        "surname": "Wang",
        "given_name": "Qilong"
      },
      {
        "surname": "Lu",
        "given_name": "Xiaoxiao"
      },
      {
        "surname": "Wang",
        "given_name": "Fasheng"
      },
      {
        "surname": "Li",
        "given_name": "Peihua"
      }
    ]
  },
  {
    "title": "Omni-supervised joint detection and pose estimation for wild animals",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.11.002",
    "abstract": "Monitoring wildlife populations and activities have significance for biology and ecology. With the rapid development of computer vision and deep learning techniques, it is possible to employ state-of-the-art convoluntional neural network (CNN) based detectors to process the big data from field surveillance cameras and assist in the following studies. However, data labelling during the training stage is a very time-consuming, labour intensive and expensive task. In this paper, we detect multiple animals (Kangaroo, emu, dingo, bird and wildcat) in the wild in an Omni-supervised learning setting. The unlabeled data from the surveillance cameras will be filtered and used for training via data distillation approach. Moreover, we also perform joint pose estimation and detection for Kangaroo which has the most samples in the dataset. To study the feasibility, we also built a large full high definition (HD) wild animal surveillance image dataset from collected data from several national parks across the State of Queensland in Australia and this dataset will be made publicly available. Extensive experiments show that the detection and pose estimation results can be further improved by using unlabeled data wisely.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518308742",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Pose",
      "Process (computing)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Teng"
      },
      {
        "surname": "Liu",
        "given_name": "Liangchen"
      },
      {
        "surname": "Zhao",
        "given_name": "Kun"
      },
      {
        "surname": "Wiliem",
        "given_name": "Arnold"
      },
      {
        "surname": "Hemson",
        "given_name": "Graham"
      },
      {
        "surname": "Lovell",
        "given_name": "Brian"
      }
    ]
  },
  {
    "title": "Systematic review of 3D facial expression recognition methods",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107108",
    "abstract": "The three-dimensional representation of the human face has emerged as a viable and effective way to characterize the facial surface for expression classification purposes. The rapid progress in the area continually demands its up-to-date characterization to guide and support research decisions, specially for newcomer researchers. This systematic literature review focus on investigating three major aspects of 3D facial expression recognition methods: face representation, preprocessing and classification experiments. The investigation of 49 specialized studies revealed the preferential types of data and regions of interest for face representation in recent years, as well as a trend towards keypoint-independent methods. In addition, it brings to light current weaknesses regarding the report of preprocessing techniques and identifies challenges concerning the current possibility of fair comparison among multiple methods. The presented findings outline essential research decisions whose the regardful report is of great value to this research community.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304091",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data pre-processing",
      "Data science",
      "Expression (computer science)",
      "Face (sociological concept)",
      "Facial expression",
      "Facial recognition system",
      "Focus (optics)",
      "Law",
      "Machine learning",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Preprocessor",
      "Programming language",
      "Psychology",
      "Representation (politics)",
      "Social psychology",
      "Social science",
      "Sociology",
      "Strengths and weaknesses"
    ],
    "authors": [
      {
        "surname": "Alexandre",
        "given_name": "Gilderlane Ribeiro"
      },
      {
        "surname": "Soares",
        "given_name": "José Marques"
      },
      {
        "surname": "Pereira Thé",
        "given_name": "George André"
      }
    ]
  },
  {
    "title": "Clustering social audiences in business information networks",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107126",
    "abstract": "Business information networks involve diverse users and rich content and have emerged as important platforms for enabling business intelligence and business decision making. A key step in an organizations business intelligence process is to cluster users with similar interests into social audiences and discover the roles they play within a business network. In this article, we propose a novel machine-learning approach, called CBIN, that co-clusters business information networks to discover and understand these audiences. The CBIN framework is based on co-factorization. The audience clusters are discovered from a combination of network structures and rich contextual information, such as node interactions and node-content correlations. Since what defines an audience cluster is data-driven, plus they often overlap, pre-determining the number of clusters is usually very difficult. Therefore, we have based CBIN on an overlapping clustering paradigm with a hold-out strategy to discover the optimal number of clusters given the underlying data. Experiments validate an outstanding performance by CBIN compared to other state-of-the-art algorithms on 13 real-world enterprise datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304273",
    "keywords": [
      "Artificial intelligence",
      "Business intelligence",
      "Cluster analysis",
      "Computer science",
      "Computer security",
      "Data mining",
      "Data science",
      "Engineering",
      "Key (lock)",
      "Node (physics)",
      "Operating system",
      "Process (computing)",
      "Structural engineering",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Yu"
      },
      {
        "surname": "Hu",
        "given_name": "Ruiqi"
      },
      {
        "surname": "Fung",
        "given_name": "Sai-fu"
      },
      {
        "surname": "Yu",
        "given_name": "Celina"
      },
      {
        "surname": "Long",
        "given_name": "Guodong"
      },
      {
        "surname": "Guo",
        "given_name": "Ting"
      },
      {
        "surname": "Pan",
        "given_name": "Shirui"
      }
    ]
  },
  {
    "title": "Sign consistency for the linear programming discriminant rule",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107083",
    "abstract": "Linear discriminant analysis (LDA) is an important conventional model for data classification. Classical theory shows that LDA is Bayes consistent for a fixed data dimensionality p and a large training sample size n. However, in high-dimensional settings when p ≫ n, LDA is difficult due to the inconsistent estimation of the covariance matrix and the mean vectors of populations. Recently, a linear programming discriminant (LPD) rule was proposed for high-dimensional linear discriminant analysis, based on the sparsity assumption over the discriminant function. It is shown that the LPD rule is Bayes consistent in high-dimensional settings. In this paper, we further show that the LPD rule is sign consistent under the sparsity assumption. Such sign consistency ensures the LPD rule to select the optimal discriminative features for high-dimensional data classification problems. Evaluations on both synthetic and real data validate our result on the sign consistency of the LPD rule.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132031930384X",
    "keywords": [
      "Artificial intelligence",
      "Bayes' theorem",
      "Bayesian probability",
      "Classification rule",
      "Computer science",
      "Consistency (knowledge bases)",
      "Covariance",
      "Curse of dimensionality",
      "Discriminant",
      "Discriminant function analysis",
      "Discriminative model",
      "Linear discriminant analysis",
      "Mathematical analysis",
      "Mathematics",
      "Optimal discriminant analysis",
      "Pattern recognition (psychology)",
      "Sign (mathematics)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zhen"
      },
      {
        "surname": "Wang",
        "given_name": "Shengzheng"
      },
      {
        "surname": "Bian",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Opinion fraud detection via neural autoencoder decision forest",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.07.013",
    "abstract": "Online reviews play an important role in influencing buyers’ daily purchase decisions. However, fake and meaningless reviews, which cannot reflect users’ genuine purchase experience and opinions, widely exist on the Web and pose great challenges for users to make right choices. Therefore, it is desirable to build a fair model that evaluates the quality of products by distinguishing spamming reviews. We present an end-to-end trainable unified model to leverage the appealing properties from Autoencoder and random forest. A stochastic decision tree model is implemented to guide the global parameter learning process. Extensive experiments were conducted on a large Amazon review dataset. The proposed model consistently outperforms a series of compared methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518303039",
    "keywords": [],
    "authors": [
      {
        "surname": "Dong",
        "given_name": "Manqing"
      },
      {
        "surname": "Yao",
        "given_name": "Lina"
      },
      {
        "surname": "Wang",
        "given_name": "Xianzhi"
      },
      {
        "surname": "Benatallah",
        "given_name": "Boualem"
      },
      {
        "surname": "Huang",
        "given_name": "Chaoran"
      },
      {
        "surname": "Ning",
        "given_name": "Xiaodong"
      }
    ]
  },
  {
    "title": "Multi-view predictive latent space learning",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.06.022",
    "abstract": "In unsupervised circumstances, multi-view learning seeks a shared latent representation by taking the consensus and complementary principles into account. However, most existing multi-view unsupervised learning approaches do not explicitly lay stress on the predictability of the latent space. In this paper, we propose a novel multi-view predictive latent space learning (MVP) model and apply it to multi-view clustering and unsupervised dimension reduction. The latent space is forced to be predictive by maximizing the correlation between the latent space and feature space of each view. By learning a multi-view graph with adaptive view-weight learning, MVP effectively combines the complementary information from multi-view data. Experimental results on benchmark datasets show that MVP outperforms the state-of-the-art multi-view clustering and unsupervised dimension reduction algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518302691",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Machine learning",
      "Operating system",
      "Space (punctuation)"
    ],
    "authors": [
      {
        "surname": "Yuan",
        "given_name": "Jirui"
      },
      {
        "surname": "Gao",
        "given_name": "Ke"
      },
      {
        "surname": "Zhu",
        "given_name": "Pengfei"
      },
      {
        "surname": "Egiazarian",
        "given_name": "Karen"
      }
    ]
  },
  {
    "title": "Ensemble Selection based on Classifier Prediction Confidence",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107104",
    "abstract": "Ensemble selection is one of the most studied topics in ensemble learning because a selected subset of base classifiers may perform better than the whole ensemble system. In recent years, a great many ensemble selection methods have been introduced. However, many of these lack flexibility: either a fixed subset of classifiers is pre-selected for all test samples (static approach), or the selection of classifiers depends upon the performance of techniques that define the region of competence (dynamic approach). In this paper, we propose an ensemble selection method that takes into account each base classifier's confidence during classification and the overall credibility of the base classifier in the ensemble. In other words, a base classifier is selected to predict for a test sample if the confidence in its prediction is higher than its credibility threshold. The credibility thresholds of the base classifiers are found by minimizing the empirical 0–1 loss on the entire training observations. In this way, our approach integrates both the static and dynamic aspects of ensemble selection. Experiments on 62 datasets demonstrate that the proposed method achieves much better performance in comparison to some ensemble methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304054",
    "keywords": [
      "Artificial intelligence",
      "Cascading classifiers",
      "Classifier (UML)",
      "Computer science",
      "Credibility",
      "Data mining",
      "Ensemble forecasting",
      "Ensemble learning",
      "Law",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Political science",
      "Random subspace method"
    ],
    "authors": [
      {
        "surname": "Nguyen",
        "given_name": "Tien Thanh"
      },
      {
        "surname": "Luong",
        "given_name": "Anh Vu"
      },
      {
        "surname": "Dang",
        "given_name": "Manh Truong"
      },
      {
        "surname": "Liew",
        "given_name": "Alan Wee-Chung"
      },
      {
        "surname": "McCall",
        "given_name": "John"
      }
    ]
  },
  {
    "title": "MeMu: Metric correlation Siamese network and multi-class negative sampling for visual tracking",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107170",
    "abstract": "Despite the great success in the computer vision field, visual tracking is still a challenging task. The main obstacle is that the target object often suffers from interference, such as occlusion. As most Siamese network-based trackers mainly sample image patches of target objects for training, the tracking algorithm lacks sufficient information about the surrounding environment. Besides, many Siamese network-based tracking algorithms build a regression only with the target object samples without considering the relationship between target and background, which may deteriorate the performance of trackers. In this paper, we propose a metric correlation Siamese network and multi-class negative sampling tracking method. For the first time, we explore a sampling approach that includes three different kinds of negative samples: virtual negative samples for pre-learning the potential occlusion situation, boundary negative samples to cope with potential tracking drift, and context negative samples to cope with potential incorrect positioning. With the three kinds of negative samples, we also propose a metric correlation method to train a correlation filter that contains metric information for better discrimination. Furthermore, we design a Siamese network-based architecture to embed the metric correlation filter module mentioned above in order to benefit from the powerful representation ability of deep learning. Extensive experiments on challenging OTB100 and VOT2017 datasets demonstrate the competitive performance of the proposed algorithm performs favorably compared with state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304704",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "BitTorrent tracker",
      "Clutter",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Correlation",
      "Economics",
      "Eye tracking",
      "Filter (signal processing)",
      "Geometry",
      "Mathematics",
      "Metric (unit)",
      "Object (grammar)",
      "Operations management",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Psychology",
      "Radar",
      "Telecommunications",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Xiao",
        "given_name": "Yafu"
      },
      {
        "surname": "Li",
        "given_name": "Jing"
      },
      {
        "surname": "Du",
        "given_name": "Bo"
      },
      {
        "surname": "Wu",
        "given_name": "Jia"
      },
      {
        "surname": "Chang",
        "given_name": "Jun"
      },
      {
        "surname": "Zhang",
        "given_name": "Wenfan"
      }
    ]
  },
  {
    "title": "Training data independent image registration using generative adversarial networks and domain adaptation",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107109",
    "abstract": "Medical image registration is an important task in automated analysis of multimodal images and temporal data involving multiple patient visits. Conventional approaches, although useful for different image types, are time consuming. Of late, deep learning (DL) based image registration methods have been proposed that outperform traditional methods in terms of accuracy and time. However, DL based methods are heavily dependent on training data and do not generalize well when presented with images of different scanners or anatomies. We present a DL based approach that can perform medical image registration of one image type despite being trained with images of a different type. This is achieved by unsupervised domain adaptation in the registration process and allows for easier application to different datasets without extensive retraining. To achieve our objective we train a network that transforms the given input image pair to a latent feature space vector using autoencoders. The resultant encoded feature space is used to generate the registered images with the help of generative adversarial networks (GANs). This feature transformation ensures greater invariance to the input image type. Experiments on chest X-ray, retinal and brain MR images show that our method, trained on one dataset gives better registration performance for other datasets, outperforming conventional methods that do not incorporate domain adaptation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304108",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Domain (mathematical analysis)",
      "Feature (linguistics)",
      "Feature vector",
      "Gene",
      "Image (mathematics)",
      "Image registration",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Mahapatra",
        "given_name": "Dwarikanath"
      },
      {
        "surname": "Ge",
        "given_name": "Zongyuan"
      }
    ]
  },
  {
    "title": "Monocular pedestrian orientation estimation based on deep 2D-3D feedforward",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107182",
    "abstract": "Accurate pedestrian orientation estimation of autonomous driving helps the ego vehicle obtain the intentions of pedestrians in the related environment, which are the base of safety measures such as collision avoidance and prewarning. However, because of relatively small sizes and high-level deformation of pedestrians, common pedestrian orientation estimation models fail to extract sufficient and comprehensive information from them, thus having their performance restricted, especially monocular ones which fail to obtain depth information of objects and related environment. In this paper, a novel monocular pedestrian orientation estimation model, called FFNet, is proposed. Apart from camera captures, the model adds the 2D and 3D dimensions of pedestrians as two other inputs according to the logic relationship between orientation and them. The 2D and 3D dimensions of pedestrians are determined from the camera captures and further utilized through two feedforward links connected to the orientation estimator. The feedforward links strengthen the logicality and interpretability of the network structure of the proposed model. Experiments show that the proposed model has at least 1.72% AOS increase than most state-of-the-art models after identical training processes. The model also has competitive results in orientation estimation evaluation on KITTI dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304820",
    "keywords": [
      "Artificial intelligence",
      "Astronomy",
      "Computer science",
      "Computer vision",
      "Control engineering",
      "Engineering",
      "Estimation",
      "Estimator",
      "Feed forward",
      "Geometry",
      "Interpretability",
      "Mathematics",
      "Monocular",
      "Orientation (vector space)",
      "Pedestrian",
      "Physics",
      "Pose",
      "Statistics",
      "Systems engineering",
      "Trajectory",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Chenchen"
      },
      {
        "surname": "Qian",
        "given_name": "Yeqiang"
      },
      {
        "surname": "Yang",
        "given_name": "Ming"
      }
    ]
  },
  {
    "title": "Writer-aware CNN for parsimonious HMM-based offline handwritten Chinese text recognition",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107102",
    "abstract": "Recently, the hybrid convolutional neural network hidden Markov model (CNN-HMM) has been introduced for offline handwritten Chinese text recognition (HCTR) and has achieved state-of-the-art performance. However, modeling each of the large vocabulary of Chinese characters with a uniform and fixed number of hidden states requires high memory and computational costs and makes the tens of thousands of HMM state classes confusing. Another key issue of CNN-HMM for HCTR is the diversified writing style, which leads to model strain and a significant performance decline for specific writers. To address these issues, we propose a writer-aware CNN based on parsimonious HMM (WCNN-PHMM). First, PHMM is designed using a data-driven state-tying algorithm to greatly reduce the total number of HMM states, which not only yields a compact CNN by state sharing of the same or similar radicals among different Chinese characters but also improves the recognition accuracy due to the more accurate modeling of tied states and the lower confusion among them. Second, WCNN integrates each convolutional layer with one adaptive layer fed by a writer-dependent vector, namely, the writer code, to extract the irrelevant variability in writer information to improve recognition performance. The parameters of writer-adaptive layers are jointly optimized with other network parameters in the training stage, while a multiple-pass decoding strategy is adopted to learn the writer code and generate recognition results. Validated on the ICDAR 2013 competition of CASIA-HWDB database, the more compact WCNN-PHMM of a 7360-class vocabulary can achieve a relative character error rate (CER) reduction of 16.6% over the conventional CNN-HMM without considering language modeling. By adopting a powerful hybrid language model (N-gram language model and recurrent neural network language model), the CER of WCNN-PHMM is reduced to 3.17%. Moreover, the state-tying results of PHMM explicitly show the information sharing among similar characters and the confusion reduction of tied state classes. Finally, we visualize the learned writer codes and demonstrate the strong relationship with the writing styles of different writers. To the best of our knowledge, WCNN-PHMM yields the best results on the ICDAR 2013 competition set, demonstrating its power when enlarging the size of the character vocabulary.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304030",
    "keywords": [
      "Artificial intelligence",
      "Code (set theory)",
      "Computer science",
      "Convolutional neural network",
      "Hidden Markov model",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Set (abstract data type)",
      "Speech recognition",
      "Vocabulary"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zi-Rui"
      },
      {
        "surname": "Du",
        "given_name": "Jun"
      },
      {
        "surname": "Wang",
        "given_name": "Jia-Ming"
      }
    ]
  },
  {
    "title": "A paired sparse representation model for robust face recognition from a single sample",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107129",
    "abstract": "Sparse representation-based classification (SRC) has been shown to achieve a high level of accuracy in face recognition (FR). However, matching faces captured in unconstrained video against a gallery with a single reference facial still per individual typically yields low accuracy. For improved robustness to intra-class variations, SRC techniques for FR have recently been extended to incorporate variational information from an external generic set into an auxiliary dictionary. Despite their success in handling linear variations, non-linear variations (e.g., pose and expressions) between probe and reference facial images cannot be accurately reconstructed with a linear combination of images in the gallery and auxiliary dictionaries because they do not share the same type of variations. In order to account for non-linear variations due to pose, a paired sparse representation model is introduced allowing for joint use of variational information and synthetic face images. The proposed model, called synthetic plus variational model, reconstructs a probe image by jointly using (1) a variational dictionary and (2) a gallery dictionary augmented with a set of synthetic images generated over a wide diversity of pose angles. The augmented gallery dictionary is then encouraged to pair the same sparsity pattern with the variational dictionary for similar pose angles by solving a newly formulated simultaneous sparsity-based optimization problem. Experimental results obtained on Chokepoint and COX-S2V datasets, using different face representations, indicate that the proposed approach can outperform state-of-the-art SRC-based methods for still-to-video FR with a single sample per person.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304303",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Face (sociological concept)",
      "Facial recognition system",
      "Gene",
      "Law",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Robustness (evolution)",
      "Set (abstract data type)",
      "Social science",
      "Sociology",
      "Sparse approximation"
    ],
    "authors": [
      {
        "surname": "Mokhayeri",
        "given_name": "Fania"
      },
      {
        "surname": "Granger",
        "given_name": "Eric"
      }
    ]
  },
  {
    "title": "Noise-robust dictionary learning with slack block-Diagonal structure for face recognition",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107118",
    "abstract": "Strict ‘0-1’ block-diagonal structure has been widely used for learning structured representation in face recognition problems. However, it is questionable and unreasonable to assume the within-class representations are the same. To circumvent this problem, in this paper, we propose a slack block-diagonal (SBD) structure for representation where the target structure matrix is dynamically updated, yet its blockdiagonal nature is preserved. Furthermore, in order to depict the noise in face images more precisely, we propose a robust dictionary learning algorithm based on mixed-noise model by utilizing the above SBD structure (SBD2L). SBD2L considers that there exists two forms of noise in data which are drawn from Laplacian and Gaussion distribution, respectively. Moreover, SBD2L introduces a low-rank constraint on the representation matrix to enhance the dictionary’s robustness to noise. Extensive experiments on four benchmark databases show that the proposed SBD2L can achieve better classification results than several state-of-the-art dictionary learning methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304194",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Block (permutation group theory)",
      "Block matrix",
      "Chemistry",
      "Computer science",
      "Constraint (computer-aided design)",
      "Diagonal",
      "Diagonal matrix",
      "Eigenvalues and eigenvectors",
      "Facial recognition system",
      "Feature learning",
      "Gene",
      "Geometry",
      "Image (mathematics)",
      "Law",
      "Mathematics",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Representation (politics)",
      "Robustness (evolution)",
      "Sparse approximation"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Zhe"
      },
      {
        "surname": "Wu",
        "given_name": "Xiao-Jun"
      },
      {
        "surname": "Yin",
        "given_name": "He-Feng"
      },
      {
        "surname": "Kittler",
        "given_name": "Josef"
      }
    ]
  },
  {
    "title": "Deep ensemble network using distance maps and body part features for skeleton based action recognition",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107125",
    "abstract": "Human action recognition is a hot research topic in the field of computer vision. The availability of low cost depth sensors in the market made the extraction of reliable skeleton maps of human objects easier. This paper proposes three subnets, referred to as SNet, TNet, and BodyNet to capture diverse spatio-temporal dynamics for action recognition task. Specifically, SNet is used to capture pose dynamics from the distance maps in the spatial domain. The second subnet (TNet) captures the temporal dynamics along the sequence. The third net (BodyNet) extracts distinct features from the fine-grained body parts in the temporal domain. With the motivation of ensemble learning, a hybrid network, referred to as HNet, is modeled using two subnets (TNet and BodyNet) to capture robust temporal dynamics. Finally, SNet and HNet are fused as one ensemble network for action classification task. Our method achieves competitive results on three widely used datasets: UTD MHAD, UT Kinect and NTU RGB+D.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304261",
    "keywords": [
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Domain (mathematical analysis)",
      "Economics",
      "Field (mathematics)",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "RGB color model",
      "Subnet",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "M.",
        "given_name": "Naveenkumar"
      },
      {
        "surname": "S.",
        "given_name": "Domnic"
      }
    ]
  },
  {
    "title": "A robust statistics approach for plane detection in unorganized point clouds",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107115",
    "abstract": "Plane detection is a key component for many applications, such as industrial reverse engineering and self-driving cars. However, existing plane-detection techniques are sensitive to noise and to user-defined parameters. We introduce a fast deterministic technique for plane detection in unorganized point clouds that is robust to noise and virtually independent of parameter tuning. It is based on a novel planarity test drawn from robust statistics and on a split and merge strategy. Its parameter values are automatically adjusted to fit the local distribution of samples in the input dataset, thus leading to good reconstruction of even small planar regions. We demonstrate the effectiveness of our solution on several real datasets, comparing its performance to state-of-art plane detection techniques, and showing that it achieves better accuracy, while still being one of the fastest.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304169",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Combinatorics",
      "Computer graphics (images)",
      "Computer science",
      "Gene",
      "Geometry",
      "Image (mathematics)",
      "Information retrieval",
      "Mathematics",
      "Merge (version control)",
      "Noise (video)",
      "Planar",
      "Planarity testing",
      "Plane (geometry)",
      "Point cloud",
      "Robustness (evolution)",
      "Statistical hypothesis testing",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Araújo",
        "given_name": "Abner M. C."
      },
      {
        "surname": "Oliveira",
        "given_name": "Manuel M."
      }
    ]
  },
  {
    "title": "Transfer learning-based discriminative correlation filter for visual tracking",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107157",
    "abstract": "Most Correlation Filter (CF)-based tracking methods can hardly handle occlusion or severe deformation, due to the lack of effective utilization of previous target information. To overcome this, we propose a novel Transfer Learning-based Discriminative Correlation Filter (TLDCF), which extracts knowledge from multiple previous tracking tasks and applies the knowledge for a new tracking task through Instance-Transfer Learning (ITL) and Probability-Transfer Learning (PTL). ITL applies knowledge of Gaussian Mixture Modelling (GMM) target representations and multi-channel filters learned in previous frames to directly train a new correlation filter. This improves the robustness of tracker for heavy occlusion and large appearance variations. Meanwhile, PTL encodes the spatio-temporal relationship predicted by Kalman Filter (KF) into a shared Gaussian prior to suppress huge location drift caused by similar targets. For optimization, we develop an efficient Alternating Direction Method of Multipliers (ADMM) based algorithm to calculate CFs on each independent channel in real time. Extensive experiments on OTB-2013 and OTB-2015 datasets well demonstrate the effectiveness of the proposed method. In particular, our method improves AUC score of the two datasets by 5.5% and 3.9% respectively compared to baseline, and achieves competitive performance against recent state-of-the-art deep trackers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304571",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "BitTorrent tracker",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Correlation",
      "Discriminative model",
      "Eye tracking",
      "Filter (signal processing)",
      "Gaussian",
      "Gene",
      "Geometry",
      "Kalman filter",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Physics",
      "Psychology",
      "Quantum mechanics",
      "Robustness (evolution)",
      "Tracking (education)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Bo"
      },
      {
        "surname": "Xu",
        "given_name": "Tingfa"
      },
      {
        "surname": "Li",
        "given_name": "Jianan"
      },
      {
        "surname": "Shen",
        "given_name": "Ziyi"
      },
      {
        "surname": "Chen",
        "given_name": "Yiwen"
      }
    ]
  },
  {
    "title": "Multi-task image set classification via joint representation with class-level sparsity and intra-task low-rankness",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.11.009",
    "abstract": "Image set classification has recently attracted great attention due to its widespread applications in computer vision and pattern recognition. The great challenges lie in effectively and efficiently measuring the similarity among image sets with high inter-class ambiguity and large intra-class variability. In this paper, we propose a joint representation based approach to image set classification, in which class-level sparse and globally low rank constraints are imposed on the representation coefficients to embody inter-set discrimination and intra-set commonality respectively. Furthermore, sometimes the small size of image sets or improper usage of a single kind of features causes useful information limited and lacking in discriminability. To address this problem, we extend the traditional image set classification to a multi-task version, i.e., modify the proposed model to incorporate multiple kinds of features. Fortunately, on the total multi-task representation coefficients, both the total class-level sparsity and the intra-task low-rankness constraints still apply. The proposed method is optimized as a non-smooth convex optimization problem by employing an alternating optimization technique. Experiments on five public datasets demonstrate that the proposed method surpasses existing joint representation models with various regularizations for image set classification and compares favorably with other state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518308845",
    "keywords": [
      "Ambiguity",
      "Artificial intelligence",
      "Class (philosophy)",
      "Combinatorics",
      "Computer science",
      "Contextual image classification",
      "Economics",
      "Image (mathematics)",
      "Law",
      "Management",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Programming language",
      "Rank (graph theory)",
      "Representation (politics)",
      "Set (abstract data type)",
      "Similarity (geometry)",
      "Sparse approximation",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Deyin"
      },
      {
        "surname": "Liu",
        "given_name": "Liangchen"
      },
      {
        "surname": "Tie",
        "given_name": "Yun"
      },
      {
        "surname": "Qi",
        "given_name": "Lin"
      }
    ]
  },
  {
    "title": "Joint temporal context exploitation and active learning for video segmentation",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107158",
    "abstract": "The segmentation of video, or separating out objects in the foreground, is an important application of pattern recognition and computer vision. Segmentation errors in pattern recognition approaches mainly come from difficulties in selecting maximally informative frames for learning. In this paper, we develop an approach to video segmentation that relies on temporal features by modeling the uncertainty of the distribution of different feature mask forms. We use those uncertainty values for unsupervised active learning. We evaluate our approach on the DAVIS16 annotated video data set and Shining3D dental video data set, and the results show our approach to be more accurate than other video segmentation approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304583",
    "keywords": [
      "Architectural engineering",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Engineering",
      "Feature (linguistics)",
      "Image segmentation",
      "Joint (building)",
      "Linguistics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Scale-space segmentation",
      "Segmentation",
      "Segmentation-based object categorization",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Tian",
        "given_name": "Yan"
      },
      {
        "surname": "Cheng",
        "given_name": "Guohua"
      },
      {
        "surname": "Gelernter",
        "given_name": "Judith"
      },
      {
        "surname": "Yu",
        "given_name": "Shihao"
      },
      {
        "surname": "Song",
        "given_name": "Chao"
      },
      {
        "surname": "Yang",
        "given_name": "Bailin"
      }
    ]
  },
  {
    "title": "Dissimilarity-based representations for one-class classification on time series",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107122",
    "abstract": "In several real-world classification problems it can be impractical to collect samples from classes other than the one of interest, hence the need for classifiers trained on a single class. There is a rich literature concerning binary and multi-class time series classification but less concerning one-class learning. In this study, we investigate the little-explored one-class time series classification problem. We represent time series as vectors of dissimilarities from a set of time series referred to as prototypes. Based on this approach, we evaluate a Cartesian product of 12 dissimilarity measures, and 8 prototype methods (strategies to select prototypes). Finally, a one-class nearest neighbor classifier is used on the dissimilarity-based representations (DBR). Experimental results show that DBR are competitive overall when compared with a strong baseline on the data-sets of the UCR/UEA archive. Additionally, DBR enable dimensionality reduction, and visual exploration of data-sets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304236",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary classification",
      "Binary number",
      "Biology",
      "Cartesian product",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Curse of dimensionality",
      "Data mining",
      "Dimensionality reduction",
      "Discrete mathematics",
      "Machine learning",
      "Mathematics",
      "One-class classification",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Programming language",
      "Series (stratigraphy)",
      "Set (abstract data type)",
      "Support vector machine",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Mauceri",
        "given_name": "Stefano"
      },
      {
        "surname": "Sweeney",
        "given_name": "James"
      },
      {
        "surname": "McDermott",
        "given_name": "James"
      }
    ]
  },
  {
    "title": "Structured self-attention architecture for graph-level representation learning",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107084",
    "abstract": "Recently, graph neural networks (GNNs) have shown to be effective in learning representative graph features. However, current pooling-based strategies for graph classification lack efficient utilization of graph representation information in which each node and layer have the same contribution to the output of graph-level representation. In this paper, we develop a novel architecture for extracting an effective graph representation by introducing structured multi-head self-attention in which the attention mechanism consists of three different forms, i.e., node-focused, layer-focused and graph-focused. In order to make full use of the information of graphs, the node-focused self-attention firstly aggregates neighbor node features with a scaled dot-product manner, and then the layer-focused and graph-focused self-attention serve as readout module to measure the importance of different nodes and layers to the model’s output. Moreover, it is able to improve the performance on graph classification tasks by combining these two self-attention mechanisms with base node-level GNNs. The proposed Structured Self-attention Architecture is evaluated on two kinds of graph benchmarks: bioinformatics datasets and social network datasets. Extensive experiments have demonstrated superior performance improvement to existing methods on predictive accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303851",
    "keywords": [
      "Artificial intelligence",
      "Attention network",
      "Computer science",
      "Data mining",
      "Feature learning",
      "Graph",
      "Pooling",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "Xiaolong"
      },
      {
        "surname": "Gong",
        "given_name": "Maoguo"
      },
      {
        "surname": "Xie",
        "given_name": "Yu"
      },
      {
        "surname": "Jiang",
        "given_name": "Fenlong"
      },
      {
        "surname": "Li",
        "given_name": "Hao"
      }
    ]
  },
  {
    "title": "Non-rigid object tracking via deep multi-scale spatial-temporal discriminative saliency maps",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107130",
    "abstract": "In this paper, we propose a novel effective non-rigid object tracking framework based on the spatial-temporal consistent saliency detection. In contrast to most existing trackers that utilize a bounding box to specify the tracked target, the proposed framework can extract accurate regions of the target as tracking outputs. It achieves a better description of the non-rigid objects and reduces the background pollution for the tracking model. Furthermore, our model has several unique characteristics. First, a tailored fully convolutional neural network (TFCN) is developed to model the local saliency prior for a given image region, which not only provides the pixel-wise outputs but also integrates the semantic information. Second, a novel multi-scale multi-region mechanism is proposed to generate local saliency maps that effectively consider visual perceptions with different spatial layouts and scale variations. Subsequently, the local saliency maps are fused via a weighted entropy method, resulting in a discriminative saliency map. Finally, we present a non-rigid object tracking algorithm based on the predicted saliency maps. By utilizing a spatial-temporal consistent saliency map (STCSM), we conduct the target-background classification and use an online fine-tuning scheme for model updating. Extensive experiments demonstrate that the proposed algorithm achieves competitive performance in both saliency detection and visual tracking, especially outperforming other related trackers on the non-rigid object tracking datasets. Source codes and compared results are released at https://github.com/Pchank/TFCNTracker.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304315",
    "keywords": [
      "Artificial intelligence",
      "BitTorrent tracker",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Discriminative model",
      "Eye tracking",
      "Image (mathematics)",
      "Minimum bounding box",
      "Object detection",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Physics",
      "Pixel",
      "Psychology",
      "Quantum mechanics",
      "Scale (ratio)",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Pingping"
      },
      {
        "surname": "Liu",
        "given_name": "Wei"
      },
      {
        "surname": "Wang",
        "given_name": "Dong"
      },
      {
        "surname": "Lei",
        "given_name": "Yinjie"
      },
      {
        "surname": "Wang",
        "given_name": "Hongyu"
      },
      {
        "surname": "Lu",
        "given_name": "Huchuan"
      }
    ]
  },
  {
    "title": "Prototype learning and collaborative representation using Grassmann manifolds for image set classification",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107123",
    "abstract": "Image set classification using manifolds is becoming increasingly more attractive since it considers non-Euclidean geometry. However, with the success of dictionary learning for image set classification using manifolds, how to learn an over-complete dictionary is still challenging. This paper proposes a novel prototype subspace learning method, in which a set of images is represented by a linear subspace and then mapped onto a Grassmann manifold. With this subspace representation, class prototypes and intra-class differences can be represented as principal components and variation subspaces, respectively. Isometric mapping further maps the manifolds into the symmetrical space via collaborative representation, which permits a closed-term solution. The proposed method is evaluated for face recognition, object recognition and action recognition. Extensive experimental results on the Honda, Extended YaleB, ETH-80 and Cambridge-Gesture datasets verify the superiority of the proposed method over the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304248",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Cognitive neuroscience of visual object recognition",
      "Combinatorics",
      "Computer science",
      "Engineering",
      "Face (sociological concept)",
      "Grassmannian",
      "Image (mathematics)",
      "Law",
      "Linear subspace",
      "Manifold (fluid mechanics)",
      "Mathematics",
      "Mechanical engineering",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Programming language",
      "Pure mathematics",
      "Representation (politics)",
      "Set (abstract data type)",
      "Social science",
      "Sociology",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Dong"
      },
      {
        "surname": "Shen",
        "given_name": "Xiaobo"
      },
      {
        "surname": "Sun",
        "given_name": "Quansen"
      },
      {
        "surname": "Gao",
        "given_name": "Xizhan"
      },
      {
        "surname": "Yan",
        "given_name": "Wenzhu"
      }
    ]
  },
  {
    "title": "Spatial–temporal multi-task learning for salient region detection",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.10.019",
    "abstract": "This paper proposes a novel multi-task learning based salient region detection method by fusing spatial and temporal features. Salient region detection has been widely used in various computer vision tasks, being as a general preprocessor to identify interest objects. Despite the recent successes, existing saliency models still lag behind the performance of human when visually perceives dynamic scenes. Most of the existing models largely rely on various spatial features. However, these spatial feature based methods have several deficiencies: (i) they can hardly adapt to the situation where moving objects are included, and (ii) they cannot model the human vision process in dynamic scenes. Recently, some saliency models introduce temporal features in their detecting process, such as the optical flow and stacking frames. The potential of temporal features for saliency optimization has been demonstrated. However, since temporal features in these models are merely used as a compensation to static features, the advantages of temporal features have not yet been fully explored. Aiming to comprehensively address these issues above, our method fuses spatial and temporal features, and learns the mapping relationship from various features to salient regions using our multi-task learning framework. The final salient region is generated by our unified Bayesian framework. The experimental results demonstrated that our proposed approach outperforms previous methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518308468",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Economics",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Management",
      "Operating system",
      "Optical flow",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Preprocessor",
      "Process (computing)",
      "Salient",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Zhe"
      },
      {
        "surname": "Wang",
        "given_name": "Ruili"
      },
      {
        "surname": "Yu",
        "given_name": "Ming"
      },
      {
        "surname": "Gao",
        "given_name": "Hongmin"
      },
      {
        "surname": "Li",
        "given_name": "Qi"
      },
      {
        "surname": "Wang",
        "given_name": "Huibin"
      }
    ]
  },
  {
    "title": "Patched-tube unitary transform for robust tensor completion",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107181",
    "abstract": "The aim of the robust tensor completion problem for third-order tensors is to recover a low-rank tensor from incomplete and/or corrupted observations. In this paper, we develop a patched-tubes unitary transform method for robust tensor completion. The proposed method is to extract similar patched-tubes to form a third-order sub-tensor, and then a transformed tensor singular value decomposition is employed to recover such low-rank incomplete and/or corrupted sub-tensor. Here the unitary transform matrix for transformed tensor singular value decomposition is constructed by using left singular vectors of the unfolding matrix arising from such sub-tensor. Moreover, we establish the perturbation results of the transformed tensor singular value decomposition for patched-tubes tensor completion. Extensive numerical experiments on hyperspectral, video and face data sets are presented to demonstrate the superior performance of the proposed patched-tubes unitary transform method over testing state-of-the-art robust tensor completion methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304819",
    "keywords": [
      "Algorithm",
      "Exact solutions in general relativity",
      "Law",
      "Mathematical analysis",
      "Mathematics",
      "Political science",
      "Pure mathematics",
      "Singular value decomposition",
      "Spherical harmonics",
      "Symmetric tensor",
      "Tensor (intrinsic definition)",
      "Tensor contraction",
      "Tensor density",
      "Tensor field",
      "Tensor operator",
      "Tensor product",
      "Unitary state"
    ],
    "authors": [
      {
        "surname": "Ng",
        "given_name": "Michael K."
      },
      {
        "surname": "Zhang",
        "given_name": "Xiongjun"
      },
      {
        "surname": "Zhao",
        "given_name": "Xi-Le"
      }
    ]
  },
  {
    "title": "Transferable heterogeneous feature subspace learning for JPEG mismatched steganalysis",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107105",
    "abstract": "Steganalysis is a technique that detects the presence of secret information in multimedia data. Many steganalysis algorithms have been proposed with high detection accuracy; however, the difference in statistical distribution between training and testing sets can cause mismatch problems, which will degrade the performance of traditional steganalysis algorithms. To solve this problem, we propose a transferable heterogeneous feature subspace learning (THFSL) algorithm for JPEG mismatched steganalysis. Our approach considers the feature space in each domain as a combination of the domain-independent features and the domain-related features. We use the transformation matrix to transfer both the domain-independent and domain-related features from the source and target domains to a common feature subspace, where each target sample can be better represented by a combination of source samples. By imposing low-rank constraints on the domain-independent features, the structures of data can be preserved, which can capture the intrinsic structures for discriminating cover and stego images. Our method can avoid a potentially negative transfer by using a sparse matrix to model the domain-related features and, thus, is more robust to different domain changes in mismatched steganalysis. Extensive experiments on various mismatched steganalysis tasks show the superiority of the proposed method over the state-of-the art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304066",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Domain (mathematical analysis)",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature vector",
      "Image (mathematics)",
      "JPEG",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Steganalysis",
      "Steganography",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Jia",
        "given_name": "Ju"
      },
      {
        "surname": "Zhai",
        "given_name": "Liming"
      },
      {
        "surname": "Ren",
        "given_name": "Weixiang"
      },
      {
        "surname": "Wang",
        "given_name": "Lina"
      },
      {
        "surname": "Ren",
        "given_name": "Yanzhen"
      },
      {
        "surname": "Zhang",
        "given_name": "Lefei"
      }
    ]
  },
  {
    "title": "Deep eigen-filters for face recognition: Feature representation via unsupervised multi-structure filter learning",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107176",
    "abstract": "Training deep convolutional neural networks (CNNs) often requires high computational cost and a large number of learnable parameters. To overcome this limitation, one solution is computing predefined convolution kernels from training data. In this paper, we propose a novel three-stage approach for filter learning alternatively. It learns filters in multiple structures including standard filters, channel-wise filters and point-wise filters which are inspired from variations of CNNs’ convolution operations. By analyzing the linear combination between learned filters and original convolution kernels in pre-trained CNNs, the reconstruction error is minimized to determine the most representative filters from the filter bank. These filters are used to build a network followed by HOG-based feature extraction for feature representation. The proposed approach shows competitive performance on color face recognition compared with other deep CNNs-based methods. Besides, it provides a perspective of interpreting CNNs by introducing the concepts of advanced convolutional layers to unsupervised filter learning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304765",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Deep learning",
      "Facial recognition system",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature learning",
      "Filter (signal processing)",
      "Law",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Ming"
      },
      {
        "surname": "Khan",
        "given_name": "Sheheryar"
      },
      {
        "surname": "Yan",
        "given_name": "Hong"
      }
    ]
  },
  {
    "title": "F-measure curves: A tool to visualize classifier performance under imbalance",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107146",
    "abstract": "Learning from imbalanced data is a challenging problem in many real-world machine learning applications due in part to the bias of performance in most classification systems. This bias may exist due to three reasons: (1) Classification systems are often optimized and compared using performance measurements that are unsuitable for imbalance problems; (2) most learning algorithms are designed and tested on a fixed imbalance level of data, which may differ from operational scenarios; (3) the preference of correct classification of classes is different from one application to another. This paper investigates specialized performance evaluation metrics and tools for imbalance problem, including scalar metrics that assume a given operating condition (skew level and relative preference of classes), and global evaluation curves or metrics that consider a range of operating conditions. We focus on the case in which the scalar metric F-measure is preferred over other scalar metrics, and propose a new global evaluation space for the F-measure that is analogous to the cost curves for expected cost. In this space, a classifier is represented as a curve that shows its performance over all of its decision thresholds and a range of possible imbalance levels for the desired preference of true positive rate to precision. Curves obtained in the F-measure space are compared to those of existing spaces (ROC, precision-recall and cost) and analogously to cost curves. The proposed F-measure space allows to visualize and compare classifiers’ performance under different operating conditions more easily than in ROC and precision-recall spaces. This space allows us to set the optimal decision threshold of a soft classifier and to select the best classifier among a group. This space also allows to empirically improve the performance obtained with ensemble learning methods specialized for class imbalance, by selecting and combining the base classifiers for ensembles using a modified version of the iterative Boolean combination algorithm that is optimized using the F-measure instead of AUC. Experiments on a real-world dataset for video face recognition show the advantages of evaluating and comparing different classifiers in the F-measure space versus ROC, precision-recall, and cost spaces. In addition, it is shown that the performance evaluated using the F-measure of Bagging ensemble method can improve considerably by using the modified iterative Boolean combination algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304479",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Composite material",
      "Computer science",
      "Data mining",
      "Economics",
      "Machine learning",
      "Management",
      "Materials science",
      "Mathematics",
      "Measure (data warehouse)",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Performance metric",
      "Precision and recall",
      "Range (aeronautics)",
      "Receiver operating characteristic",
      "Skew",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Soleymani",
        "given_name": "Roghayeh"
      },
      {
        "surname": "Granger",
        "given_name": "Eric"
      },
      {
        "surname": "Fumera",
        "given_name": "Giorgio"
      }
    ]
  },
  {
    "title": "Combining deep generative and discriminative models for Bayesian semi-supervised learning",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107156",
    "abstract": "Generative models can be used for a wide range of tasks, and have the appealing ability to learn from both labelled and unlabelled data. In contrast, discriminative models cannot learn from unlabelled data, but tend to outperform their generative counterparts in supervised tasks. We develop a framework to jointly train deep generative and discriminative models, enjoying the benefits of both. The framework allows models to learn from labelled and unlabelled data, as well as naturally account for uncertainty in predictive distributions, providing the first Bayesian approach to semi-supervised learning with deep generative models. We demonstrate that our blended discriminative and generative models outperform purely generative models in both predictive performance and uncertainty calibration in a number of semi-supervised learning tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132031930456X",
    "keywords": [],
    "authors": [
      {
        "surname": "Gordon",
        "given_name": "Jonathan"
      },
      {
        "surname": "Hernández-Lobato",
        "given_name": "José Miguel"
      }
    ]
  },
  {
    "title": "Leveraging unpaired out-of-domain data for image captioning",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.12.018",
    "abstract": "Image captioning is a focal point in the realm of computer vision due to its scientific and practical values. Although recent encoder-decoder models have achieved promising performance, they only leverage data from standard datasets, and the performance is limited to the specific datasets. There’re still large amounts of data without any additional annotations on the internet which can’t be fully utilized. In this paper, we propose a novel approach to using external unpaired images and texts to enhance the performance of image captioning system. Our method can utilize image and text data scraped from the internet respectively to improve the performance limited in concepts-decoder framework. Our approach can transfer the knowledge learned from web data to the standard dataset. We conduct extensive experiments on MS COCO and Flickr30K datasets. The result demonstrates the effectiveness of our method. On both datasets, our metrics scores are significantly improved compared with other methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518309358",
    "keywords": [
      "Artificial intelligence",
      "Closed captioning",
      "Coding (social sciences)",
      "Computer science",
      "Data mining",
      "Domain (mathematical analysis)",
      "Encoder",
      "Geometry",
      "Image (mathematics)",
      "Information retrieval",
      "Leverage (statistics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Point (geometry)",
      "Statistics",
      "The Internet",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Xinghan"
      },
      {
        "surname": "Zhang",
        "given_name": "Mingxing"
      },
      {
        "surname": "Wang",
        "given_name": "Zheng"
      },
      {
        "surname": "Zuo",
        "given_name": "Lin"
      },
      {
        "surname": "Li",
        "given_name": "Bo"
      },
      {
        "surname": "Yang",
        "given_name": "Yang"
      }
    ]
  },
  {
    "title": "Encoding features robust to unseen modes of variation with attentive long short-term memory",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107159",
    "abstract": "Long short-term memory (LSTM) is a type of recurrent neural networks that is efficient for encoding spatio-temporal features in dynamic sequences. Recent work has shown that the LSTM retains information related to the mode of variation in the input dynamic sequence which reduces the discriminability of the encoded features. To encode features robust to unseen modes of variation, we devise an LSTM adaptation named attentive mode variational LSTM. The proposed attentive mode variational LSTM utilizes the concept of attention to separate the input dynamic sequence into two parts: (1) task-relevant dynamic sequence features and (2) task-irrelevant static sequence features. The task-relevant dynamic features are used to encode and emphasize the dynamics in the input sequence. The task-irrelevant static sequence features are utilized to encode the mode of variation in the input dynamic sequence. Finally, the attentive mode variational LSTM suppresses the effect of mode variation with a shared output gate and results in a spatio-temporal feature robust to unseen variations. The effectiveness of the proposed attentive mode variational LSTM has been verified using two tasks: facial expression recognition and human action recognition. Comprehensive and extensive experiments have verified that the proposed method encodes spatio-temporal features robust to variations unseen during the training.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304595",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Astrophysics",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "ENCODE",
      "Economics",
      "Encoding (memory)",
      "Feature (linguistics)",
      "Gene",
      "Genetics",
      "Linguistics",
      "Management",
      "Mode (computer interface)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Recurrent neural network",
      "Sequence (biology)",
      "Speech recognition",
      "Task (project management)",
      "Variation (astronomy)"
    ],
    "authors": [
      {
        "surname": "Baddar",
        "given_name": "Wissam J."
      },
      {
        "surname": "Ro",
        "given_name": "Yong Man"
      }
    ]
  },
  {
    "title": "Graph regularized low-rank representation for submodule clustering",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107145",
    "abstract": "In this paper, a new submodule clustering method for imaging (2-D) data is proposed. Unlike most existing clustering methods that first convert such data into vectors as preprocessing, the proposed method arranges the data samples as lateral slices of a third-order tensor. Our algorithm is based on the union-of-free-submodules model and the samples are represented using t-product in the third-order tensor space. First, we impose a low-rank constraint on the representation tensor to capture the principle information of data. By incorporating manifold regularization into the tensor factorization, the proposed method explicitly exploits the local manifold structure of data. Meanwhile, a segmentation dependent term is employed to integrate the two pipeline steps of affinity learning and spectral clustering into a unified optimization framework. The proposed method can be efficiently solved based on the alternating direction method of multipliers and spectral clustering. Finally, a nonlinear extension is proposed to handle data drawn from a mixture of nonlinear manifolds. Extensive experimental results on five real-world image datasets confirm the effectiveness of the proposed methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304467",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Spectral clustering",
      "Tensor (intrinsic definition)"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Tong"
      }
    ]
  },
  {
    "title": "Human activity recognition from UAV-captured video sequences",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107140",
    "abstract": "This research paper introduces a new approach for human activity recognition from UAV-captured video sequences. The proposed approach involves two phases: an offline phase and an inference phase. A scene stabilization step is performed together with these two phases. The offline phase aims to generate the human/non-human model as well as a human activity model using a convolutional neural network. The inference phase makes use of the already generated models in order to detect humans and recognize their activities. Our main contribution lies in adapting the convolutional neural networks, normally dedicated to the classification task, to detect humans. In addition, the classification of human activities is carried out according to two scenarios: An instant classification of video frames and an entire classification of the video sequences. Relying on an experimental evaluation of the proposed methods for human detection and human activity classification on the UCF-ARG dataset, we validated not only these contributions but also the performance of our methods compared to the existing ones.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304418",
    "keywords": [
      "Activity recognition",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Economics",
      "Inference",
      "Machine learning",
      "Management",
      "Pattern recognition (psychology)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Mliki",
        "given_name": "Hazar"
      },
      {
        "surname": "Bouhlel",
        "given_name": "Fatma"
      },
      {
        "surname": "Hammami",
        "given_name": "Mohamed"
      }
    ]
  },
  {
    "title": "Handwriting posture prediction based on unsupervised model",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107093",
    "abstract": "Writing is an important basic skill for humans. To acquire such a skill, pupils often have to practice writing for several hours each day. However, different pupils usually possess distinct writing postures. Bad postures not only affect the speed and quality of writing, but also severely harm the healthy development of pupils’ spine and eyesight. Therefore, it is of key importance to identify or predict pupils’ writing postures and accordingly correct bad ones. In this paper, we formulate the problem of handwriting posture prediction for the first time. Further, we propose a neural network constructed with small convolution kernels to extract features from handwriting, and incorporate unsupervised learning and handwriting data analysis to predict writing postures. Extensive experiments reveal that our approach achieves an accuracy rate of 93.3%, which is significantly higher than the 76.67% accuracy of human experts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303942",
    "keywords": [
      "Affect (linguistics)",
      "Artificial intelligence",
      "Artificial neural network",
      "Communication",
      "Computer science",
      "Computer security",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Epistemology",
      "Handwriting",
      "Key (lock)",
      "Machine learning",
      "Natural language processing",
      "Philosophy",
      "Psychology",
      "Quality (philosophy)",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Bailin"
      },
      {
        "surname": "Zhang",
        "given_name": "Yulong"
      },
      {
        "surname": "Liu",
        "given_name": "Zhenguang"
      },
      {
        "surname": "Jiang",
        "given_name": "Xiaoheng"
      },
      {
        "surname": "Xu",
        "given_name": "Mingliang"
      }
    ]
  },
  {
    "title": "Wavelet-based segmentation on the sphere",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107081",
    "abstract": "Segmentation, a useful/powerful technique in pattern recognition, is the process of identifying object outlines within images. There are a number of efficient algorithms for segmentation in Euclidean space that depend on the variational approach and partial differential equation modelling. Wavelets have been used successfully in various problems in image processing, including segmentation, inpainting, noise removal, super-resolution image restoration, and many others. Wavelets on the sphere have been developed to solve such problems for data defined on the sphere, which arise in numerous fields such as cosmology and geophysics. In this work, we propose a wavelet-based method to segment images on the sphere, accounting for the underlying geometry of spherical data. Our method is a direct extension of the tight-frame based segmentation method used to automatically identify tube-like structures such as blood vessels in medical imaging. It is compatible with any arbitrary type of wavelet frame defined on the sphere, such as axisymmetric wavelets, directional wavelets, curvelets, and hybrid wavelet constructions. Such an approach allows the desirable properties of wavelets to be naturally inherited in the segmentation process. In particular, directional wavelets and curvelets, which were designed to efficiently capture directional signal content, provide additional advantages in segmenting images containing prominent directional and curvilinear features. We present several numerical experiments, applying our wavelet-based segmentation method, as well as the common K-means method, on real-world spherical images, including an Earth topographic map, a light probe image, solar data-sets, and spherical retina images. These experiments demonstrate the superiority of our method and show that it is capable of segmenting different kinds of spherical images, including those with prominent directional features. Moreover, our algorithm is efficient with convergence usually within a few iterations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303826",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Curvelet",
      "Discrete wavelet transform",
      "Gabor wavelet",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Wavelet",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Cai",
        "given_name": "Xiaohao"
      },
      {
        "surname": "Wallis",
        "given_name": "Christopher G.R."
      },
      {
        "surname": "Chan",
        "given_name": "Jennifer Y.H."
      },
      {
        "surname": "McEwen",
        "given_name": "Jason D."
      }
    ]
  },
  {
    "title": "Editorial special issue on multiple-task learning for big data",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.03.020",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519301035",
    "keywords": [
      "Artificial intelligence",
      "Big data",
      "Computer science",
      "Data mining",
      "Data science",
      "Engineering",
      "Machine learning",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Yingying"
      },
      {
        "surname": "Xie",
        "given_name": "Qing"
      },
      {
        "surname": "Thung",
        "given_name": "Kimhan"
      }
    ]
  },
  {
    "title": "Deep cascaded cross-modal correlation learning for fine-grained sketch-based image retrieval",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107148",
    "abstract": "Fine-grained Sketch-based Image Retrieval (FG-SBIR), which utilizes hand-drawn sketches to search the target object images, has recently drawn much attention. It is a challenging task because sketches and images belong to different modalities and sketches are highly abstract and ambiguous. Existing solutions to this problem either focus on visual comparisons between sketches and images and ignore the multimodal characteristics of annotated images, or treat the retrieval as a one-time process. In this paper, we formulate FG-SBIR as a coarse-to-fine process, and propose a Deep Cascaded Cross-modal Ranking Model (DCCRM) that can exploit all the beneficial multimodal information in sketches and annotated images and improve both the retrieval efficiency and the top-K ranked effectiveness. Our goal concentrates on constructing deep representations for sketches, images, and descriptions, and learning the optimized deep correlations across such different domains. Thus for a given query sketch, its relevant images with fine-grained instance-level similarities in a specific category can be returned, and the strict requirement of the instance-level retrieval for FG-SBIR is satisfied. Very positive results have been obtained in our experiments by using a large quantity of public data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304492",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Deep learning",
      "Exploit",
      "Focus (optics)",
      "Image (mathematics)",
      "Image retrieval",
      "Information retrieval",
      "Machine learning",
      "Modal",
      "Object (grammar)",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Polymer chemistry",
      "Process (computing)",
      "Ranking (information retrieval)",
      "Sketch"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yanfei"
      },
      {
        "surname": "Huang",
        "given_name": "Fei"
      },
      {
        "surname": "Zhang",
        "given_name": "Yuejie"
      },
      {
        "surname": "Feng",
        "given_name": "Rui"
      },
      {
        "surname": "Zhang",
        "given_name": "Tao"
      },
      {
        "surname": "Fan",
        "given_name": "Weiguo"
      }
    ]
  },
  {
    "title": "A new nested ensemble technique for automated diagnosis of breast cancer",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.11.004",
    "abstract": "Nowadays, breast cancer is reported as one of most common cancers amongst women. Early detection of this cancer is an essential to aid in informing subsequent treatments. This study investigates automated breast cancer prediction using machine learning and data mining techniques. We proposed the nested ensemble approach which used the Stacking and Vote (Voting) as the classifiers combination techniques in our ensemble methods for detecting the benign breast tumors from malignant cancers. Each nested ensemble classifier contains “Classifiers” and “MetaClassifiers”. MetaClassifiers can have more than two different classification algorithms. In this research, we developed the two-layer nested ensemble classifiers. In our two-layer nested ensemble classifiers the MetaClassifiers have two or three different classification algorithms. We conducted the experiments on Wisconsin Diagnostic Breast Cancer (WDBC) dataset and K-fold Cross Validation technique are used for the model evaluation. We compared the proposed two-layer nested ensemble classifiers with single classifiers (i.e., BayesNet and Naïve Bayes) in terms of the classification accuracy, precision, recall, F 1 measure, ROC and computational times of training single and nested ensemble classifiers. We also compared our best model with previous works reported in the literatures in terms of accuracy. The results demonstrate that the proposed two-layer nested ensemble models outperformance the single classifiers and most of the previous works. Both SV-BayesNet-3-MetaClassifier and SV-Naïve Bayes-3-MetaClassifier achieved accuracy 98.07% ( K = 10 ). However, SV-Naïve Bayes-3-MetaClassifier is more efficiency as it needs less time to build the model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518308766",
    "keywords": [
      "Artificial intelligence",
      "Cascading classifiers",
      "Classifier (UML)",
      "Computer science",
      "Ensemble forecasting",
      "Ensemble learning",
      "Machine learning",
      "Majority rule",
      "Naive Bayes classifier",
      "Pattern recognition (psychology)",
      "Random subspace method",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Abdar",
        "given_name": "Moloud"
      },
      {
        "surname": "Zomorodi-Moghadam",
        "given_name": "Mariam"
      },
      {
        "surname": "Zhou",
        "given_name": "Xujuan"
      },
      {
        "surname": "Gururajan",
        "given_name": "Raj"
      },
      {
        "surname": "Tao",
        "given_name": "Xiaohui"
      },
      {
        "surname": "Barua",
        "given_name": "Prabal D"
      },
      {
        "surname": "Gururajan",
        "given_name": "Rashmi"
      }
    ]
  },
  {
    "title": "No-reference mesh visual quality assessment via ensemble of convolutional neural networks and compact multi-linear pooling",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107174",
    "abstract": "Blind or No reference quality evaluation is a challenging issue since it is done without access to the original content. In this work, we propose a method based on deep learning for the mesh visual quality assessment without reference. For a given 3D model, we first compute its mesh saliency. Then, we extract views from the 3D mesh and the corresponding mesh saliency. After that, the views are split into small patches that are filtered using a saliency threshold. Only the salient patches are selected and used as input data. After that, three pre-trained deep convolutional neural networks are employed for feature learning: VGG, AlexNet, and ResNet. Each network is fine-tuned and produces a feature vector. The Compact Multi-linear Pooling (CMP) is used afterward to fuse the retrieved vectors into a global feature representation. Finally, fully connected layers followed by a regression module are used to estimate the quality score. Extensive experiments are executed on four mesh quality datasets and comparisons with existing methods demonstrate the effectiveness of our method in terms of correlation with subjective scores.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304741",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Economics",
      "Electrical engineering",
      "Engineering",
      "Feature (linguistics)",
      "Feature learning",
      "Feature vector",
      "Fuse (electrical)",
      "Gene",
      "Kernel (algebra)",
      "Linguistics",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pooling",
      "Quality Score",
      "Salient",
      "Support vector machine",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Abouelaziz",
        "given_name": "Ilyass"
      },
      {
        "surname": "Chetouani",
        "given_name": "Aladine"
      },
      {
        "surname": "El Hassouni",
        "given_name": "Mohammed"
      },
      {
        "surname": "Latecki",
        "given_name": "Longin Jan"
      },
      {
        "surname": "Cherifi",
        "given_name": "Hocine"
      }
    ]
  },
  {
    "title": "No-reference stereoscopic image quality assessment using a multi-task CNN and registered distortion representation",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107168",
    "abstract": "Scene discrepancy between the left and right views presents more challenges for image quality assessment (IQA) of stereoscopic images as opposed to monocular ones. Existing no-reference stereoscopic IQA (NR-SIQA) metrics cannot achieve a good performance on asymmetrically distorted stereoscopic images. In this paper, we propose an NR-SIQA index that first addresses scene discrepancy by means of image registration. It then uses a registered distortion representation based on the left and registered right views to represent the distortion in the stereoscopic image. Because different distortion types influence image quality differently, a multi-task convolutional neural network (CNN) is employed to learn image quality prediction and distortion-type identification simultaneously. We first design a one-column multi-task CNN model, that learns from the registered distortion representation. Then, we extend the one-column model to a three-column model, which also learns from the left and right views. Our experimental results validate the effectiveness of the proposed registered distortion representation and multi-task CNN architecture. The proposed one- and three-column models outperform the state-of-the-art NR-SIQA metrics, especially for asymmetrically distorted stereoscopic images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304686",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Distortion (music)",
      "Engineering",
      "Image (mathematics)",
      "Image quality",
      "Law",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Stereoscopy",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Yiqing"
      },
      {
        "surname": "Guo",
        "given_name": "Wenzhong"
      },
      {
        "surname": "Niu",
        "given_name": "Yuzhen"
      },
      {
        "surname": "Zhan",
        "given_name": "Jiamei"
      }
    ]
  },
  {
    "title": "Semi-supervised cross-modal image generation with generative adversarial networks",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107085",
    "abstract": "Cross-modal image generation is an important aspect of the multi-modal learning. Existing methods usually use the semantic feature to reduce the modality gap. Although these methods have achieved notable progress, there are still some limitations: (1) they usually use single modality information to learn the semantic feature; (2) they require the training data to be paired. To overcome these problems, we propose a novel semi-supervised cross-modal image generation method, which consists of two semantic networks and one image generation network. Specifically, in the semantic networks, we use image modality to assist non-image modality for semantic feature learning by using a deep mutual learning strategy. In the image generation network, we introduce an additional discriminator to reduce the image reconstruction loss. By leveraging large amounts of unpaired data, our method can be trained in a semi-supervised manner. Extensive experiments demonstrate the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303863",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Deep learning",
      "Detector",
      "Discriminator",
      "Feature (linguistics)",
      "Generative grammar",
      "Image (mathematics)",
      "Image retrieval",
      "Linguistics",
      "Machine learning",
      "Modal",
      "Modality (human–computer interaction)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Polymer chemistry",
      "Programming language",
      "Semantic feature",
      "Semantic gap",
      "Semantics (computer science)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Dan"
      },
      {
        "surname": "Du",
        "given_name": "Changde"
      },
      {
        "surname": "He",
        "given_name": "Huiguang"
      }
    ]
  },
  {
    "title": "Self-paced Learning for K-means Clustering Algorithm",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.08.028",
    "abstract": "The traditional K-means clustering algorithm is easily affected by the noise, outliers and falling into local optimal solution. This paper proposes a K-means clustering algorithm based on self-paced learning. Firstly, a best training subset is selected to construct the initial cluster model base on self-paced learning theory, and then enhances the generalization ability of the initial clustering model by adding sub-good subsets of samples one by one until the model performance is optimal or all training samples are used up. By analyzing the experimental results, the clustering algorithm proposed in this paper achieves better performance than the compare algorithms on the five real data sets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518304951",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Base (topology)",
      "CURE data clustering algorithm",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Construct (python library)",
      "Correlation clustering",
      "Data stream clustering",
      "Generalization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Hao"
      },
      {
        "surname": "Wen",
        "given_name": "Guoqiu"
      },
      {
        "surname": "Gan",
        "given_name": "Jiangzhang"
      },
      {
        "surname": "Zheng",
        "given_name": "Wei"
      },
      {
        "surname": "Lei",
        "given_name": "Cong"
      }
    ]
  },
  {
    "title": "Disentangled representation learning and residual GAN for age-invariant face verification",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107097",
    "abstract": "One of the challenges in cross-age face recognition and verification is to effectively model the facial aging process. Despite the rapid advances in face-related areas, it is still very difficult for existing methods to simultaneously achieve accurate facial feature preservation and reliable aging during aging modeling. Aiming to address this issue, we introduce a Disentangled Representation learning and Residual Generative Adversarial Network (DR-RGAN) that represents the facial features without age interference, which is achieved by explicitly disentangling the facial features and age variation. An encoder-decoder structured generator produces aging images from unstructured facial representations by using the age characteristics provided separately. It can thus take the disentangled representation to preserve personal identity for face verification. Considering that pixel-based errors may cause a loss of detail, a VGG based content loss is further equipped to preferably preserve facial features. The discriminator is trained to distinguish the real from generated faces, carry out identification prediction, and leverage an age estimator to boost the aging accuracy. It is beneficial for obtaining more photorealistic and desirable aging effects, as well as more consistent face verification results. Extensive experiments on the CACD and LFW datasets demonstrate that our DR-RGAN generates pleasing aging imageries and achieves a high accuracy of face verification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132031930398X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Computer vision",
      "Detector",
      "Discriminator",
      "Encoder",
      "Face (sociological concept)",
      "Facial recognition system",
      "Feature learning",
      "Law",
      "Leverage (statistics)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Residual",
      "Social science",
      "Sociology",
      "Speech recognition",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Shuyang"
      },
      {
        "surname": "Li",
        "given_name": "Jianwu"
      },
      {
        "surname": "Wang",
        "given_name": "Jiaxing"
      }
    ]
  },
  {
    "title": "Discovering influential factors in variational autoencoders",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107166",
    "abstract": "In the field of machine learning, it is still a critical issue to identify and supervise the learned representation without manually intervening or intuition assistance to extract useful knowledge or serve for the downstream tasks. In this work, we focus on supervising the influential factors extracted by the variational autoencoder (VAE). The VAE is proposed to learn independent low dimension representation while facing the problem that sometimes pre-set factors are ignored. We argue that the mutual information of the input and each learned factor of the representation plays a necessary indicator of discovering the influential factors. We find the VAE objective inclines to induce mutual information sparsity in factor dimension over the data intrinsic dimension and therefore result in some non-influential factors whose function on data reconstruction could be ignored. We show mutual information also influences the lower bound of VAE’s reconstruction error and downstream classification task. To make such indicator applicable, we design an algorithm for calculating the mutual information for VAE and prove its consistency. Experimental results on MNIST, CelebA and DEAP datasets show that mutual information can help determine influential factors, of which some are interpretable and can be used to further generation and classification tasks, and help discover the variant that connects with emotion on DEAP dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304662",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Computer science",
      "Consistency (knowledge bases)",
      "Curse of dimensionality",
      "Dimension (graph theory)",
      "Intrinsic dimension",
      "Law",
      "MNIST database",
      "Machine learning",
      "Mathematics",
      "Mutual information",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Pure mathematics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Shiqi"
      },
      {
        "surname": "Liu",
        "given_name": "Jingxin"
      },
      {
        "surname": "Zhao",
        "given_name": "Qian"
      },
      {
        "surname": "Cao",
        "given_name": "Xiangyong"
      },
      {
        "surname": "Li",
        "given_name": "Huibin"
      },
      {
        "surname": "Meng",
        "given_name": "Deyu"
      },
      {
        "surname": "Meng",
        "given_name": "Hongying"
      },
      {
        "surname": "Liu",
        "given_name": "Sheng"
      }
    ]
  },
  {
    "title": "Concatenation hashing: A relative position preserving method for learning binary codes",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107151",
    "abstract": "Hashing methods perform the efficient nearest neighbor search by mapping high-dimensional data to binary codes. Compared to projection-based hashing methods, hashing methods that adopt the clustering technique can encode the complex relationship of the data into binary codes. However, their search performance is affected by the boundary of the cluster. Two similar data points may be assigned to two different clusters and then encoded into two much different binary codes. In this paper, we propose a new hashing method based on the clustering technique and it can alleviate the effect from the cluster boundary. It is from an observation that the relative positions of any two close data points to each cluster center are close. An alternating optimization is developed to simultaneously discover the cluster structures of the data and learn the hash functions to preserve the relative positions of the data to each cluster center. To integrate the information in each cluster, the corresponding binary code of each data point is obtained by concatenating the substrings learnt by the hash functions in each cluster. The experiments show that our method is competitive to or better than the state-of-the-art hashing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304522",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Binary code",
      "Binary number",
      "Combinatorics",
      "Computer science",
      "Computer security",
      "Concatenation (mathematics)",
      "Double hashing",
      "Dynamic perfect hashing",
      "Economics",
      "Finance",
      "Hash function",
      "Hash table",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Position (finance)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Weng",
        "given_name": "Zhenyu"
      },
      {
        "surname": "Zhu",
        "given_name": "Yuesheng"
      }
    ]
  },
  {
    "title": "Supervised feature selection by self-paced learning regression",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.08.029",
    "abstract": "Feature selection is an effective method to reduce the feature dimension and improve the efficiency of data mining algorithms, it can be used to solve the ”curse of dimensionality” problem. Most existing feature selection methods do not adequately consider the effects of noise and outliers, thereby reducing the performance of regression. The paper proposed a new feature selection model and supervised learning method, using self-paced regularizer (SP-regularizer) terms and ℓ2, 1-norm to constrain the model. Specifically, the feature selection model is trained firstly using the most high-confidence samples base on the self-paced learning theory, and then adds the more high-confidence training samples in the remaining samples to increase the generalization ability of the initial feature selection model until the generalization ability of the feature selection model is not improved or all training samples are used up. Then use ℓ2, 1-norm remove redundant feature to improve algorithm efficiency. Experimental results on nine datasets show that our proposed method is superior to the comparison algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518304987",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Curse of dimensionality",
      "Dimensionality reduction",
      "Feature (linguistics)",
      "Feature selection",
      "Generalization",
      "Generalization error",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Gan",
        "given_name": "Jiangzhang"
      },
      {
        "surname": "Wen",
        "given_name": "Guoqiu"
      },
      {
        "surname": "Yu",
        "given_name": "Hao"
      },
      {
        "surname": "Zheng",
        "given_name": "Wei"
      },
      {
        "surname": "Lei",
        "given_name": "Cong"
      }
    ]
  },
  {
    "title": "A novel classification-selection approach for the self updating of template-based face recognition systems",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107121",
    "abstract": "The boosting on the need of security notably increased the amount of possible facial recognition applications, especially due to the success of the Internet of Things (IoT) paradigm. However, although handcrafted and deep learning-inspired facial features reached a significant level of compactness and expressive power, the facial recognition performance still suffers from intra-class variations such as ageing, facial expressions, lighting changes, and pose. These variations cannot be captured in a single acquisition and require multiple acquisitions of long duration, which are expensive and need a high level of collaboration from the users. Among others, self-update algorithms have been proposed in order to mitigate these problems. Self-updating aims to add novel templates to the users’ gallery among the inputs submitted during system operations. Consequently, computational complexity and storage space tend to be among the critical requirements of these algorithms. The present paper deals with the above problems by a novel template-based self-update algorithm, able to keep over time the expressive power of a limited set of templates stored in the system database. The rationale behind the proposed approach is in the working hypothesis that a dominating mode characterises the features’ distribution given the client. Therefore, the key point is to select the best templates around that mode. We propose two methods, which are tested on systems based on handcrafted features and deep-learning-inspired autoencoders at the state-of-the-art. Three benchmark data sets are used. Experimental results confirm that, by effective and compact feature sets which can support our working hypothesis, the proposed classification-selection approaches overcome the problem of manual updating and, in case, stringent computational requirements.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304224",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Boosting (machine learning)",
      "Computer science",
      "Computer security",
      "Face (sociological concept)",
      "Facial recognition system",
      "Geodesy",
      "Geography",
      "Key (lock)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Social science",
      "Sociology",
      "Template"
    ],
    "authors": [
      {
        "surname": "Orrù",
        "given_name": "Giulia"
      },
      {
        "surname": "Marcialis",
        "given_name": "Gian Luca"
      },
      {
        "surname": "Roli",
        "given_name": "Fabio"
      }
    ]
  },
  {
    "title": "A general extensible learning approach for multi-disease recommendations in a telehealth environment",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.11.006",
    "abstract": "In a telehealth environment, intelligent technologies are rapidly evolving toward improving the quality of patients’ lives and providing better clinical decision-making especially those who suffer from chronic diseases and require continuous monitoring and chronic-related medical measurements. A short-term disease risk prediction is a challenging task but is a great importance for teleheath care systems to provide accurate and reliable recommendations to patients. In this work, a general extensible learning approach for multi-disease recommendations is proposed to provide accurate recommendations for patients with chronic diseases in a telehealth environment. This approach generates appropriate recommendations for patients suffering from chronic diseases such as heart failure and diabetes about the need to take a medical test or not on the coming day based on the analysis of their medical data. The statistical features extracted from the sub-bands obtained after a four-level decomposition of the patient’s time series data are classified using a machine learning ensemble model. A combination of three classifiers – Least Squares-Support Vector Machine, Artificial Neural Network, and Naive Bayes – are utilized to construct the bagging-based ensemble model used to produce the final recommendations for patients. Two real-life datasets collected from chronic heart and diabetes disease patients are used for experimentations and evaluation. The experimental results show that the proposed approach yields a very good recommendation accuracy and offers an effective way to reduce the risk of incorrect recommendations as well as reduces the workload for chronic diseases patients who undergo body tests most days. Thus, the proposed approach is considered one of a promising tool for analyzing time series medical data of multi diseases and providing accurate and reliable recommendations to patients suffering from different types of chronic diseases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786551830881X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Clinical decision support system",
      "Computer science",
      "Decision support system",
      "Economic growth",
      "Economics",
      "Health care",
      "Machine learning",
      "Naive Bayes classifier",
      "Operating system",
      "Support vector machine",
      "Telehealth",
      "Telemedicine",
      "Workload"
    ],
    "authors": [
      {
        "surname": "Lafta",
        "given_name": "Raid"
      },
      {
        "surname": "Zhang",
        "given_name": "Ji"
      },
      {
        "surname": "Tao",
        "given_name": "Xiaohui"
      },
      {
        "surname": "Zhu",
        "given_name": "Xiaodong"
      },
      {
        "surname": "Li",
        "given_name": "Hongzhou"
      },
      {
        "surname": "Chang",
        "given_name": "Liang"
      },
      {
        "surname": "Deo",
        "given_name": "Ravinesh"
      }
    ]
  },
  {
    "title": "Correlation classifiers based on data perturbation: New formulations and algorithms",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107106",
    "abstract": "This paper develops a novel framework for a family of correlation classifiers that are reconstructed from uncertain convex programs under data perturbation. Under this framework, correlation classifiers are exploited from the pessimistic viewpoint under possible perturbation of data, and the max-min optimization problem is formulated by simplifying the original model in terms of adaptive uncertainty regions. The proposed model can be formulated as a minimization problem under proper conditions. The proximal majorization-minimization optimization (PMMO) based on Bregman divergences is devised to solve the proposed model that may be nonconvex or nonsmooth. It is found that using PMMO to solve the proposed model can exploit the convergence rate of the solution sequence in the nonconvex case. In the case of specific functions we can use the accelerated versions of first-order methods to solve the proposed model with convexity in order to make them have fast convergence rates in terms of the objective function. Extensive experiments on some data sets are conducted to demonstrate the feasibility and validity of the proposed model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304078",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Correlation",
      "Geometry",
      "Mathematics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Liang",
        "given_name": "Zhizheng"
      },
      {
        "surname": "Chen",
        "given_name": "Xuewen"
      },
      {
        "surname": "Zhang",
        "given_name": "Lei"
      },
      {
        "surname": "Liu",
        "given_name": "Jin"
      },
      {
        "surname": "Zhou",
        "given_name": "Yong"
      }
    ]
  },
  {
    "title": "Correlation-aware adversarial domain adaptation and generalization",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107124",
    "abstract": "Domain adaptation (DA) and domain generalization (DG) have emerged as a solution to the domain shift problem where the distribution of the source and target data is different. The task of DG is more challenging than DA as the target data is totally unseen during the training phase in DG scenarios. The current state-of-the-art employs adversarial techniques, however, these are rarely considered for the DG problem. Furthermore, these approaches do not consider correlation alignment which has been proven highly beneficial for minimizing domain discrepancy. In this paper, we propose a correlation-aware adversarial DA and DG framework where the features of the source and target data are minimized using correlation alignment along with adversarial learning. Incorporating the correlation alignment module along with adversarial learning helps to achieve a more domain agnostic model due to the improved ability to reduce domain discrepancy with unlabeled target data more effectively. Experiments on benchmark datasets serve as evidence that our proposed method yields improved state-of-the-art performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132031930425X",
    "keywords": [],
    "authors": [
      {
        "surname": "Rahman",
        "given_name": "Mohammad Mahfujur"
      },
      {
        "surname": "Fookes",
        "given_name": "Clinton"
      },
      {
        "surname": "Baktashmotlagh",
        "given_name": "Mahsa"
      },
      {
        "surname": "Sridharan",
        "given_name": "Sridha"
      }
    ]
  },
  {
    "title": "AI-GAN: Asynchronous interactive generative adversarial network for single image rain removal",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107143",
    "abstract": "Single image rain removal plays an important role in numerous multimedia applications. Existing algorithms usually tackle the deraining problem by the way of signal removal, which lead to over-smoothness and generate unexpected artifacts in de-rained images. This paper addresses the deraining problem from a completely different perspective of feature-wise disentanglement, and introduces the interactions and constraints between two disentangled latent spaces. Specifically, we propose an Asynchronous Interactive Generative Adversarial Network (AI-GAN) to progressively disentangle the rainy image into background and rain spaces in feature level through a two-branch structure. Each branch employs a two-stage synthesis strategy and interacts asynchronously by exchanging feed-forward information and sharing feedback gradients, achieving complementary adversarial optimization. This ‘adversarial’ is not only the ‘adversarial’ between the generator and the discriminator, but also means that the two generators are entangled, and interact with each other in the optimization process. Extensive experimental results demonstrate that AI-GAN outperforms state-of-the-art deraining methods and benefits various typical multimedia applications such as Image/Video Coding, Action Recognition, and Person Re-identification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304443",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Asynchronous communication",
      "Coding (social sciences)",
      "Computer network",
      "Computer science",
      "Detector",
      "Discriminator",
      "Domain (mathematical analysis)",
      "Feature (linguistics)",
      "Generative grammar",
      "Generator (circuit theory)",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Power (physics)",
      "Process (computing)",
      "Quantum mechanics",
      "Statistics",
      "Telecommunications",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Jin",
        "given_name": "Xin"
      },
      {
        "surname": "Chen",
        "given_name": "Zhibo"
      },
      {
        "surname": "Li",
        "given_name": "Weiping"
      }
    ]
  },
  {
    "title": "A domain adaptation approach to solve inverse problems in imaging via coupled deep dictionary learning",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107163",
    "abstract": "In this work, we focus on solving four standard inverse problems in imaging – denoising, deblurring, super-resolution, and reconstruction. All these problems are usually associated with image acquisition. Traditionally, signal processing techniques are used to solve such problems. However, such techniques are computationally expensive. In many applications today, when the requirement is to compute on the edge, such expensive inversion techniques are not viable solutions. Thus, one resorts to machine learning based solutions. In the past few years, variants of neural networks were developed to ‘learn’ the inversion operation. Although the learning was computationally expensive, the learnt model was light-weight and portable; suitable for deployment on leaner platforms. This work is based on the same concept, however, instead of using neural networks we treat inversion as a domain adaptation problem – a transfer from corrupted space to clean space. Our work is based on the developing field of coupled representation learning. We propose a deep version of the well known coupled dictionary learning technique; but instead of learning a single layer of dictionary, we learn multiple layers. Experimental results on standard datasets against state-of-the-art solutions for each problem, show that our method yields the best results both in terms of accuracy and speed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304637",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Deblurring",
      "Deep learning",
      "Dictionary learning",
      "Focus (optics)",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Inverse problem",
      "Inversion (geology)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Paleontology",
      "Physics",
      "Sparse approximation",
      "Structural basin",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Singhal",
        "given_name": "Vanika"
      },
      {
        "surname": "Majumdar",
        "given_name": "Angshul"
      }
    ]
  },
  {
    "title": "Generalized support vector data description for anomaly detection",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107119",
    "abstract": "Traditional anomaly detection procedures assume that normal observations are obtained from a single distribution. However, due to the complexities of modern industrial processes, the observations may belong to multiple operating modes with different distributions. In such cases, traditional anomaly detection procedures may trigger false alarms while the process is indeed in another normally operating mode. We propose a generalized support vector-based anomaly detection procedure called generalized support vector data description which can be used to determine the anomalies in multimodal processes. The proposed procedure constructs hyperspheres for each class in order to include as many observations as possible and keep other class observations as far apart as possible. In addition, we introduce a generalized Bayesian framework which does not only consider the prior information from each mode, but also highlights the relationships among the modes. The effectiveness of the proposed procedure is demonstrated through various simulation studies and real-life applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304200",
    "keywords": [
      "Algorithm",
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Bayesian probability",
      "Class (philosophy)",
      "Computer science",
      "Condensed matter physics",
      "Data mining",
      "Mode (computer interface)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Process (computing)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Turkoz",
        "given_name": "Mehmet"
      },
      {
        "surname": "Kim",
        "given_name": "Sangahn"
      },
      {
        "surname": "Son",
        "given_name": "Youngdoo"
      },
      {
        "surname": "Jeong",
        "given_name": "Myong K."
      },
      {
        "surname": "Elsayed",
        "given_name": "Elsayed A."
      }
    ]
  },
  {
    "title": "A repeatable and robust local reference frame for 3D surface matching",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107186",
    "abstract": "Local reference frames (LRFs) have been widely used for 3D local surface description. In this work, we propose a repeatable LRF with strong robustness to different nuisances. Different from existing LRF methods, the proposed LRF uses a part of neighboring points within the support region to calculate the z-axis, and performs an effective feature transformation on the neighboring points to define the x-axis. Specifically, feature transformation is applied to the data on a projection plane based on three point distribution characteristics via weighted strategies. These characteristics include the z-height, the distance to the center and the average length to 1-ring neighbors, covariance analysis is then applied to the transformed points to obtain the eigenvector with the largest eigenvalue, which points towards the maximum variance direction. Using a sign disambiguation technique, the modified eigenvector is used to define the final x-axis. Furthermore, a scale strategy is proposed to improve the robustness of the LRF with respect to mesh decimation. The proposed LRF was rigorously tested on six public benchmark datasets consisting of three different application contexts, i.e., 3D shape retrieval, 3D object recognition and registration. Experiments show that our method achieves significantly higher repeatability and stronger robustness than the state-of-the-arts under Gaussian noise, shot noise and mesh resolution variation. Finally, the descriptor matching results on four typical datasets further demonstrate the effectiveness of our LRF.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304868",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Frame (networking)",
      "Geometry",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Reference frame",
      "Statistics",
      "Surface (topology)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Ao",
        "given_name": "Sheng"
      },
      {
        "surname": "Guo",
        "given_name": "Yulan"
      },
      {
        "surname": "Tian",
        "given_name": "Jindong"
      },
      {
        "surname": "Tian",
        "given_name": "Yong"
      },
      {
        "surname": "Li",
        "given_name": "Dong"
      }
    ]
  },
  {
    "title": "CARs-Lands: An associative classifier for large-scale datasets",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107128",
    "abstract": "Associative classifiers are one of the most efficient classifiers for large datasets. However, they are unsuitable to be directly used in large-scale data problems. Associative classifiers discover frequent/rare rules or both in order to produce an efficient classifier. Discovery rules need to explore a large solution space in a well-organized manner; hence, learning of the associative classification methods of large datasets is not suitable on large-scale datasets because of memory and time-complexity constraints. The proposed method, CARs-Lands, presents an efficient distributed associative classifier. In CARs-Lands, first, a modified dataset is generated. This new dataset has sub-datasets that are completely appropriate to produce classification association rules (CARs) in a parallel manner. The produced dataset by CARs-Lands contains two types of instances: main instances and neighbor instances. Main instances can be either real instances of training dataset or meta-instances, which are not in the training dataset; each main instance has several neighbor instances from the training dataset, which together form a sub-dataset. These sub-datasets are used for parallel local association rule mining. In CARs-Lands, local association rules lead to more accurate prediction, because each test instance is classified by the association rules of their nearest neighbors in the training datasets. The proposed approach is evaluated in terms of accuracy on six real-world large-scale datasets against five recent and well-known methods. Experiment results show that the proposed classification method has high prediction accuracy and is highly competitive when compared to other classification methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304297",
    "keywords": [
      "Artificial intelligence",
      "Association rule learning",
      "Associative property",
      "Cartography",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Geography",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Scale (ratio)",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Almasi",
        "given_name": "Mehrdad"
      },
      {
        "surname": "Saniee Abadeh",
        "given_name": "Mohammad"
      }
    ]
  },
  {
    "title": "Few-shot traffic sign recognition with clustering inductive bias and random neural network",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107160",
    "abstract": "Reliable and fast traffic sign recognition (TSR) that locates the traffic sign from an image and then estimates its category is a crucial perception function for Advanced Driver Assistance Systems (ADAS) of autonomous vehicles. Most of the popular deep convolutional neural networks (DCNNs) based TSR techniques advocate discriminative feature learning for traffic signs against their appearance variability. However, such feature learning scheme may suffer from the diversity of traffic signs categories, especially when samples within each category are limited for model training (i.e., few-shot learning). Here, we present a generative feature learning based TSR network with well generalization capacity and high computational efficiency. Instead of relying on large amounts of supervision to learn discriminative features, our method devotes to learn common but unique properties of class-specific traffic signs with few training samples. Specifically, we combine clustering inductive bias with a random neural network, and then exploit computational advantages offered by a fast random projection algorithm. Experiments on two TSR benchmarks illustrate that our method achieves comparable or higher recognition accuracy than state-of-the-art DCNN-based methods with less training data and inference time consumption.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304601",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Cluster analysis",
      "Computer science",
      "Convolutional neural network",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature learning",
      "Generalization",
      "Generative grammar",
      "Generative model",
      "Inference",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Sign (mathematics)",
      "Traffic sign",
      "Traffic sign recognition"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Shichao"
      },
      {
        "surname": "Deng",
        "given_name": "Chenwei"
      },
      {
        "surname": "Piao",
        "given_name": "Zhengquan"
      },
      {
        "surname": "Zhao",
        "given_name": "Baojun"
      }
    ]
  },
  {
    "title": "MobileFAN: Transferring deep hidden representation for face alignment",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107114",
    "abstract": "Facial landmark detection is a crucial prerequisite for many face analysis applications. Deep learning-based methods currently dominate the approach of addressing the facial landmark detection. However, such works generally introduce a large number of parameters, resulting in high memory cost. In this paper, we aim for a lightweight as well as effective solution to facial landmark detection. To this end, we propose an effective lightweight model, namely Mobile Face Alignment Network (MobileFAN), using a simple backbone MobileNetV2 as the encoder and three deconvolutional layers as the decoder. The proposed MobileFAN, with only 8% of the model size and lower computational cost, achieves superior or equivalent performance compared with state-of-the-art models. Moreover, by transferring the geometric structural information of a face graph from a large complex model to our proposed MobileFAN through feature-aligned distillation and feature-similarity distillation, the performance of MobileFAN is further improved in effectiveness and efficiency for face alignment. Extensive experiment results on three challenging facial landmark estimation benchmarks including COFW, 300W and WFLW show the superiority of our proposed MobileFAN against state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304157",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Encoder",
      "Face (sociological concept)",
      "Facial recognition system",
      "Feature (linguistics)",
      "Landmark",
      "Law",
      "Linguistics",
      "Memory footprint",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Yang"
      },
      {
        "surname": "Liu",
        "given_name": "Yifan"
      },
      {
        "surname": "Shen",
        "given_name": "Chunhua"
      },
      {
        "surname": "Gao",
        "given_name": "Yongsheng"
      },
      {
        "surname": "Xiong",
        "given_name": "Shengwu"
      }
    ]
  },
  {
    "title": "Competitive and collaborative representation for classification",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.06.019",
    "abstract": "Deep network recently has achieved promising performance for classification task with massive training samples. The behavior of this model, however, would be diminished obviously when the training set is small. Meanwhile, linear representation based classifiers have widely applied into many fields. These classifiers mostly attempt to take advantage of the correct class to code the test sample through appending l 1-norm or nuclear norm which highly takes computation. Under these observations, we present a novel competitive and collaborative representation classification (Co-CRC) that employs the properties of training data with l 2-norm regularization to create a competitive environment which enables the correct class to make more contribution to coding. Additionally, the proposed competitive weight in this paper enhances the competitive relation among all classes and is beneficial for the classifier to find the correct class. Extensive experimental results over popular benchmarks including object, scene and face images database indicate that the proposed algorithm totally based on l 2-norm regularization takes less computation to obtain rather sparse coding and mostly outperforms several state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518302642",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Classifier (UML)",
      "Coding (social sciences)",
      "Computation",
      "Computer science",
      "Law",
      "Machine learning",
      "Mathematics",
      "Norm (philosophy)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Political science",
      "Regularization (linguistics)",
      "Source code",
      "Statistics",
      "Training set"
    ],
    "authors": [
      {
        "surname": "Chi",
        "given_name": "Hongmei"
      },
      {
        "surname": "Xia",
        "given_name": "Haifeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Lifang"
      },
      {
        "surname": "Zhang",
        "given_name": "Chunjiang"
      },
      {
        "surname": "Tang",
        "given_name": "Xin"
      }
    ]
  },
  {
    "title": "Statistical bootstrap-based principal mode component analysis for dynamic background subtraction",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107153",
    "abstract": "Background subtraction is needed to extract foreground information from a video sequence for further processing in many applications, such as surveillance tracking. However, due to the presence of a dynamic background and noise, extracting foreground accurately from a video sequence remains challenging. A novel projection method, namely Principal Mode Component Analysis (PMCA), is proposed to capture the most repetitive patterns of a video sequence, which is one of the key characteristics of the video background. The patterns are captured by applying the bootstrapping method together with the statistic mode measure. The bootstrapping method can model the distribution of almost any statistic of the dynamic background and complicated noise. This is different from current methods, which restrict the distribution to a closed-form function. We introduce a mathematical relaxation that can formulate the statistical mode measure for a continuous video data. A fast exhaustive search method is proposed to find the global optimal solution for the PMCA. This fast method adopts a simplification procedure that makes the optimization procedure independent of the video size. The proposed method is computationally much more traceable than existing ones. We compare the proposed method with 10 different methods, including several state-of-the-art techniques, for 19 different real-world video sequences from two popular datasets. Experiment results show that the proposed method performs the best in 16 cases and second best in 2 cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304546",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Background subtraction",
      "Biology",
      "Bootstrapping (finance)",
      "Computer science",
      "Data mining",
      "Econometrics",
      "Genetics",
      "Image (mathematics)",
      "Mathematics",
      "Measure (data warehouse)",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Pixel",
      "Principal component analysis",
      "Projection (relational algebra)",
      "Sequence (biology)",
      "Statistic",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Lam",
        "given_name": "Benson S.Y."
      },
      {
        "surname": "Chu",
        "given_name": "Amanda M.Y."
      },
      {
        "surname": "Yan",
        "given_name": "H."
      }
    ]
  },
  {
    "title": "A fusion network for road detection via spatial propagation and spatial transformation",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107141",
    "abstract": "In this paper, we address the fusion of image and point cloud data for road detection. To take advantage of both deep network and multi-modal data fusion, we propose an end-to-end road segmentation network called SPSTFN (Spatial Propagation and Spatial Transformation Fusion Network). Our method considers the model-level fusion and dual-view fusion in the network simultaneously for the first time. Specifically, the proposed SPSTFN contains three parts: the point cloud branch, the image branch, and the fusion block. Firstly, we design a simple but efficient lightweight network to handle the unordered and sparse point cloud to obtain a coarse representation of the road area. Then, an equal-resolution convolutional block is adopted to capture the low-level features of the image which are used to produce the heat diffusion coefficients of the joint anisotropic diffusion based spatial propagation model. Thirdly, we conduct the diffusion process on the coarse representation under the guidance of the learned low-level image features, both in the perspective and bird views, via the spatial transformation in the network. Finally, the diffusion results of the two views are then integrated to generate the final refined representation of the road area. The proposed fusion method is totally data-driven and parameter-free, and the whole fusion network can be trained with the standard BP (Back Propagation) algorithm. Without any additional process steps and pre-training, the proposed method obtains competitive results on the KITTI Road Benchmark.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132031930442X",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Block (permutation group theory)",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Fusion",
      "Gene",
      "Geodesy",
      "Geography",
      "Geometry",
      "Image (mathematics)",
      "Image fusion",
      "Law",
      "Linguistics",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point cloud",
      "Political science",
      "Politics",
      "Process (computing)",
      "Representation (politics)",
      "Segmentation",
      "Sensor fusion",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Fei"
      },
      {
        "surname": "Wang",
        "given_name": "Huan"
      },
      {
        "surname": "Jin",
        "given_name": "Zhong"
      }
    ]
  },
  {
    "title": "MCENN: A variant of extended nearest neighbor method for pattern recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.01.015",
    "abstract": "Recent studies have shown that extended nearest neighbor (ENN) method is able to improve the classification performance over traditional k-nearest neighbor (KNN) methods, due to its novel “two-way communication” decision making process. The ENN method classifies a test data sample by maximizing the intra-class coherence gain over the whole data set. However, the original ENN method, like KNN, is a type of instance-based learning algorithm without a training stage. This paper presents a variant of ENN method, called Maximum intra-class Coherence Extended Nearest Neighbor (MCENN), which incorporates distances between individual data sample and its nearest neighbors into the decision making process and introduces a novel distance metric learning algorithm as a training process to learn an optimal linear transformation that maximizes the overall intra-class coherence of training data. We demonstrate that the proposed MCENN approach is able to improve the discriminative performance for pattern recognition. Experimental results on real-life data sets demonstrate the effectiveness of the proposed approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300143",
    "keywords": [
      "Artificial intelligence",
      "Best bin first",
      "Biochemistry",
      "Chemistry",
      "Chromatography",
      "Class (philosophy)",
      "Coherence (philosophical gambling strategy)",
      "Computer science",
      "Data mining",
      "Discriminative model",
      "Economics",
      "Gene",
      "Large margin nearest neighbor",
      "Machine learning",
      "Mathematics",
      "Metric (unit)",
      "Nearest neighbor search",
      "Operating system",
      "Operations management",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Sample (material)",
      "Statistics",
      "Transformation (genetics)",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Bo"
      },
      {
        "surname": "He",
        "given_name": "Haibo"
      },
      {
        "surname": "Zhang",
        "given_name": "Song"
      }
    ]
  },
  {
    "title": "Nom document digitalization by deep convolution neural networks",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.02.015",
    "abstract": "Nom is an ancient script used in Vietnam until the current Latin-based Vietnamese alphabet became common, and a large number of ancient Nom documents are in existence. Due to the gradual degradation of Nom documents and a decrease in the number of scholars who can understand them, a system to digitalize Nom documents is urgently necessary. This paper presents a segmentation-based method for digitalizing Nom documents using deep convolution neural networks. Nom pages are preprocessed, segmented into isolated characters, and then recognized by a single-character OCR. The structure of the U-Net is applied to create segmentation maps and extract character regions from them. Subsequently, we propose coarse and fine combined classifiers to recognize each character pattern. The results by the best classifier are revised by a decoder using a langue model. The decoder is the same as the connectionist temporal classification decoder used in end-to-end text recognition systems. Compared with the traditional segmentation method using projection profiles and the Voronoi diagram (IoU = 81.23%), the segmentation method using the deep convolution neural network produces a better result (IoU = 92.08%) for detecting character regions. The proposed CNN models for recognizing segmented character patterns outperforms the traditional models using the modified quadratic discriminant function and the learning vector quantization with the recognition rate of 85.07%. The combination of coarse and fine classifiers, the training dataset with salt and pepper noises, and the attention layer are the key factors in the recognition rate improvement.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300556",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Character (mathematics)",
      "Computer science",
      "Connectionism",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Deep learning",
      "Geometry",
      "Image (mathematics)",
      "Learning vector quantization",
      "Mathematics",
      "Optical character recognition",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Nguyen",
        "given_name": "Kha Cong"
      },
      {
        "surname": "Nguyen",
        "given_name": "Cuong Tuan"
      },
      {
        "surname": "Nakagawa",
        "given_name": "Masaki"
      }
    ]
  },
  {
    "title": "Association between work-related features and coronary artery disease: A heterogeneous hybrid feature selection integrated with balancing approach",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.02.010",
    "abstract": "Coronary artery disease (CAD) is a leading cause of death worldwide and is associated with high healthcare expenditure. Researchers are motivated to apply machine learning (ML) for quick and accurate detection of CAD. The performance of the automated systems depends on the quality of features used. Clinical CAD datasets contain different features with varying degrees of association with CAD. To extract such features, we developed a novel hybrid feature selection algorithm called heterogeneous hybrid feature selection (2HFS). In this work, we used Nasarian CAD dataset, in which work place and environmental features are also considered, in addition to other clinical features. Synthetic minority over-sampling technique (SMOTE) and Adaptive synthetic (ADASYN) are used to handle the imbalance in the dataset. Decision tree (DT), Gaussian Naive Bayes (GNB), Random Forest (RF), and XGBoost classifiers are used. 2HFS-selected features are then input into these classifier algorithms. Our results show that, the proposed feature selection method has yielded the classification accuracy of 81.23% with SMOTE and XGBoost classifier. We have also tested our approach with other well-known CAD datasets: Hungarian dataset, Long-beach-va dataset, and Z-Alizadeh Sani dataset. We have obtained 83.94%, 81.58% and 92.58% for Hungarian dataset, Long-beach-va dataset, and Z-Alizadeh Sani dataset, respectively. Hence, our experimental results confirm the effectiveness of our proposed feature selection algorithm as compared to the existing state-of-the-art techniques which yielded outstanding results for the development of automated CAD systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300507",
    "keywords": [
      "Artificial intelligence",
      "CAD",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Decision tree",
      "Engineering",
      "Engineering drawing",
      "Feature selection",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Naive Bayes classifier",
      "Pattern recognition (psychology)",
      "Random forest",
      "Support vector machine",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Nasarian",
        "given_name": "Elham"
      },
      {
        "surname": "Abdar",
        "given_name": "Moloud"
      },
      {
        "surname": "Fahami",
        "given_name": "Mohammad Amin"
      },
      {
        "surname": "Alizadehsani",
        "given_name": "Roohallah"
      },
      {
        "surname": "Hussain",
        "given_name": "Sadiq"
      },
      {
        "surname": "Basiri",
        "given_name": "Mohammad Ehsan"
      },
      {
        "surname": "Zomorodi-Moghadam",
        "given_name": "Mariam"
      },
      {
        "surname": "Zhou",
        "given_name": "Xujuan"
      },
      {
        "surname": "Pławiak",
        "given_name": "Paweł"
      },
      {
        "surname": "Acharya",
        "given_name": "U. Rajendra"
      },
      {
        "surname": "Tan",
        "given_name": "Ru-San"
      },
      {
        "surname": "Sarrafzadegan",
        "given_name": "Nizal"
      }
    ]
  },
  {
    "title": "Contextual deconvolution network for semantic segmentation",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107152",
    "abstract": "In this paper, we propose a Contextual Deconvolution Network (CDN) and focus on context association in decoder network. Specifically, in upsampling path, we introduce two types of contextual modules to model the interdependencies of features in channel and spatial dimensions respectively. The channel contextual module captures image-level semantic information by aggregating the feature maps across spatial dimensions, and clarifies global ambiguity of features. Meanwhile, the spatial contextual module obtains patch-level semantic context by learning a spatial weight map, and enhance the feature discrimination. We embed the two contextual modules into individual components of the decoder network, thus improving the representation power and gaining more precise segment results. Thorough evaluations are performed on four challenging datasets, i.e., PASCAL VOC 2012, ADE20K, PASCAL-Context and Cityscapes dataset. Our approach achieves competitive performance with state-of-the-art models on PASCAL VOC 2012,ADE20K and Cityscapes dataset, and new state-of-the-art performance on PASCAL-Context dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304534",
    "keywords": [
      "Ambiguity",
      "Artificial intelligence",
      "Attention network",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Linguistics",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Segmentation",
      "Spatial contextual awareness",
      "Upsampling"
    ],
    "authors": [
      {
        "surname": "Fu",
        "given_name": "Jun"
      },
      {
        "surname": "Liu",
        "given_name": "Jing"
      },
      {
        "surname": "Li",
        "given_name": "Yong"
      },
      {
        "surname": "Bao",
        "given_name": "Yongjun"
      },
      {
        "surname": "Yan",
        "given_name": "Weipeng"
      },
      {
        "surname": "Fang",
        "given_name": "Zhiwei"
      },
      {
        "surname": "Lu",
        "given_name": "Hanqing"
      }
    ]
  },
  {
    "title": "Editorial for special section at Pattern Recognition Letters - IbPRIA 2019",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.03.024",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301021",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Engineering",
      "Engineering physics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Section (typography)",
      "Special section"
    ],
    "authors": [
      {
        "surname": "Marin-Jimenez",
        "given_name": "Manuel J."
      },
      {
        "surname": "Morales",
        "given_name": "Aythami"
      },
      {
        "surname": "Fierrez",
        "given_name": "Julian"
      },
      {
        "surname": "Pertusa",
        "given_name": "Antonio"
      },
      {
        "surname": "Proenca",
        "given_name": "Hugo"
      },
      {
        "surname": "Sanchez",
        "given_name": "Jose Salvador"
      }
    ]
  },
  {
    "title": "Egocentric visitor localization and artwork detection in cultural sites using synthetic data",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.02.014",
    "abstract": "Computer vision and machine learning can be used in cultural heritage to augment the experience of visitors during the exploration of the cultural site, as well as to assist its management. To achieve such goals, two fundamental tasks should be addressed, i.e., localizing visitors and recognizing the observed artworks. Wearable cameras offer a convenient setting to address both tasks through the analysis of images acquired from the visitors’ points of view. However, the engineering of approaches to address such tasks generally requires large amounts of labeled data. We propose a tool which can be used to collect and automatically label synthetic visual data suitable to study image-based localization and artwork detection. The tool simulates a virtual agent navigating the 3D model of a real cultural site and automatically captures video frames along with the related ground truth camera poses and semantic masks indicating the position of artworks. We generate a dataset of synthetic images starting from the 3D model of a museum located in Siracusa, Italy. The experiments suggest that the proposed tool allows to drastically reduce the effort needed to collect and label data, providing a means to generate large-scale datasets suitable to study localization and artwork detection in cultural sites.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300544",
    "keywords": [
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Pattern recognition (psychology)",
      "Programming language",
      "Visitor pattern"
    ],
    "authors": [
      {
        "surname": "Andrea Orlando",
        "given_name": "Santi"
      },
      {
        "surname": "Furnari",
        "given_name": "Antonino"
      },
      {
        "surname": "Farinella",
        "given_name": "Giovanni Maria"
      }
    ]
  },
  {
    "title": "Video captioning with text-based dynamic attention and step-by-step learning",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.03.001",
    "abstract": "Automatically describing video content with natural language has been attracting much attention in computer vision and natural language processing communities. Most existing methods predict one word at a time, and by feeding the last generated word back as input at the next time, while the other generated words are not fully exploited. Furthermore, traditional methods optimize the model using all the training samples in each epoch without considering their learning situations, which leads to a lot of unnecessary training and can not target the difficult samples. To address these issues, we propose a text-based dynamic attention model named TDAM, which imposes a dynamic attention mechanism on all the generated words with the motivation to improve the context semantic information and enhance the overall control of the whole sentence. Moreover, the text-based dynamic attention mechanism and the visual attention mechanism are linked together to focus on the important words. They can benefit from each other during training. In addition, the model is trained through two steps: “starting from scratch” and “checking for gaps”. The former uses all the samples to optimize the model, while the latter only trains for samples with poor control. Experimental results on the popular datasets MSVD and MSR-VTT demonstrate that our non-ensemble model outperforms the state-of-the-art video captioning benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300799",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Closed captioning",
      "Computer science",
      "Context (archaeology)",
      "Epistemology",
      "Focus (optics)",
      "Image (mathematics)",
      "Language model",
      "Linguistics",
      "Machine learning",
      "Mechanism (biology)",
      "Natural language processing",
      "Operating system",
      "Optics",
      "Paleontology",
      "Philosophy",
      "Physics",
      "Scratch",
      "Sentence",
      "Speech recognition",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Xiao",
        "given_name": "Huanhou"
      },
      {
        "surname": "Shi",
        "given_name": "Jinglun"
      }
    ]
  },
  {
    "title": "Image analysis by log-polar Exponent-Fourier moments",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107177",
    "abstract": "Moments, as a popular class of the global invariant image descriptors, have been widely used in image analysis, pattern recognition and computer vision applications. Exponent-Fourier moments (EFMs) are a new set of orthogonal moments based on exponential functions, which are suitable for image analysis and rotation invariant pattern recognition. However, EFMs lack natively the scaling-invariant property. In addition, they always suffer from high time complexity, numerical instability, and reconstruction error, especially for higher order of moments. In this paper, we introduce a class of scaling and rotation-invariant orthogonal moments, named Log-Polar Exponent-Fourier moments (LPEFMs), by extending the classical EFMs to the log-polar coordinates. Firstly, we redefined the EFMs’ basis functions in log-polar domain instead of Cartesian/polar coordinate domain in order to obtain the scaling-invariant property. Then, we develop a new framework for computing the LPEFMs by using pseudo-polar Fourier transform and frequency domain interpolation, which result in better image representation capability, numerical stability, and computational speed. Compared with the classical EFMs, the proposed LPEFMs have four advantages, scaling invariance, speed, accuracy and stability . Theoretical analysis and simulation results are provided to validate the proposed image moment and to compare its performance with previous works.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304777",
    "keywords": [
      "Algorithm",
      "Cartesian coordinate system",
      "Fourier transform",
      "Geometry",
      "Invariant (physics)",
      "Mathematical analysis",
      "Mathematical physics",
      "Mathematics",
      "Polar coordinate system",
      "Scaling"
    ],
    "authors": [
      {
        "surname": "YANG",
        "given_name": "Hong-ying"
      },
      {
        "surname": "QI",
        "given_name": "Shu-ren"
      },
      {
        "surname": "WANG",
        "given_name": "Chao"
      },
      {
        "surname": "YANG",
        "given_name": "Si-bo"
      },
      {
        "surname": "WANG",
        "given_name": "Xiang-yang"
      }
    ]
  },
  {
    "title": "Identifying the best data-driven feature selection method for boosting reproducibility in classification tasks",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107183",
    "abstract": "Considering the proliferation of extremely high-dimensional data in many domains including computer vision and healthcare applications such as computer-aided diagnosis (CAD), advanced techniques for reducing data dimensionality and identifying the most relevant features for a given classification task such as distinguishing between healthy and disordered brain states are needed. Despite the existence of many works on boosting the classification accuracy using a particular feature selection (FS) method, choosing the best one from a large pool of existing FS techniques for boosting feature reproducibility within a dataset of interest remains a formidable challenge to tackle. Notably, a good performance of a particular FS method does not necessarily imply that the experiment is reproducible and that the features identified are optimal for the entirety of the samples. Essentially, this paper presents the first attempt to address the following challenge: “Given a set of different feature selection methods { F S 1 , ⋯ , F S K } , and a dataset of interest, how to identify the most reproducible and ‘trustworthy’ connectomic features that would produce reliable biomarkers capable of accurately differentiate between two specific conditions?” To this aim, we propose FS-Select framework which explores the relationships among the different FS methods using a multi-graph architecture based on feature reproducibility power, average accuracy, and feature stability of each FS method. By extracting the ‘central’ graph node, we identify the most reliable and reproducible FS method for the target brain state classification task along with the most discriminative features fingerprinting these brain states. To evaluate the reproducibility power of FS-Select, we perturbed the training set by using different cross-validation strategies on a multi-view small-scale connectomic dataset (late mild cognitive impairment vs Alzheimer’s disease) and large-scale dataset including autistic vs healthy subjects. Our experiments revealed reproducible connectional features fingerprinting disordered brain states.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304832",
    "keywords": [
      "Artificial intelligence",
      "Boosting (machine learning)",
      "Computer science",
      "Curse of dimensionality",
      "Data mining",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature selection",
      "Graph",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Reproducibility",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Georges",
        "given_name": "Nicolas"
      },
      {
        "surname": "Mhiri",
        "given_name": "Islem"
      },
      {
        "surname": "Rekik",
        "given_name": "Islem"
      }
    ]
  },
  {
    "title": "Syntactic pattern recognition-based diagnostics of fetal palates",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.02.023",
    "abstract": "Analyzing ultrasound images of relatively small fetal structures is a difficult task. This difficulty particularly applies to diagnosing congenital defects of the fetus, including one of the most common defects, which is the cleft palate. To date, no methods have been developed for visualizing the fetal palate. Therefore, there is a need to improve the effectiveness of diagnostics in this area by developing appropriate image analysis and recognition methods using computer-based techniques. At the same time, relatively fast algorithms are being sought that can be a part of the software of an ultrasound device. The contribution of the paper consists in defining a new computer method satisfying all the requirements mentioned above. The method applies syntactic pattern recognition approach to the analysis of the image of the fetus’s palate. It is based on extracting of a sequence of images on multiple binarization thresholds (in the preprocessing phase), defining picture primitives on the basis of the histogram analysis, and applying a parser for a GDPLL(k) string grammar (Generalized Dynamically Programmed LL(k) grammar) for classifying abnormalities (in the recognition phase). The implementation of the method is computationally efficient and it can be a helpful tool for supporting doctors in the diagnostic process (as it has been verified in practice). The computer recognition of fetal defects on basis of the analysis of the structure of the fetus’s palate is a novel achievement. Such results have never been reported before.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300623",
    "keywords": [
      "Artificial intelligence",
      "Basis (linear algebra)",
      "Computer science",
      "Geometry",
      "Grammar",
      "Histogram",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Parsing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Preprocessor",
      "Process (computing)",
      "Programming language",
      "Software"
    ],
    "authors": [
      {
        "surname": "Jurek",
        "given_name": "Janusz"
      },
      {
        "surname": "Wójtowicz",
        "given_name": "Wojciech"
      },
      {
        "surname": "Wójtowicz",
        "given_name": "Anna"
      }
    ]
  },
  {
    "title": "Improving FastText with inverse document frequency of subwords",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.03.003",
    "abstract": "Word embedding is important in natural language processing, and word2vec is known as a representative algorithm. However, word2vec and many other dictionary-based word embedding algorithms create word vectors only for words that appear in the training data, ignoring morphological features of these words. The FastText algorithm was previously proposed to solve this problem: it creates a word vector from subword vectors, making it possible to create word embeddings even for words never seen during the training. Because of morphological features, FastText is strong in syntactic tasks but weak in semantic tasks, compared with word2vec. In this paper, we propose a method of improving FastText by using the inverse document frequency of subwords. Our approach is intended to overcome the weakness of FastText in semantic tasks. According to our experiments, the proposed method shows improved results in semantic tests with a little loss in syntactic tests. Our method can be applied to any word embedding algorithm that uses subwords. We additionally tested probabilistic FastText, an algorithm designed to distinguish multiple-meaning words, by adding the inverse document frequency, and the results confirmed an improved performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300817",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Embedding",
      "Geometry",
      "Mathematics",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Term (time)",
      "Word (group theory)",
      "Word embedding",
      "Word2vec",
      "tf–idf"
    ],
    "authors": [
      {
        "surname": "Choi",
        "given_name": "Jaekeol"
      },
      {
        "surname": "Lee",
        "given_name": "Sang-Woong"
      }
    ]
  },
  {
    "title": "Special issue on pattern recognition and cognitive assistants",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.03.017",
    "abstract": "This special issue proposes a space for researchers to discuss the challenges of bridging pattern recognition and cognitive assistants. This bridge was travelled in both sides. From one side, how pattern recognition methods can help intelligent systems to become more cognitive. And from the other side, how cognitive models can improve pattern recognition methods so that the final system can better assist users. So, this special issue presents research work on these topics, aiming to observe their interrelations in order to create theoretical approaches, methodologies and computational tools to advance work on cognitive assistants and pattern recognition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300957",
    "keywords": [
      "Artificial intelligence",
      "Bridge (graph theory)",
      "Bridging (networking)",
      "Cognition",
      "Computer science",
      "Computer security",
      "Data science",
      "Engineering",
      "Human–computer interaction",
      "Internal medicine",
      "Mechanical engineering",
      "Medicine",
      "Neuroscience",
      "Operating system",
      "Psychology",
      "Space (punctuation)",
      "Work (physics)"
    ],
    "authors": [
      {
        "surname": "Falomir",
        "given_name": "Zoe"
      },
      {
        "surname": "Gibert",
        "given_name": "Karina"
      },
      {
        "surname": "Plaza",
        "given_name": "Enric"
      }
    ]
  },
  {
    "title": "GPU based parallel optimization for real time panoramic video stitching",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.06.018",
    "abstract": "Panoramic video is a sort of video recorded at the same point of view to record the full scene. With the development of video surveillance and the requirement for 3D converged video surveillance in smart cities, CPU and GPU are required to possess strong processing abilities to make panoramic video. The traditional panoramic products depend on post processing, which results in high power consumption, low stability and unsatisfying performance in real time. In order to solve these problems, we propose a real-time panoramic video stitching framework. The framework we propose mainly consists of three algorithms, L-ORB image feature extraction algorithm, feature point matching algorithm based on LSH and GPU parallel video stitching algorithm based on CUDA. The experiment results show that the algorithm mentioned can improve the performance in the stages of feature extraction of images stitching and matching, the running speed of which is 11.3 times than that of the traditional ORB algorithm and 641 times than that of the traditional SIFT algorithm. Based on analyzing the GPU resources occupancy rate of each resolution image stitching, we further propose a stream parallel strategy to maximize the utilization of GPU resources. Compared with the L-ORB algorithm, the efficiency of this strategy is improved by 1.6–2.5 times, and it can make full use of GPU resources. The performance of the system accomplished in the paper is 29.2 times than that of the former embedded one, while the power dissipation is reduced to 10 W.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518307281",
    "keywords": [
      "Artificial intelligence",
      "CUDA",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Feature extraction",
      "Image (mathematics)",
      "Image stitching",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Orb (optics)",
      "Parallel computing",
      "Philosophy",
      "Scale-invariant feature transform",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Du",
        "given_name": "Chengyao"
      },
      {
        "surname": "Yuan",
        "given_name": "Jingling"
      },
      {
        "surname": "Dong",
        "given_name": "Jiansheng"
      },
      {
        "surname": "Li",
        "given_name": "Lin"
      },
      {
        "surname": "Chen",
        "given_name": "Mincheng"
      },
      {
        "surname": "Li",
        "given_name": "Tao"
      }
    ]
  },
  {
    "title": "Multi-criteria online frame-subset selection for autonomous vehicle videos",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.03.031",
    "abstract": "Data Subset selection for training learning models for a variety of tasks, has been widely studied in the literature of batch mode active learning. Recent works attempt to utilize the model specific signals in the deep learning context for computer vision tasks. Companies, in their bid to create safe autonomous driving models, train and test their models on billions of miles of driving data; not all of which may be valuable for a training task. In this paper, we study the problem of frame-subset selection from autonomous vehicle driving data, for the problem of semantic segmentation - which is a crucial component of the perception module in an autonomous driving system. We find that state of the art methods for deep active learning do not utilize pairwise similarity between incoming and existing frames. We explore both active learning settings, where labels for incoming points are not available, as well as frame selection settings and find that our method selects more valuable frames than only score-based frame subset selection, or frame subset selection without label information. We demonstrate the effectiveness of our method using DeeplabV3+ model on both benchmark as well as datasets generated by driving simulators. Our generated dataset and code will be made publicly available.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301148",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Code (set theory)",
      "Computer science",
      "Context (archaeology)",
      "Deep learning",
      "Engineering",
      "Frame (networking)",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Pairwise comparison",
      "Paleontology",
      "Programming language",
      "Segmentation",
      "Selection (genetic algorithm)",
      "Set (abstract data type)",
      "Systems engineering",
      "Task (project management)",
      "Telecommunications",
      "Variety (cybernetics)"
    ],
    "authors": [
      {
        "surname": "Das",
        "given_name": "Soumi"
      },
      {
        "surname": "Mandal",
        "given_name": "Sayan"
      },
      {
        "surname": "Bhoyar",
        "given_name": "Ashwin"
      },
      {
        "surname": "Bharde",
        "given_name": "Madhumita"
      },
      {
        "surname": "Ganguly",
        "given_name": "Niloy"
      },
      {
        "surname": "Bhattacharya",
        "given_name": "Suparna"
      },
      {
        "surname": "Bhattacharya",
        "given_name": "Sourangshu"
      }
    ]
  },
  {
    "title": "Deep hard modality alignment for visible thermal person re-identification",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.03.012",
    "abstract": "Visible Thermal Person Re-Identification (VTReID) is essentially a cross-modality problem and widely encountered in real night-time surveillance scenarios, which is still in need of vigorous performance improvement. In this work, we design a simple but effective Hard Modality Alignment Network (HMAN) framework to learn modality-robust features. Since current VTReID works do not consider the cross-modality discrepancy imbalance, their models are likely to suffer from the selective alignment behavior. To solve this problem, we propose a novel Hard Modality Alignment (HMA) loss to simultaneously balance and reduce the modality discrepancies. Specifically, we mine the hard feature subspace with large modality discrepancies and abandon the easy feature subspace with small modality discrepancies to make the modality distributions more distinguishable. For mitigating the discrepancy imbalance, we pay more attention on reducing the modality discrepancies of the hard feature subspace than that of the easy feature subspace. Furthermore, we propose to jointly relieve the modality heterogeneity of global and local visual semantics to further boost the cross-modality retrieval performance. This paper experimentally demonstrates the effectiveness of the proposed method, achieving superior performance over the state-of-the-art methods on RegDB and SYSU-MM01 datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300908",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Feature (linguistics)",
      "Identification (biology)",
      "Linguistics",
      "Modality (human–computer interaction)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Semantics (computer science)",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Pingyu"
      },
      {
        "surname": "Su",
        "given_name": "Fei"
      },
      {
        "surname": "Zhao",
        "given_name": "Zhicheng"
      },
      {
        "surname": "Zhao",
        "given_name": "Yanyun"
      },
      {
        "surname": "Yang",
        "given_name": "Lei"
      },
      {
        "surname": "Li",
        "given_name": "Yang"
      }
    ]
  },
  {
    "title": "Three-step action search networks with deep Q-learning for real-time object tracking",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107188",
    "abstract": "Sliding window and candidate sampling are two widely used search strategies for visual object tracking, but they are far behind real-time. By treating the tracking problem as a three-step decision-making process, a novel tracking network, which explores only three small subsets of candidate regions, is developed to achieve faster (real-time) localization of the target object along the frames in a video. A convolutional neural network agent is formulated to interact with a video over time, and two action-value functions are exploited to learn a favorable policy off-line to determine the best action for visual object tracking. Our model is trained in a collaborative learning way by using action classification and cumulative reward approximation in reinforcement learning. We have evaluated our proposed tracker against a number of state-of-the-art ones over three popular tracking benchmarks including OTB-2013, OTB-2015, and VOT2017. The experimental results have demonstrated that our proposed method can achieve very competitive performance on real-time object tracking.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304881",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Eye tracking",
      "Machine learning",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Psychology",
      "Q-learning",
      "Reinforcement learning",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Teng",
        "given_name": "Zhu"
      },
      {
        "surname": "Zhang",
        "given_name": "Baopeng"
      },
      {
        "surname": "Fan",
        "given_name": "Jianping"
      }
    ]
  },
  {
    "title": "A novel fitness function in genetic programming to handle unbalanced emotion recognition data",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.03.005",
    "abstract": "In the area of behavioral psychology, real-time emotion recognition by using physiological stimuli is an active topic of interest. This research considers the recognition of two class of emotions i.e., positive and negative emotions using EEG signals in response to happy, horror, sad, and neutral genres. In a noise-free framework for data acquisition of 50 participants, NeuroSky MindWave 2 is used. The dataset collected is unbalanced i.e., there are more instances of positive classes than negative ones. Therefore, accuracy is not a useful metric to assess the results of the unbalanced dataset because of biased results. So, the primary goal of this research is to address the issue of unbalanced emotion recognition dataset classification, for which we are proposing a novel fitness function known as Gap score (G score), which learns about both the classes by giving them equal importance and being unbiased. The genetic programming (GP) framework in which we implemented G score is named as G-score GP (GGP). The second goal is to assess how distinct genres affect human emotion recognition process and to identify an age group that is more active emotionally when their emotions are elicited. Experiments were conducted on EEG data acquired with a single-channel EEG device. We have compared the performance of GGP for the classification of emotions with state-of-the-art methods. The analysis shows that GGP provides 87.61% classification accuracy by using EEG. In compliance with the self-reported feelings, brain signals of 26 to 35 years of age group provided the highest emotion recognition rate.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300830",
    "keywords": [
      "Affect (linguistics)",
      "Artificial intelligence",
      "Biology",
      "Class (philosophy)",
      "Cognitive psychology",
      "Communication",
      "Computer science",
      "Economics",
      "Electroencephalography",
      "Emotion classification",
      "Evolutionary biology",
      "Feeling",
      "Fitness function",
      "Function (biology)",
      "Genetic algorithm",
      "Genetic programming",
      "Machine learning",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Psychiatry",
      "Psychology",
      "Random forest",
      "Social psychology",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Acharya",
        "given_name": "Divya"
      },
      {
        "surname": "Goel",
        "given_name": "Shivani"
      },
      {
        "surname": "Asthana",
        "given_name": "Rishi"
      },
      {
        "surname": "Bhardwaj",
        "given_name": "Arpit"
      }
    ]
  },
  {
    "title": "Understanding the decisions of CNNs: An in-model approach",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.04.004",
    "abstract": "With the outstanding predictive performance of Convolutional Neural Networks on different tasks and their widespread use in real-world scenarios, it is essential to understand and trust these black-box models. While most of the literature focuses on post-model methods, we propose a novel in-model joint architecture, composed by an explainer and a classifier. This architecture outputs not only a class label, but also a visual explanation of such decision, without the need for additional labelled data to train the explainer besides the image class. The model is trained end-to-end, with the classifier taking as input an image and the explainer’s resulting explanation, thus allowing for the classifier to focus on the relevant areas of such explanation. Moreover, this approach can be employed with any classifier, provided that the necessary connections to the explainer are made. We also propose a three-phase training process and two alternative custom loss functions that regularise the produced explanations and encourage desired properties, such as sparsity and spatial contiguity. The architecture was validated in two datasets (a subset of ImageNet and a cervical cancer dataset) and the obtained results show that it is able to produce meaningful image- and class-dependent visual explanations, without direct supervision, aligned with intuitive visual features associated with the data. Quantitative assessment of explanation quality was conducted through iterative perturbation of the input image according to the explanation heatmaps. The impact on classification performance is studied in terms of average function value and AOPC (Area Over the MoRF (Most Relevant First) Curve). For further evaluation, we propose POMPOM (Percentage of Meaningful Pixels Outside the Mask) as another measurable criteria of explanation goodness. These analyses showed that the proposed method outperformed state-of-the-art post-model methods, such as LRP (Layer-wise Relevance Propagation).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301240",
    "keywords": [
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Contextual image classification",
      "Convolutional neural network",
      "Image (mathematics)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Rio-Torto",
        "given_name": "Isabel"
      },
      {
        "surname": "Fernandes",
        "given_name": "Kelwin"
      },
      {
        "surname": "Teixeira",
        "given_name": "Luís F."
      }
    ]
  },
  {
    "title": "Sample imbalance disease classification model based on association rule feature selection",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.03.016",
    "abstract": "In the research of computer-aided diagnosis, the shortage of disease feature dimension curse and the imbalance of medical samples have always been the focus of research on diagnostic decision support systems. For these two problems, we propose a feature selection algorithm based on association rules and an integrated classification algorithm based on random equilibrium sampling. We extracted and cleaned the electronic medical record text obtained from the hospital to obtain a diabetes data set. The proposed algorithm was verified in this data set and the public data set UCI. Experimental results show that the feature selection algorithm based on association rules is better than the CART, ReliefF and RFE-SVM algorithms in terms of feature dimension and classification accuracy. The proposed integrated classification algorithm based on random equalization sampling is superior to the comparative SMOTE-Boost and SMOTE-RF algorithms in macro precision, macro-full rate and macro F1 value, which embodies the robustness of the algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300945",
    "keywords": [
      "Artificial intelligence",
      "Association rule learning",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Curse of dimensionality",
      "Data mining",
      "Dimensionality reduction",
      "Feature selection",
      "Gene",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Random forest",
      "Robustness (evolution)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Chenxi"
      },
      {
        "surname": "Huang",
        "given_name": "Xin"
      },
      {
        "surname": "Fang",
        "given_name": "Yu"
      },
      {
        "surname": "Xu",
        "given_name": "Jianfeng"
      },
      {
        "surname": "Qu",
        "given_name": "Yi"
      },
      {
        "surname": "Zhai",
        "given_name": "Pengjun"
      },
      {
        "surname": "Fan",
        "given_name": "Lin"
      },
      {
        "surname": "Yin",
        "given_name": "Hua"
      },
      {
        "surname": "Xu",
        "given_name": "Yilu"
      },
      {
        "surname": "Li",
        "given_name": "Jiahang"
      }
    ]
  },
  {
    "title": "Using persistent homology to quantify a diurnal cycle in hurricanes",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.02.022",
    "abstract": "The diurnal cycle of tropical cyclones (TCs) is a daily cycle in clouds that appears in satellite images and may have implications for TC structure and intensity. The diurnal pattern can be seen in infrared (IR) satellite imagery as cyclical pulses in the cloud field that propagate radially outward from the center of nearly all Atlantic-basin TCs. These diurnal pulses, a distinguishing characteristic of this diurnal cycle, begin forming in the storm’s inner core near sunset each day, appearing as a region of cooling cloud-top temperatures. The area of cooling takes on a ring-like appearance as cloud-top warming occurs on its inside edge and the cooling moves away from the storm overnight, reaching several hundred kilometers from the circulation center by the following afternoon. The state-of-the-art TC diurnal cycle measurement in IR satellite imagery has a limited ability to analyze the behavior beyond qualitative observations. We present a method for quantifying the TC diurnal cycle using one-dimensional persistent homology, a tool from Topological Data Analysis, by tracking maximum persistence and quantifying the cycle using the discrete Fourier transform. Using Geostationary Operational Environmental Satellite IR imagery from Hurricanes Felix and Ivan, our method is able to detect an approximate daily cycle.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300611",
    "keywords": [
      "Astronomy",
      "Atmospheric sciences",
      "Climatology",
      "Diurnal cycle",
      "Diurnal temperature variation",
      "Environmental science",
      "Geography",
      "Geology",
      "Geostationary orbit",
      "Meteorology",
      "Physics",
      "Satellite",
      "Storm"
    ],
    "authors": [
      {
        "surname": "Tymochko",
        "given_name": "Sarah"
      },
      {
        "surname": "Munch",
        "given_name": "Elizabeth"
      },
      {
        "surname": "Dunion",
        "given_name": "Jason"
      },
      {
        "surname": "Corbosiero",
        "given_name": "Kristen"
      },
      {
        "surname": "Torn",
        "given_name": "Ryan"
      }
    ]
  },
  {
    "title": "Fuzzy directional enlacement landscapes for the evaluation of complex spatial relations",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107185",
    "abstract": "Structural spatial relations between image components are fundamental in the human perception of image similarity, and constitute a challenging topic in the domain of image analysis. By definition, some specific relations are ambiguous and difficult to formalize precisely by humans. In this work, we deal with the issue of evaluating complex spatial configurations, where objects can surround each other, potentially with multiple levels of depth. Based on a recently introduced spatial relation called enlacement, which generalizes the idea of surrounding for arbitrary objects, we propose a fuzzy landscape model that allows both to visualize and evaluate this relation directly in the image space, following different directions. Experiments on several characteristic examples highlight the interest and the behavior of this approach, allowing for rich interpretations of these complex spatial configurations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304856",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Data mining",
      "Domain (mathematical analysis)",
      "Fuzzy logic",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Neuroscience",
      "Operating system",
      "Pattern recognition (psychology)",
      "Perception",
      "Relation (database)",
      "Similarity (geometry)",
      "Space (punctuation)",
      "Spatial intelligence",
      "Spatial relation"
    ],
    "authors": [
      {
        "surname": "Clément",
        "given_name": "Michaël"
      },
      {
        "surname": "Kurtz",
        "given_name": "Camille"
      },
      {
        "surname": "Wendling",
        "given_name": "Laurent"
      }
    ]
  },
  {
    "title": "Research on real-time analysis technology of urban land use based on support vector machine",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.03.022",
    "abstract": "One of the main problems that traditional support vector machine (SVM) has to solve is how to dynamically determine the kernel parameters and penalty parameters of the kernel function in time, along with the increasing amount of data and the changing data structure and characteristics. A new method is proposed for dynamic acquisition of SVM parameters by fruit fly optimization algorithm (FOA) based on the analysis of the classification and aggregation of land use data in urban industry. FOA-SVM aims at the relationship between feature words in the classification process and the core words of different activity semantics in context. In an incomplete date set of initial feature words, FOA-SVM can extract new feature words from the semantic association of feature words to improve the feature word date set. The dynamic parameters of SVM can be obtained through continuous training with FOA, and the accuracy of classification can be improved. The experimental results showed that FOA-SVM can process multi-feature synchronous classification according to different activity semantics and efficiently control the operation of the whole classification process, so as to obtain higher classification accuracy and stronger robustness in multi-source web date categorization. The efficiency of land use real-time analysis is improved.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301008",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Feature (linguistics)",
      "Gene",
      "Kernel (algebra)",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Programming language",
      "Robustness (evolution)",
      "Semantics (computer science)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Tian",
        "given_name": "Ye"
      },
      {
        "surname": "Chen",
        "given_name": "Chenru"
      },
      {
        "surname": "Chen",
        "given_name": "Xinyi"
      },
      {
        "surname": "Zhang",
        "given_name": "Qianqian"
      },
      {
        "surname": "Sun",
        "given_name": "Ruizhi"
      }
    ]
  },
  {
    "title": "Quantifying the regularity of a 3D set of points on the surface of an ellipsoidal object",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.02.012",
    "abstract": "Several natural and artificial structures, such as human skin and mammals cortices, exhibit a compound organization, with basic elements being distributed along a surface. The problem of quantifying the geometrical uniformity of this type of biological and physical compound structures is addressed in this work. This required the solution of several problems, including the detection, along the surface, of the borders of the compound system, defining the adjacency between the elements in the 3D space, and obtaining a reference of uniformity for calculating the polygonality. Specific approaches were devised and applied to address each of these difficulties, including connectivity criteria ensuring the adjacency to remain within the considered surface as well as the extension of the polygonality, originally suggested for 2D structures, to 3D compound systems. The potential of the so-obtained method is illustrated with respect to compound eyes of fungus gnats (small, forest dwelling flies), and interesting results are reported and discussed, including the fact that the uniformity tends to increase toward the center of the system, and the absence of correlation with two measurements traditionally used for characterizing this type of eyes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300519",
    "keywords": [
      "Adjacency list",
      "Algorithm",
      "Artificial intelligence",
      "Astronomy",
      "Biological system",
      "Biology",
      "Computer science",
      "Ecology",
      "Ellipsoid",
      "Extension (predicate logic)",
      "Geometry",
      "Mathematics",
      "Object (grammar)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Set (abstract data type)",
      "Space (punctuation)",
      "Surface (topology)",
      "Type (biology)"
    ],
    "authors": [
      {
        "surname": "Comin",
        "given_name": "Cesar H."
      },
      {
        "surname": "Taylor",
        "given_name": "Gavin J."
      },
      {
        "surname": "Costa",
        "given_name": "Luciano da F."
      }
    ]
  },
  {
    "title": "Modernizing historical documents: A user Study",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.02.027",
    "abstract": "Accessibility to historical documents is mostly limited to scholars. This is due to the language barrier inherent in human language and the linguistic properties of these documents. Given a historical document, modernization aims to generate a new version of it, written in the modern version of the document’s language. Its goal is to tackle the language barrier, decreasing the comprehension difficulty and making historical documents accessible to a broader audience. In this work, we proposed a new neural machine translation approach that profits from modern documents to enrich its systems. We tested this approach with both automatic and human evaluation, and conducted a user study. Results showed that modernization is successfully reaching its goal, although it still has room for improvement.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300726",
    "keywords": [
      "Artificial intelligence",
      "Comprehension",
      "Computer science",
      "Engineering",
      "Human language",
      "Language barrier",
      "Law",
      "Linguistics",
      "Machine translation",
      "Mechanical engineering",
      "Modernization theory",
      "Natural language processing",
      "Philosophy",
      "Political science",
      "Programming language",
      "Work (physics)"
    ],
    "authors": [
      {
        "surname": "Domingo",
        "given_name": "Miguel"
      },
      {
        "surname": "Casacuberta",
        "given_name": "Francisco"
      }
    ]
  },
  {
    "title": "Ensemble adversarial black-box attacks against deep learning systems",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107184",
    "abstract": "Deep learning (DL) models, e.g., state-of-the-art convolutional neural networks (CNNs), have been widely applied into security sensitivity tasks, such as face payment, security monitoring, automated driving, etc. Then their vulnerability analysis is an emergent topic, especially for black-box attacks, where adversaries do not know the model internal architectures or training parameters. In this paper, two types of ensemble-based black-box attack strategies, selective cascade ensemble strategy (SCES) and stack parallel ensemble strategy (SPES), are proposed to explore the vulnerability of DL system and potential factors that contribute to the high-efficiency attacks are explored. SCES adopts a boosting structure of ensemble learning and SPES employs a bagging structure. Moreover, two pairwise and non-pairwise diversity measures are adopted to examine the relationship between the diversity in substitutes ensembles and transferability of generated adversarial examples. Experimental results show that proposed ensemble adversarial black-box attack strategies can successfully attack the DL system with some defense mechanism, such as adversarial training and ensemble adversarial training. The experimental results also show the greater the diversity in substitute ensembles enables stronger transferability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304844",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Black box",
      "Boosting (machine learning)",
      "Computer science",
      "Computer security",
      "Convolutional neural network",
      "Deep learning",
      "Ensemble forecasting",
      "Ensemble learning",
      "Geodesy",
      "Geography",
      "Logit",
      "Machine learning",
      "Pairwise comparison",
      "Transferability",
      "Vulnerability (computing)"
    ],
    "authors": [
      {
        "surname": "Hang",
        "given_name": "Jie"
      },
      {
        "surname": "Han",
        "given_name": "Keji"
      },
      {
        "surname": "Chen",
        "given_name": "Hui"
      },
      {
        "surname": "Li",
        "given_name": "Yun"
      }
    ]
  },
  {
    "title": "Controllable digital restoration of ancient paintings using convolutional neural network and nearest neighbor",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.02.033",
    "abstract": "Ancient paintings are valuable culture legacy which can help archaeologists and culture researchers to study history and humanity. Most ancient artworks have damage problems, such as degradation, flaking and cracking. This work presents a novel controllable image inpainting framework with capability of incorporating suggestions from experts, which can help artists envisage how the ancient painting may have looked after a restoration. The framework leverages the content prediction power of deep convolutional neural network (CNN) and the nearest neighbor based pixel matching, where a deep CNN is designed to produce a coarse estimation of complete paintings by filling in missing regions and nearest neighbor based pixel matching is designed to map a mid-frequency estimation obtained from the deep CNN to high quality outputs in a controllable manner. In addition, we design a pixel descriptor using multi-scale neural features from different layers of a pre-trained deep network to capture different amounts of spatial context. Experimental results demonstrate that the proposed approach successfully predicts information in large missing regions and generates controllable high-frequency photo-realistic inpainting results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300787",
    "keywords": [
      "Archaeology",
      "Art",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Convolutional neural network",
      "Deep learning",
      "Geography",
      "Image (mathematics)",
      "Inpainting",
      "Matching (statistics)",
      "Mathematics",
      "Painting",
      "Pattern recognition (psychology)",
      "Pixel",
      "Statistics",
      "Visual arts",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Zeng",
        "given_name": "Yuan"
      },
      {
        "surname": "Gong",
        "given_name": "Yi"
      },
      {
        "surname": "Zeng",
        "given_name": "Xiangrui"
      }
    ]
  },
  {
    "title": "Feature extraction from EEG spectrograms for epileptic seizure detection",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.03.006",
    "abstract": "Identification of EEG signals is currently an open problem where performance analysis in terms of accuracy is relevant in several fields, such as biomedicine and brain computer interfaces. Nevertheless, performance depends on the feature extraction phase, where the aim is to find relevant patterns related to different mental activities. Thus, in this work, an approach to extract features from EEG signals is proposed based on spectrograms: Firstly, STFT is applied to EEG to obtain time-frequency representations, where parameters such as window length and type are experimented based on the EEG signal frequency. After that, spectral peaks are found to be used as reference in order to obtain descriptors per spectrogram. Three ways for extracting features from EEG are presented, the first based on frequency and surfaces, the second using K-means to extract features and the adaptation of local ternary pattern, and finally, a third using maximum peaks. The extracted descriptors are evaluated by means of a multilayer perceptron, support vector machines, and k-nearest neighbors. The proposed approach was evaluated using the dataset from Bonn University, identifying a healthy person and an epileptic attack classes as main task. According to the experimental results, the proposed method obtains acceptable accuracy (100%) in several cases by considering fewer features than those extracted by other related works.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300842",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Electroencephalography",
      "Feature (linguistics)",
      "Feature extraction",
      "Filter (signal processing)",
      "Fourier analysis",
      "Fourier transform",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Multilayer perceptron",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Psychiatry",
      "Psychology",
      "SIGNAL (programming language)",
      "Short-time Fourier transform",
      "Spectrogram",
      "Speech recognition",
      "Support vector machine",
      "Time–frequency analysis"
    ],
    "authors": [
      {
        "surname": "Ramos-Aguilar",
        "given_name": "Ricardo"
      },
      {
        "surname": "Olvera-López",
        "given_name": "J. Arturo"
      },
      {
        "surname": "Olmos-Pineda",
        "given_name": "Ivan"
      },
      {
        "surname": "Sánchez-Urrieta",
        "given_name": "Susana"
      }
    ]
  },
  {
    "title": "Diagnosis of carpal tunnel syndrome: A comparative study of shear wave elastography, morphometry and artificial intelligence techniques",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.02.020",
    "abstract": "Ultrasonography is an acceptable modality to evaluate median nerve (MN) in patients with carpal tunnel syndrome (CTS). Additional investigations are needed to evaluate sonographic parameters and compare their performances with artificial intelligence (AI) methods. The aim of this study is to compare the performance of shear wave elastography, morphometry, and AI techniques to predict MN entrapment accurately. 200 wrists including 100 CTS and 100 control wrists were included. Twelve morphological and five elasticity parameters were measured from each MN. Two AI techniques namely, support vector machine (SVM), and convolutional neural network (CNN) were used to diagnose CTS. MN area with area under receiver-operating characteristic curve (AUC) of 0.949 and mean elasticity with AUC of 0.942 showed the highest performance to differentiate CTS from control wrists among morphological and elasticity parameters, respectively. The CNN achieved the best performance with AUC of 0.980, while SVM obtained AUC of 0.943 in testing dataset to diagnose CTC. MN is larger, stiffer, more irregular and extended in CTS patients. Deep learning technique yielded the highest performance in diagnosing CTS automatically. AI methods have vast potential to be implemented in clinical practice as an auxiliary tool for the assessment of CTS with high accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300593",
    "keywords": [
      "Area under curve",
      "Artificial intelligence",
      "Artificial neural network",
      "Carpal tunnel",
      "Carpal tunnel syndrome",
      "Computer science",
      "Convolutional neural network",
      "Elastography",
      "Internal medicine",
      "Machine learning",
      "Median nerve",
      "Medicine",
      "Pattern recognition (psychology)",
      "Pharmacokinetics",
      "Radiology",
      "Receiver operating characteristic",
      "Support vector machine",
      "Surgery",
      "Ultrasound"
    ],
    "authors": [
      {
        "surname": "Ardakani",
        "given_name": "Ali Abbasian"
      },
      {
        "surname": "Afshar",
        "given_name": "Ahmadreza"
      },
      {
        "surname": "Bhatt",
        "given_name": "Shweta"
      },
      {
        "surname": "Bureau",
        "given_name": "Nathalie J"
      },
      {
        "surname": "Tahmasebi",
        "given_name": "Aylin"
      },
      {
        "surname": "Acharya",
        "given_name": "U Rajendra"
      },
      {
        "surname": "Mohammadi",
        "given_name": "Afshin"
      }
    ]
  },
  {
    "title": "Automated invasive ductal carcinoma detection based using deep transfer learning with whole-slide images",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.03.011",
    "abstract": "Advances in artificial intelligence technologies have made it possible to obtain more accurate and reliable results using digital images. Due to the advances in digital histopathological images obtained using whole slide image (WSI) scanners, automated analysis of digital images by computer support systems has become interesting. In particular, deep learning architectures, are one of the preferred approaches in the analysis of digital histopathology images. The deeper networks trained on large amounts of image data are adapted for different tasks using transfer learning technique. In this study, automated detection of invasive ductal carcinoma (IDC), which is the most common subtype of breast cancers, is proposed using deep transfer learning technique. We have used deep learning pre-trained models, ResNet-50 and DenseNet-161 for the IDC detection task. The public histopathology dataset containing 277,524 image patches were used in our experimental studies. As a result of training on the last layers of pre-trained deep networks, DenseNet-161 model has yielded F-sore of 92.38% and balanced accuracy value of 91.57%. Similarly, we have obtained F-score of 94.11% and balanced accuracy value of 90.96% using ResNet-50 architecture. In addition, our developed model is validated using the publicly available BreakHis breast cancer dataset and obtained promising results in classifying magnification independent histopathology images into benign and malignant classes. Our developed system obtained the highest classification performance as compared to the state-of-art techniques and is ready to be tested with more diverse huge databases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300891",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Deep learning",
      "Digital image",
      "Digital pathology",
      "Image (mathematics)",
      "Image processing",
      "Machine learning",
      "Magnification",
      "Pattern recognition (psychology)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Celik",
        "given_name": "Yusuf"
      },
      {
        "surname": "Talo",
        "given_name": "Muhammed"
      },
      {
        "surname": "Yildirim",
        "given_name": "Ozal"
      },
      {
        "surname": "Karabatak",
        "given_name": "Murat"
      },
      {
        "surname": "Acharya",
        "given_name": "U Rajendra"
      }
    ]
  },
  {
    "title": "Supervised learning of the next-best-view for 3d object reconstruction",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.02.024",
    "abstract": "Motivated by the advances in 3D sensing technology and the spreading of low-cost robotic platforms, 3D object reconstruction has become a common task in many areas. Nevertheless, the selection of the optimal sensor pose that maximizes the reconstructed surface is a problem that remains open. It is known in the literature as the next-best-view planning problem. In this paper, we propose a novel next-best-view planning scheme based on supervised deep learning. The scheme contains an algorithm for automatic generation of datasets and an original three-dimensional convolutional neural network (3D-CNN) used to learn the next-best-view. Unlike previous work where the problem is addressed as a search, the trained 3D-CNN directly predicts the sensor pose. We present an experimental comparison of the proposed architecture against two alternative networks; we also compare it with state-of-the-art next-best-view methods in the reconstruction of several unknown objects. Our method is faster and reaches high coverage.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518305531",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Cognitive neuroscience of visual object recognition",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Economics",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Scheme (mathematics)",
      "Selection (genetic algorithm)",
      "Supervised learning",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Mendoza",
        "given_name": "Miguel"
      },
      {
        "surname": "Vasquez-Gomez",
        "given_name": "J. Irving"
      },
      {
        "surname": "Taud",
        "given_name": "Hind"
      },
      {
        "surname": "Sucar",
        "given_name": "L. Enrique"
      },
      {
        "surname": "Reta",
        "given_name": "Carolina"
      }
    ]
  },
  {
    "title": "Visual question answering with attention transfer and a cross-modal gating mechanism",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.02.031",
    "abstract": "Visual question answering (VQA) is challenging since it requires to understand both language information and corresponding visual contents. A lot of efforts have been made to capture single-step language and visual interactions. However, answering complex questions requires multiple steps of reasoning which gradually adjusts the region of interest to the most relevant part of the given image, which has not been well investigated. To integrate question related object relations into attention mechanism, we propose a multi-step attention architecture to facilitate the modeling of multi-modal correlations. Firstly, an attention transfer mechanism is integrated to gradually adjust the region of interest considering reasoning representation of questions. Secondly, we propose a cross-modal gating strategy to filter out irrelevant information based on multi-modal correlations. Finally, we achieve the state-of-the-art performance on the VQA 1.0 dataset and favorable results on the VQA 2.0 dataset, which verifies the effectiveness of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300763",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Epistemology",
      "Filter (signal processing)",
      "Gating",
      "Law",
      "Machine learning",
      "Mechanism (biology)",
      "Modal",
      "Natural language processing",
      "Object (grammar)",
      "Philosophy",
      "Physiology",
      "Political science",
      "Politics",
      "Polymer chemistry",
      "Question answering",
      "Representation (politics)",
      "Visual reasoning"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Wei"
      },
      {
        "surname": "Sun",
        "given_name": "Jianhui"
      },
      {
        "surname": "Liu",
        "given_name": "Ge"
      },
      {
        "surname": "Zhao",
        "given_name": "Linglan"
      },
      {
        "surname": "Fang",
        "given_name": "Xiangzhong"
      }
    ]
  },
  {
    "title": "Multiple Instance Learning with Genetic Pooling for medical data analysis",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.02.025",
    "abstract": "Multiple Instance Learning is a weakly supervised learning technique which is particularly well suited for medical data analysis as the class labels are often not available at desired granularity. Multiple Instance Learning through Deep Neural Networks is relatively a new paradigm in machine learning. The most important part of Multiple Instance Learning through Deep Neural Networks is designing a trainable pooling function which determines the instance-to bag relationship. In this paper, we propose a Multiple Instance pooling technique based on Genetic Algorithm called Genetic Pooling. In this technique, instance labels inside a bag are optimized by minimizing bag-level losses. The main contribution of the paper is that the bag level pooling layer for generating attention weights for bag instances are replaced by random initialization of attention weights and finding the optimized attention weights through Genetic Algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300702",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Class (philosophy)",
      "Computer science",
      "Data mining",
      "Genetic algorithm",
      "Granularity",
      "Initialization",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Pooling",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Bhattacharjee",
        "given_name": "Kamanasish"
      },
      {
        "surname": "Pant",
        "given_name": "Millie"
      },
      {
        "surname": "Zhang",
        "given_name": "Yu-Dong"
      },
      {
        "surname": "Satapathy",
        "given_name": "Suresh Chandra"
      }
    ]
  },
  {
    "title": "Multi-scale superpatch matching using dual superpixel descriptors",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.02.018",
    "abstract": "Over-segmentation into superpixels is a very effective dimensionality reduction strategy, enabling fast dense image processing. The main issue of this approach is the inherent irregularity of the image decomposition compared to standard hierarchical multi-resolution schemes, especially when searching for similar neighboring patterns. Several works have attended to overcome this issue by taking into account the region irregularity into their comparison model. Nevertheless, they remain sub-optimal to provide robust and accurate superpixel neighborhood descriptors, since they only compute features within each region, poorly capturing contour information at superpixel borders. In this work, we address these limitations by introducing the dual superpatch, a novel superpixel neighborhood descriptor. This structure contains features computed in reduced superpixel regions, as well as at the interfaces of multiple superpixels to explicitly capture contour structure information. A fast multi-scale non-local matching framework is also introduced for the search of similar descriptors at different resolution levels in an image dataset. The proposed dual superpatch enables to more accurately capture similar structured patterns at different scales, and we demonstrate the robustness and performance of this new strategy on matching and supervised labeling applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552030057X",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Curse of dimensionality",
      "Dimensionality reduction",
      "Dual (grammatical number)",
      "Gene",
      "Image (mathematics)",
      "Literature",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Robustness (evolution)",
      "Scale (ratio)",
      "Segmentation",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Giraud",
        "given_name": "Rémi"
      },
      {
        "surname": "Boyer",
        "given_name": "Merlin"
      },
      {
        "surname": "Clément",
        "given_name": "Michaël"
      }
    ]
  },
  {
    "title": "Validating the robustness of an internet of things based atrial fibrillation detection system",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.02.005",
    "abstract": "This paper describes the validation of a deep learning model for Internet of Things (IoT) based health care applications. As such, the deep learning model was created to detect episodes of Atrial Fibrillation (AF) using Heart Rate (HR) signals. The initial Long Short-Term Memory (LSTM) model was developed using 20 data sets, from distinct subjects, obtained from the AFDB database on PhysioNet. This model achieved an AF detection accuracy of 98.51% with ten fold cross validation. In this study, we validated the initial results by testing the developed deep learning model with unknown data. To be specific, we fed the data from 82 subjects to the deep learning system and compared the classification results with the diagnosis results indicated by human practitioners. The validation results show 94% accuracy with an area under the Receiver Operating Characteristic (ROC) curve of 96.58. These results indicate that the LSTM model is able to extract the feature maps from the unknown data and hence detect the AF periods accurately. With this blindfold validation testing we violated a well known design rule for learning systems which states that more data should be used for training than for testing. By doing so, we have established that our deep learning system is fit for practical deployment, because in a practical situation the diagnosis support system must apply the knowledge, extracted from a limited training data set, to a HR trace from a patient.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552030043X",
    "keywords": [
      "Artificial intelligence",
      "Atrial fibrillation",
      "Biochemistry",
      "Cardiology",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Gene",
      "Machine learning",
      "Medicine",
      "Operating system",
      "Programming language",
      "Receiver operating characteristic",
      "Robustness (evolution)",
      "Software deployment",
      "Test data",
      "The Internet",
      "Training set",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Faust",
        "given_name": "Oliver"
      },
      {
        "surname": "Kareem",
        "given_name": "Murtadha"
      },
      {
        "surname": "Shenfield",
        "given_name": "Alex"
      },
      {
        "surname": "Ali",
        "given_name": "Ali"
      },
      {
        "surname": "Acharya",
        "given_name": "U Rajendra"
      }
    ]
  },
  {
    "title": "Discriminative distribution alignment: A unified framework for heterogeneous domain adaptation",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107165",
    "abstract": "Heterogeneous domain adaptation (HDA) aims to leverage knowledge from a source domain for helping learn an accurate model in a heterogeneous target domain. HDA is exceedingly challenging since the feature spaces of domains are distinct. To tackle this issue, we propose a unified learning framework called Discriminative Distribution Alignment (DDA) for deriving a domain-invariant subspace. The proposed DDA can simultaneously match the discriminative directions of domains, align the distributions across domains, and enhance the separability of data during adaptation. To achieve this, DDA trains an adaptive classifier by both reducing the distribution divergence and enlarging distances between class centroids. Based on the proposed DDA framework, we further develop two methods, by embedding the cross-entropy loss and squared loss into this framework, respectively. We conduct experiments on the tasks of categorization across domains and modalities. Experimental results clearly demonstrate that the proposed DDA outperforms several state-of-the-art models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304650",
    "keywords": [
      "Artificial intelligence",
      "Centroid",
      "Classifier (UML)",
      "Computer science",
      "Discriminative model",
      "Domain adaptation",
      "Embedding",
      "Entropy (arrow of time)",
      "Leverage (statistics)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Yao",
        "given_name": "Yuan"
      },
      {
        "surname": "Zhang",
        "given_name": "Yu"
      },
      {
        "surname": "Li",
        "given_name": "Xutao"
      },
      {
        "surname": "Ye",
        "given_name": "Yunming"
      }
    ]
  },
  {
    "title": "Feature fusion network based on attention mechanism for 3D semantic segmentation of point clouds",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.03.021",
    "abstract": "3D scene parsing has always been a hot topic and point clouds are efficient data format to represent scenes. The semantic segmentation of point clouds is critical to the 3D scene, which is a challenging problem due to the unordered structure of point clouds. The max-pooling operation is typically used to obtain the order invariant features, while the point-wise features are destroyed after the max-pooling operation. In this paper, we propose a feature fusion network that fuses point-wise features and local features by attention mechanism to compensate for the loss caused by max-pooling operation. By incorporating point-wise features into local features, the point-wise variation is preserved to obtain a refined segmentation accuracy, and the attention mechanism is used to measure the importance of the point-wise features and local features for each 3D point. Extensive experiments show that our method achieves better performances than other prestigious methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300994",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Parsing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point (geometry)",
      "Point cloud",
      "Pooling",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Heng"
      },
      {
        "surname": "Fang",
        "given_name": "Zhijun"
      },
      {
        "surname": "Gao",
        "given_name": "Yongbin"
      },
      {
        "surname": "Huang",
        "given_name": "Bo"
      },
      {
        "surname": "Zhong",
        "given_name": "Cengsi"
      },
      {
        "surname": "Shang",
        "given_name": "Ruoxi"
      }
    ]
  },
  {
    "title": "Learning deep face representation with long-tail data: An aggregate-and-disperse approach",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.02.007",
    "abstract": "In this work, we study the problem of deep representation learning on a large face dataset with long-tail distribution. Training convolutional neural networks on such dataset with conventional strategy suffers from imbalance problem which results in biased classification boundary, and the few-shot classes lying in tail parts further make the model prone to overfitting. Aiming to learn more discriminative CNN model from long-tail data, we propose a novel aggregate-and-disperse training schema. Firstly, our proposed method aggregates similar classes in tail part to avoid imbalance problem. Based on the aggregated super classes and those original head classes, a model is pre-trained to capture accurate discrimination in head classes as well as coarse discrinimation in tail classes. Secondly, we selectively disperses those aggregated super classes to learn precise inter-class variations and refine the representation for better generalization. We perform extensive experiments on MS-Celeb-1M, BLUFR and MegaFace. Compared with baselines and existing methods, our method achieves better performance of face recognition, demonstrating its effectiveness of handling long-tail distribution.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300465",
    "keywords": [
      "Aggregate (composite)",
      "Artificial intelligence",
      "Artificial neural network",
      "Composite material",
      "Computer science",
      "Convolutional neural network",
      "Discriminative model",
      "Face (sociological concept)",
      "Generalization",
      "Law",
      "Long tail",
      "Machine learning",
      "Materials science",
      "Mathematical analysis",
      "Mathematics",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Social science",
      "Sociology",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Yuhao"
      },
      {
        "surname": "Kan",
        "given_name": "Meina"
      },
      {
        "surname": "Shan",
        "given_name": "Shiguang"
      },
      {
        "surname": "Chen",
        "given_name": "Xilin"
      }
    ]
  },
  {
    "title": "Adjusting the imbalance ratio by the dimensionality of imbalanced data",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.03.004",
    "abstract": "Class-imbalance extent metrics measure how imbalanced the data are. In pattern classification, it is usually expected that the higher the imbalance extent, the worse the classification performance, and thus an appropriate imbalance extent metric should show a negative correlation with the classification performance. Existing metrics, such as the popular imbalance ratio (IR), only consider the effect of the sample sizes of different classes. However, we note that the dimensionality of imbalanced data also affects the classification performance. Datasets with the same IR can present distinct classification performances when their dimensionalities are different, making IR suboptimal to reflect the imbalance extent for classification. We also observe that the classification performance becomes better with more discriminative features. Inspired by these observations, we propose a new imbalance extent metric, the adjusted IR, by adding a penalty term of the number of discriminative features that is effectively determined by the Pearson correlation test. The adjusted IR adaptively revises the IR when the number of discriminative features varies. The empirical studies demonstrate the effectiveness of the adjusted IR, in terms of its better negative correlation with the classification performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300829",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Correlation",
      "Curse of dimensionality",
      "Discriminative model",
      "Economics",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Rui"
      },
      {
        "surname": "Guo",
        "given_name": "Yiwen"
      },
      {
        "surname": "Xue",
        "given_name": "Jing-Hao"
      }
    ]
  },
  {
    "title": "Text alignment in early printed books combining deep learning and dynamic programming",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.02.016",
    "abstract": "We describe a technique for transcript alignment in early printed books by using deep models in combination with dynamic programming algorithms. Two object detection models, based on Faster R-CNN, are trained to locate words. We first train an initial model to recognize generic words and hyphens by using information about the number of words in text lines. Using the model prediction on pages with a line-by-line ground-truth annotation is available, we train a second model able to detect landmark words. The alignment is then based on the identification of landmark words in pages where we only know the text corresponding to zones in the page. The proposed technique is evaluated on a publicly available digitization of the Gutenberg Bible while the transcription is based on the Vulgata, a late 4th century Latin translation of the Bible.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300568",
    "keywords": [
      "Algorithm",
      "Annotation",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Digitization",
      "Dynamic programming",
      "Geometry",
      "Ground truth",
      "Information retrieval",
      "Landmark",
      "Line (geometry)",
      "Linguistics",
      "Mathematics",
      "Natural language processing",
      "Philosophy",
      "Transcription (linguistics)"
    ],
    "authors": [
      {
        "surname": "Ziran",
        "given_name": "Zahra"
      },
      {
        "surname": "Pic",
        "given_name": "Xavier"
      },
      {
        "surname": "Undri Innocenti",
        "given_name": "Simone"
      },
      {
        "surname": "Mugnai",
        "given_name": "Daniele"
      },
      {
        "surname": "Marinai",
        "given_name": "Simone"
      }
    ]
  },
  {
    "title": "Graph based semi-supervised classification with probabilistic nearest neighbors",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.01.021",
    "abstract": "Label propagation (LP) is one of the state-of-the-art graph based semi-supervised learning (GSSL) algorithm. Probability transition matrix (PTM) is the key for LP to propagate label information among samples. Conventionally, PTM is calculated based on the graph constructed in advance, and graph construction independent of PTM calculation. It leads to complex steps for acquiring PTM, and more importantly, brings about the lack of correlation between graph construction and inference. Based on adaptive neighbors-based method, probabilistic nearest neighbors (PNN) based graph construction algorithm is proposed for effective ℓ2 norm optimization, and the solving process of the objective function is optimized by incorporating min-max normalization. The derived PNN matrix is more discriminative and directly serve as PTM for LP. It makes PTM computation more conveniently and more applicable for classification task. In addition, number of neighbors is adaptively determined on the premise of its preset value. Experimental results show that the proposed PNN algorithm specializes in reflecting probability differences of neighboring nodes in a graph, and positive results are achieved in semi-supervised classification. The average classification accuracy on synthetic data sets is 84.24%, and that on image data sets achieves 89.08%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300337",
    "keywords": [
      "Algorithm",
      "Anthropology",
      "Artificial intelligence",
      "Computation",
      "Computer science",
      "Discriminative model",
      "Graph",
      "Mathematics",
      "Normalization (sociology)",
      "Pattern recognition (psychology)",
      "Probabilistic logic",
      "Sociology",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Junliang"
      },
      {
        "surname": "Xiao",
        "given_name": "Bing"
      },
      {
        "surname": "Deng",
        "given_name": "Cheng"
      }
    ]
  },
  {
    "title": "Measuring user relevance in online debates through an argumentative model",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.02.008",
    "abstract": "Online debating forums are important social media for people to voice their opinions and engage in debates with each other. Measuring user relevance on these forums can be useful to identify different user profiles or behaviors in online debates, for example, users that tend to participate at the beginning of a debate and whose comments trigger participation, or users that post relevant comments but are not replied too much. To help users to distinguish such different user profiles, we propose graded measures based on users’ influence, the controversy that they generate throughout the debates, their contribution to the polarization of the debates, and their social acceptance, that we extract by analyzing the debates in which the users participate. Our approach is based on an argumentation-based analysis that represents a debate as a valued argumentation framework, in which comments of a debate are arguments, the attack relation between arguments models disagreement between comments, and values for arguments represent the overall support of users for comments. Finally, we test our measures with a sample of users from Reddit debates, identifying four main groups of users, from users with almost no impact on the debate to very active ones with decisive comments for the outcome of the debate.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300477",
    "keywords": [
      "Argumentation theory",
      "Argumentative",
      "Computer science",
      "Database",
      "Epistemology",
      "Internet privacy",
      "Law",
      "Online discussion",
      "Philosophy",
      "Political science",
      "Psychology",
      "Relation (database)",
      "Relevance (law)",
      "Social media",
      "Social psychology",
      "Sociology",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Alsinet",
        "given_name": "Teresa"
      },
      {
        "surname": "Argelich",
        "given_name": "Josep"
      },
      {
        "surname": "Béjar",
        "given_name": "Ramón"
      },
      {
        "surname": "Martínez",
        "given_name": "Santi"
      }
    ]
  },
  {
    "title": "Multiscale summarization and action ranking in egocentric videos",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.02.029",
    "abstract": "Useful information extraction from egocentric videos has evolved as an important research problem for both computer vision and multimedia communities. In this paper, we have addressed two problems, namely (i) generating multiscale summaries, i.e., multiple summaries of different lengths and (ii) priority-based ranking of various actions present in egocentric videos. A new algorithm, termed as Multiscale Egocentric Video Summarization and Action Ranking (MEVSAR), with agglomerative clustering as its backbone, is proposed to solve the above problems. Importantly, the MEVSAR algorithm follows an “analyze once, generate many” principle to generate multiple summaries in a single run and subsequently rank actions from the generated summaries. Experimental evaluation on two well-known publicly available datasets clearly demonstrate the merits of the proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552030074X",
    "keywords": [
      "Action (physics)",
      "Action recognition",
      "Artificial intelligence",
      "Automatic summarization",
      "Class (philosophy)",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Information retrieval",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Rank (graph theory)",
      "Ranking (information retrieval)"
    ],
    "authors": [
      {
        "surname": "Sahu",
        "given_name": "Abhimanyu"
      },
      {
        "surname": "Chowdhury",
        "given_name": "Ananda S."
      }
    ]
  },
  {
    "title": "Fitting local, low-dimensional parameterizations of optical turbulence modeled from optimal transport velocity vectors",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.10.023",
    "abstract": "This work exploits a connection between optimal transport theory and the physics of image propagation to yield a locally low-dimensional model of turbulence-corrupted imagery. Optimal transport produces an invertible, pixel-wise linear trajectories to approximate the globally nonlinear turbulence between a clean and turbulence corrupted image pair. We use the low-dimensional model to fit subsets of the optimal transport vector fields and stitch the local models into a surrogate for the global map to be used for image cleaning. Experiments are performed on laboratory generated data of beam propagation using different values of the Fried parameter (a scale measuring turbulence coherence) as well as a toy data set. The results suggest this is a fruitful direction, and first step, towards using multiple realizations of turbulence corrupted images to learn a blind surrogate for the optimal transport vector field for image cleaning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303022",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Coherence (philosophical gambling strategy)",
      "Computer science",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Mechanics",
      "Nonlinear system",
      "Physics",
      "Pixel",
      "Quantum mechanics",
      "Statistical physics",
      "Statistics",
      "Turbulence",
      "Vector field"
    ],
    "authors": [
      {
        "surname": "Emerson",
        "given_name": "Tegan H."
      },
      {
        "surname": "Nichols",
        "given_name": "Jonathan M."
      }
    ]
  },
  {
    "title": "Towards explaining anomalies: A deep Taylor decomposition of one-class models",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107198",
    "abstract": "Detecting anomalies in the data is a common machine learning task, with numerous applications in the sciences and industry. In practice, it is not always sufficient to reach high detection accuracy, one would also like to be able to understand why a given data point has been predicted to be anomalous. We propose a principled approach for one-class SVMs (OC-SVM), that draws on the novel insight that these models can be rewritten as distance/pooling neural networks. This ‘neuralization’ step lets us apply deep Taylor decomposition (DTD), a methodology that leverages the model structure in order to quickly and reliably explain decisions in terms of input features. The proposed method (called ‘OC-DTD’) is applicable to a number of common distance-based kernel functions, and it outperforms baselines such as sensitivity analysis, distance to nearest neighbor, or edge detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300054",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Class (philosophy)",
      "Computer science",
      "Decomposition",
      "Econometrics",
      "Mathematics",
      "Organic chemistry"
    ],
    "authors": [
      {
        "surname": "Kauffmann",
        "given_name": "Jacob"
      },
      {
        "surname": "Müller",
        "given_name": "Klaus-Robert"
      },
      {
        "surname": "Montavon",
        "given_name": "Grégoire"
      }
    ]
  },
  {
    "title": "Learning to realign hierarchy for image segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.03.010",
    "abstract": "A hierarchical image segmentation is a set of image segmentations at different detail levels. However, objects (or even parts of the same object) may appear at different scales due to their size differences or to their distinct distances from the camera. One possible solution to cope with that is to realign the hierarchy such that every region containing an object (or its parts) is at the same level. In this work, we have explored the use of regression models to predict score values for regions belonging to a hierarchy of partitions, which are used to realign it. We have also proposed a new score calculation and a new assessment strategy considering all user-defined segmentations that exist in the ground-truth. Experimental results have pointed out that the use of new proposed score was able to improve final segmentation results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552030088X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Economics",
      "Ground truth",
      "Hierarchy",
      "Image (mathematics)",
      "Image segmentation",
      "Market economy",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Regression",
      "Scale-space segmentation",
      "Segmentation",
      "Segmentation-based object categorization",
      "Set (abstract data type)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Adão",
        "given_name": "Milena M."
      },
      {
        "surname": "Guimarães",
        "given_name": "Silvio Jamil F."
      },
      {
        "surname": "Patrocínio Jr",
        "given_name": "Zenilton K.G."
      }
    ]
  },
  {
    "title": "Enhancing deep neural networks via multiple kernel learning",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107194",
    "abstract": "Deep neural networks and Multiple Kernel Learning are representation learning methodologies of widespread use and increasing success. While the former aims at learning representations through a hierarchy of features of increasing complexity, the latter provides a principled approach for the combination of base representations. In this paper, we introduce a general framework in which the internal representations computed by a deep neural network are optimally combined by means of Multiple Kernel Learning. The resulting ensemble methodology is instantiated for Multi-layer Perceptrons architectures (both fully trained and with random-weights), and for Convolutional Neural Networks. Experimental results on several benchmark datasets concretely show the advantages and potentialities of the proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300017",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Combinatorics",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Geodesy",
      "Geography",
      "Kernel (algebra)",
      "Kernel method",
      "Law",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Perceptron",
      "Political science",
      "Politics",
      "Radial basis function kernel",
      "Representation (politics)",
      "Support vector machine",
      "Tree kernel"
    ],
    "authors": [
      {
        "surname": "Lauriola",
        "given_name": "Ivano"
      },
      {
        "surname": "Gallicchio",
        "given_name": "Claudio"
      },
      {
        "surname": "Aiolli",
        "given_name": "Fabio"
      }
    ]
  },
  {
    "title": "Metric Learning from Imbalanced Data with Generalization Guarantees",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.03.008",
    "abstract": "Since many machine learning algorithms require a distance metric to capture dis/similarities between data points, metric learning has received much attention during the past decade. Surprisingly, very few methods have focused on learning a metric in an imbalanced scenario where the number of positive examples is much smaller than the negatives, and even fewer derived theoretical guarantees in this setting. Here, we address this difficult task and design a new Mahalanobis metric learning algorithm (IML) which deals with class imbalance. We further prove a generalization bound involving the proportion of positive examples using the uniform stability framework. The empirical study performed on a wide range of datasets shows the efficiency of IML.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300866",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Composite material",
      "Computer science",
      "Discrete mathematics",
      "Economics",
      "Generalization",
      "Generalization error",
      "Machine learning",
      "Mahalanobis distance",
      "Management",
      "Materials science",
      "Mathematical analysis",
      "Mathematics",
      "Metric (unit)",
      "Metric space",
      "Operations management",
      "Range (aeronautics)",
      "Stability (learning theory)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Gautheron",
        "given_name": "Leo"
      },
      {
        "surname": "Habrard",
        "given_name": "Amaury"
      },
      {
        "surname": "Morvant",
        "given_name": "Emilie"
      },
      {
        "surname": "Sebban",
        "given_name": "Marc"
      }
    ]
  },
  {
    "title": "A lightweight face detector by integrating the convolutional neural network with the image pyramid",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.03.002",
    "abstract": "Recently, significant improvements on face detection have been achieved by convolutional neural networks. However, the speed of a CNN-based face detector is hampered by the large computational complexity and the huge number of parameters. It is still challenging to achieve real-time face detection as well as maintain high accuracy. In this work, we introduce a single-stage face detector with an extremely lightweight CNN to achieve fast and accurate detection. Specifically, our method has a structure to integrate the network with the image pyramid for fully utilizing the calculated features. Benefiting from weight sharing, the network size still can keep small. We also analysis the detection capability of anchors with various scales, and reserve the most effective anchors in our model. Besides, to avoid the too difficult training samples which the small network can’t learn, each ground truth face is assigned with a 0-or-1 weight during training. When tested on WIDERFACE and FDDB, it outperforms existing lightweight face detectors on accuracy with the smallest model size. The outstanding detection performance and lightweight model size signify its effectiveness and practicability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300805",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Detector",
      "Face (sociological concept)",
      "Face detection",
      "Facial recognition system",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pyramid (geometry)",
      "Social science",
      "Sociology",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Jiapeng"
      },
      {
        "surname": "Liu",
        "given_name": "Jiaying"
      },
      {
        "surname": "Lin",
        "given_name": "Jun"
      },
      {
        "surname": "Wang",
        "given_name": "Zhongfeng"
      }
    ]
  },
  {
    "title": "Machine Learning for Cultural Heritage: A Survey",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.02.017",
    "abstract": "The application of Machine Learning (ML) to Cultural Heritage (CH) has evolved since basic statistical approaches such as Linear Regression to complex Deep Learning models. The question remains how much of this actively improves on the underlying algorithm versus using it within a ‘black box’ setting. We survey across ML and CH literature to identify the theoretical changes which contribute to the algorithm and in turn them suitable for CH applications. Alternatively, and most commonly, when there are no changes, we review the CH applications, features and pre/post-processing which make the algorithm suitable for its use. We analyse the dominant divides within ML, Supervised, Semi-supervised and Unsupervised, and reflect on a variety of algorithms that have been extensively used. From such an analysis, we give a critical look at the use of ML in CH and consider why CH has only limited adoption of ML.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300532",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Cultural heritage",
      "Geography",
      "Machine learning",
      "Supervised learning",
      "Unsupervised learning",
      "Variety (cybernetics)"
    ],
    "authors": [
      {
        "surname": "Fiorucci",
        "given_name": "Marco"
      },
      {
        "surname": "Khoroshiltseva",
        "given_name": "Marina"
      },
      {
        "surname": "Pontil",
        "given_name": "Massimiliano"
      },
      {
        "surname": "Traviglia",
        "given_name": "Arianna"
      },
      {
        "surname": "Del Bue",
        "given_name": "Alessio"
      },
      {
        "surname": "James",
        "given_name": "Stuart"
      }
    ]
  },
  {
    "title": "Image segmentation based on ultimate levelings: From attribute filters to machine learning strategies",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.03.013",
    "abstract": "Ultimate levelings are operators that extract important image contrast information from a scale-space based on levelings. Along with the residual extraction process, some residues usually come from undesirable regions, and they should be filtered out. For this, some strategies can be applied to filter these undesirable residues. In this paper, we introduce a new approach to detect desirable regions from ultimate levelings through a new hierarchical structure called residual tree. From this structure, we extract attribute vectors to build a machine learning model which gives a matching value between ground truth regions and residual tree nodes. Thus, from the selected residual tree nodes, we present a new approach to choose the best disjoint residual nodes which gives the regions of the ultimate levelings. Finally, from the ultimate levelings, we use its partition associated function to solve the segmentation problem. In order to evaluate our new approach, some experiments were carried out with a plant dataset and results report the state-of-the-art performance in plant segmentation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552030091X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Binary tree",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Disjoint sets",
      "Ground truth",
      "Image segmentation",
      "Machine learning",
      "Matching (statistics)",
      "Mathematical analysis",
      "Mathematics",
      "Partition (number theory)",
      "Pattern recognition (psychology)",
      "Residual",
      "Segmentation",
      "Statistics",
      "Tree (set theory)",
      "Tree structure"
    ],
    "authors": [
      {
        "surname": "Alves",
        "given_name": "Wonder A.L."
      },
      {
        "surname": "Gobber",
        "given_name": "Charles F."
      },
      {
        "surname": "Silva",
        "given_name": "Dennis J."
      },
      {
        "surname": "Morimitsu",
        "given_name": "Alexandre"
      },
      {
        "surname": "Hashimoto",
        "given_name": "Ronaldo F."
      },
      {
        "surname": "Marcotegui",
        "given_name": "Beatriz"
      }
    ]
  },
  {
    "title": "UcoSLAM: Simultaneous localization and mapping by fusion of keypoints and squared planar markers",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107193",
    "abstract": "Simultaneous Localization and Mapping is the process of simultaneously creating a map of the environment while navigating in it. Most of the SLAM approaches use natural features (e.g. keypoints) that are unstable over time, repetitive in many cases or their number insufficient for a robust tracking (e.g. in indoor buildings). Other researchers, on the other hand, have proposed the use of artificial landmarks, such as squared fiducial markers, placed in the environment to help tracking and relocalization. This paper proposes a novel SLAM approach by fusing natural and artificial landmarks in order to achieve long-term robust tracking in many scenarios. Our method has been compared to the start-of-the-art methods ORB-SLAM2 [1], LDSO [2] and SPM-SLAM [3] in the public datasets Kitti [4], Euroc-MAV [5], TUM [6] and SPM [3], obtaining better precision, robustness and speed. Our tests also show that the combination of markers and keypoints achieves better accuracy than each one of them independently.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304923",
    "keywords": [],
    "authors": [
      {
        "surname": "Muñoz-Salinas",
        "given_name": "Rafael"
      },
      {
        "surname": "Medina-Carnicer",
        "given_name": "R."
      }
    ]
  },
  {
    "title": "Text-to-image via mask anchor points",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.02.013",
    "abstract": "Text-to-image is a process of generating an image from the input text. It has a variety of applications in art generation, computer-aided design, and data synthesis. In this paper, we propose a new framework which leverages mask anchor points to incorporate two major steps in the image synthesis. In the first step, the mask image is generated from the input text and the mask dataset. In the second step, the mask image is fed into the state-of-the-art mask-to-image generator. Note that the mask image captures the semantic information and the location relationship via the anchor points. We also developed a user-friendly interface which helps parse the input text into the meaningful semantic objects. As a result, our framework is able to produce clear, reasonable, and more realistic images. The experiments on the most challenging COCO-stuff dataset illustrate the superiority of our proposed approach over the previous state of the arts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300520",
    "keywords": [
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Generator (circuit theory)",
      "Image (mathematics)",
      "Image editing",
      "Image synthesis",
      "Parsing",
      "Physics",
      "Power (physics)",
      "Process (computing)",
      "Programming language",
      "Quantum mechanics",
      "Variety (cybernetics)"
    ],
    "authors": [
      {
        "surname": "Baraheem",
        "given_name": "Samah S."
      },
      {
        "surname": "Nguyen",
        "given_name": "Tam V."
      }
    ]
  },
  {
    "title": "The classification of gliomas based on a Pyramid dilated convolution resnet model",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.03.007",
    "abstract": "Gliomas are characterized by high morbidity and high mortality in primary tumors. The identification of glioma type is helpful for radiologists to facilitate correct medical judgments and better prognosis for patients. In order to avoid harm to patients caused by a biopsy, radiologists attempt to classify Magnetic Resonance Images(MRI) using deep learning methods. In the present paper, we propose a deep learning convolutional neural network ResNet based on the pyramid dilated convolution for Gliomas classification. The pyramid dilated convolution is integrated into the bottom of Resnet to increase the receptive field of the original network and improve the classification accuracy. After adding the pyramid dilated convolution model, the receptive field of the original network underlying convolution was improved. A clinical dataset is used to test the pyramid dilated convolution ResNet neural network model proposed in this paper. The experimental results demonstrate that the proposed method can effectively improve glioma classification performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300854",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Deep learning",
      "Geometry",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pyramid (geometry)",
      "Residual neural network"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Zhenyu"
      },
      {
        "surname": "Bai",
        "given_name": "Yanzhong"
      },
      {
        "surname": "Chen",
        "given_name": "Yi"
      },
      {
        "surname": "Su",
        "given_name": "Chunqiu"
      },
      {
        "surname": "Lu",
        "given_name": "Shanshan"
      },
      {
        "surname": "Zhan",
        "given_name": "Tianming"
      },
      {
        "surname": "Hong",
        "given_name": "Xunning"
      },
      {
        "surname": "Wang",
        "given_name": "Shuihua"
      }
    ]
  },
  {
    "title": "Multi-task CNN for restoring corrupted fingerprint images 1",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107203",
    "abstract": "Fingerprint image enhancement is one of the fundamental modules in an automated fingerprint recognition system (AFRS). While the performance of AFRS advances with sophisticated fingerprint matching algorithms, poor fingerprint image quality remains a major issue to achieve accurate fingerprint recognition. In this paper, we present a multi-task convolutional neural network (CNN) based method to recover fingerprint ridge structures from corrupted fingerprint images. By learning from the noises and corruptions caused by various undesirable conditions of finger and sensor, the proposed CNN model consists of two streams that reconstruct the fingerprint image and orientation field simultaneously. The enhanced fingerprint is further refined using the orientation field information. Moreover, we create a deliberately corrupted fingerprint image dataset associated with ground truth images to facilitate the supervised learning of the proposed CNN model. Experimental results show significant improvement on both image quality and fingerprint matching accuracy after applying the proposed fingerprint image enhancement technique to several well-known fingerprint datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300108",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Field (mathematics)",
      "Fingerprint (computing)",
      "Fingerprint Verification Competition",
      "Fingerprint recognition",
      "Geometry",
      "Image (mathematics)",
      "Image quality",
      "Matching (statistics)",
      "Mathematics",
      "Minutiae",
      "Orientation (vector space)",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wong",
        "given_name": "Wei Jing"
      },
      {
        "surname": "Lai",
        "given_name": "Shang-Hong"
      }
    ]
  },
  {
    "title": "Automated detection and classification of fundus diabetic retinopathy images using synergic deep learning model",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.02.026",
    "abstract": "In recent days, the incidence of Diabetic Retinopathy (DR)has become high, affecting the eyes because of drastic increase in the glucose level in blood. Globally, almost half of the people under the age of 70 gets severely affected by diabetes. In the absence of earlier recognition and proper medication, the DR patients tend to lose their vision. When the warning signs are tracked down, the severity level of the disease has to be validated so to take decisions regarding appropriate treatment further. The current research paper focuses on the concept of classification of DR fundus images on the basis of severity level using a deep learning model. This paper proposes a deep learning-based automated detection and classification model for fundus DR images. The proposed method involves various processes namely preprocessing, segmentation and classification. The methods begins with preprocessing stage in which unnecessary noise that exists in the edges is removed. Next, histogram-based segmentation takes place to extract the useful regions from the image. Then, Synergic Deep Learning (SDL) model was applied to classify the DR fundus images to various severity levels. The justification for the presented SDL model was carried out on Messidor DR dataset. The experimentation results indicated that the presented SDL model offers better classification over the existing models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300714",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Diabetes mellitus",
      "Diabetic retinopathy",
      "Endocrinology",
      "Fundus (uterus)",
      "Histogram",
      "Image (mathematics)",
      "Medicine",
      "Ophthalmology",
      "Pattern recognition (psychology)",
      "Preprocessor",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Shankar",
        "given_name": "K."
      },
      {
        "surname": "Sait",
        "given_name": "Abdul Rahaman Wahab"
      },
      {
        "surname": "Gupta",
        "given_name": "Deepak"
      },
      {
        "surname": "Lakshmanaprabu",
        "given_name": "S.K."
      },
      {
        "surname": "Khanna",
        "given_name": "Ashish"
      },
      {
        "surname": "Pandey",
        "given_name": "Hari Mohan"
      }
    ]
  },
  {
    "title": "Generating (co)homological information using boundary scale",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.02.028",
    "abstract": "In this paper we develop a new computational technique called boundary scale-space theory. This technique is based on the topol 1 1 comples -> complex ogical paradigm consisting of representing a geometric subdivided object K using a one-parameter family of geometric objects {Ki } i ≥ 1 all of them having the same number of closed pieces than K. Each piece of Ki (∀i ≥ 1) presents the same interior part than the corresponding one in K, and a different boundary part depending on the scale i. Working with coefficients in a field, a scale is installed for the algebraic boundary of each piece and a new invariant for cell complex isomorphisms is given in terms of the Betti numbers of the generated boundary-scale-space cell complexes. Moreover, the so called homology boundary scale-space model of K (hbss-model for short) is introduced here. This model consists of a hierarchical graph whose nodes are the homology generators of the different boundary scale levels and whose edges are specified by homology generators of consecutive boundary scale indices linked by (hbss-transition maps) preserving homology classes. Various codes for each connected subgraph of an hbss-model are defined, which besides being fast and efficient similarity measures for cellular structures, they are as well relevant interpretive tools for the hbss-model. Finally, experimentations mainly aimed at clarifying and understanding the notion of hbss-model, as well as conjecturing about new graph isomorphism invariants (seeing graphs as a 1-dimensional cell complexes), are performed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300738",
    "keywords": [
      "Betti number",
      "Biochemistry",
      "Boundary (topology)",
      "Chemistry",
      "Crystal structure",
      "Crystallography",
      "Discrete mathematics",
      "Gene",
      "Graph",
      "Graph isomorphism",
      "Homology (biology)",
      "Isomorphism (crystallography)",
      "Line graph",
      "Mathematical analysis",
      "Mathematics"
    ],
    "authors": [
      {
        "surname": "Molina-Abril",
        "given_name": "Helena"
      },
      {
        "surname": "Real",
        "given_name": "Pedro"
      },
      {
        "surname": "Díaz-del-Río",
        "given_name": "Fernando"
      }
    ]
  },
  {
    "title": "Automated detection of abnormal EEG signals using localized wavelet filter banks",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.03.009",
    "abstract": "Epilepsy is a neural disorder that is associated with the central nervous system (CNS) in which the brain activity sometimes becomes abnormal, which may lead to seizures, loss of awareness, unusual sensations, and behavior. Electroencephalograms (EEG) are widely used to detect epilepsy accurately. However, the interpretation of a particular type of abnormality using the EEG signal is a subjective affair and may vary from clinician-to-clinician. Visual inspection of the EEG signal by observing a change in frequency or amplitude in long-duration signals is an arduous task for the clinicians. It may lead to an erroneous classification of EEGs. The proposed methodology focuses on automated detection of epilepsy using a novel stop-band energy (SBE) minimized orthogonal wavelet filter bank. Using the wavelet decomposition, we obtain subbands (SBs) of EEG signals. Subsequently, fuzzy entropy, logarithmic of the squared norm, and fractal dimension are computed for each SB. The different combinations of the extracted features were supplied to various classifiers for the classification of normal and abnormal EEG signals. In the proposed method, we have used a single-channel EEG dataset of Temple University Hospital. The dataset is the most substantial EEG data publicly available, which contains an EEG recording of 2130 distinct subjects. Our proposed system obtained the highest classification accuracy (CACC) of 78.4% and 79.34% during training and evaluation using the SVM classifier. We achieved the highest F1-score of 0.88.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300878",
    "keywords": [
      "Abnormality",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Electroencephalography",
      "Epilepsy",
      "Filter (signal processing)",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Psychology",
      "Social psychology",
      "Speech recognition",
      "Wavelet"
    ],
    "authors": [
      {
        "surname": "Sharma",
        "given_name": "Manish"
      },
      {
        "surname": "Patel",
        "given_name": "Sohamkumar"
      },
      {
        "surname": "Acharya",
        "given_name": "U. Rajendra"
      }
    ]
  },
  {
    "title": "A permutational-based Differential Evolution algorithm for feature subset selection",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.02.021",
    "abstract": "This paper describes a permutational-based Differential Evolution algorithm implemented in a wrapper scheme to find a feature subset to be applied in the construction of a near-optimal classifier. In this approach, the relevance of a feature chosen to build a better classifier is represented through its relative position in an integer-valued vector, and by using a permutational-based mutation operator, it is possible to create new feasible candidate solutions only. Furthermore, to provide a controlled diversity rate in the population, a straightforward repair-based recombination operator is utilized to evolve a population of candidate solutions. Unlike the other approaches in the existing literature using integer-valued vectors and requiring a predefined subset size, in this approach, this size is determined by an additional element included in the encoding scheme, allowing to find an adequate feature subset size to each specific dataset. Experimental results show that this approach is an effective way to create more accurate classifiers as they are compared with those obtained by other similar approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552030060X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Demography",
      "Differential evolution",
      "Feature selection",
      "Feature vector",
      "Integer (computer science)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Population",
      "Programming language",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Rivera-López",
        "given_name": "Rafael"
      },
      {
        "surname": "Mezura-Montes",
        "given_name": "Efrén"
      },
      {
        "surname": "Canul-Reich",
        "given_name": "Juana"
      },
      {
        "surname": "Cruz-Chávez",
        "given_name": "Marco Antonio"
      }
    ]
  },
  {
    "title": "Image-to-video person re-identification with cross-modal embeddings",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.03.003",
    "abstract": "Despite the great progress achieved, image-to-video person re-identification is still challenging in the cross-modal scenario. Currently, state-of-the-art approaches mainly concentrate on the task-specific data, neglecting the extra information from the different but related tasks. In this paper, we propose an end-to-end neural network framework for image-to-video person re-identification with cross-modal embeddings learned from extra information. Concretely speaking, cross-modal embedding layers from image captioning and video captioning models, are incorporated to learn common latent embeddings for multiple modalities. The learned multimodal embeddings are expected to focus on person’s prominent distinctions, due to textual descriptive information generally paying close attention to person’s explicit characteristics. Apart from that, our proposed framework resorts to CNNs and LSTMs for extracting visual and spatiotemporal features, and combines the strengths of identification and verification model to improve the discriminative ability of the learned features. The experimental results demonstrate the effectiveness of our framework on narrowing down the gap between heterogeneous data and obtaining observable improvement in the image-to-video person re-identification task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518307165",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Chemistry",
      "Closed captioning",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Economics",
      "Embedding",
      "Focus (optics)",
      "Identification (biology)",
      "Image (mathematics)",
      "Machine learning",
      "Management",
      "Modal",
      "Modalities",
      "Natural language processing",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Polymer chemistry",
      "Social science",
      "Sociology",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Xie",
        "given_name": "Zhongwei"
      },
      {
        "surname": "Li",
        "given_name": "Lin"
      },
      {
        "surname": "Zhong",
        "given_name": "Xian"
      },
      {
        "surname": "Zhong",
        "given_name": "Luo"
      },
      {
        "surname": "Xiang",
        "given_name": "Jianwen"
      }
    ]
  },
  {
    "title": "Error-tolerant approximate graph matching utilizing node centrality information",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.03.019",
    "abstract": "Graph matching is the task of finding the similarity between the two graphs. Error-tolerant graph matching is the process of computing the similarity between the two graphs, where some flexibility to noise or error is allowed to approximate the value of graph matching. In this paper, we present a framework for graph matching by utilizing the centrality measures to ignore the least central nodes of the graphs. Experimental evaluation shows that this approach can be useful to reduce the overall matching time and it can provide time versus accuracy trade-off.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300970",
    "keywords": [
      "Algorithm",
      "Centrality",
      "Combinatorics",
      "Computer science",
      "Graph",
      "Matching (statistics)",
      "Mathematics",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Dwivedi",
        "given_name": "Shri Prakash"
      },
      {
        "surname": "Singh",
        "given_name": "Ravi Shankar"
      }
    ]
  },
  {
    "title": "Classification of origin with feature selection and network construction for folk tunes",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.03.023",
    "abstract": "We address the question to what extent origin of folk songs can be predicted, and construct a song similarity network. For this we use feature selection and train a random forest classifier on extracted melody n-grams and rhythm grams of songs from 7 different groups of origin, 80 from each. We use its importances to reduce the feature space dimension for the construction of an informative network, which we visualized in a interactive web application. These tools have vast application in large-scale exploration in digitized music databases and for specific questions in musicology.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552030101X",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Construct (python library)",
      "Data mining",
      "Feature (linguistics)",
      "Feature selection",
      "Feature vector",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Random forest",
      "Selection (genetic algorithm)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Metzig",
        "given_name": "Cornelia"
      },
      {
        "surname": "Gould",
        "given_name": "Matthew"
      },
      {
        "surname": "Noronha",
        "given_name": "Roshan"
      },
      {
        "surname": "Abbey",
        "given_name": "Roshani"
      },
      {
        "surname": "Sandler",
        "given_name": "Mark"
      },
      {
        "surname": "Colijn",
        "given_name": "Caroline"
      }
    ]
  },
  {
    "title": "HscoreNet: A Deep network for estrogen and progesterone scoring using breast IHC images",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107200",
    "abstract": "Estrogen and progesterone receptors serve as an important predictive and prognostic biomarkers for breast cancer immunohistological analysis. For breast cancer prognosis, pathologists manually compute the score based on the visual expression and the number of immunopositive and immunonegative nuclei. This manual scoring technique is time-consuming, cumbersome, expensive, error-prone, and susceptible to intra- and interobserver ambiguities. To solve these issues, we proposed a deep neural network (i.e., HscoreNet), which consists of three parts, i.e., encoder, decoder, and scoring layer. A total of 600 (300 ER and 300 PR) regions of interest at 40 × magnification from 100 histologically confirmed slides were used in this study. The size of each region of interest was 2048 × 1536 pixels (width × height). The encoder layer has been used to transform input pixels into a lower-dimensional representation, whereas the decoder reconstructs the output of the encoder through minimization of a cost function. The decoder generates an image that only contains immunopositive and immunonegative nuclei. The output of the decoder is fed to the input of the scoring layer. This layer computes the Histo-score or H-score based on the staining intensity, the color expression, and the number of immunopositive and immunonegative nuclei. Pathologists compute this score to subcategorize the cancer grades and to decide proper treatment procedures. Our proposed approach is affordable, accurate, and fast. We achieved excellent performance, with 95.87% precision and 94.53% classification accuracy. Our proposed approach streamlines the human error-prone and time-consuming process. This methodology can also be used for other types of histology and immunohistology image segmentation and scoring.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300078",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Breast cancer",
      "Cancer",
      "Computer science",
      "Encoder",
      "Estrogen receptor",
      "Internal medicine",
      "Magnification",
      "Medicine",
      "Operating system",
      "Pattern recognition (psychology)",
      "Pixel",
      "Progesterone receptor"
    ],
    "authors": [
      {
        "surname": "Saha",
        "given_name": "Monjoy"
      },
      {
        "surname": "Arun",
        "given_name": "Indu"
      },
      {
        "surname": "Ahmed",
        "given_name": "Rosina"
      },
      {
        "surname": "Chatterjee",
        "given_name": "Sanjoy"
      },
      {
        "surname": "Chakraborty",
        "given_name": "Chandan"
      }
    ]
  },
  {
    "title": "Scene recognition: A comprehensive survey",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107205",
    "abstract": "With the success of deep learning in the field of computer vision, object recognition has made important breakthroughs, and its recognition accuracy has been drastically improved. However, the performance of scene recognition is still not sufficient to some extent because of complex configurations. Over the past several years, scene recognition algorithms have undergone important evolution as a result of the development of machine learning and Deep Convolutional Neural Networks (DCNN). This paper reviews many of the most popular and effective approaches to scene recognition, which is expected to create benefits for future research and practical applications. We seek to establish relationships among different algorithms and determine the critical components that lead to remarkable performance. Through the analysis of some representative schemes, motivation and insights are identified, which will help to facilitate the design of better recognition architectures. In addition, current available scene datasets and benchmarks are presented for evaluation and comparison. Finally, potential problems and promising directions are highlighted.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030011X",
    "keywords": [
      "Artificial intelligence",
      "Cognitive neuroscience of visual object recognition",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Field (mathematics)",
      "Gesture",
      "Gesture recognition",
      "Machine learning",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Sketch recognition"
    ],
    "authors": [
      {
        "surname": "Xie",
        "given_name": "Lin"
      },
      {
        "surname": "Lee",
        "given_name": "Feifei"
      },
      {
        "surname": "Liu",
        "given_name": "Li"
      },
      {
        "surname": "Kotani",
        "given_name": "Koji"
      },
      {
        "surname": "Chen",
        "given_name": "Qiu"
      }
    ]
  },
  {
    "title": "DeepCT: A novel deep complex-valued network with learnable transform for video saliency prediction",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107234",
    "abstract": "The past decade has witnessed the success of transformed domain methods for image saliency prediction. However, it is intractable to develop a transformed domain method for video saliency prediction, due to the limited choices on spatio-temporal transforms. In this paper, we propose learning the transform from training data, rather than the predefined transform in the existing methods. Specifically, we develop a novel deep Complex-valued network with learnable Transform (DeepCT) for video saliency prediction. The architecture of DeepCT includes the Complex-valued Transform Module (CTM), inverse CTM (iCTM) and Complex-valued Stacked Convolutional Long Short-Term Memory network (CS-ConvLSTM). In the CTM and iCTM, multi-scale pyramid structures are introduced, as we find that transforms at multiple receptive scales can improve the accuracy of saliency prediction. To make the CTM and iCTM “invertible”, we further propose the cycle consistency loss in training DeepCT, which is composed of frame reconstruction loss and complex feature reconstruction loss. Additionally, the CS-ConvLSTM is developed to learn the temporal saliency transition across video frames. Finally, the experimental results show that our DeepCT method outperforms other 13 state-of-the-art methods for video saliency prediction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300406",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Deep learning",
      "Domain (mathematical analysis)",
      "Feature (linguistics)",
      "Geometry",
      "Invertible matrix",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pure mathematics",
      "Pyramid (geometry)"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Lai"
      },
      {
        "surname": "Xu",
        "given_name": "Mai"
      },
      {
        "surname": "Zhang",
        "given_name": "Shanyi"
      },
      {
        "surname": "Sigal",
        "given_name": "Leonid"
      }
    ]
  },
  {
    "title": "Error-tolerant graph matching in linear computational cost using an initial small partial matching",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.04.003",
    "abstract": "Error-tolerant graph matching has been demonstrated to be an NP-problem, therefore, its exact computation has an exponential computational cost and several sub-optimal algorithms have been presented with the aim of making the runtime acceptable in some applications. Some well-known sub-optimal algorithms have sixth, cubic or quadratic computational costs with respect to the order of the graphs. Although these computational costs could be considered very low, when applications deal with large graphs (for instance in social networks), the quadratic cost continues to be unacceptable. For this reason, we present an error-tolerant graph-matching algorithm that has a O(d 3.5 · ) computational cost, d being the number of output edges per node and n the order of the graphs. Note that, usually, in social networks, it holds that d ≪ n and for this reason we consider the cost to be linear, in other words O(k · ), k being a low constant. Our method needs an initial seed, which is composed of one or several node-to-node mappings. The algorithm has been applied to analyse the evolution of social networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518301235",
    "keywords": [
      "Algorithm",
      "Computation",
      "Computational complexity theory",
      "Computer science",
      "Constant (computer programming)",
      "Engineering",
      "Exponential function",
      "Geometry",
      "Graph",
      "Matching (statistics)",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Node (physics)",
      "Programming language",
      "Quadratic equation",
      "Statistics",
      "Structural engineering",
      "Theoretical computer science",
      "Time complexity"
    ],
    "authors": [
      {
        "surname": "Santacruz",
        "given_name": "Pep"
      },
      {
        "surname": "Serratosa",
        "given_name": "Francesc"
      }
    ]
  },
  {
    "title": "An accelerated correlation filter tracker",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107172",
    "abstract": "Recent visual object tracking methods have witnessed a continuous improvement in the state-of-the-art with the development of efficient discriminative correlation filters (DCF) and robust deep neural network features. Despite the outstanding performance achieved by the above combination, existing advanced trackers suffer from the burden of high computational complexity of the deep feature extraction and online model learning. We propose an accelerated ADMM optimisation method obtained by adding a momentum to the optimisation sequence iterates, and by relaxing the impact of the error between DCF parameters and their norm. The proposed optimisation method is applied to an innovative formulation of the DCF design, which seeks the most discriminative spatially regularised feature channels. A further speed up is achieved by an adaptive initialisation of the filter optimisation process. The significantly increased convergence of the DCF filter is demonstrated by establishing the optimisation process equivalence with a continuous dynamical system for which the convergence properties can readily be derived. The experimental results obtained on several well-known benchmarking datasets demonstrate the efficiency and robustness of the proposed ACFT method, with a tracking accuracy comparable to the start-of-the-art trackers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304728",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmarking",
      "Biochemistry",
      "BitTorrent tracker",
      "Business",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Eye tracking",
      "Filter (signal processing)",
      "Gene",
      "Iterated function",
      "Machine learning",
      "Marketing",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Tianyang"
      },
      {
        "surname": "Feng",
        "given_name": "Zhen-Hua"
      },
      {
        "surname": "Wu",
        "given_name": "Xiao-Jun"
      },
      {
        "surname": "Kittler",
        "given_name": "Josef"
      }
    ]
  },
  {
    "title": "Guided CNN for generalized zero-shot and open-set recognition using visual and semantic prototypes",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107263",
    "abstract": "In the process of exploring the world, the curiosity constantly drives humans to cognize new things. Supposing you are a zoologist, for a presented animal image, you can recognize it immediately if you know its class. Otherwise, you would more likely attempt to cognize it by exploiting the side-information (e.g., semantic information, etc.) you have accumulated. Inspired by this, this paper decomposes the generalized zero-shot learning (G-ZSL) task into an open set recognition (OSR) task and a zero-shot learning (ZSL) task, where OSR recognizes seen classes (if we have seen (or known) them) and rejects unseen classes (if we have never seen (or known) them before), while ZSL identifies the unseen classes rejected by the former. Simultaneously, without violating OSR’s assumptions (only known class knowledge is available in training), we also first attempt to explore a new generalized open set recognition (G-OSR) by introducing the accumulated side-information from known classes to OSR. For G-ZSL, such a decomposition effectively solves the class overfitting problem with easily misclassifying unseen classes as seen classes. The problem is ubiquitous in most existing G-ZSL methods. On the other hand, for G-OSR, introducing such semantic information of known classes not only improves the recognition performance but also endows OSR with the cognitive ability of unknown classes. Specifically, a visual and semantic prototypes-jointly guided convolutional neural network (VSG-CNN) is proposed to fulfill these two tasks (G-ZSL and G-OSR) in a unified end-to-end learning framework. Extensive experiments on benchmark datasets demonstrate the advantages of our learning framework.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300686",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Linguistics",
      "Mathematics",
      "Natural language processing",
      "Open set",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Set (abstract data type)",
      "Shot (pellet)",
      "Speech recognition",
      "Zero (linguistics)"
    ],
    "authors": [
      {
        "surname": "Geng",
        "given_name": "Chuanxing"
      },
      {
        "surname": "Tao",
        "given_name": "Lue"
      },
      {
        "surname": "Chen",
        "given_name": "Songcan"
      }
    ]
  },
  {
    "title": "Protein function prediction as a graph-transduction game",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.04.002",
    "abstract": "Motivated by the observation that network-based methods for the automatic prediction of protein functions can greatly benefit from exploiting both the similarity between proteins and the similarity between functional classes (as encoded, e.g., in the Gene Ontology), in this paper we propose a novel approach to the problem, based on the notion of a “graph transduction game.” We envisage a (non-cooperative) game, played over a graph, where the players (graph vertices) represent proteins, the functional classes correspond to the (pure) strategies, and protein- and function-level similarities are combined into a suitable payoff function. Within this formulation, Nash equilibria turn out to provide consistent functional labelings of proteins, and we use classical replicator dynamics from evolutionary game theory to find them. To test the effectiveness of our approach we conducted experiments on five different organisms and three ontologies, and the results obtained show that our method compares favorably with state-of-the-art algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518301223",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Computer science",
      "Demography",
      "Function (biology)",
      "Gene",
      "Gene expression",
      "Gene ontology",
      "Genetics",
      "Graph",
      "Image (mathematics)",
      "Mathematical economics",
      "Mathematics",
      "Population",
      "Protein function",
      "Protein function prediction",
      "Replicator equation",
      "Similarity (geometry)",
      "Sociology",
      "Stochastic game",
      "Theoretical computer science",
      "Transduction (biophysics)"
    ],
    "authors": [
      {
        "surname": "Vascon",
        "given_name": "Sebastiano"
      },
      {
        "surname": "Frasca",
        "given_name": "Marco"
      },
      {
        "surname": "Tripodi",
        "given_name": "Rocco"
      },
      {
        "surname": "Valentini",
        "given_name": "Giorgio"
      },
      {
        "surname": "Pelillo",
        "given_name": "Marcello"
      }
    ]
  },
  {
    "title": "Efficient k-nearest neighbors search in graph space",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.05.001",
    "abstract": "The k-nearest neighbors classifier has been widely used to classify graphs in pattern recognition. An unknown graph is classified by comparing it to all the graphs in the training set and then assigning it the class to which the majority of the nearest neighbors belong. When the size of the database is large, the search of k-nearest neighbors can be very time consuming. On this basis, researchers proposed optimization techniques to speed up the search for the nearest neighbors. However, to the best of our knowledge, all the existing works compared the unknown graph to each train graph separately and thus none of them considered finding the k nearest graphs from a query as a single problem. In this paper, we define a new problem called multi graph edit distance to which k-nearest neighbor belongs. As a first algorithm to solve this problem, we take advantage of a recent exact branch-and-bound graph edit distance approach in order to speed up the classification stage. We extend this algorithm by considering all the search spaces needed for the dissimilarity computation between the unknown and the training graphs as a single search space. Results showed that this approach drastically outperformed the original approach under limited time constraints. Moreover, the proposed approach outperformed fast graph edit distance algorithms in terms of average execution time especially when the number of graphs is tremendous.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518301673",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Graph",
      "Nearest neighbor graph",
      "Nearest neighbor search",
      "Pattern recognition (psychology)",
      "Theoretical computer science",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Abu-Aisheh",
        "given_name": "Zeina"
      },
      {
        "surname": "Raveaux",
        "given_name": "Romain"
      },
      {
        "surname": "Ramel",
        "given_name": "Jean-Yves"
      }
    ]
  },
  {
    "title": "Fast linear sum assignment with error-correction and no cost constraints",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.03.032",
    "abstract": "We propose an algorithm that efficiently solves the linear sum assignment problem with error-correction and no cost constraints. This problem is encountered for instance in the approximation of the graph edit distance. The fastest currently available solvers for the linear sum assignment problem require the pairwise costs to respect the triangle inequality. Our algorithm is as fast as these algorithms, but manages to drop the cost constraint. The main technical ingredient of our algorithm is a cost-dependent factorization of the node substitutions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518301120",
    "keywords": [
      "Algorithm",
      "Approximation algorithm",
      "Artificial intelligence",
      "Assignment problem",
      "Computer science",
      "Constraint (computer-aided design)",
      "Discrete mathematics",
      "Geometry",
      "Graph",
      "Mathematical optimization",
      "Mathematics",
      "Pairwise comparison",
      "Theoretical computer science",
      "Triangle inequality"
    ],
    "authors": [
      {
        "surname": "Bougleux",
        "given_name": "Sébastien"
      },
      {
        "surname": "Gaüzère",
        "given_name": "Benoit"
      },
      {
        "surname": "Blumenthal",
        "given_name": "David B."
      },
      {
        "surname": "Brun",
        "given_name": "Luc"
      }
    ]
  },
  {
    "title": "Local-global nested graph kernels using nested complexity traces",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.06.016",
    "abstract": "In this paper, we propose two novel local-global nested graph kernels, namely the nested aligned kernel and the nested reproducing kernel, drawing on depth-based complexity traces. Both of the nested kernels gauge the nested depth complexity trace through a family of K-layer expansion subgraphs rooted at the centroid vertex, i.e., the vertex with minimum shortest path length variance to the remaining vertices. Specifically, for a pair of graphs, we commence by computing the centroid depth-based complexity traces rooted at the centroid vertices. The first nested kernel is defined by measuring the global alignment kernel, which is based on the dynamic time warping framework, between the complexity traces. Since the required global alignment kernel incorporates the whole spectrum of alignment costs between the complexity traces, this nested kernel can provide rich statistic measures. The second nested kernel, on the other hand, is defined by measuring the basic reproducing kernel between the complexity traces. Since the associated reproducing kernel only requires time complexity O(1), this nested kernel has very low computational complexity. We theoretically show that both of the proposed nested kernels can simultaneously reflect the local and global graph characteristics in terms of the nested complexity traces. Experiments on standard graph datasets abstracted from bioinformatics and computer vision databases demonstrate the effectiveness and efficiency of the proposed graph kernels.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518302617",
    "keywords": [
      "Algorithm",
      "Combinatorics",
      "Computer science",
      "Kernel (algebra)",
      "Mathematics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Bai",
        "given_name": "Lu"
      },
      {
        "surname": "Cui",
        "given_name": "Lixin"
      },
      {
        "surname": "Rossi",
        "given_name": "Luca"
      },
      {
        "surname": "Xu",
        "given_name": "Lixiang"
      },
      {
        "surname": "Bai",
        "given_name": "Xiao"
      },
      {
        "surname": "Hancock",
        "given_name": "Edwin"
      }
    ]
  },
  {
    "title": "A novel density-based clustering algorithm using nearest neighbor graph",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107206",
    "abstract": "Density-based clustering has several desirable properties, such as the abilities to handle and identify noise samples, discover clusters of arbitrary shapes, and automatically discover of the number of clusters. Identifying the core samples within the dense regions of a dataset is a significant step of the density-based clustering algorithm. Unlike many other algorithms that estimate the density of each samples using different kinds of density estimators and then choose core samples based on a threshold, in this paper, we present a novel approach for identifying local high-density samples utilizing the inherent properties of the nearest neighbor graph (NNG). After using the density estimator to filter noise samples, the proposed algorithm ADBSCAN in which “A” stands for “Adaptive” performs a DBSCAN-like clustering process. The experimental results on artificial and real-world datasets have demonstrated the significant performance improvement over existing density-based clustering algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300121",
    "keywords": [
      "1-planar graph",
      "Algorithm",
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Core (optical fiber)",
      "Correlation clustering",
      "DBSCAN",
      "Data mining",
      "Dense graph",
      "Determining the number of clusters in a data set",
      "Estimator",
      "Filter (signal processing)",
      "Graph",
      "Image (mathematics)",
      "Line graph",
      "Mathematics",
      "Nearest-neighbor chain algorithm",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Statistics",
      "Telecommunications",
      "Theoretical computer science",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Hao"
      },
      {
        "surname": "Liu",
        "given_name": "Xiaojie"
      },
      {
        "surname": "Li",
        "given_name": "Tao"
      },
      {
        "surname": "Gan",
        "given_name": "Rundong"
      }
    ]
  },
  {
    "title": "PoseConvGRU: A Monocular Approach for Visual Ego-motion Estimation by Learning",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107187",
    "abstract": "Visual ego-motion estimation is one of the longstanding problems which estimates the movement of cameras from images. Learning based ego-motion estimation methods have seen an increasing attention since its desirable properties of robustness to image noise and camera calibration independence. In this work, we propose a data-driven approach of learning based visual ego-motion estimation for a monocular camera. We use an end-to-end learning approach in allowing the model to learn a map from input image pairs to the corresponding ego-motion, which is parameterized as 6-DoF transformation matrix. We introduce a two-module Long-term Recurrent Convolutional Neural Networks called PoseConvGRU. The feature-encoding module encodes the short-term motion feature in an image pair, while the memory-propagating module captures the long-term motion feature in the consecutive image pairs. The visual memory is implemented with convolutional gated recurrent units, which allows propagating information over time. At each time step, two consecutive RGB images are stacked together to form a 6-channel tensor for feature-encoding module to learn how to extract motion information and estimate poses. The sequence of output maps is then passed through the memory-propagating module to generate the relative transformation pose of each image pair. In addition, we have designed a series of data augmentation methods to avoid the overfitting problem and improve the performance of the model when facing challengeable scenarios such as high-speed or reverse driving. We evaluate the performance of our proposed approach on the KITTI Visual Odometry benchmark and Malaga 2013 Dataset. The experiments show a competitive performance of the proposed method to the state-of-the-art monocular geometric and learning methods and encourage further exploration of learning-based methods for the purpose of estimating camera ego-motion even though geometrical methods demonstrate promising results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132031930487X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Gene",
      "Linguistics",
      "Monocular",
      "Motion estimation",
      "Overfitting",
      "Philosophy",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Zhai",
        "given_name": "Guangyao"
      },
      {
        "surname": "Liu",
        "given_name": "Liang"
      },
      {
        "surname": "Zhang",
        "given_name": "Linjian"
      },
      {
        "surname": "Liu",
        "given_name": "Yong"
      },
      {
        "surname": "Jiang",
        "given_name": "Yunliang"
      }
    ]
  },
  {
    "title": "Online signature verification using single-template matching with time-series averaging and gradient boosting",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107227",
    "abstract": "In keeping with recent developments in artificial intelligence in the era of big data, there is a demand for online signature verification systems that operate at high speeds, provide a high level of security, and allow high tolerances while achieving sufficient performance. In response to these needs, the present study proposes a novel, single-template strategy using a mean template set and weighted multiple dynamic time warping (DTW) distances for a function-based approach to online signature verification. Specifically, to obtain an effective mean template for each feature while reflecting intra-user variability between all the reference samples, we adopt a novel time-series averaging method based on Euclidean barycenter-based DTW barycenter averaging. Then, by using the mean template set, we calculate multiple DTW distances from multivariate time series based on dependent and independent warping. Finally, to boost the discriminative power, we apply a weighting scheme using a gradient boosting model to efficiently combine the multiple DTW distances. Experimental results using the common SVC2004 Task1/Task2 and MCYT-100 signature datasets confirm that the proposed method is effective for online signature verification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300339",
    "keywords": [
      "Artificial intelligence",
      "Boosting (machine learning)",
      "Computer science",
      "Data mining",
      "Discriminative model",
      "Dynamic time warping",
      "Euclidean distance",
      "Geometry",
      "Gradient boosting",
      "Mathematics",
      "Medicine",
      "Pattern recognition (psychology)",
      "Radiology",
      "Random forest",
      "Signature (topology)",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Okawa",
        "given_name": "Manabu"
      }
    ]
  },
  {
    "title": "Comparing performance of graph matching algorithms on huge graphs",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.06.025",
    "abstract": "Graph matching algorithms are gaining more and more interest in the last years from different scientific communities; indeed, they allow comparing any kind of objects represented using their intrinsic structure, represented in terms of attributed relational graphs. The challenge is to make these algorithms able to provide solutions over huge graphs, with many thousands of nodes, and in a time that is adequate for practical applications; in this paper, we propose a comparison among the best performing algorithms available in the literature on a variety of very large graph databases used for performance assessment. The chosen datasets vary in terms of graph structure, size, density, presence of symmetric or repetitive substructures; this variability makes such datasets very challenging. The aim of this paper is to characterize the performance of the compared algorithms with respect to the typology, the size and other structural properties of the graphs; in this way, the user may consciously select the best suited algorithm for a given purpose. The results of an impressive experimentation that required 556 days of machine time are here presented and extensively discussed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518302733",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Data structure",
      "Graph",
      "Matching (statistics)",
      "Mathematics",
      "Programming language",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Carletti",
        "given_name": "Vincenzo"
      },
      {
        "surname": "Foggia",
        "given_name": "Pasquale"
      },
      {
        "surname": "Greco",
        "given_name": "Antonio"
      },
      {
        "surname": "Saggese",
        "given_name": "Alessia"
      },
      {
        "surname": "Vento",
        "given_name": "Mario"
      }
    ]
  },
  {
    "title": "Identity-aware CycleGAN for face photo-sketch synthesis and recognition",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107249",
    "abstract": "Face photo-sketch synthesis and recognition has many applications in digital entertainment and law enforcement. Recently, generative adversarial networks (GANs) based methods have significantly improved the quality of image synthesis, but they have not explicitly considered the purpose of recognition. In this paper, we first propose an Identity-Aware CycleGAN (IACycleGAN) model that applies a new perceptual loss to supervise the image generation network. It improves CycleGAN on photo-sketch synthesis by paying more attention to the synthesis of key facial regions, such as eyes and nose, which are important for identity recognition. Furthermore, we develop a mutual optimization procedure between the synthesis model and the recognition model, which iteratively synthesizes better images by IACycleGAN and enhances the recognition model by the triplet loss of the generated and real samples. Extensive experiments are performed on both photo-to-sketch and sketch-to-photo tasks using the widely used CUFS and CUFSF databases. The results show that the proposed method performs better than several state-of-the-art methods in terms of both synthetic image quality and photo-sketch recognition accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300558",
    "keywords": [
      "Acoustics",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Face (sociological concept)",
      "Facial recognition system",
      "Generative grammar",
      "Gesture",
      "Gesture recognition",
      "Identity (music)",
      "Key (lock)",
      "Pattern recognition (psychology)",
      "Physics",
      "Rendering (computer graphics)",
      "Sketch",
      "Sketch recognition",
      "Social science",
      "Sociology",
      "View synthesis"
    ],
    "authors": [
      {
        "surname": "Fang",
        "given_name": "Yuke"
      },
      {
        "surname": "Deng",
        "given_name": "Weihong"
      },
      {
        "surname": "Du",
        "given_name": "Junping"
      },
      {
        "surname": "Hu",
        "given_name": "Jiani"
      }
    ]
  },
  {
    "title": "Uncertain motion tracking based on convolutional net with semantics estimation and region proposals",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107232",
    "abstract": "In real world, most of the tracking methods suffer from uncertain motion, which may make the tracker failure because of a local search window with the motion smooth assumption. To address this problem, a novel tracking framework based on convolutional net with semantics estimation and region proposals is proposed. Firstly, we present a semantics object proposals generation strategy, including category-level semantics proposals, one-object-level semantics estimation and semantics-contextual proposals generation, to obtain a few of high-quality object-oriented proposals covering uncertain motion. Secondly, combining the globally sparse semantics region proposals prediction and correlation filter prediction, a hybrid semantics tracking algorithm is proposed, which obtains a coarse object location by the decision of multiple response maps. Finally, we learn and train independent correlation filter to estimate the scale of target for a higher tracking accuracy. Extensive experiments on two visual tracking benchmarks and results demonstrate our method achieves state-of-the-art performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300388",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Eye tracking",
      "Motion (physics)",
      "Motion estimation",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Programming language",
      "Psychology",
      "Semantics (computer science)",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Huanlong"
      },
      {
        "surname": "Chen",
        "given_name": "Jian"
      },
      {
        "surname": "Nie",
        "given_name": "Guohao"
      },
      {
        "surname": "Hu",
        "given_name": "Shiqiang"
      }
    ]
  },
  {
    "title": "Hierarchical graphs for coarse-to-fine error tolerant matching",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.02.001",
    "abstract": "During the last years, graph-based representations are experiencing a growing usage in visual recognition and retrieval due to their ability to capture both structural and appearance-based information. Thus, they provide a greater representational power than classical statistical frameworks. However, graph-based representations leads to high computational complexities usually dealt by graph embeddings or approximated matching techniques. Despite their representational power, they are very sensitive to noise and small variations of the input image. With the aim to cope with the time complexity and the variability present in the generated graphs, in this paper we propose to construct a novel hierarchical graph representation. Graph clustering techniques adapted from social media analysis have been used in order to contract a graph at different abstraction levels while keeping information about the topology. Abstract nodes attributes summarise information about the contracted graph partition. For the proposed representations, a coarse-to-fine matching technique is defined. Hence, small graphs are used as a filtering before more accurate matching methods are applied. This approach has been validated in real scenarios such as classification of colour images or retrieval of handwritten words (i.e. word spotting).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519300261",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Clustering coefficient",
      "Computer science",
      "Data mining",
      "Graph",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Riba",
        "given_name": "Pau"
      },
      {
        "surname": "Lladós",
        "given_name": "Josep"
      },
      {
        "surname": "Fornés",
        "given_name": "Alicia"
      }
    ]
  },
  {
    "title": "Synthesizing Talking Faces from Text and Audio: An Autoencoder and Sequence-to-Sequence Convolutional Neural Network",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107231",
    "abstract": "Synthesizing talking face from text and audio is increasingly becoming a direction in human-machine and face-to-face interactions. Although progress has been made, several existing methods either have unsatisfactory co-articulation modeling effects or ignore relations between adjacent inputs. Moreover, some of these methods often train models on shaky head videos or utilize linear-based face parameterization strategies, which further decrease synthesized quality. To address the above issues, this study proposes a sequence-to-sequence convolutional neural network to automatically synthesize talking face video with accurate lip sync. First, an advanced landmark location pipeline is used to accurately locate the facial landmarks, which can effectively reduce landmark shake. Then, a part-based autoencoder is presented to encode face images into a low-dimensional space and obtain compact representations. A sequence-to-sequence network is also presented to encode the relation of neighboring frames with multiple loss functions, and talking faces are synthesized through a reconstruction strategy with a decoder. Experiments on two public audio-visual datasets and a new dataset called CCTV news demonstrate the effectiveness of the proposed method against other state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300376",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "ENCODE",
      "Encoder",
      "Face (sociological concept)",
      "Frame (networking)",
      "Gene",
      "Genetics",
      "Landmark",
      "Law",
      "Operating system",
      "Pattern recognition (psychology)",
      "Pipeline (software)",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Sequence (biology)",
      "Social science",
      "Sociology",
      "Speech recognition",
      "Telecommunications",
      "sync"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Na"
      },
      {
        "surname": "Zhou",
        "given_name": "Tao"
      },
      {
        "surname": "Ji",
        "given_name": "Yunfeng"
      },
      {
        "surname": "Zhao",
        "given_name": "Ziyi"
      },
      {
        "surname": "Wan",
        "given_name": "Lihong"
      }
    ]
  },
  {
    "title": "Special issue on “Applications of graph-based techniques to pattern recognition”",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.04.008",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519301199",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Economics",
      "Graph",
      "Keyword spotting",
      "Machine learning",
      "Operations management",
      "Pattern recognition (psychology)",
      "Performance improvement",
      "Programming language",
      "Set (abstract data type)",
      "Spotting",
      "Theoretical computer science",
      "Training set"
    ],
    "authors": [
      {
        "surname": "Foggia",
        "given_name": "Pasquale"
      },
      {
        "surname": "Vento",
        "given_name": "Mario"
      },
      {
        "surname": "Liu",
        "given_name": "Cheng-Lin"
      }
    ]
  },
  {
    "title": "A quadrilateral scene text detector with two-stage network architecture",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107230",
    "abstract": "Many of the state-of-the-art methods can only localize scene texts with rotated rectangle boundaries, which may result in incorrect rectification of the detected scene texts and erroneous elimination of proposals or detections during non-maximum suppression (NMS). A few existing methods that can detect scene texts with quadrilateral boundaries, are just based on one-stage architectures or sliding windows scanning and thus have sub-optimal performance. To address these problems, we propose an end-to-end two-stage network architecture for scene text detection, which can accurately localize scene texts with quadrilateral boundaries. At the first stage, we propose a quadrilateral region proposal network (QRPN) for generating quadrilateral proposals, based on a newly proposed quadrilateral regression algorithm. At the second stage, we introduce a novel weighted RoI pooling module with learned weight masks to pool the features, and then classify the proposals and refine their shapes with the proposed quadrilateral regression algorithm again. Specially, during training, we adopt a dual-branch structure of detection heads, that is, jointly train the quadrilateral detection head and an additional rotated rectangle detection head. Furthermore, we develop an accelerated NMS algorithm with O(nlogn) complexity, for redundant quadrilateral text proposals and detections eliminating during the first and the second stage, respectively. Experiments on several challenging benchmarks demonstrate the superior performance of the proposed method, which achieves state-of-the-art results on widely used benchmarks ICDAR 2017 MLT, RCTW, and ICDAR 2015 Incidental Scene Text benchmark.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300364",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Finite element method",
      "Geodesy",
      "Geography",
      "Geometry",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Pooling",
      "Power (physics)",
      "Quadrilateral",
      "Quantum mechanics",
      "Rectangle",
      "Rectification",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Siwei"
      },
      {
        "surname": "Liu",
        "given_name": "Yudong"
      },
      {
        "surname": "He",
        "given_name": "Zheqi"
      },
      {
        "surname": "Wang",
        "given_name": "Yongtao"
      },
      {
        "surname": "Tang",
        "given_name": "Zhi"
      }
    ]
  },
  {
    "title": "Smooth robust tensor principal component analysis for compressed sensing of dynamic MRI",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107252",
    "abstract": "Dynamic magnetic resonance imaging (DMRI) often requires a long time for measurement acquisition, and it is a crucial problem about the enhancement of reconstruction quality from a limited set of under-samples. The low-rank plus sparse decomposition model, which is also called robust principal component analysis (RPCA), is widely used for reconstruction of DMRI data in the model-based way. In this paper, considering that DMRI data are naturally in tensor form with block-wise smoothness, we propose a smooth robust tensor principal component analysis (SRTPCA) method for DMRI reconstruction. Compared with classical RPCA approaches, the low rank and sparsity terms are extended to tensor versions to fully exploit the spatial and temporal data structures. Moreover, a tensor total variation regularization term is used to encourage the multi-dimensional block-wise smoothness for the reconstructed DMRI data. The relaxed convex optimization model can be divided into several sub-problems by the alternating direction method of multipliers. Numerical experiments on cardiac perfusion and cine datasets demonstrate that the proposed SRTPCA method outperforms the state-of-the-art ones in terms of recovery accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300571",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Gene",
      "Iterative reconstruction",
      "Mathematical analysis",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Principal component analysis",
      "Pure mathematics",
      "Robust principal component analysis",
      "Robustness (evolution)",
      "Singular value decomposition",
      "Smoothness",
      "Tensor (intrinsic definition)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yipeng"
      },
      {
        "surname": "Liu",
        "given_name": "Tengteng"
      },
      {
        "surname": "Liu",
        "given_name": "Jiani"
      },
      {
        "surname": "Zhu",
        "given_name": "Ce"
      }
    ]
  },
  {
    "title": "Threshold optimization for F measure of macro-averaged precision and recall",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107250",
    "abstract": "There are two different approaches to macro-averaging F measure for multi-label classification. The first encloses averaging F measure over all classes, which makes it easy to optimize. The second, extensively investigated in this paper, comprises the F measure of macro precision and recall calculation. We examine and compare these two measures when applied to different multi-label datasets. To optimize the performance measure, we adopt a widely known and proven modular approach. Classifiers sort the instances in descending order, according to a real-valued score of belonging to a corresponding class. After that, thresholds are selected so as to optimize the performance measure. If the number of classes is sufficiently large and the second alternative of macro-averaging F measure is employed, then it becomes non-trivial to define the optimal number of instances to assign to each class. Cyclic optimization procedure is widely used for threshold optimization although it results in a maximum in a special coordinate-wise sense. For a micro averaged F measure, such a coordinate-wise optimum is a maximum in the conventional sense of this term but it is not true for the F measure of macro precision and recall, which is shown by a counterexample. We reduce the problem of selecting the optimal threshold for each class to the problem of obtaining a fixed point of a specifically introduced transformation of a unit square. The suggested algorithm lets us localize all possible coordinate-wise maximums and detect the optimal among them. The approach is applied to datasets from diverse application domains.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030056X",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Class (philosophy)",
      "Computer science",
      "Data mining",
      "Gene",
      "Geometry",
      "Macro",
      "Mathematics",
      "Measure (data warehouse)",
      "Point (geometry)",
      "Precision and recall",
      "Programming language",
      "Transformation (genetics)",
      "sort"
    ],
    "authors": [
      {
        "surname": "Berger",
        "given_name": "Anna"
      },
      {
        "surname": "Guda",
        "given_name": "Sergey"
      }
    ]
  },
  {
    "title": "Fingerprint pore matching using deep features",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107208",
    "abstract": "As a popular living fingerprint feature, sweat pore has been adopted to build robust high resolution automated fingerprint recognition systems (AFRSs). Pore matching is an important step in high resolution fingerprint recognition. This paper proposes a novel pore matching method with high recognition accuracy. The method mainly solves the pore representation problem in the state-of-the-art direct pore matching method. By making full use of the diversity and large quantities of sweat pores on fingerprints, deep convolutional networks are carefully designed to learn a deep feature (denoted as DeepPoreID) for each pore. The inter-class difference and intra-class similarity of pore patch pairs can be well solved using deep learning. The DeepPoreID is then used to describe the local feature for each pore and finally integrated into the classical direct pore matching method. More specifically, pore patches, which are cropped from both Query and Template fingerprint images, are imported into the well-trained networks to generate DeepPoreID for pore representation. The similarity between those DeepPoreIDs are then obtained by calculating the Euclidian Distance between them. Subsequently, one-to-many coarse pore correspondences are established via comparing their similarity. Finally, classical Weighted RANdom SAmple Consensus (WRANSAC) is employed to pick true pore correspondences from coarse ones. The experiments carried on the two public high resolution fingerprint database have shown the effectiveness of the proposed DeepPoreID, especially for fingerprint matching with small image size. Meanwhile, better recognition accuracy is achieved by the proposed method when compared with the existing state-of-the-art methods. About 35% rise in equal error rate (EER) and about 30% rise in FMR1000 when compared with the best result evaluated on the database with image size of 320 × 240 pixels.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300145",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Euclidean distance",
      "Feature (linguistics)",
      "Feature extraction",
      "Fingerprint (computing)",
      "Image (mathematics)",
      "Law",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Similarity (geometry)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Feng"
      },
      {
        "surname": "Zhao",
        "given_name": "Yuanhao"
      },
      {
        "surname": "Liu",
        "given_name": "Guojie"
      },
      {
        "surname": "Shen",
        "given_name": "Linlin"
      }
    ]
  },
  {
    "title": "Filters for graph-based keyword spotting in historical handwritten documents",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.03.030",
    "abstract": "The accessibility to handwritten historical documents is often constrained by the limited feasibility of automatic full transcriptions. Keyword Spotting (KWS), that allows to retrieve arbitrary query words from documents, has been proposed as alternative. In the present paper, we make use of graphs for representing word images. The actual keyword spotting is thus based on matching a query graph with all documents graphs. However, even with relative fast approximation algorithms the shear amount of matchings might limit the practical application of this approach. For this reason we present two novel filters with linear time complexity that allow to substantially reduce the number of graph matchings actually required. In particular, these filters estimate a graph dissimilarity between a query graph and all document graphs based on their node and edge distribution in a polar coordinate system. Eventually, all graphs from the document with distributions that differ to heavily from the query’s node/edge distribution are eliminated. In an experimental evaluation on four different historical documents, we show that about 90% of the matchings can be omitted, while the KWS accuracy is not negatively affected.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518301090",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Graph",
      "Information retrieval",
      "Keyword search",
      "Keyword spotting",
      "Spotting",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Stauffer",
        "given_name": "Michael"
      },
      {
        "surname": "Fischer",
        "given_name": "Andreas"
      },
      {
        "surname": "Riesen",
        "given_name": "Kaspar"
      }
    ]
  },
  {
    "title": "Deep morphological networks",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107246",
    "abstract": "Mathematical morphology provides powerful nonlinear operators for a variety of image processing tasks such as filtering, segmentation, and edge detection. In this paper, we propose a way to use these nonlinear operators in an end-to-end deep learning framework and illustrate them on different applications. We demonstrate on various examples that new layers making use of the morphological non-linearities are complementary to convolution layers. These new layers can be used to integrate the non-linear operations and pooling into a joint operation. We finally enhance results obtained in boundary detection using this new family of layers with just 0.01% of the parameters of competing state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300522",
    "keywords": [
      "Algorithm",
      "Architectural engineering",
      "Artificial intelligence",
      "Artificial neural network",
      "Boundary (topology)",
      "Computer science",
      "Convolution (computer science)",
      "Deep learning",
      "Edge detection",
      "Engineering",
      "Enhanced Data Rates for GSM Evolution",
      "Image (mathematics)",
      "Image processing",
      "Joint (building)",
      "Mathematical analysis",
      "Mathematical morphology",
      "Mathematics",
      "Nonlinear system",
      "Pattern recognition (psychology)",
      "Physics",
      "Pooling",
      "Quantum mechanics",
      "Segmentation",
      "Variety (cybernetics)"
    ],
    "authors": [
      {
        "surname": "Franchi",
        "given_name": "Gianni"
      },
      {
        "surname": "Fehri",
        "given_name": "Amin"
      },
      {
        "surname": "Yao",
        "given_name": "Angela"
      }
    ]
  },
  {
    "title": "Invariant subspace learning for time series data based on dynamic time warping distance",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107210",
    "abstract": "Low-dimensional and compact representation of time series data is of importance for mining and storage. In practice, time series data are vulnerable to various temporal transformations, such as shift and temporal scaling, however, which are unavoidable in the process of data collection. If a learning algorithm directly calculates the difference between such transformed data based on Euclidean distance, the measurement cannot faithfully reflect the similarity and hence could not learn the underlying discriminative features. In order to solve this problem, we develop a novel subspace learning algorithm based on dynamic time warping (DTW) distance which is an elastic distance defined in a DTW space. The algorithm aims to minimize the reconstruction error in the DTW space. However, since DTW space is a semi-pseudo metric space, it is difficult to generalize common subspace learning algorithms for such semi-pseudo metric spaces. In this work, we introduce warp operators with which DTW reconstruction error can be approximated by reconstruction error between transformed series and their reconstructions in a subspace. The warp operators align time series data with their linear representations in the DTW space, which is in particular important for misaligned time series, so that the subspace can be learned to obtain an intrinsic basis (dictionary) for the representation of the data. The warp operators and the subspace are optimized alternatively until reaching equilibrium. Experiments results show that the proposed algorithm outperforms traditional subspace learning algorithms and temporal transform-invariance based methods (including SIDL, Kernel PCA, and SPMC et. al), and obtains competitive results with the state-of-the-art algorithms, such as BOSS algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300169",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Discriminative model",
      "Dynamic time warping",
      "Economics",
      "Euclidean distance",
      "Invariant (physics)",
      "Machine learning",
      "Mathematical physics",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Series (stratigraphy)",
      "Subspace topology",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Deng",
        "given_name": "Huiqi"
      },
      {
        "surname": "Chen",
        "given_name": "Weifu"
      },
      {
        "surname": "Shen",
        "given_name": "Qi"
      },
      {
        "surname": "Ma",
        "given_name": "Andy J."
      },
      {
        "surname": "Yuen",
        "given_name": "Pong C."
      },
      {
        "surname": "Feng",
        "given_name": "Guocan"
      }
    ]
  },
  {
    "title": "Correspondence edit distance to obtain a set of weighted means of graph correspondences",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.08.027",
    "abstract": "Given a pair of data structures, such as strings, trees, graphs or sets of points, several correspondences (also referred in literature as labellings, matchings or assignments) can be defined between their local parts. The Hamming distance has been largely used to define the dissimilarity of a pair of correspondences between two data structures. Although it has the advantage of being simple in computation, it does not consider the data structures themselves, which the correspondences relate to. In this paper, we extend the definitions of a recently presented distance between correspondences based on the concept of the edit distance, which we called Correspondence edit distance. Moreover, we present an algorithm to compute the set of weighted means between a pair of graph correspondences. Both the Correspondence edit distance and the computation of the set of weighted means are necessary for the calculation of a more representative prototype between a set of correspondences. In the validation section, we show how the use of the Correspondence edit distance increases the quality of the set of weighted means compared to using the Hamming distance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518304276",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Block code",
      "Combinatorics",
      "Computation",
      "Computer science",
      "Decoding methods",
      "Distance matrices in phylogeny",
      "Distance measures",
      "Edit distance",
      "Graph",
      "Hamming code",
      "Hamming distance",
      "Hamming graph",
      "Mathematics",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Moreno-García",
        "given_name": "Carlos Francisco"
      },
      {
        "surname": "Serratosa",
        "given_name": "Francesc"
      },
      {
        "surname": "Jiang",
        "given_name": "Xiaoyi"
      }
    ]
  },
  {
    "title": "Graph edit distance: Accuracy of local branching from an application point of view",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.03.033",
    "abstract": "In the context of graph-based representations, comparing and measuring the dissimilarities between graphs can be done by solving the Graph Edit Distance (GED) problem. It is well known and embedded in many application fields such as Computer Vision and Cheminformatics. GED is a NP-hard minimization problem, therefore the optimal solution cannot be found in reasonable time. The GED problem has been addressed by exact approaches like Mixed Integer Linear Programs (MILP) formulations and heuristic approaches like beam-search, bipartite graph matching among others. Recently, a novel heuristic, called local branching (LocBra) for the GED problem, has been proposed and shown to be efficient. In this work, the focus is on evaluating LocBra with other competitive heuristics available in the literature from an application point of view. Moreover, it tries to answer the following question: is it important to compute an accurate GED regarding the final applications? Similarity search and graph matching are considered as final applications. Three experiments are conducted to evaluate the accuracy and efficiency of the heuristics. The quality of the obtained solutions and matching w.r.t. optimal solutions and ground-truth matching is studied. The results of those experiments show that LocBra has a high correlation with the optimal solutions and the ground-truth matching.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518301119",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bipartite graph",
      "Computer science",
      "Graph",
      "Ground truth",
      "Heuristic",
      "Heuristics",
      "Matching (statistics)",
      "Mathematical optimization",
      "Mathematics",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Darwiche",
        "given_name": "Mostafa"
      },
      {
        "surname": "Conte",
        "given_name": "Donatello"
      },
      {
        "surname": "Raveaux",
        "given_name": "Romain"
      },
      {
        "surname": "T’Kindt",
        "given_name": "Vincent"
      }
    ]
  },
  {
    "title": "Ensemble deep learning for automated visual classification using EEG signals",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107147",
    "abstract": "This paper proposes an automated visual classification framework in which a novel analysis method (LSTMS-B) of EEG signals guides the selection of multiple networks that leads to the improvement of classification performance. The method, called LSTMS-B, combines deep learning and ensemble learning to extract the category-dependent representations of EEG signals. Specifically, it introduces Swish activation function into traditional LSTM which reduces the effect of vanishing gradient and optimize the training process. Besides, the Bagging theory is applied to increase the generalization. The LSTMS-B method reaches the average precision of 97.13% for learning EEG visual presentations, which greatly outperforms traditional LSTM network and other contrast models. Then, to verify its application value, a ResNet-based regression is trained using original images and relevant EEG representations learned before. We use the output of the regression as the features to classify the images, and finally obtain the average classification accuracy of 90.16%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304480",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Contrast (vision)",
      "Deep learning",
      "Electroencephalography",
      "Ensemble learning",
      "Generalization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Psychiatry",
      "Psychology",
      "Regression",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Xiao"
      },
      {
        "surname": "Chen",
        "given_name": "Wanzhong"
      },
      {
        "surname": "You",
        "given_name": "Yang"
      },
      {
        "surname": "Jiang",
        "given_name": "Yun"
      },
      {
        "surname": "Li",
        "given_name": "Mingyang"
      },
      {
        "surname": "Zhang",
        "given_name": "Tao"
      }
    ]
  },
  {
    "title": "Domain adaptive representation learning for facial action unit recognition",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107127",
    "abstract": "Learning robust representations for applications with multiple modalities of input can have a significant impact on improving performance. Traditional representation learning methods rely on projecting the input modalities to a common subspace to maximize agreement amongst the modalities for a particular task. We propose a novel approach to representation learning that uses a latent representation decoder to reconstruct the target modality and thereby employ the target modality purely as a supervision signal for discovering correlations between the modalities. Through cross modality supervision, we demonstrate that the learnt representation is able to improve upon the performance of the task of facial action unit (AU) recognition over modality specific representations and even their fused counterparts. As an extension, we explore a new transfer learning technique to adapt the learnt representation to the target domain. We also present a shared representation based feature fusion methodology to improve the performance of any multi-modal system. Our experiments on three AU recognition datasets - MMSE, BP4D and DISFA, show strong performance gains producing state-of-the-art results in spite of the absence of data from a modality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304285",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Economics",
      "Feature (linguistics)",
      "Feature learning",
      "Law",
      "Linguistics",
      "Machine learning",
      "Management",
      "Modalities",
      "Modality (human–computer interaction)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Social science",
      "Sociology",
      "Subspace topology",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Sankaran",
        "given_name": "Nishant"
      },
      {
        "surname": "Mohan",
        "given_name": "Deen Dayal"
      },
      {
        "surname": "Lakshminarayana",
        "given_name": "Nagashri N."
      },
      {
        "surname": "Setlur",
        "given_name": "Srirangaraj"
      },
      {
        "surname": "Govindaraju",
        "given_name": "Venu"
      }
    ]
  },
  {
    "title": "On the exact computation of the graph edit distance",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.05.002",
    "abstract": "The graph edit distance is a widely used distance measure for labelled graph. However, A ★ − GED , the standard approach for its exact computation, suffers from huge runtime and memory requirements. Recently, three better performing algorithms have been proposed: The general algorithms DF − GED and BIP − GED , and the algorithm CSI − GED , which only works for uniform edit costs. All newly proposed algorithms outperform the standard approach A ★ − GED . However, cross-comparisons are lacking. This paper consolidates and extends these recent advances. To this purpose, we present all existing algorithms in a unified way and show that the slightly different definitions of the graph edit distance underlying A ★ − GED and DF − GED , on the one side, and CSI − GED , on the other side, can be harmonised. This harmonisation allows us to develop a generalisation of CSI − GED to non-uniform edit cost. Moreover, we present a speed-up of A ★ − GED and DF − GED for uniform edit costs, which build upon the fact that, in the uniform case, a continuously used subroutine can be implemented to run in linear rather than cubic time. We also suggest an algorithm MIP − GED which builds upon a very compact new mixed integer linear programming formulation. Finally, we carry out a thorough empirical evaluation, which, for the first time, compares all existing exact algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518301685",
    "keywords": [
      "Algorithm",
      "Computation",
      "Computer science",
      "Edit distance",
      "Graph",
      "Parallel computing",
      "Programming language",
      "Speedup",
      "Subroutine",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Blumenthal",
        "given_name": "David B."
      },
      {
        "surname": "Gamper",
        "given_name": "Johann"
      }
    ]
  },
  {
    "title": "Auto-weighted multi-view co-clustering via fast matrix factorization",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107207",
    "abstract": "Multi-view clustering is a hot research topic in machine learning and pattern recognition, however, it remains high computational complexity when clustering multi-view data sets. Although a number of approaches have been proposed to accelerate the computational efficiency, most of them do not consider the data duality between features and samples. In this paper, we propose a novel co-clustering approach termed as Fast Multi-view Bilateral K-means (FMVBKM), which can implement clustering task on row and column of the input data matrix, simultaneously. Specifically, FMVBKM applies the relaxed K-means clustering technique to multi-view data clustering. In addition, to decrease information loss in matrix factorization, we further introduce a new co-clustering method named as Fast Multi-view Matrix Tri-Factorization (FMVMTF). Extensive experimental results on six benchmark data sets show that the proposed two approaches not only have comparable clustering performance but also present the high computational efficiency, in comparison with state-of-the-art multi-view clustering methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300133",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biclustering",
      "CURE data clustering algorithm",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Computational complexity theory",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Data stream clustering",
      "Eigenvalues and eigenvectors",
      "Geodesy",
      "Geography",
      "Matrix decomposition",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Nie",
        "given_name": "Feiping"
      },
      {
        "surname": "Shi",
        "given_name": "Shaojun"
      },
      {
        "surname": "Li",
        "given_name": "Xuelong"
      }
    ]
  },
  {
    "title": "Unsupervised representation learning by discovering reliable image relations",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107107",
    "abstract": "Learning robust representations that allow to reliably establish relations between images is of paramount importance for virtually all of computer vision. Annotating the quadratic number of pairwise relations between training images is simply not feasible, while unsupervised inference is prone to noise, thus leaving the vast majority of these relations to be unreliable. To nevertheless find those relations which can be reliably utilized for learning, we follow a divide-and-conquer strategy: We find reliable similarities by extracting compact groups of images and reliable dissimilarities by partitioning these groups into subsets, converting the complicated overall problem into few reliable local subproblems. For each of the subsets we obtain a representation by learning a mapping to a target feature space so that their reliable relations are kept. Transitivity relations between the subsets are then exploited to consolidate the local solutions into a concerted global representation. While iterating between grouping, partitioning, and learning, we can successively use more and more reliable relations which, in turn, improves our image representation. In experiments, our approach shows state-of-the-art performance on unsupervised classification on ImageNet with 46.0% and competes favorably on different transfer learning tasks on PASCAL VOC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132031930408X",
    "keywords": [],
    "authors": [
      {
        "surname": "Milbich",
        "given_name": "Timo"
      },
      {
        "surname": "Ghori",
        "given_name": "Omair"
      },
      {
        "surname": "Diego",
        "given_name": "Ferran"
      },
      {
        "surname": "Ommer",
        "given_name": "Björn"
      }
    ]
  },
  {
    "title": "Directed and undirected network evolution from Euler–Lagrange dynamics",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.03.029",
    "abstract": "In this paper, we investigate both undirected and directed network evolution using the Euler–Lagrange equation. We use the Euler–Lagrange equation to develop a variational principle based on the von Neumann entropy for time-varying network structure. Commencing from recent work to approximate the von Neumann entropy using simple degree statistics, the changes in entropy between different time epochs are determined by correlations in the degree difference in the edge connections. Our Euler–Lagrange equation minimises the change in entropy and allows to develop a dynamic model to simulate the changes of node degree with time. We first explore the effect of network dynamics on the three widely studied complex network models, namely (a) Erdős-Rényi random graphs, (b) Watts-Strogatz small-world networks, and (c) Barabási-Albert scale-free networks. Our model effectively captures both undirected and directed structural transitions in the dynamic network models. We apply our model to a network time sequence representing the evolution of stock prices on the New York Stock Exchange(NYSE) and sequences of Drosophila gene regulatory networks containing different developmental phases of the organism from embryo to adult. Here we use the model to differentiate between periods of stable and unstable stock price trading and to detect periods of anomalous network evolution. Our experiments show that the presented model not only provides an accurate simulation of the degree statistics in time-varying networks but also captures the topological variations taking place when the structure of a network changes violently.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518301089",
    "keywords": [
      "Acoustics",
      "Applied mathematics",
      "Combinatorics",
      "Complex network",
      "Computer science",
      "Degree (music)",
      "Entropy (arrow of time)",
      "Euler's formula",
      "Mathematical analysis",
      "Mathematics",
      "Physics",
      "Quantum",
      "Quantum entanglement",
      "Quantum mechanics",
      "Statistical physics",
      "Von Neumann entropy"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jianjia"
      },
      {
        "surname": "Wilson",
        "given_name": "Richard C."
      },
      {
        "surname": "Hancock",
        "given_name": "Edwin R."
      }
    ]
  },
  {
    "title": "Blind single image super-resolution with a mixture of deep networks",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107169",
    "abstract": "Existing deep neural network based image super-resolution (SR) methods are mostly designed for non-blind cases, where the blur kernel used to generate the low-resolution (LR) images is assumed to be known and fixed. However, this assumption does not hold in many real scenarios. Motivated by the observation that SR of LR images generated by different blur kernels are essentially different but also correlated, we propose a mixture model of deep networks, which is capable of clustering SR tasks of different blur kernels into a set of groups. Each group is composed of correlated SR tasks with similar blur kernels and can be effectively handled by a combination of specific networks in the mixture model. To achieve automatic SR tasks clustering and network selection, we model the blur kernel with a latent variable, which is inferred from the input image by an encoder network. Since the ground-truth of the latent variable is unknown in the training stage, we initialize the encoder network by pre-training it on the blur kernel classification task to avoid trivial solutions. To jointly train the mixture model and the encoder network, we further derive a lower bound of the likelihood function, which circumvents the intractability in direct maximum likelihood estimation. Extensive evaluations are performed on benchmark data sets and validate the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304698",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Kernel (algebra)",
      "Latent variable",
      "Mathematics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yifan"
      },
      {
        "surname": "Wang",
        "given_name": "Lijun"
      },
      {
        "surname": "Wang",
        "given_name": "Hongyu"
      },
      {
        "surname": "Li",
        "given_name": "Peihua"
      },
      {
        "surname": "Lu",
        "given_name": "Huchuan"
      }
    ]
  },
  {
    "title": "Sparse filtered SIRT for electron tomography",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107253",
    "abstract": "Electron tomographic reconstruction is a method for obtaining a three-dimensional image of a specimen with a series of two dimensional microscope images taken from different viewing angles. Filtered backprojection, one of the most popular tomographic reconstruction methods, does not work well under the existence of image noises and missing wedges. This paper presents a new approach to largely mitigate the effect of noises and missing wedges. We propose a novel filtered backprojection that optimizes the filter of the backprojection operator in terms of a reconstruction error. This data-dependent filter adaptively chooses the spectral domains of signals and noises, suppressing the noise frequency bands, so it is very effective in denoising. We also propose the new filtered backprojection embedded within the simultaneous iterative reconstruction iteration for mitigating the effect of missing wedges. Our numerical study is presented to show the performance gain of the proposed approach over the state-of-the-art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300583",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Filter (signal processing)",
      "Gene",
      "Image (mathematics)",
      "Iterative reconstruction",
      "Mathematics",
      "Noise (video)",
      "Noise reduction",
      "Operator (biology)",
      "Optics",
      "Physics",
      "Repressor",
      "Tomographic reconstruction",
      "Tomography",
      "Transcription factor"
    ],
    "authors": [
      {
        "surname": "Mu",
        "given_name": "Chen"
      },
      {
        "surname": "Park",
        "given_name": "Chiwoo"
      }
    ]
  },
  {
    "title": "Multi-scale differential feature for ECG biometrics with collective matrix factorization",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107211",
    "abstract": "Electrocardiogram (ECG) biometrics has recently received considerable attention and is considered to be a promising biometric trait. Although some promising results on ECG biometrics have been reported, it is still challenging to perform this technique robustly and precisely. To address these issues, this paper presents a novel ECG biometrics framework: Multi-Scale Differential Feature for ECG biometrics with Collective Matrix Factorization (CMF). First, we extract the Multi-Scale Differential Feature (MSDF) from the one-dimensional ECG signal and then fuse MSDF with 1DMRLBP to generate the MSDF-1DMRLBP, which acts as the base feature of the ECG signal. Second, to extract discriminative information from the intermediate base features, we leverage the CMF technique to generate the final robust ECG representations by simultaneously embedding MSDF-1DMRLBP and label information. Consequently, the final robust features could preserve the intra-subject and inter-subject similarities. Extensive experiments are conducted on four ECG databases, and the results demonstrate that the proposed method can outperform the state-of-the-art in terms of both accuracy and efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300170",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Data mining",
      "Discriminative model",
      "Eigenvalues and eigenvectors",
      "Feature (linguistics)",
      "Leverage (statistics)",
      "Linguistics",
      "Matrix decomposition",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Kuikui"
      },
      {
        "surname": "Yang",
        "given_name": "Gongping"
      },
      {
        "surname": "Huang",
        "given_name": "Yuwen"
      },
      {
        "surname": "Yin",
        "given_name": "Yilong"
      }
    ]
  },
  {
    "title": "A two-step hypergraph reduction based fitting method for unbalanced data",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.11.003",
    "abstract": "In this paper, we propose a two-step hypergraph reduction based fitting for unbalanced data. A hypergraph effectively characterizes the relationship between model hypotheses and data points for model fitting. However, a hypergraph-based fitting method often suffers from the problem of high computational cost due to the complex relationship between hyperedges and vertices. Hypergraph reduction algorithms are used to alleviate this problem, but they cannot work well for unbalanced data. To deal with the unbalanced model fitting problem, we first locally remove hyperedges corresponding to the same model instances in data, and then globally remove hyperedges corresponding to the bad model hypotheses. Moreover, we extend the binary incident matrix of a normal hypergraph to a continuous (soft) generalization, to improve the accuracy of hypergraph partition for model fitting. Experimental results on both synthetic data and real images demonstrate that the proposed method has significant superiority over several other state-of-the-art fitting methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518308754",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Binary data",
      "Binary number",
      "Combinatorics",
      "Composite material",
      "Computer science",
      "Data mining",
      "Discrete mathematics",
      "Generalization",
      "Geometry",
      "Hypergraph",
      "Materials science",
      "Mathematical analysis",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Partition (number theory)",
      "Reduction (mathematics)"
    ],
    "authors": [
      {
        "surname": "Xiao",
        "given_name": "Guobao"
      },
      {
        "surname": "Zhou",
        "given_name": "Xiong"
      },
      {
        "surname": "Yan",
        "given_name": "Yan"
      },
      {
        "surname": "Wang",
        "given_name": "Hanzi"
      }
    ]
  },
  {
    "title": "Long video question answering: A Matching-guided Attention Model",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107248",
    "abstract": "Existing video question answering methods answer given questions based on short video snippets. The underlying assumption is that the visual content indicating the ground truth answer ubiquitously exists in the snippet. It might be problematic for long video applications, since involving large numbers of answer-irrelevant snippets will dramatically degenerate the performance. To deal with this issue, we focus on a rarely investigated but practically important problem, namely long video QA, by predicting answers directly from long videos rather than manually pre-extracted short video snippets. We accordingly propose a Matching-guided Attention Model (MAM) which jointly extracts question-related video snippets and predicts answers in a unified framework. To localize questions accurately and efficiently, we calculate corresponding matching scores and boundary regression results for candidate video snippet proposals generated by sliding windows of limited granularity. Guided by the matching scores, the model pays different attention to the extracted video snippet proposals for each question. Finally, we use the attended visual features along with the question to predict the answer in a classification manner. A key obstacle to training our model is that publicly available video QA datasets only contain short videos especially designed for short video QA. Thus, we generate two new datasets for this task on the top of TACoS Multi-level dataset and MSR-VTT dataset by generating QA pairs from the video captions, called TACoS-QA and MSR-VTT-QA. Experimental results show the effectiveness of our proposed method on both datasets by comparing with two short video QA methods and a baseline method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300546",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer security",
      "Economics",
      "Focus (optics)",
      "Information retrieval",
      "Key (lock)",
      "Law",
      "Machine learning",
      "Management",
      "Matching (statistics)",
      "Mathematics",
      "Natural language processing",
      "Obstacle",
      "Optics",
      "Physics",
      "Political science",
      "Pruning",
      "Question answering",
      "Snippet",
      "Statistics",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Weining"
      },
      {
        "surname": "Huang",
        "given_name": "Yan"
      },
      {
        "surname": "Wang",
        "given_name": "Liang"
      }
    ]
  },
  {
    "title": "Learning error-correcting graph matching with a multiclass neural network",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.03.031",
    "abstract": "Many tasks in computer vision and pattern recognition are formulated as graph matching problems. Despite the NP-hard nature of such problems, fast and accurate approximations have led to significant progress in a wide range of applications. However, learning graph matching from observed data, remains a challenging issue. In practice, the node correspondences ground truth is rarely available. This paper presents an effective scheme for optimizing the graph matching problem in a classification context. For this, we propose a representation that is based on a parametrized model graph, and optimize the associated parameters. The objective of the optimization problem is to increase the classification rate. Experimental results on seven public datasets demonstrate the effectiveness (in terms of accuracy and speed) of our approach compared to four reference graph classifiers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518301107",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Graph",
      "Ground truth",
      "Machine learning",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Martineau",
        "given_name": "Chloé"
      },
      {
        "surname": "Raveaux",
        "given_name": "Romain"
      },
      {
        "surname": "Conte",
        "given_name": "Donatello"
      },
      {
        "surname": "Venturini",
        "given_name": "Gilles"
      }
    ]
  },
  {
    "title": "Object recognition based on convex hull alignment",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107199",
    "abstract": "A common approach to recognition of objects in cluttered scenes is to generate hypotheses about objects present in the scene by matching local descriptors of point features. These hypotheses are then evaluated by measuring how well they explain a particular part of the scene. In this paper, we investigate an alternative approach, which is based on alignment of convex hulls of segments detected in a depth image with convex hulls of target 3D object models or their parts. This alignment is performed using the Convex Template Instance descriptor. This descriptor was originally proposed for fruit recognition and classification of segmented objects. We have adapted this approach to recognize objects in complex scenes. Furthermore, we propose a novel three-level hypothesis evaluation strategy which can be used to achieve highly efficient object recognition. The proposed approach is evaluated by comparison with nine state-of-the-art approaches using three challenging benchmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300066",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cognitive neuroscience of visual object recognition",
      "Computer science",
      "Computer vision",
      "Convex hull",
      "Geodesy",
      "Geography",
      "Geometry",
      "Image (mathematics)",
      "Matching (statistics)",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Point (geometry)",
      "Regular polygon",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Cupec",
        "given_name": "Robert"
      },
      {
        "surname": "Vidović",
        "given_name": "Ivan"
      },
      {
        "surname": "Filko",
        "given_name": "Damir"
      },
      {
        "surname": "Đurović",
        "given_name": "Petra"
      }
    ]
  },
  {
    "title": "MIMN-DPP: Maximum-information and minimum-noise determinantal point processes for unsupervised hyperspectral band selection",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107213",
    "abstract": "Band selection plays an important role in hyperspectral imaging for reducing the data and improving the efficiency of data acquisition and analysis whilst significantly lowering the cost of the imaging system. Without the category labels, it is challenging to select an effective and low-redundancy band subset. In this paper, a new unsupervised band selection algorithm is proposed based on a new band search criterion and an improved Determinantal Point Processes (DPP). First, to preserve the original information of hyperspectral image, a novel band search criterion is designed for searching the bands with high information entropy and low noise. Unfortunately, finding the optimal solution based on the search criteria to select a low-redundancy band subset is a NP-hard problem. To solve this problem, we consider the correlation of bands from both original hyperspectral image and its spatial information to construct a double-graph model to describe the relationship between spectral bands. Besides, an improved DPP algorithm is proposed for the approximate search of a low-redundancy band subset from the double-graph model. Experiment results on several well-known datasets show that the proposed optical band selection algorithm achieves better performance than many other state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300194",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Entropy (arrow of time)",
      "Graph",
      "Hyperspectral imaging",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Redundancy (engineering)",
      "Selection (genetic algorithm)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Weizhao"
      },
      {
        "surname": "Yang",
        "given_name": "Zhijing"
      },
      {
        "surname": "Ren",
        "given_name": "Jinchang"
      },
      {
        "surname": "Cao",
        "given_name": "Jiangzhong"
      },
      {
        "surname": "Cai",
        "given_name": "Nian"
      },
      {
        "surname": "Zhao",
        "given_name": "Huimin"
      },
      {
        "surname": "Yuen",
        "given_name": "Peter"
      }
    ]
  },
  {
    "title": "Semantic-aware scene recognition",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107256",
    "abstract": "Scene recognition is currently one of the top-challenging research fields in computer vision. This may be due to the ambiguity between classes: images of several scene classes may share similar objects, which causes confusion among them. The problem is aggravated when images of a particular scene class are notably different. Convolutional Neural Networks (CNNs) have significantly boosted performance in scene recognition, albeit it is still far below from other recognition tasks (e.g., object or image recognition). In this paper, we describe a novel approach for scene recognition based on an end-to-end multi-modal CNN that combines image and context information by means of an attention module. Context information, in the shape of a semantic segmentation, is used to gate features extracted from the RGB image by leveraging on information encoded in the semantic representation: the set of scene objects and stuff, and their relative locations. This gating process reinforces the learning of indicative scene content and enhances scene disambiguation by refocusing the receptive fields of the CNN towards them. Experimental results on three publicly available datasets show that the proposed approach outperforms every other state-of-the-art method while significantly reducing the number of network parameters. All the code and data used along this paper is available at: https://github.com/vpulab/Semantic-Aware-Scene-Recognition",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300613",
    "keywords": [
      "Ambiguity",
      "Artificial intelligence",
      "Biology",
      "Cognitive neuroscience of visual object recognition",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Convolutional neural network",
      "Object (grammar)",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Programming language",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "López-Cifuentes",
        "given_name": "Alejandro"
      },
      {
        "surname": "Escudero-Viñolo",
        "given_name": "Marcos"
      },
      {
        "surname": "Bescós",
        "given_name": "Jesús"
      },
      {
        "surname": "García-Martín",
        "given_name": "Álvaro"
      }
    ]
  },
  {
    "title": "Deep and joint learning of longitudinal data for Alzheimer's disease prediction",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107247",
    "abstract": "Alzheimer's disease (AD) is an irreversible and progressive neurodegenerative disease. The close AD monitoring of this disease is essential for the patient treatment plan adjustment. For AD monitoring, clinical score prediction via neuroimaging data is highly desirable since it is able to reveal the disease status, adequately. For this task, most previous studies are focused on a single time point without considering relationship between neuroimaging data (e.g., Magnetic Resonance Imaging (MRI)) and clinical scores at multiple time points. Differing from these studies, we propose to build a framework based on longitudinal multiple time points data to predict clinical scores. Specifically, the proposed framework consists of three parts, feature selection based on correntropy regularized joint learning, feature encoding based on deep polynomial network, and ensemble learning for regression via the support vector regression method. Two scenarios are designed for scores prediction. Namely, scenario 1 uses the baseline data to achieve the longitudinal scores prediction, while scenario 2 utilizes all the previous time points data to obtain the predicted scores at the next time point, which can improve the score prediction's accuracy. Meanwhile, the missing clinical scores at longitudinal multiple time points are imputated to solve the incompleteness of the data. Extensive experiments on the public database of Alzheimer's Disease Neuroimaging Initiative (ADNI) demonstrate that our proposed framework can effectively reveal the relationship between clinical score and MRI data and outperforms the state-of-the-art methods in scores prediction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300534",
    "keywords": [
      "Aesthetics",
      "Alzheimer's Disease Neuroimaging Initiative",
      "Alzheimer's disease",
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Disease",
      "Feature (linguistics)",
      "Feature selection",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Medicine",
      "Missing data",
      "Neuroimaging",
      "Pathology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Psychiatry",
      "Regression",
      "Statistics",
      "Time point"
    ],
    "authors": [
      {
        "surname": "Lei",
        "given_name": "Baiying"
      },
      {
        "surname": "Yang",
        "given_name": "Mengya"
      },
      {
        "surname": "Yang",
        "given_name": "Peng"
      },
      {
        "surname": "Zhou",
        "given_name": "Feng"
      },
      {
        "surname": "Hou",
        "given_name": "Wen"
      },
      {
        "surname": "Zou",
        "given_name": "Wenbin"
      },
      {
        "surname": "Li",
        "given_name": "Xia"
      },
      {
        "surname": "Wang",
        "given_name": "Tianfu"
      },
      {
        "surname": "Xiao",
        "given_name": "Xiaohua"
      },
      {
        "surname": "Wang",
        "given_name": "Shuqiang"
      }
    ]
  },
  {
    "title": "CAGNet: Content-Aware Guidance for Salient Object Detection",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107303",
    "abstract": "Beneficial from Fully Convolutional Neural Networks (FCNs), saliency detection methods have achieved promising results. However, it is still challenging to learn effective features for detecting salient objects in complicated scenarios, in which i) non-salient regions may have “salient-like” appearance; ii) the salient objects may have different-looking regions. To handle these complex scenarios, we propose a Feature Guide Network which exploits the nature of low-level and high-level features to i) make foreground and background regions more distinct and suppress the non-salient regions which have “salient-like” appearance; ii) assign foreground label to different-looking salient regions. Furthermore, we utilize a Multi-scale Feature Extraction Module (MFEM) for each level of abstraction to obtain multi-scale contextual information. Finally, we design a loss function which outperforms the widely used Cross-entropy loss. By adopting four different pre-trained models as the backbone, we prove that our method is very general with respect to the choice of the backbone model. Experiments on six challenging datasets demonstrate that our method achieves the state-of-the-art performance in terms of different evaluation metrics. Additionally, our approach contains fewer parameters than the existing ones, does not need any post-processing, and runs fast at a real-time speed of 28 FPS when processing a 480 × 480 image.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301072",
    "keywords": [
      "Abstraction",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Entropy (arrow of time)",
      "Epistemology",
      "Feature (linguistics)",
      "Feature extraction",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Mohammadi",
        "given_name": "Sina"
      },
      {
        "surname": "Noori",
        "given_name": "Mehrdad"
      },
      {
        "surname": "Bahri",
        "given_name": "Ali"
      },
      {
        "surname": "Ghofrani Majelan",
        "given_name": "Sina"
      },
      {
        "surname": "Havaei",
        "given_name": "Mohammad"
      }
    ]
  },
  {
    "title": "LG: A clustering framework supported by point proximity relations",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107265",
    "abstract": "Clustering is a research problem based on the data's proximity relationship which is not made full use of by all the existing algorithms. In this paper, we present a novel two-stage LG framework consisting of the proposed Local Energy Gradient Oppression (LEGO) and the Guide Point Assignation (GPA) strategies which are closely related to the data points’ proximity relations. In the LG framework, it is crucial to locate the appropriate centers for the subsequent data label assignment, and therefore we introduce the nuclear model viewing the dataset as a collection of charged particles, which is the basis of LEGO, and the points with local maximum potential energy are ascertained as the cluster centers. Besides, the GPA strategy innovatively adopts the idea that the cluster center actively selects data points as the same cluster, enabling the LG framework still to be effective when dealing with datasets of arbitrary shape distribution. Superiorities of the proposed framework and the two strategies are demonstrated on four synthetic datasets and three real-world faces image datasets in terms of two clustering performance metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300704",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Geometry",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Point (geometry)"
    ],
    "authors": [
      {
        "surname": "Qv",
        "given_name": "Hui"
      },
      {
        "surname": "Yin",
        "given_name": "Jihao"
      },
      {
        "surname": "Luo",
        "given_name": "Xiaoyan"
      }
    ]
  },
  {
    "title": "One-step spectral clustering based on self-paced learning",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.03.035",
    "abstract": "Aiming at traditional spectral clustering method still suffers from the following issues: 1) unable to handle the incomplete data, 2) two-step clustering strategies tend to perform poorly due to the heterogeneity between the similarity matrix learning model and the clustering model, 3) constructing the affinity matrix from original data which often contains noises and outliers. To address these issues, this paper proposes a robust one-step clustering method based on self-paced learning. Specifically, this paper first designs a missing value mapping matrix to handle missing data, and then employs self-paced regularizer to sort the importance of samples, reduce the interference of noise and outliers of the model, second, fusing affinity matrix learning and spectral decomposition into one model to achieve the purpose of one-step clustering. Experimental results on 8 real data sets, verified the effectiveness of our proposed one-step spectral clustering method, compared to state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301197",
    "keywords": [
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Cluster analysis",
      "Composite material",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Data stream clustering",
      "Eigenvalues and eigenvectors",
      "Fuzzy clustering",
      "Image (mathematics)",
      "Information retrieval",
      "Machine learning",
      "Materials science",
      "Matrix (chemical analysis)",
      "Matrix decomposition",
      "Noise (video)",
      "Outlier",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Similarity (geometry)",
      "Spectral clustering",
      "sort"
    ],
    "authors": [
      {
        "surname": "Tong",
        "given_name": "Tao"
      },
      {
        "surname": "Gan",
        "given_name": "Jiangzhang"
      },
      {
        "surname": "Wen",
        "given_name": "Guoqiu"
      },
      {
        "surname": "Li",
        "given_name": "Yangding"
      }
    ]
  },
  {
    "title": "Steganographic universal adversarial perturbations",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.04.025",
    "abstract": "We propose a steganography based technique to generate adversarial perturbations to fool deep models on any image. The proposed perturbations are computed in a transform domain where a single secret image embedded in any target image makes any deep model misclassify the target image with high probability. The attack resulting from our perturbation is ideal for black-box setting, as it does not require any information about the target model. Moreover, being a non-iterative technique, our perturbation estimation remains computationally efficient. The computed perturbations are also imperceptible to humans while they achieve high fooling ratios for the models trained on large-scale ImageNet dataset. We demonstrate successful fooling of ResNet-50, VGG-16, Inception-V3 and MobileNet-V2, achieving up to 89% fooling of these popular classification models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552030146X",
    "keywords": [
      "Adversarial system",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Image (mathematics)",
      "Pattern recognition (psychology)",
      "Perturbation (astronomy)",
      "Physics",
      "Quantum mechanics",
      "Steganography"
    ],
    "authors": [
      {
        "surname": "Ud Din",
        "given_name": "Salah"
      },
      {
        "surname": "Akhtar",
        "given_name": "Naveed"
      },
      {
        "surname": "Younis",
        "given_name": "Shahzad"
      },
      {
        "surname": "Shafait",
        "given_name": "Faisal"
      },
      {
        "surname": "Mansoor",
        "given_name": "Atif"
      },
      {
        "surname": "Shafique",
        "given_name": "Muhammad"
      }
    ]
  },
  {
    "title": "An improved GrabCut on multiscale features",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107292",
    "abstract": "the GrabCut can effectively extract the foreground according to features in a cartoon image; however, the performance is not so effective for a real image, because the feature extraction is independent of segmentation. To improve segmentation performance, this paper proposes an improved GrabCut which combines the segmentation and multiscale feature extraction into a unified model. In this model, the segmentation relies on multiscale features, and the multiscale features depend on multiscale decomposition. A novel total variation regularization is proposed in multiscale decomposition to preserve edges and remove the region inhomogeneity, by which the generalization of features for segmentation is improved. The features obtained by the multiscale decomposition are integrated into the segmentation process, and the foreground can be easily extracted from a proper scale. Experimental results indicate that, compared to the existing GrabCut and improved techniques, this method provides competitive performance in terms of the segmentation accuracy, while being insensitive to inhomogeneity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300960",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Feature extraction",
      "Image segmentation",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Regularization (linguistics)",
      "Scale-space segmentation",
      "Segmentation",
      "Segmentation-based object categorization"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Kun"
      },
      {
        "surname": "Wang",
        "given_name": "Dan"
      },
      {
        "surname": "Tong",
        "given_name": "Miao"
      },
      {
        "surname": "Zhu",
        "given_name": "Zhijuan"
      }
    ]
  },
  {
    "title": "Fundamental sampling patterns for low-rank multi-view data completion",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107307",
    "abstract": "We consider the multi-view data completion problem, i.e., to complete a matrix U = [ U 1 | U 2 ] where the ranks of U, U 1, and U 2 are given. In particular, we investigate the fundamental conditions on the sampling pattern, i.e., locations of the sampled entries for finite completability of such a multi-view data given the corresponding rank constraints. We provide a geometric analysis on the manifold structure for multi-view data to incorporate more than one rank constraint. We derive a probabilistic condition in terms of the number of samples per column that guarantees finite completability with high probability. Finally, we derive the guarantees for unique completability. Numerical results demonstrate reduced sampling complexity when the multi-view structure is taken into account as compared to when only low-rank structure of individual views is taken into account. Then, we propose an apporach using Newton’s method to almost achieve these information-theoretic bounds for mulit-view data retrieval by taking advantage of the rank decomposition and the analysis in this work.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301114",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Filter (signal processing)",
      "Mathematics",
      "Rank (graph theory)",
      "Sampling (signal processing)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Ashraphijuo",
        "given_name": "Morteza"
      },
      {
        "surname": "Wang",
        "given_name": "Xiaodong"
      },
      {
        "surname": "Aggarwal",
        "given_name": "Vaneet"
      }
    ]
  },
  {
    "title": "Depth upsampling based on deep edge-aware learning",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107274",
    "abstract": "Depth map upsampling will unavoidably smoothen the edges leading to blurry results on the depth boundaries, especially at large upscaling factors. Given that edges represent the most important cue in addressing the task of depth upsampling, we propose a novel depth upsampling framework based on deep edge-aware learning. Unlike existing CNN-based approaches that directly predict depth values from low resolution (LR) depth input, our framework firstly learns edge information of depth boundaries from the known LR depth map and its corresponding high resolution (HR) color image as reconstruction cues. Then, two depth restoration modules, i.e., a fast depth filling strategy and a cascaded restoration network, are proposed to recover HR depth map by leveraging the predicted edge map and the HR color image. Extensive comparisons on both edge inference and depth upsampling under noisy and noiseless cases demonstrate the superiority of the proposed approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300790",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Depth map",
      "Depth perception",
      "Enhanced Data Rates for GSM Evolution",
      "Image (mathematics)",
      "Inference",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Perception",
      "Upsampling"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zhihui"
      },
      {
        "surname": "Ye",
        "given_name": "Xinchen"
      },
      {
        "surname": "Sun",
        "given_name": "Baoli"
      },
      {
        "surname": "Yang",
        "given_name": "Jingyu"
      },
      {
        "surname": "Xu",
        "given_name": "Rui"
      },
      {
        "surname": "Li",
        "given_name": "Haojie"
      }
    ]
  },
  {
    "title": "Learning to infer human attention in daily activities",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107314",
    "abstract": "The first attention model in the computer science community is proposed in 1998. In the following years, human attention has been intensively studied. However, these studies mainly refer human attention as the image regions that draw the attention of a human (outside the image) who is looking at the image. In this paper, we infer the attention of a human inside a third-person view video where the human is doing a task, and define human attention as attentional objects that coincide with the task the human is doing. To infer human attention, we propose a deep neural network model that fuses both low-level human pose cue and high-level task encoding cue. Due to the lack of appropriate public datasets for studying this problem, we newly collect a video dataset in complex Virtual-Reality (VR) scenes. In the experiments, we widely compare our method with three other methods on this VR dataset. In addition, we re-annotate a public real dataset and conduct the extensional experiments on this real dataset. The experiment results validate the effectiveness of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301187",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Economics",
      "Encoding (memory)",
      "Human behavior",
      "Image (mathematics)",
      "Machine learning",
      "Management",
      "Task (project management)",
      "Virtual reality"
    ],
    "authors": [
      {
        "surname": "Nan",
        "given_name": "Zhixiong"
      },
      {
        "surname": "Shu",
        "given_name": "Tianmin"
      },
      {
        "surname": "Gong",
        "given_name": "Ran"
      },
      {
        "surname": "Wang",
        "given_name": "Shu"
      },
      {
        "surname": "Wei",
        "given_name": "Ping"
      },
      {
        "surname": "Zhu",
        "given_name": "Song-Chun"
      },
      {
        "surname": "Zheng",
        "given_name": "Nanning"
      }
    ]
  },
  {
    "title": "Cross-modal guidance based auto-encoder for multi-video summarization",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.04.011",
    "abstract": "Multi-Video Summarization (MVS) aims at condensing a great number of videos with the same search query to obtain a compact storyboard, which improves user’s browsing experience. Recently, a line of research resorts to a mass of extra web images to specify the search intent for a query. However, these web images are indirect and noisy. In contrast, the tag information for queried videos is more accurate and concise than those web images, which can also be directly employed as side information. To this end, we proposed to employ tag information, i.e., titles and descriptions, as the side information for the generation of summarization. Specifically, we employed sparse auto-encoder as the main model to generate the final summarization, where the input was the multiple videos, the output was the keyframes set. Meanwhile, we fused the visual and tag information to guide visual features, which constrained the sparse auto-encoder to select the important candidate keyframes. The proposed approach is called Cross-modal guidance based Auto-Encoder for MVS approach (CAE-MVS). Extensive experimental results demonstrate that the CAE-MVS clearly outperforms the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301318",
    "keywords": [
      "Artificial intelligence",
      "Automatic summarization",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Encoder",
      "Information retrieval",
      "Modal",
      "Multimedia",
      "Operating system",
      "Polymer chemistry",
      "Programming language",
      "Search engine",
      "Set (abstract data type)",
      "Storyboard"
    ],
    "authors": [
      {
        "surname": "Ji",
        "given_name": "Zhong"
      },
      {
        "surname": "Zhao",
        "given_name": "Yuxiao"
      },
      {
        "surname": "Pang",
        "given_name": "Yanwei"
      },
      {
        "surname": "Li",
        "given_name": "Xuelong"
      }
    ]
  },
  {
    "title": "Fast minutiae extractor using neural network",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107273",
    "abstract": "In this paper, we propose a fast and reliable neural network-based algorithm for fingerprint minutiae extraction. In particular, our algorithm involves a two-stage process: in the first stage, a network generates candidate patches in which minutiae may exist; in the second stage, another network extracts minutiae from every patch.These two networks share a common part to reduce the running time. Moreover, we analyze the properties of fingerprint images and propose a principle for designing efficient networks for minutiae extraction. For efficiency, our algorithm extracts minutiae directly from raw fingerprint images, without traditional pre-processes. Another benefit of this design is that the networks only require datasets with minutiae labels for training. On the public fingerprint datasets (FVC 2002 and 2004), our algorithm requires 26 ms on average to extract minutiae from one fingerprint on a single GPU. Compared with other neural network-based algorithms, our algorithm runs approximately 10 times faster and does not lose substantial accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300789",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Data mining",
      "Engineering",
      "Extractor",
      "Fingerprint (computing)",
      "Fingerprint recognition",
      "Minutiae",
      "Pattern recognition (psychology)",
      "Process engineering"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Baicun"
      },
      {
        "surname": "Han",
        "given_name": "Congying"
      },
      {
        "surname": "Liu",
        "given_name": "Yonghong"
      },
      {
        "surname": "Guo",
        "given_name": "Tiande"
      },
      {
        "surname": "Qin",
        "given_name": "Jin"
      }
    ]
  },
  {
    "title": "On the importance of similarity characteristics of curve clustering and its applications",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.04.024",
    "abstract": "This paper presents an overview of curve clustering from the similarity characteristics perspective, with a goal of providing useful advice and references regarding fundamental concepts that are accessible to the broad community of curve clustering practitioners. We introduce a new taxonomy of curve clustering by proposing four major similarity characteristics. We reviewed some contributions to curve clustering with respect to their similarity characteristics along with their applications. Lastly, we give an in-depth discussion of the overall challenges in this field with respect to similarity characteristics, highlight open research questions and discuss guidelines for further progress.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301471",
    "keywords": [
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Consensus clustering",
      "Data mining",
      "Data science",
      "Field (mathematics)",
      "Fuzzy clustering",
      "Image (mathematics)",
      "Information retrieval",
      "Mathematics",
      "Perspective (graphical)",
      "Pure mathematics",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Cheam",
        "given_name": "Amay S.M."
      },
      {
        "surname": "Fredette",
        "given_name": "Marc"
      }
    ]
  },
  {
    "title": "Efficient component-hypertree construction based on hierarchy of partitions",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.02.032",
    "abstract": "The component-hypertree is a data structure that generalizes the concept of component-tree to multiple (increasing) neighborhoods. However, construction of a component-hypertree is costly because it needs to process a high number of neighbors. In this article, we present some properties used to obtain optimized neighborhoods for component-hypertree computation. Using these properties, we explore a new strategy to obtain neighboring elements based on hierarchy of partitions, leading to a more efficient algorithm with the drawback of a slight loss of precision on the distance of merged nodes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300775",
    "keywords": [
      "Algorithm",
      "Combinatorics",
      "Component (thermodynamics)",
      "Computation",
      "Computer science",
      "Differential geometry",
      "Economics",
      "Hierarchy",
      "Hyperbolic geometry",
      "Hyperbolic tree",
      "Market economy",
      "Mathematical analysis",
      "Mathematics",
      "Physics",
      "Process (computing)",
      "Programming language",
      "Theoretical computer science",
      "Thermodynamics",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Morimitsu",
        "given_name": "Alexandre"
      },
      {
        "surname": "Passat",
        "given_name": "Nicolas"
      },
      {
        "surname": "L Alves",
        "given_name": "Wonder A."
      },
      {
        "surname": "Hashimoto",
        "given_name": "Ronaldo F."
      }
    ]
  },
  {
    "title": "Phoneme classification in reconstructed phase space with convolutional neural networks",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.05.002",
    "abstract": "In this paper, we analyse segmented speech phonemes with Convolutional filters, after embedding them in Reconstructed Phase Space (RPS). These feature extracting Convolutional filters are trained on the embedded speech data from scratch and are also fine-tuned from networks trained with other data. Reconstruction of Phase Space portrays the dynamics of an observed system as a geometric representation. We present a study highlighting the discriminative capacity of the features extracted through Convolutional Neural Network (CNN) from the textural pattern and shape of this geometric representation. CNNs are heavily used in image-related tasks, but have not seen application on phase space portraits, possibly due to the higher dimensionality of the embedding. However, we find that the application of CNN on restricted bi-dimensional RPS, characterizes the space well than prior methods on high dimensional embeddings. We show experimental results supporting the use of RPS with CNN (RPS-CNN) for phoneme classification. The results affirm that essential signal characteristics are automatically quantified from the phase portraits of speech and can be used in place of conventional techniques involving frequency domain transformations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301720",
    "keywords": [
      "Artificial intelligence",
      "Bifurcation",
      "Computer science",
      "Convolutional neural network",
      "Curse of dimensionality",
      "Discriminative model",
      "Embedding",
      "Feature (linguistics)",
      "Feature vector",
      "Law",
      "Linguistics",
      "Nonlinear system",
      "Pattern recognition (psychology)",
      "Phase portrait",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Representation (politics)",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "John Wesley",
        "given_name": "R."
      },
      {
        "surname": "Nayeemulla Khan",
        "given_name": "A."
      },
      {
        "surname": "Shahina",
        "given_name": "A."
      }
    ]
  },
  {
    "title": "Joint image restoration and matching method based on distance-weighted sparse representation prior",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.04.003",
    "abstract": "Image matching is a classic problem in the field of image processing, aims to locate the unique position of the real-time images and has been widely used in visual-based navigation systems. However, most of the works on image matching simply assume the ideal inputs without considering the degradation of the real world, such as image blur. In the presence of such a situation, the traditional matching methods usually first resort to image restoration and then perform image matching with the restored image. However, by treating the restoration and matching separately, the accuracy of blurred image matching will be reduced by the defective output of the image restoration. In this paper, we propose a joint blurred image restoration and matching method based on distance-weighted sparse representation (JRM-DSR), which utilizes the sparse representation prior to exploit the correlation between restoration and matching. This prior assumes that the blurred image, if correctly restored, can be well represented as a sparse linear combination of the dictionary constructed by the reference image. In order to achieve more accurate matching results to help image restoration, we consider both local and sparse information as well as adopt distance-weighted sparse representation to obtain better representation coefficients. By iterative restoring the input image in pursuit of the sparest representation, our approach can achieve restoration and match simultaneous, and these two tasks can benefit greatly from each other. Moreover, a coarse-to-fine matching strategy is proposed to further improve the matching accuracy and search efficiency. Experiments demonstrate the effectiveness of our method compared with conventional methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301239",
    "keywords": [
      "Artificial intelligence",
      "Compressed sensing",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Law",
      "Matching (statistics)",
      "Matching pursuit",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Sparse approximation",
      "Statistics",
      "Template matching"
    ],
    "authors": [
      {
        "surname": "Shao",
        "given_name": "Yuanjie"
      },
      {
        "surname": "Sang",
        "given_name": "Nong"
      },
      {
        "surname": "Li",
        "given_name": "Yacheng"
      },
      {
        "surname": "Li",
        "given_name": "Wenhao"
      },
      {
        "surname": "Gao",
        "given_name": "Changxin"
      }
    ]
  },
  {
    "title": "Adaptive quantile low-rank matrix factorization",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107310",
    "abstract": "Low-rank matrix factorization (LRMF) has received much popularity owing to its successful applications in both computer vision and data mining. By assuming noise to come from a Gaussian, Laplace or mixture of Gaussian distributions, significant efforts have been made on optimizing the (weighted) L 1 or L 2-norm loss between an observed matrix and its bilinear factorization. However, the type of noise distribution is generally unknown in real applications and inappropriate assumptions will inevitably deteriorate the behavior of LRMF. On the other hand, real data are often corrupted by skew rather than symmetric noise. To tackle this problem, this paper presents a novel LRMF model called AQ-LRMF by modeling noise with a mixture of asymmetric Laplace distributions. An efficient algorithm based on the expectation-maximization (EM) algorithm is also offered to estimate the parameters involved in AQ-LRMF. The AQ-LRMF model possesses the advantage that it can approximate noise well no matter whether the real noise is symmetric or skew. The core idea of AQ-LRMF lies in solving a weighted L 1 problem with weights being learned from data. The experiments conducted on synthetic and real data sets show that AQ-LRMF outperforms several state-of-the-art techniques. Furthermore, AQ-LRMF also has the superiority over the other algorithms in terms of capturing local structural information contained in real images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030114X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Factorization",
      "Gaussian noise",
      "Image (mathematics)",
      "Mathematical optimization",
      "Mathematics",
      "Noise (video)",
      "Skew",
      "Synthetic data",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Shuang"
      },
      {
        "surname": "Zhang",
        "given_name": "Chunxia"
      },
      {
        "surname": "Zhang",
        "given_name": "Jiangshe"
      }
    ]
  },
  {
    "title": "A parallel fuzzy rule-base based decision tree in the framework of map-reduce",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107326",
    "abstract": "Decision trees are commonly used for learning and extracting classification rules from data. The fuzzy rule based decision tree (FRDT) is very representative owing to its better robustness and generalization. However, FRDT cannot work well on the analysis of large-scale data sets. One solution for this problem is parallel computing. A proved effective parallel computing model is Map-Reduce. Ensemble learning is an effective strategy which can significantly improve the generalization ability of machine learning systems. The objective of this paper is to develop a fuzzy rule-base based decision tree on the strategies of parallel computing and ensemble learning. First, we implement a parallel fusing fuzzy rule based classification system via Map-Reduce (MR-FFRCS) to display how to extract fuzzy rules from data in parallel and how to evaluate the fuzzy rules in an ensemble learning way. Then, taking MR-FFRCS as a fundamental module, we propose a parallel fuzzy rule-base based decision tree (MR-FRBDT) to improve the original FRDT algorithm. The experimental studies mainly focus on feasibility and parallelism. Compared with FRDT on 23 UCI benchmark data sets, the proposed MR-FRBDT algorithm with fewer parameters is effective and has the ability to handle large-scale data sets. Furthermore, some numerical experiments conducted on several large-scale data sets verify the parallel performance on reducing computing time and avoiding memory restrictions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301291",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Decision tree",
      "Fuzzy logic",
      "Fuzzy rule",
      "Fuzzy set",
      "Gene",
      "Generalization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Mu",
        "given_name": "Yashuang"
      },
      {
        "surname": "Liu",
        "given_name": "Xiaodong"
      },
      {
        "surname": "Wang",
        "given_name": "Lidong"
      },
      {
        "surname": "Zhou",
        "given_name": "Juxiang"
      }
    ]
  },
  {
    "title": "Handling Gaussian blur without deconvolution",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107264",
    "abstract": "The paper presents a new theory of invariants to Gaussian blur. Unlike earlier methods, the blur kernel may be arbitrary oriented, scaled and elongated. Such blurring is a semi-group action in the image space, where the orbits are classes of blur-equivalent images. We propose a non-linear projection operator which extracts blur-insensitive component of the image. The invariants are then formally defined as moments of this component but can be computed directly from the blurred image without an explicit construction of the projections. Image description by the new invariants does not require any prior knowledge of the blur kernel parameters and does not include any deconvolution. The invariance property could be extended also to linear transformation of the image coordinates and combined affine-blur invariants can be constructed. Experimental comparison to three other blur-invariant methods is given. Potential applications of the new invariants are in blur/position invariant image recognition and in robust template matching.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300698",
    "keywords": [
      "Affine transformation",
      "Algorithm",
      "Artificial intelligence",
      "Blind deconvolution",
      "Computer science",
      "Computer vision",
      "Deconvolution",
      "Gaussian",
      "Gaussian blur",
      "Gaussian function",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Invariant (physics)",
      "Kernel (algebra)",
      "Mathematical physics",
      "Mathematics",
      "Physics",
      "Pure mathematics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Kostková",
        "given_name": "Jitka"
      },
      {
        "surname": "Flusser",
        "given_name": "Jan"
      },
      {
        "surname": "Lébl",
        "given_name": "Matěj"
      },
      {
        "surname": "Pedone",
        "given_name": "Matteo"
      }
    ]
  },
  {
    "title": "Semantically consistent text to fashion image synthesis with an enhanced attentional generative adversarial network",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.02.030",
    "abstract": "Recent advancements in Generative Adversarial Networks (GANs) have led to significant improvements in various image generation tasks including image synthesis based on text descriptions. In this paper, we present an enhanced Attentional Generative Adversarial Network (e-AttnGAN) with improved training stability for text-to-image synthesis. e-AttnGAN’s integrated attention module utilizes both sentence and word context features and performs feature-wise linear modulation (FiLM) to fuse visual and natural language representations. In addition to multimodal similarity learning for text and image features of AttnGAN [1], similarity and feature matching losses between real and generated images are included while employing classification losses for “significant attributes”. In order to improve the stability of the training and solve the mode collapse issue, spectral normalization and two-time scale update rule are used for the discriminator together with instance noise. Our experiments show that e-AttnGAN outperforms state-of-the-art methods using the FashionGen and DeepFashion-Synthesis datasets in terms of inception score, R-precision and classification accuracy. A detailed ablation study has been conducted to observe the effect of each component.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300751",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Generative adversarial network",
      "Generative grammar",
      "Image (mathematics)",
      "Image synthesis",
      "Natural language processing",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Ak",
        "given_name": "Kenan E."
      },
      {
        "surname": "Lim",
        "given_name": "Joo Hwee"
      },
      {
        "surname": "Tham",
        "given_name": "Jo Yew"
      },
      {
        "surname": "Kassim",
        "given_name": "Ashraf A."
      }
    ]
  },
  {
    "title": "Efficient binocular stereo correspondence matching with 1-D Max-Trees",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.02.019",
    "abstract": "Extraction of depth from images is of great importance for various computer vision applications. Methods based on convolutional neural networks are very accurate but have high computation requirements, which can be achieved with GPUs. However, GPUs are difficult to use on devices with low power requirements like robots and embedded systems. In this light, we propose a stereo matching method appropriate for applications in which limited computational and energy resources are available. The algorithm is based on a hierarchical representation of image pairs which is used to restrict disparity search range. We propose a cost function that takes into account region contextual information and a cost aggregation method that preserves disparity borders. We tested the proposed method on the Middlebury and KITTI benchmark data sets and on the TrimBot2020 synthetic data. We achieved accuracy and time efficiency results that show that the method is suitable to be deployed on embedded and robotics systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300581",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Composite material",
      "Computation",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Geodesy",
      "Geography",
      "Law",
      "Matching (statistics)",
      "Materials science",
      "Mathematics",
      "Political science",
      "Politics",
      "Range (aeronautics)",
      "Representation (politics)",
      "Robot",
      "Robotics",
      "Statistics",
      "Stereopsis"
    ],
    "authors": [
      {
        "surname": "Brandt",
        "given_name": "Rafaël"
      },
      {
        "surname": "Strisciuglio",
        "given_name": "Nicola"
      },
      {
        "surname": "Petkov",
        "given_name": "Nicolai"
      },
      {
        "surname": "Wilkinson",
        "given_name": "Michael H.F."
      }
    ]
  },
  {
    "title": "Modelling visual impressions for Chinese and Pakistani ethnic groups",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107259",
    "abstract": "This work describes an investigation into the relationship between the visual impressions of a number of facial images each described by a set of parameters related to the position and size of discrete components: the eyes, the nose and the lips. Observations were made by members of two ethnic groups, Chinese and Pakistani, and the images comprised female faces from the same two groups. The observers used 16 impression scales to assess each image and a forced-choice scaling technique. From the factor analysis, the results showed that three visual impressions, attractive, feminine and mature, can well represent all the 16 scales. In the second experiment, the observers assessed visual impressions of more images using only these three impressions. Data are presented relating to the differences between the observations for the various facial locations, as well as between observers from different ethnic groups. Models have been developed that describe the data and their predictions outperformed traditional models, i.e.symmetry, golden ratio and neoclassical canons. The differences between the results of the two ethnic groups were found to be small; there were however, some significant differences in the responses to different facial features.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300649",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Computer science",
      "Ethnic group",
      "Geography",
      "Psychology",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Mughal",
        "given_name": "Muhammad Farhan"
      },
      {
        "surname": "Luo",
        "given_name": "Ming Ronnier"
      },
      {
        "surname": "Pointer",
        "given_name": "Michael"
      }
    ]
  },
  {
    "title": "IOS-Net: An inside-to-outside supervision network for scale robust text detection in the wild",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107304",
    "abstract": "Accurately detecting scene text is a challenging task due to perspective distortion, scale variance, varied orientations, uneven illumination. Among them, scale variance has always been a core issue and generally involves two types: various size and diverse aspect ratios of the text regions. In contrast to most existing approaches focusing on addressing one type of scale variance, this paper presents a novel inside-to-outside supervision network (IOS-Net) that can well tackle both two. Specifically, we design a hierarchical supervision module (HSM), which consists of a new inception unit with parallel asymmetric convolution and a skip-layer fusion structure. Inside the HSM, we introduce hierarchical supervision into the new inception unit to effectively capture the texts with diverse aspect ratios. Outside the HSM, we adopt multiple-scale supervision on the stacked HSMs to accurately detect the texts with various sizes. Moreover, a position-sensitive segmentation is used to enhance the representation of difficult text objects and the discrimination of adjacent ones. The proposed method achieves state-of-the-art performance on representative public benchmarks, reaching 86% F-score and 11.5 frames per second (FPS) on the ICDAR 2015 incidental text dataset, 47% F-score and 16.1 FPS on the COCO-Text dataset, 69% F-score and 11.7 FPS on the ICDAR 2013 video text dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301084",
    "keywords": [
      "Accounting",
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Business",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Convolutional neural network",
      "Distortion (music)",
      "Image (mathematics)",
      "Law",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Perspective distortion",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Representation (politics)",
      "Scale (ratio)",
      "Segmentation",
      "Variance (accounting)"
    ],
    "authors": [
      {
        "surname": "Cai",
        "given_name": "Yuanqiang"
      },
      {
        "surname": "Wang",
        "given_name": "Weiqiang"
      },
      {
        "surname": "Chen",
        "given_name": "Yuting"
      },
      {
        "surname": "Ye",
        "given_name": "Qixiang"
      }
    ]
  },
  {
    "title": "Learning shape and motion representations for view invariant skeleton-based action recognition",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107293",
    "abstract": "Skeleton-based action recognition is an increasing attentioned task that analyses spatial configuration and temporal dynamics of a human action from skeleton data, which has been widely applied in intelligent video surveillance and human-computer interaction. How to design an effective framework to learn discriminative spatial and temporal characteristics for skeleton-based action recognition is still a challenging problem. The shape and motion representations of skeleton sequences are the direct embodiment of spatial and temporal characteristics respectively, which can well address for human action description. In this work, we propose an original unified framework to learn comprehensive shape and motion representations from skeleton sequences by using Geometric Algebra. We firstly construct skeleton sequence space as a subset of Geometric Algebra to represent each skeleton sequence along both the spatial and temporal dimensions. Then rotor-based view transformation method is proposed to eliminate the effect of viewpoint variation, which remains the relative spatio-temporal relations among skeleton frames in a sequence. We also construct spatio-temporal view invariant model (STVIM) to collectively integrate spatial configuration and temporal dynamics of skeleton joints and bones. In STVIM, skeleton sequence shape and motion representations which mutually compensate are jointly learned to describe skeleton-based actions comprehensively. Furthermore, a selected multi-stream Convolutional Neural Network is employed to extract and fuse deep features from mapping images of the learned representations for skeleton-based action recognition. Experimental results on NTU RGB+D, Northwestern-UCLA and UTD-MHAD datasets consistently verify the effectiveness of our proposed method and the superior performance over state-of-the-art competitors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300972",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Discriminative model",
      "Genetics",
      "Human skeleton",
      "Invariant (physics)",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Sequence (biology)",
      "Skeleton (computer programming)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yanshan"
      },
      {
        "surname": "Xia",
        "given_name": "Rongjie"
      },
      {
        "surname": "Liu",
        "given_name": "Xing"
      }
    ]
  },
  {
    "title": "Securing the internet of vehicles through lightweight block ciphers",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.04.038",
    "abstract": "In recent years, the automotive industry has made significant investments in the Internet of Things (IoT) paradigm, aiming to develop technologies and services for connected vehicles that focus on user-oriented solutions, providing unique and tailored driving experiences. Nowadays, the vehicles are connected to the car manufacturer network, the road operator networks, and the Internet. Such interconnections are used for infotainment applications and driving assistance services, such as real-time HD-maps, and road safety applications. For these reasons, the concept of the Internet of Vehicles (IoVs) is increasingly emerging and, vehicles become Internet terminals exposed to cyber-attacks. In this context, the main weakness is given by the Controller Area Network (CAN) protocol. This protocol, which governs the in-vehicle network, was designed to reduce transmission error problems and minimize latency times. However, it does not employ any security facility to protect the communication. In particular, one of the main issues of this protocol is that it uses unencrypted communication. Improving the security of this protocol is necessary while preserving its performance in terms of communication efficiency. This paper investigates the possibility of using lightweight block ciphers to secure in-vehicle devices such as microcontrollers, which have constrained hardware and software capabilities. The results obtained have shown that this proposal while guaranteeing a high degree of safety, has a negligible impact on the vehicle’s performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301689",
    "keywords": [
      "Aerospace engineering",
      "Alternative medicine",
      "Automotive industry",
      "Biology",
      "Block (permutation group theory)",
      "Computer network",
      "Computer science",
      "Computer security",
      "Context (archaeology)",
      "Embedded system",
      "Engineering",
      "Geometry",
      "Mathematics",
      "Medicine",
      "Operating system",
      "Paleontology",
      "Pathology",
      "Protocol (science)",
      "Software",
      "The Internet"
    ],
    "authors": [
      {
        "surname": "Castiglione",
        "given_name": "Arcangelo"
      },
      {
        "surname": "Palmieri",
        "given_name": "Francesco"
      },
      {
        "surname": "Colace",
        "given_name": "Francesco"
      },
      {
        "surname": "Lombardi",
        "given_name": "Marco"
      },
      {
        "surname": "Santaniello",
        "given_name": "Domenico"
      },
      {
        "surname": "D’Aniello",
        "given_name": "Giuseppe"
      }
    ]
  },
  {
    "title": "Learning motion representation for real-time spatio-temporal action localization",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107312",
    "abstract": "The current deep learning based spatio-temporal action localization methods that using motion information (predominated is optical flow) obtain the state-of-the-art performance. However, since the optical flow is pre-computed, leading to these methods face two problems – the computational efficiency is low and the whole network is not end-to-end trainable. We propose a novel spatio-temporal action localization approach with an integrated optical flow sub-network to address these two issues. Specifically, our designed flow subnet can estimate optical flow efficiently and accurately by using multiple consecutive RGB frames rather than two adjacent frames in a deep network, simultaneously, action localization is implemented in the same network interactive with flow computation end-to-end. To faster the speed, we exploit a neural network based feature fusion method in a pyramid hierarchical manner. It fuses spatial and temporal features at different granularities via combination function (i.e. concatenation) and point-wise convolution to obtain multiscale spatio-temporal action features. Experimental results on three publicly available datasets, e.g. UCF101-24, JHMDB and AVA show that with both RGB appearance and optical flow cues, the proposed method gets the state-of-the-art performance in both efficiency and accuracy. Noticeably, it gets a significant improvement on efficiency. Compared to the currently most efficient method, it is 1.9 times faster in the running speed and 1.3% video-mAP more accurate on the UCF101-24. Our proposed method reaches real-time computation for the first time (up to 38 FPS).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301163",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computation",
      "Computer science",
      "Computer vision",
      "Concatenation (mathematics)",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Geometry",
      "Image (mathematics)",
      "Law",
      "Linguistics",
      "Mathematics",
      "Optical flow",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Pyramid (geometry)",
      "RGB color model",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Dejun"
      },
      {
        "surname": "He",
        "given_name": "Linchao"
      },
      {
        "surname": "Tu",
        "given_name": "Zhigang"
      },
      {
        "surname": "Zhang",
        "given_name": "Shifu"
      },
      {
        "surname": "Han",
        "given_name": "Fei"
      },
      {
        "surname": "Yang",
        "given_name": "Boxiong"
      }
    ]
  },
  {
    "title": "New fractional-order Legendre-Fourier moments for pattern recognition applications",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107324",
    "abstract": "Orthogonal moments enable computer-based systems to discriminate between similar objects. Mathematicians proved that the orthogonal polynomials of fractional-orders outperformed their corresponding counterparts in representing the fine details of a given function. In this work, novel orthogonal fractional-order Legendre-Fourier moments are proposed for pattern recognition applications. The basis functions of these moments are defined and the essential mathematical equations for the recurrence relations, orthogonality and the similarity transformations (rotation and scaling) are derived. The proposed new fractional-order moments are tested where their performance is compared with the existing orthogonal quaternion, multi-channel and fractional moments. New descriptors were found to be superior to the existing ones in terms of accuracy, stability, noise resistance, invariance to similarity transformations, recognition rates and computational times.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301278",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Classical mechanics",
      "Computer science",
      "Discrete wavelet transform",
      "Fourier transform",
      "Geometry",
      "Image (mathematics)",
      "Legendre polynomials",
      "Legendre wavelet",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Moment (physics)",
      "Optics",
      "Orthogonal basis",
      "Orthogonal functions",
      "Orthogonal polynomials",
      "Orthogonality",
      "Physics",
      "Quantum mechanics",
      "Quaternion",
      "Similarity (geometry)",
      "Stability (learning theory)",
      "Wavefront",
      "Wavelet",
      "Wavelet transform",
      "Zernike polynomials"
    ],
    "authors": [
      {
        "surname": "Hosny",
        "given_name": "Khalid M"
      },
      {
        "surname": "Darwish",
        "given_name": "Mohamed M"
      },
      {
        "surname": "Aboelenen",
        "given_name": "Tarek"
      }
    ]
  },
  {
    "title": "Zero shot learning based on class visual prototypes and semantic consistency",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.04.029",
    "abstract": "Zero shot classification is to recognize unseen images which are not present in training set, which is a quite difficult task. Traditional zero shot classification methods ignore the semantic inconsistencies between visual and semantic spaces which make those methods less effective. Projecting semantic representations to visual space can alleviate hubness problem. However, directly utilizing the semantic to visual mapping function learnt by seen classes to unseen classes will lead to domain shift problem. We propose a zero shot learning method which learns visual prototypes and preserves semantic consistency across visual and semantic spaces simultaneously, to handle semantic inconsistency problem and domain shift problem. The semantic consistency is represented by a shared sparse graph of visual space and semantic space. Our key insight is that the visual prototypes learning and the sparse graph learning are unified into a single process. Extensive experiments demonstrate that the results by the proposed method could be boosted significantly.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301513",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Class (philosophy)",
      "Computer science",
      "Consistency (knowledge bases)",
      "Graph",
      "Machine learning",
      "Natural language processing",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Perception",
      "Semantic Web",
      "Semantic compression",
      "Semantic computing",
      "Semantic technology",
      "Theoretical computer science",
      "Visual space"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xiao"
      },
      {
        "surname": "Fang",
        "given_name": "Min"
      },
      {
        "surname": "Li",
        "given_name": "Haikun"
      },
      {
        "surname": "Wu",
        "given_name": "Jinqiao"
      }
    ]
  },
  {
    "title": "Image fusion for stabilized medical video sequence using multimodal parametric registration",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.04.001",
    "abstract": "Medical imaging is a boon to mankind. Processing of medical images is extensively blooming with the aid of technological advancements. Medical images taken from different modalities are stabilized to remove the jittery artifacts and further processed for fusing the images to enable the physicians to visualize the combined features of CT and MRI. Alignment of both the images is performed to ensure parametric registration so as to effectively blend and overlay for proper diagnosis. To assure the quality of the images, basic optimization is implemented and the subjective visualization and objective evaluation are performed. The proposed approach, image fusion with stabilization and registration outperforms the existing techniques in terms of subjective and objective evaluation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301215",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Image fusion",
      "Image processing",
      "Image registration",
      "Mathematics",
      "Medical imaging",
      "Overlay",
      "Parametric statistics",
      "Programming language",
      "Statistics",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Dolly",
        "given_name": "D Raveena Judie"
      },
      {
        "surname": "Peter",
        "given_name": "J Dinesh"
      },
      {
        "surname": "Josemin Bala",
        "given_name": "G."
      },
      {
        "surname": "Jagannath",
        "given_name": "D J"
      }
    ]
  },
  {
    "title": "Image decomposition by bidimensional ensemble patch transform",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.03.029",
    "abstract": "This paper proposes a new algorithm for image decomposition, termed bidimensional ensemble patch transform that adopts a patch process and an ensemble process. The proposed method is based on a nontrivial two-dimensional extension of the one-dimensional ensemble patch transform. It is required to have a successful recognition of local characteristics of the image. The proposed method is capable of efficiently decomposing an image into two-dimensional intrinsic mode functions considering spatial patterns and yields decomposition results that are robust to random fluctuations of the image such as noises. Furthermore, the proposed method is computationally easy to implement.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301136",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Decomposition",
      "Image (mathematics)",
      "Organic chemistry",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Oh",
        "given_name": "Hee-Seok"
      },
      {
        "surname": "Kim",
        "given_name": "Donghoh"
      }
    ]
  },
  {
    "title": "Training bidirectional generative adversarial networks with hints",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107320",
    "abstract": "The generative adversarial network (GAN) is composed of a generator and a discriminator where the generator is trained to transform random latent vectors to valid samples from a distribution and the discriminator is trained to separate such “fake” examples from true examples of the distribution, which in turn forces the generator to generate better fakes. The bidirectional GAN (BiGAN) also has an encoder working in the inverse direction of the generator to produce the latent space vector for a given example. This added encoder allows defining auxiliary reconstruction losses as hints for a better generator. On five widely-used data sets, we showed that BiGANs trained with the Wasserstein loss and augmented with hints learn better generators in terms of image generation quality and diversity, as measured numerically by the 1-nearest neighbor test, Fréchet inception distance, and reconstruction error, and qualitatively by visually analyzing the generated samples.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301230",
    "keywords": [
      "Adversarial system",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Detector",
      "Discriminator",
      "Encoder",
      "Generative grammar",
      "Generator (circuit theory)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Mutlu",
        "given_name": "Uras"
      },
      {
        "surname": "Alpaydın",
        "given_name": "Ethem"
      }
    ]
  },
  {
    "title": "Weighted sigmoid gate unit for an activation function of deep neural network",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.05.017",
    "abstract": "An activation function has a crucial role in a deep neural network. A simple rectified linear unit (ReLU) is widely used for the activation function. In this paper, a weighted sigmoid gate unit (WiG) is proposed as the activation function. The proposed WiG consists of a multiplication of inputs and the weighted sigmoid gate. It is shown that the WiG includes the ReLU and the same activation functions as a special case. Many activation functions have been proposed to overcome the ReLU. In the literature, the performance is mainly evaluated with an object recognition task. The proposed WiG is evaluated with the object recognition task and the image restoration task. Then, the experimental comparisons demonstrate the proposed WiG overcomes the existing activation functions including the ReLU.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518307773",
    "keywords": [
      "Activation function",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Deep neural networks",
      "Engineering",
      "Evolutionary biology",
      "Function (biology)",
      "Mathematics",
      "Mathematics education",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Sigmoid function",
      "Systems engineering",
      "Task (project management)",
      "Unit (ring theory)"
    ],
    "authors": [
      {
        "surname": "Tanaka",
        "given_name": "Masayuki"
      }
    ]
  },
  {
    "title": "A novel PCA-based approach for building on-board sensor classifiers for water contaminant detection",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.05.015",
    "abstract": "Water pollution causes an ever-increasing number of diseases and represents a worldwide concern, both for governments and researchers, as well as public opinion. This pollution also regards drinkable water, with two billion people plagued by this problem. Therefore, it is crucial to find reliable and low-cost technologies for a continuous and diffused monitoring of water. In this paper, we present a novel approach that allows the detection of water contaminants by using an ad-hoc classification system that can be implemented aboard low-cost sensors. To this aim, we first project the input data from the sensors into a 3-D space by using the PCA algorithm, then we use an ad-hoc devised classifier to distinguish the contaminants in the transformed space. We used an evolutionary algorithm to learn the parameters of the classifiers. The experiments were performed on a large dataset containing data from four contaminants, with the phosphoric and sulphuric acids, among the others. The results obtained confirm the effectiveness of the proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301872",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Ecology",
      "Machine learning",
      "Pollution"
    ],
    "authors": [
      {
        "surname": "De Stefano",
        "given_name": "Claudio"
      },
      {
        "surname": "Ferrigno",
        "given_name": "Luigi"
      },
      {
        "surname": "Fontanella",
        "given_name": "Francesco"
      },
      {
        "surname": "Gerevini",
        "given_name": "Luca"
      },
      {
        "surname": "Scotto di Freca",
        "given_name": "Alessandra"
      }
    ]
  },
  {
    "title": "Two-stream collaborative network for multi-label chest X-ray Image classification with lung segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.04.016",
    "abstract": "Automated chest X-ray (CXR) image analysis is often subject to serious disruption and misguided by its imaging artifacts and noise regions. To minimize such negative effects, many state-of-the-art works have made great efforts in the precise segmentation of lung fields. Based on these works, some image-based features can be extracted directly from lung filed to provide clues for many types of lung diseases such as lung nodule, cardiomegaly, pneumothorax, or emphysema. In this paper, we propose a novel two-stream collaborative network call TSCN for multi-label CXR image classification based on lung segmentation. Specifically, we first train a robust lung segmentor with U-Net and apply it to capture the lung filed from the original CXR image. Then we perform a two-stream feature fusion operation to aggregate the contextual information in both the global image and lung field for complementary feature learning. By taking advantage of the two-stream deep structures and two types of image inputs, a novel self-adaptive weighted fusion scheme is designed to jointly learn these two feature streams in the end-to-end training phase, in order to realize adaptive two-stream feature subsets selection and optimization. Extensive experiments on the ChestX-ray14 dataset demonstrate the effectiveness of the proposed method as compared with the state-of-the-art baselines.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301380",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Linguistics",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Bingzhi"
      },
      {
        "surname": "Zhang",
        "given_name": "Zheng"
      },
      {
        "surname": "Lin",
        "given_name": "Jianyong"
      },
      {
        "surname": "Chen",
        "given_name": "Yi"
      },
      {
        "surname": "Lu",
        "given_name": "Guangming"
      }
    ]
  },
  {
    "title": "Rubik Gaussian-based patterns for dynamic texture classification",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.04.007",
    "abstract": "Illumination, noise, and changes of environments, scales negatively impact on encoding chaotic motions for dynamic texture (DT) representation. This paper proposes a new method to overcome those issues by addressing the following novel concepts. First, different Gaussian-based kernels are taken into account as an effective filtered pre-processing with low computational cost to point out robust and invariant features. Second, a discriminative operator, named Local Rubik-based Pattern (LRP), is introduced to adequately capture both shape and motion cues of DTs by proposing a new concept of complemented components together with an effective encoding method. In addition, it also addresses a novel thresholding to take into account rich spatio-temporal relationships extracted from a new model of neighborhood supporting region. Finally, an efficient framework for DT description is presented by exploiting operator LRP for encoding various instances of Gaussian-based volumes in order to form a robust descriptor against noise, changes of illumination, scale, and environment. Experiments for DT classification on benchmark datasets have authenticated the interest of our proposal.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301276",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chaotic",
      "Chemistry",
      "Computer science",
      "Discriminative model",
      "Encoding (memory)",
      "Gaussian",
      "Gene",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Invariant (physics)",
      "Law",
      "Mathematical physics",
      "Mathematics",
      "Noise (video)",
      "Operator (biology)",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Representation (politics)",
      "Repressor",
      "Thresholding",
      "Transcription factor"
    ],
    "authors": [
      {
        "surname": "Nguyen",
        "given_name": "Thanh Tuan"
      },
      {
        "surname": "Nguyen",
        "given_name": "Thanh Phuong"
      },
      {
        "surname": "Bouchara",
        "given_name": "Frédéric"
      }
    ]
  },
  {
    "title": "Simplifying TugGraph using zipping algorithms",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107257",
    "abstract": "Graphs are an invaluable modelling tool in many domains, but visualising large graphs in their entirety can be difficult. Hierarchical graph visualisation – recursively clustering a graph’s nodes to view it at a higher level of abstraction – has thus become popular. However, this can hide important information that a user needs to understand a graph’s topology, e.g. nodes’ neighbourhoods. TugGraph addressed this by ‘separating out’ a given node’s neighbours from their hierarchy ancestors to visualise them independently. Its original implementation was straightforward, but copied parts of the hierarchy, making it slow and memory-hungry. An optimised later version, which we refer to as FastTug, avoided this, but at a cost in clarity. Optimising TugGraph without sacrificing clarity is difficult because of the need to keep every hierarchy node connected, a common challenge for graph hierarchy editing algorithms. Recently, this problem has been addressed by ‘zipping’ algorithms, multi-level split/merge algorithms that preserve hierarchy node connectedness and can be built upon for higher-level editing. In this paper, we generalise the original unzipping algorithms to implement SimpleTug, a simple, modular version of TugGraph that is easy to understand and implement, and even faster and more memory-efficient than FastTug. We formally prove its equivalence to FastTug, and show how both can be parallelised. Using our millipede hierarchical image segmentation system, we show experimentally that both the serial and parallel versions of SimpleTug are around 25% faster than their FastTug counterparts, whilst using considerably less memory. Finally, we discuss the interesting theoretical connections between TugGraph and zipping, and suggest ideas for further work.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300625",
    "keywords": [
      "Algorithm",
      "Biochemistry",
      "CLARITY",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Economics",
      "Graph",
      "Graph drawing",
      "Hierarchy",
      "Market economy",
      "Modular design",
      "Programming language",
      "Psychology",
      "Psychotherapist",
      "Social connectedness",
      "Theoretical computer science",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Golodetz",
        "given_name": "S."
      },
      {
        "surname": "Arnab",
        "given_name": "A."
      },
      {
        "surname": "Voiculescu",
        "given_name": "I.D."
      },
      {
        "surname": "Cameron",
        "given_name": "S.A."
      }
    ]
  },
  {
    "title": "Real time human action recognition using triggered frame extraction and a typical CNN heuristic",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.04.031",
    "abstract": "Recognition of human actions with optimum accuracy and with less computational overhead has always been a challenging task. A suitable framework towards this problem can provide a robust solution for numerous domains of expertise that demands automatic surveillance. In this paper, a two-fold framework has been proposed for the frame extraction and recognition of human action therein from a video input. A Fuzzy inferencing technique has been suitably used for the purpose of video frame extraction in a smarter way. Frames are extracted from a video only when an event of action is initiated and about to commit. By this the additional computational load of continuous frame extraction is eased efficiently. Subsequent to this process, we also introduce a typical convolutional neural network (CNN) that is used for the purpose of human action recognition. This typical CNN has been designed so as to suit the nature of input that is supplied to it. Experimental evaluation has been performed on the standard HMDB51 database. A total of eight distinct and prominent actions which are mostly common in daily routine of human life are considered for the purpose. Performance comparison of the proposed scheme with other state of the art schemes is also performed with three different measures. Evaluation results for the proposed scheme indicates overall rate of accuracies of 96.5% and 98% for frame extraction and action recognition respectively. The proposed scheme outperformed the others in most of the measuring aspects.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301537",
    "keywords": [
      "Action (physics)",
      "Artificial intelligence",
      "Commit",
      "Computer science",
      "Convolutional neural network",
      "Database",
      "Economics",
      "Frame (networking)",
      "Frame rate",
      "Heuristic",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Overhead (engineering)",
      "Pattern recognition (psychology)",
      "Physics",
      "Process (computing)",
      "Quantum mechanics",
      "Scheme (mathematics)",
      "Task (project management)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Mishra",
        "given_name": "Soumya Ranjan"
      },
      {
        "surname": "Mishra",
        "given_name": "Tusar Kanti"
      },
      {
        "surname": "Sanyal",
        "given_name": "Goutam"
      },
      {
        "surname": "Sarkar",
        "given_name": "Anirban"
      },
      {
        "surname": "Satapathy",
        "given_name": "Suresh Chandra"
      }
    ]
  },
  {
    "title": "Shallow2Deep: Indoor scene modeling by single image understanding",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107271",
    "abstract": "Dense indoor scene modeling from 2D images has been bottlenecked due to the absence of depth information and cluttered occlusions. We present an automatic indoor scene modeling approach using deep features from neural networks. Given a single RGB image, our method simultaneously recovers semantic contents, 3D geometry and object relationship by reasoning indoor environment context. Particularly, we design a shallow-to-deep architecture on the basis of convolutional networks for semantic scene understanding and modeling. It involves multi-level convolutional networks to parse indoor semantics/geometry into non-relational and relational knowledge. Non-relational knowledge extracted from shallow-end networks (e.g. room layout, object geometry) is fed forward into deeper levels to parse relational semantics (e.g. support relationship). A Relation Network is proposed to infer the support relationship between objects. All the structured semantics and geometry above are assembled to guide a global optimization for 3D scene modeling. Qualitative and quantitative analysis demonstrates the feasibility of our method in understanding and modeling semantics-enriched indoor scenes by evaluating the performance of reconstruction accuracy, computation performance and scene complexity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300765",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Convolutional neural network",
      "Data mining",
      "Deep learning",
      "Object (grammar)",
      "Paleontology",
      "Parsing",
      "Programming language",
      "RGB color model",
      "Relation (database)",
      "Semantics (computer science)"
    ],
    "authors": [
      {
        "surname": "Nie",
        "given_name": "Yinyu"
      },
      {
        "surname": "Guo",
        "given_name": "Shihui"
      },
      {
        "surname": "Chang",
        "given_name": "Jian"
      },
      {
        "surname": "Han",
        "given_name": "Xiaoguang"
      },
      {
        "surname": "Huang",
        "given_name": "Jiahui"
      },
      {
        "surname": "Hu",
        "given_name": "Shi-Min"
      },
      {
        "surname": "Zhang",
        "given_name": "Jian Jun"
      }
    ]
  },
  {
    "title": "Fusion of complex networks and randomized neural networks for texture analysis",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107189",
    "abstract": "This paper presents a high discriminative texture analysis method based on the fusion of complex networks and randomized neural networks. In this approach, the input image is modeled as a complex network and its topological properties as well as the image pixels are used to train randomized neural networks to create a signature that represents the deep characteristics of the texture. The results obtained surpassed the accuracy of many methods available in the literature. This performance demonstrates that our proposed approach opens a promising source of research, which consists of exploring the synergy of neural networks and complex networks in the texture analysis field.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304893",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Deep neural networks",
      "Discriminative model",
      "Field (mathematics)",
      "Fusion",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Pure mathematics",
      "Signature (topology)",
      "Texture (cosmology)"
    ],
    "authors": [
      {
        "surname": "Ribas",
        "given_name": "Lucas C."
      },
      {
        "surname": "Sá Junior",
        "given_name": "Jarbas Joaci de Mesquita"
      },
      {
        "surname": "Scabini",
        "given_name": "Leonardo F. S."
      },
      {
        "surname": "Bruno",
        "given_name": "Odemir M."
      }
    ]
  },
  {
    "title": "Peeling off image layers on topographic architectures",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.04.023",
    "abstract": "Information patterns are ubiquitous and their automatic detection is fundamental in many computer vision tasks. Robust and fast detection of object candidates is essential as well as the ability to adapt the system to new situations. We propose a general, task independent method that i) locates interesting patterns on binary images by creating a hierarchical layered structure based on neighborhood topography of connected components, and ii) identifies object groups applying saliency principles. Saliency values are assigned to both object groups and individuals, that can be referred as a priority queue of image regions, or alternatively a proposal for blob processing order. Empirical analysis through several computer vision examples is provided including applications for blind and visually impaired people. The proposed algorithm can be efficiently implemented on processor arrays, since it mostly contains standard topographic instructions, and we also introduce a real-time implementation on the Eye-RIS/Toshiba SPS vision system.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301458",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Cognitive neuroscience of visual object recognition",
      "Computer science",
      "Computer vision",
      "Economics",
      "Image (mathematics)",
      "Image processing",
      "Machine vision",
      "Management",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Programming language",
      "Queue",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Radvanyi",
        "given_name": "Mihaly"
      },
      {
        "surname": "Karacs",
        "given_name": "Kristof"
      }
    ]
  },
  {
    "title": "Automatic characteristic-calibrated registration (ACC-REG): Hippocampal surface registration using eigen-graphs",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107142",
    "abstract": "In this paper, we propose an efficient algorithm, the ACC-REG, to automatically extract intrinsic key characteristics on hippocampal mesh surfaces and hence compute an accurate registration mapping between them. Given a pair of hippocampal surface mesh, the proposed algorithm constructs the eigen-graphs, an intrinsic feature on the surface, on each surface as its representative. The eigen-graphs are then calibrated along the longitudinal direction of the hippocampal surfaces. Accurately corresponded intrinsic characteristics on each hippocampus can thus be extracted. As a result, the two surfaces can be registered with improved accuracy and low computation cost. Experiments on ADNI data demonstrate the effectiveness of the proposed ACC-REG model over existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304431",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computation",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Eigenvalues and eigenvectors",
      "Feature (linguistics)",
      "Geometry",
      "Hippocampal formation",
      "Key (lock)",
      "Linguistics",
      "Mathematics",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Surface (topology)"
    ],
    "authors": [
      {
        "surname": "CHAN",
        "given_name": "Hei Long"
      },
      {
        "surname": "YAM",
        "given_name": "Tsz Chun"
      },
      {
        "surname": "LUI",
        "given_name": "Lok Ming"
      }
    ]
  },
  {
    "title": "Shape-from-focus reconstruction using nonlocal matting Laplacian prior followed by MRF-based refinement",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107302",
    "abstract": "In this paper, we address the problem of depth recovery from a sequence of multi-focus images, known as shape-from-focus (SFF). The conventional SFF techniques typically exhibit poor performance over textureless regions, and it is difficult to preserve depth edges and fine details while maintaining spatial consistency. Therefore, we propose an SFF depth recovery framework composed of depth reconstruction and refinement processes. We first formulate the depth reconstruction as a maximum a posterior (MAP) estimation problem with the inclusion of matting Laplacian prior. The nonlocal principle is adopted in matting Laplacian matrix construction to preserve depth edges and fine details. As the nonlocal principle breaks the spatial consistency, the reconstructed depth image is spatially inconsistent and suffers from the texture-copy artifacts. To smooth the noise and suppress the texture-copy artifacts, a closed-form edge-preserving depth refinement is proposed, which is formulated as a MAP estimation problem using Markov random fields (MRFs). Experimental results over synthetic and real scene datasets demonstrate the superiority of our algorithm in terms of robustness, and the ability to preserve edges and fine details while maintaining spatial consistency compared to existing approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301060",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Depth map",
      "Focus (optics)",
      "Gene",
      "Image (mathematics)",
      "Laplace operator",
      "Laplacian matrix",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Zhiqiang"
      },
      {
        "surname": "Kim",
        "given_name": "Dongjoon"
      },
      {
        "surname": "Shin",
        "given_name": "Yeong-Gil"
      }
    ]
  },
  {
    "title": "Automatic region of interest segmentation for breast thermogram image classification",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.03.025",
    "abstract": "Breast thermography images are a new type of data that has been analyzed in recent years in order to detect abnormalities, which can lead to a future breast cancer. This paper proposes a methodology for breast thermal image classification, which is useful in Computer-Aided Detection Systems. The main contribution is an automatic method to segment the region of interest (ROI) based on local operations, local analysis, interpolation and statistical operators. For our experiments, we used an image database that is widely used in this research area, obtaining accuracy results between 90.17% and 98.33%, which are competitive with respect to related works.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301033",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Image segmentation",
      "Image texture",
      "Infrared",
      "Interpolation (computer graphics)",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Region growing",
      "Region of interest",
      "Segmentation",
      "Thermography"
    ],
    "authors": [
      {
        "surname": "Sánchez-Ruiz",
        "given_name": "Daniel"
      },
      {
        "surname": "Olmos-Pineda",
        "given_name": "Ivan"
      },
      {
        "surname": "Olvera-López",
        "given_name": "J. Arturo"
      }
    ]
  },
  {
    "title": "Novel dimensionality reduction approach for unsupervised learning on small datasets",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107291",
    "abstract": "We focus on an image classification task in which only several unlabeled images per class are available for learning and low computational complexity is required. We recall the state-of-the-art methods that are used to solve the task: autoencoder-based approaches and manifold-decomposition-based approaches. Next, we introduce our proposed method, which is based on a combination of the F-transform and (kernel) principal component analysis. F-transform significantly reduces the computation time of PCA and increases the robustness of PCA to translation, while PCA proposes more descriptive features. This combination performs 3D reduction: the F-transform reduces dimensionality over a single 2D image, while PCA reduces dimensionality through the whole set of processed images. Based on the benchmark results, our method may outperform deep-learning-based methods in limited settings. For completeness, we also address other image resampling algorithms that can be used instead of the F-transform, and we find that the F-transform is the most suitable.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300959",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Deep learning",
      "Dimensionality reduction",
      "Gene",
      "Kernel method",
      "Kernel principal component analysis",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Principal component analysis",
      "Robustness (evolution)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Hurtik",
        "given_name": "Petr"
      },
      {
        "surname": "Molek",
        "given_name": "Vojtech"
      },
      {
        "surname": "Perfilieva",
        "given_name": "Irina"
      }
    ]
  },
  {
    "title": "Trends in IoT based solutions for health care: Moving AI to the edge",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.05.016",
    "abstract": "In recent times, we assist to an ever growing diffusion of smart medical sensors and Internet of things devices that are heavily changing the way healthcare is approached worldwide. In this context, a combination of Cloud and IoT architectures is often exploited to make smart healthcare systems capable of supporting near realtime applications when processing and performing Artificial Intelligence on the huge amount of data produced by wearable sensor networks. Anyway, the response time and the availability of cloud based systems, together with security and privacy, still represent critical issues that prevents Internet of Medical Things (IoMT) devices and architectures from being a reliable and effective solution to the aim. Lately, there is a growing interest towards architectures and approaches that exploit Edge and Fog computing as an answer to compensate the weaknesses of the cloud. In this paper, we propose a short review about the general use of IoT solutions in health care, starting from early health monitoring solutions from wearable sensors up to a discussion about the latest trends in fog/edge computing for smart health.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301884",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Cloud computing",
      "Computer science",
      "Computer security",
      "Context (archaeology)",
      "Data science",
      "Economic growth",
      "Economics",
      "Edge computing",
      "Edge device",
      "Embedded system",
      "Enhanced Data Rates for GSM Evolution",
      "Exploit",
      "Health care",
      "Internet of Things",
      "Operating system",
      "Paleontology",
      "Smartwatch",
      "Wearable computer",
      "Wearable technology"
    ],
    "authors": [
      {
        "surname": "Greco",
        "given_name": "Luca"
      },
      {
        "surname": "Percannella",
        "given_name": "Gennaro"
      },
      {
        "surname": "Ritrovato",
        "given_name": "Pierluigi"
      },
      {
        "surname": "Tortorella",
        "given_name": "Francesco"
      },
      {
        "surname": "Vento",
        "given_name": "Mario"
      }
    ]
  },
  {
    "title": "Improved covariant local feature detector",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.03.027",
    "abstract": "Local feature detection is a fundamental problem in computer vision. Recently, the research of local feature detection has been switched from handcrafted methods to learning based ones, especially deep learning based ones. A recent successful deep learning based feature detector is the covariant local feature detector that conducts keypoint detection by predicting the transformation of keypoints from nearby pixels. Although this method adopts a new detection framework compared to those methods by computing the keypoint’s likelihood, it treats each pixel equally which may incorrectly detect unstable keypoints. On the other hand, other methods computing the keypoint probability could capture different evidence for keypoint detection as well as provide a natural weight for each prediction in the covariant detector. So, fusing information from other detectors into the covariant detector could improve its performance. Under this motivation, this paper proposes an improved covariant local feature detector by fusing feature information obtained from another detector, which is served as a confidence to guide the voting procedure when converting the predicted transformations into a meaningful score map for keypoint detection. In this way, the fused information can enhance the features that are considered to be good and weaken those unstable features. The proposed method is evaluated on four widely used benchmarks and consistent performance improvement over previous works is observed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301057",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Covariant transformation",
      "Detector",
      "Feature (linguistics)",
      "Feature extraction",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Huo",
        "given_name": "Zhanqiang"
      },
      {
        "surname": "Zhang",
        "given_name": "Yanqi"
      },
      {
        "surname": "Liu",
        "given_name": "Hongmin"
      },
      {
        "surname": "Wang",
        "given_name": "Jing"
      },
      {
        "surname": "Liu",
        "given_name": "Xin"
      },
      {
        "surname": "Zhang",
        "given_name": "Jiyong"
      }
    ]
  },
  {
    "title": "Online tracking of ants based on deep association metrics: method, dataset and evaluation",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107233",
    "abstract": "Tracking movement of insects in a social group (such as ants) is challenging, because the individuals are not only similar in appearance but also likely to perform intensive body contact and sudden movement adjustment (start/stop, direction changes). To address this challenge, we introduce an online multi-object tracking framework that combines both the motion and appearance information of ants. We obtain the appearance descriptors by using the ResNet model for offline training on a small (N=50) sample dataset. For online association, a cosine similarity metric computes the matching degree between historical appearance sequences of the trajectory and the current detection. We validate our method in both indoor (lab setup) and outdoor video sequences. The results show that our model obtains 99.3% ± 0.5% MOTA and 91.9% ± 2.1% MOTP across 24,050 testing samples in five indoor sequences, with real-time tracking performance. In an outdoor sequence, we achieve 99.3% MOTA and 92.9% MOTP across 22,041 testing samples. The datasets and code are made publicly available for future research in relevant domains.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030039X",
    "keywords": [
      "Artificial intelligence",
      "Association (psychology)",
      "Astronomy",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Computer vision",
      "Cosine similarity",
      "Data association",
      "Data mining",
      "Economics",
      "Epistemology",
      "Image (mathematics)",
      "Matching (statistics)",
      "Mathematics",
      "Metric (unit)",
      "Object (grammar)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Philosophy",
      "Physics",
      "Probabilistic logic",
      "Psychology",
      "Sample (material)",
      "Similarity (geometry)",
      "Statistics",
      "Tracking (education)",
      "Trajectory",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Cao",
        "given_name": "Xiaoyan"
      },
      {
        "surname": "Guo",
        "given_name": "Shihui"
      },
      {
        "surname": "Lin",
        "given_name": "Juncong"
      },
      {
        "surname": "Zhang",
        "given_name": "Wenshu"
      },
      {
        "surname": "Liao",
        "given_name": "Minghong"
      }
    ]
  },
  {
    "title": "Radical analysis network for learning hierarchies of Chinese characters",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107305",
    "abstract": "Chinese characters have a valuable property, this is, numerous Chinese characters are composed of a compact set of fundamental and structural radicals. This paper introduces a radical analysis network (RAN) that makes full use of this valuable property to implement radical-based Chinese character recognition. The proposed RAN employs an attention mechanism to extract radicals from Chinese characters and to detect spatial structures among the radicals. Then, the decoder in RAN generates a hierarchical composition of Chinese characters based on the knowledge of the extracted radicals and their internal structures. The method of treating a Chinese character as a composition of radicals rather than as a single character category is a human-like method that can reduce the size of the vocabulary, ignore redundant information among similar characters and enable the system to recognize unseen Chinese character categories, i.e., zero-shot learning. Through experiments, we assess the practicality of RAN for recognizing Chinese characters in natural scenes. Furthermore, a RAN framework can be proposed for scene text recognition with the extension of a dense recurrent neural network (denseRNN) encoder, a multihead coverage attention model and HSV representations. The proposed approach achieved the best performance in the ICPR MTWI 2018 competition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301096",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Character (mathematics)",
      "Chinese characters",
      "Computer network",
      "Computer science",
      "Deep learning",
      "Encoder",
      "Epistemology",
      "Geometry",
      "Mathematics",
      "Natural language processing",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Property (philosophy)",
      "Ran",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Jianshu"
      },
      {
        "surname": "Du",
        "given_name": "Jun"
      },
      {
        "surname": "Dai",
        "given_name": "Lirong"
      }
    ]
  },
  {
    "title": "Automatic soccer field of play registration",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107278",
    "abstract": "This paper proposes a strategy for the automatic registration of soccer images on a model of the field of play. First, a robust and efficient preprocessing is applied to discard the areas of the image that do not belong to the field of play and eliminate most edge points that are not part of the line marks. Then, a novel probabilistic decision tree is used to identify the most probable classification for the set of all the straight lines in the image. Additionally, the line surrounding the center circle is also modeled for providing results when only few straight lines are visible. Finally, a three-step analysis stage is applied to ensure the validity of the results. To assess its quality, the strategy has been tested on several sequences corresponding to three stadiums with different characteristics. The results obtained have shown that the registration is successful in most images (94%).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300832",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Enhanced Data Rates for GSM Evolution",
      "Field (mathematics)",
      "Geometry",
      "Image (mathematics)",
      "Image registration",
      "Line (geometry)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Preprocessor",
      "Probabilistic logic",
      "Programming language",
      "Pure mathematics",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Cuevas",
        "given_name": "Carlos"
      },
      {
        "surname": "Quilón",
        "given_name": "Daniel"
      },
      {
        "surname": "García",
        "given_name": "Narciso"
      }
    ]
  },
  {
    "title": "A survey on image and video cosegmentation: Methods, challenges and analyses",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107297",
    "abstract": "Image and video cosegmentation is a newly emerging and rapidly progressing area, which aims at delineating common objects at pixel-level from a group of images or a set of videos. Plenty of related works have been published and implemented in varied applications, but there lacks a systematic survey on both image and video cosegmentation. This paper provides a comprehensive overview including the existing methods, applications, and challenges. Specifically, different cosegmentation problem settings are described, the formulation details of the methods are summarized and their potential applications are listed. Moreover, the benchmark datasets and standard evaluation metrics are also given; and the future directions and unsolved challenges are discussed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301011",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Geography",
      "Image (mathematics)",
      "Information retrieval",
      "Pixel",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Ren",
        "given_name": "Yan"
      },
      {
        "surname": "Kong",
        "given_name": "Adams Wai Kin"
      },
      {
        "surname": "Jiao",
        "given_name": "Licheng"
      }
    ]
  },
  {
    "title": "Mean oriented Riesz features for micro expression classification",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.05.008",
    "abstract": "Micro-expressions are brief and subtle facial expressions that go on and off the face in a fraction of a second. This kind of facial expressions usually occurs in high stake situations and is considered to reflect a human’s real intent. There has been some interest in micro-expression analysis, however, a great majority of the methods are based on classically established computer vision methods such as local binary patterns, histogram of gradients and optical flow. A novel methodology for micro-expression recognition using the Riesz pyramid, a multi-scale steerable Hilbert transform is presented. In fact, an image sequence is transformed with this tool, then the image phase variations are extracted and filtered as proxies for motion. Furthermore, the dominant orientation constancy from the Riesz transform is exploited to average the micro-expression sequence into an image pair. Based on that, the Mean Oriented Riesz Feature description is introduced. Finally the performance of our methods are tested in two spontaneous micro-expressions databases and compared to state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301781",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Expression (computer science)",
      "Face (sociological concept)",
      "Facial expression",
      "Feature (linguistics)",
      "Filter (signal processing)",
      "Genetics",
      "Geometry",
      "Hilbert–Huang transform",
      "Histogram",
      "Image (mathematics)",
      "Linguistics",
      "Local binary patterns",
      "Mathematics",
      "Orientation (vector space)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Pure mathematics",
      "Pyramid (geometry)",
      "Riesz transform",
      "Sequence (biology)"
    ],
    "authors": [
      {
        "surname": "Duque",
        "given_name": "Carlos Arango"
      },
      {
        "surname": "Alata",
        "given_name": "Olivier"
      },
      {
        "surname": "Emonet",
        "given_name": "Rémi"
      },
      {
        "surname": "Konik",
        "given_name": "Hubert"
      },
      {
        "surname": "Legrand",
        "given_name": "Anne-Claire"
      }
    ]
  },
  {
    "title": "Unsupervised hashing based on the recovery of subspace structures",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107261",
    "abstract": "Unsupervised semantic hashing should in principle keep the semantics among samples consistent with the intrinsic geometric structures of the dataset. In this paper, we propose a novel multiple stage unsupervised hashing method, named “Unsupervised Hashing based on the Recovery of Subspace Structures” (RSSH) for image retrieval. Specifically, we firstly adapt the Low-rank Representation (LRR) model into a new variant which treats the real-world data as samples drawn from a union of several low-rank subspaces. Then, the pairwise similarities are represented in a space-and-time saving manner based on the learned low-rank correlation matrix of the modified LRR. Next, the challenging discrete graph hashing is employed for binary hashing codes. Notably, we convert the original graph hashing model into an optimization-friendly formalization, which is addressed with efficient closed-form solutions for its subproblems. Finally, the devised linear hash functions are fast achieved for out-of-samples. Retrieval experiments on four image datasets testify the superiority of RSSH to several state-of-the-art hashing models. Besides, it’s worth mentioning that RSSH, a shallow model, significantly outperforms two recently proposed unsupervised deep hashing methods, which further confirms its effectiveness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300662",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary code",
      "Binary number",
      "Computer science",
      "Computer security",
      "Double hashing",
      "Dynamic perfect hashing",
      "Feature hashing",
      "Geometry",
      "Graph",
      "Hash function",
      "Hash table",
      "Image (mathematics)",
      "Image retrieval",
      "K-independent hashing",
      "Linear subspace",
      "Locality-sensitive hashing",
      "Mathematics",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Subspace topology",
      "Theoretical computer science",
      "Universal hashing"
    ],
    "authors": [
      {
        "surname": "Tian",
        "given_name": "Zhibao"
      },
      {
        "surname": "Zhang",
        "given_name": "Hui"
      },
      {
        "surname": "Chen",
        "given_name": "Yong"
      },
      {
        "surname": "Zhang",
        "given_name": "Dell"
      }
    ]
  },
  {
    "title": "Hyper-parameter optimization in classification: To-do or not-to-do",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107245",
    "abstract": "Hyper-parameter optimization is a process to find suitable hyper-parameters for predictive models. It typically incurs highly demanding computational costs due to the need of the time-consuming model training process to determine the effectiveness of each set of candidate hyper-parameter values. A priori, there is no guarantee that hyper-parameter optimization leads to improved performance. In this work, we propose a framework to address the problem of whether one should apply hyper-parameter optimization or use the default hyper-parameter settings for traditional classification algorithms. We implemented a prototype of the framework, which we use a basis for a three-fold evaluation with 486 datasets and 4 algorithms. The results indicate that our framework is effective at supporting modeling tasks in avoiding adverse effects of using ineffective optimizations. The results also demonstrate that incrementally adding training datasets improves the predictive performance of framework instantiations and hence enables “life-long learning.”",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300510",
    "keywords": [
      "A priori and a posteriori",
      "Algorithm",
      "Artificial intelligence",
      "Basis (linear algebra)",
      "Computer science",
      "Epistemology",
      "Geometry",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Model parameter",
      "Operating system",
      "Optimization problem",
      "Philosophy",
      "Process (computing)",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Tran",
        "given_name": "Ngoc"
      },
      {
        "surname": "Schneider",
        "given_name": "Jean-Guy"
      },
      {
        "surname": "Weber",
        "given_name": "Ingo"
      },
      {
        "surname": "Qin",
        "given_name": "A.K."
      }
    ]
  },
  {
    "title": "Circular object arrangement using spherical embeddings",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107192",
    "abstract": "We consider the problem of recovering a circular arrangement of data instances with respect to some proximity measure, such that nearby instances are more similar. Applications of this problem, also referred to as circular seriation, can be found in various disciplines such as genome sequencing, data visualization and exploratory data analysis. Circular seriation can be expressed as a quadratic assignment problem, which is in general an intractable problem. Spectral-based approaches can be used to find approximate solutions, but are shown to perform well only for a specific class of data matrices. We propose a bilevel optimization framework where we employ a spherical embedding approach together with a spectral method for circular ordering in order to recover circular arrangements of the embedded data. Experiments on real and synthetic datasets demonstrate the competitive performance of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132031930490X",
    "keywords": [
      "Algorithm",
      "Archaeology",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Data mining",
      "Embedding",
      "Geometry",
      "History",
      "Mathematical optimization",
      "Mathematics",
      "Measure (data warehouse)",
      "Object (grammar)",
      "Quadratic equation",
      "Seriation (archaeology)",
      "Theoretical computer science",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Evangelopoulos",
        "given_name": "Xenophon"
      },
      {
        "surname": "Brockmeier",
        "given_name": "Austin J."
      },
      {
        "surname": "Mu",
        "given_name": "Tingting"
      },
      {
        "surname": "Goulermas",
        "given_name": "John Y."
      }
    ]
  },
  {
    "title": "A new approach for optimal time-series segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.04.006",
    "abstract": "Emerging technologies have led to the creation of huge databases that require reducing their high dimensionality to be analysed. Many suboptimal methods have been proposed for this purpose. On the other hand, few efficient optimal methods have been proposed due to their high computational complexity. However, these methods are necessary to evaluate the performance of suboptimal methods. This paper proposes a new optimal approach, called OSTS, to improve the segmentation of time series. The proposed method is based on A* algorithm and it uses an improved version of the well-known Salotti method for obtaining optimal polygonal approximations. Firstly, a suboptimal method for time-series segmentation is applied to obtain pruning values. In this case, a suboptimal method based on Bottom-Up technique is selected. Then, the results of the suboptimal method are used as pruning values to reduce the computational time of the proposed method. The proposal has been compared to other suboptimal methods and the results have shown that the method is optimal, and, in some cases, the computational time is similar to other suboptimal methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301264",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computational complexity theory",
      "Computer science",
      "Curse of dimensionality",
      "Mathematical optimization",
      "Mathematics",
      "Paleontology",
      "Pruning",
      "Segmentation",
      "Series (stratigraphy)"
    ],
    "authors": [
      {
        "surname": "Carmona-Poyato",
        "given_name": "Ángel"
      },
      {
        "surname": "Fernández-García",
        "given_name": "Nicolás Luis"
      },
      {
        "surname": "Madrid-Cuevas",
        "given_name": "Francisco José"
      },
      {
        "surname": "Durán-Rosal",
        "given_name": "Antonio Manuel"
      }
    ]
  },
  {
    "title": "Attentive multi-stage convolutional neural network for crowd counting",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.05.009",
    "abstract": "Crowd counting is an important problem in computer vision, whose application can be found in a wide range of tasks. Although this problem has been well studied, how to effectively deal with scale variations and perspective distortions is still a challenge. High-quality crowd density map depends heavily on how well these problems are solved. In this paper, we propose a novel network architecture called Attentive Multi-stage CNN for Crowd Counting (AMCNN). The AMCNN contains two subnetworks, i.e., hierarchical density estimator(HDE) and auxiliary count classifier (AUCC). The HDE adopts a hierarchical strategy to mine semantic features in a coarse-to-fine manner to tackle the problem of scale changes and perspective distortions. And the obtained composite features are used to generate the final density map. In addition, to further improve the density map quality, a soft attention mechanism is integrated into the AMCNN to distinct the foreground and the background. Furthermore, the AUCC is employed to achieve the count classification task, which is complementary to the task of density estimation. We evaluate our model on three public datasets: ShanghaiTech, UCF_CC_50 and Mall. Extensive experiments demonstrate that our counting model is on par with some state-of-the-art methods. Source code will be released at:https://github.com/wxq-ahu/crowd-count-amcnn.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301793",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Code (set theory)",
      "Computer science",
      "Convolutional neural network",
      "Density estimation",
      "Economics",
      "Estimator",
      "Management",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Scale (ratio)",
      "Set (abstract data type)",
      "Statistics",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Ming"
      },
      {
        "surname": "Wang",
        "given_name": "Xuqing"
      },
      {
        "surname": "Tang",
        "given_name": "Jun"
      },
      {
        "surname": "Wang",
        "given_name": "Nian"
      },
      {
        "surname": "Qu",
        "given_name": "Lei"
      }
    ]
  },
  {
    "title": "HoPPF: A novel local surface descriptor for 3D object recognition",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107272",
    "abstract": "Three-dimensional feature descriptors play an important role in 3D computer vision because they are widely employed in many 3D perception applications to extract point correspondences between two point clouds. However, most existing description methods suffer from either weak robustness, low descriptiveness, or costly computation. Thus, a 3D local feature descriptor named Histograms of Point Pair Features (HoPPF) is proposed in this paper, and it is aimed at robust representation, high descriptiveness, and efficient computation. First, we propose a novel method to redirect surface normals and use the Poisson-disk sampling strategy to solve the problem of data redundancy in data pre-processing. Second, a new technique is applied to divide the local point pair set of each keypoint into eight regions. Then, the distribution of local point pairs of each region is used to construct the corresponding sub-features. Finally, the proposed HoPPF is generated by concatenating all sub-features into a vector. The performance of the HoPPF method is rigorously evaluated on several standard datasets. The results of the experiments and comparisons with other state-of-the-art methods validate the superiority of the HoPPF descriptor in term of robustness, descriptiveness, and efficiency. Moreover, the proposed technique for division of point pair sets is used to modify the other typical point-pair-based descriptor (i.e., PFH) to show its generalization ability. The proposed HoPPF is also applied to object recognition on real datasets captured by different devices (e.g., Kinect and LiDAR) to verify the feasibility of this method for 3D vision applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300777",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Cognitive neuroscience of visual object recognition",
      "Computation",
      "Computer science",
      "Computer vision",
      "Feature extraction",
      "Feature vector",
      "Gene",
      "Histogram",
      "Image (mathematics)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Point cloud",
      "Redundancy (engineering)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Huan"
      },
      {
        "surname": "Tang",
        "given_name": "Minjie"
      },
      {
        "surname": "Ding",
        "given_name": "Han"
      }
    ]
  },
  {
    "title": "Multi scale mirror connection based encoder decoder network for text localization",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.04.002",
    "abstract": "Encoder decoder models with multi-scale feature concatenations have become ubiquitous for various natural scene segmentation tasks. In the current approach, a similar model with an improved mirror connection from encoders to decoder has been proposed. Three different types of mirror connections, namely, linear, parametric and convolutional, have been demonstrated in the proposed work. We have also implemented the use of internal skips to facilitate better gradient propagation within the encoder-decoder architecture. The proposed model also consists of an ensemble module that combines outputs from models with different kernel sizes, such as, 3 × 3, 5 × 5 and 7 × 7 to combine multi-scale features for efficient detections. The model was tested on the ICDAR 2003, SVT, ICDAR 2015 and the Total-Text dataset where it proved to be superior to other state of the art encoder-decoder architectures for pixel level classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301227",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Connection (principal bundle)",
      "Convolutional neural network",
      "Encoder",
      "Feature (linguistics)",
      "Geometry",
      "Kernel (algebra)",
      "Linguistics",
      "Mathematics",
      "Operating system",
      "Parametric statistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Dutta",
        "given_name": "Kalpita"
      },
      {
        "surname": "Bal",
        "given_name": "Malyaban"
      },
      {
        "surname": "Basak",
        "given_name": "Arpita"
      },
      {
        "surname": "Ghosh",
        "given_name": "Swarnendu"
      },
      {
        "surname": "Das",
        "given_name": "Nibaran"
      },
      {
        "surname": "Kundu",
        "given_name": "Mahantapas"
      },
      {
        "surname": "Nasipuri",
        "given_name": "Mita"
      }
    ]
  },
  {
    "title": "Frobenius correlation based u-shapelets discovery for time series clustering",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107301",
    "abstract": "An u-shapelet is a sub-sequence of a time series used for the clustering of time series datasets. The purpose of this paper is to discover u-shapelets on uncertain time series. To achieve this goal, we propose a dissimilarity score called FOTS whose computation is based on the eigenvector decomposition and the comparison of the autocorrelation matrices of the time series. This score is robust to the presence of uncertainty; it is not very sensitive to transient changes; it allows capturing complex relationships between time series such as oscillations and trends, and it is also well adapted to the comparison of short time series. The FOTS score is used with the Scalable Unsupervised Shapelet Discovery algorithm for the clustering of 63 datasets, and it has shown a substantial improvement in the quality of the clustering with respect to the Rand Index. This work defines a novel framework for the clustering of uncertain time series.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301059",
    "keywords": [
      "Artificial intelligence",
      "Autocorrelation",
      "Biology",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Database",
      "Machine learning",
      "Mathematics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Rand index",
      "Scalability",
      "Series (stratigraphy)",
      "Statistics",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Siyou Fotso",
        "given_name": "Vanel Steve"
      },
      {
        "surname": "Mephu Nguifo",
        "given_name": "Engelbert"
      },
      {
        "surname": "Vaslin",
        "given_name": "Philippe"
      }
    ]
  },
  {
    "title": "Parametric PCA for unsupervised metric learning",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.05.011",
    "abstract": "In pattern recognition, the problem of quantifying a suitable similarity measure between different objects in a collection is a challenging task, especially in cases where the standard Euclidean distance is not a reasonable choice. In this context, dimensionality reduction algorithms are powerful tools for unsupervised metric learning. In this paper, we propose a framework to build dimensionality reduction methods for unsupervised metric learning based on the mapping of local neighborhoods of the KNN graph to a parametric feature space, defined in terms of a statistical model. By incorporating a non-Euclidean metric based on the Bhattacharyya coefficient, we define the parametric kernel matrix, a surrogate for the covariance matrix of the parametric feature vectors. Inspired by PCA, we use the eigenvalues of the parametric kernel matrix to learn features for the original data. Numerical experiments with real datasets show that Parametric PCA is capable of producing better defined clusters and also more discriminant features in comparison to regular PCA, kernel PCA, sparse PCA, robust PCA and manifold learning algorithms, making the proposed method a promising alternative for unsupervised metric learning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301835",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Dimensionality reduction",
      "Economics",
      "Euclidean distance",
      "Kernel (algebra)",
      "Kernel method",
      "Kernel principal component analysis",
      "Mathematics",
      "Metric (unit)",
      "Nonlinear dimensionality reduction",
      "Operations management",
      "Parametric statistics",
      "Pattern recognition (psychology)",
      "Principal component analysis",
      "Sparse PCA",
      "Statistics",
      "Support vector machine",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Levada",
        "given_name": "Alexandre L.M."
      }
    ]
  },
  {
    "title": "Single image-based head pose estimation with spherical parametrization and 3D morphing",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107316",
    "abstract": "Head pose estimation plays a vital role in various applications, e.g., driver-assistance systems, human-computer interaction, virtual reality technology, and so on. We propose a novel geometry-based method for accurately estimating the head pose from a single 2D face image at a very low computational cost. Specifically, the rectangular coordinates of only four non-coplanar feature points from a predefined 3D facial model as well as the corresponding ones automatically/manually extracted from a 2D face image are first normalized to exclude the effect of external factors (i.e., scale factor and translation parameters). Then, the four normalized 3D feature points are represented in spherical coordinates with reference to the uniquely determined sphere by themselves. Due to the spherical parametrization, the coordinates of feature points can then be morphed along all the three directions in the rectangular coordinates effectively. Finally, the rotation matrix indicating the head pose is obtained by minimizing the Euclidean distance between the normalized 2D feature points and the 2D re-projections of the morphed 3D feature points. Comprehensive experimental results over two popular datasets, i.e., Pointing’04 and Biwi Kinect, demonstrate that the proposed method can estimate head poses with higher accuracy and lower run time than state-of-the-art geometry-based methods. Even compared with start-of-the-art learning-based methods or geometry-based methods with additional depth information, our method still produces comparable performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301205",
    "keywords": [
      "3D pose estimation",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Face (sociological concept)",
      "Feature (linguistics)",
      "Gene",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Messenger RNA",
      "Morphing",
      "Parametrization (atmospheric modeling)",
      "Philosophy",
      "Physics",
      "Pose",
      "Quantum mechanics",
      "Radiative transfer",
      "Rotation (mathematics)",
      "Rotation matrix",
      "Social science",
      "Sociology",
      "Spherical coordinate system",
      "Translation (biology)"
    ],
    "authors": [
      {
        "surname": "Yuan",
        "given_name": "Hui"
      },
      {
        "surname": "Li",
        "given_name": "Mengyu"
      },
      {
        "surname": "Hou",
        "given_name": "Junhui"
      },
      {
        "surname": "Xiao",
        "given_name": "Jimin"
      }
    ]
  },
  {
    "title": "Image stitching with positional relationship constraints of feature points and lines",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.05.003",
    "abstract": "The elimination of parallax and the processing of natural issue in complex scenes are challenging tasks for image stitching. In this paper, an image stitching method with positional relationship constraints of feature points and lines, which can accomplish accurate alignment and reduce projection distortion, is proposed. At first, to reduce the computational cost and the number of outliers on subsequent feature matching, we combine the template matching to propose a quick way for detecting overlapping regions. Then, the appropriate reference image is determined to mitigate the projection distortion that the image warping is in the cases of non-planar geometry of the scenes. Furthermore, a local mesh model based on dual features is established to guide the mesh deformation. And an energy function is designed to refine alignment. In addition to alignment error terms, a novel positional relationship constraint term is proposed to improve quality of naturalness of final stitching results. Finally, experimental results demonstrate that our approach is superior to the existing image stitching algorithms in improving quality of the stitched image naturalness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301732",
    "keywords": [
      "Algorithm",
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Biochemistry",
      "Chemistry",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Distortion (music)",
      "Feature (linguistics)",
      "Gene",
      "Image (mathematics)",
      "Image stitching",
      "Image warping",
      "Linguistics",
      "Messenger RNA",
      "Parallax",
      "Philosophy",
      "Projection (relational algebra)",
      "Translation (biology)"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Xiaoyuan"
      },
      {
        "surname": "Li",
        "given_name": "Yang"
      },
      {
        "surname": "Yan",
        "given_name": "Jing"
      },
      {
        "surname": "Guan",
        "given_name": "Xinping"
      }
    ]
  },
  {
    "title": "CNN-DMRI: A Convolutional Neural Network for Denoising of Magnetic Resonance Images",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.03.036",
    "abstract": "Magnetic Resonance Images (MRI) are often contaminated by rician noise at the acquisition time. This type of noise typically deteriorates the performance of disease diagnosis by a human observer or an automated system. Thus, it is necessary to remove the rician noise from MRI scans as a preprocessing step. In this letter, we propose a novel Convolutional Neural Network (CNN), viz. CNN-DMRI, for denoising of MRI scans. The network uses a set of convolutions to separate the image features from the noise. The network also employs encoder-decoder structure for preserving the prominent features of the image while ignoring unnecessary ones. The training of the network is carried out in an end-to-end way by utilizing residual learning scheme. The performance of the proposed CNN has been tested qualitatively and quantitatively on one simulated and four real MRI datasets. Extensive experimental findings suggest that the proposed network can denoise MRI images effectively without losing crucial image details.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301203",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Decoding methods",
      "Fading",
      "Image (mathematics)",
      "Noise (video)",
      "Noise reduction",
      "Pattern recognition (psychology)",
      "Preprocessor",
      "Residual",
      "Rician fading"
    ],
    "authors": [
      {
        "surname": "Tripathi",
        "given_name": "Prasun Chandra"
      },
      {
        "surname": "Bag",
        "given_name": "Soumen"
      }
    ]
  },
  {
    "title": "A concave optimization algorithm for matching partially overlapping point sets",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107322",
    "abstract": "Matching partially overlapping point sets is a challenging problem in computer vision. To achieve this goal, we model point matching as a mixed linear assignment - least square problem. By eliminating the transformation variable, we reduce the minimization problem to a concave optimization problem with the property that the objective function can be converted into a form with few nonlinear terms. We then use a heuristic variant of the branch-and-bound algorithm for optimization where convergence of the upper bound is used as the stopping criterion. We also propose a new lower bounding scheme which involves solving a k-cardinality linear assignment problem. Two cases of transformations, transformation output being linear with respect to parameters and 2D/3D similarity transformations, are discussed, resulting in ability to handle unknown arbitrary translation and similarity, respectively. Experimental results demonstrate better robustness of the algorithm over state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301254",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Assignment problem",
      "Biochemistry",
      "Bounding overwatch",
      "Cardinality (data modeling)",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Gene",
      "Heuristic",
      "Linear programming",
      "Matching (statistics)",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Matrix similarity",
      "Optimization problem",
      "Partial differential equation",
      "Robustness (evolution)",
      "Statistics",
      "Upper and lower bounds"
    ],
    "authors": [
      {
        "surname": "Lian",
        "given_name": "Wei"
      },
      {
        "surname": "Zhang",
        "given_name": "Lei"
      }
    ]
  },
  {
    "title": "Spatially regularized active diffusion learning for high-dimensional images",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.04.021",
    "abstract": "An active learning method for the classification of high-dimensional images is proposed in which spatially-regularized nonlinear diffusion geometry is used to characterize cluster cores. The proposed method samples from estimated cluster cores in order to generate a small but potent set of training labels which propagate to the remainder of the dataset via the underlying diffusion process. By spatially regularizing the rich, high-dimensional spectral information of the image to efficiently estimate the most significant and influential points in the data, our approach avoids redundancy in the training dataset. This allows it to produce high-accuracy labelings with a very small number of training labels. The proposed algorithm admits an efficient numerical implementation that scales essentially linearly in the number of data points under a suitable data model and enjoys state-of-the-art performance on real hyperspectral images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301446",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Computer science",
      "Diffusion",
      "Diffusion map",
      "Dimensionality reduction",
      "High dimensional",
      "Hyperspectral imaging",
      "Image (mathematics)",
      "Mathematics",
      "Nonlinear dimensionality reduction",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Redundancy (engineering)",
      "Remainder",
      "Set (abstract data type)",
      "Thermodynamics",
      "Training set"
    ],
    "authors": [
      {
        "surname": "Murphy",
        "given_name": "James M."
      }
    ]
  },
  {
    "title": "Structural sparse representation with class-specific dictionary for ECG biometric recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.04.022",
    "abstract": "For traditional person recognition, most methods are knowledge-based and token-based such as the methods using password, ID number or other secret information, and to enhance its security, biometric recognition is widely used in many areas. As a biometric trait, electrocardiogram (ECG) signal is gaining more attention due to the fact that it is not easy to be faked compared with other traits and only exists in living subjects. However, ECG signals are influenced by numerous factors easily and have low stability, resulting in unsatisfied recognition performance. In this paper, we propose a structural sparse representation algorithm for ECG signals and learn class-specific dictionary for each class to address these problems. Firstly, we segment heartbeats for each subject’s signal. Moreover, the mixed regularization l 2,1 norm is utilized for ECG heartbeats to obtain a robust and stable feature. It is encouraged that if the atom is used to describe the heartbeat of one subject’s heartbeat, it may be used to describe other heartbeats of the same subject. In addition, a graph regularization is used to minimize the difference between heartbeats of intra-class. Finally, we conduct experiments on three public ECG databases to verify the proposed method and the subject recognition accuracy can reach to 99.66% on PTB database. From the experimental result, the proposed method can take full advantage of ECG structural characteristic and get a better performance compared with state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301434",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Class (philosophy)",
      "Computer network",
      "Computer science",
      "Computer security",
      "Heartbeat",
      "Password",
      "Pattern recognition (psychology)",
      "Security token",
      "Sparse approximation",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Jingxiao"
      },
      {
        "surname": "Yang",
        "given_name": "Gongping"
      },
      {
        "surname": "Wang",
        "given_name": "Kuikui"
      },
      {
        "surname": "Huang",
        "given_name": "Yuwen"
      },
      {
        "surname": "Liu",
        "given_name": "Haiying"
      },
      {
        "surname": "Yin",
        "given_name": "Yilong"
      }
    ]
  },
  {
    "title": "Three-stream fusion network for first-person interaction recognition",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107279",
    "abstract": "First-person interaction recognition is a challenging task because of unstable video conditions resulting from the camera wearer’s movement. For human interaction recognition from a first-person viewpoint, this paper proposes a three-stream fusion network with two main parts: three-stream architecture and three-stream correlation fusion. The three-stream architecture captures the characteristics of the target appearance, target motion, and camera ego-motion. Meanwhile the three-stream correlation fusion combines the feature map of each of the three streams to consider the correlations among the target appearance, target motion, and camera ego-motion. The fused feature vector is robust to the camera movement and compensates for the noise of the camera ego-motion. Short-term intervals are modeled using the fused feature vector, and a long short-term memory (LSTM) model considers the temporal dynamics of the video. We evaluated the proposed method on two public benchmark datasets to validate the effectiveness of our approach. The experimental results show that the proposed fusion method successfully generated a discriminative feature vector, and our network outperformed all competing activity recognition methods in first-person videos where considerable camera ego-motion occurs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300844",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Linguistics",
      "Motion (physics)",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Ye-Ji"
      },
      {
        "surname": "Lee",
        "given_name": "Dong-Gyu"
      },
      {
        "surname": "Lee",
        "given_name": "Seong-Whan"
      }
    ]
  },
  {
    "title": "TVENet: Temporal variance embedding network for fine-grained action representation",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107267",
    "abstract": "With the breakthroughs in general action understanding, it has become an inevitable trend to analyze the actions in finer granularity. However, related researches have been largely hindered by the lack of fine-grained datasets and the difficulty of capturing subtle differences between fine-grained actions that are highly similar overall. In this paper, we address the above challenges by constructing a fine-grained action dataset, i.e., Figure Skating, which can be used for end-to-end network training and presenting a framework for the joint optimization of classification and similarity constraints. We propose to incorporate the triplet loss into the training of Convolutional Neural Network, which learns a mapping from fine-grained actions to a compact Euclidean space where distances directly correspond to a measure of action similarity. Triplet loss compels actions of distinct classes to have larger distances than actions of the same class. Besides, to boost the discrimination of the fine-grained actions, we further propose a temporal variance embedding network (TVENet) embedding temporal context variances into the feature embeddings during the joint network training. The experimental results on Figure Skating dataset, HMDB51 dataset as well as UCF101 dataset demonstrate the effectiveness of TVENet representation for fine-grained action search.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300728",
    "keywords": [
      "Accounting",
      "Action (physics)",
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Embedding",
      "Law",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Representation (politics)",
      "Variance (accounting)"
    ],
    "authors": [
      {
        "surname": "Han",
        "given_name": "Tingting"
      },
      {
        "surname": "Yao",
        "given_name": "Hongxun"
      },
      {
        "surname": "Xie",
        "given_name": "Wenlong"
      },
      {
        "surname": "Sun",
        "given_name": "Xiaoshuai"
      },
      {
        "surname": "Zhao",
        "given_name": "Sicheng"
      },
      {
        "surname": "Yu",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Time-sync comments denoising via graph convolutional and contextual encoding",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.05.004",
    "abstract": "Time-Sync Comments (TSC), which is a new kind of textual comments on online video websites, has showed its great potential in fine-grain video analysis. However, as a crowd-sourced resource, there are many low quality comments in TSC data and this is an impediment to make full use of TSC. Thus a denoising method is necessary when we are dealing with these comments. In this study, we propose GCCED, a graph convolutional and contextual encoding denoising model for TSC semantic denoising problem. A TSC graph is built on the whole corpus and semantic embedding of words are learned through graph convolution. Moreover, we exploit the relations between TSC and its context and design an embedding method based on the word graph. Experiments on real world dataset are conducted and the result demonstrate the proposed model outperforming other baselines in almost all classification metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301744",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Computer security",
      "Context (archaeology)",
      "Embedding",
      "Encoding (memory)",
      "Exploit",
      "Graph",
      "Machine learning",
      "Natural language processing",
      "Noise reduction",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Theoretical computer science",
      "Word embedding",
      "sync"
    ],
    "authors": [
      {
        "surname": "Liao",
        "given_name": "Zhenyu"
      },
      {
        "surname": "Xian",
        "given_name": "Yikun"
      },
      {
        "surname": "Li",
        "given_name": "Jiangfeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Chenxi"
      },
      {
        "surname": "Zhao",
        "given_name": "Shengjie"
      }
    ]
  },
  {
    "title": "Gaze-based classification of autism spectrum disorder",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.04.028",
    "abstract": "People with autism spectrum disorder (ASD) display impairments in social interaction and communication skills, as well as restricted interests and repetitive behaviors, which greatly affect daily life functioning. Current identification of ASD involves a lengthy process that requires an experienced clinician to assess multiple domains of functioning. Considering this, we propose a method for classifying multiple levels of risk of ASD using eye gaze and demographic feature descriptors such as a subject's age and gender. We construct feature descriptors that incorporate the subject's age and gender, as well as features based on eye gaze patterns. We also present an analysis of eye gaze patterns validating the use of the selected hand-crafted features. We assess the efficacy of our descriptors to classify ASD on a National Database for Autism Research dataset, using multiple classifiers including a random forest, C4.5 decision tree, PART, and a deep feedforward neural network.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301501",
    "keywords": [
      "Affect (linguistics)",
      "Artificial intelligence",
      "Autism",
      "Autism spectrum disorder",
      "Biology",
      "Botany",
      "Cognitive psychology",
      "Communication",
      "Computer science",
      "Construct (python library)",
      "Decision tree",
      "Developmental psychology",
      "Feature (linguistics)",
      "Gaze",
      "Identification (biology)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Psychology"
    ],
    "authors": [
      {
        "surname": "Fabiano",
        "given_name": "Diego"
      },
      {
        "surname": "Canavan",
        "given_name": "Shaun"
      },
      {
        "surname": "Agazzi",
        "given_name": "Heather"
      },
      {
        "surname": "Hinduja",
        "given_name": "Saurabh"
      },
      {
        "surname": "Goldgof",
        "given_name": "Dmitry"
      }
    ]
  },
  {
    "title": "The equivalence of two definitions of compatible homography matrices",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.03.033",
    "abstract": "In many computer vision applications, one acquires images of planar surfaces from two different vantage points. One can use a projective transformation to map pixel coordinates associated with a particular planar surface from one image to another. The transformation, called a homography, can be represented by a unique, to within a scale factor, 3 × 3 matrix. One requires a different homography matrix, scale differences apart, for each planar surface whose two images one wants to relate. However, a collection of homography matrices forms a valid set only if the matrices satisfy consistency constraints implied by the rigidity of the motion and the scene. We explore what it means for a set of homography matrices to be compatible and show that two seemingly disparate definitions are in fact equivalent. Our insight lays the theoretical foundations upon which the derivation of various sets of homography consistency constraints can proceed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301173",
    "keywords": [
      "Algebra over a field",
      "Artificial intelligence",
      "Atlas (anatomy)",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Gene",
      "Homography",
      "Mathematics",
      "Paleontology",
      "Planar",
      "Programming language",
      "Projective space",
      "Projective test",
      "Pure mathematics",
      "Set (abstract data type)",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Chojnacki",
        "given_name": "Wojciech"
      },
      {
        "surname": "Szpak",
        "given_name": "Zygmunt L."
      },
      {
        "surname": "Wadenbäck",
        "given_name": "Mårten"
      }
    ]
  },
  {
    "title": "Masking domain-specific information for cross-domain deception detection",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.04.020",
    "abstract": "The facilities provided by social media and computer-mediated communication make easy the dissemination of deceptive behavior, after which different entities or people could be affected. The deception detection by supervised learning has been widely studied; however, the scenario in which there is one domain of interest and the labeled data is in another domain has received poor attention. This paper presents, to our knowledge, the first domain adaptation approach for cross-domain deception detection in texts. Our proposal consists in modifying original texts from the source and target domains in a form in which common content and style information is maintained, but domain-specific information is masked. In order to adequately select domain-specific terms to be masked, the proposed method uses unlabeled instances from both domains. Our experiments demonstrate that the masking technique is a good idea for detecting deception in cross-domain scenarios; and the performance could be further improved if unlabeled information from the target domain is considered.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301422",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Deception",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Machine learning",
      "Masking (illustration)",
      "Mathematical analysis",
      "Mathematics",
      "Psychology",
      "Social psychology",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Sánchez-Junquera",
        "given_name": "Javier"
      },
      {
        "surname": "Villaseñor-Pineda",
        "given_name": "Luis"
      },
      {
        "surname": "Montes-y-Gómez",
        "given_name": "Manuel"
      },
      {
        "surname": "Rosso",
        "given_name": "Paolo"
      },
      {
        "surname": "Stamatatos",
        "given_name": "Efstathios"
      }
    ]
  },
  {
    "title": "Unsupervised learning of optical flow with patch consistency and occlusion estimation",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107191",
    "abstract": "Recent works have shown that deep networks can be trained for optical flow estimation without supervision. Based on the photometric constancy assumption, most of these methods adopt the reconstruction loss as the supervision by point-based backward warping. Inspired by the traditional patch matching based approaches, we propose a patch-based consistency to improve the vanilla unsupervised learning method Ren et al. [1]. Instead of only comparing the corresponding pixel intensity, we locate the correspondence by using the image patches with census transform, which is more robust for the illumination variation and occlusion. Moreover, a novel parallel branch is devised to estimate a soft occlusion mask jointly in an unsupervised way. The mask is adopted to weight our patch-based consistency loss to alleviate the influence of the occlusion. The plenty of experiments have been implemented on Flying Chairs, KITTI and MPI-Sintel benchmarks. The results show that our method is efficient and outperforms the peer unsupervised learning methods that are using the FlowNet-liked network.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304911",
    "keywords": [
      "Artificial intelligence",
      "Cardiology",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Geometry",
      "Image (mathematics)",
      "Image warping",
      "Matching (statistics)",
      "Mathematics",
      "Medicine",
      "Occlusion",
      "Optical flow",
      "Pattern recognition (psychology)",
      "Point (geometry)",
      "Statistics",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Ren",
        "given_name": "Zhe"
      },
      {
        "surname": "Yan",
        "given_name": "Junchi"
      },
      {
        "surname": "Yang",
        "given_name": "Xiaokang"
      },
      {
        "surname": "Yuille",
        "given_name": "Alan"
      },
      {
        "surname": "Zha",
        "given_name": "Hongyuan"
      }
    ]
  },
  {
    "title": "Cross-view hashing via supervised deep discrete matrix factorization",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107270",
    "abstract": "Matrix factorization has been utilized for the task of cross-view hashing, where basis functions are learned to map data from different views to the same hamming embedding. It is possible that the basis functions between the hamming embedding and the original data matrix contain rather complex hierarchical information, which existing work can not capture. In addition, previous work employs relaxation technique in the matrix factorization based hashing which may lead to large quantization error. To address these issues, this paper presents a novel Supervised Discrete Deep Matrix Factorization (SDDMF) for cross-view hashing. We introduce deep matrix factorization so that SDDMF is able to learn a set of hierarchical basis functions and unified binary codes from different views. In addition, a classification error term is incorporated into the objective to learn discriminative binary codes. We then employ a linearization technique to directly optimize the discrete constraints which can significantly reduce the quantization error. Experimental results on three standard datasets with image-text modalities verify that SDDMF significantly outperforms several state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300753",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Eigenvalues and eigenvectors",
      "Hash function",
      "Matrix decomposition",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Xiong",
        "given_name": "Yingjun"
      },
      {
        "surname": "Xu",
        "given_name": "Yan"
      },
      {
        "surname": "Shu",
        "given_name": "Xin"
      }
    ]
  },
  {
    "title": "Driving maneuver early detection via sequence learning from vehicle signals and video images",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107276",
    "abstract": "Driving Maneuver Early Detection (DMED) is particularly useful for many applications of intelligent vehicle systems, including driver warning and collision avoidance systems. In this paper, we introduce a robust DMED model, denoted as University of Michigan Dearborn (UMD)-DMED, developed using innovative features and deep learning techniques. The UMD-DMED model contains three major computational components, distance based representation of driving context, combined vehicle trajectory features and visual features, and a Long Short-Term Memory (LSTM)-based neural network that captures temporal dependencies of driving maneuvers. To properly evaluate the performances of UMD-DMED, we developed two DMED systems based on the UMD-DMED model, one system is based on partially observed evidence of maneuver events, and another on features observed ahead of the time that driving maneuvers take place. We conducted the extensive experiments using a data set containing 1078 maneuver events extracted from 37 hours of real world driving trips. The results demonstrate that the UMD-DMED model is capable of learning the latent features of five different classes of driving maneuvers, i.e. left turn, right turn, left lane change, right lane change, driving straight. Comparing to four different state-of-the-art DMED systems, the UMD-DMED achieved better detection performances in both, the detection based on partial observations of driver maneuvering, and based on driving context observed ahead-of-time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300819",
    "keywords": [
      "Artificial intelligence",
      "Astronomy",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Deep learning",
      "Paleontology",
      "Physics",
      "Trajectory"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Xishuai"
      },
      {
        "surname": "Murphey",
        "given_name": "Yi Lu"
      },
      {
        "surname": "Liu",
        "given_name": "Ruirui"
      },
      {
        "surname": "Li",
        "given_name": "Yuanxiang"
      }
    ]
  },
  {
    "title": "Local low-rank matrix recovery for hyperspectral image denoising with ℓ0 gradient constraint",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.04.012",
    "abstract": "Hyperspectral image (HSI) acquisition often suffers from the mixed noise, which greatly limits its subsequent applications. This paper proposes a novel HSI denoising method by using local low-rank matrix recovery and ℓ0 gradient, which can simultaneously identify the low-rank structures of the clean HSI and the sparse components of the mixed noise. Specifically, the HSI is modeled locally and a scheme of rank-fixed low-rank matrix recovery is employed to separate the latent clean HSI patches from the noisy counterpart. Meanwhile, the ℓ0 gradient constraint mechanism is utilized to characterize the piecewise smooth structure along both the spectral and spatial dimensions of the reconstructed image from the patches. Moreover, the ℓ1 norm regularization is adopted to suppress the sparse noise, such as stripes, deadlines, impulse noise, and so on. Also, an efficient iterative schema is developed based on an augmented Lagrange algorithm. The corresponding closed-form solutions of the subproblems are derived for calculating an approximate result of the nonconvex optimization problem. Extensive experiments on both simulated and real HSIs prove that the ℓ0 gradient constraint can preserve the edge details well and the proposed method can yield better performance than the state-of-the-art methods for HSI denoising.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552030132X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Eigenvalues and eigenvectors",
      "Hyperspectral imaging",
      "Impulse noise",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Matrix norm",
      "Noise reduction",
      "Pattern recognition (psychology)",
      "Physics",
      "Piecewise",
      "Pixel",
      "Quantum mechanics",
      "Rank (graph theory)",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Yanhong"
      },
      {
        "surname": "Zheng",
        "given_name": "Jianwei"
      },
      {
        "surname": "Chen",
        "given_name": "Shengyong"
      }
    ]
  },
  {
    "title": "Exploring temporal consistency for human pose estimation in videos",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107258",
    "abstract": "In this paper, we introduce a method of exploring temporal information for estimating human poses in videos. The current state-of-the-art methods utilizing temporal information can be categorized into two major branches. The first category is a model-based method that captures the temporal information entirely by using a learnable function such as RNN or 3D convolution. However, these methods are limited in exploring temporal consistency, which is essential for estimating human joint positions in videos. The second category is the posterior enhancement method, where an independent post-processing step (e.g., using optical flow) is applied to enhance the prediction. However, operations such as optical flow estimation can be susceptible to the occlusion and motion blur problems, which will adversely affect the final performance. We propose a novel Temporal Consistency Exploration (TCE) module to address both shortcomings. Compared to previous approaches, the TCE module is more efficient as it captures the temporal consistency at the feature level without having to post-process and calculate extra optical flow. Further, to capture the rich spatial context in video data, we design a multi-scale TCE to explore the time consistency information at multi-scale spatial levels. Finally, a video-based pose estimation network is designed, which is based on the encoder-decoder architecture and extended with the powerful multi-scale TCE module. We comprehensively evaluate the proposed model on two video datasets, Sub-JHMDB and Penn, and our model achieves state-of-the-art performance on both datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300637",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Context (archaeology)",
      "Convolution (computer science)",
      "Data mining",
      "Encoder",
      "Image (mathematics)",
      "Operating system",
      "Optical flow",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Physics",
      "Process (computing)",
      "Quantum mechanics",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yang"
      },
      {
        "surname": "Li",
        "given_name": "Kan"
      },
      {
        "surname": "Wang",
        "given_name": "Xinxin"
      },
      {
        "surname": "Xu",
        "given_name": "Richard Yi Da"
      }
    ]
  },
  {
    "title": "Webly-supervised learning for salient object detection",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107308",
    "abstract": "End-to-end training of a deep CNN-Based model for salient object detection usually requires a huge number of training samples with pixel-level annotations, which are costly and time-consuming to obtain. In this paper, we propose an approach that can utilize large amounts of web data for learning a deep salient object detection model. With thousands of images collected from the Web, we first employ several bottom-up saliency detection techniques to generate salient object masks for all images, and then use a novel quality evaluation method to pick out a subset of images with reliable masks for training. After that, we develop a self-training approach to boost the performance of our initial network, which iterates between the network training process and the training set updating process. Importantly, different from existing webly-supervised or weakly-supervised methods, our approach is able to automatically select reliable images for network training without requiring any human intervention (e.g., dividing images into different difficulty levels). Results of extensive experiments on several widely-used benchmarks demonstrate that our method has achieved state-of-the-art performance. It significantly outperforms existing unsupervised and weakly-supervised salient object detection methods, and achieves competitive or even better performance than fully supervised approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301126",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Machine learning",
      "Object (grammar)",
      "Object detection",
      "Operating system",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Salient",
      "Supervised learning"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Ao"
      },
      {
        "surname": "Li",
        "given_name": "Xin"
      },
      {
        "surname": "Yang",
        "given_name": "Fan"
      },
      {
        "surname": "Jiao",
        "given_name": "Zhicheng"
      },
      {
        "surname": "Cheng",
        "given_name": "Hong"
      }
    ]
  },
  {
    "title": "Pedestrian detection in underground mines via parallel feature transfer network",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107195",
    "abstract": "Pedestrian detection has been one of the key technologies in computer vision for autonomous driving in underground mines. However, such pedestrian detection is easily affected by complex environmental factors, such as uneven light, dense dust and cable interference. Recently, the problem of pedestrian detection is solved as an object detection task, which has achieved significant advances with the framework of deep neural networks. In this paper, we propose a novel parallel feature transfer network based detector called PftNet that achieves better efficiency than one-stage methods and maintains comparable accuracy of two-stage methods. PftNet consists of two interconnected modules, i.e., the pedestrian identification module and the pedestrian location module. The former aims to roughly adjust the location and size of the anchor box, filter out the negative anchor box, and provide better initialization for the regression. The latter enables PftNet to adapt to different scales and aspect ratios of objects and further improves the regression accuracy. Meanwhile, a feature transfer block compromising gated units is well designed to transmit the pedestrian characteristics between two modules. Extensive experiments on self-annotated underground dataset as well as INRIA and ETH datasets show that PftNet achieves state-of-the-art detection efficiency with high accuracy, which is significant to realizing unmanned driving systems in mines.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300029",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Block (permutation group theory)",
      "Botany",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Engineering",
      "Feature (linguistics)",
      "Filter (signal processing)",
      "Geometry",
      "Identification (biology)",
      "Initialization",
      "Key (lock)",
      "Linguistics",
      "Mathematics",
      "Object detection",
      "Pattern recognition (psychology)",
      "Pedestrian",
      "Pedestrian detection",
      "Philosophy",
      "Programming language",
      "Real-time computing",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Xing"
      },
      {
        "surname": "Zhang",
        "given_name": "Haitao"
      },
      {
        "surname": "Liu",
        "given_name": "Shaofan"
      },
      {
        "surname": "Lu",
        "given_name": "Yang"
      }
    ]
  },
  {
    "title": "Topology-learnable graph convolution for skeleton-based action recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.05.005",
    "abstract": "Graph convolutional networks (GCNs) generalize convolutional neural networks into irregular graph-like structures. Generally, graph topologies are set by hand and fixed over all layers. Handcrafted connections may not be optimal and cannot fully use the self-learning ability of deep learning. In this work, we explore a topology-learnable graph convolution for skeleton-based action recognition. Specifically, a spatial graph convolution can be decomposed into a feature learning component that evolves the features of each graph vertex, and a graph vertex fusion component in which the latent graph topologies can be learned adaptively. Different initialization strategies for the learnable fusion matrix are evaluated. Experimental results that are based on the spatial-temporal GCNs for skeleton-based action recognition, demonstrate that convolution can work on graphs like on images, even if only a specific fusion matrix initialization that uses adjacency matrices is applied. Moreover, the self-learning process can learn the latent topology of a graph beyond the handcrafted topology, thereby making graph convolution flexible and universal.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301756",
    "keywords": [
      "Action recognition",
      "Adjacency matrix",
      "Artificial intelligence",
      "Class (philosophy)",
      "Combinatorics",
      "Computer science",
      "Graph",
      "Initialization",
      "Line graph",
      "Mathematics",
      "Null graph",
      "Pattern recognition (psychology)",
      "Programming language",
      "Theoretical computer science",
      "Topological graph theory",
      "Topology (electrical circuits)",
      "Voltage graph"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Guangming"
      },
      {
        "surname": "Zhang",
        "given_name": "Liang"
      },
      {
        "surname": "Li",
        "given_name": "Hongsheng"
      },
      {
        "surname": "Shen",
        "given_name": "Peiyi"
      },
      {
        "surname": "Shah",
        "given_name": "Syed Afaq Ali"
      },
      {
        "surname": "Bennamoun",
        "given_name": "Mohammed"
      }
    ]
  },
  {
    "title": "Robust pruning for efficient CNNs",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.03.034",
    "abstract": "Deep convolutional neural network (CNN) with considerable number of parameters is one of the promising methods for image recognition. There, however, is generally difficult in applying deep CNNs to resource constrained devices due to the heavy computational burden. For reducing computational cost of CNNs while retaining the classification performance, it is effective to apply pruning methods that remove from CNNs redundant parameters less contributing to classification. The contribution of parameters can be estimated by the empirical classification loss computed over training samples to which ground-truth labels are assigned. The empirical classification loss, however, might be vulnerable to the outlier samples and/or the hard ones that are difficult to classify, and thus the pruning would accordingly be degraded. In this paper, we propose a pruning method based on a novel criterion to measure the redundancy of the parameters in CNNs through empirical classification loss. We start with the Taylor expansion of the loss function and then derive the mathematical formulation of the pruning criterion so as to be robust against some sort of outlier samples. The proposed pruning criterion can also provide stable metric for parameters and evaluate layers of various depth fairly without biases toward shallower or deeper layers. In addition, we present an effective method to normalize the criterion scores for further improving performance. In the experiments on image classification, our method exhibits favorable performance compared with the other methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301185",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computation",
      "Computer science",
      "Contextual image classification",
      "Convolutional neural network",
      "Economics",
      "Geometry",
      "Image (mathematics)",
      "Information retrieval",
      "Machine learning",
      "Mathematics",
      "Metric (unit)",
      "Operating system",
      "Operations management",
      "Outlier",
      "Pattern recognition (psychology)",
      "Pruning",
      "Reduction (mathematics)",
      "Redundancy (engineering)",
      "sort"
    ],
    "authors": [
      {
        "surname": "Ide",
        "given_name": "Hidenori"
      },
      {
        "surname": "Kobayashi",
        "given_name": "Takumi"
      },
      {
        "surname": "Watanabe",
        "given_name": "Kenji"
      },
      {
        "surname": "Kurita",
        "given_name": "Takio"
      }
    ]
  },
  {
    "title": "Open-set face identification with index-of-max hashing by learning",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107277",
    "abstract": "Large-scale face identification or 1-to-N matching where N is huge, plays a vital role in biometrics and surveillance. The system demands accurate and speedy matching where compact facial feature representation and a simple matcher are favored. On the other hand, most research considers closed-set identification that assumes that all identities of probe samples are enclosed in the gallery. On the contrary, open-set identification expects that some probe identities are not known to the system. This setup poses an additional challenge, where the system should be able to reject those probes that correspond to unknown identities. In this paper, we address the large-scale open-set face identification problem with a compact facial representation that is based on the index-of-maximum (IoM) hashing, which was designed for biometric template protection. To be specific, the existing random IoM hashing is advanced to a data-driven based hashing technique, where the hashed face code can be made compact and matching can be easily performed by the Hamming distance, which can offer highly efficient matching. Furthermore, since IoM hashing transforms the original facial features non-invertibly, the privacy of users can also be preserved. Along with IoM hashed face code, we explore several fusion strategies to address the open-set face identification problem. The comprehensive evaluations are carried out with three large-scale unconstrained face datasets, namely LFW, VGG2 and IJB-C.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300820",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Biometrics",
      "Botany",
      "Code (set theory)",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Face (sociological concept)",
      "Facial recognition system",
      "Feature (linguistics)",
      "Hamming distance",
      "Hash function",
      "Hash table",
      "Identification (biology)",
      "Law",
      "Linguistics",
      "Locality-sensitive hashing",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Set (abstract data type)",
      "Social science",
      "Sociology",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Dong",
        "given_name": "Xingbo"
      },
      {
        "surname": "Kim",
        "given_name": "Soohyung"
      },
      {
        "surname": "Jin",
        "given_name": "Zhe"
      },
      {
        "surname": "Hwang",
        "given_name": "Jung Yeon"
      },
      {
        "surname": "Cho",
        "given_name": "Sangrae"
      },
      {
        "surname": "Teoh",
        "given_name": "Andrew Beng Jin"
      }
    ]
  },
  {
    "title": "Textual data summarization using the Self-Organized Co-Clustering model",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107315",
    "abstract": "Recently, different studies have demonstrated the use of co-clustering, a data mining technique which simultaneously produces row-clusters of observations and column-clusters of features. The present work introduces a novel co-clustering model to easily summarize textual data in a document-term format. In addition to highlighting homogeneous co-clusters as other existing algorithms do we also distinguish noisy co-clusters from significant co-clusters, which is particularly useful for sparse document-term matrices. Furthermore, our model proposes a structure among the significant co-clusters, thus providing improved interpretability to users. The approach proposed contends with state-of-the-art methods for document and term clustering and offers user-friendly results. The model relies on the Poisson distribution and on a constrained version of the Latent Block Model, which is a probabilistic approach for co-clustering. A Stochastic Expectation-Maximization algorithm is proposed to run the model’s inference as well as a model selection criterion to choose the number of co-clusters. Both simulated and real data sets illustrate the efficiency of this model by its ability to easily identify relevant co-clusters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301199",
    "keywords": [
      "Artificial intelligence",
      "Automatic summarization",
      "Biclustering",
      "CURE data clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Expectation–maximization algorithm",
      "Inference",
      "Interpretability",
      "Mathematics",
      "Maximum likelihood",
      "Probabilistic logic",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Selosse",
        "given_name": "Margot"
      },
      {
        "surname": "Jacques",
        "given_name": "Julien"
      },
      {
        "surname": "Biernacki",
        "given_name": "Christophe"
      }
    ]
  },
  {
    "title": "Bit-string representation of a fingerprint image by normalized local structures",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107323",
    "abstract": "Conventional minutia-based fingerprint recognition requires a complicated geometric matching and hard to be adopted in the bit-string based cancellable biometrics or bio-encryption, as the minutia data representing a fingerprint image is geometrical, unordered and variable in size. In this paper, we propose a new method to represent a fingerprint image by an ordered and fixed-length bit-string to cope with those difficulties with providing a faster matching, compressibility and improved accuracy performance as well. Firstly, we devised a novel minutia-based local structure modeled by a mixture of 2D elliptical Gaussian functions to represent a minutia in the image pixel space. Then, each local structure was mapped to a point in a Euclidean space by normalizing the local structure by the number of minutiae in it. This simple yet crucial computation for converting the image space to the Euclidean-space enabled the fast dissimilarity computation of two local structures and all followed processes in our proposed method. A complementary texture-based local structure to the minutia-based local structure was also introduced, whereby both were compressed via principal component analysis and fused in the compressed Euclidean space. The fused local structures were then converted to a K-bit ordered string using the K-means clustering algorithm. This chain of computations with the sole use of Euclidean distance was vital for speedy and discriminative bit-string conversion. The accuracy was further improved by the finger-specific bit-training algorithm, in which two criteria were leveraged to select the useful bit positions for matching. Experiments were performed on Fingerprint Verification Competition (FVC) databases for comparisons with the existing techniques to show the superiority of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301266",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Bit array",
      "Computation",
      "Computer science",
      "Discriminative model",
      "Ecology",
      "Fingerprint (computing)",
      "Fingerprint recognition",
      "Mathematical physics",
      "Mathematics",
      "Minutiae",
      "Pattern recognition (psychology)",
      "String (physics)",
      "Type (biology)"
    ],
    "authors": [
      {
        "surname": "Kho",
        "given_name": "Jun Beom"
      },
      {
        "surname": "Teoh",
        "given_name": "Andrew B.J."
      },
      {
        "surname": "Lee",
        "given_name": "Wonjune"
      },
      {
        "surname": "Kim",
        "given_name": "Jaihie"
      }
    ]
  },
  {
    "title": "DevsNet: Deep Video Saliency Network using Short-term and Long-term Cues",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107294",
    "abstract": "Recently, there have been various saliency detection methods proposed for still images based on deep learning techniques. However, the research on saliency detection for video sequences is still limited. In this study, we introduce a novel deep learning framework of saliency detection for video sequences, namely Deep Video Saliency Network (DevsNet). DevsNet mainly consists of two components: 3D Convolutional Network (3D-ConvNet) and Bidirectional Convolutional Long-Short Term Memory Network (B-ConvLSTM). 3D-ConvNet is constructed to learn short-term spatiotemporal information and the long-term spatiotemporal features are learned by B-ConvLSTM. The designed B-ConvLSTM can extract the temporal information not just from the previous video frames but also from the next frames, which demonstrates that the proposed model considers both the forward and backward temporal information. By combining the short-term and long-term spatiotemporal cues, the proposed DevsNet can extract saliency information for video sequences effectively and efficiently. Extensive experiments have been conducted to show that the proposed model can obtain better performance in spatiotemporal saliency prediction than the state-of-the-art models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300984",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Term (time)"
    ],
    "authors": [
      {
        "surname": "Fang",
        "given_name": "Yuming"
      },
      {
        "surname": "Zhang",
        "given_name": "Chi"
      },
      {
        "surname": "Min",
        "given_name": "Xiongkuo"
      },
      {
        "surname": "Huang",
        "given_name": "Hanqin"
      },
      {
        "surname": "Yi",
        "given_name": "Yugen"
      },
      {
        "surname": "Zhai",
        "given_name": "Guangtao"
      },
      {
        "surname": "Lin",
        "given_name": "Chia-Wen"
      }
    ]
  },
  {
    "title": "Historical building point cloud segmentation combining hierarchical watershed transform and curvature analysis",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.04.010",
    "abstract": "Segmenting accurately point clouds is of great relevance in several fields of engineering and construction. Users are interested in properly dividing an point cloud into their components and then recognizing them. Point clouds representing historical buildings present an additional challenge because image details could be related to a cultural or architectural aspect. Therefore, the way the results are evaluated is also important. In this paper, we present a novel point cloud approach for segmenting historical building of different architectural styles and periods. In our approach, that works for organized and unorganized point clouds, we combine Hierarchical Watershed Transform and curvature analysis from region growing methods in order to obtain more suitable seeds. Experiments were conducted involving historical building acquired using drones and terrestrial laser scanner. The data was combined into a single point cloud. Finally, we evaluated our results qualitatively and quantitatively, by comparing them to a dataset containing the ground truth. The quantitative metrics demonstrate the effectiveness of our method when compared with state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301306",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Curvature",
      "Data mining",
      "Geometry",
      "Laser",
      "Laser scanning",
      "Law",
      "Mathematics",
      "Optics",
      "Physics",
      "Point (geometry)",
      "Point cloud",
      "Political science",
      "Programming language",
      "Relevance (law)",
      "Reverse engineering",
      "Segmentation",
      "Watershed"
    ],
    "authors": [
      {
        "surname": "Paiva",
        "given_name": "Pedro V.  V."
      },
      {
        "surname": "Cogima",
        "given_name": "Camila K."
      },
      {
        "surname": "Dezen-Kempter",
        "given_name": "Eloisa"
      },
      {
        "surname": "Carvalho",
        "given_name": "Marco A.  G."
      }
    ]
  },
  {
    "title": "Global and local sensitivity guided key salient object re-augmentation for video saliency detection",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107275",
    "abstract": "Image saliency is determined by spatial semantic features, while video saliency is affected by multiple factors such as spatial and temporal information. Since human eyes stay extremely short on each frame, the dynamic salient area is more focused and concentrated on one salient object. In order to better simulate the human visual attention mechanism in dynamic scenes, we propose a key salient object re-augmentation method (KSORA) based on the guidance of both bottom-up weighted features and top-down semantic knowledge. The bottom-up feature weighting strategy effectively eliminates noisy and redundancy, and provide accurate local spatiotemporal features for saliency inference. The top-down key object enhancement strategy ranks salient candidates based on global statistical knowledge, so as to explicitly enhance the saliency proportion of the key object. The fusion of the local weighted spatiotemporal features and the global key object augmentation features not only ensures spatiotemporal consistency, but also facilitates obtaining more concentrated salient prediction. Results on three large datasets validate that our proposed method has the capability of improving the detection accuracy in complex scenes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300807",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Feature (linguistics)",
      "Inference",
      "Key (lock)",
      "Linguistics",
      "Medicine",
      "Object (grammar)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Radiology",
      "Redundancy (engineering)",
      "Salient",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zheng"
      },
      {
        "surname": "Zhou",
        "given_name": "Ziqi"
      },
      {
        "surname": "Lu",
        "given_name": "Huchuan"
      },
      {
        "surname": "Jiang",
        "given_name": "Jianmin"
      }
    ]
  },
  {
    "title": "A novel Pooling Block for improving lightweight deep neural networks",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.05.012",
    "abstract": "Since lightweight neural networks such as MobileNet, SqueezeNet were designed, it has been possible to run deep neural networks on mobile devices. However, the performance of these lightweight networks is not as good as that of the general deeper networks, especially for image classification and object detection. To solve this issue, we design a general subnetwork, called Pooling Block, to extract the information from input, and fuse it with the information from the original backbone network. Our study shows that the main backbone network combined with the Pooling Block gains better performance with a few extra parameters and computation cost for the lightweight networks. Meanwhile, we find that the Pooling Block can evidently accelerate the speed of convergence during the training phase. We adopt Pooling Block into MobileNet V2 to conduct the image classification, object detection and pedestrian detection experiments on many widely used datasets, including Animal with Attributes 2, Caltech-101, Caltech-256, PASCAL VOC 2007, MS COCO 2017, Caltech Pedestrian and Citypersons. Experimental results show that adding Pooling Block leads to 2% improvement on image classification datasets, and more than 0.5% improvement on object detection datasets, with little or no loss of computational speed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301847",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Backbone network",
      "Block (permutation group theory)",
      "Computation",
      "Computer network",
      "Computer science",
      "Contextual image classification",
      "Data mining",
      "Geometry",
      "Image (mathematics)",
      "Machine learning",
      "Mathematics",
      "Object detection",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Pooling",
      "Programming language",
      "Subnetwork",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Xiao",
        "given_name": "Bo"
      },
      {
        "surname": "Li",
        "given_name": "Xiang-yu"
      },
      {
        "surname": "Li",
        "given_name": "Chun-guang"
      },
      {
        "surname": "Xu",
        "given_name": "Qian-fang"
      }
    ]
  },
  {
    "title": "Graph convolutional network with structure pooling and joint-wise channel attention for action recognition",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107321",
    "abstract": "Recently, graph convolutional networks (GCNs) have achieved state-of-the-art results for skeleton based action recognition by expanding convolutional neural networks (CNNs) to graphs. However, due to the lack of effective feature aggregation method, e.g. max pooling in CNN, existing GCN-based methods only learn local information among adjacent joints and are hard to obtain high-level interaction features, such as interactions between five parts of human body. Moreover, subtle differences of confusing actions often hide in specific channels of key joints’ features, this kind of discriminative information is rarely exploited in previous methods. In this paper, we propose a novel graph convolutional network with structure based graph pooling (SGP) scheme and joint-wise channel attention (JCA) modules. The SGP scheme pools the human skeleton graph according to the prior knowledge of human body’s typology. This pooling scheme not only leads to more global representations but also reduces the amount of parameters and computation cost. The JCA module learns to selectively focus on discriminative joints of skeleton and pays different levels of attention to different channels. This novel attention mechanism enhance the model’s ability to classify confusing actions. We evaluate our SGP scheme and JCA module on three most challenging skeleton based action recognition datasets: NTU-RGB+D, Kinetics-M, and SYSU-3D. Our method outperforms the state-of-art methods on three benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301242",
    "keywords": [
      "Action recognition",
      "Architectural engineering",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Engineering",
      "Graph",
      "Joint (building)",
      "Pattern recognition (psychology)",
      "Pooling",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yuxin"
      },
      {
        "surname": "Ma",
        "given_name": "Gaoqun"
      },
      {
        "surname": "Yuan",
        "given_name": "Chunfeng"
      },
      {
        "surname": "Li",
        "given_name": "Bing"
      },
      {
        "surname": "Zhang",
        "given_name": "Hui"
      },
      {
        "surname": "Wang",
        "given_name": "Fangshi"
      },
      {
        "surname": "Hu",
        "given_name": "Weiming"
      }
    ]
  },
  {
    "title": "Deviation based clustering for unsupervised person re-identification",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.04.039",
    "abstract": "Recently, unsupervised person re-identification (re-ID) has gained a lot of attention, since it does not depend on intensive manual annotation and is more practical to deploy in the real world directly. An inspiring method, the Bottom-up Clustering (BUC), achieves the state-of-the-art among unsupervised re-ID methods and outperforms most semi-supervised and transfer learning algorithms. The BUC utilizes the minimum-distance between samples in different clusters as the merging criterion, and the number of samples as the penalization term. However, the minimum-distance criterion only considers one pair of samples between two clusters, and cannot exploit the information of all samples in clusters. The penalization, intuitively, is unsuitable for the dataset with irregular sample quantity. To relieve this problem, we propose a deviation based clustering re-ID approach, which takes the inter- and intra-cluster deviation into consideration. The inter-cluster deviation denotes the increase of deviation after merging two clusters, considering all samples between the two clusters to merge. The intra-cluster deviation, working as penalization, denotes the distance between samples and the center in each cluster, and thus it can help to mitigate the side-effect of irregular datasets. The two criterions can reflect the inter- and intra- dispersion precisely. Based on these criterions, we group similar samples into clusters and utilize the cluster identities as a pseudo annotation to train our model. To evaluate our proposed approach, we implement abundant experiments on two popular re-ID datasets, where one has irregular sample quality (i.e., Market-1501) and the other has regular sample quality (i.e., DukeMTMC-reID). Evaluations show that our method outperforms BUC by 1.8% on Rank-1 (i.e., 68.0% accuracy) and 2.1% on mAP (i.e., 40.4% accuracy) for the Market-1501 dataset, and well maintains the benefit of BUC on the DukeMIMC-reID dataset with the accuracy of 47.8% in Rank-1 and 27.0% in mAP.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301707",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Chemistry",
      "Chromatography",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Computer science",
      "Computer security",
      "Data mining",
      "Exploit",
      "Identification (biology)",
      "Information retrieval",
      "Merge (version control)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Sample (material)",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Ning",
        "given_name": "Munan"
      },
      {
        "surname": "Zeng",
        "given_name": "Kaiwei"
      },
      {
        "surname": "Guo",
        "given_name": "Yang"
      },
      {
        "surname": "Wang",
        "given_name": "Yaohua"
      }
    ]
  },
  {
    "title": "Learning transferable features in meta-learning for few-shot text classification",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.05.007",
    "abstract": "One of the main issues accompanying the current deep learning models for classification problems is that massive data has to be fed into the training process, while obtaining sufficient annotated samples is usually time-consuming and labor-intensive. To address the problem of few-shot learning, meta-learning has made significant progress recently, which encourages fast adaptation to solve new learning tasks with only a limited number of training examples. Although it has gained increasing attention and impact in the realm of computer vision, its practical use in natural language processing has been rarely exploited. To learn transferable features effectively for few-shot text classification in a meta-learning framework, we propose a novel method which leverages the maximum mean discrepancy metric of the adaptation layer to minimize the distance between the support and query distributions within each learning task and regularize the meta-gradient update. We perform extensive empirical experiments to suggest best practices, as well as evaluate the effectiveness of our proposed method. In comparison with the well-established Model-Agnostic Meta-Learning (MAML) framework, our method achieves better generalization performance and produces remarkable accuracy gains on various text classification datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552030177X",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Computer science",
      "Economics",
      "Generalization",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Meta learning (computer science)",
      "Metric (unit)",
      "Operating system",
      "Operations management",
      "Optics",
      "Physics",
      "Process (computing)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Jincheng"
      },
      {
        "surname": "Du",
        "given_name": "Qingfeng"
      }
    ]
  },
  {
    "title": "Modal features for image texture classification",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.04.036",
    "abstract": "Feature extraction is a key step in image processing for pattern recognition and machine learning processes. Its purpose lies in reducing the dimensionality of the input data through the computing of features which accurately describe the original information. In this article, a new feature extraction method based on Discrete Modal Decomposition (DMD) is introduced, to extend the group of space and frequency based features. These new features are called modal features. Initially aiming to decompose a signal into a modal basis built from a vibration mechanics problem, the DMD projection is applied to images in order to extract modal features with two approaches. The first one, called full scale DMD, consists in exploiting directly the decomposition resulting coordinates as features. The second one, called filtering DMD, consists in using the DMD modes as filters to obtain features through a local transformation process. Experiments are performed on image texture classification tasks including several widely used data bases, compared to several classic feature extraction methods. We show that the DMD approach achieves good classification performances, comparable to the state of the art techniques, with a lower extraction time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301653",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Curse of dimensionality",
      "Feature extraction",
      "Gene",
      "Modal",
      "Pattern recognition (psychology)",
      "Polymer chemistry",
      "Projection (relational algebra)",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Lacombe",
        "given_name": "Thomas"
      },
      {
        "surname": "Favreliere",
        "given_name": "Hugues"
      },
      {
        "surname": "Pillet",
        "given_name": "Maurice"
      }
    ]
  },
  {
    "title": "Pattern recognition techniques for provenance classification of archaeological ceramics using ultrasounds",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.04.013",
    "abstract": "This paper presents a novel application of pattern recognition to the provenance classification of archaeological ceramics. This is a challenging problem for archaeologists, which involves assigning a making location to a fragment of archaeological pottery that was found along with other fragments of pieces made in different distant locations from the find. The pieces look very similar to each other and, often, other contextual information about the use of the pieces cannot be used due to the small size of the fragments. Current standard methods to solve this problem are limited since they are time consuming, require costly equipment, and can lead to the destruction of a part of the pieces. The proposed method overcome those limitations using non-destructive ultrasonic testing and incorporates versatile data analysis through advanced pattern recognition techniques. Those techniques include the following: feature ranking, sample augmentation, semi-supervision based on active learning; and optimal fusion. This latter is based in the concept of alpha integration, which allows optimal fitting of the fusion model parameters. Different provenance classification problems are showcased: provenance classification of terra sigillata ceramic pieces from Aretina, Northern Italy and Sud-Gaul origins; and provenance classification of Iberian ceramic pieces from archaeological sites of Paterna, and Les Jovaes in Valencia, Spain. We demonstrate that the proposed fusion-based method achieves the best results, in terms of balanced classification accuracy and F1 score, compared with competitive methods like linear discriminant analysis, random forest, and support vector machine. Experiments for simulating small sample sizes and uncertainty in labeling of the pieces are included. In addition, the paper provides a design of a practical specialized device that could be used in different applications of archaeological ceramic classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301343",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Feature (linguistics)",
      "Geography",
      "Geology",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Petrology",
      "Philosophy",
      "Pottery",
      "Provenance",
      "Random forest",
      "Ranking (information retrieval)",
      "Sample (material)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Salazar",
        "given_name": "Addisson"
      },
      {
        "surname": "Safont",
        "given_name": "Gonzalo"
      },
      {
        "surname": "Vergara",
        "given_name": "Luis"
      },
      {
        "surname": "Vidal",
        "given_name": "Enrique"
      }
    ]
  },
  {
    "title": "Deep support vector machine for hyperspectral image classification",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107298",
    "abstract": "To improve on the robustness of traditional machine learning approaches, emphasis has recently shifted to the integration of such methods with Deep Learning techniques. However, the classification problems, complexity and inconsistency in several spectral classifiers developed for hyperspectral images are some reasons warranting further research. This study investigates the application of Deep Support Vector Machine (DSVM) for hyperspectral image classification. Two hyperspectral images, Indian Pines and University of Pavia are used as tentative test beds for the experiment. The DSVM is implemented with four kernel functions: Exponential Radial Basis Function (ERBF), Gaussian Radial Basis Function (GRBF), neural and polynomial. Stand-alone Support Vector Machines form the interconnecting weights of the entire network. The network is trained with one hundred input datasets, and the interconnecting weights of the network are initialised using the regularisation parameter of the model. Numerical results show that the classification accuracies of the DSVM for Indian Pines and University of Pavia based on each DSVM kernel functions are: ERBF (98.87%, 98.16%), GRBF (98.90%, 98.47%), neural (98.41%, 97.27%), and polynomial (99.24%, 98.79%). By comparing the DSVM algorithm against well-known classifiers, Support Vector Machine (SVM), Deep Neural Network (DNN), Gaussian Mixture Model (GMM), K Nearest Neighbour (KNN), and K Means (KM) classifiers, the mean classification accuracies for Indian Pines and University of Pavia are: DSVM (98.86%, 98.17%), SVM (76.03%, 73.52%), DNN (94.45%, 93.79%), GMM (76.82%, 78.35%), KNN (76.87%, 78.80%), and KM (21.65%, 18.18%). These results indicate that the DSVM outperformed the other classification algorithms. The high accuracy obtained with the DSVM validates its efficacy as state-of-the-art algorithm for hyperspectral image classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301023",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Contextual image classification",
      "Gaussian",
      "Gaussian function",
      "Gene",
      "Hyperspectral imaging",
      "Image (mathematics)",
      "Kernel (algebra)",
      "Kernel method",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Polynomial",
      "Polynomial kernel",
      "Quantum mechanics",
      "Radial basis function",
      "Radial basis function kernel",
      "Robustness (evolution)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Okwuashi",
        "given_name": "Onuwa"
      },
      {
        "surname": "Ndehedehe",
        "given_name": "Christopher E."
      }
    ]
  },
  {
    "title": "A novel approach combined transfer learning and deep learning to predict TMB from histology image",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.04.008",
    "abstract": "Tumor Mutation Burden(TMB) is a quantifiable clinical indicator that can be used to predict the responses to immunotherapy of a range of tumors. However, the current DNA sequencing-based TMB measurement method represented by Whole Exome Sequencing (WES) is expensive and time-consuming, which limits its utilization in clinical practice. In this paper, we design a method through deep learning in order to predict TMB from available H&E stained whole slide images of gastrointestinal cancer. Experimental results demonstrate that our approach is capable of distinguishing high and low TMB with an AUC higher than 0.75. We further performed post-processing to improve the accuracy on both test sets to above 0.7 (0.71 accuracy for TMB-STAD and 0.77 accuracy for TMB-COAD-DX). Furthermore, the predicted low and high TMB patients with gastric and colon cancer have different survival rates, with p values of 0.348 and 0.8113, respectively, which indicates that our study is potentially helpful for practical treatment.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301288",
    "keywords": [
      "Aerospace engineering",
      "Artificial intelligence",
      "Biology",
      "Cancer",
      "Colorectal cancer",
      "Computer science",
      "DNA",
      "DNA sequencing",
      "Deep learning",
      "Deep sequencing",
      "Engineering",
      "Exome sequencing",
      "Gene",
      "Genetics",
      "Genome",
      "Internal medicine",
      "Machine learning",
      "Medicine",
      "Mutation",
      "Pattern recognition (psychology)",
      "Range (aeronautics)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Liansheng"
      },
      {
        "surname": "Jiao",
        "given_name": "Yudi"
      },
      {
        "surname": "Qiao",
        "given_name": "Ying"
      },
      {
        "surname": "Zeng",
        "given_name": "Nianyin"
      },
      {
        "surname": "Yu",
        "given_name": "Rongshan"
      }
    ]
  },
  {
    "title": "Pseudo distribution on unseen classes for generalized zero shot learning",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.05.021",
    "abstract": "Although Zero Shot Learning (ZSL) has attracted more and more attention due to its powerful ability of recognizing new objects without retraining, it has a serious drawback that it only focuses on unseen classes during prediction. To solve this issue, Generalized ZSL (GZSL) extends the search range to both seen and unseen classes, which makes it a more realistic and challenging task. Conventional methods on GZSL often suffer from the domain shift problem on seen classes because they have only seen data for training. Deep Calibration Network (DCN) tries to minimize the entropy of assigning seen data to unseen classes to balance the training on both seen and unseen classes. However, there are still two problems for DCN, one is the hubness problem and another is the lack of training guidance. In this paper, to solve the two problems, we propose a novel method called PSeudo Distribution (PSD), which exploits the attribute similarity between seen classes and unseen classes as the training guidance to assign the seen data to unseen classes. In addition, the attribute similarity is also compressed to one-hot vector to further encourage the certainty of the model. Besides, the visual space is utilized as the embedding space, which can well settle the hubness problem. Extensive experiments are conducted on four popular datasets, and the results show the superiority of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302002",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Embedding",
      "Exploit",
      "Image (mathematics)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Haofeng"
      },
      {
        "surname": "Liu",
        "given_name": "Jingren"
      },
      {
        "surname": "Yao",
        "given_name": "Yazhou"
      },
      {
        "surname": "Long",
        "given_name": "Yang"
      }
    ]
  },
  {
    "title": "Comparison of incremental linear dimension reduction methods for streaming data",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.03.028",
    "abstract": "Traditional linear dimension reduction methods such as Principal component analysis (PCA) and linear discriminant analysis (LDA) have been used in many application areas due to simplicity and high performance. However, in data streams where data instances are generated continuously over time, it is difficult to apply traditional PCA or LDA. Moreover, data streams can have drifting concepts over time. In this paper, we compared several incremental linear dimension reduction algorithms which can be applied for classification in streaming data. Also, the performance comparison for prediction accuracy and time complexity was conducted in various streaming environments such as low dimensional data streams, high dimensional data streams, and data streams with concept drifts. Experimental results showed that incremental least squares formulation (ILS) combined with incremental PCA can be used effectively for classification in streaming data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301124",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Data mining",
      "Data reduction",
      "Data stream mining",
      "Dimension (graph theory)",
      "Dimensionality reduction",
      "Geometry",
      "Linear discriminant analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Principal component analysis",
      "Pure mathematics",
      "Reduction (mathematics)",
      "STREAMS",
      "Streaming data"
    ],
    "authors": [
      {
        "surname": "Park",
        "given_name": "Cheong Hee"
      },
      {
        "surname": "Lee",
        "given_name": "Gyeong-Hoon"
      }
    ]
  },
  {
    "title": "Towards automated computer vision: analysis of the AutoCV challenges 2019",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.04.030",
    "abstract": "We present the results of recent challenges in Automated Computer Vision (AutoCV, renamed here for clarity AutoCV1 and AutoCV2, 2019), which are part of a series of challenge on Automated Deep Learning (AutoDL). These two competitions aim at searching for fully automated solutions for classification tasks in computer vision, with an emphasis on any-time performance. The first competition was limited to image classification while the second one included both images and videos. Our design imposed to the participants to submit their code on a challenge platform for blind testing on five datasets, both for training and testing, without any human intervention whatsoever. Winning solutions adopted deep learning techniques based on already published architectures, such as AutoAugment, MobileNet and ResNet, to reach state-of-the-art performance in the time budget of the challenge (only 20 minutes of GPU time). The novel contributions include strategies to deliver good preliminary results at any time during the learning process, such that a method can be stopped early and still deliver good performance. This feature is key for the adoption of such techniques by data analysts desiring to obtain rapidly preliminary results on large datasets and to speed up the development process. The soundness of our design was verified in several aspects: (1) Little overfitting of the on-line leaderboard providing feedback on 5 development datasets was observed, compared to the final blind testing on the 5 (separate) final test datasets, suggesting that winning solutions might generalize to other computer vision classification tasks; (2) Error bars on the winners’ performance allow us to say with confident that they performed significantly better than the baseline solutions we provided; (3) The ranking of participants according to the any-time metric we designed, namely the Area under the Learning Curve, was different from that of the fixed-time metric, i.e. AUC at the end of the fixed time budget. We released all winning solutions under open-source licenses. At the end of the AutoDL challenge series, all data of the challenge will be made publicly available, thus providing a collection of uniformly formatted datasets, which can serve to conduct further research, particularly on meta-learning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301525",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "CLARITY",
      "Chemistry",
      "Computer science",
      "Deep learning",
      "Feature (linguistics)",
      "Human–computer interaction",
      "Linguistics",
      "Machine learning",
      "Operating system",
      "Overfitting",
      "Philosophy",
      "Process (computing)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Zhengying"
      },
      {
        "surname": "Xu",
        "given_name": "Zhen"
      },
      {
        "surname": "Escalera",
        "given_name": "Sergio"
      },
      {
        "surname": "Guyon",
        "given_name": "Isabelle"
      },
      {
        "surname": "Jacques Junior",
        "given_name": "Julio C.S."
      },
      {
        "surname": "Madadi",
        "given_name": "Meysam"
      },
      {
        "surname": "Pavao",
        "given_name": "Adrien"
      },
      {
        "surname": "Treguer",
        "given_name": "Sebastien"
      },
      {
        "surname": "Tu",
        "given_name": "Wei-Wei"
      }
    ]
  },
  {
    "title": "Markerless detection of ancient rock carvings in the wild: rock art in Vathy, Astypalaia",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.03.026",
    "abstract": "In this paper, we discuss the problem of object detection in a cultural heritage application context. In particular, the objects to be detected are ancient rock carvings, discovered at the archaeological site of Vathy, Astypalaia in Greece. Without the help of a marker or a human expert, the rock carvings are extremely difficult for a visitor of the site to discern from their surroundings. We explore the possibility of using a computational method that could replace the human expert and detect the rock carvings of interest without the aid of a specific marker. We present a dataset of images that is comprised of annotated photographs of the rock carvings, taken in situ and under differing poses and lighting parameters. Two methods for detection are applied; the first method makes use of a supervised, deep learning-based model, while the other relies on feature point-based matching to an annotated template, in the context of which we propose a simple image matching distance. We show that each method is applicable under different conditions, and evaluate their effectiveness with numerical trials.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301045",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Cultural heritage",
      "Feature (linguistics)",
      "Geography",
      "Geology",
      "Geometry",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point (geometry)",
      "Programming language",
      "Rock art",
      "Statistics",
      "Visitor pattern"
    ],
    "authors": [
      {
        "surname": "Tsigkas",
        "given_name": "Giorgos"
      },
      {
        "surname": "Sfikas",
        "given_name": "Giorgos"
      },
      {
        "surname": "Pasialis",
        "given_name": "Anastasios"
      },
      {
        "surname": "Vlachopoulos",
        "given_name": "Andreas"
      },
      {
        "surname": "Nikou",
        "given_name": "Christophoros"
      }
    ]
  },
  {
    "title": "Decomposition and construction of higher-dimensional neighbourhood operations",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.04.015",
    "abstract": "We prove that the 2n-neighbourhood in an n-dimensional digital space is decomposed into the 2 ( n − 1 ) -neighbourhoods in the mutually orthogonal ( n − 1 ) -dimensional digital spaces. This decomposition and construction relation of the neighbourhoods and objects implies that morphological operations in an n-dimensional digital space can be computed as the union of one- and two-dimensional morphological operations on isothetic digital lines and planes intersecting with the digital object in the digital space.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301355",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Combinatorics",
      "Computer science",
      "Decomposition",
      "Digital geometry",
      "Digital image",
      "Ecology",
      "Geometry",
      "Image (mathematics)",
      "Image processing",
      "Mathematical analysis",
      "Mathematics",
      "Neighbourhood (mathematics)",
      "Operating system",
      "Space (punctuation)"
    ],
    "authors": [
      {
        "surname": "Imiya",
        "given_name": "Atsushi"
      }
    ]
  },
  {
    "title": "Graph-based selective rank fusion for unsupervised image retrieval",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.03.032",
    "abstract": "Nowadays, there is a great variety of visual features available for image retrieval tasks. While fusion strategies have been established as a promising alternative, an inherent difficulty in unsupervised scenarios is the task of selecting the features to combine. In this paper, a Graph-based Selective Rank Fusion is proposed. The graph is used to represent the effectiveness estimation of features and the complementarity among them. The selected combinations are defined by the Connected Components of the graph. High-effective retrieval results were achieved through a comprehensive experimental evaluation considering different public datasets, dozens of features and comparisons with related methods. Relative gains up to +54.73% were obtained in relation to the best isolated feature.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301161",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Combinatorics",
      "Complementarity (molecular biology)",
      "Computer science",
      "Data mining",
      "Fusion",
      "Genetics",
      "Graph",
      "Image (mathematics)",
      "Image retrieval",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Rank (graph theory)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Valem",
        "given_name": "Lucas Pascotti"
      },
      {
        "surname": "Pedronette",
        "given_name": "Daniel Carlos Guimarães"
      }
    ]
  },
  {
    "title": "Saliency detection using a deep conditional random field network",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107266",
    "abstract": "Saliency detection has made remarkable progress along with the development of deep learning. While how to integrate the low-level intrinsic context with high-level semantic information to keep the object boundary sharp and restrain the background noise is still a challenging problem. Many attempts on network structures and refinement strategies have been explored, such as using Conditional Random Field (CRF) to improve the accuracy of saliency map, but it is independent from the deep network and cannot be trained end-to-end. To tackle this issue, we propose a novel Deep Conditional Random Field network (DCRF) to take both deep feature and neighbor information into consideration. First, Multi-scale Feature Extraction Module (MFEM) is adopted to capture the low level texture and high level semantic features, multi-stacks of deconvolution layers are employed to improve the spatial resolution of deep layers. Then we employ Backward Optimization Module (BOM) to guide shallower layers by high-level location and shape information derived from deeper layers, which intrinsically enhance the representational capacity of low-level features. Finally, a Deep Conditional Random Field Module (DCRFM) with unary and pairwise potentials is designed to concentrate on spatial neighbor relations to obtain a compact and uniformed saliency map. Extensive experimental results on 5 datasets in terms of 6 evaluation metrics demonstrate that the proposed method achieves state-of-the-art performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300716",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "CRFS",
      "Combinatorics",
      "Computer science",
      "Conditional random field",
      "Context (archaeology)",
      "Deconvolution",
      "Deep learning",
      "Feature (linguistics)",
      "Field (mathematics)",
      "Linguistics",
      "Mathematics",
      "Pairwise comparison",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pure mathematics",
      "Random field",
      "Spatial analysis",
      "Statistics",
      "Unary operation"
    ],
    "authors": [
      {
        "surname": "Qiu",
        "given_name": "Wenliang"
      },
      {
        "surname": "Gao",
        "given_name": "Xinbo"
      },
      {
        "surname": "Han",
        "given_name": "Bing"
      }
    ]
  },
  {
    "title": "Joint image deblurring and matching with feature-based sparse representation prior",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107300",
    "abstract": "Image matching aims to find a similar area of the small image in the large image, which is one of the key steps in image fusion and vision-based navigation; however, most matching methods perform poorly when the images to be matched are blurred. Traditional approaches for blurred image matching usually follow a two-stage framework - first resorting to image deblurring and then performing image matching with the recovered image. However, the matching accuracy of these methods often suffers greatly from the deficiency of image deblurring. Recently, a joint image deblurring and matching method that utilizes the sparse representation prior to exploit the correlation between deblurring and matching was proposed to address this problem and found to obtain a higher matching accuracy. Yet, that technique is not efficient when the image is seriously blurred, and the method’s time complexity is excessive. In this paper, we propose a joint image deblurring and matching approach with a feature-based sparse representation prior. Our approach utilizes two-directional two-dimensional (2D)2PCA to extract feature vectors from images and obtains a sparse representation prior in a robust feature space rather than the original pixel space, thus mitigating the influence of image blur. Moreover, the reduction in the feature dimension can also increase the computational efficiency. Extensive experiments show that our approach significantly outperforms state-of-the-art approaches in terms of both accuracy and speed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301047",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deblurring",
      "Feature (linguistics)",
      "Feature detection (computer vision)",
      "Feature extraction",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Sparse approximation",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Juncai"
      },
      {
        "surname": "Shao",
        "given_name": "Yuanjie"
      },
      {
        "surname": "Sang",
        "given_name": "Nong"
      },
      {
        "surname": "Gao",
        "given_name": "Changxin"
      }
    ]
  },
  {
    "title": "Learning variable-length representation of words",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107306",
    "abstract": "A standard word embedding algorithm, such as ‘word2vec’, embeds each word as a dense vector of a preset dimensionality, the components of which are learned by maximizing the likelihood of predicting the context around it. However, as an inherent linguistic phenomenon, it is evident that there is a varying degree of difficulty in identifying words from their contexts. This suggests that a variable granularity in word vector representation may be useful to obtain sparser and more compressed word representations, requiring less storage space. To that end, in this paper, we propose a word vector training algorithm that uses a variable number of components to represent words. Given a text collection of documents, our algorithm, similar to the skip-gram approach of word2vec, learns to predict the context of a word given the current instance of a word. However, in contrast to skip-gram, which uses a static number of dimensions for each word vector, we propose to dynamically increase the dimensionality as a stochastic function of the prediction error. Our experiments with standard test collections demonstrate that our word representation method is able to achieve comparable (and sometimes even better) effectiveness than skip-gram word2vec, using a significantly smaller number of parameters (achieving compression ratio of around 65%).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301102",
    "keywords": [
      "Artificial intelligence",
      "Bag-of-words model",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Curse of dimensionality",
      "Embedding",
      "Geometry",
      "Law",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Variable (mathematics)",
      "Word (group theory)",
      "Word embedding",
      "Word2vec"
    ],
    "authors": [
      {
        "surname": "Ganguly",
        "given_name": "Debasis"
      }
    ]
  },
  {
    "title": "Unsupervised generation of polygonal approximations based on the convex hull",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.04.014",
    "abstract": "The present paper proposes a new non-optimal but unsupervised algorithm, called ICT-RDP, for generation of polygonal approximations based on the convex hull. Firstly, the new algorithm takes into account the convex hull of the 2D closed curves or contours to select a set of initial points; secondly, the significance levels of the contour points are computed using a symmetric version of the well-known Ramer, Douglas-Peucker algorithm; and, finally, a thresholding process is applied to obtain the vertices or dominant points of the polygonal approximation. Since the convex hull can select many initial points in rounded parts of the contour, an additional deletion process is required to remove quasi-collinear dominant points. Furthermore, an additional improvement process is applied to shift the dominant points in order to increase the quality of the polygonal approximation. Experiments performed on a public available dataset show that the new proposal outperforms other unsupervised algorithms for generation of polygonal approximations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301331",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Approximations of π",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Convex hull",
      "Engineering",
      "Geometry",
      "Hull",
      "Image (mathematics)",
      "Marine engineering",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Polygonal chain",
      "Process (computing)",
      "Programming language",
      "Regular polygon",
      "Set (abstract data type)",
      "Thresholding"
    ],
    "authors": [
      {
        "surname": "Fernández García",
        "given_name": "Nicolás Luis"
      },
      {
        "surname": "Martínez",
        "given_name": "Luis Del-Moral"
      },
      {
        "surname": "Poyato",
        "given_name": "Ángel Carmona"
      },
      {
        "surname": "Madrid Cuevas",
        "given_name": "Francisco José"
      },
      {
        "surname": "Carnicer",
        "given_name": "Rafael Medina"
      }
    ]
  },
  {
    "title": "Explaining away results in accurate and tolerant template matching",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107337",
    "abstract": "Recognising and locating image patches or sets of image features is an important task underlying much work in computer vision. Traditionally this has been accomplished using template matching. However, template matching is notoriously brittle in the face of changes in appearance caused by, for example, variations in viewpoint, partial occlusion, and non-rigid deformations. This article tests a method of template matching that is more tolerant to such changes in appearance and that can, therefore, more accurately identify image patches. In traditional template matching the comparison between a template and the image is independent of the other templates. In contrast, the method advocated here takes into account the evidence provided by the image for the template at each location and the full range of alternative explanations represented by the same template at other locations and by other templates. Specifically, the proposed method of template matching is performed using a form of probabilistic inference known as “explaining away”. The algorithm used to implement explaining away has previously been used to simulate several neurobiological mechanisms, and been applied to image contour detection and pattern recognition tasks. Here it is applied for the first time to image patch matching, and is shown to produce superior results in comparison to the current state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301400",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Economics",
      "Face (sociological concept)",
      "Image (mathematics)",
      "Management",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Social science",
      "Sociology",
      "Statistics",
      "Task (project management)",
      "Template",
      "Template matching"
    ],
    "authors": [
      {
        "surname": "Spratling",
        "given_name": "M.W."
      }
    ]
  },
  {
    "title": "OccGAN: Semantic image augmentation for driving scenes",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.06.011",
    "abstract": "Difficult images with complicated environments and occlusion have significant impacts on the performance of algorithms. They obey the long-tail distribution in the widely used datasets, which results in rare samples being overwhelmed during training. This paper presents a new approach to generate plausible occluded images with annotation as a kind of data augmentation with scenes semantics. To achieve this task, we proposed the Occlusion-based Generative Adversarial Network (OccGAN) structure, which consists of a Rationality Module and an Authenticity Module. The Rationality Module generated preliminary occluded samples under the guidance of prior semantic knowledge. And the Authenticity Module is a generative adversarial structure to ensure the reality of the produced images. Qualitative results of the visualization process are given to verify the ablation study. Experiments on the semantic segmentation task indicate that several state-of-the-art algorithms combined with our OccGAN such as DRN, Deeplabv3+, PSPNet and ResNet-38, have boosts on IoU class scores and IoU category scores successfully.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302294",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Economics",
      "Generative adversarial network",
      "Generative grammar",
      "Image (mathematics)",
      "Machine learning",
      "Management",
      "Natural language processing",
      "Operating system",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Programming language",
      "Segmentation",
      "Semantics (computer science)",
      "Task (project management)",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yidong"
      },
      {
        "surname": "Mo",
        "given_name": "Lisha"
      },
      {
        "surname": "Ma",
        "given_name": "Huimin"
      },
      {
        "surname": "Yuan",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "Multilabel naïve Bayes classification considering label dependence",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.06.021",
    "abstract": "Multilabel classification is the task of assigning relevant labels to an instance, and it has received considerable attention in recent years. This task can be performed by extending a single-label classifier, such as the naïve Bayes classifier, to utilize the useful relations among labels for achieving better multilabel classification accuracy. However, the conventional multilabel naïve Bayes classifier treats each label independently and hence neglects the relations among labels, resulting in degenerated accuracy. We propose a new multilabel naïve Bayes classifier that considers the relations or dependence among labels. Experimental results show that the proposed method outperforms conventional multilabel classifiers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302397",
    "keywords": [
      "Artificial intelligence",
      "Bayes classifier",
      "Bayes error rate",
      "Bayes' theorem",
      "Bayesian probability",
      "Classifier (UML)",
      "Computer science",
      "Machine learning",
      "Multi-label classification",
      "Naive Bayes classifier",
      "Pattern recognition (psychology)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Hae-Cheon"
      },
      {
        "surname": "Park",
        "given_name": "Jin-Hyeong"
      },
      {
        "surname": "Kim",
        "given_name": "Dae-Won"
      },
      {
        "surname": "Lee",
        "given_name": "Jaesung"
      }
    ]
  },
  {
    "title": "Receiver operating characteristic curves with an indeterminacy zone",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.04.035",
    "abstract": "This work extends Receiver Operating Characteristic (ROC) curve to the situation where some cases, falling in an intermediate ”indeterminacy zone” of the predictor, are not classified. It addresses two challenges: definition of sensitivity and specificity bounds for this case; and summarization of the large number of possibilities arising from different choices of indeterminacy zones.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301665",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Automatic summarization",
      "Computer science",
      "Electronic engineering",
      "Engineering",
      "Epistemology",
      "Indeterminacy (philosophy)",
      "Mathematics",
      "Philosophy",
      "Receiver operating characteristic",
      "Sensitivity (control systems)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Parmigiani",
        "given_name": "Giovanni"
      }
    ]
  },
  {
    "title": "Key protected classification for collaborative learning",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107327",
    "abstract": "Large-scale datasets play a fundamental role in training deep learning models. However, dataset collection is difficult in domains that involve sensitive information. Collaborative learning techniques provide a privacy-preserving solution, by enabling training over a number of private datasets that are not shared by their owners. However, recently, it has been shown that the existing collaborative learning frameworks are vulnerable to an active adversary that runs a generative adversarial network (GAN) attack. In this work, we propose a novel classification model that is resilient against such attacks by design. More specifically, we introduce a key-based classification model and a principled training scheme that protects class scores by using class-specific private keys, which effectively hide the information necessary for a GAN attack. We additionally show how to utilize high dimensional keys to improve the robustness against attacks without increasing the model complexity. Our detailed experiments demonstrate the effectiveness of the proposed technique. Source code will be made available at https://github.com/mbsariyildiz/key-protected-classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301308",
    "keywords": [
      "Adversarial system",
      "Adversary",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Class (philosophy)",
      "Code (set theory)",
      "Computer science",
      "Computer security",
      "Deep learning",
      "Gene",
      "Generative adversarial network",
      "Generative grammar",
      "Key (lock)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Programming language",
      "Robustness (evolution)",
      "Scheme (mathematics)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Sariyildiz",
        "given_name": "Mert Bulent"
      },
      {
        "surname": "Cinbis",
        "given_name": "Ramazan Gokberk"
      },
      {
        "surname": "Ayday",
        "given_name": "Erman"
      }
    ]
  },
  {
    "title": "Multi-label feature selection with shared common mode",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107344",
    "abstract": "Multi-label feature selection plays an indispensable role in multi-label learning, which eliminates irrelevant and redundant features while retaining relevant features. Most of existing multi-label feature selection methods employ two strategies to construct feature selection models: extracting label correlations to guide feature selection process and maintaining the consistency between the feature matrix and the reduced low-dimensional feature matrix. However, the data information is described by two data matrices: the feature matrix and the label matrix. Previous methods devote attention to either of the two data matrices. To address this issue, we propose a novel feature selection method named Feature Selection considering Shared Common Mode between features and labels (SCMFS). First, we utilize Coupled Matrix Factorization (CMF) to extract the shared common mode between the feature matrix and the label matrix, considering the comprehensive data information in the two matrices. Additionally, Non-negative Matrix Factorization (NMF) is adopted to enhance the interpretability for feature selection. Extensive experiments are implemented on fifteen real-world benchmark data sets for multiple evaluation metrics, the experimental results demonstrate the classification superiority of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301473",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Composite material",
      "Computer science",
      "Consistency (knowledge bases)",
      "Data mining",
      "Dimensionality reduction",
      "Eigenvalues and eigenvectors",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature selection",
      "Geodesy",
      "Geography",
      "Interpretability",
      "Linguistics",
      "Machine learning",
      "Materials science",
      "Matrix (chemical analysis)",
      "Matrix decomposition",
      "Minimum redundancy feature selection",
      "Non-negative matrix factorization",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Liang"
      },
      {
        "surname": "Li",
        "given_name": "Yonghao"
      },
      {
        "surname": "Gao",
        "given_name": "Wanfu"
      },
      {
        "surname": "Zhang",
        "given_name": "Ping"
      },
      {
        "surname": "Hu",
        "given_name": "Juncheng"
      }
    ]
  },
  {
    "title": "A text-based visual context modulation neural model for multimodal machine translation",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.06.010",
    "abstract": "We introduce a novel multimodal machine translation model that integrates image features modulated by its caption. Generally, images contain vastly more information rather than just their description. Furthermore, in multimodal machine translation task, feature maps are commonly extracted from pre-trained network for objects. Therefore, it is not appropriate to utilize these feature map directly. To extract the visual features associated with the text, we design a modulation network based on the textual information from the encoder and visual information from the pretrained CNN. However, because multimodal translation data is scarce, using overly complicated models could result in poor performance. For simplicity, we apply a feature-wise multiplicative transformation. Therefore, our model is a modular trainable network embedded in the architecture in existing multimodal translation models. We verified our model by conducting experiments on the Transformer model with the Multi30k dataset and evaluating translation quality using the BLEU and METEOR metrics. In general, our model was an improvements over a text-based model and other existing models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302282",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Encoder",
      "Feature (linguistics)",
      "Gene",
      "Linguistics",
      "Machine learning",
      "Machine translation",
      "Messenger RNA",
      "Modular design",
      "Natural language processing",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Speech recognition",
      "Transformer",
      "Translation (biology)",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Kwon",
        "given_name": "Soonmo"
      },
      {
        "surname": "Go",
        "given_name": "Byung-Hyun"
      },
      {
        "surname": "Lee",
        "given_name": "Jong-Hyeok"
      }
    ]
  },
  {
    "title": "Uncertainty-aware integration of local and flat classifiers for food recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.06.013",
    "abstract": "Food image recognition has recently attracted the attention of many researchers, due to the challenging problem it poses, the ease collection of food images, and its numerous applications to health and leisure. In real applications, it is necessary to analyze and recognize thousands of different foods. For this purpose, we propose a novel prediction scheme based on a class hierarchy that considers local classifiers, in addition to a flat classifier. In order to make a decision about which approach to use, we define different criteria that take into account both the analysis of the Epistemic Uncertainty estimated from the ‘children’ classifiers and the prediction from the ‘parent’ classifier. We evaluate our proposal using three Uncertainty estimation methods, tested on two public food datasets. The results show that the proposed method reduces parent-child error propagation in hierarchical schemes and improves classification results compared to the single flat classifier, meanwhile maintains good performance regardless the Uncertainty estimation method chosen.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302312",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Propagation of uncertainty",
      "Random subspace method"
    ],
    "authors": [
      {
        "surname": "Aguilar",
        "given_name": "Eduardo"
      },
      {
        "surname": "Radeva",
        "given_name": "Petia"
      }
    ]
  },
  {
    "title": "Discriminative block-diagonal covariance descriptors for image set classification",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.05.018",
    "abstract": "Image set classification has recently received much attention due to its various applications in pattern recognition and computer vision. To compare and match image sets, the major challenges are to devise an effective and efficient representation and to define a measure of similarity between image sets. In this paper, we propose a method for representing image sets based on block-diagonal Covariance Descriptors (CovDs). In particular, the proposed image set representation is in the form of non-singular covariance matrices, also known as Symmetric Positive Definite (SPD) matrices, that lie on Riemannian manifold. By dividing each image of an image set into square blocks of the same size, we compute the corresponding block CovDs instead of the global one. Taking the relative discriminative power of these block CovDs into account, a block-diagonal SPD matrix can be constructed to achieve a better discriminative capability. We extend the proposed approach to work with bidirectional CovDs and achieve a further boost in performance. The resulting block-diagonal SPD matrices combined with Riemannian metrics are shown to provide a powerful basis for image set classification. We perform an extensive evaluation on four datasets for several image set classification tasks. The experimental results demonstrate the effectiveness and efficiency of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301951",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Combinatorics",
      "Computer science",
      "Contextual image classification",
      "Covariance",
      "Covariance intersection",
      "Covariance matrix",
      "Diagonal",
      "Discriminative model",
      "Estimation of covariance matrices",
      "Geometry",
      "Image (mathematics)",
      "Law",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Set (abstract data type)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Ren",
        "given_name": "Jieyi"
      },
      {
        "surname": "Wu",
        "given_name": "Xiao-jun"
      },
      {
        "surname": "Kittler",
        "given_name": "Josef"
      }
    ]
  },
  {
    "title": "Cascade of encoder-decoder CNNs with learned coordinates regressor for robust facial landmarks detection",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.10.012",
    "abstract": "Convolutional Neural Nets (CNNs) have become the reference technology for many computer vision problems. Although CNNs for facial landmark detection are very robust, they still lack accuracy when processing images acquired in unrestricted conditions. In this paper we investigate the use of a cascade of Neural Net regressors to increase the accuracy of the estimated facial landmarks. To this end we append two encoder-decoder CNNs with the same architecture. The first net produces a set of heatmaps with a rough estimation of landmark locations. The second, trained with synthetically generated occlusions, refines the location of ambiguous and occluded landmarks. Finally, a densely connected layer with shared weights among all heatmaps, accurately regresses the landmark coordinates. The proposed approach achieves state-of-the-art results in 300W, COFW and WFLW that are widely considered the most challenging public data sets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519302910",
    "keywords": [
      "Artificial intelligence",
      "Cascade",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Encoder",
      "Face (sociological concept)",
      "Landmark",
      "Operating system",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Valle",
        "given_name": "Roberto"
      },
      {
        "surname": "Buenaposada",
        "given_name": "José M."
      },
      {
        "surname": "Baumela",
        "given_name": "Luis"
      }
    ]
  },
  {
    "title": "Vector score alpha integration for classifier late fusion",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.05.014",
    "abstract": "Alpha integration is a family of integrators that encompasses many classic fusion operators (e.g., mean, product, minimum, maximum) as particular cases. This paper proposes vector score integration (VSI), a new alpha integration method for late fusion of multiple classifiers considering the joint effect of all the classes of the multi-class problem. Theoretical derivations to optimize the parameters of VSI for achieving the minimum probability of error are provided. VSI was applied to two classification tasks using electroencephalographic signals. The first task was the automatic stage classification of a neuropsychological test performed by epileptic subjects and the second one was the classification of sleep stages from apnea patients. Four single classifiers (linear and quadratic discriminant analysis, naive Bayes, and random forest) and three competitive fusion methods were estimated for comparison: mean, majority voting, and separated score integration (SSI). SSI is based on alpha integration, but unlike the proposed method, it considers the scores from each class in isolation, not accounting for possible dependencies among scores corresponding to different classes. VSI was able to optimally combine the results from all the single classifiers, in terms of accuracy and kappa coefficient, and outperformed the results of the other fusion methods in both applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301860",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Machine learning",
      "Mathematics",
      "Naive Bayes classifier",
      "Pattern recognition (psychology)",
      "Quadratic classifier",
      "Random forest",
      "Statistics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Safont",
        "given_name": "Gonzalo"
      },
      {
        "surname": "Salazar",
        "given_name": "Addisson"
      },
      {
        "surname": "Vergara",
        "given_name": "Luis"
      }
    ]
  },
  {
    "title": "End-to-end training of CNN ensembles for person re-identification",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107319",
    "abstract": "We propose an end-to-end ensemble method for person re-identification (ReID) to address the problem of overfitting in discriminative models. These models are known to converge easily, but they are biased to the training data in general and may produce a high model variance, which is known as overfitting. The ReID task is more prone to this problem due to the large discrepancy between training and test distributions. To address this problem, our proposed ensemble learning framework produces several diverse and accurate base learners in a single DenseNet. Since most of the costly dense blocks are shared, our method is computationally efficient, which makes it favorable compared to the conventional ensemble models. Experiments on several benchmark datasets demonstrate that our method achieves state-of-the-art results. Noticeable performance improvements, especially on relatively small datasets, indicate that the proposed method deals with the overfitting problem effectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301229",
    "keywords": [
      "Accounting",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Biology",
      "Botany",
      "Business",
      "Computer science",
      "Discriminative model",
      "Economics",
      "Ensemble forecasting",
      "Ensemble learning",
      "Geodesy",
      "Geography",
      "Identification (biology)",
      "Machine learning",
      "Management",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Task (project management)",
      "Training set",
      "Variance (accounting)"
    ],
    "authors": [
      {
        "surname": "Serbetci",
        "given_name": "Ayse"
      },
      {
        "surname": "Akgul",
        "given_name": "Yusuf Sinan"
      }
    ]
  },
  {
    "title": "Remote sensing image segmentation using geodesic-kernel functions and multi-feature spaces",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107333",
    "abstract": "Image representation is the key factor influencing the accuracy of remote sensing image segmentation. Traditional algorithms rely on the pixel-wise characteristics exhibited in the feature space. They introduce spatial information by establishing the connections between neighboring pixels in the neighborhood system. But the spectral-spatial features cannot be well expressed. In this paper, a Riemannian manifold space is introduced to express the contextual information by jointly mapping the spectral features of a pixel and its neighboring ones on to it. To benefit from the expression ability and geometric properties of the Riemannian manifold, a data submanifold and a parameter submanifold are established to depict the characteristics of the detected image and all possible segmentation results. On the parameter submanifold, only points representing objects of the current segmentation are active. Then distance between a point on the data submanifold and an active point on the parameter submanifold is measured by a geodesic-kernel function. Consequently, four geodesic-kernel function-based manifold projection criteria are proposed to explore the complementation between features expressed in different feature spaces. Experiments on synthetic and real remote sensing images demonstrated that the proposed geodesic-kernel function-based manifold projection algorithm outperforms traditional ones and features expressed in different feature spaces did contain some complementary information.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301369",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Geodesic",
      "Image (mathematics)",
      "Image segmentation",
      "Kernel (algebra)",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pure mathematics",
      "Scale-space segmentation",
      "Segmentation",
      "Segmentation-based object categorization"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Xuemei"
      },
      {
        "surname": "Wang",
        "given_name": "Haijian"
      },
      {
        "surname": "Wu",
        "given_name": "Jun"
      },
      {
        "surname": "Li",
        "given_name": "Yu"
      },
      {
        "surname": "Zhao",
        "given_name": "Shijie"
      }
    ]
  },
  {
    "title": "Palm vein recognition based on competitive coding scheme using multi-scale local binary pattern with ant colony optimization",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.05.030",
    "abstract": "Among the various biometric traits that can be extracted from the hand, the palm vein structure that represents a reliable and secure source for identifying and/or verifying the identity of a person. Several recognition methods were proposed in the literature exploiting this modality; among them, the attractive approaches based on a competitive coding. Aiming to further improve the performance of these approaches, this paper presents a novel palm vein recognition method for personal authentication and identification based on a competitive coding scheme using Multi-scale local binary pattern (MLBP) with Ant colony optimization (ACO). ACO allows to override potential blocking points related to image quality or contrast problems that can be encountered with images from the Near infrared spectral band. The pre-processed images will be then sorted with a competitive coding scheme using MLBP; where the final image will be composed of the winning code from the different MLBP images. The matching process for making-decision is then performed using Kullback-Leibler divergence and Jaccard distance. The experimental results obtained on MS-PolyU database has shown that the proposed method achieves improved performances for both identification and verification modes up to 99.64% in terms of CIR for the identification and 0.00078% in terms of EER for the verification; and also outperforms the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302075",
    "keywords": [
      "ANT",
      "Ant colony optimization algorithms",
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Cartography",
      "Coding (social sciences)",
      "Computer network",
      "Computer science",
      "Geography",
      "Mathematical analysis",
      "Mathematics",
      "Palm",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)",
      "Scheme (mathematics)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Aberni",
        "given_name": "Yassir"
      },
      {
        "surname": "Boubchir",
        "given_name": "Larbi"
      },
      {
        "surname": "Daachi",
        "given_name": "Boubaker"
      }
    ]
  },
  {
    "title": "GAN-based person search via deep complementary classifier with center-constrained Triplet loss",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107350",
    "abstract": "This paper addresses the person search task, which is a computer vision technology that finds the location of a pedestrian and retrieves it in a video taken by a single camera or multiple cameras. This task is much more challenging than the conventional settings for person re-identification or pedestrian detection since the search is susceptible to factors such as different resolutions, similar pedestrians, lighting, viewing angles and occlusion. Moreover, the person search task is a typical big data-small sample problem because each pedestrian only has a few images. It is difficult for the model to learn the discriminant features of pedestrians with a small quantity of pedestrian data. This paper proposes a framework for person search that uses the original training set without collecting extra data by implementing a generative adversarial network (GAN) to generate unlabeled samples. We propose a deep complementary classifier for pedestrian detection to leverage complementary object regions for pedestrian/non-pedestrian classification. In the re-identification section, we propose a center-constrained triplet loss that avoids the complicated triplet selection of the triplet loss and simultaneously pushes away all the distances of rather similar negative centers and the positive center. Experiments show that the GAN-generated data can effectively help to improve the discriminating ability of the CNN model. On the two large-scale datasets, CUHK-SYSU and PRW, we achieve a performance improvement over the baseline CNN. We additionally apply the proposed center-constrained triplet loss and complementary classifiers in the training model, and we achieve mAP improvements over the original method of +1.9% on CUHK-SYSU and +2.5% on PRW.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301539",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Generative grammar",
      "Generative model",
      "Leverage (statistics)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Pedestrian",
      "Pedestrian detection",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Yao",
        "given_name": "Rui"
      },
      {
        "surname": "Gao",
        "given_name": "Cunyuan"
      },
      {
        "surname": "Xia",
        "given_name": "Shixiong"
      },
      {
        "surname": "Zhao",
        "given_name": "Jiaqi"
      },
      {
        "surname": "Zhou",
        "given_name": "Yong"
      },
      {
        "surname": "Hu",
        "given_name": "Fuyuan"
      }
    ]
  },
  {
    "title": "Knowledge graph based methods for record linkage",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.05.025",
    "abstract": "Nowadays, it is common in Historical Demography the use of individual-level data as a consequence of a predominant life-course approach for the understanding of the demographic behaviour, family transition, mobility, etc. Advanced record linkage is key since it allows increasing the data complexity and its volume to be analyzed. However, current methods are constrained to link data from the same kind of sources. Knowledge graph are flexible semantic representations, which allow to encode data variability and semantic relations in a structured manner. In this paper we propose the use of knowledge graph methods to tackle record linkage tasks. The proposed method, named WERL, takes advantage of the main knowledge graph properties and learns embedding vectors to encode census information. These embeddings are properly weighted to maximize the record linkage performance. We have evaluated this method on benchmark data sets and we have compared it to related methods with stimulating and satisfactory results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301823",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Demography",
      "ENCODE",
      "Embedding",
      "Gene",
      "Graph",
      "Knowledge graph",
      "Linkage (software)",
      "Linked data",
      "Population",
      "Record linkage",
      "Semantic Web",
      "Sociology",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Gautam",
        "given_name": "Bhaskar"
      },
      {
        "surname": "Ramos Terrades",
        "given_name": "Oriol"
      },
      {
        "surname": "Pujadas-Mora",
        "given_name": "Joana Maria"
      },
      {
        "surname": "Valls",
        "given_name": "Miquel"
      }
    ]
  },
  {
    "title": "Ensembling complex network ‘perspectives’ for mild cognitive impairment detection with artificial neural networks",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.06.001",
    "abstract": "In this paper, we propose a novel method for mild cognitive impairment detection based on jointly exploiting the complex network and the neural network paradigm. In particular, the method is based on ensembling different brain structural “perspectives” with artificial neural networks. On one hand, these perspectives are obtained with complex network measures tailored to describe the altered brain connectivity. In turn, the brain reconstruction is obtained by combining diffusion-weighted imaging (DWI) data to tractography algorithms. On the other hand, artificial neural networks provide a means to learn a mapping from topological properties of the brain to the presence or absence of cognitive decline. The effectiveness of the method is studied on a well-known benchmark data set in order to evaluate if it can provide an automatic tool to support the early disease diagnosis. Also, the effects of balancing issues are investigated to further assess the reliability of the complex network approach to DWI data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302130",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Cognition",
      "Complex network",
      "Computer science",
      "Data set",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Neuroscience",
      "Physics",
      "Power (physics)",
      "Programming language",
      "Psychology",
      "Quantum mechanics",
      "Reliability (semiconductor)",
      "Set (abstract data type)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Lella",
        "given_name": "Eufemia"
      },
      {
        "surname": "Vessio",
        "given_name": "Gennaro"
      }
    ]
  },
  {
    "title": "Recognizing actions in images by fusing multiple body structure cues",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107341",
    "abstract": "Although Convolutional Neural Networks (CNNs) have made substantial improvements in many computer vision tasks, there remains room for improvements in image-based action recognition due to the limited capability to exploit the body structure information.In this work, we propose a unified deep model to explicitly explore body structure information and fuse multiple body structure cues for robust action recognition in images.In order to fully explore the body structure information, we design the Body Structure Exploration sub-network.It generates two novel body structure cues, Structural Body Parts and Limb Angle Descriptor, which capture structure information of human bodies from the global and local perspectives respectively. And then, we design the Action Classification sub-network to fuse the predictions from multiple body structure cues to obtain precise results. Moreover, we integrate the two sub-networks into a unified model by sharing the bottom convolutional layers, which improves the computational efficiency in both training and testing stages. We comprehensively evaluate our network on the challenging image-based human action datasets, Pascal VOC 2012 Action and Stanford40. Our approach achieves 93.5% and 93.8% mAP respectively, which outperforms all recent approaches in this field.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301448",
    "keywords": [
      "Action (physics)",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Convolutional neural network",
      "Electrical engineering",
      "Engineering",
      "Exploit",
      "Fuse (electrical)",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yang"
      },
      {
        "surname": "Li",
        "given_name": "Kan"
      },
      {
        "surname": "Wang",
        "given_name": "Xinxin"
      }
    ]
  },
  {
    "title": "Using an autoencoder in the design of an anomaly detector for smart manufacturing",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.06.008",
    "abstract": "According to the smart manufacturing paradigm, the analysis of assets’ time series with a machine learning approach can effectively prevent unplanned production downtimes by detecting assets’ anomalous operational conditions. To support smart manufacturing operators with no data science background, we propose an anomaly detection approach based on deep learning and aimed at providing a manageable machine learning pipeline and easy to interpret outcome. To do so we combine (i) an autoencoder, a deep neural network able to produce an anomaly score for each provided time series, and (ii) a discriminator based on a general heuristics, to automatically discern anomalies from regular instances. We prove the convenience of the proposed approach by comparing its performances against isolation forest with different case studies addressing industrial laundry assets’ power consumption and bearing vibrations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302269",
    "keywords": [
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Computer science",
      "Computer vision",
      "Condensed matter physics",
      "Detector",
      "Pattern recognition (psychology)",
      "Physics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Alfeo",
        "given_name": "Antonio L."
      },
      {
        "surname": "Cimino",
        "given_name": "Mario G.C.A."
      },
      {
        "surname": "Manco",
        "given_name": "Giuseppe"
      },
      {
        "surname": "Ritacco",
        "given_name": "Ettore"
      },
      {
        "surname": "Vaglini",
        "given_name": "Gigliola"
      }
    ]
  },
  {
    "title": "Heuristic algorithms for diversity-aware balanced multi-way number partitioning",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.05.022",
    "abstract": "Number partitioning is a classic problem in artificial intelligence. And balanced multi-way number partitioning problem (BMNP) aims to partition a set of numbers into multiple subsets, such that (1) each subset contains the same number of numbers and (2) the subset sums are equal. The BMNP problem has various applications in real world scenarios, including task allocation, CPU scheduling, file placement in data center, multi-source data processing, etc. In this paper, we consider the problem of diversity-aware balanced multi-way number partitioning (DBMNP). DBMNP differs from BMNP, in that each number is associated with a type attribute. In addition to the two goals of BMNP, DBMNP also requires that the types of numbers in each subset are as diversified as possible. To solve the problem, we propose three heuristic algorithms to minimize the difference between subset sums and at the same time maximize diversify of each subset. Extensive experiments are conducted to evaluate the effectiveness of our proposed algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302026",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Heuristic",
      "Mathematical optimization",
      "Mathematics",
      "Partition (number theory)",
      "Scheduling (production processes)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Jilian"
      },
      {
        "surname": "Wei",
        "given_name": "Kaimin"
      },
      {
        "surname": "Deng",
        "given_name": "Xuelian"
      }
    ]
  },
  {
    "title": "A new approach for classification skin lesion based on transfer learning, deep learning, and IoT system",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.05.019",
    "abstract": "Melanoma skin cancer is one of the most common diseases in the world. It is essential to diagnose melanoma at an early stage. Visual inspection during the medical examination of skin lesions is not a simple task, as there is a similarity between lesions. Also, medical experience and disposition can result in inaccurate diagnoses. Technologies such as the Internet of Things (IoT) have helped to create effective health systems. Doctors can use them anywhere, with the guarantee that more people can be diagnosed without prejudice to subjective factors. Transfer Learning and Deep Learning are increasingly significant in the clinical diagnosis of different diseases. This work proposes the use of Transfer Learning and Deep Learning in an IoT system to assist doctors in the diagnosis of common skin lesions, typical nevi, and melanoma. This work uses Convolutional Neural Networks (CNNs) as resource extractors. The CNN models used were: Visual Geometry Group (VGG), Inception, Residual Networks (ResNet), Inception-ResNet, Extreme Inception (Xception), MobileNet, Dense Convolutional Network (DenseNet), and Neural Architecture Search Network (NASNet). For the classification of injuries, the Bayes, Support Vector Machines (SVM), Random Forest (RF), Perceptron Multilayer (MLP), and the K-Nearest Neighbors (KNN) classifiers are used. This study used two datasets: the first provided by the International Skin Imaging Collaboration (ISIC) at the International Biomedical Imaging Symposium (ISBI); the second is PH 2. For ISBI-ISIC, this study examined lesions between nevi and melanomas. In PH 2, this work analyzed the diagnosis based on lesions of common nevus, atypical nevi, and melanomas. The DenseNet201 extraction model, combined with the KNN classifier achieved an accuracy of 96.805% for the ISBI-ISIC dataset and 93.167% for the PH 2. Thus, an approach focused on the IoT system is reliable and efficient for doctors who assist in the diagnosis of skin lesions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301987",
    "keywords": [
      "Artificial intelligence",
      "Cancer",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Internal medicine",
      "Machine learning",
      "Medical diagnosis",
      "Medicine",
      "Pattern recognition (psychology)",
      "Radiology",
      "Skin cancer",
      "Support vector machine",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Rodrigues",
        "given_name": "Douglas de A."
      },
      {
        "surname": "Ivo",
        "given_name": "Roberto F."
      },
      {
        "surname": "Satapathy",
        "given_name": "Suresh Chandra"
      },
      {
        "surname": "Wang",
        "given_name": "Shuihua"
      },
      {
        "surname": "Hemanth",
        "given_name": "Jude"
      },
      {
        "surname": "Filho",
        "given_name": "Pedro P. Rebouças"
      }
    ]
  },
  {
    "title": "Adaptive learning of minority class prior to minority oversampling",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.05.020",
    "abstract": "The minority oversampling techniques have substantiated their appropriateness and utility in the domain of class-imbalance learning. However, this does not affirm the true class of the synthetic minority points. In this work, Adaptive Learning of Minority Class prior to Minority Oversampling (ALMCMO), we work towards bridging this gap by estimating the minority set before oversampling the synthetic points. We estimate a varying and adaptive volume of minority space around the minority points. We aim to guarantee the class-memberships of the synthetic minority points by sampling them from the estimated minority spaces. In our empirical study, we have used six comparing methods, 23 datasets and two classifiers. The results indicate the certain superiority of the proposed method over the six competing schemes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301999",
    "keywords": [
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Class (philosophy)",
      "Computer network",
      "Computer science",
      "Machine learning",
      "Mathematics",
      "Oversampling",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Sadhukhan",
        "given_name": "Payel"
      },
      {
        "surname": "Palit",
        "given_name": "Sarbani"
      }
    ]
  },
  {
    "title": "Imbalance-XGBoost: leveraging weighted and focal losses for binary label-imbalanced classification with XGBoost",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.05.035",
    "abstract": "The paper presents Imbalance-XGBoost, a Python package that combines the powerful XGBoost software with weighted and focal losses to tackle binary label-imbalanced classification tasks. Though a small-scale program in terms of size, the package is, to the best of our knowledge, the first of its kind which provides an integrated implementation for the two loss functions on XGBoost and brings a general-purpose extension to XGBoost for label-imbalanced scenarios. In this paper, the design and usage of the package are discussed and illustrated with examples. Furthermore, as the first- and second-order derivatives of the loss functions are essential for the implementations, the algebraic derivation is discussed and it can be deemed as a separate contribution. The performances of the methods implemented in the package are extensively evaluated on Parkinson’s disease classification dataset, and multiple competitive performances are presented with the ROC and Precision-Recall (PR) curves. To further assert the superiority of the methods, the performances on four other benchmark datasets from the UCI machine learning repository are additionally reported. Given the scalable nature of XGBoost, the package has great potentials to be broadly applied to real-life binary classification tasks, which are usually of large-scale and label-imbalanced.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302129",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary classification",
      "Binary number",
      "Computer science",
      "Data mining",
      "Database",
      "Implementation",
      "Machine learning",
      "Mathematics",
      "Programming language",
      "Python (programming language)",
      "Scalability",
      "Software",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Chen"
      },
      {
        "surname": "Deng",
        "given_name": "Chengyuan"
      },
      {
        "surname": "Wang",
        "given_name": "Suzhen"
      }
    ]
  },
  {
    "title": "Deep learning strategies for foetal electrocardiogram signal synthesis",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.06.016",
    "abstract": "One of the most difficult tasks for the physicians is to acquire a quality foetal electrocardiogram (fECG) to analyze, manage and plan according to the condition of the foetus in the womb. Hence the foetal electrocardiogram signal is not preferred to execute the analysis to monitor the Foetal condition. Other traditional methods are being used to access the foetal condition. The foetal electrocardiogram signal can be acquired either by using invasive or non- invasive techniques. Since the invasive technique is harmful for the foetus, non-invasive technique is mostly adopted. The foetal electrocardiogram signal can be acquired only after twenty five weeks the foetus is developed in the womb, which is referred as the Antepartum period. This article portrays the use of Deep learning techniques for non-invasive foetal electrocardiogram signal synthesis using artificial intelligent techniques. Convolutional neural network (CNN), Deep belief neural networks (BNN) and Back propagation Neural Network (BPNN) have been utilized and tested for the proposal. The outcomes and performance are compared with reference to the synthesized high quality foetal electrocardiogram signal.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302348",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Programming language",
      "SIGNAL (programming language)"
    ],
    "authors": [
      {
        "surname": "Jagannath",
        "given_name": "D.J."
      },
      {
        "surname": "Raveena Judie Dolly",
        "given_name": "D."
      },
      {
        "surname": "Dinesh Peter",
        "given_name": "J."
      }
    ]
  },
  {
    "title": "Deep tracking using double-correlation filters by membership weighted decision",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.06.004",
    "abstract": "Correlation filters are well-known for tracking robustness and accuracy while convolutional neural networks (CNN) is famous for representation learning capability. However, how to combine them to further boost tracking performance remains an open problem. In this paper, we are resolved to derive a more compact double-correlation filter and incorporate an ensemble of double-correlation filters in a membership-based decision fashion where filters are trained on features obtained from different layers of a CNN respectively. The novel double-correlation filter is constructed by maximizing the similarity between the Gaussian-shape label and the correlation of template and training samples, producing a more concise solution that means more computational efficiency. Multiple filters are learned based on multiple-layer CNN features obtained. The final tracking prediction is a membership-weighted decision where membership of each tracker, which is computed according to their performance in previous frames, shows how close a weak tracker-s result is to the truth. Hence our framework not only combines correlation filters and CNN together, but also fully utilizes both deep features providing semantic information to distinguish target from background and their shallow counterparts retaining details beneficial for precise localization. We experiment on benchmark OTB and VOT where our algorithm demonstrates competitive performance versus other state-of-the-art trackers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552030221X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "BitTorrent tracker",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Correlation",
      "Eye tracking",
      "Filter (signal processing)",
      "Gaussian",
      "Gene",
      "Geodesy",
      "Geography",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Physics",
      "Psychology",
      "Quantum mechanics",
      "Robustness (evolution)",
      "Similarity (geometry)",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yang"
      },
      {
        "surname": "Tian",
        "given_name": "Xiaolin"
      },
      {
        "surname": "Jia",
        "given_name": "Nan"
      },
      {
        "surname": "Wang",
        "given_name": "Fengge"
      },
      {
        "surname": "Jiao",
        "given_name": "Licheng"
      }
    ]
  },
  {
    "title": "Temporal logistic neural Bag-of-Features for financial time series forecasting leveraging limit order book data",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.06.006",
    "abstract": "Time series forecasting is a crucial component of many important applications, ranging from forecasting the stock markets to energy load prediction. The high-dimensionality, velocity and variety of the data collected in many of these applications pose significant and unique challenges that must be carefully addressed for each of them. In this work, a novel Temporal Logistic Neural Bag-of-Features approach, that can be used to tackle these challenges, is proposed. The proposed method can be effectively combined with deep neural networks, leading to powerful deep learning models for time series analysis. However, combining existing BoF formulations with deep feature extractors pose significant challenges: the distribution of the input features is not stationary, tuning the hyper-parameters of the model can be especially difficult and the normalizations involved in the BoF model can cause significant instabilities during the training process. The proposed method is capable of overcoming these limitations by a employing a novel adaptive scaling mechanism and replacing the classical Gaussian-based density estimation involved in the regular BoF model with a logistic kernel. The effectiveness of the proposed approach is demonstrated using extensive experiments on a large-scale limit order book dataset that consists of more than 4 million limit orders.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302245",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Curse of dimensionality",
      "Data mining",
      "Deep learning",
      "Estimator",
      "Kernel density estimation",
      "Limit (mathematics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Paleontology",
      "Series (stratigraphy)",
      "Statistics",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Passalis",
        "given_name": "Nikolaos"
      },
      {
        "surname": "Tefas",
        "given_name": "Anastasios"
      },
      {
        "surname": "Kanniainen",
        "given_name": "Juho"
      },
      {
        "surname": "Gabbouj",
        "given_name": "Moncef"
      },
      {
        "surname": "Iosifidis",
        "given_name": "Alexandros"
      }
    ]
  },
  {
    "title": "Special Section: CIARP 2018",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.10.014",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519302934",
    "keywords": [
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Engineering",
      "Engineering physics",
      "Operating system",
      "Section (typography)",
      "Special section"
    ],
    "authors": [
      {
        "surname": "Fierrez",
        "given_name": "Julian"
      },
      {
        "surname": "Morales",
        "given_name": "Aythami"
      },
      {
        "surname": "Vera-Rodriguez",
        "given_name": "Ruben"
      },
      {
        "surname": "Montes-y-Gomez",
        "given_name": "Manuel"
      },
      {
        "surname": "Velastin",
        "given_name": "Sergio A."
      }
    ]
  },
  {
    "title": "A Multi-Scale Model based on the Long Short-Term Memory for day ahead hourly wind speed forecasting",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.10.011",
    "abstract": "Crucial to wind energy penetration in electrical systems is the precise forecasting of wind speed, which turns into accurate future wind power estimates. Current trends in wind speed forecasting involve using Recurrent Neural Networks to model complex temporal dynamics in the time-series. These networks, however, have problems learning long temporal dependencies in the data. To address this issue, we devise a Multi-scale Model Based on the Long Short-Term Memory for the day-ahead hourly wind speed forecasting task. Our model uses dense layers to build sub-sequences of different timescales which are used as input for multiple Long Short-Term Memory Networks (LSTM), which model each temporal scale and integrate their information accordingly. An experiment with altered wind speed data shows that our proposal is better able to learn long term dependencies than the stacked LSTM. Furthermore, results on four wind speed datasets of varying length from northern Chile reveal that our approach outperforms several models in terms of MAE and RMSE. Training times also exhibit that adding depth to the model does not increase computational times substantially, making it a more efficient approach than the stacked LSTM.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519302922",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Cartography",
      "Computer science",
      "Electrical engineering",
      "Engineering",
      "Geography",
      "Long short term memory",
      "Machine learning",
      "Memory model",
      "Meteorology",
      "Operating system",
      "Physics",
      "Quantum mechanics",
      "Recurrent neural network",
      "Scale (ratio)",
      "Shared memory",
      "Term (time)",
      "Time series",
      "Wind power",
      "Wind speed"
    ],
    "authors": [
      {
        "surname": "Araya",
        "given_name": "Ignacio A."
      },
      {
        "surname": "Valle",
        "given_name": "Carlos"
      },
      {
        "surname": "Allende",
        "given_name": "Héctor"
      }
    ]
  },
  {
    "title": "Anisotropic tubular minimal path model with fast marching front freezing scheme",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107349",
    "abstract": "In this work, we introduce an anisotropic minimal path model based on a new Riemannian tensor integrating the crossing-adaptive anisotropic radius-lifted tensor field and the front freezing indicator by appearance and path features. The non-local path feature only can be obtained during the geodesic distance computation process by the fast marching method. The predefined criterion derived from path feature is able to steer the front evolution by freezing the point causing high bending of the geodesic to solve the shortcut problem. We performed qualitative and quantitative experiments on synthetic and real images (including retinal vessels, rivers and roads) and compare with the minimal path models with classical anisotropic Riemannian metric and dynamic isotropic metric, which demonstrated the proposed method can detect desired targets from complex tubular tree structures.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301527",
    "keywords": [
      "Algorithm",
      "Anisotropy",
      "Combinatorics",
      "Computer science",
      "Engineering",
      "Fast marching method",
      "Feature (linguistics)",
      "Geodesic",
      "Geometry",
      "Isotropy",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Optics",
      "Path (computing)",
      "Philosophy",
      "Physics",
      "Programming language",
      "Tensor (intrinsic definition)",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Li"
      },
      {
        "surname": "Chen",
        "given_name": "Da"
      },
      {
        "surname": "Cohen",
        "given_name": "Laurent D."
      },
      {
        "surname": "Wu",
        "given_name": "Jiasong"
      },
      {
        "surname": "Paques",
        "given_name": "Michel"
      },
      {
        "surname": "Shu",
        "given_name": "Huazhong"
      }
    ]
  },
  {
    "title": "Corrigendum to “Image fusion via sparse regularization with non-convex penalties” Pattern Recognition Letters Volume 131, March 2020, Pages 355-360",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.03.018",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520300969",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Fusion",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Regular polygon",
      "Regularization (linguistics)",
      "Volume (thermodynamics)"
    ],
    "authors": [
      {
        "surname": "Anantrasirichai",
        "given_name": "Nantheera"
      },
      {
        "surname": "Zheng",
        "given_name": "Rencheng"
      },
      {
        "surname": "Selesnick",
        "given_name": "Ivan"
      },
      {
        "surname": "Achim",
        "given_name": "Alin"
      }
    ]
  },
  {
    "title": "Adversarial joint domain adaptation of asymmetric feature mapping based on least squares distance",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.06.007",
    "abstract": "Joint domain adaptation aims to learn a high-quality classifier for an unlabeled dataset with the help of auxiliary data. Most methods reduce domain shifts through some carefully designed distance measures. Adversarial learning, which is rarely used for joint domain adaptation, can learn more transferable features while avoiding explicit distance measures. However, it usually suffers from a gradient vanishing problem during the training process. In order to solve the above problems, we propose a novel adversarial joint domain adaptation method, namely Asymmetric Feature mapping based on Least Squares Distance (AFLSD), which consists of asymmetric marginal distribution alignment and conditional distribution alignment. The asymmetric feature mapping, which can get closer features with more flexible parameters, is optimized by the least squares distance to reduce the gradient vanishing problem. The results of classification and other comparative experiments show that AFLSD is superior to the most advanced domain adaptation methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302257",
    "keywords": [
      "Adaptation (eye)",
      "Adversarial system",
      "Algorithm",
      "Architectural engineering",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Engineering",
      "Feature (linguistics)",
      "Joint (building)",
      "Joint probability distribution",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Yuan",
        "given_name": "Yumeng"
      },
      {
        "surname": "Li",
        "given_name": "Yuhua"
      },
      {
        "surname": "Zhu",
        "given_name": "Zhenlong"
      },
      {
        "surname": "Li",
        "given_name": "Ruixuan"
      },
      {
        "surname": "Gu",
        "given_name": "Xiwu"
      }
    ]
  },
  {
    "title": "SaHAN: Scale-aware hierarchical attention network for scene text recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.06.009",
    "abstract": "Scene text recognition has become a research hotspot owing to its abundant semantic information and various applications. Recent methods of scene text recognition usually focus on handling shape distortion, attention drift, or background noise, ignoring that text recognition encounters character scale-variation problem. To address this issue, in this paper, we propose a new scale-aware hierarchical attention network (SaHAN) for scene text recognition. Inspired by feature pyramid network, we exploit the inherent pyramidal structure of a deep convolutional network to retain multi-scale features for flexible receptive fields. Then, we construct a hierarchical attention decoder that performs the attention mechanism twice on multi-scale features to collect the most fine-grained information for prediction. The SaHAN is trained in a weak supervision way, requiring only images and corresponding text labels. Extensive experiments on seven benchmarks reveal that SaHAN achieves state-of-the-art performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302270",
    "keywords": [
      "Artificial intelligence",
      "Attention network",
      "Computer science",
      "Computer security",
      "Convolutional neural network",
      "Exploit",
      "Feature (linguistics)",
      "Focus (optics)",
      "Linguistics",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pyramid (geometry)",
      "Quantum mechanics",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Jiaxin"
      },
      {
        "surname": "Luo",
        "given_name": "Canjie"
      },
      {
        "surname": "Jin",
        "given_name": "Lianwen"
      },
      {
        "surname": "Wang",
        "given_name": "Tianwei"
      },
      {
        "surname": "Li",
        "given_name": "Ziyan"
      },
      {
        "surname": "Zhou",
        "given_name": "Weiying"
      }
    ]
  },
  {
    "title": "Re-ranking image-text matching by adaptive metric fusion",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107351",
    "abstract": "Image-text matching has drawn much attention recently with the rapid growth of multi-modal data. Many effective approaches have been proposed to solve this challenging problem, but limited effort has been devoted to re-ranking methods. Compared with the uni-modal re-ranking methods, modality heterogeneity is the major difficulty when designing a re-ranking method in the cross-modal field, which mainly lies in two aspects of different visual and textual feature spaces and different distributions in inverse directions. In this paper, we propose a heuristic re-ranking method called Adaptive Metric Fusion (AMF) for image-text matching. The method can obtain a better metric by adaptively fusing metrics based on two modules: 1) Cross-modal Reciprocal Encoding , which considers ranks in inverse directions to comprehensively evaluate a metric. The sentence retrieval and image retrieval have different distribution characteristics and galleries in different modalities, thus it is necessary to exploit them simultaneously for appropriate metric fusion. 2) Query Replacement Gap , which quantifies the gap between cross-modal and uni-modal similarities to alleviate the influence of different visual and textual feature spaces on the fused metric. The proposed re-ranking method can be implemented in an unsupervised way without requiring any human interaction or annotated data, and can be easily applied to any initial ranking result. Extensive experiments and analysis validate the effectiveness of our method on the large-scale MS-COCO and Flickr30K datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301540",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Economics",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Matching (statistics)",
      "Mathematics",
      "Metric (unit)",
      "Modal",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Polymer chemistry",
      "Ranking (information retrieval)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Niu",
        "given_name": "Kai"
      },
      {
        "surname": "Huang",
        "given_name": "Yan"
      },
      {
        "surname": "Wang",
        "given_name": "Liang"
      }
    ]
  },
  {
    "title": "Semi-supervised network embedding with text information",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107347",
    "abstract": "Network embedding plays a pivotal role in network analysis, due to the capability of encoding each node to a low-dimensional dense feature vector. However, most existing network embedding approaches only focus on preserving structural information in the network. The text features and category attributes of nodes are ignored, which are important to network analysis. In this paper, we propose an innovative semi-supervised network embedding (SNE) model integrating structural information, text features and category attributes into embedding vectors simultaneously. Specifically, we design a structure preserving module and a text representation module to capture the global structural information and the text features separately. Meanwhile, a label indicator matrix and a supervised loss are proposed for preserving category information and mapping nodes in the same class closer. We utilize stacked auto-encoders to explore the highly nonlinear characteristics of the network. By optimizing the reconstruction loss and the designed supervised loss jointly in the proposed semi-supervised model, the embedding vectors are finally learned. Extensive experiments on real-world datasets demonstrate that our method is superior to the state-of-the-art baselines in a variety of tasks, including visualization, node classification and clustering.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301503",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Embedding",
      "Encoder",
      "Encoding (memory)",
      "Engineering",
      "Feature (linguistics)",
      "Feature learning",
      "Feature vector",
      "Focus (optics)",
      "Law",
      "Linguistics",
      "Machine learning",
      "Node (physics)",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Structural engineering",
      "Variety (cybernetics)",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Gong",
        "given_name": "Maoguo"
      },
      {
        "surname": "Yao",
        "given_name": "Chuanyu"
      },
      {
        "surname": "Xie",
        "given_name": "Yu"
      },
      {
        "surname": "Xu",
        "given_name": "Mingliang"
      }
    ]
  },
  {
    "title": "Joint graph regularized dictionary learning and sparse ranking for multi-modal multi-shot person re-identification",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107352",
    "abstract": "The promising achievement of sparse ranking in image-based recognition gives rise to a number of development on person re-identification (Re-ID) which aims to reconstruct the probe as a linear combination of few atoms/images from an over-complete dictionary/gallery. However, most of the existing sparse ranking based Re-ID methods lack considering the geometric relationships between probe, gallery, and cross-modal images of the same person in multi-shot Re-ID. In this paper, we propose a novel joint graph regularized dictionary learning and sparse ranking method for multi-modal multi-shot person Re-ID. First, we explore the probe-based geometrical structure by enforcing the smoothness between the codings/coefficients, which refers to the multi-shot images from the same person in probe. Second, we explore the gallery-based geometrical structure among gallery images, which encourages the multi-shot images from the same person in the gallery making similar contributions while reconstructing a certain probe image. Third, we explore the cross-modal geometrical structure by enforcing the smoothness between the cross-modal images and thus extend our model for the multi-modal case. Finally, we design an APG based optimization to solve the problem. Comprehensive experiments on benchmark datasets demonstrate the superior performance of the proposed model. The code is available at https://github.com/ttaalle/Lhc.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301552",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Botany",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Geodesy",
      "Geography",
      "Graph",
      "Identification (biology)",
      "Image (mathematics)",
      "Machine learning",
      "Modal",
      "Pattern recognition (psychology)",
      "Polymer chemistry",
      "Ranking (information retrieval)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Aihua"
      },
      {
        "surname": "Li",
        "given_name": "Hongchao"
      },
      {
        "surname": "Jiang",
        "given_name": "Bo"
      },
      {
        "surname": "Zheng",
        "given_name": "Wei-Shi"
      },
      {
        "surname": "Luo",
        "given_name": "Bin"
      }
    ]
  },
  {
    "title": "Exponential sparsity preserving projection with applications to image recognition",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107357",
    "abstract": "Sparsity preserving projection (SPP), as a widely used linear unsupervised dimensionality reduction (DR) method, is designed to preserve the sparse reconstructive relationship of the raw data. SPP constructs an affinity weight matrix by solving a sparse representation model which does not need any parameters. Moreover, the obtained projection may contain some discriminating information even if no prior knowledge is provided. Although SPP may be more conveniently used in practice due to these advantages, it still suffers from the so-called small-sample-size problem as may other DR methods do. To solve this problem, we propose an exponential sparsity preserving projection (ESPP) by using matrix exponential, and present two efficiently numerical methods for solving the corresponding large-scale matrix exponential eigenvalue problem. ESPP avoids the singularity of the coefficient matrices, and obtains more valuable information for the SPP. Image recognition experiments are conducted on several real-world image databases and the experimental results illustrate the outperformances of ESPP.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301606",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Composite material",
      "Computer science",
      "Differential equation",
      "Dimensionality reduction",
      "Eigenvalues and eigenvectors",
      "Exponential function",
      "Law",
      "Materials science",
      "Mathematical analysis",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Matrix exponential",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Projection (relational algebra)",
      "Quantum mechanics",
      "Representation (politics)",
      "Sparse approximation"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Wei"
      },
      {
        "surname": "Dai",
        "given_name": "Hua"
      },
      {
        "surname": "Liang",
        "given_name": "Wei-tai"
      }
    ]
  },
  {
    "title": "Biclustering with dominant sets",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107318",
    "abstract": "Biclustering can be defined as the simultaneous clustering of rows and columns in a data matrix and it has been recently applied to many scientific scenarios such as bioinformatics, text analysis and computer vision to name a few. In this paper we propose a novel biclustering approach, that is based on the concept of dominant-set clustering and extends such algorithm to the biclustering problem. In more detail, we propose a novel encoding of the biclustering problem as a graph so to use the dominant set concept to analyse rows and columns simultaneously. Moreover, we extend the Dominant Set Biclustering approach to facilitate the insertion of prior knowledge that may be available on the domain. We evaluated the proposed approach on a synthetic benchmark and on two computer vision tasks: multiple structure recovery and region-based correspondence. The empirical evaluation shows that the method achieves promising results that are comparable to the state-of-the-art and that outperforms competitors in various cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301217",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biclustering",
      "CURE data clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Database",
      "Geodesy",
      "Geography",
      "Graph",
      "Pattern recognition (psychology)",
      "Programming language",
      "Row",
      "Row and column spaces",
      "Set (abstract data type)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Denitto",
        "given_name": "M."
      },
      {
        "surname": "Bicego",
        "given_name": "M."
      },
      {
        "surname": "Farinelli",
        "given_name": "A."
      },
      {
        "surname": "Vascon",
        "given_name": "S."
      },
      {
        "surname": "Pelillo",
        "given_name": "M."
      }
    ]
  },
  {
    "title": "Three-dimensional reconstruction of CT image features based on multi-threaded deep learning calculation",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.04.033",
    "abstract": "Traditional technology uses serial processing method in CT image feature extraction. It is prone to loss of image data, which causes problems such as ring distortion of the reconstructed image and long reconstruction time. Therefore, a three-dimensional (3D) reconstruction algorithm for CT image features based on multi-threaded deep learning calculation is designed. Feature image textures are segmented using parameters such as gray, mean, and variance. Fusion calculates co-occurrence logarithm of the gray level with segmented sub-block and its pixels. Co-occurrence probability of the sub-block gray levels is obtained, followed by getting optimal feature volume data, which is stored in 3D texture and 1D texture for interpolation calculation.The optimal feature volume data is stored in 3D and 1D texture for interpolation calculation. Rotation matrix is stored in the global storage space of CUDA, which is used for multi-threaded calculations. After completing multi-threaded batch CT image hardware configuration and algorithm flow settings, the thread's index and bounding box is calculated. 3D reconstruction of CT image features is achieved by model accumulation. The experimental simulation proves that local detail information loss is small after reconstruction of the proposed method. Its reconstruction time is short and has good applicability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552030163X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Image (mathematics)",
      "Iterative reconstruction",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Feng"
      },
      {
        "surname": "Muhammad",
        "given_name": "Khan"
      },
      {
        "surname": "Wang",
        "given_name": "Shui-Hua"
      }
    ]
  },
  {
    "title": "Multi-task learning for natural language processing in the 2020s: Where are we going?",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.05.031",
    "abstract": "Multi-task learning (MTL) significantly pre-dates the deep learning era, and it has seen a resurgence in the past few years as researchers have been applying MTL to deep learning solutions for natural language tasks. While steady MTL research has always been present, there is a growing interest driven by the impressive successes published in the related fields of transfer learning and pre-training, such as BERT, and the release of new challenge problems, such as GLUE and the NLP Decathlon (decaNLP). These efforts place more focus on how weights are shared across networks, evaluate the re-usability of network components and identify use cases where MTL can significantly outperform single-task solutions. This paper strives to provide a comprehensive survey of the numerous recent MTL contributions to the field of natural language processing and provide a forum to focus efforts on the hardest unsolved problems in the next decade. While novel models that improve performance on NLP benchmarks are continually produced, lasting MTL challenges remain unsolved which could hold the key to better language understanding, knowledge discovery and natural language interfaces.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302087",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Data science",
      "Deep learning",
      "Engineering",
      "Field (mathematics)",
      "Focus (optics)",
      "History",
      "Human–computer interaction",
      "Key (lock)",
      "Mathematics",
      "Natural (archaeology)",
      "Natural language",
      "Natural language processing",
      "Natural language understanding",
      "Optics",
      "Physics",
      "Pure mathematics",
      "Systems engineering",
      "Task (project management)",
      "Transfer of learning",
      "Usability"
    ],
    "authors": [
      {
        "surname": "Worsham",
        "given_name": "Joseph"
      },
      {
        "surname": "Kalita",
        "given_name": "Jugal"
      }
    ]
  },
  {
    "title": "A neural model for text localization, transcription and named entity recognition in full pages",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.05.001",
    "abstract": "In the last years, the consolidation of deep neural network architectures for information extraction in document images has brought big improvements in the performance of each of the tasks involved in this process, consisting of text localization, transcription, and named entity recognition. However, this process is traditionally performed with separate methods for each task. In this work we propose an end-to-end model that combines a one stage object detection network with branches for the recognition of text and named entities respectively in a way that shared features can be learned simultaneously from the training error of each of the tasks. By doing so the model jointly performs handwritten text detection, transcription, and named entity recognition at page level with a single feed forward step. We exhaustively evaluate our approach on different datasets, discussing its advantages and limitations compared to sequential approaches. The results show that the model is capable of benefiting from shared features by simultaneously solving interdependent tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301719",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Economics",
      "Linguistics",
      "Machine learning",
      "Management",
      "Named-entity recognition",
      "Natural language processing",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Task (project management)",
      "Transcription (linguistics)"
    ],
    "authors": [
      {
        "surname": "Carbonell",
        "given_name": "Manuel"
      },
      {
        "surname": "Fornés",
        "given_name": "Alicia"
      },
      {
        "surname": "Villegas",
        "given_name": "Mauricio"
      },
      {
        "surname": "Lladós",
        "given_name": "Josep"
      }
    ]
  },
  {
    "title": "Positive-unlabeled learning for open set domain adaptation",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.06.003",
    "abstract": "Open Set Domain Adaptation (OSDA) focuses on bridging the domain gap between a labeled source domain and an unlabeled target domain, while also rejecting target classes that are not present in the source as unknown. The challenges of this task are closely related to those of Positive-Unlabeled (PU) learning where it is essential to discriminate between positive (known) and negative (unknown) class samples in the unlabeled target data. With this newly discovered connection, we leverage the theoretical framework of PU learning for OSDA and, at the same time, we extend PU learning to tackle uneven data distributions. Our method combines domain adversarial learning with a new non-negative risk estimator for PU learning based on self-supervised sample reconstruction. With experiments on digit recognition and object classification, we validate our risk estimator and demonstrate that our approach allows reducing the domain gap without suffering from negative transfer.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302221",
    "keywords": [
      "Artificial intelligence",
      "Bridging (networking)",
      "Classifier (UML)",
      "Computer network",
      "Computer science",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Estimator",
      "Leverage (statistics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Statistics",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Loghmani",
        "given_name": "Mohammad Reza"
      },
      {
        "surname": "Vincze",
        "given_name": "Markus"
      },
      {
        "surname": "Tommasi",
        "given_name": "Tatiana"
      }
    ]
  },
  {
    "title": "An attention-based row-column encoder-decoder model for text recognition in Japanese historical documents",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.05.026",
    "abstract": "This paper presents an attention-based row-column encoder-decoder (ARCED) model for recognizing an input image of multiple text lines from Japanese historical documents without explicit segmentation of lines. The recognition system has three main parts: a feature extractor, a row-column encoder, and a decoder. We introduce a row-column BLSTM in the encoder and a residual LSTM network in the decoder. The whole system is trained end-to-end by a standard cross-entropy loss function, requiring only document images and their ground-truth text. We experimentally evaluate the performance of ARCED on the dataset of Japanese historical documents: Kana-PRMU. The results of the experiments show that ARCED outperforms the state-of-the-art recognition methods on the dataset. Furthermore, we demonstrate that the row-column BLSTM in the encoder and the residual LSTM in the decoder improves the performance of the encoder-decoder model for the recognition of Japanese historical document.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301811",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Column (typography)",
      "Computer science",
      "Encoder",
      "Feature (linguistics)",
      "Frame (networking)",
      "Linguistics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Residual",
      "Segmentation",
      "Speech recognition",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Ly",
        "given_name": "Nam Tuan"
      },
      {
        "surname": "Nguyen",
        "given_name": "Cuong Tuan"
      },
      {
        "surname": "Nakagawa",
        "given_name": "Masaki"
      }
    ]
  },
  {
    "title": "Feature selection and learning for graphlet kernel",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.05.023",
    "abstract": "Graph-based representations have been used with considerable success in Bioinformatics. One of the challenging problems with graph-based representation is that of estimating the similarity between two input graphs. Graph kernels are an answer to this problem that aim at bridging the gap between a vectorised representation and a structured representation. However, existing graph kernels suffer from one of two problems. They are either computationally very expensive or have low classification accuracy. In this paper we present a method that can be used to improve the accuracy and efficiency of one of the most popular graph kernels, i.e., graphlet kernel. The main idea behind graphlet kernel is to use a graphlet frequency vector as a feature vector. We propose a framework that can be used to select a subset of features that can be used to estimate the similarity between graphs. We show that the proposed method not only increases the efficiency of the resulting kernel but also increases the classification accuracy. We enrich the feature vector by identifying a set of higher-order graphlets that can be efficiently computed. We also show that different datasets from bioinformatics domain share common graphlets. Therefore the set of features learned from one bioinformatics dataset can also be used to classify graphs in another bioinformatics dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302014",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Feature learning",
      "Feature selection",
      "Feature vector",
      "Graph",
      "Graph kernel",
      "Kernel (algebra)",
      "Kernel method",
      "Machine learning",
      "Mathematics",
      "Multiple kernel learning",
      "Pattern recognition (psychology)",
      "Radial basis function kernel",
      "Support vector machine",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Aziz",
        "given_name": "Furqan"
      },
      {
        "surname": "Ullah",
        "given_name": "Afan"
      },
      {
        "surname": "Shah",
        "given_name": "Faiza"
      }
    ]
  },
  {
    "title": "Clustering quality metrics for subspace clustering",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107328",
    "abstract": "We study the problem of clustering validation, i.e., clustering evaluation without knowledge of ground-truth labels, for the increasingly-popular framework known as subspace clustering. Existing clustering quality metrics (CQMs) rely heavily on a notion of distance between points, but common metrics fail to capture the geometry of subspace clustering. We propose a novel point-to-point pseudometric for points lying on a union of subspaces and show how this allows for the application of existing CQMs to the subspace clustering problem. We provide theoretical and empirical justification for the proposed point-to-point distance, and then demonstrate on a number of common benchmark datasets that our proposed methods generally outperform existing graph-based CQMs in terms of choosing the best clustering and the number of clusters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030131X",
    "keywords": [
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Computer science",
      "Constrained clustering",
      "Correlation clustering",
      "Data mining",
      "Data stream clustering",
      "Fuzzy clustering",
      "Geometry",
      "Linear subspace",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Point (geometry)",
      "Single-linkage clustering",
      "Subspace topology",
      "k-medians clustering"
    ],
    "authors": [
      {
        "surname": "Lipor",
        "given_name": "John"
      },
      {
        "surname": "Balzano",
        "given_name": "Laura"
      }
    ]
  },
  {
    "title": "Data augmentation method for improving the accuracy of human pose estimation with cropped images",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.06.015",
    "abstract": "Neural networks have improved the accuracy of human pose estimation from a single RGB image. However, such estimation remains difficult, especially when the human body is only partially visible due to a limited field of view of the camera or occlusions. In this paper, we introduce a data augmentation method called body-cropping augmentation (BCA), which generalizes the dataset for effective training in human pose estimation. This technique includes the policies of data generation and the training strategy using the augmented data. The experiments with the COCO val2017 dataset with ground-truth bounding boxes show BCA consistently enhances accuracies of state-of-the-art neural networks by an average of 1.08% without any modification to the network architecture. Moreover, the proposed BCA technique effectively reduces the false negatives of localizing keypoints, especially in an input image with a few visible keypoints.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302336",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Bounding overwatch",
      "Computer science",
      "Computer vision",
      "Ground truth",
      "Image (mathematics)",
      "Pattern recognition (psychology)",
      "Pose",
      "RGB color model"
    ],
    "authors": [
      {
        "surname": "Park",
        "given_name": "Soonchan"
      },
      {
        "surname": "Lee",
        "given_name": "Sang-baek"
      },
      {
        "surname": "Park",
        "given_name": "Jinah"
      }
    ]
  },
  {
    "title": "Editorial of the special issue TIAR: Topological image analysis and recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.06.012",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302300",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computational biology",
      "Computer science",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Topological data analysis"
    ],
    "authors": [
      {
        "surname": "Christinal",
        "given_name": "Dr. A. Hepzibah"
      },
      {
        "surname": "Díaz-del-Río",
        "given_name": "Dr. Fernando"
      },
      {
        "surname": "Marfil",
        "given_name": "Dr. Rebeca"
      },
      {
        "surname": "Abril",
        "given_name": "Dr. Helena Molina"
      },
      {
        "surname": "Moaca",
        "given_name": "Dr. Darian Onchis"
      },
      {
        "surname": "Real",
        "given_name": "Dr. Pedro"
      }
    ]
  },
  {
    "title": "Two-stage multi-modal MR images fusion method based on Parametric Logarithmic Image Processing (PLIP) Model",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.05.027",
    "abstract": "MRI is one of the most compliant technique that is used for the screening of Brain Tumor. MRI can be acquired in four available modalities which are MR-T1, MR-T2, MR-PD and MR-Gad; among these MR-T2 comprises of most of the detailed information of the tumorous tissues. However, the accuracy and reliability of the diagnosis may be affected due to lack of sufficient details in each modality (as different MRI modalities highlight different set of tissues). Therefore, MR Image(s) fusion is essential to obtain a more illustrative image containing the requisite complementary details of each modality. For this purpose, multi-modal fusion of MR-T2 with MR-T1, MR-PD and MR-Gad have been dealt in this work using the proposed fusion method. This paper presents a two-stage fusion method using Stationary Wavelet Transform (SWT) in combination with Parameterized Logarithmic Image Processing (PLIP) model. At Stage-I of sub-band decomposition: the first level SWT coefficients contain large amount of noise thus suppressing the necessary edge information. This aspect has been resolved at Stage-II by employing second level SWT decomposition along with Principal Component Analysis (PCA). The fusion coefficients from both the stages are finally fused using PLIP operators (prior to reconstruction). The obtained results are compared qualitatively as well as quantitatively using fusion metrics like Entropy, Fusion Factor, Standard Deviation and Edge Strength. Noteworthy visual response is obtained with PLIP fusion model in coherence with Human Visual System (HVS) characteristics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302051",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Fusion",
      "Fusion rules",
      "Image (mathematics)",
      "Image fusion",
      "Image processing",
      "Linguistics",
      "Mathematics",
      "Modality (human–computer interaction)",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Bhateja",
        "given_name": "Vikrant"
      },
      {
        "surname": "Nigam",
        "given_name": "Mansi"
      },
      {
        "surname": "Bhadauria",
        "given_name": "Anuj Singh"
      },
      {
        "surname": "Arya",
        "given_name": "Anu"
      }
    ]
  },
  {
    "title": "Two-stage human hair segmentation in the wild using deep shape prior",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.06.014",
    "abstract": "Human hair is a crucial biometric characteristic with rich color and texture information. In this paper, we propose a novel hair segmentation approach integrating a deep shape prior into a carefully designed two-stage Fully Convolutional Neural Network (FCNN) pipeline. First, we utilize a FCNN with an Atrous Spatial Pyramid Pooling (ASPP) module to train a human hair shape prior based on a specific distance transform. In the second stage, we combine the hair shape prior and the original image to form the input of a symmetric encoder-decoder FCNN with a border refinement module to get the final hair segmentation output. Both quantitative and qualitative results show that our method achieves state-of-the-art performance on the LFW-Part and Figaro1k datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302324",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Encoder",
      "Geometry",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Pipeline (software)",
      "Pooling",
      "Programming language",
      "Pyramid (geometry)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Yongzhe"
      },
      {
        "surname": "Duffner",
        "given_name": "Stefan"
      },
      {
        "surname": "Naturel",
        "given_name": "Xavier"
      },
      {
        "surname": "Berthelier",
        "given_name": "Anthony"
      },
      {
        "surname": "Garcia",
        "given_name": "Christophe"
      },
      {
        "surname": "Blanc",
        "given_name": "Christophe"
      },
      {
        "surname": "Chateau",
        "given_name": "Thierry"
      }
    ]
  },
  {
    "title": "Visual domain adaptation based on modified A − distance and sparse filtering",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107254",
    "abstract": "Domain adaptation studies how to build a robust model to solve pattern recognition problems when training in a source domain while testing in a related but different target domain. The existing methods focus on how to shorten the distance between the two domains, however, they have limited considerations on the preservation of data structures. In this paper, we propose a novel model for unsupervised domain adaptation. For the reduction of domain discrepancy, we propose modified A − distance, which is computationally fast and can be optimized using gradient information. Moreover, in order to capture the internal structures of target samples, within-domain normalization based sparse filtering is raised, which proved to be more powerful for domain adaptation. Extensive experiments compared to both shallow and deep methods demonstrate the effectiveness of our approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300595",
    "keywords": [
      "Algorithm",
      "Anthropology",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Focus (optics)",
      "Mathematical analysis",
      "Mathematics",
      "Normalization (sociology)",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Han",
        "given_name": "Chao"
      },
      {
        "surname": "Lei",
        "given_name": "Yu"
      },
      {
        "surname": "Xie",
        "given_name": "Yu"
      },
      {
        "surname": "Zhou",
        "given_name": "Deyun"
      },
      {
        "surname": "Gong",
        "given_name": "Maoguo"
      }
    ]
  },
  {
    "title": "Fuzzy logic and histogram of normal orientation-based 3D keypoint detection for point clouds",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.05.010",
    "abstract": "Point cloud processing has gained consideration for 3D object recognition and classification tasks. In this context, an important task is to detect the distinct and repeatable 3D keypoints. Many 3D keypoint detectors with low repeatability and distinctiveness have been proposed. The detection of highly repeatable and distinct keypoints is still an open problem. To address this issue, we propose a fuzzy logic and Histogram of Normal Orientation (HoNO)-based 3D keypoint detection scheme for Point Cloud (PC) data. To measure saliency, we exploit the structure of the PC and compute the eigenvalues of the covariance matrix and the HoNO to measure saliency. The histogram (HoNO) salient value is computed by the kurtosis values, which estimate the spread of the histogram. From the kurtosis and smallest eigenvalues, we compute the difference of the kurtosis values and the difference of the smallest eigenvalues of the query point against all the neighbouring points. The difference of kurtosis values and difference of smallest eigenvalues are applied to a fuzzy rule-based scheme for the keypoints detection. We compare the proposed algorithm with the state-of-the-art 3D keypoint detectors on five benchmark datasets. Experimental results demonstrate the superior performance of the proposed detector on most of the benchmark datasets both in terms of absolute and relative repeatability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552030180X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Fuzzy logic",
      "Histogram",
      "Image (mathematics)",
      "Kurtosis",
      "Mathematics",
      "Measure (data warehouse)",
      "Pattern recognition (psychology)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Iqbal",
        "given_name": "Muhammad Zafar"
      },
      {
        "surname": "Bobkov",
        "given_name": "Dmytro"
      },
      {
        "surname": "Steinbach",
        "given_name": "Eckehard"
      }
    ]
  },
  {
    "title": "A computer vision system for automatic cherry beans detection on coffee trees",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.05.034",
    "abstract": "Coffee production estimation is an essential task for coffee farmers in terms of money investment and planning time. In Colombia, the traditional methodology to estimate the total amount of cherry coffee beans is through direct measurements in the field; leave out the cherry beans collected of coffee production (destructive sampling). The cherry coffee dropped in this process cannot be harvest by the producer. In this sense, we found several shortcomings in this methodology as counting errors in the sampling process, insufficient coffee bean samples, significant expenses of costs and time, and coffee beans losses. To handle these issues, we propose a classic Computer Vision (CV) approach to detect cherry beans in coffee trees. This approach substitutes the destructive counting method as a first step to estimate coffee production. To evaluate the CV proposed, seven coffee farmers counted the number of cherry beans on 600 images of coffee trees (castillo, bourbon, and caturra varieties) by human visual perception (ground truth). From evaluations of coffee farmers, we computed statistical measures like precision, recall and, F1-score. The CV system achieved the best results for bourbon coffee trees with 0.594 of precision; 0.669 of total relevant cherry beans correctly classified.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302117",
    "keywords": [
      "Biology",
      "Coffea arabica",
      "Coffee bean",
      "Food science",
      "Green coffee",
      "Horticulture",
      "Mathematics",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Rodríguez",
        "given_name": "Jhonn Pablo"
      },
      {
        "surname": "Corrales",
        "given_name": "David Camilo"
      },
      {
        "surname": "Aubertot",
        "given_name": "Jean-Noël"
      },
      {
        "surname": "Corrales",
        "given_name": "Juan Carlos"
      }
    ]
  },
  {
    "title": "SGM-Net: Skeleton-guided multimodal network for action recognition",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107356",
    "abstract": "Single-modality human action recognition on RGB or skeleton has been extensively studied. Each of these two modalities has its own advantages as well as limitations, because they depict action from different perspectives. The feature of different modalities can complement each other for describing actions. Therefore, it is meaningful to fuse these two modalities using their complementarity for action recognition. However, existing multimodal methods fail to fully exploit the complementarity of RGB and skeleton modalities. In this paper, we propose a Skeleton-Guided Multimodal Network (SGM-Net) for human action recognition. The proposed method takes full use of the complementarity of these two modalities at semantic feature level. From the technical perspective, we introduce a guided block, the key component of SGM-Net. It enables skeleton feature to guide on RGB feature, so that the important RGB information strongly related to the action is enhanced. Moreover, in the guided block, two schemes of correlation operation are explored. We perform a series of ablation experiments to verify the effectiveness of the guided block. The experimental results show that our approach achieves state-of-the-art performance over the existing methods on NTU and Sub-JHMDB datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030159X",
    "keywords": [
      "Action recognition",
      "Artificial intelligence",
      "Biology",
      "Block (permutation group theory)",
      "Class (philosophy)",
      "Complementarity (molecular biology)",
      "Computer science",
      "Feature (linguistics)",
      "Genetics",
      "Geometry",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Modalities",
      "Modality (human–computer interaction)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "RGB color model",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Jianan"
      },
      {
        "surname": "Xie",
        "given_name": "Xuemei"
      },
      {
        "surname": "Pan",
        "given_name": "Qingzhe"
      },
      {
        "surname": "Cao",
        "given_name": "Yuhan"
      },
      {
        "surname": "Zhao",
        "given_name": "Zhifu"
      },
      {
        "surname": "Shi",
        "given_name": "Guangming"
      }
    ]
  },
  {
    "title": "Simplified long short-term memory model for robust and fast prediction",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.05.033",
    "abstract": "Long short-term memory(LSTM) is an effective solution to time sequence prediction. Considering the data perturbations, in this letter, a variant model of LSTM is proposed to achieve robustness of prediction. Specifically, data processing procedure in the recurrent unit of proposed model is reformulated, the gates are controlled by only one variable, and the variable is the sum of long-term memory and the current input. Due to the simplified two-gate structure of proposed model, the speed of prediction is improved as well. The experiments on three datasets verify that the proposed model with simplified structure has higher robustness and shorter running time than the traditional LSTM model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302105",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Gene",
      "Long short term memory",
      "Mathematical analysis",
      "Mathematics",
      "Memory model",
      "Parallel computing",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Recurrent neural network",
      "Robustness (evolution)",
      "Shared memory",
      "Term (time)",
      "Variable (mathematics)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yong"
      },
      {
        "surname": "Hao",
        "given_name": "Xin"
      },
      {
        "surname": "Zhang",
        "given_name": "Biling"
      },
      {
        "surname": "Zhang",
        "given_name": "Yuyan"
      }
    ]
  },
  {
    "title": "Meter classification of Arabic poems using deep bidirectional recurrent neural networks",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.05.028",
    "abstract": "Poetry is an important component of any language. Much of a nation’s history and culture are documented in poems. A poem has a rhythmic flow which is quite different as compared to a prose. Each language has its own set of rhythmical structures for poems, called meters. Identifying the meters of Arabic poems is a lengthy and complicated process. To classify a poem’s meter, the text of the poem should be encoded in a special Arudi form which needs complex rule-based transformations before another set of rules can be used to finally classify the meters. This paper introduces a novel method for classifying poem meters of Arabic poems using RNN-based deep learning. It bypasses the need to transform the poem to the Arudi form as well as the need to explicitly encode the complex rules that are usually followed to determine the meter. The presented method was evaluated on a large dataset collected specifically for this purpose. We are able to classify the poem meters with an accuracy of 94.32% on an independent test set.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552030204X",
    "keywords": [
      "Arabic",
      "Art",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "ENCODE",
      "Gene",
      "Linguistics",
      "Literature",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Poetry",
      "Process (computing)",
      "Programming language",
      "Set (abstract data type)",
      "Test set"
    ],
    "authors": [
      {
        "surname": "Al-shaibani",
        "given_name": "Maged S."
      },
      {
        "surname": "Alyafeai",
        "given_name": "Zaid"
      },
      {
        "surname": "Ahmad",
        "given_name": "Irfan"
      }
    ]
  },
  {
    "title": "Timed-image based deep learning for action recognition in video sequences",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107353",
    "abstract": "The paper addresses two issues relative to machine learning on 2D + X data volumes, where 2D refers to image observation and X denotes a variable that can be associated with time, depth, wavelength, etc. The first issue addressed is conditioning these structured volumes for compatibility with respect to convolutional neural networks operating on 2D image file formats. The second issue is associated with sensitive action detection in the “2D + Time” case (video clips and image time series). For the data conditioning issue, the paper first highlights that referring 2D spatial convolution to its 1D Hilbert based instance is highly accurate for information compressibility upon tight frames of convolutional networks. As a consequence of this compressibility, the paper proposes converting the 2D + X data volume into a single meta-image file format, prior to machine learning frameworks. This conversion is such that any 2D frame of the 2D + X data is reshaped as a 1D array indexed by a Hilbert space-filling curve and the third variable X of the initial file format becomes the second variable in the meta-image format. For the sensitive action recognition issue, the paper provides: (i) a 3 category video database involving non-violent, moderate and extreme violence actions; (ii) the conversion of this database into a timed meta-image database from the 2D + Time to 2D conditioning stage described above and (iii) outstanding 2-level and 3-level violence classification results from deep convolutional neural networks operating on meta-image databases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301564",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Atto",
        "given_name": "Abdourrahmane Mahamane"
      },
      {
        "surname": "Benoit",
        "given_name": "Alexandre"
      },
      {
        "surname": "Lambert",
        "given_name": "Patrick"
      }
    ]
  },
  {
    "title": "Accelerating projections to kernel-induced spaces by feature approximation",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.05.029",
    "abstract": "A method for speeding-up data projections onto kernel-induced feature spaces (derived using e.g. kernel Principal Component Analysis - kPCA) is presented in the paper. The proposed idea is to simplify the derived features, implicitly defined by all training samples and dominant eigenvectors of problem-specific generalized eigenproblems, by appropriate approximations. Instead of employing the whole training set, we propose to use a small pool of its appropriately selected representatives and we formulate a rule for deriving the corresponding weight vectors that replace the considered dominant eigenvectors. The representatives are determined via clustering of training data, whereas weighting coefficients are chosen to minimize original feature approximation errors. The concept has been experimentally verified for kernel-PCA using both artificial and real datasets. It has been shown that the presented approach provides reduction in feature-extraction complexity, which implies a proportional increase in data projection speed, by one-to-two orders of magnitude, without sacrificing data analysis accuracy. Therefore, the proposed approach is well-suited for kernel-based, intelligent data analysis applications that are to be executed on resource-limited systems, such as embedded or IoT devices, or for systems where processing time is critical.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302063",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Eigenvalues and eigenvectors",
      "Feature (linguistics)",
      "Kernel (algebra)",
      "Kernel method",
      "Kernel principal component analysis",
      "Linguistics",
      "Mathematics",
      "Medicine",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Principal component analysis",
      "Projection (relational algebra)",
      "Quantum mechanics",
      "Radiology",
      "Support vector machine",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Adamiak",
        "given_name": "Krzysztof"
      },
      {
        "surname": "Kim",
        "given_name": "Hyongsuk"
      },
      {
        "surname": "Ślot",
        "given_name": "Krzysztof"
      }
    ]
  },
  {
    "title": "Topic modelling for routine discovery from egocentric photo-streams",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107330",
    "abstract": "Developing tools to understand and visualize lifestyle is of high interest when addressing the improvement of habits and well-being of people. Routine, defined as the usual things that a person does daily, helps describe the individuals’ lifestyle. With this paper, we are the first ones to address the development of novel tools for automatic discovery of routine days of an individual from his/her egocentric images. In the proposed model, sequences of images are firstly characterized by semantic labels detected by pre-trained CNNs. Then, these features are organized in temporal-semantic documents to later be embedded into a topic models space. Finally, Dynamic-Time-Warping and Spectral-Clustering methods are used for final day routine/non-routine discrimination. Moreover, we introduce a new EgoRoutine-dataset, a collection of 104 egocentric days with more than 100.000 images recorded by 7 users. Results show that routine can be discovered and behavioural patterns can be observed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301333",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Data science",
      "Dynamic time warping",
      "Image warping",
      "Information retrieval",
      "Operating system",
      "Space (punctuation)"
    ],
    "authors": [
      {
        "surname": "Talavera",
        "given_name": "Estefania"
      },
      {
        "surname": "Wuerich",
        "given_name": "Carolin"
      },
      {
        "surname": "Petkov",
        "given_name": "Nicolai"
      },
      {
        "surname": "Radeva",
        "given_name": "Petia"
      }
    ]
  },
  {
    "title": "Robust visual tracking by embedding combination and weighted-gradient optimization",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107339",
    "abstract": "Existing tracking-by-detection approaches build trackers on binary classifiers. Despite achieving state-of-the-art performance on tracking benchmarks, these trackers pay limited attention to data imbalance issue, e.g, positive and negative, easy and hard. In this paper, we demonstrate that separately learning feature embeddings corresponding to negative samples with different semantic characteristics is effective in reducing the background diversity to handle the imbalance between positive and negative samples, which facilitates background awareness of classifiers. Specifically, we propose a negative sample embedding combination network, which helps to learn several sub-embeddings and combine them to build a robust classifier. In addition, we propose a weighted-gradient loss to handle the imbalance between easy and hard samples. The gradient contribution of each sample to model training is dynamically weighted according to the gradient distribution, which prevents easy samples from overwhelming model training. Extensive experiments on benchmarks demonstrate that our tracker performs favorably against state-of-the-art algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301424",
    "keywords": [
      "Artificial intelligence",
      "Binary classification",
      "Biochemistry",
      "BitTorrent tracker",
      "Chemistry",
      "Chromatography",
      "Classifier (UML)",
      "Computer science",
      "Embedding",
      "Eye tracking",
      "Feature (linguistics)",
      "Gene",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Robustness (evolution)",
      "Sample (material)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Feng",
        "given_name": "Jin"
      },
      {
        "surname": "Xu",
        "given_name": "Peng"
      },
      {
        "surname": "Pu",
        "given_name": "Shi"
      },
      {
        "surname": "Zhao",
        "given_name": "Kaili"
      },
      {
        "surname": "Zhang",
        "given_name": "Honggang"
      }
    ]
  },
  {
    "title": "How distance metrics influence missing data imputation with k-nearest neighbours",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.05.032",
    "abstract": "In missing data contexts, k-nearest neighbours imputation has proven beneficial since it takes advantage of the similarity between patterns to replace missing values. When dealing with heterogeneous data, researchers traditionally apply the HEOM distance, that handles continuous, nominal and missing data. Although other heterogeneous distances have been proposed, they have not yet been investigated and compared for k-nearest neighbours imputation. In this work, we study the effect of several heterogeneous distances on k-nearest neighbours imputation on a large benchmark of publicly-available datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302099",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Imputation (statistics)",
      "Machine learning",
      "Missing data",
      "Nearest neighbor search",
      "Nearest neighbour",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Santos",
        "given_name": "Miriam Seoane"
      },
      {
        "surname": "Abreu",
        "given_name": "Pedro Henriques"
      },
      {
        "surname": "Wilk",
        "given_name": "Szymon"
      },
      {
        "surname": "Santos",
        "given_name": "João"
      }
    ]
  },
  {
    "title": "Deep imitator: Handwriting calligraphy imitation via deep attention networks",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107080",
    "abstract": "Calligraphy imitation (CI) from a handful of target handwriting samples is such a challenging task that most of the existing writing style analysis or handwriting generation methods do not exhibit satisfactory performance. In this paper, we propose a novel multi-module framework to address the problem of CI. Firstly, we utilized a deep convolution neural network (CNN) to extract personalized calligraphical features. Then we built a calligraphy-clustering attention module and a mata-style matrix (msM) to compute an embedding of calligraphy. The structure of conditional gated recurrent unit (cGRU) is then improved to predict the probabilistic density of pen tip movement displacement by dual condition inputs. Finally, we generated personalized handwriting stroke sequences through iterative sampling with Gaussian mixture model (GMM). Experiments on public online handwriting databases verify that the proposed method could achieve satisfactory performance; the generated samples achieved high similarities with original handwriting examples.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319303814",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Calligraphy",
      "Computer science",
      "Deep learning",
      "Handwriting",
      "Painting",
      "Pattern recognition (psychology)",
      "Speech recognition",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Bocheng"
      },
      {
        "surname": "Tao",
        "given_name": "Jianhua"
      },
      {
        "surname": "Yang",
        "given_name": "Minghao"
      },
      {
        "surname": "Tian",
        "given_name": "Zhengkun"
      },
      {
        "surname": "Fan",
        "given_name": "Cunhang"
      },
      {
        "surname": "Bai",
        "given_name": "Ye"
      }
    ]
  },
  {
    "title": "On the performance of Matthews correlation coefficient (MCC) for imbalanced dataset",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.03.030",
    "abstract": "The Matthews Correlation Coefficient (MCC) is one of the popular measurements for classification accuracy. It has been generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The study of this paper finds that this is not true. MCC deteriorates seriously when the dataset in classification are imbalanced. Experiment results and analysis show that MCC is not suitable for classification accuracy measurement on imbalanced datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552030115X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Correlation",
      "Correlation coefficient",
      "Data mining",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Matthews correlation coefficient",
      "Measure (data warehouse)",
      "Pattern recognition (psychology)",
      "Statistics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Qiuming"
      }
    ]
  },
  {
    "title": "Modality-specific and shared generative adversarial network for cross-modal retrieval",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107335",
    "abstract": "Cross-modal retrieval aims to realize accurate and flexible retrieval across different modalities of data, e.g., image and text, which has achieved significant progress in recent years, especially since generative adversarial networks (GAN) were used. However, there still exists much room for improvement. How to jointly extract and utilize both the modality-specific (complementarity) and modality-shared (correlation) features effectively has not been well studied. In this paper, we propose an approach named Modality-Specific and Shared Generative Adversarial Network (MS2GAN) for cross-modal retrieval. The network architecture consists of two sub-networks that aim to learn modality-specific features for each modality, followed by a common sub-network that aims to learn the modality-shared features for each modality. Network training is guided by the adversarial scheme between the generative and discriminative models. The generative model learns to predict the semantic labels of features, model the inter- and intra-modal similarity with label information, and ensure the difference between the modality-specific and modality-shared features, while the discriminative model learns to classify the modality of features. The learned modality-specific and shared feature representations are jointly used for retrieval. Experiments on three widely used benchmark multi-modal datasets demonstrate that MS2GAN can outperform state-of-the-art related works.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301382",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Chemistry",
      "Computer science",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature learning",
      "Generative grammar",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Modal",
      "Modalities",
      "Modality (human–computer interaction)",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Polymer chemistry",
      "Similarity (geometry)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Fei"
      },
      {
        "surname": "Jing",
        "given_name": "Xiao-Yuan"
      },
      {
        "surname": "Wu",
        "given_name": "Zhiyong"
      },
      {
        "surname": "Ji",
        "given_name": "Yimu"
      },
      {
        "surname": "Dong",
        "given_name": "Xiwei"
      },
      {
        "surname": "Luo",
        "given_name": "Xiaokai"
      },
      {
        "surname": "Huang",
        "given_name": "Qinghua"
      },
      {
        "surname": "Wang",
        "given_name": "Ruchuan"
      }
    ]
  },
  {
    "title": "Video semantic segmentation via feature propagation with holistic attention",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107268",
    "abstract": "Since the frames of a video are inherently contiguous, information redundancy is ubiquitous. Unlike previous works densely process each frame of a video, in this paper we present a novel method to focus on efficient feature propagation across frames to tackle the challenging video semantic segmentation task. Firstly, we propose a Light, Efficient and Real-time network (denoted as LERNet) as a strong backbone network for per-frame processing. Then we mine rich features within a key frame and propagate the across-frame consistency information by calculating a temporal holistic attention with the following non-key frame. Each element of the attention matrix represents the global correlation between pixels of a non-key frame and the previous key frame. Concretely, we propose a brand-new attention module to capture the spatial consistency on low-level features along temporal dimension. Then we employ the attention weights as a spatial transition guidance for directly generating high-level features of the current non-key frame from the weighted corresponding key frame. Finally, we efficiently fuse the hierarchical features of the non-key frame and obtain the final segmentation result. Extensive experiments on two popular datasets, i.e. the CityScapes and the CamVid, demonstrate that the proposed approach achieves a remarkable balance between inference speed and accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030073X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Feature (linguistics)",
      "Frame (networking)",
      "Key (lock)",
      "Key frame",
      "Linguistics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Redundancy (engineering)",
      "Segmentation",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Junrong"
      },
      {
        "surname": "Wen",
        "given_name": "Zongzheng"
      },
      {
        "surname": "Zhao",
        "given_name": "Sanyuan"
      },
      {
        "surname": "Huang",
        "given_name": "Kele"
      }
    ]
  },
  {
    "title": "SceneAdapt: Scene-based domain adaptation for semantic segmentation using adversarial learning",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.06.002",
    "abstract": "Semantic segmentation methods have achieved outstanding performance thanks to deep learning. Nevertheless, when such algorithms are deployed to new contexts not seen during training, it is necessary to collect and label scene-specific data in order to adapt them to the new domain using fine-tuning. This process is required whenever an already installed camera is moved or a new camera is introduced in a camera network due to the different scene layouts induced by the different viewpoints. To limit the amount of additional training data to be collected, it would be ideal to train a semantic segmentation method using labeled data already available and only unlabeled data coming from the new camera. We formalize this problem as a domain adaptation task and introduce a novel dataset of urban scenes with the related semantic labels. As a first approach to address this challenging task, we propose SceneAdapt, a method for scene adaptation of semantic segmentation algorithms based on adversarial learning. Experiments and comparisons with state-of-the-art approaches to domain adaptation highlight that promising performance can be achieved using adversarial learning both when the two scenes have different but points of view, and when they comprise images of completely different scenes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302208",
    "keywords": [
      "Adaptation (eye)",
      "Adversarial system",
      "Art",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Economics",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Process (computing)",
      "Segmentation",
      "Task (project management)",
      "Viewpoints",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Di Mauro",
        "given_name": "Daniele"
      },
      {
        "surname": "Furnari",
        "given_name": "Antonino"
      },
      {
        "surname": "Patanè",
        "given_name": "Giuseppe"
      },
      {
        "surname": "Battiato",
        "given_name": "Sebastiano"
      },
      {
        "surname": "Farinella",
        "given_name": "Giovanni Maria"
      }
    ]
  },
  {
    "title": "Understanding trained CNNs by indexing neuron selectivity",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.10.013",
    "abstract": "The impressive performance of Convolutional Neural Networks (CNNs) when solving different vision problems is shadowed by their black-box nature and our consequent lack of understanding of the representations they build and how these representations are organized. To help understanding these issues, we propose to describe the activity of individual neurons by their Neuron Feature visualization and quantify their inherent selectivity with two specific properties. We explore selectivity indexes for: an image feature (color); and an image label (class membership). Our contribution is a framework to seek or classify neurons by indexing on these selectivity properties. It helps to find color selective neurons, such as a red-mushroom neuron in layer Conv4 or class selective neurons such as dog-face neurons in layer Conv5 in VGG-M, and establishes a methodology to derive other selectivity properties. Indexing on neuron selectivity can statistically draw how features and classes are represented through layers in a moment when the size of trained nets is growing and automatic tools to index neurons can be helpful.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519302909",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Linguistics",
      "Neuron",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Psychology",
      "Search engine indexing",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Rafegas",
        "given_name": "Ivet"
      },
      {
        "surname": "Vanrell",
        "given_name": "Maria"
      },
      {
        "surname": "Alexandre",
        "given_name": "Luís A."
      },
      {
        "surname": "Arias",
        "given_name": "Guillem"
      }
    ]
  },
  {
    "title": "Multi-focus image fusion based on non-negative sparse representation and patch-level consistency rectification",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107325",
    "abstract": "Most existing sparse representation-based (SR) fusion methods consider the local information of each image patch independently during fusion. Some spatial artifacts are easily introduced to the fused image. A sliding window technology is often employed by these methods to overcome this issue. However, this comes at the cost of high computational complexity. Alternatively, we come up with a novel multi-focus image fusion method that takes full consideration of the strong correlations among spatially adjacent image patches with NO need for a sliding window. To this end, a non-negative SR model with local consistency constraint (CNNSR) on the representation coefficients is first constructed to encode each image patch. Then a patch-level consistency rectification strategy is presented to merge the input image patches, by which the spatial artifacts in the fused images are greatly reduced. As well, a compact non-negative dictionary is constructed for the CNNSR model. Experimental results demonstrate that the proposed fusion method outperforms some state-of-the art methods. Moreover, the proposed method is computationally efficient, thereby facilitating real-world applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030128X",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Constraint satisfaction problem",
      "ENCODE",
      "Focus (optics)",
      "Fusion",
      "Gene",
      "Image (mathematics)",
      "Image fusion",
      "Information retrieval",
      "Law",
      "Linguistics",
      "Local consistency",
      "Merge (version control)",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Power (physics)",
      "Probabilistic logic",
      "Quantum mechanics",
      "Rectification",
      "Representation (politics)",
      "Sliding window protocol",
      "Sparse approximation",
      "Window (computing)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Qiang"
      },
      {
        "surname": "Li",
        "given_name": "Guanghe"
      },
      {
        "surname": "Cao",
        "given_name": "Yunfeng"
      },
      {
        "surname": "Han",
        "given_name": "Jungong"
      }
    ]
  },
  {
    "title": "Overview of deep-learning based methods for salient object detection in videos",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107340",
    "abstract": "Video salient object detection is a challenging and important problem in computer vision domain. In recent years, deep-learning based methods have contributed to significant improvements in this domain. This paper provides an overview of recent developments in this domain and compares the corresponding methods up to date, including 1) Classification of the state-of-the-art methods and their frameworks; 2) summary of the benchmark datasets and commonly used evaluation metrics; 3) experimental comparison of the performances of the state-of-the-art methods; 4) suggestions of some promising future works for unsolved challenges.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301436",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Domain (mathematical analysis)",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Qiong"
      },
      {
        "surname": "Zhang",
        "given_name": "Lu"
      },
      {
        "surname": "Li",
        "given_name": "Yan"
      },
      {
        "surname": "Kpalma",
        "given_name": "Kidiyo"
      }
    ]
  },
  {
    "title": "Video anomaly detection and localization using motion-field shape description and homogeneity testing",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107394",
    "abstract": "Detection and localization of abnormal behaviors in surveillance videos of crowded scenes is challenging, where high-density people and various objects performing highly unpredictable motions lead to severe occlusions, making object segmentation and tracking extremely difficult. We associate the optical flows between multiple frames to capture short-term trajectories and introduce the histogram-based shape descriptor to describe such short-term trajectories, which reflects faithfully the motion trend and details in local patches. Furthermore, we propose a method to detect anomalies over time and space by judging whether the similarities between the testing sample and the retrieved K-NN samples follow the pattern distribution of homogeneous intra-class similarities, which is unsupervised one-class learning requiring no clustering nor prior assumption. Such a scheme can adapt to the whole scene, since the probability is used to judge and the calculation of probability is not affected by motion distortions arising from perspective distortion, which gains advantage over the existing solutions. We conduct experiments on real-world surveillance videos, and the results demonstrate that the proposed method can reliably detect and locate the abnormal events in video sequences, outperforming the state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301977",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Histogram",
      "Image (mathematics)",
      "Optical flow",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Perspective distortion",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Xinfeng"
      },
      {
        "surname": "Yang",
        "given_name": "Su"
      },
      {
        "surname": "Zhang",
        "given_name": "Jiulong"
      },
      {
        "surname": "Zhang",
        "given_name": "Weishan"
      }
    ]
  },
  {
    "title": "Deep multi-person kinship matching and recognition for family photos",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107342",
    "abstract": "In this paper, we propose a novel Deep Kinship Matching and Recognition (DKMR) framework for multi-person kinship matching and recognition, which is a complicated and challenging task with little previous literature. Compared with most existing kinship understanding methods that mainly work on matching kinship in pairwise face images, we target at recognizing the exact kinship in nuclear family photos consisting of multiple persons. The proposed DKMR framework contains three modules. Firstly, we design a deep kinship matching model (termed DKM-TRL) to predict kin-or-not scores by integrating the triple ranking loss into a Siamese CNN model. Secondly, we develop a deep kinship recognition model (named DKR-GA) to predict the exact kinship categories, in which gender and relative age attributes are utilized to learn more discriminative representations. Thirdly, based on the outputs of DKM-TRL and DKR-GA, we propose a reasoning conditional random field (R-CRF) model to infer the corresponding optimal family tree by exploiting the common kinship knowledge of a nuclear family. To evaluate the effectiveness of our DKMR framework, we conduct extensive experiments and the results show that it can gain superior performance on Group-Face dataset, TSKinFace dataset and FIW dataset over state-of-the-arts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030145X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Conditional random field",
      "Discriminative model",
      "Face (sociological concept)",
      "Facial recognition system",
      "Kinship",
      "Law",
      "Machine learning",
      "Matching (statistics)",
      "Mathematics",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Political science",
      "Random forest",
      "Ranking (information retrieval)",
      "Social science",
      "Sociology",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Mengyin"
      },
      {
        "surname": "Shu",
        "given_name": "Xiangbo"
      },
      {
        "surname": "Feng",
        "given_name": "Jiashi"
      },
      {
        "surname": "Wang",
        "given_name": "Xun"
      },
      {
        "surname": "Tang",
        "given_name": "Jinhui"
      }
    ]
  },
  {
    "title": "Learning Direct Optimization for scene understanding",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107369",
    "abstract": "We develop a Learning Direct Optimization (LiDO) method for the refinement of a latent variable model that describes input image x. Our goal is to explain a single image x with an interpretable 3D computer graphics model having scene graph latent variables z (such as object appearance, camera position). Given a current estimate of z we can render a prediction of the image g(z), which can be compared to the image x. The standard way to proceed is then to measure the error E(x, g(z)) between the two, and use an optimizer to minimize the error. However, it is unknown which error measure E would be most effective for simultaneously addressing issues such as misaligned objects, occlusions, textures, etc. In contrast, the LiDO approach trains a Prediction Network to predict an update directly to correct z, rather than minimizing the error with respect to z. Experiments show that LiDO converges rapidly as it does not need to perform a search on the error landscape, produces better solutions than error-based competitors, and is able to handle the mismatch between the data and the fitted scene model. We apply LiDO to a realistic synthetic dataset, and show that the method also transfers to work well with real images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301722",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Data mining",
      "Economics",
      "Finance",
      "Graph",
      "Graphics",
      "Image (mathematics)",
      "Latent variable",
      "Mathematical analysis",
      "Mathematics",
      "Measure (data warehouse)",
      "Object (grammar)",
      "Position (finance)",
      "Theoretical computer science",
      "Variable (mathematics)"
    ],
    "authors": [
      {
        "surname": "Romaszko",
        "given_name": "Lukasz"
      },
      {
        "surname": "Williams",
        "given_name": "Christopher K.I."
      },
      {
        "surname": "Winn",
        "given_name": "John"
      }
    ]
  },
  {
    "title": "Stacked squeeze-and-excitation recurrent residual network for visual-semantic matching",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107359",
    "abstract": "In recent years, visual-textual matching has been widely studied in the intersection of computer vision and natural language processing communities. A feasible scheme for learning discriminative representations is leveraging hierarchical features to align both modalities at multiple semantic levels. However, most existing approaches rely on pre-trained object detectors or semantic parsers to generate multi-level representations, whose performance is overly dependent on the extra supervision and thereby leads to its vulnerability. In this paper, we introduce a Stacked Squeeze-and-Excitation Recurrent Residual Network (SER2-Net) for visual-textual matching. Firstly, an efficient multi-level representation module is presented to produce a series of semantically discriminative features without the aid of extra supervision, which is built by stacking the squeeze-and-excitation recurrent residual (SER2) learning components. Specifically, SER2 incorporates the residual learning and inverse recurrent connection into the squeeze-and-excitation learning block, which allows for utilizing complementary current information and residual information to improve the modality-specific representation ability. Besides, to capture the implicit correlations contained among multi-level features, we propose a novel objective namely Cross-modal Semantic Discrepancy (CMSD) loss, which is characterized by exploiting the interdependency among different semantic levels to narrow the cross-modal distribution discrepancy. Extensive experiments on two benchmark datasets validate the superiority of our model, which compares favorably with the state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030162X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Matching (statistics)",
      "Mathematics",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Residual",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Haoran"
      },
      {
        "surname": "Ji",
        "given_name": "Zhong"
      },
      {
        "surname": "Lin",
        "given_name": "Zhigang"
      },
      {
        "surname": "Pang",
        "given_name": "Yanwei"
      },
      {
        "surname": "Li",
        "given_name": "Xuelong"
      }
    ]
  },
  {
    "title": "Spectral rotation for deep one-step clustering",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107175",
    "abstract": "Previous spectral clustering methods sequentially conduct three steps, i.e., similarity matrix learning from original data, spectral representation learning, and K-means clustering on spectral representation, respectively, to difficultly output robust clustering result even though each of three steps achieves individual optimization. The reason is that each goal of former two steps is not focused on achieving optimal clustering result. Moreover, original data usually contains noise to affect the clustering result, as well as has high-dimensional representation to easily result in the curse of dimensionality. In this paper, we propose a deep spectral clustering method which embeds four parts (i.e., similarity matrix learning, spectral representation learning, optimized K-means clustering, and transformation matrix learning) in a unified framework with the following advantages: 1) similarity matrix is obtained from the low-dimensional feature space of original data where the influence of both noise and high-dimensional data are considered; 2) optimized K-means clustering rotates original result of K-means clustering to search optimized clustering hyperplane which partitions data points into clusters; and 3) each of four parts is iteratively updated so that the clustering result is obtained based on the feedback of other three parts. As a result, our proposed framework develops a two-task deep clustering model with linear activation functions to output effective clustering result. Experimental results on real data sets show the effectiveness of our method in terms of four clustering evaluation metrics, compared to state-of-the-art clustering methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304753",
    "keywords": [
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Computer science",
      "Correlation clustering",
      "Data stream clustering",
      "Fuzzy clustering",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Similarity (geometry)",
      "Single-linkage clustering",
      "Spectral clustering"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Xiaofeng"
      },
      {
        "surname": "Zhu",
        "given_name": "Yonghua"
      },
      {
        "surname": "Zheng",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Human behaviour modelling for welfare technology using hidden Markov models",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.09.022",
    "abstract": "Human behaviour modelling for welfare technology is the task of recognizing a person's behaviour patterns in order to construct a safe environment for that person. It is useful in building environments for older adults or to help any person in his or her daily life. The aim of this study is to model the behaviour of a person living in a smart house environment in order to detect abnormal behaviour and assist the person if help is needed. Hidden Markov models, location of the person in the house, posture of the person, and time frame rules are implemented using a real-world, open-source dataset for training and testing. The proposed model presented in this study models the normal behaviour of a person and detects anomalies in the usual pattern. The model shows good results in the identification of abnormal behaviour when tested.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519302685",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Construct (python library)",
      "Economics",
      "Engineering",
      "Finance",
      "Frame (networking)",
      "Hidden Markov model",
      "Identification (biology)",
      "Machine learning",
      "Markov chain",
      "Markov model",
      "Order (exchange)",
      "Programming language",
      "Systems engineering",
      "Task (project management)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Sánchez",
        "given_name": "Veralia Gabriela"
      },
      {
        "surname": "Lysaker",
        "given_name": "Ola Marius"
      },
      {
        "surname": "Skeie",
        "given_name": "Nils-Olav"
      }
    ]
  },
  {
    "title": "Heterogenous output regression network for direct face alignment",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107311",
    "abstract": "Face alignment has gained great popularity in computer vision due to its wide-spread applications. In this paper, we propose a novel learning architecture, i.e., heterogenous output regression network (HORNet), for face alignment, which directly predicts facial landmarks from images. HORNet is based on kernel approximations and establishes a new compact multi-layer architecture. A nonlinear layer with cosine activations disentangles nonlinear relationships between representations of images and shapes of facial landmarks. A linear layer with identity activations explicitly encodes landmark correlations by low-rank learning via matrix elastic nets. HORNet is highly flexible and can work either with pre-built feature representations or with convolutional architectures for end-to-end learning. HORNet leverages the strengths of both kernel methods in modeling nonlinearities and of neural networks in structural prediction. This combination renders it effective and efficient for direct face alignment. Extensive experiments on five in-the-wild datasets show that HORNet delivers high performance and consistently exceeds state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301151",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Convolutional neural network",
      "Face (sociological concept)",
      "Feature (linguistics)",
      "Kernel (algebra)",
      "Landmark",
      "Layer (electronics)",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Network architecture",
      "Nonlinear system",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Zhen",
        "given_name": "Xiantong"
      },
      {
        "surname": "Yu",
        "given_name": "Mengyang"
      },
      {
        "surname": "Xiao",
        "given_name": "Zehao"
      },
      {
        "surname": "Zhang",
        "given_name": "Lei"
      },
      {
        "surname": "Shao",
        "given_name": "Ling"
      }
    ]
  },
  {
    "title": "Adaptive iterative attack towards explainable adversarial robustness",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107309",
    "abstract": "Image classifiers based on deep neural networks show severe vulnerability when facing adversarial examples crafted on purpose. Designing more effective and efficient adversarial attacks is attracting considerable interest due to its potential contribution to interpretability of deep learning and validation of neural networks’ robustness. However, current iterative attacks use a fixed step size for each noise-adding step, making further investigation into the effect of variable step size on model robustness ripe for exploration. We prove that if the upper bound of noise added to the original image is fixed, the attack effect can be improved if the step size is positively correlated with the gradient obtained at each step by querying the target model. In this paper, we propose Ada-FGSM (Adaptive FGSM), a new iterative attack that adaptively allocates step size of noises according to gradient information at each step. Improvement of success rate and accuracy decrease measured on ImageNet with multiple models emphasizes the validity of our method. We analyze the process of iterative attack by visualizing their trajectory and gradient contour, and further explain the vulnerability of deep neural networks to variable step size adversarial examples.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301138",
    "keywords": [
      "Adversarial system",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Deep neural networks",
      "Gene",
      "Interpretability",
      "Iterative method",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Yucheng"
      },
      {
        "surname": "Han",
        "given_name": "Yahong"
      },
      {
        "surname": "Zhang",
        "given_name": "Quanxin"
      },
      {
        "surname": "Kuang",
        "given_name": "Xiaohui"
      }
    ]
  },
  {
    "title": "Efficient adaptive inference for deep convolutional neural networks using hierarchical early exits",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107346",
    "abstract": "Early exits are capable of providing deep learning models with adaptive computational graphs that can readily adapt on-the-fly to the available resources. Despite their advantages, existing early exit methods suffer from many limitations which limit their performance, e.g., they ignore the information extracted from previous exit layers, they are unable to efficiently handle feature maps with large sizes, etc. To overcome these limitations we propose a Bag-of-Features (BoF)-based method that is capable of constructing efficient hierarchical early exit layers with minimal computational overhead, while also providing an adaptive inference method that allows for early stopping the inference process when the network is confident enough for its output, leading to significant performance benefits. To this end, the BoF model is extended and adapted to the needs of early exits by constructing additive shared histogram spaces that gradually refine the information extracted from the various layers of a network, in a hierarchical manner, while also employing a classification layer reuse strategy to further reduce the number of parameters needed per exit layer. Note that the proposed method is generic and can be readily combined with any neural network architecture. The effectiveness of the proposed method is demonstrated using five different image datasets, proving that early exits can be readily transformed into a practical tool, which can be effectively used in various real-world embedded applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301497",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Chemistry",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Ecology",
      "Feature (linguistics)",
      "Inference",
      "Layer (electronics)",
      "Linguistics",
      "Machine learning",
      "Operating system",
      "Organic chemistry",
      "Overhead (engineering)",
      "Philosophy",
      "Process (computing)",
      "Reuse"
    ],
    "authors": [
      {
        "surname": "Passalis",
        "given_name": "Nikolaos"
      },
      {
        "surname": "Raitoharju",
        "given_name": "Jenni"
      },
      {
        "surname": "Tefas",
        "given_name": "Anastasios"
      },
      {
        "surname": "Gabbouj",
        "given_name": "Moncef"
      }
    ]
  },
  {
    "title": "Reinterpreting CTC training as iterative fitting",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107392",
    "abstract": "The connectionist temporal classification (CTC) enables end-to-end sequence learning by maximizing the probability of correctly recognizing sequences during training. The outputs of a CTC-trained model tend to form a series of spikes separated by strongly predicted blanks, know as the spiky problem. To figure out the reason for it, we reinterpret the CTC training process as an iterative fitting task that is based on frame-wise cross-entropy loss. It offers us an intuitive way to compare target probabilities with model outputs for each iteration, and explain how the model outputs gradually turns spiky. Inspired by it, we put forward two ways to modify the CTC training. The experiments demonstrate that our method can well solve the spiky problem and moreover, lead to faster convergence over various training settings. Beside this, the reinterpretation of CTC, as a brand new perspective, may be potentially useful in other situations. The code is publicly available at https://github.com/hzli-ucas/caffe/tree/ctc.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301953",
    "keywords": [
      "Acoustics",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Code (set theory)",
      "Computer science",
      "Connectionism",
      "Frame (networking)",
      "Genetics",
      "Iterative and incremental development",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Process (computing)",
      "Programming language",
      "Reinterpretation",
      "Sequence (biology)",
      "Set (abstract data type)",
      "Software engineering",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Hongzhu"
      },
      {
        "surname": "Wang",
        "given_name": "Weiqiang"
      }
    ]
  },
  {
    "title": "COMBAHO: A deep learning system for integrating brain injury patients in society",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.02.013",
    "abstract": "In the last years, the care of dependent people, either by disease, accident, disability, or age, is one of the current priority research topics in developed countries. Moreover, such care is intended to be at patients home, in order to minimize the cost of therapies. Patients rehabilitation will be fulfilled when their integration in society is achieved, either in the family or in a work environment. To address this challenge, we propose the development and evaluation of an assistant for people with acquired brain injury or dependents. This assistant is twofold: in the patient’s home is based on the design and use of an intelligent environment with abilities to monitor and active learning, combined with an autonomous social robot for interactive assistance and stimulation. On the other hand, it is complemented with an outdoor assistant, to help patients under disorientation or complex situations. This involves the integration of several existing technologies and provides solutions to a variety of technological challenges. Deep leaning-based techniques are proposed as core technology to solve these problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519300534",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Human–computer interaction",
      "Medicine",
      "Physical therapy",
      "Psychology",
      "Rehabilitation",
      "Variety (cybernetics)"
    ],
    "authors": [
      {
        "surname": "Garcia-Rodriguez",
        "given_name": "Jose"
      },
      {
        "surname": "Gomez-Donoso",
        "given_name": "Francisco"
      },
      {
        "surname": "Oprea",
        "given_name": "Sergiu"
      },
      {
        "surname": "Garcia-Garcia",
        "given_name": "Alberto"
      },
      {
        "surname": "Cazorla",
        "given_name": "Miguel"
      },
      {
        "surname": "Orts-Escolano",
        "given_name": "Sergio"
      },
      {
        "surname": "Bauer",
        "given_name": "Zuria"
      },
      {
        "surname": "Castro-Vargas",
        "given_name": "John"
      },
      {
        "surname": "Escalona",
        "given_name": "Felix"
      },
      {
        "surname": "Ivorra-Piqueres",
        "given_name": "David"
      },
      {
        "surname": "Martinez-Gonzalez",
        "given_name": "Pablo"
      },
      {
        "surname": "Aguirre",
        "given_name": "Eugenio"
      },
      {
        "surname": "Garcia-Silviente",
        "given_name": "Miguel"
      },
      {
        "surname": "Garcia-Perez",
        "given_name": "Marcelo"
      },
      {
        "surname": "Cañas",
        "given_name": "Jose M."
      },
      {
        "surname": "Martin-Rico",
        "given_name": "Francisco"
      },
      {
        "surname": "Gines",
        "given_name": "Jonathan"
      },
      {
        "surname": "Rivas-Montero",
        "given_name": "Francisco"
      }
    ]
  },
  {
    "title": "Wearable assistive devices for visually impaired: A state of the art survey",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.10.031",
    "abstract": "Recent statistics of the World Health Organization (WHO), published in October 2017, estimate that more than 253 million people worldwide suffer from visual impairment (VI) with 36 million of blinds and 217 million people with low vision. In the last decade, there was a tremendous amount of work in developing wearable assistive devices dedicated to the visually impaired people, aiming at increasing the user cognition when navigating in known/unknown, indoor/outdoor environments, and designed to improve the VI quality of life. This paper presents a survey of wearable/assistive devices and provides a critical presentation of each system, while emphasizing related strengths and limitations. The paper is designed to inform the research community and the VI people about the capabilities of existing systems, the progress in assistive technologies and provide a glimpse in the possible short/medium term axes of research that can improve existing devices. The survey is based on various features and performance parameters, established with the help of the blind community that allows systems classification using both qualitative and quantitative measu.res of evaluation. This makes it possible to rank the analyzed systems based on their potential impact on the VI people life.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518308602",
    "keywords": [
      "Assistive technology",
      "Augmented reality",
      "Computer science",
      "Data science",
      "Embedded system",
      "Human–computer interaction",
      "Low vision",
      "Medicine",
      "Multimedia",
      "Optometry",
      "Presentation (obstetrics)",
      "Psychiatry",
      "Psychology",
      "Radiology",
      "Visual impairment",
      "Visually impaired",
      "Wearable computer",
      "Wearable technology"
    ],
    "authors": [
      {
        "surname": "Tapu",
        "given_name": "Ruxandra"
      },
      {
        "surname": "Mocanu",
        "given_name": "Bogdan"
      },
      {
        "surname": "Zaharia",
        "given_name": "Titus"
      }
    ]
  },
  {
    "title": "Supervised dimensionality reduction of proportional data using mixture estimation",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107379",
    "abstract": "In this paper, an effective novel approach for dimensionality reduction of labeled proportional data is proposed. By avoiding formulating an eigenvalue problem and constructing a neighborhood graph, the introduced method mitigates some of the major problems from which the well-known algorithms in this category suffer. These disadvantages include problem handling multi-modal or sparse data as well as curse of dimensionality. The devised method transfers the data from high-dimensional space into low-dimensional space using a linear transform which is optimized using an information theoretic measure. To find this projection, a novel approach has been adopted in which projected data are transfered into the low-dimensional space first, and a mixture of distributions is estimated using the projected data for each class separately. In the next step, the distance between the estimated distributions is used as a measure of separation for data classes, and a heuristic search is carried on to find the optimal projection. The effectiveness of the proposed algorithm is demonstrated using different datasets in different scenarios in comparison with other well-known algorithms in the literature.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301825",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Computer science",
      "Curse of dimensionality",
      "Data mining",
      "Dimensionality reduction",
      "Eigenvalues and eigenvectors",
      "Graph",
      "Heuristic",
      "Mathematical optimization",
      "Mathematics",
      "Measure (data warehouse)",
      "Pattern recognition (psychology)",
      "Physics",
      "Projection (relational algebra)",
      "Quantum mechanics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Masoudimansour",
        "given_name": "Walid"
      },
      {
        "surname": "Bouguila",
        "given_name": "Nizar"
      }
    ]
  },
  {
    "title": "Deep quantization generative networks",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107338",
    "abstract": "Equipped with powerful convolutional neural networks (CNNs), generative models have achieved tremendous success in various vision applications. However, deep generative networks suffer from high computational and memory costs in both model training and deployment. While many efforts have been devoted to accelerate discriminative models by quantization, effectively reducing the costs for deep generative models is more challenging and remains unexplored. In this work, we investigate applying quantization technology to deep generative models. We find that keeping as much information as possible for quantized activations is key to obtain high-quality generative models. With this in mind, we propose Deep Quantization Generative Networks (DQGNs) to effectively accelerate and compress deep generative networks. By expanding the dimensions of the quantization basis space, DQGNs can achieve lower quantization error and are highly adaptive to complex data distributions. Various experiments on two powerful frameworks (i.e., variational auto-encoders, and generative adversarial networks) and two practical applications (i.e., style transfer, and super-resolution) demonstrate our findings and the effectiveness of our proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301412",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Discriminative model",
      "Generative grammar",
      "Generative model",
      "Machine learning",
      "Quantization (signal processing)"
    ],
    "authors": [
      {
        "surname": "Wan",
        "given_name": "Diwen"
      },
      {
        "surname": "Shen",
        "given_name": "Fumin"
      },
      {
        "surname": "Liu",
        "given_name": "Li"
      },
      {
        "surname": "Zhu",
        "given_name": "Fan"
      },
      {
        "surname": "Huang",
        "given_name": "Lei"
      },
      {
        "surname": "Yu",
        "given_name": "Mengyang"
      },
      {
        "surname": "Shen",
        "given_name": "Heng Tao"
      },
      {
        "surname": "Shao",
        "given_name": "Ling"
      }
    ]
  },
  {
    "title": "Projection based weight normalization: Efficient method for optimization on oblique manifold in DNNs",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107317",
    "abstract": "Optimizing deep neural networks (DNNs) often suffers from the ill-conditioned problem. We observe that the scaling based weight space symmetry (SBWSS) in rectified nonlinear network will cause this negative effect. Therefore, we propose to constrain the incoming weights of each neuron to be unit-norm, which is formulated as an optimization problem over the Oblique manifold. A simple yet efficient method referred to as projection based weight normalization (PBWN) is also developed to solve this problem. This proposed method has the property of regularization and collaborates well with the commonly used batch normalization technique. We conduct comprehensive experiments on several widely-used image datasets including CIFAR-10, CIFAR-100, SVHN and ImageNet for supervised learning over the state-of-the-art neural networks. The experimental results show that our method is able to improve the performance of different architectures consistently. We also apply our method to Ladder network for semi-supervised learning on permutation invariant MNIST dataset, and our method achievers the state-of-the-art methods: we obtain test errors as 2.52%, 1.06%, and 0.91% with only 20, 50, and 100 labeled samples, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301175",
    "keywords": [
      "Algorithm",
      "Anthropology",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Deep neural networks",
      "Invariant (physics)",
      "MNIST database",
      "Mathematical physics",
      "Mathematics",
      "Nonlinear system",
      "Normalization (sociology)",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Regularization (linguistics)",
      "Resampling",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Lei"
      },
      {
        "surname": "Liu",
        "given_name": "Xianglong"
      },
      {
        "surname": "Qin",
        "given_name": "Jie"
      },
      {
        "surname": "Zhu",
        "given_name": "Fan"
      },
      {
        "surname": "Liu",
        "given_name": "Li"
      },
      {
        "surname": "Shao",
        "given_name": "Ling"
      }
    ]
  },
  {
    "title": "Learning and recognition for assistive computer vision",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.11.006",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519303204",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Combinatorics",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Decomposition",
      "Digital geometry",
      "Digital image",
      "Ecology",
      "Image (mathematics)",
      "Image processing",
      "Mathematical analysis",
      "Mathematics",
      "Neighbourhood (mathematics)",
      "Object (grammar)",
      "Operating system",
      "Relation (database)",
      "Space (punctuation)",
      "Three-dimensional space"
    ],
    "authors": [
      {
        "surname": "Farinella",
        "given_name": "Giovanni Maria"
      },
      {
        "surname": "Leo",
        "given_name": "Marco"
      },
      {
        "surname": "Medioni",
        "given_name": "Gerard G."
      },
      {
        "surname": "Trivedi",
        "given_name": "Mohan"
      }
    ]
  },
  {
    "title": "Robust contactless pulse transit time estimation based on signal quality metric",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.06.016",
    "abstract": "The pulse transit time (PTT) can provide valuable insight into cardiovascular health, specifically regarding arterial stiffness and blood pressure. Traditionally, PTT is derived by calculating the time difference between two photoplethysmography (PPG) measurements, which require a set of body-worn sensors attached to the skin. Recently, remote photoplethysmography (rPPG) has been proposed as a contactless monitoring alternative. The main problem with rPPG based PTT estimation is that motion artifacts affect the shape of waveform leading to the shift or over-detected peaks, which decreases the accuracy of PTT. To overcome this problem, this paper presents a robust pulse-by-pulse PTT estimation framework using a signal quality metric. By exploiting the local temporal information and global periodic characteristics, the metric automatically assesses pulse quality of signal on a pulse-by-pulse basis, and calculates the probabilities of the pulse peak being the actual peak. Furthermore, in order to cope with over-detected and shift pulse peaks, Kalman filter complemented by the proposed signal quality metric is used to adaptively adjust the peaks based on the estimated probability. All the refined peaks are finally used for pulse-by-pulse PTT estimation. The experiment results are promising, suggesting that the proposed framework provides a robust and more accurate PTT estimation in real applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519301837",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Detector",
      "Engineering",
      "Filter (signal processing)",
      "Metric (unit)",
      "Operations management",
      "Photoplethysmogram",
      "Programming language",
      "Pulse (music)",
      "Radar",
      "SIGNAL (programming language)",
      "Telecommunications",
      "Waveform"
    ],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "Xijian"
      },
      {
        "surname": "Tjahjadi",
        "given_name": "Tardi"
      }
    ]
  },
  {
    "title": "Unsupervised feature selection with adaptive multiple graph learning",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107375",
    "abstract": "Unsupervised feature selection methods try to select features which can well preserve the intrinsic structure of data. To represent such structure, conventional methods construct various graphs from data. In most cases, those different graphs often contain some consensus and complementary information. To make full use of such information, we construct multiple base graphs and learn an adaptive consensus graph from these base graphs for feature selection. In our method, we integrate the multiple graph learning and the feature selection into a unified framework, which can jointly characterize the structure of the data and select the features to preserve such structure. The underlying optimization problem is hard to solve, and we solve it via a block coordinate descent schema, whose convergence is guaranteed. The extensive experiments well demonstrate the effectiveness of our proposed framework.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301783",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Construct (python library)",
      "Coordinate descent",
      "Data mining",
      "Feature learning",
      "Feature selection",
      "Graph",
      "Machine learning",
      "Programming language",
      "Schema (genetic algorithms)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Peng"
      },
      {
        "surname": "Du",
        "given_name": "Liang"
      },
      {
        "surname": "Li",
        "given_name": "Xuejun"
      },
      {
        "surname": "Shen",
        "given_name": "Yi-Dong"
      },
      {
        "surname": "Qian",
        "given_name": "Yuhua"
      }
    ]
  },
  {
    "title": "Deep transductive network for generalized zero shot learning",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107370",
    "abstract": "Zero Shot Learning (ZSL) aims to learn projective functions on labeled seen data and transfer the learned functions to unseen classes by discovering their relationship with semantic embeddings. However, the mapping process often suffers from the domain shift problem caused by only using the labeled seen data. In this paper, we propose a novel explainable Deep Transductive Network (DTN) for the task of Generalized ZSL (GZSL) by training on both labeled seen data and unlabeled unseen data, with subsequent testing on both seen classes and unseen classes. The proposed network exploits a KL Divergence constraint to iteratively refine the probability of classifying unlabeled instances by learning from their high confidence assignments with the assistance of an auxiliary target distribution. Besides, to avoid the meaningless ascription assumption of unseen data on GZSL, we also propose an experimental paradigm by splitting the unseen data into two equivalent parts for training and testing respectively. Extensive experiments and detailed analysis demonstrate that our DTN can efficiently handle the problems and achieve the state-of-the-art performance on four popular datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301734",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Constraint (computer-aided design)",
      "Divergence (linguistics)",
      "Economics",
      "Exploit",
      "Geometry",
      "Labeled data",
      "Linguistics",
      "Machine learning",
      "Management",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Haofeng"
      },
      {
        "surname": "Liu",
        "given_name": "Li"
      },
      {
        "surname": "Long",
        "given_name": "Yang"
      },
      {
        "surname": "Zhang",
        "given_name": "Zheng"
      },
      {
        "surname": "Shao",
        "given_name": "Ling"
      }
    ]
  },
  {
    "title": "Feature mask network for person re-identification",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.02.015",
    "abstract": "Person re-identification aims at establishing the identity of a pedestrian from a gallery that contains images of people obtained from a multi-camera system, which has many applications in video surveillance for public security and safety. Many challenges such as occlusions, drastic lighting and pose variations across the camera views, and noise make this task highly challenging. While most approaches focus on learning features and metrics to derive better representations, we hypothesize that both local and global contextual cues are crucial for an accurate identity matching. To this end, we propose a Feature Mask Network (FMN) that takes advantage of ResNet high-level features to predict a feature map mask and then imposes it on the low-level features to dynamically re-weight different object parts for a complementary feature representation. This serves as an attention mechanism by allowing the network to focus on local details selectively. We frame the network training as a multi-task objective optimization, which further improves the learned feature descriptions. We conduct experiments on Market-1501, DukeMTMC-reID and CUHK03 datasets, where the proposed approach respectively achieves significant improvements and competitive results when compared to the state-of-the-art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519300546",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Computer vision",
      "Economics",
      "Feature (linguistics)",
      "Feature learning",
      "Focus (optics)",
      "Identification (biology)",
      "Law",
      "Linguistics",
      "Management",
      "Matching (statistics)",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Statistics",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Ding",
        "given_name": "Guodong"
      },
      {
        "surname": "Khan",
        "given_name": "Salman"
      },
      {
        "surname": "Tang",
        "given_name": "Zhenmin"
      },
      {
        "surname": "Porikli",
        "given_name": "Fatih"
      }
    ]
  },
  {
    "title": "Gated CNN: Integrating multi-scale feature layers for object detection",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107131",
    "abstract": "Different convolutional layers in an explainable CNN usually encode different kinds of semantic information for an image, thus the feature fusion approaches like SSD, DSSD, and FPN are widely employed to enhance the detection performance by integrating different results based on multiple convolutional layers. However, the typical fusion approaches first need to independently detect objects based on one convolutional layer before fusion, and this single layer may exist noises or be irrelevant to objects, resulting in detection failure. To tackle the above problem, this paper proposes “Gated CNN” (short for “G-CNN”) to introduce a “gate” structure to integrate multiple convolutional layers for object detection. Injected by multi-scale feature layers, a gate employs several filters to extract useful information and block noises by executing one more convolutional or deconvolutional operation simultaneously, thus a gate-based feature layer is more effective and efficient as compared to the convolutional one. Besides, G-CNN employs a detector with two branches to predict the locations and categories of objects, respectively, as well as an inter-class loss to help detectors learn discrepant information among categories. Therefore, the learned detectors could better differentiate similar objects of different categories. Extensive experiments are conducted on two image datasets (PASCAL VOC and COCO), and the results demonstrate that G-CNN outperforms the state-of-the-art approaches, with a mAP of 40.9% at 10.6 FPS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304327",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Block (permutation group theory)",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Convolutional code",
      "Convolutional neural network",
      "Decoding methods",
      "Detector",
      "ENCODE",
      "Feature (linguistics)",
      "Feature extraction",
      "Gene",
      "Geometry",
      "Layer (electronics)",
      "Linguistics",
      "Mathematics",
      "Object detection",
      "Organic chemistry",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Yuan",
        "given_name": "Jin"
      },
      {
        "surname": "Xiong",
        "given_name": "Heng-Chang"
      },
      {
        "surname": "Xiao",
        "given_name": "Yi"
      },
      {
        "surname": "Guan",
        "given_name": "Weili"
      },
      {
        "surname": "Wang",
        "given_name": "Meng"
      },
      {
        "surname": "Hong",
        "given_name": "Richang"
      },
      {
        "surname": "Li",
        "given_name": "Zhi-Yong"
      }
    ]
  },
  {
    "title": "BrainPrint: EEG biometric identification based on analyzing brain connectivity graphs",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107381",
    "abstract": "Research on brain biometrics using electroencephalographic (EEG) signals has received increasing attentions in recent years. In particular, it has been recognized that the brain functional connectivity reflects individual variability. However, many questions need to be answered before we can properly use distinctive characteristics of brain connectivity for biometric applications. This paper proposes a graph-based method for EEG biometric identification. It consists of a network estimation module to generate brain connectivity networks and a graph analysis module to generate topological features based on brain networks. Specifically, we investigate seven different connectivity metrics for the network estimation module, each of which is characterized by a certain signal interaction mechanism, defining a peculiar subjective brain network. A new connectivity metric is proposed based on the algorithmic complexity of EEG signals from a information-theoretic perspective. Meanwhile, six nodal features and six global features are proposed and studied for the graph analysis module. A comprehensive evaluation is carried out to assess the impact of different connectivity metrics, graph features, and EEG frequency bands on biometric identification performance. The results demonstrate that the graph-based method proposed in this study is effective in improving the recognition rate and inter-state stability of EEG-based biometric identification systems. Our findings about the network patterns and graph features bring a further understanding of distinctiveness of humans’ EEG functional connectivity and provide useful guidance for the design of graph-based EEG biometric systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301849",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Biometrics",
      "Botany",
      "Computer science",
      "Electroencephalography",
      "Identification (biology)",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Psychology",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Min"
      },
      {
        "surname": "Hu",
        "given_name": "Jiankun"
      },
      {
        "surname": "Abbass",
        "given_name": "Hussein A."
      }
    ]
  },
  {
    "title": "Supervised deep hashing with a joint deep network",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107368",
    "abstract": "Hashing has gained great attention in large-scale image retrieval due to efficient storage and fast search. Recently, many deep hashing approaches have achieved good results since deep neural network owns powerful learning capability. However, these deep hashing approaches can perform deep features learning and binary-like codes learning synchronously, the information loss between binary-like codes and binary codes will increase due to the binarization operation. A further deficiency is that binary-like codes learning based on deep feature representations is a shallow learning procedure, which cannot fully exploit deep feature representations to generate hash codes. To solve the above problems, we propose a Deep Learning Supervised Hashing (DLSH) method which adopts deep structure to learn binary codes based on deep feature representations for large-scale image retrieval. Specifically, we integrate deep features learning module, deep mapping module and binary codes learning module in one unified architecture. The network is trained in an end-to-end way. In addition, a new objective function is designed to preserve the balancing property and semantic similarity of binary codes by incorporating the semantic similarity term and the balanceable property term. Experimental results on four benchmarks demonstrate that the proposed approach outperforms several state-of-the-art hashing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301710",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary code",
      "Binary number",
      "Computer science",
      "Computer security",
      "Deep belief network",
      "Deep learning",
      "Feature (linguistics)",
      "Feature learning",
      "Hash function",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Similarity (geometry)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yaxiong"
      },
      {
        "surname": "Lu",
        "given_name": "Xiaoqiang"
      },
      {
        "surname": "Li",
        "given_name": "Xuelong"
      }
    ]
  },
  {
    "title": "Binary neural networks: A survey",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107281",
    "abstract": "The binary neural network, largely saving the storage and computation, serves as a promising technique for deploying deep models on resource-limited devices. However, the binarization inevitably causes severe information loss, and even worse, its discontinuity brings difficulty to the optimization of the deep network. To address these issues, a variety of algorithms have been proposed, and achieved satisfying progress in recent years. In this paper, we present a comprehensive survey of these algorithms, mainly categorized into the native solutions directly conducting binarization, and the optimized ones using techniques like minimizing the quantization error, improving the network loss function, and reducing the gradient error. We also investigate other practical aspects of binary neural networks such as the hardware-friendly design and the training tricks. Then, we give the evaluation and discussions on different tasks, including image classification, object detection and semantic segmentation. Finally, the challenges that may be faced in future research are prospected.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300856",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Artificial neural network",
      "Binary number",
      "Computation",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Deep neural networks",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Quantization (signal processing)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Qin",
        "given_name": "Haotong"
      },
      {
        "surname": "Gong",
        "given_name": "Ruihao"
      },
      {
        "surname": "Liu",
        "given_name": "Xianglong"
      },
      {
        "surname": "Bai",
        "given_name": "Xiao"
      },
      {
        "surname": "Song",
        "given_name": "Jingkuan"
      },
      {
        "surname": "Sebe",
        "given_name": "Nicu"
      }
    ]
  },
  {
    "title": "Person identification using EEG channel selection with hybrid flower pollination algorithm",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107393",
    "abstract": "Recently, electroencephalogram (EEG) signal presents a great potential for a new biometric system to deal with a cognitive task. Several studies defined the EEG with uniqueness features, universality, and natural robustness that can be used as a new track to prevent spoofing attacks. The EEG signals are the graphical recording of the brain electrical activities which can be measured by placing electrodes (channels) in various positions of the scalp. With a large number of channels, some channels have very important information for biometric system while others not. The channel selection problem has been recently formulated as an optimisation problem and solved by optimisation techniques. This paper proposes hybrid optimisation techniques based on binary flower pollination algorithm (FPA) and β-hill climbing (called FPAβ-hc) for selecting the most relative EEG channels (i.e., features) that come up with efficient accuracy rate of personal identification. Each EEG signals with three different groups of EEG channels have been utilized (i.e., time domain, frequency domain, and time-frequency domain). The FPAβ-hc is measured using a standard EEG signal dataset, namely, EEG motor movement/imagery dataset with a real world data taken from 109 persons each with 14 different cognitive tasks using 64 channels. To evaluate the performance of the FPAβ-hc, five measurement criteria are considered:accuracy (Acc), (ii) sensitivity (Sen), (iii) F-score (F_s), (v) specificity (Spe), and (iv) number of channels selected (No. Ch). The proposed method is able to identify the personals with high Acc, Sen., F_s, Spe, and less number of channels selected. Interestingly, the experimental results suggest that FPAβ-hc is able to reduce the number of channels with accuracy rate up to 96% using time-frequency domain features. For comparative evaluation, the proposed method is able to achieve results better than those produced by binary-FPA-OPF method using the same EEG motor movement/imagery datasets. In a nutshell, the proposed method can be very beneficial for effective use of EEG signals in biometric applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301965",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Brain–computer interface",
      "Channel (broadcasting)",
      "Chemistry",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Electroencephalography",
      "Frequency domain",
      "Gene",
      "Pattern recognition (psychology)",
      "Psychiatry",
      "Psychology",
      "Robustness (evolution)",
      "Speech recognition",
      "Spoofing attack",
      "Time domain"
    ],
    "authors": [
      {
        "surname": "Alyasseri",
        "given_name": "Zaid Abdi Alkareem"
      },
      {
        "surname": "Khader",
        "given_name": "Ahamad Tajudin"
      },
      {
        "surname": "Al-Betar",
        "given_name": "Mohammed Azmi"
      },
      {
        "surname": "Alomari",
        "given_name": "Osama Ahmad"
      }
    ]
  },
  {
    "title": "Simplified unsupervised image translation for semantic segmentation adaptation",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107343",
    "abstract": "Image to image translation achieves superior performance with the advent of generative adversarial networks. In this paper, we propose a Simplified Unsupervised Image Translation (SUIT) model for domain adaptation on semantic segmentation. We adopt adversarial training for superior image generation, and design a novel semantic-content loss to enhance visual appearance preservation. Thus, the high-fidelity generated images with target-style can help the model generalize to the target domain. Besides, the semantic-content loss contains two components, which focus on label- and content-consistency, respectively. Both of them can be derived from existing modules of SUIT, which makes it simple yet suitable for domain adaptation on semantic segmentation tasks. Meanwhile, since the transformation network (generator) is decoupled from the segmentation network, the former can be easily transplanted to other semantic segmentation models. Extensive experimental results demonstrate that these translated images within SUIT can significantly improve performance of the model on the target domain, and our model with FCN8s-VGG16 architecture achieves around 13 percentage points improvement in terms of mIoU on multiple semantic segmentation adaptation benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301461",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Domain (mathematical analysis)",
      "Fidelity",
      "Focus (optics)",
      "Gene",
      "Generator (circuit theory)",
      "Image (mathematics)",
      "Image segmentation",
      "Image translation",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Messenger RNA",
      "Natural language processing",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Power (physics)",
      "Programming language",
      "Quantum mechanics",
      "Segmentation",
      "Semantics (computer science)",
      "Telecommunications",
      "Translation (biology)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Rui"
      },
      {
        "surname": "Cao",
        "given_name": "Wenming"
      },
      {
        "surname": "Jiao",
        "given_name": "Qianfen"
      },
      {
        "surname": "Wu",
        "given_name": "Si"
      },
      {
        "surname": "Wong",
        "given_name": "Hau-San"
      }
    ]
  },
  {
    "title": "Revisiting spectral clustering for near-convex decomposition of 2D shape",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107371",
    "abstract": "We present a novel 2D shape decomposition algorithm via a recursive partitioning process. Starting with the contour points of a shape, we repeatedly separate the points into two parts by spectral clustering, until the stopping condition is met. Motivated by the fact that the points in a convex part are mutually visible, we regard the visibility matrix of points as the affinity matrix of spectral clustering to obtain a near-convex decomposition. Additionally, we present an efficient stopping rule to avoid over-segmentation on the shape branches. The stopping criterion is based on a novel shape signature called visible protrusion strength which can be used to measure the segmentability of a sub-shape. Finally, we demonstrate the efficiency of our algorithm on a variety of publicly available shapes, and provide qualitative and quantitative comparisons with state-of-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301746",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Cluster analysis",
      "Composite material",
      "Computer science",
      "Decomposition",
      "Ecology",
      "Geometry",
      "Materials science",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Process (computing)",
      "Programming language",
      "Regular polygon",
      "Segmentation",
      "Shape analysis (program analysis)",
      "Spectral clustering",
      "Static analysis",
      "Visibility"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Zhiyang"
      },
      {
        "surname": "Hu",
        "given_name": "Jia"
      },
      {
        "surname": "Stojmenovic",
        "given_name": "Milos"
      },
      {
        "surname": "Liu",
        "given_name": "Zhaobin"
      },
      {
        "surname": "Liu",
        "given_name": "Weijiang"
      }
    ]
  },
  {
    "title": "Optical flow-based structure-from-motion for the reconstruction of epithelial surfaces",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107391",
    "abstract": "This paper details a novel optical flow-based structure from motion (SfM) approach for the reconstruction of surfaces with few textures using video sequences acquired under strong illumination changes. An original image search and grouping strategy allows to reconstruct each 3D scene point using a large set of 2D homologous points extracted from a reference image and its superimposed images acquired from different viewpoints. A variational optical flow scheme with a descriptor-based data term leads to a robust, accurate and dense homologous point determination between the image pairs. Thus, contrary to classical SfM usable for textured scenes, the proposed dense point cloud reconstruction algorithm requires neither a feature point tracking method nor any multi-view stereo technique. The performance of the proposed SfM approach is assessed on phantoms with known ground truth and on very complex patient data of various medical examinations and image modalities.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301941",
    "keywords": [
      "3D reconstruction",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Geometry",
      "Ground truth",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Motion estimation",
      "Optical flow",
      "Pedagogy",
      "Philosophy",
      "Point (geometry)",
      "Point cloud",
      "Psychology",
      "Structure from motion",
      "Surface (topology)",
      "Surface reconstruction",
      "Tracking (education)",
      "USable",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Phan",
        "given_name": "Tan-Binh"
      },
      {
        "surname": "Trinh",
        "given_name": "Dinh-Hoan"
      },
      {
        "surname": "Wolf",
        "given_name": "Didier"
      },
      {
        "surname": "Daul",
        "given_name": "Christian"
      }
    ]
  },
  {
    "title": "Enhancing perception for the visually impaired with deep learning techniques and low-cost wearable sensors",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.03.008",
    "abstract": "As estimated by the World Health Organization, there are millions of people who lives with some form of vision impairment. As a consequence, some of them present mobility problems in outdoor environments. With the aim of helping them, we propose in this work a system which is capable of delivering the position of potential obstacles in outdoor scenarios. Our approach is based on non-intrusive wearable devices and focuses also on being low-cost. First, a depth map of the scene is estimated from a color image, which provides 3D information of the environment. Then, an urban object detector is in charge of detecting the semantics of the objects in the scene. Finally, the three-dimensional and semantic data is summarized in a simpler representation of the potential obstacles the users have in front of them. This information is transmitted to the user through spoken or haptic feedback. Our system is able to run at about 3.8 fps and achieved a 87.99% mean accuracy in obstacle presence detection. Finally, we deployed our system in a pilot test which involved an actual person with vision impairment, who validated the effectiveness of our proposal for improving its navigation capabilities in outdoors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519300881",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Embedded system",
      "GRASP",
      "Haptic technology",
      "Human–computer interaction",
      "Law",
      "Neuroscience",
      "Object detection",
      "Obstacle",
      "Pattern recognition (psychology)",
      "Perception",
      "Political science",
      "Politics",
      "Programming language",
      "Psychiatry",
      "Psychology",
      "Representation (politics)",
      "Semantics (computer science)",
      "Visual impairment",
      "Wearable computer"
    ],
    "authors": [
      {
        "surname": "Bauer",
        "given_name": "Zuria"
      },
      {
        "surname": "Dominguez",
        "given_name": "Alejandro"
      },
      {
        "surname": "Cruz",
        "given_name": "Edmanuel"
      },
      {
        "surname": "Gomez-Donoso",
        "given_name": "Francisco"
      },
      {
        "surname": "Orts-Escolano",
        "given_name": "Sergio"
      },
      {
        "surname": "Cazorla",
        "given_name": "Miguel"
      }
    ]
  },
  {
    "title": "Self-attention driven adversarial similarity learning network",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107331",
    "abstract": "Similarity learning is a kind of machine learning algorithm that aims to measure the relevance between given objects. However, conventional similarity learning algorithms usually measure the distance between the entire given objects in the latent feature space. Consequently, the obtained similarity scores only represent how close are the entire given objects, but are incapable of demonstrating which part of them are similar to each other and how semantically similar are they. To address the above problems, in this paper, we propose a self-attention driven adversarial similarity learning network. Discriminative self-attention weights are firstly assigned to different regions of the given objects. The similarity learning step measures the relevance between these self-attention weighted feature maps of given objects under various topic vectors. The topic vectors are conditioned to capture and preserve hidden semantic information within data distribution by a generator-discriminator model with adversarial loss. This model aims to generate objects from topic vectors and propagates the difference between the generated and the real objects back to the similarity learning step, which forces the topic vectors to not only assign discriminative similarity scores to different object pairs but also further mine the hidden semantic information within data distribution. The final similarity scores represent how tight the given objects are connected to the topics. In addition, the regions with higher self-attention weights make more contribution to the discriminative similarity scores. The effectiveness of the proposed method is demonstrated through evaluations based on image retrieval task and document retrieval task and compared against various state-of-the-art algorithms in the field. The visualization results of topic vectors and self-attention weighted feature maps are demonstrated to make our proposed method explainable.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301345",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Detector",
      "Discriminative model",
      "Discriminator",
      "Economics",
      "Feature (linguistics)",
      "Feature vector",
      "Generator (circuit theory)",
      "Image (mathematics)",
      "Law",
      "Linguistics",
      "Machine learning",
      "Management",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Political science",
      "Power (physics)",
      "Quantum mechanics",
      "Relevance (law)",
      "Semantic similarity",
      "Similarity (geometry)",
      "Similarity learning",
      "Similarity measure",
      "Task (project management)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Xinjian"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhao"
      },
      {
        "surname": "Mu",
        "given_name": "Tingting"
      },
      {
        "surname": "Zhang",
        "given_name": "Xudong"
      },
      {
        "surname": "Cui",
        "given_name": "Chaoran"
      },
      {
        "surname": "Wang",
        "given_name": "Meng"
      }
    ]
  },
  {
    "title": "Spectral bounding: Strictly satisfying the 1-Lipschitz property for generative adversarial networks",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2019.107179",
    "abstract": "Imposing the 1-Lipschitz constraint is a problem of key importance in the training of Generative Adversarial Networks (GANs), which has been proved to productively improve stability of GAN training. Although some interesting alternative methods have been proposed to enforce the 1-Lipschitz property, these existing approaches (e.g., weight clipping, gradient penalty (GP), and spectral normalization (SN)) are only partially successful. In this paper, we propose a novel method, which we refer to as spectral bounding (SB) to strictly enforce the 1-Lipschitz constraint. Our method adopts very cost-effective terms of both 1-norm and ∞-norm, and yet allows us to efficiently approximate the upper bound of spectral norms. In this way, our method provide important insights to the relationship between an alternative of strictly satisfying the Lipschitz property and explainable training stability improvements of GAN. Our proposed method thus significantly enhances the stability of GAN training and the quality of generated images. Extensive experiments are conducted, showing that the proposed method outperforms GP and SN on both CIFAR-10 and ILSVRC2015 (ImagetNet) dataset in terms of the standard inception score.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320319304790",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Bounding overwatch",
      "Computer science",
      "Constraint (computer-aided design)",
      "Epistemology",
      "Generative grammar",
      "Geometry",
      "Law",
      "Lipschitz continuity",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Norm (philosophy)",
      "Normalization (sociology)",
      "Philosophy",
      "Political science",
      "Property (philosophy)",
      "Pure mathematics",
      "Sociology",
      "Stability (learning theory)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zhihong"
      },
      {
        "surname": "Zeng",
        "given_name": "Yangbin"
      },
      {
        "surname": "Bai",
        "given_name": "Lu"
      },
      {
        "surname": "Hu",
        "given_name": "Yiqun"
      },
      {
        "surname": "Wu",
        "given_name": "Meihong"
      },
      {
        "surname": "Wang",
        "given_name": "Shuai"
      },
      {
        "surname": "Hancock",
        "given_name": "Edwin R."
      }
    ]
  },
  {
    "title": "Deep video-to-video transformations for accessibility with an application to photosensitivity",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.01.019",
    "abstract": "We demonstrate how to construct a new class of visual assistive technologies that, rather than extract symbolic information, learn to transform the visual environment to make it more accessible. We do so without engineering which transformations are useful allowing for arbitrary modifications of the visual input. As an instantiation of this idea we tackle a problem that affects and hurts millions worldwide: photosensitivity. Any time an affected person opens a website, video, or some other medium that contains an adverse visual stimulus, either intended or unintended, they might experience a seizure with potentially significant consequences. We show how a deep network can learn a video-to-video transformation rendering such stimuli harmless while otherwise preserving the video. This approach uses a specification of the adverse phenomena, the forward transformation, to learn the inverse transformation. We show how such a network generalizes to real-world videos that have triggered numerous seizures, both by mistake and in politically-motivated attacks. A number of complimentary approaches are demonstrated including using a hand-crafted generator and a GAN using a differentiable perceptual metric. Such technology can be deployed offline to protect videos before they are shown or online with assistive glasses or real-time post processing. Other applications of this general technique include helping those with limited vision, attention deficit hyperactivity disorder, and autism.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519300133",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Fidelity",
      "Gene",
      "Human–computer interaction",
      "Programming language",
      "Rendering (computer graphics)",
      "Rewriting",
      "Telecommunications",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Barbu",
        "given_name": "Andrei"
      },
      {
        "surname": "Banda",
        "given_name": "Dalitso"
      },
      {
        "surname": "Katz",
        "given_name": "Boris"
      }
    ]
  },
  {
    "title": "Beyond context: Exploring semantic similarity for small object detection in crowded scenes",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.03.009",
    "abstract": "Small object detection in crowded scene aims to find those tiny targets with very limited resolution from crowded scenes. Due to very little information available on tiny objects, it is often not suitable to detect them merely based on the information presented inside their bounding boxes, resulting low accuracy. In this paper, we propose to exploit the semantic similarity among all predicted objects’ candidates to boost the performance of detectors when handling tiny objects. For this purpose, we construct a pairwise constraint to depict such semantic similarity and propose a new framework based on Discriminative Learning and Graph-Cut techniques. Experiments conducted on three widely used benchmark datasets demonstrate the improvement over the state-of-the-art approaches gained by applying this idea.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519300893",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Bounding overwatch",
      "Computer science",
      "Computer security",
      "Context (archaeology)",
      "Discriminative model",
      "Exploit",
      "Geodesy",
      "Geography",
      "Graph",
      "Image (mathematics)",
      "Machine learning",
      "Object detection",
      "Pairwise comparison",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Similarity (geometry)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Xi",
        "given_name": "Yue"
      },
      {
        "surname": "Zheng",
        "given_name": "Jiangbin"
      },
      {
        "surname": "He",
        "given_name": "Xiangjian"
      },
      {
        "surname": "Jia",
        "given_name": "Wenjing"
      },
      {
        "surname": "Li",
        "given_name": "Hanhui"
      },
      {
        "surname": "Xie",
        "given_name": "Yefan"
      },
      {
        "surname": "Feng",
        "given_name": "Mingchen"
      },
      {
        "surname": "Li",
        "given_name": "Xiuxiu"
      }
    ]
  },
  {
    "title": "Graph-based neural networks for explainable image privacy inference",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107360",
    "abstract": "With the development of social media and smartphones, people share their daily lives via a large number of images, but the convince also raises a problem of privacy leakage. Therefore, effective methods are needed to infer the privacy risk of images and identify images that may disclose privacy. Several works have tried to solve this problem with deep learning models. However, we know little about how the models infer the privacy label of an image, thus it is not easy to understand why the image may disclose privacy. Inspired by recent research on graph neural networks, we introduce prior knowledge to the deep models to make the inference more explainable. We propose the Graph-based neural networks for Image Privacy (GIP) to infer the privacy risk of images. The GIP mainly focuses on objects in an image, and the knowledge graph is extracted from the objects in the dataset without reliance on extra knowledge. Experimental results show that the GIP achieves higher performance compared with the object-based methods and comparable performance even compared with the multi-modal fusion method. The results show that the introduction of the knowledge graph not only makes the deep model more explainable but also makes better use of the information of objects provided by the images. Combing the knowledge graph with deep learning is a promising way to help protect image privacy that is worth exploring.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301631",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Deep learning",
      "Graph",
      "Image (mathematics)",
      "Inference",
      "Machine learning",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Guang"
      },
      {
        "surname": "Cao",
        "given_name": "Juan"
      },
      {
        "surname": "Chen",
        "given_name": "Zhineng"
      },
      {
        "surname": "Guo",
        "given_name": "Junbo"
      },
      {
        "surname": "Li",
        "given_name": "Jintao"
      }
    ]
  },
  {
    "title": "Automated glaucoma detection using GIST and pyramid histogram of oriented gradients (PHOG) descriptors",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.04.004",
    "abstract": "Effective diagnosis of glaucoma mainly relies on the analysis of optic disc characteristics of retina. Glaucoma is considered as second leading cause of blindness and its early detection prevents patients from temporary or permanent blindness. It effects the intensity and shape near optic disc of the retina. Fundus photography has revolutionized the field of ophthalmology and helped in visualizing the structure of optic disc. The proposed work aims to develop an automated diagnostic system based on fundus images for glaucoma disease. It focuses on extraction of GIST and pyramid histogram of oriented gradients (PHOG) features from preprocessed fundus images. The extracted features are ranked and selected through principal component analysis (PCA) to choose significant features. The classification into glaucomatous images is done with SVM classifier on fundus images of Drishti-GS1 and HRF databases. The results obtained from the proposed method are compared with recent glaucoma detection techniques in the literature, including deep learning methodologies, on the basis of accuracy and AUC parameters. The performance of the system is also validated by glaucoma expert from Sharp Sight Group of Eye Hospitals, Delhi-NCR, India.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519301151",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Fundus (uterus)",
      "Geometry",
      "Glaucoma",
      "Histogram",
      "Histogram of oriented gradients",
      "Image (mathematics)",
      "Mathematics",
      "Medicine",
      "Ophthalmology",
      "Optic disc",
      "Optic nerve",
      "Optometry",
      "Pattern recognition (psychology)",
      "Pyramid (geometry)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Gour",
        "given_name": "Neha"
      },
      {
        "surname": "Khanna",
        "given_name": "Pritee"
      }
    ]
  },
  {
    "title": "Robust twin support vector regression based on rescaled Hinge loss",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107395",
    "abstract": "In this work, with the help of the rescaled Hinge loss, we propose a twin support vector regression (TSVR) model that is robust to noise. The corresponding optimization problem turns out to be non-convex with smooth l 2 regularizer. To solve the problem efficiently, we convert it to its dual form, thereby transforming it into a convex optimization problem. An algorithm, named Res-TSVR, is provided to solve the formulated dual problem. The proof of the convergence of the algorithm is given. It is shown that the maximum number of iterations to achieve an ε-precision solution to the dual problem is O ( log ( 1 ε ) ) . We conduct a set of numerical experiments to compare the proposed method with the recently proposed robust approaches of TSVR and the standard SVR. Experimental results reveal that the proposed approach outperforms other robust methods of TSVR in terms of generalization performance and robustness to noise with comparable training time. This claim is based on the experiments performed using seven real-world data sets and three synthetic data sets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301989",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Gene",
      "Generalization",
      "Geometry",
      "Hinge loss",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Noise (video)",
      "Optimization problem",
      "Regular polygon",
      "Robustness (evolution)",
      "Support vector machine",
      "Synthetic data"
    ],
    "authors": [
      {
        "surname": "Singla",
        "given_name": "Manisha"
      },
      {
        "surname": "Ghosh",
        "given_name": "Debdas"
      },
      {
        "surname": "Shukla",
        "given_name": "K.K."
      },
      {
        "surname": "Pedrycz",
        "given_name": "Witold"
      }
    ]
  },
  {
    "title": "Gait recognition invariant to carried objects using alpha blending generative adversarial networks",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107376",
    "abstract": "Gait recognition invariant to carried objects (COs) is very difficult in a real-life scene because the COs can have various shapes and sizes, in addition to unpredictable carrying locations (e.g., front, back, and side, or multiple locations). Therefore, in this paper, we propose a robust method for gait recognition against various COs by reconstructing a gait template without COs. A straightforward approach is to directly generate a gait template without COs given a gait template with COs as the input using a conventional generative adversarial network. There is, however, a potential risk of unnecessarily altering parts that were originally unaffected by COs (e.g., leg parts for a person carrying a backpack). Because we do not want to touch such unaffected parts in the original template, we first estimate a gait template without COs, and then blend it with the original template by an estimated alpha matte that indicates the blending parameters. We then create an alpha-blended template from the original template and the generated template without COs based on the estimated alpha matte. We use two independent generators to estimate the alpha matte and the generated template without COs. Finally, we feed the alpha-blended gait template into a state-of-the-art discrimination network for gait recognition. The experimental results on three publicly available gait databases with real-life COs demonstrate the state-of-the-art performance of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301795",
    "keywords": [
      "Alpha (finance)",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Construct validity",
      "Gait",
      "Generative adversarial network",
      "Image (mathematics)",
      "Invariant (physics)",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physiology",
      "Psychometrics",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xiang"
      },
      {
        "surname": "Makihara",
        "given_name": "Yasushi"
      },
      {
        "surname": "Xu",
        "given_name": "Chi"
      },
      {
        "surname": "Yagi",
        "given_name": "Yasushi"
      },
      {
        "surname": "Ren",
        "given_name": "Mingwu"
      }
    ]
  },
  {
    "title": "Scoring disease-microRNA associations by integrating disease hierarchy into graph convolutional networks",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107385",
    "abstract": "In this study, we present an updated predictor DimiG 2.0, which uses a semi-supervised multi-label graph convolutional network (GCN) to infer disease-associated microRNAs (miRNAs) on an interaction network between protein coding genes (PCGs) and miRNAs using disease-PCG associations. DimiG 2.0 benefits from integrating the hierarchy of diseases into the GCN. DimiG 2.0 has the following updates: 1) It incorporates the hierarchy of diseases to regularize the GCN, encouraging diseases in the hierarchy to share similar miRNAs. 2) It integrates the PCGs with interacting partners but without associated diseases into model training, these unlabeled PCGs increase the size of the constructed interaction network. 3) It is able to predict associated miRNAs for 1017 diseases (updated from 248). 4) It updates expression data across tissues from the latest GTEx v7, and the expression values are quantified in Transcripts Per Million (TPM). Our results show that DimiG 2.0 outperforms state-of-the-art semi-supervised and supervised methods on the constructed benchmarked sets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301886",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Coding (social sciences)",
      "Computational biology",
      "Computer science",
      "Disease",
      "Economics",
      "Gene",
      "Genetics",
      "Graph",
      "Hierarchy",
      "Machine learning",
      "Market economy",
      "Mathematics",
      "Medicine",
      "Pathology",
      "Statistics",
      "Theoretical computer science",
      "microRNA"
    ],
    "authors": [
      {
        "surname": "Pan",
        "given_name": "Xiaoyong"
      },
      {
        "surname": "Shen",
        "given_name": "Hong-Bin"
      }
    ]
  },
  {
    "title": "Learning EEG topographical representation for classification via convolutional neural network",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107390",
    "abstract": "Electroencephalography (EEG) topographical representation (ETR) can monitor regional brain activities and is emerging as a successful technique for causally exploring cortical mechanisms and connections. However, it is a challenge to find a robust method supporting high-dimensional EEG data with low signal-to-noise ratios from multiple objects and multiple channels. To address this issue, a new ETR energy calculation method for learning the EEG patterns of brain activities using a convolutional neural network is reported. It is able to customize temporal ETR training and recognize multiple objects within a common learning model. Specifically, an open-access dataset from the 2008 Brain-Computer Interface (BCI) Competition IV-2a is used for classification of five classes containing four Motor Imagery actions and one relax action. The proposed classification framework outperforms the best state-of-the-art classification method by 10.11% in average subject accuracy. Furthermore, by studying the ETR parameter optimization, a user interface for BCI applications is obtained and a real-time method implemented.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030193X",
    "keywords": [
      "Artificial intelligence",
      "Brain–computer interface",
      "Bubble",
      "Computer science",
      "Convolutional neural network",
      "Electroencephalography",
      "Image (mathematics)",
      "Interface (matter)",
      "Law",
      "Machine learning",
      "Maximum bubble pressure method",
      "Motor imagery",
      "Neuroscience",
      "Noise (video)",
      "Parallel computing",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Psychology",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Meiyan"
      },
      {
        "surname": "Yao",
        "given_name": "Junfeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhihong"
      },
      {
        "surname": "Li",
        "given_name": "Rui"
      },
      {
        "surname": "Yang",
        "given_name": "Baorong"
      },
      {
        "surname": "Li",
        "given_name": "Chunyan"
      },
      {
        "surname": "Li",
        "given_name": "Jun"
      },
      {
        "surname": "Zhang",
        "given_name": "Junsong"
      }
    ]
  },
  {
    "title": "Positive technology for elderly well-being: A review",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.03.016",
    "abstract": "In the last decades, given the necessity of assisting fragile citizens, of which elderly represent a significant portion, a considerable research effort has been devoted to the use of information and communication technologies (ICT) in daily living to promote activity, social connections, and independence. With similar purposes, in recent years psychologists proposed the novel paradigm of Positive Psychology (PP), the scientific study of positive human functioning and flourishing on multiple levels. The joint effort between ICT and PP has led to the definition of the emerging field of Positive Technology (PT), with the aim of developing technology consciously designed to foster well-being in individuals and groups. In this paper we review PT focusing on frameworks involving computer vision and machine learning for promoting cognitive, physical, emotional and social elderly well-being. Our discussion highlights a significant gap between theoretical needs and technological systems availability, suggesting future lines of research.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519300996",
    "keywords": [
      "Computer science",
      "Field (mathematics)",
      "Flourishing",
      "Independence (probability theory)",
      "Information and Communications Technology",
      "Knowledge management",
      "Mathematics",
      "Psychology",
      "Pure mathematics",
      "Social psychology",
      "Statistics",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Grossi",
        "given_name": "Giuliano"
      },
      {
        "surname": "Lanzarotti",
        "given_name": "Raffaella"
      },
      {
        "surname": "Napoletano",
        "given_name": "Paolo"
      },
      {
        "surname": "Noceti",
        "given_name": "Nicoletta"
      },
      {
        "surname": "Odone",
        "given_name": "Francesca"
      }
    ]
  },
  {
    "title": "Robust one-stage object detection with location-aware classifiers",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107334",
    "abstract": "Recent progress on one-stage detectors focuses on improving the quality of bounding boxes, while they pay less attention to the classification head. In this work, we focus on investigating the influence of the classification head. To understand the behavior of the classifier in one-stage detectors, we resort to the methods of the Explainable deep learning area. We visualize its learned representations via activation maps and analyze its robustness to image scene context. Based on the analysis, we observe that the classifier limits the performance of the detector due to its limited receptive field and the lack of object locations. Then, we design a simple but efficient location-aware multi-dilation module (LAMD) to enhance the weak classifier. We conduct extensive experiments on the COCO benchmark to validate the effectiveness of LAMD. The results suggest that our LAMD can achieve consistent improvements and leads to robust detection across various one-stage detectors with different backbones.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301370",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Bounding overwatch",
      "Chemistry",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Detector",
      "Gene",
      "Image (mathematics)",
      "Machine learning",
      "Minimum bounding box",
      "Object detection",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Qiang"
      },
      {
        "surname": "Wang",
        "given_name": "Peisong"
      },
      {
        "surname": "Cheng",
        "given_name": "Anda"
      },
      {
        "surname": "Wang",
        "given_name": "Wanguo"
      },
      {
        "surname": "Zhang",
        "given_name": "Yifan"
      },
      {
        "surname": "Cheng",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "Learning residual refinement network with semantic context representation for real-time saliency object detection",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107372",
    "abstract": "Salient object detection (SOD) aims to precisely segment out the most attractive areas in a single image. With the rapid development of deep learning, much effort has been paid to learn an effective representation for SOD from bottom-up or top-down pathways. However, they fail to precisely separate out the whole salient object with fine boundaries due to the repeated subsampling operations such as pooling and striding leading to the loss of fine structures and spatial details. To address these issues, in this paper, we propose a residual refinement network with semantic context features for SOD. First, we design an encoder-decoder structure with side-connections to capture the sharper object boundaries, which can not only gradually recover the spatial details in each feature map from top to down, but also enhance the features at all scales with high-level semantic context information. The semantic context enhanced features are further strengthen by using a set of atrous convolutional filters with multiple atrous rates to encode multi-scale context information. Finally, using the side-output features as input, we develop a recurrent residual module to gradually learn to recover the missing boundary details in the previous coarsely predicted saliency map in a coarse-to-fine manner. Extensive evaluations on six popular SOD benchmark datasets demonstrate leading performance of the proposed approach compared with state-of-the-art methods. Especially, our approach runs in real-time at a speed of 29 fps.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301758",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Convolutional neural network",
      "Deep learning",
      "ENCODE",
      "Encoder",
      "Feature (linguistics)",
      "Feature learning",
      "Gene",
      "Geodesy",
      "Geography",
      "Law",
      "Linguistics",
      "Object (grammar)",
      "Object detection",
      "Operating system",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Pooling",
      "Representation (politics)",
      "Residual"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Tengpeng"
      },
      {
        "surname": "Song",
        "given_name": "Huihui"
      },
      {
        "surname": "Zhang",
        "given_name": "Kaihua"
      },
      {
        "surname": "Liu",
        "given_name": "Qingshan"
      }
    ]
  },
  {
    "title": "Towards interpretable and robust hand detection via pixel-wise prediction",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107202",
    "abstract": "The lack of interpretability of existing CNN-based hand detection methods makes it difficult to understand the rationale behind their predictions. In this paper, we propose a novel neural network model, which introduces interpretability into hand detection for the first time. The main improvements include: (1) Detect hands at pixel level to explain what pixels are the basis for its decision and improve transparency of the model. (2) The explainable Highlight Feature Fusion block highlights distinctive features among multiple layers and learns discriminative ones to gain robust performance. (3) We introduce a transparent representation, the rotation map, to learn rotation features instead of complex and non-transparent rotation and derotation layers. (4) Auxiliary supervision accelerates the training process, which saves more than 10 h in our experiments. Experimental results on the VIVA and Oxford hand detection and tracking datasets show competitive accuracy of our method compared with state-of-the-art methods with higher speed. Models and code are available: https://isrc.iscas.ac.cn/gitlab/research/pr2020-phdn.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300091",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Feature (linguistics)",
      "Geometry",
      "Interpretability",
      "Law",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Rotation (mathematics)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Dan"
      },
      {
        "surname": "Zhang",
        "given_name": "Libo"
      },
      {
        "surname": "Luo",
        "given_name": "Tiejian"
      },
      {
        "surname": "Tao",
        "given_name": "Lili"
      },
      {
        "surname": "Wu",
        "given_name": "Yanjun"
      }
    ]
  },
  {
    "title": "Context from within: Hierarchical context modeling for semantic segmentation",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107358",
    "abstract": "Conditional Random Fields (CRFs) have been widely adopted in conjunction with Fully Convolutional Networks (FCNs) to model and integrate contextual information in the semantic segmentation procedure. In contrast to existing approaches applying CRFs in parallel or in cascade with FCNs, we propose a new paradigm to incorporate CRFs deeper inside the architecture of FCNs to model the context exhibited within the middle layers of an FCN. We approximate the mean-field inference process of a dense CRF as a multi-dimensional Gated Recurrent Unit (GRU) layer, termed CRF-GRU layer, effectively extracting intermediate context within an FCN. More importantly, multiple CRF-GRU layers can be injected into an FCN to model hierarchical contexts presented in multiple middle layers, showing competitive results on the PASCAL VOC 2012 and PASCAL-Context datasets. Secondly, we contribute a new approach to automatically learn, from the training data, the optimal segmentation architecture of the FCN with multiple CRF-GRU layers injected. The proposed approach relies on Genetic Evolution Strategies to allow the existing architecture to iteratively evolve towards higher accuracy instances. The discovered network not only outperforms state-of-the-art segmentation techniques, but also provides exciting new insights into the design of the segmentation networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301618",
    "keywords": [
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Biology",
      "CRFS",
      "Computer science",
      "Conditional random field",
      "Context (archaeology)",
      "Inference",
      "Machine learning",
      "Paleontology",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Segmentation",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Nguyen",
        "given_name": "Kien"
      },
      {
        "surname": "Fookes",
        "given_name": "Clinton"
      },
      {
        "surname": "Sridharan",
        "given_name": "Sridha"
      }
    ]
  },
  {
    "title": "Thermal comfort measurement using thermal-depth images for robotic monitoring",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2019.02.014",
    "abstract": "This paper describes an application of thermal-depth images to human thermal comfort measurement. A mobile monitoring of the elderly and residents of care houses is one of the promising applications of mobile assistive robots. Monitoring if a person feels comfortable is an important task of such robots. We rely on an established comfort measure in the architecture domain, namely, predicted mean vote (PMV). PMV is calculated mainly by six factors and one of which is the clothing insulation or clo-value. Clo-values are usually measured by a thermal mannequin, a specially-designed apparatus for the purpose. We apply human recognition techniques in thermal-depth images to efficiently measure clo-values, thereby enabling on-line assessment of thermal comfort. We evaluate the method and develop a mobile robot system for experimental testing.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865519300558",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Clothing",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Engineering",
      "History",
      "Human–computer interaction",
      "Measure (data warehouse)",
      "Meteorology",
      "Mobile robot",
      "Physics",
      "Real-time computing",
      "Robot",
      "Simulation",
      "Systems engineering",
      "Task (project management)",
      "Thermal",
      "Thermal comfort",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Miura",
        "given_name": "Jun"
      },
      {
        "surname": "Demura",
        "given_name": "Mitsuhiro"
      },
      {
        "surname": "Nishi",
        "given_name": "Kaichiro"
      },
      {
        "surname": "Oishi",
        "given_name": "Shuji"
      }
    ]
  },
  {
    "title": "Non-rigid infrared and visible image registration by enhanced affine transformation",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107377",
    "abstract": "Image registration is a prerequisite for infrared (IR) and visible (VIS) image fusion. In practical application, most scenes are not planar and there is significant distinctness between IR and VIS cameras. Therefore, for non-rigid IR and VIS image registration, non-linear transformation is more applicable than affine transformation. Typically, non-linear transformation is modeled with point feature. However, this can degrade the generalization ability of transformation model and increase computational complexity. Aim at this problem, we propose an enhanced affine transformation (EAT) for non-rigid IR and VIS image registration. In this paper, image registration is transformed into point set registration and then the optimal EAT model constructed by global deformation is estimated from local feature. At first, a Gaussian-fields-based objective function is established and simplified by using the potential correspondence between an image pair. With the combination of affine and polynomial transformation, the EAT model is then proposed to describe the regular pattern of non-rigid and global deformation between an image pair. Finally, a coarse-to-fine strategy based on quasi-Newton method is designed and applied to determine the optimal transformation coefficients from edge point feature of IR and VIS images, in order to accomplish non-rigid image registration. The qualitative and quantitative comparisons on synthesized point sets and real images demonstrate that the proposed method is superior over the state-of-the-art methods in the accuracy and efficiency of image registration.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301801",
    "keywords": [
      "Affine transformation",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Gene",
      "Geometry",
      "Image (mathematics)",
      "Image registration",
      "Linguistics",
      "Mathematics",
      "Philosophy",
      "Point (geometry)",
      "Point set registration",
      "Rigid transformation",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Min",
        "given_name": "Chaobo"
      },
      {
        "surname": "Gu",
        "given_name": "Yan"
      },
      {
        "surname": "Li",
        "given_name": "Yingjie"
      },
      {
        "surname": "Yang",
        "given_name": "Feng"
      }
    ]
  },
  {
    "title": "Preface",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.09.007",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552030341X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Curse of dimensionality",
      "Database",
      "Discrete mathematics",
      "Economics",
      "Epistemology",
      "Image (mathematics)",
      "Mathematics",
      "Metric (unit)",
      "Metric space",
      "Operations management",
      "Paleontology",
      "Philosophy",
      "Property (philosophy)",
      "Scalability",
      "Search engine indexing",
      "Similarity (geometry)",
      "Theoretical computer science",
      "Triangle inequality"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Yao"
      },
      {
        "surname": "Lin",
        "given_name": "Chunyu"
      }
    ]
  },
  {
    "title": "Improving the performance of lightweight CNNs for binary classification using quadratic mutual information regularization",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107407",
    "abstract": "In this paper, we propose regularized lightweight deep convolutional neural network models, capable of effectively operating in real-time on-drone for high-resolution video input. Furthermore, we study the impact of hinge loss against the cross entropy loss on the classification performance, mainly in binary classification problems. Finally, we propose a novel regularization method motivated by the Quadratic Mutual Information, in order to improve the generalization ability of the utilized models. Extensive experiments on various binary classification problems involved in autonomous systems are performed, indicating the effectiveness of the proposed models. The experimental evaluation on four datasets indicates that hinge loss is the optimal choice for binary classification problems, considering lightweight deep models. Finally, the effectiveness of the proposed regularizer in enhancing the generalization ability of the proposed models is also validated.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302107",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary classification",
      "Binary number",
      "Computer science",
      "Convolutional neural network",
      "Cross entropy",
      "Entropy (arrow of time)",
      "Generalization",
      "Geometry",
      "Hinge loss",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Mutual information",
      "Pattern recognition (psychology)",
      "Physics",
      "Quadratic equation",
      "Quantum mechanics",
      "Regularization (linguistics)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Tzelepi",
        "given_name": "Maria"
      },
      {
        "surname": "Tefas",
        "given_name": "Anastasios"
      }
    ]
  },
  {
    "title": "Partial attention and multi-attribute learning for vehicle re-identification",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.07.034",
    "abstract": "Intelligent monitoring systems are in increasing need with the rapid growth of traffic nowadays. Vehicle re-identification has vital applications in digital forensics to track suspected vehicles in camera network. It is very challenging to learn discriminative information because of violent changes including the illumination and the viewpoint when a vehicle appears in different cameras, which will lead to the difficulty in distinguishing different vehicles and confusing the same vehicle. To improve the discrimination and the robustness of vehicle re-identification, we propose a partial attention and multi-attribute learning network. Focusing on the local areas which contain abundant discriminative information, we employ partial attention based on vehicle keypoint detection model. Moreover, because the color and the model of a vehicle are relatively stable in different viewpoints, we employ the branch networks to extract multi-attribute features which will improve the robustness. To validate our approach, experiments are carried out on VeRi and VehicleID datasets, and results show that the proposed method achieves higher accuracy compared with other methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302907",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Botany",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Discriminative model",
      "Gene",
      "Identification (biology)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Viewpoints",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Tumrani",
        "given_name": "Saifullah"
      },
      {
        "surname": "Deng",
        "given_name": "Zhiyi"
      },
      {
        "surname": "Lin",
        "given_name": "Haoyang"
      },
      {
        "surname": "Shao",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "τ-SS3: A text classifier with dynamic n-grams for early risk detection over text streams",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.07.001",
    "abstract": "A recently introduced classifier, called SS3, has shown to be well suited to deal with early risk detection (ERD) problems on text streams. It obtained state-of-the-art performance on early depression and anorexia detection on Reddit in the CLEF’s eRisk open tasks. SS3 was designed to deal with ERD problems naturally since: it supports incremental training and classification over text streams, and it can visually explain its rationale. However, SS3 processes the input using a bag-of-word model lacking the ability to recognize important word sequences. This aspect could negatively affect the classification performance and also reduces the descriptiveness of visual explanations. In the standard document classification field, it is very common to use word n-grams to try to overcome some of these limitations. Unfortunately, when working with text streams, using n-grams is not trivial since the system must learn and recognize which n-grams are important “on the fly”. This paper introduces τ-SS3, an extension of SS3 that allows it to recognize useful patterns over text streams dynamically. We evaluated our model in the eRisk 2017 and 2018 tasks on early depression and anorexia detection. Experimental results suggest that τ-SS3 is able to improve both current results and the richness of visual explanations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302476",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Burdisso",
        "given_name": "Sergio G."
      },
      {
        "surname": "Errecalde",
        "given_name": "Marcelo"
      },
      {
        "surname": "Montes-y-Gómez",
        "given_name": "Manuel"
      }
    ]
  },
  {
    "title": "CtrlFaceNet: Framework for geometric-driven face image synthesis",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.08.026",
    "abstract": "In this work, we introduce a novel framework based on Generative Adversarial Networks to control the pose, expression and facial features of a given face image using another face image. It can then be used for data augmentation, pose invariant face identification, face verification, and lightweight image editing. Generating new realistic face images with controllable poses, facial features, and expressions is a challenging generative learning problem due to skin tone variations, the identity preservation problem, necessity to deal with unseen large poses, and the absence of ground truth images in the training process. We make the following contributions. First, we present a network, CtrlFaceNet that can control a source face image while preserving the identity and skin tone. Second, we introduce a method for training the framework in fully self-supervised mode using a large-scale dataset of unconstrained face images. Third, we show that the style loss function can be used to preserve the skin tone of the source image. The experimental results show that our approach outperforms all other baselines. Furthermore, to the best of our knowledge, we are the first to train such a model using large-scale dataset of unconstrained face images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303299",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Face (sociological concept)",
      "Generative grammar",
      "Generative model",
      "Identity (music)",
      "Image (mathematics)",
      "Image editing",
      "Invariant (physics)",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Zeno",
        "given_name": "Bassel"
      },
      {
        "surname": "Kalinovskiy",
        "given_name": "Ilya"
      },
      {
        "surname": "Matveev",
        "given_name": "Yuri"
      },
      {
        "surname": "Alkhatib",
        "given_name": "Bassel"
      }
    ]
  },
  {
    "title": "Real-MFF: A large realistic multi-focus image dataset with ground truth",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.08.002",
    "abstract": "Multi-focus image fusion, a technique to generate an all-in-focus image from two or more partially-focused source images, can benefit many computer vision tasks. However, currently there is no large and realistic dataset to perform convincing evaluation and comparison of algorithms in multi-focus image fusion. Moreover, it is difficult to train a deep neural network for multi-focus image fusion without a suitable dataset. In this letter, we introduce a large and realistic multi-focus dataset called Real-MFF, which contains 710 pairs of source images with corresponding ground truth images. The dataset is generated by light field images, and both the source images and the ground truth images are realistic. To serve as both a well-established benchmark for existing multi-focus image fusion algorithms and an appropriate training dataset for future development of deep-learning-based methods, the dataset contains a variety of scenes, including buildings, plants, humans, shopping malls, squares and so on. We also evaluate 10 typical multi-focus algorithms on this dataset for the purpose of illustration.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303007",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Field (mathematics)",
      "Focus (optics)",
      "Geodesy",
      "Geography",
      "Ground truth",
      "Image (mathematics)",
      "Image fusion",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Pure mathematics"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Juncheng"
      },
      {
        "surname": "Liao",
        "given_name": "Qingmin"
      },
      {
        "surname": "Liu",
        "given_name": "Shaojun"
      },
      {
        "surname": "Ma",
        "given_name": "Haoyu"
      },
      {
        "surname": "Yang",
        "given_name": "Wenming"
      },
      {
        "surname": "Xue",
        "given_name": "Jing-Hao"
      }
    ]
  },
  {
    "title": "Learning label correlations for multi-label image recognition with graph networks",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.07.040",
    "abstract": "Multi-label image recognition is a task that predicts a set of object labels in an image. As the objects co-occur in the physical world, it is desirable to model label dependencies. Previous existing methods resort to either recurrent networks or pre-defined label correlation graphs for this purpose. In this paper, instead of using a pre-defined graph which is inflexible and may be sub-optimal for multi-label classification, we propose the A-GCN, which leverages the popular Graph Convolutional Networks with an Adaptive label correlation graph to model label dependencies. Specifically, we introduce a plug-and-play Label Graph (LG) module to learn label correlations with word embeddings, and then utilize traditional GCN to map this graph into label-dependent object classifiers which are further applied to image features. The basic LG module incorporates two 1 × 1 convolutional layers and uses the dot product to generate label graphs. In addition, we propose a sparse correlation constraint to enhance the LG module, and also explore different LG architectures. We validate our method on two diverse multi-label datasets: MS-COCO and Fashion550K. Experimental results show that our A-GCN significantly improves baseline methods and achieves performance superior or comparable to the state of the art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302968",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Correlation",
      "Geometry",
      "Graph",
      "Image (mathematics)",
      "Mathematics",
      "Multi-label classification",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Qing"
      },
      {
        "surname": "Peng",
        "given_name": "Xiaojiang"
      },
      {
        "surname": "Qiao",
        "given_name": "Yu"
      },
      {
        "surname": "Peng",
        "given_name": "Qiang"
      }
    ]
  },
  {
    "title": "Nonparallel support vector machine with large margin distribution for pattern classification",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107374",
    "abstract": "The large margin distribution machine (LDM) combines the working principle of support vector machine (SVM) and the margin distribution to directly improve the algorithm's generalization. The margin distribution can be expressed with the margin mean and margin variance. It has been proved to be an efficient algorithm for binary classification. Inspired by the LDM, a novel classifier termed as LMD-NPSVM is proposed to improve the generalization performance of the nonparallel support vector machine (NPSVM) in this paper. Firstly, to meet the structure of NPSVM, the large margin distribution is reconstructed. Then, the linear LMD-NPSVM is built by introducing the reconstructed margin distribution into NPSVM. In addition, the linear case is extended to the nonlinear case with a kernel trick. All experiments show that our LMD-NPSVM is superior to the state-of-the-art algorithms in generalization performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301771",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Binary classification",
      "Binary number",
      "Classifier (UML)",
      "Combinatorics",
      "Computer science",
      "Generalization",
      "Kernel (algebra)",
      "Kernel method",
      "Machine learning",
      "Margin (machine learning)",
      "Margin classifier",
      "Mathematical analysis",
      "Mathematics",
      "Nonlinear system",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Relevance vector machine",
      "Structured support vector machine",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Liming"
      },
      {
        "surname": "Chu",
        "given_name": "Maoxiang"
      },
      {
        "surname": "Gong",
        "given_name": "Rongfen"
      },
      {
        "surname": "Peng",
        "given_name": "Yongcheng"
      }
    ]
  },
  {
    "title": "Vanishing region loss for crowd density estimation",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.08.001",
    "abstract": "Crowd density estimation is a crucial component in surveillance systems to construct safe and efficient urban environments. Due to perspective distortion, individuals in crowd scenes diminish in size as they converge toward the vanishing point. Hence, there are significant visual variations in individuals’ size and appearance, which may lead to inaccurate estimations of crowd counts. This paper proposes an intuitive and effective loss function for the error estimation of crowd counts, particularly in vanishing crowd regions. Specifically, estimation errors in vanishing crowd regions are used to refine and generate network filters that are adaptive toward perspective distortion during network training. Extensive experiments on the challenging UCF-QNRF, WorldExpo, and ShanghaiTech benchmark datasets demonstrate the effectiveness of our novel loss function for training a network to achieve accurate crowd density estimation, particularly in the presence of perspective distortion.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302993",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Benchmark (surveying)",
      "Biology",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Construct (python library)",
      "Density estimation",
      "Distortion (music)",
      "Economics",
      "Estimation",
      "Estimator",
      "Evolutionary biology",
      "Function (biology)",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Machine learning",
      "Management",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Perspective distortion",
      "Programming language",
      "Statistics",
      "Vanishing point"
    ],
    "authors": [
      {
        "surname": "Yılmaz",
        "given_name": "Bedir"
      },
      {
        "surname": "Sheikh Abdullah",
        "given_name": "Siti Norul Huda"
      },
      {
        "surname": "Kok",
        "given_name": "Ven Jyn"
      }
    ]
  },
  {
    "title": "Weighted-capsule routing via a fuzzy gaussian model",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.08.009",
    "abstract": "Capsule network (CapsNet) is a novel architecture that takes into account the hierarchical pose relationships between object parts, which had achieved desirable results on image classification. EM-Routing (EM-R) used in CapsNet is the process of assigning child capsules (parts) to each parent capsule (objects) based on a level of agreement, which is similar to the fuzzy clustering process. However, CapsNet still struggles with backgrounds and the presence of noise. In this paper, a new routing algorithm based on a weighted capsule fuzzy gaussian model (WCFGM-R) and a pose loss function are proposed. The proposed algorithm aims to prohibit atypical child capsules from contaminating the parent capsules by incorporating the activations of capsules in a lower layer as weights that play the role of precision. The pose loss provides the best inter-class separation and improves the ability of pattern classification. Indeed, the experimental analyses demonstrate that CapsNet with WCFGM-R outperforms the CapsNet with EM-R in which it shows excellent results on three datasets (MNIST-bg-img, MNIST-bg-rnd, and CIFAR10).",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552030307X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Computer network",
      "Computer science",
      "Fuzzy logic",
      "Image (mathematics)",
      "Layer (electronics)",
      "MNIST database",
      "Operating system",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Routing (electronic design automation)"
    ],
    "authors": [
      {
        "surname": "Amira",
        "given_name": "Ouafa"
      },
      {
        "surname": "Xu",
        "given_name": "Shuang"
      },
      {
        "surname": "Du",
        "given_name": "Fang"
      },
      {
        "surname": "Zhang",
        "given_name": "Jiangshe"
      },
      {
        "surname": "Zhang",
        "given_name": "Chunxia"
      },
      {
        "surname": "Hamza",
        "given_name": "Rafik"
      }
    ]
  },
  {
    "title": "Controlling information capacity of binary neural network",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.07.033",
    "abstract": "Despite the growing popularity of deep learning technologies, high memory requirements and power consumption are essentially limiting their application in mobile and IoT areas. While binary convolutional networks can alleviate these problems, the limited bitwidth of weights is often leading to significant degradation of prediction accuracy. In this paper, we present a method for training binary networks that maintains a stable predefined level of their information capacity throughout the training process by applying Shannon entropy based penalty to convolutional filters. The results of experiments conducted on the SVHN, CIFAR and ImageNet datasets demonstrate that the proposed approach can statistically significantly improve the accuracy of binary networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302877",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary Independence Model",
      "Binary number",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Engineering",
      "Entropy (arrow of time)",
      "Limiting",
      "Machine learning",
      "Mathematics",
      "Mechanical engineering",
      "Operating system",
      "Physics",
      "Power (physics)",
      "Power consumption",
      "Process (computing)",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Ignatov",
        "given_name": "Dmitry"
      },
      {
        "surname": "Ignatov",
        "given_name": "Andrey"
      }
    ]
  },
  {
    "title": "Intelligent medical heterogeneous big data set balanced clustering using deep learning",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.08.027",
    "abstract": "In order to address the clustering problem of intelligent medical data, the data sets were not preprocessed using the traditional method, leading to a large amount of calculation, low efficiency, and large data cluster center offset distance. We proposed a balanced clustering algorithm for intelligent medical heterogeneous big data set using deep learning. Firstly, a deep neural network model based on incremental updating was constructed, and adaptive training and adjustment were made according to data scale, and the multi-layer feature learning of heterogeneous big data sets of intelligent medical care. Secondly, under-sampling preprocessing was carried out on the data set so that the data of the heterogeneous big data set was in a balanced state, and on this basis, clustering calculation of the heterogeneous big data was conducted. Then, the clustering center was set according to the kernel density estimation results, and the data cluster center was updated iteratively until convergence by combining the data features obtained from deep learning and euclidean distance calculation, so as to complete the balanced clustering of the heterogeneous big data set of intelligent medical treatment. The results show that the proposed algorithm has the advantages of small data cluster center offset distance, short clustering time, low energy consumption, high Macro-F1 value and NMI value, and the accuracy of clustering can be as high as 95%, the calculational cost is low, which has certain advantages. 2020 Elsevier Ltd. All rights reserved.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303329",
    "keywords": [
      "Artificial intelligence",
      "Big data",
      "CURE data clustering algorithm",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Data pre-processing",
      "Data set",
      "Data stream clustering",
      "Deep learning",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xiaofeng"
      },
      {
        "surname": "Jiao",
        "given_name": "Hongshuang"
      },
      {
        "surname": "Li",
        "given_name": "Dong"
      }
    ]
  },
  {
    "title": "Community enhanced graph convolutional networks",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.08.015",
    "abstract": "Graph representation learning is a key technology for processing graph-structured data. Graph convolutional networks (GCNs), as a type of currently emerging and commonly used model for graph representation learning, have achieved significant performance improvement. However, GCNs acquire node representations mainly through aggregating their neighbor information, largely ignoring the community structure which is one of the most important feature of the graph. In this paper, we propose a novel method called Community Enhanced Graph Convolutional Networks (CE-GCN), which integrates both neighborhood and community information to learn node representations. Specifically, the neighborhood information of nodes is aggregated by a graph convolutional network. The community information of nodes is calculated by a modularity constraint. Finally, we incorporate the modularity constraint into the graph convolutional network, and then form a unified model framework. Experimental results on five real-world network datasets demonstrate that CE-GCN significantly outperforms state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303135",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Feature learning",
      "Genetics",
      "Graph",
      "Modularity (biology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yanbei"
      },
      {
        "surname": "Wang",
        "given_name": "Qi"
      },
      {
        "surname": "Wang",
        "given_name": "Xiao"
      },
      {
        "surname": "Zhang",
        "given_name": "Fang"
      },
      {
        "surname": "Geng",
        "given_name": "Lei"
      },
      {
        "surname": "Wu",
        "given_name": "Jun"
      },
      {
        "surname": "Xiao",
        "given_name": "Zhitao"
      }
    ]
  },
  {
    "title": "Towards practical implementations of person re-identification from full video frames",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.08.023",
    "abstract": "With the major adoption of automation for cities security, person re-identification (Re-ID) has been extensively studied recently. In this paper, we argue that the current way of studying person re-identification, i.e. by trying to re-identify a person within already detected and pre-cropped images of people, is not sufficient to implement practical security applications, where the inputs to the system are the full frames of the video streams. To support this claim, we introduce the Full Frame Person Re-ID setting (FF-PRID) and define specific metrics to evaluate FF-PRID implementations. To improve robustness, we also formalize the hybrid human-machine collaboration framework, which is inherent to any Re-ID security applications. To demonstrate the importance of considering the FF-PRID setting, we build an experiment showing that combining a good people detection network with a good Re-ID model does not necessarily produce good results for the final application. This underlines a failure of the current formulation in assessing the quality of a Re-ID model and justifies the use of different metrics. We hope that this work will motivate the research community to consider the full problem in order to develop algorithms that are better suited to real-world scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303287",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Botany",
      "Chemistry",
      "Computer science",
      "Epistemology",
      "Frame (networking)",
      "Gene",
      "Identification (biology)",
      "Implementation",
      "Machine learning",
      "Philosophy",
      "Quality (philosophy)",
      "Robustness (evolution)",
      "Software engineering",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Sumari",
        "given_name": "Felix O."
      },
      {
        "surname": "Machaca",
        "given_name": "Luigy"
      },
      {
        "surname": "Huaman",
        "given_name": "Jose"
      },
      {
        "surname": "Clua",
        "given_name": "Esteban W.G."
      },
      {
        "surname": "Guérin",
        "given_name": "Joris"
      }
    ]
  },
  {
    "title": "A new focus detection criterion in holograms of planktonic organisms",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.08.004",
    "abstract": "With the increased use of digital holography to observe biological events in three dimensions the need for autofocusing becomes essential. We introduced a modified image quality evaluation technique as a focus criterion in the reconstruction of digital holograms. The method employed was initially proposed by [Wang et al. (2008)] to evaluate image inpainting quality from three aspects: luminance, definition, and gradient similarity. We considered these criteria for automatic focusing, individually and in combination. Our numerical simulation and experimental results obtained from 96 holograms of planktonic organisms show that the combined approach provided more accurate results compared to the individual metrics. We propose the use of such combined criterion to implement autofocusing algorithms for serial holograms, such as from observations of performances of plankton behavior.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303020",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Digital holography",
      "Focus (optics)",
      "Holography",
      "Image (mathematics)",
      "Image quality",
      "Inpainting",
      "Luminance",
      "Optics",
      "Physics",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Moreno",
        "given_name": "Gelaysi"
      },
      {
        "surname": "Ascaneo",
        "given_name": "Jefferson S."
      },
      {
        "surname": "Ricardo",
        "given_name": "Jorge O."
      },
      {
        "surname": "De La Cruz",
        "given_name": "Leandro T."
      },
      {
        "surname": "Arias",
        "given_name": "Yaumel"
      },
      {
        "surname": "Strickler",
        "given_name": "J. Rudi"
      },
      {
        "surname": "Lopes",
        "given_name": "Rubens M."
      }
    ]
  },
  {
    "title": "Editorial — Virtual Special Issue: “Hierarchical Representations: New Results and Challenges for Image Analysis”",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.07.019",
    "abstract": "This editorial introduces the Virtual Special Issue: “Hierarchical Representations: New Results and Challenges for Image Analysis” that was handled between May 2019 and June 2020.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302658",
    "keywords": [
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Data science",
      "Image (mathematics)"
    ],
    "authors": [
      {
        "surname": "Passat",
        "given_name": "Nicolas"
      },
      {
        "surname": "Kurtz",
        "given_name": "Camille"
      },
      {
        "surname": "Vacavant",
        "given_name": "Antoine"
      }
    ]
  },
  {
    "title": "Effective schizophrenia recognition using discriminative eye movement features and model-metric based features",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.09.017",
    "abstract": "Eye movement abnormalities have been effective biomarkers that provide the possibility of distinguishing patients with schizophrenia from healthy controls. The existing methods for measuring eye movement abnormalities mostly focus on synchronic parameters, such as fixation duration and saccade amplitude, which can be directly obtained from eye movement data, while lack of considering more thorough features. In this paper, to better characterize eye-tracking dysfunction, we create a dataset containing 100 images with eye movement data of 40 patients and 30 healthy controls via a free-viewing task, and propose two types of features for effective schizophrenia recognition, i.e. the hand-crafted discriminative eye movement features and the model-metric based features via utilizing the computational models of fixation prediction and the metrics of evaluating their prediction performance. Using the proposed features, two commonly used classifiers including support vector machine and random forest have been trained for classification between patients and controls. Experimental results demonstrate the effectiveness of the proposed features for improving classification performance, and the potential that our method can serve as an alternative and promising approach for the computer-aided diagnosis of schizophrenia.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303536",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Economics",
      "Environmental health",
      "Eye movement",
      "Eye tracking",
      "Fixation (population genetics)",
      "Machine learning",
      "Medicine",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Population",
      "Random forest",
      "Saccade",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Lijin"
      },
      {
        "surname": "Wei",
        "given_name": "Weijie"
      },
      {
        "surname": "Liu",
        "given_name": "Zhi"
      },
      {
        "surname": "Zhang",
        "given_name": "Tianhong"
      },
      {
        "surname": "Wang",
        "given_name": "Jijun"
      },
      {
        "surname": "Xu",
        "given_name": "Lihua"
      },
      {
        "surname": "Chen",
        "given_name": "Weiyu"
      },
      {
        "surname": "Le Meur",
        "given_name": "Olivier"
      }
    ]
  },
  {
    "title": "Joint multi-scale discrimination and region segmentation for person re-ID",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.08.022",
    "abstract": "Most existing person re-identification methods are mainly based on human part partition with horizontal stripes or human body semantic segmentation. In this paper, we propose a method called MDRS (Multi-scale Discriminative network with Region Segmentation) to integrate multi-scale discriminative feature learning, horizontal stripe partition and semantic segmentation in a single framework, in which multi-scale horizontal stripe partition and usage of both global and local features make the framework be robust to human pose variation, occlusion and background clutter, and semantic segmentation boosts the performance of person identification via shared multi-scale feature extraction. MDRS is trained end-to-end with a multi-task learning strategy that considers three tasks simultaneously: person identification, triplet prediction and pixel-wise semantic segmentation. Comprehensive experiments confirm that our approach exceeds many methods and robustly achieves excellent performances on mainstream evaluation datasets including Market-1501, DukeMTMC-reid and CUHK03.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303275",
    "keywords": [
      "Artificial intelligence",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Geography",
      "Joint (building)",
      "Pattern recognition (psychology)",
      "Scale (ratio)",
      "Segmentation",
      "Structural engineering"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Jialiang"
      },
      {
        "surname": "Liu",
        "given_name": "Bo"
      },
      {
        "surname": "Fu",
        "given_name": "Lihua"
      }
    ]
  },
  {
    "title": "MAGAN: A masked autoencoder generative adversarial network for processing missing IoT sequence data",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.07.025",
    "abstract": "Missing sequence data prevent local data from reflecting the overall distribution of a sample, hindering data analysis. The problem of missing data during actual production is a serious issue and results in a high defect rate, low dimensionality, and high noise level. In this study, a Masked Generative Adversarial Network (MAGAN) model is proposed that is less affected by the data loss rate than a baseline comparison model, and at an 80% missing data rate, the model can still better reflect the distribution of real data. MAGAN shows better results than a traditional processing method for dealing with missing data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302713",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Baseline (sea)",
      "Biology",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Curse of dimensionality",
      "Data mining",
      "Deep learning",
      "Generative adversarial network",
      "Generative grammar",
      "Generative model",
      "Genetics",
      "Geology",
      "Machine learning",
      "Missing data",
      "Oceanography",
      "Pattern recognition (psychology)",
      "Sample (material)",
      "Sequence (biology)",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Weihan",
        "given_name": "Wang"
      }
    ]
  },
  {
    "title": "Discriminative sampling via deep reinforcement learning for kinship verification",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.06.019",
    "abstract": "In this paper, we propose a discriminative sampling method to select most effective negative samples via deep reinforcement learning for kinship verification. Unlike most existing facial kinship verification methods which focus on extracting effective features with the random sampling strategy, we develop a deep reinforcement learning method to select samples which are more suitable for learning discriminative features, so that the overall performance can be improved. Specifically, our method uses two subnetworks to achieve the kinship verification task: one DQN-based sampling network to filter the negative samples, and one multi-layer convolutional network to verify the kin relationship. Experimental results on the KinFaceW-I and KinFaceW-II datasets show the superiority of our proposed approach over the state-of-the-arts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302373",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Discriminative model",
      "Economics",
      "Filter (signal processing)",
      "Focus (optics)",
      "Kinship",
      "Law",
      "Machine learning",
      "Management",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Psychology",
      "Reinforcement",
      "Reinforcement learning",
      "Sampling (signal processing)",
      "Social psychology",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Shiwei"
      },
      {
        "surname": "Yan",
        "given_name": "Haibin"
      }
    ]
  },
  {
    "title": "Preface: In memory of Alfredo Petrosino",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.07.038",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302944",
    "keywords": [
      "Arithmetic",
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Mathematics"
    ],
    "authors": [
      {
        "surname": "Ceccarelli",
        "given_name": "Michele"
      }
    ]
  },
  {
    "title": "Nonparametric maximum likelihood estimation using neural networks",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.09.006",
    "abstract": "Estimation of probability density functions is an essential component of various applications. Nonparametric techniques have been widely used for this task owing to the difficulty in parameterization of data. In particular, certain kernel density estimation methods have been developed. However, they are either incapable of maximum likelihood estimation or require the maintenance of a training set to process new patterns. In this study, a new approach, called the nonparametric maximum likelihood neural network (MLNN), is proposed. This is a nonparametric method, relying on maximum likelihood and neural network. It is compact in form and does not require the maintenance of training patterns. Theoretical and experimental analyses demonstrate the efficacy of the proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303391",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Density estimation",
      "Engineering",
      "Estimation",
      "Estimator",
      "Kernel (algebra)",
      "Kernel density estimation",
      "Kernel method",
      "Machine learning",
      "Mathematics",
      "Maximum likelihood",
      "Nonparametric statistics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Programming language",
      "Set (abstract data type)",
      "Statistics",
      "Support vector machine",
      "Systems engineering",
      "Variable kernel density estimation"
    ],
    "authors": [
      {
        "surname": "Huynh",
        "given_name": "Hieu Trung"
      },
      {
        "surname": "Nguyen",
        "given_name": "Linh"
      }
    ]
  },
  {
    "title": "Deep k-Means: Jointly clustering with k-Means and learning representations",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.07.028",
    "abstract": "We study in this paper the problem of jointly clustering and learning representations. As several previous studies have shown, learning representations that are both faithful to the data to be clustered and adapted to the clustering algorithm can lead to better clustering performance, all the more so that the two tasks are performed jointly. We propose here such an approach for k-Means clustering based on a continuous reparametrization of the objective function that leads to a truly joint solution. The behavior of our approach is illustrated on various datasets showing its efficacy in learning representations for objects while clustering them.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302749",
    "keywords": [],
    "authors": [
      {
        "surname": "Moradi Fard",
        "given_name": "Maziar"
      },
      {
        "surname": "Thonet",
        "given_name": "Thibaut"
      },
      {
        "surname": "Gaussier",
        "given_name": "Eric"
      }
    ]
  },
  {
    "title": "If dropout limits trainable depth, does critical initialisation still matter? A large-scale statistical analysis on ReLU networks",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.06.025",
    "abstract": "Recent work in signal propagation theory has shown that dropout limits the depth to which information can propagate through a neural network. In this paper, we investigate the effect of initialisation on training speed and generalisation for ReLU networks within this depth limit. We ask the following research question: given that critical initialisation is crucial for training at large depth, if dropout limits the depth at which networks are trainable, does initialising critically still matter? We conduct a large-scale controlled experiment, and perform a statistical analysis of over 12 000 trained networks. We find that (1) trainable networks show no statistically significant difference in performance over a wide range of non-critical initialisations; (2) for initialisations that show a statistically significant difference, the net effect on performance is small; (3) only extreme initialisations (very small or very large) perform worse than criticality. These findings also apply to standard ReLU networks of moderate depth as a special case of zero dropout. Our results therefore suggest that, in the shallow-to-moderate depth setting, critical initialisation provides zero performance gains when compared to off-critical initialisations and that searching for off-critical initialisations that might improve training speed or generalisation, is likely to be a fruitless endeavour.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302439",
    "keywords": [
      "Aerospace engineering",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Criticality",
      "Dropout (neural networks)",
      "Engineering",
      "Limit (mathematics)",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Nuclear physics",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Range (aeronautics)",
      "Scale (ratio)",
      "Zero (linguistics)"
    ],
    "authors": [
      {
        "surname": "Pretorius",
        "given_name": "Arnu"
      },
      {
        "surname": "van Biljon",
        "given_name": "Elan"
      },
      {
        "surname": "van Niekerk",
        "given_name": "Benjamin"
      },
      {
        "surname": "Eloff",
        "given_name": "Ryan"
      },
      {
        "surname": "Reynard",
        "given_name": "Matthew"
      },
      {
        "surname": "James",
        "given_name": "Steve"
      },
      {
        "surname": "Rosman",
        "given_name": "Benjamin"
      },
      {
        "surname": "Kamper",
        "given_name": "Herman"
      },
      {
        "surname": "Kroon",
        "given_name": "Steve"
      }
    ]
  },
  {
    "title": "Person re-identification for smart cities: State-of-the-art and the path ahead",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.07.030",
    "abstract": "One of the indispensable pillars of a smart city is its surveillance infrastructure, and it requires smart techniques to analyze the videos acquired from the surveillance cameras. Person re-identification (PRId) is one of the fundamental tasks in automated visual surveillance, and it has been an area of extensive research spanning the past decade. PRId aims at finding a person who has previously been seen or identified using some unique descriptor of the person. This survey comprises a broad spectrum of PRId methods spanning from traditional to deep-learning, being analyzed and compared. This survey also discusses various PRId frameworks based on machine learning and deep learning. This study emphasizes the challenges in building PRId systems for the benefits of smart cities and presents a critical overview of recent progress and the state-of-the-art approaches to solving some significant challenges of existing PRId systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302762",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Data science",
      "Deep learning",
      "Identification (biology)",
      "Machine learning",
      "Path (computing)",
      "Programming language",
      "State (computer science)",
      "State of art"
    ],
    "authors": [
      {
        "surname": "Behera",
        "given_name": "Nayan Kumar Subhashis"
      },
      {
        "surname": "Sa",
        "given_name": "Pankaj Kumar"
      },
      {
        "surname": "Bakshi",
        "given_name": "Sambit"
      }
    ]
  },
  {
    "title": "Robust discriminant analysis using multi-directional projection pursuit",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.09.013",
    "abstract": "While linear discriminant analysis (LDA) is a widely used classification method, it is highly affected by outliers which commonly occur in various real datasets. Therefore, several robust LDA methods have been proposed. However, they either rely on robust estimation of the sample means and covariance matrix which may have noninvertible Hessians or can only handle binary classes or low dimensional cases. The proposed robust discriminant analysis is a multi-directional projection-pursuit approach which can classify multiple classes without estimating the covariance or Hessian matrix and work for high dimensional cases. The weight function effectively gives smaller weights to the points more deviant from the class center. The discriminant vectors and scoring vectors are solved by the proposed iterative algorithm. It inherits good properties of the weight function and multi-directional projection pursuit for reducing the influence of outliers on estimating the discriminant directions and producing robust classification which is less sensitive to outliers. We show that when a weight function is appropriately chosen, then the influence function is bounded and discriminant vectors and scoring vectors are both consistent as the percentage of outliers goes to zero. The experimental results show that the robust optimal scoring discriminant analysis is effective and efficient.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552030338X",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Binary classification",
      "Computer science",
      "Covariance",
      "Covariance matrix",
      "Discriminant",
      "Estimation of covariance matrices",
      "Hessian matrix",
      "Linear discriminant analysis",
      "Mathematics",
      "Optimal discriminant analysis",
      "Outlier",
      "Pattern recognition (psychology)",
      "Projection (relational algebra)",
      "Projection pursuit",
      "Scatter matrix",
      "Statistics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Hsin-Hsiung"
      },
      {
        "surname": "Zhang",
        "given_name": "Teng"
      }
    ]
  },
  {
    "title": "Scale balance for prototype-based binary quantization",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107409",
    "abstract": "Nowadays, prototype-based binary quantization (PBQ) is a promising solution for the approximate nearest neighbor search problem, which simultaneously preserves the affinity structures of prototypes in both Euclidean space as well as those of their codes in binary space. To learn longer binary codes, space decomposition based on product quantization is usually adopted. In practice, we find that the scale between Euclidean distance and Hamming distance usually varies across these decomposed subspaces, which degenerates the performance of PBQ based methods. We make an attempt to balance the scale of these subspaces via a joint optimization problem in the classic PBQ model, and present both an iterative and alternate algorithm for optimization. We conducted experiments on 6 public databases, and demonstrated that our scale balancing based methods SKMH and SABQ outperform state-of-the-art hashing methods including popular prototype-based binary quantization methods, with up to 81.62% relative performance gains when learning 256-bit binary codes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302120",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Binary code",
      "Binary number",
      "Block code",
      "Computer science",
      "Computer security",
      "Decoding methods",
      "Euclidean distance",
      "Geometry",
      "Hamming code",
      "Hamming distance",
      "Hamming space",
      "Hash function",
      "Linear subspace",
      "Mathematical optimization",
      "Mathematics",
      "Quantization (signal processing)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Zhiyang"
      },
      {
        "surname": "Qu",
        "given_name": "Wenyu"
      },
      {
        "surname": "Cao",
        "given_name": "Yuan"
      },
      {
        "surname": "Qi",
        "given_name": "Heng"
      },
      {
        "surname": "Stojmenovic",
        "given_name": "Milos"
      },
      {
        "surname": "Hu",
        "given_name": "Jia"
      }
    ]
  },
  {
    "title": "Iris presentation attack detection: Where are we now?",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.08.018",
    "abstract": "As the popularity of iris recognition systems increases, the importance of effective security measures against presentation attacks becomes paramount. This work presents an overview of the most important advances in the area of iris presentation attack detection published in the recent two years. Newly-released, publicly-available datasets for development and evaluation of iris presentation attack detection are discussed. Recent literature can be seen to be broken into three categories: traditional “hand-crafted” feature extraction and classification, deep learning-based solutions, and hybrid approaches fusing both methodologies. Conclusions of modern approaches underscore the difficulty of this task. Finally, commentary on possible directions for future research is provided.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303226",
    "keywords": [],
    "authors": [
      {
        "surname": "Boyd",
        "given_name": "Aidan"
      },
      {
        "surname": "Fang",
        "given_name": "Zhaoyuan"
      },
      {
        "surname": "Czajka",
        "given_name": "Adam"
      },
      {
        "surname": "Bowyer",
        "given_name": "Kevin W."
      }
    ]
  },
  {
    "title": "Assessing similarity in handwritten texts",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.08.011",
    "abstract": "Today, people rely almost full time on digital texts. It is not surprising that handwriting earned a special status, and solutions to mimic real handwriting became attractive. A particular field called handwriting synthesis generates renderings of text which resemble natural writing but are synthesized from actual handwriting samples. The main idea behind samples’ current solutions is to collect enough samples to capture a given subject’s writing style, and therefore be able to reproduce it in new texts, with natural variability. Nevertheless, the question remains of how much input variability is enough to represent specific handwriting. In this paper, we address sample acquisition for handwriting synthesis. We conducted a study comparing written text similarity between two sets of samples, one using augmented pangrams (with a total of 473 characters) and the other using general texts (with 1586 characters). Our results show that the samples collected with pangrams are statistically equivalent in variation with samples collected using general texts, with many benefits, particularly the shorter time needed to collect the samples. We also made our data collection publicly available, providing a valuable original resource for future research.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303093",
    "keywords": [
      "Archaeology",
      "Art",
      "Artificial intelligence",
      "Astrophysics",
      "Chemistry",
      "Chromatography",
      "Computer network",
      "Computer science",
      "Field (mathematics)",
      "Handwriting",
      "History",
      "Image (mathematics)",
      "Information retrieval",
      "Linguistics",
      "Literature",
      "Mathematics",
      "Natural (archaeology)",
      "Natural language processing",
      "Philosophy",
      "Physics",
      "Pure mathematics",
      "Resource (disambiguation)",
      "Sample (material)",
      "Similarity (geometry)",
      "Speech recognition",
      "Style (visual arts)",
      "Variation (astronomy)",
      "Writing style"
    ],
    "authors": [
      {
        "surname": "Balreira",
        "given_name": "Dennis Giovani"
      },
      {
        "surname": "Filho",
        "given_name": "Danilo Marcondes"
      },
      {
        "surname": "Walter",
        "given_name": "Marcelo"
      }
    ]
  },
  {
    "title": "Attention-aware invertible hashing network with skip connections",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.09.002",
    "abstract": "In recent years, Convolutional Neural Networks (CNNs) have shown promising performance on image hashing retrieval. However, due to the information-discarded nature of CNN, some meaningful information can not be further extracted into a deep level and embedded into hash codes. To solve the problem, this study attempts to design an invertible CNN feature extractor to fully maintain input information meanwhile having well generalization ability. Specifically, we propose a novel Attention-Aware Invertible Hashing Network with Skip Connection (AIHN-SC) for image retrieval. Represented by an invertible feature, the hash code can be learned and generated from image characteristics preserving all input information. For achieving favourable generalization ability in our invertible architecture, we present a novel spatial attention mechanism to highlight regions involving semantic information. In addition, we introduce two kinds of skip connection, i.e. hierarchical and residual connections, which aim to provide richer knowledges for hash code learning and ease our training process. Extensive experiments on benchmark datasets demonstrate the effectiveness of our proposed AIHN-SC and show the significant performance in image retrieval against the state-of-the-arts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303342",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Code (set theory)",
      "Computer science",
      "Convolutional neural network",
      "Double hashing",
      "Feature (linguistics)",
      "Feature hashing",
      "Generalization",
      "Geodesy",
      "Geography",
      "Hash function",
      "Hash table",
      "Image (mathematics)",
      "Image retrieval",
      "Invertible matrix",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Pure mathematics",
      "Set (abstract data type)",
      "Source code",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Shanshan"
      },
      {
        "surname": "Cai",
        "given_name": "Qiang"
      },
      {
        "surname": "Li",
        "given_name": "Zhuangzi"
      },
      {
        "surname": "Li",
        "given_name": "Haisheng"
      },
      {
        "surname": "Zhang",
        "given_name": "Naiguang"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaoyu"
      }
    ]
  },
  {
    "title": "Probabilistic graph-based valuation model for measuring the relative patent value in a valuation scenario",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.07.023",
    "abstract": "With the intense competition of global intellectual property, the increasing number of patents promotes the potential of patent transactions. However,an unknown patent value degrades the patent transaction rate. Automatic patent valuation faces some challenges, including the following: (1) how to represent a valuation object, (2) how to construct the valuation scenario, and (3) how to generate and measure the patent value. To solve the above issues, we propose a probabilistic graph-based patent valuation model. In the model, the textual parts are combined with some structured parts of patents to represent a valuation object. A heterogeneous association network is constructed as the valuation scenario. Thereafter, a patent valuation model is formed by a generative process, which is represented by a probabilistic graphical model.The patent value distribution is learned by making inference using the valuation model. We evaluate our model by comparing it with state-of-the-art models on patent data sets. The results show that our model outperforms other models in the evaluation measurements.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302695",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Econometrics",
      "Economics",
      "Finance",
      "Graph",
      "Inference",
      "Probabilistic logic",
      "Theoretical computer science",
      "Valuation (finance)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Weidong"
      },
      {
        "surname": "Liu",
        "given_name": "Xin"
      },
      {
        "surname": "Qiao",
        "given_name": "Wenbo"
      }
    ]
  },
  {
    "title": "Weighted hybrid fusion with rank consistency",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.07.037",
    "abstract": "This paper proposes a weighted hybrid multi-view fusion method for the semi-supervised classification problem. Instead of getting access to the features from different views directly, this method utilizes the square losses of the multi-view classifiers to exploit the between-view relationship, which preserves the privacy of data. Considering the different prediction capability of classifiers on multiple views, an objective function with the constraint of rank consistency is constructed to weight view-specific learners adaptively, where the constraint makes each view-specific learner improve its performance by exploring the predicted results of other learners. Furthermore, an iterative algorithm based on the Variant Alternating Splitting Augmented Lagrangian Method (VASALM) and the quadratic programming method is developed to optimize the objective function. Experimental results on different real-world datasets demonstrate the effectiveness of the proposed method for multi-view learning. The experiments also analyze parameter sensitivity and convergency of the optimization algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302932",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Consistency (knowledge bases)",
      "Fusion",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Rank (graph theory)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Song"
      },
      {
        "surname": "Guo",
        "given_name": "Xin"
      },
      {
        "surname": "Tie",
        "given_name": "Yun"
      },
      {
        "surname": "Lee",
        "given_name": "Ivan"
      },
      {
        "surname": "Qi",
        "given_name": "Lin"
      },
      {
        "surname": "Guan",
        "given_name": "Ling"
      }
    ]
  },
  {
    "title": "Multi-lingual scene text detection and language identification",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.06.024",
    "abstract": "Scene text analysis is a field of research that poses challenges to researchers owing to the background complexities, image quality, text orientation, text size, etc. The problem gets more complex when the image contains multi-lingual texts. Most scene text detection techniques approach the problem as either a feature-based or deep learning-based problem. In this work, an end-to-end system is proposed for scene text detection, localization and language identification to combine feature-based and deep learning-based approaches. The model uses Maximally Stable Extremal Regions and Stroke Width Transform for generating text proposals, followed by proposal refinement using Generative Adversarial Network. Finally, a Convolution Neural Network based model is used for language identification of the detected scene texts. Experiments have been conducted on standard datasets like KAIST, COCO, CTW1500, CVSI and ICDAR along with an in-house multi-lingual Indic scene text dataset for which the proposed model achieves satisfactory results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302427",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Botany",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Deep learning",
      "Feature (linguistics)",
      "Field (mathematics)",
      "Generative adversarial network",
      "Geometry",
      "Identification (biology)",
      "Image (mathematics)",
      "Language identification",
      "Linguistics",
      "Mathematics",
      "Natural language",
      "Natural language processing",
      "Orientation (vector space)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pure mathematics",
      "Text detection"
    ],
    "authors": [
      {
        "surname": "Saha",
        "given_name": "Shaswata"
      },
      {
        "surname": "Chakraborty",
        "given_name": "Neelotpal"
      },
      {
        "surname": "Kundu",
        "given_name": "Soumyadeep"
      },
      {
        "surname": "Paul",
        "given_name": "Sayantan"
      },
      {
        "surname": "Mollah",
        "given_name": "Ayatullah Faruk"
      },
      {
        "surname": "Basu",
        "given_name": "Subhadip"
      },
      {
        "surname": "Sarkar",
        "given_name": "Ram"
      }
    ]
  },
  {
    "title": "A PSO-based algorithm for mining association rules using a guided exploration strategy",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.05.006",
    "abstract": "Association rule mining is one of the most important and active research areas in data mining. In the literature, several association rule miners have been proposed; among them, those based on particle swarm optimization (PSO) have reported the best results. However, these algorithms tend to prematurely fall into local solutions, avoiding a wide exploration that could produce even better results. In this paper, an algorithm based on PSO, called PSO-GES, for mining association rules using a Guided Exploration Strategy is introduced. Our experiments, over real-world transactional databases, show that our proposed algorithm mines better quality association rules than the most recent PSO-based algorithms for mining association rules of the state of the art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301768",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Association (psychology)",
      "Association rule learning",
      "Computer science",
      "Data mining",
      "Epistemology",
      "Machine learning",
      "Particle swarm optimization",
      "Philosophy",
      "Quality (philosophy)"
    ],
    "authors": [
      {
        "surname": "Bernal Baró",
        "given_name": "Gretel"
      },
      {
        "surname": "Martínez-Trinidad",
        "given_name": "José Francisco"
      },
      {
        "surname": "Valdovinos Rosas",
        "given_name": "Rosa María"
      },
      {
        "surname": "Carrasco Ochoa",
        "given_name": "Jesús A."
      },
      {
        "surname": "Rodríguez González",
        "given_name": "Ansel Y."
      },
      {
        "surname": "Lazo Cortés",
        "given_name": "Manuel S."
      }
    ]
  },
  {
    "title": "An efficient attribute-space connected filter on graphs to reconstruct paths in point-clouds",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107467",
    "abstract": "Measurements by many multi-sensor systems can be considered as point-clouds. One such system is the tracker for the PANDA experiment. Charged particles passing through the tracker produce patterns representing their paths. We present a new, graph-based, attribute-space morphological connected filter for reconstructing particle paths through such a detector. We introduce the concept of attribute-spaces and attribute-space connected filters on graphs, rather than binary images and show a new processing scheme to reduce the size of the memory required to store the attribute-space representations of binary images and graphs. The result is an O(Nlog (N)) algorithm with a total recognition error of approximately 0.10, a significant improvement compared to our previous state-of-the-art O(N 2) algorithm with a total error of 0.17.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302703",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Computer science",
      "Computer vision",
      "Filter (signal processing)",
      "Geometry",
      "Graph",
      "Mathematics",
      "Operating system",
      "Particle filter",
      "Pedagogy",
      "Point (geometry)",
      "Point cloud",
      "Psychology",
      "Space (punctuation)",
      "Theoretical computer science",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Babai",
        "given_name": "M."
      },
      {
        "surname": "Kalantar-Nayestanaki",
        "given_name": "N."
      },
      {
        "surname": "Messchendorp",
        "given_name": "J.G."
      },
      {
        "surname": "Wilkinson",
        "given_name": "M.H.F."
      }
    ]
  },
  {
    "title": "Deep supervised feature selection for social relationship recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.08.005",
    "abstract": "Social relationships link everyone in human society. Exploring social relationships in still images promotes researches of behaviors or characteristics among persons. Previous literature has discovered that face and body attributes can provide effective semantic information for social relationship recognition. However, they ignore that attributes contribute much differently to the recognition accuracy, and these multi-source attributes may contain redundancies and noises. This work aims to promote social relationship recognition accuracy by abstracting multi-source attribute features more efficiently. To this end, we propose a novel Deep Supervised Feature Selection (DSFS) framework to recognize social relationships in photos, which fuses the deep learning algorithm with l 2,1-norm to learn a discriminative feature subset from multi-source features by leveraging the face and body attributes. Experimental results on PIPA-relation dataset qualitatively demonstrate the effectiveness of the proposed DSFS framework.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303032",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Discriminative model",
      "Face (sociological concept)",
      "Facial recognition system",
      "Feature (linguistics)",
      "Feature selection",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Relation (database)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Mengyin"
      },
      {
        "surname": "Du",
        "given_name": "Xiaoyu"
      },
      {
        "surname": "Shu",
        "given_name": "Xiangbo"
      },
      {
        "surname": "Wang",
        "given_name": "Xun"
      },
      {
        "surname": "Tang",
        "given_name": "Jinhui"
      }
    ]
  },
  {
    "title": "Quantification of malaria parasitaemia using trainable semantic segmentation and capsnet",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.07.002",
    "abstract": "Malaria is a life-threatening mosquito (Anopheles)-borne blood disease caused by the plasmodium parasite. Microscopic examination of peripheral blood smears by experts helps to identify parasites precisely. The manual assessment technique is a tedious and time-consuming process. The present study focuses on developing a hybrid screening algorithm for automated identification and classification of malaria parasite-infected red blood cells (RBCs). Initially, a semantic blood cell segmentation method is adopted where a supervised classifier-regulated pixel-based segmentation is adopted to segment individual RBC present in an image. In pixel-based classification, foreground (RBCs) and background regions are considered, a pixel-based large feature dataset is generated, and an artificial neural network (ANN) classifier is trained. The trained model generates a probability map of an image which is later post-processed by Graph-cut and Marker-controlled Watershed method for developing cropped RBC image set. The proposed segmentation method achieves 99.1% accuracy. Finally, a trained modified Capsule Network (CapsNet) model is used for classification of segmented blood cells to identify the species and stages of the parasites. Here, two specific parasite species viz., Plasmodium vivax and Plasmodium falciparum with stages are considered for classification. The performance of the proposed two-steps hybrid malaria screening is promising and the training and testing on local and benchmark dataset with respect to ground truth yield 98.7% accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302488",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Classifier (UML)",
      "Computer science",
      "Image segmentation",
      "Immunology",
      "Malaria",
      "Pattern recognition (psychology)",
      "Pixel",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Maity",
        "given_name": "Maitreya"
      },
      {
        "surname": "Jaiswal",
        "given_name": "Ayush"
      },
      {
        "surname": "Gantait",
        "given_name": "Kripasindhu"
      },
      {
        "surname": "Chatterjee",
        "given_name": "Jyotirmoy"
      },
      {
        "surname": "Mukherjee",
        "given_name": "Anirban"
      }
    ]
  },
  {
    "title": "A comprehensive security analysis of match-in-database fingerprint biometric system",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.07.024",
    "abstract": "The match-in-database fingerprint biometric system has emerged as the most widely used human identification and authentication method in public and private sectors due to its high efficiency, low-cost, and ease of use. This paper provides an exhaustive insight into the security aspect of such systems. We propose a comprehensive threat model depicting sixteen vulnerable attack points, which can act as a reference model while designing a new biometric application. We provide a comparison between the existing and proposed threat model. This article pinpoints all probable attacks at each vulnerable location and suggests possible mitigation techniques to thwart such attack attempts. We investigate the attacks on such systems, present a threat model, and categorize them into eight classes while specifying the risk factor associated with each class. This facilitates any new attack on such a system to be classified into one of these eight classes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302701",
    "keywords": [
      "Artificial intelligence",
      "Authentication (law)",
      "Biology",
      "Biometrics",
      "Botany",
      "Categorization",
      "Class (philosophy)",
      "Computer science",
      "Computer security",
      "Data mining",
      "Database",
      "Fingerprint (computing)",
      "Fingerprint recognition",
      "Identification (biology)",
      "Threat model"
    ],
    "authors": [
      {
        "surname": "Joshi",
        "given_name": "Mahesh"
      },
      {
        "surname": "Mazumdar",
        "given_name": "Bodhisatwa"
      },
      {
        "surname": "Dey",
        "given_name": "Somnath"
      }
    ]
  },
  {
    "title": "Manifold learning for user profiling and identity verification using motion sensors",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107408",
    "abstract": "Mobile devices are becoming ubiquitous and being increasingly used for data-sensitive activities such as communication, personal media storage, and banking. The protection of such data commonly relies on passwords and biometric traits such as fingerprints. These methods perform the user authentication sporadically and often require action from the user, which may make them susceptible to spoofing attacks. This scenario can be mitigated if we bring to bear motion-sensing based methods for authentication, which operate continuously and without requiring user action, hence are harder to attack. Such methods could be used allied with traditional authentication methods or on their own. This paper explores this idea in a novel user-agnostic approach for identity verification based on motion traits acquired by mobile sensors. The proposed approach does not require user-specific training before deployment in mobile devices nor does it require any extra sensor in the device. This solution is capable of learning a user profiling manifold from a small user subset and extend it to unknown users. We validated the proposal on two public datasets. The reported experiments demonstrate remarkable results under a cross-dataset protocol and an open-set setup. Moreover, we performed several analyses aiming at answering critical questions of a biometric method and the presented solution.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302119",
    "keywords": [
      "Artificial intelligence",
      "Authentication (law)",
      "Biometrics",
      "Computer science",
      "Computer security",
      "Human–computer interaction",
      "Mobile device",
      "Operating system",
      "Password",
      "Profiling (computer programming)",
      "Software deployment",
      "Spoofing attack",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Santos",
        "given_name": "Geise"
      },
      {
        "surname": "Pisani",
        "given_name": "Paulo Henrique"
      },
      {
        "surname": "Leyva",
        "given_name": "Roberto"
      },
      {
        "surname": "Li",
        "given_name": "Chang-Tsun"
      },
      {
        "surname": "Tavares",
        "given_name": "Tiago"
      },
      {
        "surname": "Rocha",
        "given_name": "Anderson"
      }
    ]
  },
  {
    "title": "MinReduct: A new algorithm for computing the shortest reducts",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.07.004",
    "abstract": "This paper deals with the problem of computing the shortest reducts of a decision system. The shortest reducts are useful for attribute reduction in classification problems and data size reduction. Unfortunately, finding all the shortest reducts is an NP-hard problem. There are some algorithms reported in the literature to overcome the complexity of computing the shortest reducts. However, most of these algorithms relay on costly operations for candidate evaluation. In this paper, we propose a new algorithm for computing all the shortest reducts; based on binary cumulative operations over a pair-wise comparison matrix, and a fast candidate evaluation process. Binary cumulative operations save computation time by avoiding repetitive calculations. Furthermore, unlike other algorithms reported in the literature, our candidate evaluation process relays on low-cost operations which reduce the runtime in most cases. Our experiments over synthetic and real-world decision systems show that our proposal is faster than state of the art algorithms in most decision systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302506",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Binary decision diagram",
      "Binary number",
      "Computation",
      "Computer science",
      "Data mining",
      "Geometry",
      "Mathematics",
      "Operating system",
      "Process (computing)",
      "Reduction (mathematics)"
    ],
    "authors": [
      {
        "surname": "Rodríguez-Diez",
        "given_name": "Vladímir"
      },
      {
        "surname": "Martínez-Trinidad",
        "given_name": "José Fco"
      },
      {
        "surname": "Carrasco-Ochoa",
        "given_name": "J Ariel"
      },
      {
        "surname": "Lazo-Cortés",
        "given_name": "Manuel S"
      },
      {
        "surname": "Olvera-López",
        "given_name": "J Arturo"
      }
    ]
  },
  {
    "title": "Multistage attention network for image inpainting",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107448",
    "abstract": "Image inpainting refers to the process of restoring the mask regions of damaged images. Existing inpainting algorithms have exhibited outstanding performance on certain inpainting tasks that are focused on recovering small masks or square masks. Tasks that attempt to reconstruct large proportion of damaged images can still be improved. Although many attention-related algorithms have been proposed to solve image inpainting tasks, most of them ignore the requirements to balancing the detail and style level. In this paper, we propose a novel image inpainting method for large-scale irregular masks. We introduce a special multistage attention module that considers structure consistency and detail fineness. The proposed multistage attention module operates in a coarse to-fine manner, where the early stage performs large feature patch swapping and ensures the global consistency in images, and the next stage swaps small patches to refine the texture. Then, we adopt a partial convolution strategy to avoid the misuse of invalid data during convolution. Several losses are combined as the training objective function to generate excellent results with global consistency and exquisite detail. Qualitative and quantitative experiments on the Paris StreetView, CelebA, and Places2 datasets demonstrate the superior performance of the proposed approach compared with state-of-the-art models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030251X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Convolution (computer science)",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Image processing",
      "Image texture",
      "Inpainting",
      "Linguistics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Texture synthesis"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Ning"
      },
      {
        "surname": "Ma",
        "given_name": "Sihan"
      },
      {
        "surname": "Li",
        "given_name": "Jingyuan"
      },
      {
        "surname": "Zhang",
        "given_name": "Yipeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Lefei"
      }
    ]
  },
  {
    "title": "Improved ASD classification using dynamic functional connectivity and multi-task feature selection",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.07.005",
    "abstract": "Accurate diagnosis of autism spectrum disorder (ASD), which is a neurodevelopmental disorder and often accompanied by abnormal social skills, communication skills, interests and behavior patterns, has always been a challenging task in clinical practice. Recent studies have shown great potential for using fMRI data to distinguish ASD from typical control (TC). However, it has always been a challenging problem to extract which features from fMRI data and how to combine these different types of features to achieve improved ASD/TC classification performance. To address this problem, in this study we propose an improved ASD/TC classification framework based on dynamic functional connectivity (DFC) and multi-task feature selection. Our proposed ASD/TC classification framework is evaluated on 871 subjects with fMRI data from the Autism Brain Imaging Data Exchange I (ABIDE I) via a 10-fold cross validation strategy. Experimental results show that our proposed method achieves an accuracy of 76.8% and an area under the receiver operating characteristic curve (AUC) of 0.81 for ASD/TC classification. In addition, compared with some existing state-of-the-art methods, our proposed method achieves better accuracy and AUC for ASD/TC classification. Overall, our proposed ASD/TC classification framework is effective and promising for automatic diagnosis of ASD in clinical practice.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302518",
    "keywords": [
      "Artificial intelligence",
      "Autism",
      "Autism spectrum disorder",
      "Computer science",
      "Developmental psychology",
      "Economics",
      "Feature (linguistics)",
      "Feature selection",
      "Functional connectivity",
      "Linguistics",
      "Machine learning",
      "Management",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Psychology",
      "Receiver operating characteristic",
      "Selection (genetic algorithm)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Jin"
      },
      {
        "surname": "Sheng",
        "given_name": "Yu"
      },
      {
        "surname": "Lan",
        "given_name": "Wei"
      },
      {
        "surname": "Guo",
        "given_name": "Rui"
      },
      {
        "surname": "Wang",
        "given_name": "Yufei"
      },
      {
        "surname": "Wang",
        "given_name": "Jianxin"
      }
    ]
  },
  {
    "title": "mDixon-based synthetic CT generation via transfer and patch learning",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.06.017",
    "abstract": "We propose a practicable method for generating synthetic CT images from modified Dixon (mDixon) MR data for the challenging body section of the abdomen and extending into the pelvis. Attenuation correction is necessary to make quantitatively accurate PET but is problematic withPET/MR scanning as MR data lack the information of photon attenuation. Multiple methods were proposed to generate synthetic CT from MR images. However, due to the challenge to distinguish bone and air in MR signals, most existing methods require advanced MR sequences that entail long acquisition time and have limited availablity. To address this problem, we propose a voxel-oriented method for synthetic CT generation using both the transfer and patch learning (SCG-TPL). The overall framework of SCG-TPL includes three stages. Stage I extracts seven-dimensional texture features from mDixon MR images using the weighted convolutional sum; Stage II enlists the knowledge-leveraged transfer fuzzy c-means (KL-TFCM) clustering as well as the patch learning-oriented semi-supervised LapSVM classification to train multiple candidate four-tissue-type-identifiers (FTTIs); Stage III synthesizes CT for new patients’ mDixon images using the candidate FTTIs and voting principle. The significance of our method is threefold: (1) As the global model for patch learning, guiding by the referenced knowledge, KL-TFCM can credibly initialize MR data with overcoming the individual diversity. As the local complement, LapSVM can adaptively model each patch with low time and labor costs. (2) Jointly using the transfer KL-TFCM clustering and patch learning-oriented LapSVM classification, SCG-TPL is able to output accurate synthetic CT in the abdomen. (3) SCG-TPL synthesizes CT only using easily-obtainable mDixon MR images, which greatly facilitates its clinical practicability. Experimental studies on ten subjects’ mDixon MR data verified the superiority of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552030235X",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Transfer of learning",
      "Voxel"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Xin"
      },
      {
        "surname": "Qian",
        "given_name": "Pengjiang"
      },
      {
        "surname": "Zheng",
        "given_name": "Jiamin"
      },
      {
        "surname": "Jiang",
        "given_name": "Yizhang"
      },
      {
        "surname": "Xia",
        "given_name": "Kaijian"
      },
      {
        "surname": "Traughber",
        "given_name": "Bryan"
      },
      {
        "surname": "Wu",
        "given_name": "Dongrui"
      },
      {
        "surname": "Muzic",
        "given_name": "Raymond F."
      }
    ]
  },
  {
    "title": "RefineU-Net: Improved U-Net with progressive global feedbacks and residual attention guided local refinement for medical image segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.07.013",
    "abstract": "Motivated by the recent advances in medical image segmentation using a fully convolutional network (FCN) called U-Net and its modified variants, we propose a novel improved FCN architecture called RefineU-Net. The proposed RefineU-Net consists of three modules: encoding module (EM), global refinement module (GRM) and local refinement module (LRM). EM is backboned by pretrained VGG-16 using ImageNet. GRM is proposed to generate intermediate layers in the skip connections in U-Net. It progressively upsamples the top side output of EM and fuses the resulted upsampled features with the side outputs of EM at each resolution level. Such fused features combine the global context information in shallow layers and the semantic information in deep layers for global refinement. Subsequently, to facilitate local refinement, LRM is proposed using residual attention gate (RAG) to generate discriminative attentive features to be concatenated with the decoded features in the expansive path of U-Net. Three modules are trained jointly in an end-to-end manner thereby both global and local refinement are performed complementarily. Extensive experiments conducted on four public datasets of polyp and skin lesion segmentation show the superiority of the proposed RefineU-Net to multiple state-of-the-art related methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302592",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Discriminative model",
      "Geometry",
      "Mathematics",
      "Net (polyhedron)",
      "Paleontology",
      "Path (computing)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Residual",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Dongyun"
      },
      {
        "surname": "Li",
        "given_name": "Yiqun"
      },
      {
        "surname": "Nwe",
        "given_name": "Tin Lay"
      },
      {
        "surname": "Dong",
        "given_name": "Sheng"
      },
      {
        "surname": "Oo",
        "given_name": "Zaw Min"
      }
    ]
  },
  {
    "title": "Blockchain-based anomaly detection of electricity consumption in smart grids",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.07.020",
    "abstract": "The big data generated by Industry 4.0 is expected to increase 20-fold in the next ten years and it has raised various challenges in Industrial Wireless Sensor Networks (IWSNs). Among these challenges, detecting different types of anomalies of industrial electricity consumption in an accurate and timely manner is a priority. If not handled properly, these anomalies could lead to serious consequences, such as irregular fire and paralyzed power system components. While existing anomaly detection techniques may be efficient for old systems, they are now faced with big transmitted data. Therefore, it is important to design new methods that can detect the electricity consumption anomaly and carry out appropriate actions. In this article, we first review several existing work on anomaly detection schemes, and then introduce the system and monitoring models. Then, we present a new framework that aims to detect electricity consumption anomalies accurately and timely using sensor processing, smart meter readings, machine learning and blockchain.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552030266X",
    "keywords": [
      "Anomaly (physics)",
      "Anomaly detection",
      "Big data",
      "Blockchain",
      "Computer network",
      "Computer science",
      "Computer security",
      "Condensed matter physics",
      "Consumption (sociology)",
      "Data mining",
      "Electrical engineering",
      "Electricity",
      "Electricity meter",
      "Engineering",
      "Physics",
      "Power (physics)",
      "Power consumption",
      "Quantum mechanics",
      "Real-time computing",
      "Smart grid",
      "Social science",
      "Sociology",
      "Wireless sensor network"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Meng"
      },
      {
        "surname": "Zhang",
        "given_name": "Keli"
      },
      {
        "surname": "Liu",
        "given_name": "Jiamou"
      },
      {
        "surname": "Gong",
        "given_name": "Hanxiao"
      },
      {
        "surname": "Zhang",
        "given_name": "Zijian"
      }
    ]
  },
  {
    "title": "Person re-identification based on multi-scale constraint network",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.08.012",
    "abstract": "Combining features of different scales to learn a more discriminative model is an essential solution for person re-identification (Re-ID) tasks. Most existing multi-scale methods are based on the fusion of features from different scales, which cannot exploit information throughly at each scale and cause gradient chaos in optimizing. To address this problem, in this paper we propose an end-to-end multi-scale constraint network(MSCN) to capture detailed information from multiple scales which can independently train each scale and integrate the features of each scale for prediction. In order to retain more information at different scales, we uniformly divide the feature maps into several parts, and vary the number of parts in different scales, then concatenate all the parts in each scale as the entire feature for training. We use both classification loss and metric loss to optimize the network from different aspects. Extensive experiments on three datasets demonstrate that our method achieves very competitive performance. Especially on the CUHK03 dataset, our approach achieves the state-of-the-art results outperforming the current best method by 2.4%/2.0% in Rank-1/mAP.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552030310X",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Combinatorics",
      "Computer science",
      "Computer security",
      "Constraint (computer-aided design)",
      "Data mining",
      "Discriminative model",
      "Economics",
      "Exploit",
      "Feature (linguistics)",
      "Geometry",
      "Identification (biology)",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Rank (graph theory)",
      "Scale (ratio)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Sishang"
      },
      {
        "surname": "Liu",
        "given_name": "Xueliang"
      },
      {
        "surname": "Zhao",
        "given_name": "Ye"
      },
      {
        "surname": "Wang",
        "given_name": "Meng"
      }
    ]
  },
  {
    "title": "Encoding multi-granularity structural information for joint Chinese word segmentation and POS tagging",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.07.017",
    "abstract": "Recent studies show that the joint Chinese word segmentation and POS tagging can enhance the mutual interaction and yield better performances for two tasks. However, existing joint methods fail to effectively take the advantage of the multiple granularity of information, e.g., character, word and subword, which has been proven prominently useful. In this paper, we propose to improve the joint tasks by leveraging such multi-granularity of information, by exploiting the lattice-LSTM and Convolutional Network (GCN) models for effectively encoding the graph information. On five benchmark datasets our proposed model shows highly competitive performances, achieving the new state-of-the-art results in the literature. Further analysis reveals that the multi-granularity information can relieve the out-of-vocabulary and the long-range dependency issues. Also the GCN structure is more effective for encoding the multi-granularity graph information, compared with the lattice structure.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302634",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Encoding (memory)",
      "Granularity",
      "Graph",
      "Linguistics",
      "Mutual information",
      "Natural language processing",
      "Operating system",
      "Philosophy",
      "Segmentation",
      "Theoretical computer science",
      "Vocabulary"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Ling"
      },
      {
        "surname": "Zhang",
        "given_name": "Ailian"
      },
      {
        "surname": "Liu",
        "given_name": "Ying"
      },
      {
        "surname": "Fei",
        "given_name": "Hao"
      }
    ]
  },
  {
    "title": "Fast matching via ergodic markov chain for super-large graphs",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107418",
    "abstract": "In theory, graph matching is a combinatorial problem. One state-of-the-art technique in graph matching, called spectral matching, relaxes the matching problem for consistent correspondence into spectral decomposition of the affinity matrix of graphs, but the most variations of spectral based algorithms suffer from their O(n 4) memory requirement. In this paper we propose a probabilistic spectral matching approach, in which the graph matching problem is formulated as an ergodic Markov chain, and the process of matching is addressed to reach the steady-state of the Markov chain. The approach decomposes the probability transition matrix, and solves the matching problem in O(n 2) space complexity using limited computing resource and RAM. This property makes the approach suitable for super-large graphs matching (for example, graphs with the number of points over 1000). We evaluate our algorithm on both the synthetic and the real datasets, and demonstrate that the proposed approach is significantly faster, and consumes smaller memory than SM, RRWM and FaSM with no loss of accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302211",
    "keywords": [
      "3-dimensional matching",
      "Algorithm",
      "Artificial intelligence",
      "Blossom algorithm",
      "Computer science",
      "Ergodic theory",
      "Machine learning",
      "Markov chain",
      "Matching (statistics)",
      "Mathematical analysis",
      "Mathematics",
      "Probabilistic logic",
      "Statistics",
      "Stochastic matrix",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Yali"
      },
      {
        "surname": "Pan",
        "given_name": "Lili"
      },
      {
        "surname": "Qian",
        "given_name": "Jiye"
      },
      {
        "surname": "Guo",
        "given_name": "Hongliang"
      }
    ]
  },
  {
    "title": "Prudence when assuming normality: An advice for machine learning practitioners",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.06.026",
    "abstract": "In a binary classification problem the feature vector (predictor) is the input to a scoring function that produces a decision value (score), which is compared to a particular chosen threshold to provide a final class prediction (output). Although the normal assumption of the scoring function is important in many applications, sometimes it is severely violated even under the simple multinormal assumption of the feature vector. This article proves this result mathematically with a counterexample to provide an advice for practitioners to avoid blind assumptions of normality. On the other hand, the article provides a set of experiments that illustrate some of the expected and well-behaved results of the Area Under the ROC curve (AUC) under the multinormal assumption of the feature vector. Therefore, the message of the article is not to avoid the normal assumption of either the input feature vector or the output scoring function; however, a prudence is needed when adopting either of both.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302440",
    "keywords": [],
    "authors": [
      {
        "surname": "Yousef",
        "given_name": "Waleed A."
      }
    ]
  },
  {
    "title": "A general model to define the substitution, insertion and deletion graph edit costs based on an embedded space",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.07.010",
    "abstract": "The paper presents a method to learn the substitution, deletion and insertion costs on nodes and edges applied to the graph edit distance. We model the learning strategy as a general model and then we concretise it in two different architectures: the first architecture is based on a neural network and the second architecture is based on a multivariate normal distribution, which have been previously trained. The insertion, deletion and substitution costs on nodes and edges are defined as functions that depend on the output of the machine learning architecture. Other machine learning architectures have been presented in the literature to define the graph edit distance costs. Nevertheless, the main feature of our method is that the insertion, deletion and substitution costs are learned together, training the same machine learning and generating only one model. Thus, these costs are influenced one to another, achieving a higher accuracy in the pattern recognition stage than previous methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302567",
    "keywords": [
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Feature (linguistics)",
      "Graph",
      "Linguistics",
      "Philosophy",
      "Programming language",
      "Substitution (logic)",
      "Theoretical computer science",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Serratosa",
        "given_name": "Francesc"
      }
    ]
  },
  {
    "title": "Cross-modal retrieval via label category supervised matrix factorization hashing",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.08.007",
    "abstract": "Due to the emergence and development of big data, cross-modal hash retrieval has become progressively more important in large-scale multi-modal retrieval tasks depending on its accuracy and efficiency. It completes the retrieval task in a common low-dimensional space by finding a common semantic space for heterogeneous data of different modalities. Recently, many works have concentrated on supervised cross-modal hashing and achieved higher retrieval accuracy. However, there are still many challenges in how to maintain the local geometric structure of the original space in the public space and how to use the supervision information efficiently. To deal with such issues, this paper proposes a hash retrieval method that incorporates supervised information based on matrix factorization (LCSMFH) by maintain the inter-modal and the intra-modal similarity in the original space and make the most of the label information to improve the retrieval task effect. Through experiments on two benchmark data sets, our method is more effective and outperforms state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303056",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Data mining",
      "Eigenvalues and eigenvectors",
      "Geodesy",
      "Geography",
      "Hash function",
      "Information retrieval",
      "Machine learning",
      "Matrix decomposition",
      "Modal",
      "Pattern recognition (psychology)",
      "Physics",
      "Polymer chemistry",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Xue",
        "given_name": "Feng"
      },
      {
        "surname": "Wang",
        "given_name": "Wenbo"
      },
      {
        "surname": "Zhou",
        "given_name": "Wenjie"
      },
      {
        "surname": "Zeng",
        "given_name": "Tao"
      },
      {
        "surname": "Yang",
        "given_name": "Tian"
      }
    ]
  },
  {
    "title": "Multi-view clustering by exploring complex mapping relationship between views",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.07.031",
    "abstract": "Almost all of the existing methods assume that the samples between different views have a strict one-to-one relationship whether it is for complete multi-view data or for partial multi-view data. In this paper, we refer to the neglected many-to-many relationship between cross-view samples as the complex mapping relationship between views. To address this issue, we propose a resultful Complex Mapping Multi-View Clustering (CMMVC) method by exploring the complex mapping relationship between views. We firstly construct a complex mapping relationship matrix for each pair of views by using the nearest neighbor relationship between cross-view samples. Then the complex mapping relationship matrix is introduced into the framework of multi-view clustering based on non-negative matrix factorization to guide multi-view information fusion in order to obtain more compact representation of multi-view data space. Finally, we give the objective function of CMMVC and an effective optimization scheme. The experimental results demonstrate the advantages of the proposed CMMVC method on multi-view clustering tasks by mining the complex mapping relationship between different views.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302774",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Composite material",
      "Computer science",
      "Construct (python library)",
      "Data mining",
      "Law",
      "Materials science",
      "Matrix (chemical analysis)",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Hong"
      },
      {
        "surname": "Xiong",
        "given_name": "Jing"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaoxia"
      }
    ]
  },
  {
    "title": "A novel online action detection framework from untrimmed video streams",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107396",
    "abstract": "Online temporal action localization from an untrimmed video stream is a challenging problem in computer vision. It is challenging because of i) in an untrimmed video stream, more than one action instance may appear, including background scenes, and ii) in online settings, only past and current information is available. Therefore, temporal priors, such as the average action duration of training data, which have been exploited by previous action detection methods, are not suitable for this task because of the high intra-class variation in human actions. We propose a novel online action detection framework that considers actions as a set of temporally ordered subclasses and leverages a future frame generation network to cope with the limited information issue associated with the problem outlined above. Additionally, we augment our data by varying the lengths of videos to allow the proposed method to learn about the high intra-class variation in human actions. We evaluate our method using two benchmark datasets, THUMOS’14 and ActivityNet, for an online temporal action localization scenario and demonstrate that the performance is comparable to state-of-the-art methods that have been proposed for offline settings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301990",
    "keywords": [
      "Action (physics)",
      "Artificial intelligence",
      "Astrophysics",
      "Bayesian probability",
      "Benchmark (surveying)",
      "Class (philosophy)",
      "Computer science",
      "Data mining",
      "Economics",
      "Frame (networking)",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Management",
      "Pattern recognition (psychology)",
      "Physics",
      "Prior probability",
      "Programming language",
      "Quantum mechanics",
      "Set (abstract data type)",
      "Task (project management)",
      "Telecommunications",
      "Variation (astronomy)"
    ],
    "authors": [
      {
        "surname": "Yoon",
        "given_name": "Da-Hye"
      },
      {
        "surname": "Cho",
        "given_name": "Nam-Gyu"
      },
      {
        "surname": "Lee",
        "given_name": "Seong-Whan"
      }
    ]
  },
  {
    "title": "Adaptive ROI generation for video object segmentation using reinforcement learning",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107465",
    "abstract": "The task of the proposed method is semi-supervised video object segmentation where only the ground-truth segmentation of the first frame is provided. The existing approaches rely on selecting the region of interest for model update; however it is rough and inflexible, leading to performance degradation. To overcome this limitation, a novel approach is proposed which utilizes reinforcement learning to select optimal adaptation areas for each frame, based on the historical segmentation information. The RL model learns to take optimal actions to adjust the region of interest inferred from the previous frame for online model updating. To speed up the model adaption, a novel multi-branch tree based exploration method is designed to quickly select the best state action pairs. The proposed method is evaluated on three common video object segmentation datasets including DAVIS 2016, SegTrack V2 and Youtube-Object. The results show that the proposed work improves the state-of-the-art of the mean region similarity to 87.1% on the DAVIS 2016 dataset, and to 79.5% on the Youtube-Object dataset. Meanwhile, competitive performance is obtained on the SegTrack V2 dataset. Code is at https://github.com/insomnia94/ARG.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302685",
    "keywords": [
      "Artificial intelligence",
      "Code (set theory)",
      "Computer science",
      "Computer vision",
      "Economics",
      "Frame (networking)",
      "Ground truth",
      "Image segmentation",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Reinforcement learning",
      "Segmentation",
      "Set (abstract data type)",
      "Task (project management)",
      "Telecommunications",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Mingjie"
      },
      {
        "surname": "Xiao",
        "given_name": "Jimin"
      },
      {
        "surname": "Lim",
        "given_name": "Eng Gee"
      },
      {
        "surname": "Xie",
        "given_name": "Yanchun"
      },
      {
        "surname": "Feng",
        "given_name": "Jiashi"
      }
    ]
  },
  {
    "title": "Adversarial learning based attentional scene text recognizer",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.07.027",
    "abstract": "In this paper, we propose an adversarial learning based attentional scene text recognizer to solve the distortion problem of scene text image. We choose a rectification module which can rectify images in both horizontal and vertical directions, and use a recognizer based on the attention mechanism. Through the adversarial learning of the rectification network and the recognition network, we iteratively improve the rectification effect and the recognition performance. The entire network is trained with weak supervision, so only images and corresponding text labels are needed. Our method achieves high performance for both regular and irregular scene text images, and the experimental results tested on multiple benchmarks prove that our method achieves the performance of state-of-the-art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302737",
    "keywords": [
      "Adversarial system",
      "Amplifier",
      "Artificial intelligence",
      "Attention network",
      "Bandwidth (computing)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Distortion (music)",
      "Image (mathematics)",
      "Image rectification",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Rectification",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Jinyuan"
      },
      {
        "surname": "Wang",
        "given_name": "Yanna"
      },
      {
        "surname": "Xiao",
        "given_name": "Baihua"
      },
      {
        "surname": "Shi",
        "given_name": "Cunzhao"
      },
      {
        "surname": "Jiang",
        "given_name": "Jingzhong"
      },
      {
        "surname": "Wang",
        "given_name": "Chunheng"
      }
    ]
  },
  {
    "title": "Spatial probabilistic distribution map-based two-channel 3D U-net for visual pathway segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.09.003",
    "abstract": "Precise segmentation of the visual pathway is significant in preoperative planning to prevent the surgeon from touching it during the operation. Manual segmentation is time consuming and tedious. Thus, automatic segmentation strategies are necessary to assist clinical evaluation. However, the low contrast and blurred boundary between the target and the background in the image make automatic segmentation a challenging problem. This paper proposed a spatial probabilistic distribution map (SPDM)-based two-channel 3D U-Net to make shape and position prior information available for deep learning. First, an atlas calculated by group-wise registration was used to register each training volume image for deformation field determination. Second, the deformation field was used to transform the label of the corresponding training image to the template space, and then all the warped labels were summed up to create an SPDM. Third, the region of interest of the image and SPDM were sent to the network to predict the final segmentation. The proposed method was evaluated and compared against a conventional 3D U-Net on two datasets. Experimental results indicated that our method overcame the problem of low contrast and achieved better performance than previous methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303354",
    "keywords": [
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Image segmentation",
      "Pattern recognition (psychology)",
      "Probabilistic logic",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Ai",
        "given_name": "Danni"
      },
      {
        "surname": "Zhao",
        "given_name": "Zhiqi"
      },
      {
        "surname": "Fan",
        "given_name": "Jingfan"
      },
      {
        "surname": "Song",
        "given_name": "Hong"
      },
      {
        "surname": "Qu",
        "given_name": "Xiaoxia"
      },
      {
        "surname": "Xian",
        "given_name": "Junfang"
      },
      {
        "surname": "Yang",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "Recurrent bag-of-features for visual information analysis",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107380",
    "abstract": "Deep Learning (DL) has provided powerful tools for visual information analysis. For example, Convolutional Neural Networks (CNNs) are excelling in complex and challenging image analysis tasks by extracting meaningful feature vectors with high discriminative power. However, these powerful feature vectors are crushed through the pooling layers of the network, that usually implement the pooling operation in a less sophisticated manner. This can lead to significant information loss, especially in cases where the informative content of the data is sequentially distributed over the spatial or temporal dimension, e.g., videos, which often require extracting fine-grained temporal information. A novel stateful recurrent pooling approach, that can overcome the aforementioned limitations, is proposed in this paper. The proposed method is inspired by the well-known Bag-of-Features (BoF) model, but employs a stateful trainable recurrent quantizer, instead of plain static quantization, allowing for efficiently processing sequential data and encoding both their temporal, as well as their spatial aspects. The effectiveness of the proposed Recurrent BoF model to enclose spatio-temporal information compared to other competitive methods is demonstrated using six different datasets and two different tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301837",
    "keywords": [
      "Artificial intelligence",
      "Boltzmann machine",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Discriminative model",
      "Feature (linguistics)",
      "Linguistics",
      "Machine learning",
      "Network packet",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pooling",
      "Quantization (signal processing)",
      "Stateful firewall"
    ],
    "authors": [
      {
        "surname": "Krestenitis",
        "given_name": "Marios"
      },
      {
        "surname": "Passalis",
        "given_name": "Nikolaos"
      },
      {
        "surname": "Iosifidis",
        "given_name": "Alexandros"
      },
      {
        "surname": "Gabbouj",
        "given_name": "Moncef"
      },
      {
        "surname": "Tefas",
        "given_name": "Anastasios"
      }
    ]
  },
  {
    "title": "Semantically-guided low-light image enhancement",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.07.041",
    "abstract": "Recently, extensive research efforts have been made on low-light image enhancement. Many novel models have been proposed, such as the ones based on the Retinex theory, multiple exposure fusion, and deep neural networks. However, current models do not directly consider the semantic information in the modeling process. As a result, they tend to introduce more artifacts, such as boosted noises and unnatural visual appearances. To address this issue, we propose a fusion-based low-light enhancement model that explicitly harnesses the scene semantics into the enhancement process. In constructing the fusion map, the image regions with a specific semantic category is firstly extracted via semantic segmentation. Then, they are further combined and refined jointly with an illumination-aware map estimated from the scene illumination. Guided by the semantic information, our model is able to intentionally enhance a part of the dark regions, which therefore generates enhanced results with more natural appearances and less artifacts. In experiments, we first validate our model with some empirical studies, including parameter sensitivity and segmentation error tolerance. Then we compare our model with several state-of-the-art low-light enhancement methods, which further shows the effectiveness and advantage of our model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552030297X",
    "keywords": [
      "Artificial intelligence",
      "Color constancy",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Programming language",
      "Segmentation",
      "Semantics (computer science)"
    ],
    "authors": [
      {
        "surname": "Xie",
        "given_name": "Junyi"
      },
      {
        "surname": "Bian",
        "given_name": "Hao"
      },
      {
        "surname": "Wu",
        "given_name": "Yuanhang"
      },
      {
        "surname": "Zhao",
        "given_name": "Yu"
      },
      {
        "surname": "Shan",
        "given_name": "Linmin"
      },
      {
        "surname": "Hao",
        "given_name": "Shijie"
      }
    ]
  },
  {
    "title": "Greedy AutoAugment",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.08.024",
    "abstract": "A major problem in data augmentation is to ensure that the generated new samples cover the search space. This is a challenging problem and requires exploration for data augmentation policies to ensure their effectiveness in covering the search space. In this paper, we propose Greedy AutoAugment as a highly efficient search algorithm to find the best augmentation policies. We use a greedy approach to reduce the exponential growth of the number of possible trials to linear growth. The Greedy Search also helps us to lead the search towards the sub-policies with better results, which eventually helps to increase the accuracy. The proposed method can be used as a reliable addition to the current artifitial neural networks. Our experiments on four datasets (Tiny ImageNet, CIFAR-10, CIFAR-100, and SVHN) show that Greedy AutoAugment provides better accuracy, while using 360 times fewer computational resources.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303305",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Cover (algebra)",
      "Data mining",
      "Engineering",
      "Greedy algorithm",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Mechanical engineering",
      "Operating system",
      "Space (punctuation)"
    ],
    "authors": [
      {
        "surname": "Naghizadeh",
        "given_name": "Alireza"
      },
      {
        "surname": "Abavisani",
        "given_name": "Mohammadsajad"
      },
      {
        "surname": "Metaxas",
        "given_name": "Dimitris N."
      }
    ]
  },
  {
    "title": "Fused 3-D spectral-spatial deep neural networks and spectral clustering for hyperspectral image classification",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.08.020",
    "abstract": "Recently, classification and dimensionality reduction (DR) have become important issues of hyperspectral image (HSI) analysis. Especially, HSI classification is a challenging task due to the high-dimensional feature space, with a large number of spectral bands, and a low number of labeled samples. In this paper, we propose a new HSI classification approach, which is called fused 3-D spectral-spatial deep neural networks for hyperspectral image classification. We propose an unsupervised band selection method to avoid the problem of redundancy between spectral bands and automatically find a set of groups Ck each one containing similar spectral bands. Moreover, the model uses the different groups of selected bands to extract spectral-spatial features in order to improve the classification rate. Each group is associated with a 3-D CNN model, which are then fused to improve the precision of classification. The main advantage of the proposed method is to keep the initial spectral-spatial features by automatically selecting relevant spectral bands, which improves the classification of HSI using a low number of labeled samples. Experiments on two real HSIs, Indian Pines and Salinas datasets, are performed to demonstrate the effectiveness of the proposed method. Results show that the proposed method reaches competitive good performances, and achieves better classification rates compared to various state-of-the-art techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552030324X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Cluster analysis",
      "Computer science",
      "Contextual image classification",
      "Curse of dimensionality",
      "Dimensionality reduction",
      "Geography",
      "Hyperspectral imaging",
      "Image (mathematics)",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Redundancy (engineering)",
      "Remote sensing",
      "Spectral bands",
      "Spectral clustering"
    ],
    "authors": [
      {
        "surname": "Sellami",
        "given_name": "Akrem"
      },
      {
        "surname": "Ben Abbes",
        "given_name": "Ali"
      },
      {
        "surname": "Barra",
        "given_name": "Vincent"
      },
      {
        "surname": "Farah",
        "given_name": "Imed Riadh"
      }
    ]
  },
  {
    "title": "Graph-based boosting algorithm to learn labeled and unlabeled data",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107417",
    "abstract": "Ensemble learning is an effective technique to learn the information of data by combining multiple models. But usually the combined models are supervised learning algorithms which need a lot of labeled data to tune their parameters. Some ensemble learning algorithms were proposed to exploit the information of unlabeled data. These methods had to learn the samples with pseudo-labels due to the scarcity of labeled data. But it’s inevitable for the samples with pseudo-labels to bring wrong information during training process. In this paper, we will propose a novel graph-based boosting (GBB) algorithm to learn labeled and unlabeled data. GBB is a framework combining many models linearly. And pseudo-labels will not occur during training process. GBB will assign a new weighting vector for the labeled samples and a transformed similarity matrix for all samples to train the combined model at each iteration. We also extend GBB, termed as weighted GBB (WGBB), to learn imbalanced data by adding a weighting vector for the labeled data. Finally, 14 relatively balanced datasets and 22 imbalanced datasets are used to validate the performances of GBB and WGBB respectively. Experimental results illustrate that GBB can achieve a competitive performance and WGBB has an obvious advantage to handle classification problem of imbalanced data, comparing with other related algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030220X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Boosting (machine learning)",
      "Computer science",
      "Computer security",
      "Data mining",
      "Ensemble learning",
      "Exploit",
      "Graph",
      "Labeled data",
      "Machine learning",
      "Medicine",
      "Pattern recognition (psychology)",
      "Radiology",
      "Semi-supervised learning",
      "Theoretical computer science",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Zheng"
      },
      {
        "surname": "Jin",
        "given_name": "Wei"
      },
      {
        "surname": "Mu",
        "given_name": "Ying"
      }
    ]
  },
  {
    "title": "Semantic scene segmentation in unstructured environment with modified DeepLabV3+",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.07.029",
    "abstract": "Semantic scene segmentation has become a key application in computer vision and is an essential part of intelligent transportation systems for complete scene understanding of the surrounding environment. While several methods based on deep fully Convolutional Neural Network (CNN) have been emerging, there are two main challenges: (i) They mainly focus on improvement of the accuracy than efficiency. (ii) They assume structured driving environment like in USA and Europe. While most of the current works focus on the well structured driving environment, we focus our research on India Driving Dataset (IDD) which contains data from unstructured traffic scenario. In this paper, we propose modifications in the DeepLabV3+ framework by using lower atrous rates in Atrous Spatial Pyramid Pooling (ASPP) module for dense traffic prediction. We propose to use dilated Xception network as the backbone for feature extraction. A lightweight segmentation framework is also presented by exploring the effectiveness of MobileNetV2 architecture, which achieves competitively high accuracy and is much smaller than other state-of-art architectures. The performance is evaluated in terms of mean Intersection over Union (mIoU) on 26 fine grained classes of IDD. Our proposed model with 24 M parameters achieves 68.41 mIoU on test set and efficient mobile model achieves mIoU of 61.6 by reducing the parameters to 2.2 M only.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302750",
    "keywords": [
      "Artificial intelligence",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Feature (linguistics)",
      "Feature extraction",
      "Focus (optics)",
      "Geography",
      "Intersection (aeronautics)",
      "Linguistics",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pooling",
      "Programming language",
      "Pyramid (geometry)",
      "Segmentation",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Baheti",
        "given_name": "Bhakti"
      },
      {
        "surname": "Innani",
        "given_name": "Shubham"
      },
      {
        "surname": "Gajre",
        "given_name": "Suhas"
      },
      {
        "surname": "Talbar",
        "given_name": "Sanjay"
      }
    ]
  },
  {
    "title": "Learnable pooling weights for facial expression recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.09.001",
    "abstract": "Pooling layers are spatial down-sampling layers used in convolutional neural networks (CNN) to gradually downscale the feature map, increase the receptive field size and reduce the number of the parameters in the model. The use of pooling layers leads to less computing complexity and memory consumption reduction but also introduces invariance to certain filter distortions which may induce subtle detail loss. This behaviour is undesired for some fine-grained recognition tasks such as facial expression recognition (FER) which highly relies on specific regional distortion detection. In this paper, we introduce a more filter distortion aware pooling layer based on kernel functions. The proposed pooling reduces the feature map dimensions while keeping track of the majority of the information fed to the next layer instead of ignoring part of them. The experiments on RAF, FER2013 and ExpW databases demonstrate the benefits of such layer and show that our model achieves competitive results with respect to the state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303330",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Chemistry",
      "Combinatorics",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Distortion (music)",
      "Feature (linguistics)",
      "Filter (signal processing)",
      "Kernel (algebra)",
      "Layer (electronics)",
      "Linguistics",
      "Mathematics",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pooling",
      "Receptive field"
    ],
    "authors": [
      {
        "surname": "Mahmoudi",
        "given_name": "M. Amine"
      },
      {
        "surname": "Chetouani",
        "given_name": "Aladine"
      },
      {
        "surname": "Boufera",
        "given_name": "Fatma"
      },
      {
        "surname": "Tabia",
        "given_name": "Hedi"
      }
    ]
  },
  {
    "title": "Pedestrian attribute recognition based on multiple time steps attention",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.07.018",
    "abstract": "Pedestrian Attribute Recognition (PAR) plays an important role in intelligent video surveillance. This paper tackles two severe challenges in it i.e., complex relations between images and attributes, and imbalanced distribution of pedestrian attributes. Specifically, a new multiple time steps attention mechanism is proposed to boost the modeling of the relations. Different from existing attention approaches that only focus on the current and previous time steps, it also exploits the knowledge of next time step. By adaptively capturing the knowledge of multiple time steps, more contextual knowledge is exploited. Meanwhile, to alleviate the challenge of imbalanced distribution of pedestrian attributes, a focal balance loss function is developed by increasing the cost of those attributes difficult to recognize. The proposed framework is dubbed as MTA-Net, which is demonstrated to be effective on two benchmark datasets, i.e., PETA and RAP.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302646",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Geography",
      "Pattern recognition (psychology)",
      "Pedestrian"
    ],
    "authors": [
      {
        "surname": "Ji",
        "given_name": "Zhong"
      },
      {
        "surname": "Hu",
        "given_name": "Zhenfei"
      },
      {
        "surname": "He",
        "given_name": "Erlu"
      },
      {
        "surname": "Han",
        "given_name": "Jungong"
      },
      {
        "surname": "Pang",
        "given_name": "Yanwei"
      }
    ]
  },
  {
    "title": "Ellipse fitting by spatial averaging of random ensembles",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107406",
    "abstract": "Earlier ellipse fitting methods often consider the algebraic and geometric forms of the ellipse. The work presented here makes use of an ensemble to provide better results. The method proposes a new ellipse parametrization based on the coordinates of both foci, and the distance between them and each point of the ellipse where the Euclidean norm is applied. Besides, a certain number of subsets are uniformly drawn without replacement from the overall training set which allows estimating the center of the distribution robustly by employing the L1 median of each estimated focus. An additional postprocessing stage is proposed to filter out the effect of bad fits. In order to evaluate the performance of this method, four different error measures were considered. Results show that our proposal outperforms all its competitors, especially when higher levels of outliers are presented. Several synthetic and real data tests were developed and confirmed such finding.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302090",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Ellipse",
      "Focus (optics)",
      "Geometry",
      "Mathematics",
      "Optics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Physics"
    ],
    "authors": [
      {
        "surname": "Thurnhofer-Hemsi",
        "given_name": "Karl"
      },
      {
        "surname": "López-Rubio",
        "given_name": "Ezequiel"
      },
      {
        "surname": "Blázquez-Parra",
        "given_name": "Elidia Beatriz"
      },
      {
        "surname": "Ladrón-de-Guevara-Muñoz",
        "given_name": "M. Carmen"
      },
      {
        "surname": "de-Cózar-Macias",
        "given_name": "Óscar David"
      }
    ]
  },
  {
    "title": "Transcoding across 3D shape representations for unsupervised learning of 3D shape feature",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.07.012",
    "abstract": "Unsupervised learning of 3D shape feature is a challenging yet important problem for organizing a large collection of 3D shape models that do not have annotations. Recently proposed neural network-based approaches attempt to learn meaningful 3D shape feature by autoencoding a single 3D shape representation such as voxel, 3D point set, or multiview 2D images. However, using single shape representation isn't sufficient in training an effective 3D shape feature extractor, as none of existing shape representation can fully describe geometry of 3D shapes by itself. In this paper, we propose to use transcoding across multiple 3D shape representations as the unsupervised method to obtain expressive 3D shape feature. A neural network called Shape Auto-Transcoder (SAT) learns to extract 3D shape features via cross-prediction of multiple heterogeneous 3D shape representations. Architecture and training objective of SAT are carefully designed to obtain effective feature embedding. Experimental evaluation using 3D model retrieval and 3D model classification scenarios demonstrates high accuracy as well as compactness of the proposed 3D shape feature. The code of SAT is available at https://github.com/takahikof/ShapeAutoTranscoder.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302580",
    "keywords": [
      "Active shape model",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Embedding",
      "Feature (linguistics)",
      "Feature learning",
      "Heat kernel signature",
      "Law",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Segmentation",
      "Set (abstract data type)",
      "Shape analysis (program analysis)",
      "Static analysis",
      "Voxel"
    ],
    "authors": [
      {
        "surname": "Furuya",
        "given_name": "Takahiko"
      },
      {
        "surname": "Ohbuchi",
        "given_name": "Ryutarou"
      }
    ]
  },
  {
    "title": "Dynamics of soil surface temperature with unmanned aerial systems",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.07.003",
    "abstract": "Thermographies are a source of rich information, valuable in precision agriculture tasks such as crop stress assessment, plant disease analysis, and soil moisture evaluation. Traditionally, practitioners obtain soil temperature from the ground or using satellites and other airborne methods, which are costly and offer limited spatial and temporal resolution. In this paper, we introduce a method to measure soil surface temperature dynamics with the use of an unmanned aerial system (UAS). In our approach, we fuse information from thermal and multispectral cameras with ambient variables to generate estimates for soil temperature using computational intelligence models. Using the images, we produce a spatial reconstruction using structure from motion (SfM). After the multimodal registration of the resulting geo-referenced orthomosaics, we characterize the dynamics of the soil surface temperature using the differences between consecutively captured temperature orthomosaics. In our results, we are capable of estimating soil surface temperature from a UAS flying at 30 m AGL with a RMSE of 3.24∘ C ± 0.3 and 1.77∘ C ± 0.2, at one standard deviation, for two test fields with average ground sampling distances below 6.0 cm/pixel, using a Random Forest regressor.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552030249X",
    "keywords": [
      "Agriculture",
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Environmental science",
      "Filter (signal processing)",
      "Geography",
      "Geology",
      "Geotechnical engineering",
      "Image resolution",
      "Mathematics",
      "Mean squared error",
      "Motion (physics)",
      "Multispectral image",
      "Pixel",
      "Precision agriculture",
      "Remote sensing",
      "Sampling (signal processing)",
      "Soil science",
      "Spatial variability",
      "Standard deviation",
      "Statistics",
      "Structure from motion",
      "Water content"
    ],
    "authors": [
      {
        "surname": "Basurto-Lozada",
        "given_name": "Daniela"
      },
      {
        "surname": "Hillier",
        "given_name": "Adeline"
      },
      {
        "surname": "Medina",
        "given_name": "David"
      },
      {
        "surname": "Pulido",
        "given_name": "Dagoberto"
      },
      {
        "surname": "Karaman",
        "given_name": "Sertac"
      },
      {
        "surname": "Salas",
        "given_name": "Joaquin"
      }
    ]
  },
  {
    "title": "On the application of convex transforms to metric search",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.08.008",
    "abstract": "Scalable similarity search in metric spaces relies on using the mathematical properties of the space in order to allow efficient querying. Most important in this context is the triangle inequality property, which can allow the majority of individual similarity comparisons to be avoided for a given query. However many important metric spaces, typically those with high dimensionality, are not amenable to such techniques. In the past convex transforms have been studied as a pragmatic mechanism which can overcome this effect; however the problem with this approach is that the metric properties may be lost, leading to loss of accuracy. Here, we study the underlying properties of such transforms and their effect on metric indexing mechanisms. We show there are some spaces where certain transforms may be applied without loss of accuracy, and further spaces where we can understand the engineering tradeoffs between accuracy and efficiency. We back these observations with experimental analysis. To highlight the value of the approach, we show three large spaces deriving from practical domains whose dimensionality prevents normal indexing techniques, but where the transforms applied give scalable access with a relatively small loss of accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303068",
    "keywords": [
      "Algorithm",
      "Combinatorics",
      "Computer science",
      "Economics",
      "Geometry",
      "Mathematical optimization",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Regular polygon"
    ],
    "authors": [
      {
        "surname": "Connor",
        "given_name": "Richard"
      },
      {
        "surname": "Dearle",
        "given_name": "Alan"
      },
      {
        "surname": "Mic",
        "given_name": "Vladimir"
      },
      {
        "surname": "Zezula",
        "given_name": "Pavel"
      }
    ]
  },
  {
    "title": "Community detection in complex network based on APT method",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.07.021",
    "abstract": "Community detection is a significant methodology in network science. Traditional methods show limitations in dealing with multi-scale and high-dimensional complex data. As one of the most popular unsupervised algorithms, affinity propagation algorithm (AP) has been widely applied in community detection. However, its negative Euclidean similarity and inflexible parameter may lead to high time or memory consumption and excessive detection. Thus, this article presents a novel affinity propagation algorithm in t-distribution (APT), integrated with manifold learning, for detecting community structure. In APT algorithm, the data is compressed by dimensionality reduction, and joint probability is applied to construct the similarity matrix. Further, based on optimized modularity, parameters are adjusted to improve the accuracy. Experiments show that APT has better adaptability and universality than AP algorithm. In contrast to other mainstream algorithms, our algorithm can extract more meaningful communities from multi-scale and high-dimensional networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302671",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Complex network",
      "Computer science",
      "Data mining",
      "Dimensionality reduction",
      "Image (mathematics)",
      "Machine learning",
      "Nonlinear dimensionality reduction",
      "Pattern recognition (psychology)",
      "Similarity (geometry)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Qingfeng"
      },
      {
        "surname": "Qiao",
        "given_name": "YuLu"
      },
      {
        "surname": "Hu",
        "given_name": "Fang"
      },
      {
        "surname": "Li",
        "given_name": "Yongjie"
      },
      {
        "surname": "Tan",
        "given_name": "Kai"
      },
      {
        "surname": "Zhu",
        "given_name": "Mingrui"
      },
      {
        "surname": "Zhang",
        "given_name": "Chengqi"
      }
    ]
  },
  {
    "title": "EBIT: Weakly-supervised image translation with edge and boundary enhancement",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.08.025",
    "abstract": "In this paper, targeting image translation between the thermal and visible domains, we propose a novel framework to enhance the edge and boundary features during translation. We tackle the unsupervised training task where sample image pairs from two domains are randomly chosen so the image content does not match, but we also take advantage of paired feature maps during the feature disentanglement process. This can be considered to be weakly-supervisedtraining. First, since thermal images usually have vague edges, we propose to apply a Canny operator to strengthen the edge features of the thermal images. Next, to define the correct object boundaries, we extract the boundary features from silhouette masks which are paired with the visible domain images. Then we disentangle the features of both domains into a domain-shared latent space and a domain-exclusive latent space. In the domain-shared latent space, the edge and boundary features extracted from the thermal and visible domains respectively act as domain-shared information which is used to render the edges and define the boundaries of the translated images. In the domain-exclusive latent space, domain-exclusive information such as colour is used to render the colour of objects of the translated images. In addition, we propose a pixel-wise adversarial loss rather than more traditional ones. The experimental results show that the proposed method has the ability to render realistic edge and colour features within the correct object boundaries and outperforms several state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303263",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Boundary (topology)",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Domain (mathematical analysis)",
      "Edge detection",
      "Enhanced Data Rates for GSM Evolution",
      "Feature (linguistics)",
      "Gene",
      "Image (mathematics)",
      "Image processing",
      "Image translation",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Messenger RNA",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Translation (biology)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Tianren"
      },
      {
        "surname": "Zhang",
        "given_name": "Teng"
      },
      {
        "surname": "Lovell",
        "given_name": "Brian C."
      }
    ]
  },
  {
    "title": "Memetic algorithm for multivariate time-series segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.06.022",
    "abstract": "In recent years, analyzing time-series data has become an ever important research topic due to an increased number of temporal datasets in science and engineering. Segmentation is widely used in time-series data analysis because it provides a more compact representation by dividing the series into segments. Segmentation approaches based on genetic algorithms have been proposed to extract segments and patterns with a given objective, such as a low rate of change or periodicity from time-series data. However, they may not be effective in obtaining the precise solution because they perform global search. In this study, we propose a memetic algorithm for multivariate time-series segmentation. For efficient local refinement, we calculate a likelihood-based score for all time points and use it in the evolutionary process. Experiments demonstrate that the proposed method is superior to conventional segmentation methods. The source code of the proposed method can be downloaded from https://github.com/hlim-kist/ma_mts",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302403",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Data mining",
      "Law",
      "Local search (optimization)",
      "Machine learning",
      "Memetic algorithm",
      "Multivariate statistics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Segmentation",
      "Series (stratigraphy)",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Lim",
        "given_name": "Hyunki"
      },
      {
        "surname": "Choi",
        "given_name": "Heeseung"
      },
      {
        "surname": "Choi",
        "given_name": "Yeji"
      },
      {
        "surname": "Kim",
        "given_name": "Ig-Jae"
      }
    ]
  },
  {
    "title": "Structure alignment of attributes and visual features for cross-dataset person re-identification",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107414",
    "abstract": "In cross-dataset person re-identification, it is challenging to address the problem of domain shift between training and test data. Although unsupervised domain adaptation methods have been developed, the performance is still much weaker compared with that of supervised methods because these models cannot follow a supervised optimization in unlabeled target domains. To address this problem, a transductive structure alignment-based self-reconstruction dictionary learning approach is proposed in this paper for cross-dataset person re-identification (PRID). Specifically, visual-attribute embedding is first learned to achieve knowledge transfer from the source domain to the target domain. In this process, visual-attribute structures are aligned via class prototype dictionaries to promote the discrimination of predicted semantic attributes by exploiting structure information between the visual feature and class prototype. Moreover, to mitigate domain shift, domain-invariant visual-attribute self-reconstruction is integrated into our dictionary learning framework. An identifier is then constructed by integrating the discriminativeness of attribute and compatibility matrix shared both source domain and target domain. Finally, the pre-learned model is tuned by selecting samples from the target domain which are not labeled but assigned pseudo-labels. Extensive experimental results on benchmark datasets show that our approach outperforms several state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030217X",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Botany",
      "Computer science",
      "Domain (mathematical analysis)",
      "Embedding",
      "Geodesy",
      "Geography",
      "Identification (biology)",
      "Identifier",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Huafeng"
      },
      {
        "surname": "Kuang",
        "given_name": "Zhenyu"
      },
      {
        "surname": "Yu",
        "given_name": "Zhengtao"
      },
      {
        "surname": "Luo",
        "given_name": "Jiebo"
      }
    ]
  },
  {
    "title": "Depth occlusion perception feature analysis for person re-identification",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.09.009",
    "abstract": "Person re-identification (ReID) has achieved significant improvement under the setting of matching two holistic person images. However, persons are easily occluded by the various objects and other persons in real-world scenarios, making Person ReID a challenging task. In this paper, we propose a novel method named Pose-Driven Visibility Model (PDVM) to effectively solve the degradation of recognition performance caused by occlusion. Firstly, we extract non-occluded human body features through pose estimation, pay attention to the salient features of non-human parts through self-attention mechanism, and obtains the final feature representation after the combination. Secondly, we more accurately locate person body parts by utilizing the detected human keypoints in different occlusion situations, effectively reducing the impact of unalignment and realizing better matching for persons. We implement extensive experiments on Occluded-DukeMTMC and Partial-REID. Our proposed method achieves state of the art performances which reaches 53.0% Rank-1 accuracy on Occluded-DukeMTMC dataset and ablation analysis also verify the effectiveness of our method. 2020 Elsevier Ltd. All rights reserved",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303500",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Cardiology",
      "Computer science",
      "Computer vision",
      "Economics",
      "Feature (linguistics)",
      "Identification (biology)",
      "Law",
      "Linguistics",
      "Management",
      "Matching (statistics)",
      "Mathematics",
      "Medicine",
      "Neuroscience",
      "Occlusion",
      "Optics",
      "Pattern recognition (psychology)",
      "Perception",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Salient",
      "Statistics",
      "Task (project management)",
      "Visibility"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Shuren"
      },
      {
        "surname": "Wu",
        "given_name": "Jie"
      },
      {
        "surname": "Zhang",
        "given_name": "Fan"
      },
      {
        "surname": "Sehdev",
        "given_name": "Paramjit"
      }
    ]
  },
  {
    "title": "Every node counts: Self-ensembling graph convolutional networks for semi-supervised learning",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107451",
    "abstract": "Graph convolutional network (GCN) provides a powerful means for graph-based semi-supervised tasks. However, as a localized first-order approximation of spectral graph convolution, the classic GCN can not take full advantage of unlabeled data, especially when the unlabeled node is far from labeled ones. To capitalize on the information from unlabeled nodes to boost the training for GCN, we propose a novel framework named Self-Ensembling GCN (SEGCN), which marries GCN with Mean Teacher – a powerful self-ensemble learning mechanism for semi-supervised task. SEGCN contains a student model and a teacher model. As a student, it not only learns to correctly classify the labeled nodes, but also tries to be consistent with the teacher on unlabeled nodes in more challenging situations, such as a high dropout rate and graph corrosion. As a teacher, it averages the student model weights and generates more accurate predictions to lead the student. In such a mutual-promoting process, both labeled and unlabeled samples can be fully utilized for backpropagating effective gradients to train GCN. In a variety of semi-supervised classification benchmarks, i.e. Citeseer, Cora, Pubmed and NELL, we validate that the proposed method matches the state of the arts in the classification accuracy. The code is publicly available at https://github.com/RoyalVane/SEGCN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302545",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Engineering",
      "Graph",
      "Labeled data",
      "Machine learning",
      "Node (physics)",
      "Pattern recognition (psychology)",
      "Semi-supervised learning",
      "Structural engineering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Yawei"
      },
      {
        "surname": "Ji",
        "given_name": "Rongrong"
      },
      {
        "surname": "Guan",
        "given_name": "Tao"
      },
      {
        "surname": "Yu",
        "given_name": "Junqing"
      },
      {
        "surname": "Liu",
        "given_name": "Ping"
      },
      {
        "surname": "Yang",
        "given_name": "Yi"
      }
    ]
  },
  {
    "title": "Security in smart cities: A brief review of digital forensic schemes for biometric data",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.07.009",
    "abstract": "A smart city is engineered to be a self-sustained ecosystem driven by Internet-of-Things (IoT) devices. Smooth functioning of smart cities is conditioned on seamless communication between users and devices. Smart devices equipped with biometric authentication can offer security as well as personalized experience to the end users. Currently, a number of smart devices employ face, fingerprint, and voice modalities for user verification. However, the biometric data acquired by these devices can be digitally manipulated or tampered with, that can compromise the security of the smart environment. Further, the preponderance of biometric data such as face and voice in social media applications, necessitates the validation of their integrity. In this work, we review state-of-the-art digital forensic schemes for audio-visual biometric data that can be leveraged by applications designed for smart cities.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302555",
    "keywords": [
      "Artificial intelligence",
      "Authentication (law)",
      "Biometrics",
      "Computer science",
      "Computer security",
      "Digital forensics",
      "Facial recognition system",
      "Feature extraction",
      "Fingerprint (computing)",
      "Fingerprint recognition",
      "Identity theft",
      "Internet of Things",
      "Internet privacy",
      "Minutiae",
      "Modalities",
      "Smart card",
      "Smart environment",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Ross",
        "given_name": "Arun"
      },
      {
        "surname": "Banerjee",
        "given_name": "Sudipta"
      },
      {
        "surname": "Chowdhury",
        "given_name": "Anurag"
      }
    ]
  },
  {
    "title": "User personality prediction based on topic preference and sentiment analysis using LSTM model",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.07.035",
    "abstract": "Based on the original text information, this paper converts the users' theme preferences and text sentiment features into attention information and combines different forms with the LSTM (Long Short-Term Memory) model to predict the personality characteristics of social network users. Finally, the experimental results of multiple groups’ show that the Attention-based LSTM model proposed in the paper can achieve better results than the currently popular methods in the recognition of user personality traits and that the model has good generalization ability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302919",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Big Five personality traits",
      "Computer science",
      "Generalization",
      "Long short term memory",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Personality",
      "Preference",
      "Psychology",
      "Recurrent neural network",
      "Sentiment analysis",
      "Social psychology",
      "Statistics",
      "Theme (computing)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Jinghua"
      },
      {
        "surname": "Zeng",
        "given_name": "Dalin"
      },
      {
        "surname": "Xiao",
        "given_name": "Yujie"
      },
      {
        "surname": "Che",
        "given_name": "Liping"
      },
      {
        "surname": "Wang",
        "given_name": "Mengjiao"
      }
    ]
  },
  {
    "title": "Probabilistic SVM classifier ensemble selection based on GMDH-type neural network",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107373",
    "abstract": "Support vector machine (SVM) provides a good classification and regression ability, especially, for small sample learning. However, in practice, the learning ability of implemented SVM is occasionally far from the expected level. Group method of data handling neural network (GMDH-NN) has been applied in various fields for pattern recognition and data mining. It makes it possible to automatically find interrelations in data, to select an optimal structure of network or model and to improve the accuracy of existing algorithms. In this work we propose to take the advantages of GMDH-NN for further increasing the classification performance of SVM. One weakness of the symmetric regularity criterion of GMDH-NN is that if one of the input attributes has a relatively big range, then it may overcome the other attributes. Thus, we first define a standardized symmetric regularity criterion (SSRC) to evaluate and select the candidate models, and optimize a classifier ensemble selection approach. Secondly, we define a novel structure of initial model of GMDH-NN which is from the posterior probability outputs of SVMs. These probabilistic outputs are generated from the improved Platt’s probabilistic outputs. Thirdly, in real classification tasks, different classifiers usually have different classification advantages. So we use probabilistic SVM as base learner and integrate the probabilistic SVMs with GMDH-NN, and then propose a special classifier ensemble selection approach for probabilistic SVM classifiers based on GMDH-NN called GMDH-PSVM. Moreover, we use the Borda sorting and Random weighted Borda sorting to discuss the results of our experiments. Experiments on standard UCI datasets demonstrate the effectiveness of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030176X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Group method of data handling",
      "Machine learning",
      "Naive Bayes classifier",
      "Pattern recognition (psychology)",
      "Probabilistic classification",
      "Probabilistic logic",
      "Probabilistic neural network",
      "Random subspace method",
      "Support vector machine",
      "Time delay neural network"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Lixiang"
      },
      {
        "surname": "Wang",
        "given_name": "Xiaofeng"
      },
      {
        "surname": "Bai",
        "given_name": "Lu"
      },
      {
        "surname": "Xiao",
        "given_name": "Jin"
      },
      {
        "surname": "Liu",
        "given_name": "Qi"
      },
      {
        "surname": "Chen",
        "given_name": "Enhong"
      },
      {
        "surname": "Jiang",
        "given_name": "Xiaoyi"
      },
      {
        "surname": "Luo",
        "given_name": "Bin"
      }
    ]
  },
  {
    "title": "SMPLR: Deep learning based SMPL reverse for 3D human pose and shape recovery",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107472",
    "abstract": "In this paper we propose to embed SMPL within a deep-based model to accurately estimate 3D pose and shape from a still RGB image. We use CNN-based 3D joint predictions as an intermediate representation to regress SMPL pose and shape parameters. Later, 3D joints are reconstructed again in the SMPL output. This module can be seen as an autoencoder where the encoder is a deep neural network and the decoder is SMPL model. We refer to this as SMPL reverse (SMPLR). By implementing SMPLR as an encoder-decoder we avoid the need of complex constraints on pose and shape. Furthermore, given that in-the-wild datasets usually lack accurate 3D annotations, it is desirable to lift 2D joints to 3D without pairing 3D annotations with RGB images. Therefore, we also propose a denoising autoencoder (DAE) module between CNN and SMPLR, able to lift 2D joints to 3D and partially recover from structured error. We evaluate our method on SURREAL and Human3.6M datasets, showing improvement over SMPL-based state-of-the-art alternatives by about 4 and 12 mm, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302752",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Encoder",
      "Lift (data mining)",
      "Machine learning",
      "Operating system"
    ],
    "authors": [
      {
        "surname": "Madadi",
        "given_name": "Meysam"
      },
      {
        "surname": "Bertiche",
        "given_name": "Hugo"
      },
      {
        "surname": "Escalera",
        "given_name": "Sergio"
      }
    ]
  },
  {
    "title": "CAN-GAN: Conditioned-attention normalized GAN for face age synthesis",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.08.021",
    "abstract": "This work aims to freely translate an input face to an aging face with robust identity preservation, satisfying aging effect and authentic visual appearance. Witnessing the success of GAN in image synthesis, researchers employ GAN to address the problem of face aging synthesis. However, most GAN-based methods hold that the aging changing of all facial regions is equal, which ignores the fact that different facial regions have distinct aging speeds and aging patterns. To this end, we propose a novel Conditioned-Attention Normalization GAN (CAN-GAN) for age synthesis by leveraging the aging difference between two age groups to capture facial aging regions with different attention factors. In particular, a new Conditioned-Attention Normalization (CAN) layer is designed to enhance the aging-relevant information of face, while smoothing the aging-irrelevant information of face by attention map. Since different facial attributes contribute to the discrimination of age groups with divers degrees, we further present a Contribution-Aware Age Classifier (CAAC) that finely measures the importance of face vector’s elements in terms of the age classification. Qualitative and quantitative experiments on several commonly-used datasets show the advance of CAN-GAN compared with the other competitive methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303251",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Classifier (UML)",
      "Cognitive psychology",
      "Computer science",
      "Computer vision",
      "Face (sociological concept)",
      "Normalization (sociology)",
      "Pattern recognition (psychology)",
      "Psychology",
      "Smoothing",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Chenglong"
      },
      {
        "surname": "Zhang",
        "given_name": "Jiachao"
      },
      {
        "surname": "Yao",
        "given_name": "Yazhou"
      },
      {
        "surname": "Sun",
        "given_name": "Yunlian"
      },
      {
        "surname": "Rao",
        "given_name": "Huaming"
      },
      {
        "surname": "Shu",
        "given_name": "Xiangbo"
      }
    ]
  },
  {
    "title": "Weak supervision for generating pixel–level annotations in scene text segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.06.023",
    "abstract": "Providing pixel–level supervisions for scene text segmentation is inherently difficult and costly, so that only few small datasets are available for this task. To face the scarcity of training data, previous approaches based on Convolutional Neural Networks (CNNs) rely on the use of a synthetic dataset for pre–training. However, synthetic data cannot reproduce the complexity and variability of natural images. In this work, we propose to use a weakly supervised learning approach to reduce the domain–shift between synthetic and real data. Leveraging the bounding–box supervision of the COCO–Text and the MLT datasets, we generate weak pixel–level supervisions of real images. In particular, the COCO–Text–Segmentation (COCO_TS) and the MLT–Segmentation (MLT_S) datasets are created and released. These two datasets are used to train a CNN, the Segmentation Multiscale Attention Network (SMANet), which is specifically designed to face some peculiarities of the scene text segmentation task. The SMANet is trained end–to–end on the proposed datasets, and the experiments show that COCO_TS and MLT_S are a valid alternative to synthetic images, allowing to use only a fraction of the training samples, with a significant improvement in performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302415",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Dice",
      "Economics",
      "Face (sociological concept)",
      "Geometry",
      "Image (mathematics)",
      "Machine learning",
      "Management",
      "Mathematics",
      "Minimum bounding box",
      "Pattern recognition (psychology)",
      "Pixel",
      "Segmentation",
      "Social science",
      "Sociology",
      "Synthetic data",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Bonechi",
        "given_name": "Simone"
      },
      {
        "surname": "Bianchini",
        "given_name": "Monica"
      },
      {
        "surname": "Scarselli",
        "given_name": "Franco"
      },
      {
        "surname": "Andreini",
        "given_name": "Paolo"
      }
    ]
  },
  {
    "title": "LieToMe: Preliminary study on hand gestures for deception detection via Fisher-LSTM",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.08.014",
    "abstract": "The ability to discern lies, more broadly known as deception detection, is an invaluable skill that can strongly influence the outcome of relevant situations such as court trials and police interrogatories. Several devices currently exist and are being used (e.g., magnetic resonance and polygraphs) to ease those tasks; although, due to the subject awareness of such tools, their effectiveness can be compromised by the person intentional behavioural changes. Thus, alternative ways to discriminate lies without using physical devices, could become critical assets for the aforementioned situations, especially in ever improving smart cities environments. In this letter, we present an unorthodox deception detection approach, based on hand gestures found in RGB videos of famous trials. The proposed system first extrapolates hands skeletons from the RGB sequences, then computes meaningful features which are summarized into Fisher Vectors (FVs), and finally feeds this representation to a Long-Short Term Memory (LSTM) network, defined Fisher-LSTM, to try and discern if a lie is being told. In the experimental results, we show how the FV representation can help a LSTM network grasp hand gestures characteristics that could otherwise be missed. What is more, the devised Fisher-LSTM, due to its real-time computation, can be employed in smart environments as an alternative lie detector in situations requiring an immediate response, such as the aforementioned law enforcement examples.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303123",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Deception",
      "GRASP",
      "Gesture",
      "Human–computer interaction",
      "Law",
      "Lie detection",
      "Machine learning",
      "Political science",
      "Politics",
      "Programming language",
      "Psychology",
      "Representation (politics)",
      "Social psychology",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Avola",
        "given_name": "Danilo"
      },
      {
        "surname": "Cinque",
        "given_name": "Luigi"
      },
      {
        "surname": "De Marsico",
        "given_name": "Maria"
      },
      {
        "surname": "Fagioli",
        "given_name": "Alessio"
      },
      {
        "surname": "Foresti",
        "given_name": "Gian Luca"
      }
    ]
  },
  {
    "title": "Structure-aware human pose estimation with graph convolutional networks",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107410",
    "abstract": "Human pose estimation is the task of localizing body key points from still images. As body key points are inter-connected, it is desirable to model the structural relationships between body key points to further improve the localization performance. In this paper, based on original graph convolutional networks, we propose a novel model, termed Pose Graph Convolutional Network (PGCN), to exploit these important relationships for pose estimation. Specifically, our model builds a directed graph between body key points according to the natural compositional model of a human body. Each node (key point) is represented by a 3-D tensor consisting of multiple feature maps, initially generated by our backbone network, to retain accurate spatial information. Furthermore, attention mechanism is presented to focus on crucial edges (structured information) between key points. PGCN is then learned to map the graph into a set of structure-aware key point representations which encode both structure of human body and appearance information of specific key points. Additionally, we propose two modules for PGCN, i.e., the Local PGCN (L-PGCN) module and Non-Local PGCN (NL-PGCN) module. The former utilizes spatial attention to capture the correlations between the local areas of adjacent key points to refine the location of key points. While the latter captures long-range relationships via non-local operation to associate the challenging key points. By equipping with these two modules, our PGCN can further improve localization performance. Experiments both on single- and multi-person estimation benchmark datasets show that our method consistently outperforms competing state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302132",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Data mining",
      "ENCODE",
      "Exploit",
      "Gene",
      "Geodesy",
      "Geography",
      "Graph",
      "Key (lock)",
      "Pattern recognition (psychology)",
      "Pose",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Bin",
        "given_name": "Yanrui"
      },
      {
        "surname": "Chen",
        "given_name": "Zhao-Min"
      },
      {
        "surname": "Wei",
        "given_name": "Xiu-Shen"
      },
      {
        "surname": "Chen",
        "given_name": "Xinya"
      },
      {
        "surname": "Gao",
        "given_name": "Changxin"
      },
      {
        "surname": "Sang",
        "given_name": "Nong"
      }
    ]
  },
  {
    "title": "Orthogonal neighborhood preserving discriminant analysis with patch embedding for face recognition",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107450",
    "abstract": "Intuitively, all facial images of a person are located on or near a manifold in the high-dimensional image space, and the process of face recognition can be regarded as the recovery process of multiple low-dimensional manifolds. To preserve the manifold structure information of intra-class samples after dimensionality reduction, we proposed a patch-based multi-manifold orthogonal neighborhood-preserving discriminant analysis algorithm, namely ONPDA. From the perspective of path alignment, we consider the intra-class compactness, intra-class structure and inter-class separability simultaneously. Moreover, we infuse intra-class structure information described by the sample reconstruction into intra-class compactness loss, considering the compactness of two reconstruction groups instead of sample pairs in the same class. By analyzing the relationship between the projection direction and the maximum inter-class margin, we select the samples that should participate in the inter-class separability on the patch. Meanwhile, a fast orthogonalization method is performed to obtain the orthogonal projection matrix. Besides, we perform ONPDA in reproducing kernel Hilbert space which gives rise to nonlinear maps, resulting in the kernel ONPDA (KONPDA). Experimental results compared with some state-of-the-art methods on a toy dataset and several benchmark face image databases demonstrate the effectiveness of ONPDA and KONPDA.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302533",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Compact space",
      "Computer science",
      "Dimensionality reduction",
      "Embedding",
      "Engineering",
      "Facial recognition system",
      "Kernel (algebra)",
      "Linear discriminant analysis",
      "Manifold (fluid mechanics)",
      "Mathematics",
      "Mechanical engineering",
      "Nonlinear dimensionality reduction",
      "Orthogonalization",
      "Orthographic projection",
      "Pattern recognition (psychology)",
      "Projection (relational algebra)",
      "Pure mathematics",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Liangchen"
      },
      {
        "surname": "Zhang",
        "given_name": "Wensheng"
      }
    ]
  },
  {
    "title": "Data-augmented matched subspace detector for hyperspectral subpixel target detection",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107464",
    "abstract": "The performance of subspace-based methods such as matched subspace detector (MSD) and MSD with interaction effects (MSDinter) heavily depends on the background subspace and the target subspace. Nonetheless, constructing a representative target subspace is challenging due to the limited availability of target spectra in a collected hyperspectral image. In this paper, we propose two new hyperspectral target detection methods termed data-augmented MSD (DAMSD) and data-augmented MSDinter (DAMSDI) that can effectively solve the scarcity problem of target spectra and from which a representative target-background mixed subspace can be learned. We first synthesise target-background mixed spectra based on classical hyperspectral mixing models and then learn a target-background mixed subspace via principal component analysis. Compared with MSD and MSDinter, the learned mixed subspace is more representative as spectral variability of target spectra is explained to the largest extent and it leads to an improvement in computational speed and numerical stability. We demonstrate the efficacy of DAMSD and DAMSDI for subpixel target detection on two public hyperspectral image datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302673",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Detector",
      "Full spectral imaging",
      "Hyperspectral imaging",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pixel",
      "Principal component analysis",
      "Subpixel rendering",
      "Subspace topology",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Xiaochen"
      },
      {
        "surname": "Dong",
        "given_name": "Mingzhi"
      },
      {
        "surname": "Wang",
        "given_name": "Ziyu"
      },
      {
        "surname": "Gao",
        "given_name": "Lianru"
      },
      {
        "surname": "Zhang",
        "given_name": "Lefei"
      },
      {
        "surname": "Xue",
        "given_name": "Jing-Hao"
      }
    ]
  },
  {
    "title": "Pattern recognition and beyond: Alfredo Petrosino’s scientific results",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.07.032",
    "abstract": "We summarize the main scientific contributions of our friend and colleague Alfredo Petrosino, full professor in computer science at the University of Naples Parthenope, Italy. They mainly cover topics in high-performance computing, neural network models, soft and granular computing, computer vision, and machine learning. We also highlight how most of his research activity lays the foundation for biometry and its applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302865",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Artificial neural network",
      "Cognitive science",
      "Computer science",
      "Cover (algebra)",
      "Deep learning",
      "Engineering",
      "Foundation (evidence)",
      "History",
      "Mechanical engineering",
      "Psychology",
      "Soft computing"
    ],
    "authors": [
      {
        "surname": "Maddalena",
        "given_name": "Lucia"
      },
      {
        "surname": "Gori",
        "given_name": "Marco"
      },
      {
        "surname": "Pal",
        "given_name": "Sankar K."
      }
    ]
  },
  {
    "title": "On minimum spanning tree streaming for hierarchical segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.07.006",
    "abstract": "The minimum spanning tree (MST) is one the most popular data structure used to extract hierarchical information from images. This work addresses MST construction in streaming for images. First, we focus on the problem of computing a MST of the union of two graphs with a non-empty intersection. Then we show how our solution can be applied to streaming images. The proposed solution relies on the decomposition of the data in two parts. One stable that does not change in the future. This can be stocked or used for further treatments. The other unstable needs further information before becoming stable. The correctness of proposed algorithm has been proven and confirmed in the case of morphological segmentation of remote sensing images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552030252X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Binary tree",
      "Biology",
      "Cartography",
      "Combinatorics",
      "Computer science",
      "Correctness",
      "Decomposition",
      "Ecology",
      "Focus (optics)",
      "Geography",
      "Image segmentation",
      "Intersection (aeronautics)",
      "Mathematics",
      "Minimum spanning tree",
      "Optics",
      "Physics",
      "Segmentation",
      "Spanning tree",
      "Tree (set theory)",
      "Tree structure"
    ],
    "authors": [
      {
        "surname": "Gigli",
        "given_name": "Leonardo"
      },
      {
        "surname": "Velasco-Forero",
        "given_name": "Santiago"
      },
      {
        "surname": "Marcotegui",
        "given_name": "Beatriz"
      }
    ]
  },
  {
    "title": "Special issue on deep learning for video text analysis",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.07.036",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302920",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Block (permutation group theory)",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Evolutionary biology",
      "Expectation–maximization algorithm",
      "Function (biology)",
      "Hidden Markov model",
      "Latent variable",
      "Machine learning",
      "Markov chain",
      "Markov process",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Maximum likelihood",
      "Statistics",
      "Stochastic block model"
    ],
    "authors": [
      {
        "surname": "Basu",
        "given_name": "Subhadip"
      },
      {
        "surname": "Maulik",
        "given_name": "Ujjwal"
      },
      {
        "surname": "Pal",
        "given_name": "Umapada"
      }
    ]
  },
  {
    "title": "Distributed discrete-time event-triggered algorithm for economic dispatch problem",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.08.016",
    "abstract": "In this paper, a distributed discrete-time event-triggered algorithm is proposed to deal with the economic dispatch problem with equality constraint. The communication can be reduced and the energy of systems can be saved by adopting event-triggered communication mechanism. The Zeno behavior is naturally excluded based on discrete iteration scheme. Moreover, the proposed algorithm has been proved to be exponentially convergent with the aid of convex optimization and Lyapunov stability theory that the optimal value is obtained with discrete exponential convergence rate. Finally, the effectiveness of the proposed algorithm is verified via a numerical example.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303147",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Constraint (computer-aided design)",
      "Convergence (economics)",
      "Convex optimization",
      "Discrete time and continuous time",
      "Economic dispatch",
      "Economic growth",
      "Economics",
      "Electric power system",
      "Geometry",
      "Lyapunov function",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Nonlinear system",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Regular polygon",
      "Scheme (mathematics)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Jin",
        "given_name": "Renyun"
      },
      {
        "surname": "Qiu",
        "given_name": "Haifeng"
      },
      {
        "surname": "Weng",
        "given_name": "Liguo"
      }
    ]
  },
  {
    "title": "Multi-view subspace clustering via simultaneously learning the representation tensor and affinity matrix",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107441",
    "abstract": "Multi-view subspace clustering aims at separating data points into multiple underlying subspaces according to their multi-view features. Existing low-rank tensor representation-based multi-view subspace clustering algorithms are robust to noise and can preserve the high-order correlations of multi-view features. However, they may suffer from two common problems: (1) the local structures and different importance of each view feature are often neglected; (2) the low-rank representation tensor and affinity matrix are learned separately. To address these issues, we propose a unified framework to learn the Graph regularized Low-rank representation Tensor and Affinity matrix (GLTA) for multi-view subspace clustering. In the proposed GLTA framework, the tensor singular value decomposition-based tensor nuclear norm is adopted to explore the high-order cross-view correlations. The manifold regularization is exploited to preserve the local structures embedded in high-dimensional space. The importance of different features is automatically measured when constructing the final affinity matrix. An iterative algorithm is developed to solve GLTA using the alternating direction method of multipliers. Extensive experiments on seven challenging datasets demonstrate the superiority of GLTA over the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302442",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Eigenvalues and eigenvectors",
      "Law",
      "Linear subspace",
      "Mathematics",
      "Matrix norm",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Pure mathematics",
      "Quantum mechanics",
      "Rank (graph theory)",
      "Representation (politics)",
      "Singular value decomposition",
      "Spectral clustering",
      "Subspace topology",
      "Tensor (intrinsic definition)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yongyong"
      },
      {
        "surname": "Xiao",
        "given_name": "Xiaolin"
      },
      {
        "surname": "Zhou",
        "given_name": "Yicong"
      }
    ]
  },
  {
    "title": "DAFNE: A dataset of fresco fragments for digital anastlylosis",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.09.015",
    "abstract": "Restoring artworks seriously damaged or completely destroyed is a challenging task. In particular, the reconstruction of frescoes has to deal with problems such as very small fragments, irregular shapes and missing pieces. Several attempts have been done to develop new techniques for helping restorers in the matching process, starting from traditional image processing methods to the more recent deep learning approaches. However, as often happens in the Cultural Heritage field, the availability of labeled data to test new strategies is limited, and publicly available datasets contain only few samples. For this reason, in this paper we introduce DAFNE, a large dataset that includes hundreds of thousands of images of fresco fragments artificially generated to guarantee a high variability in terms of shapes and dimensions. Fragments have been obtained starting from 62 images of famous frescoes of various artists and historical periods, in order to consider different artistic styles, subjects and colors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303408",
    "keywords": [
      "Archaeology",
      "Art",
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Cultural heritage",
      "Engineering",
      "Fresco",
      "Geography",
      "Matching (statistics)",
      "Mathematics",
      "Painting",
      "Pattern recognition (psychology)",
      "Statistics",
      "Systems engineering",
      "Task (project management)",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Dondi",
        "given_name": "Piercarlo"
      },
      {
        "surname": "Lombardi",
        "given_name": "Luca"
      },
      {
        "surname": "Setti",
        "given_name": "Alessandra"
      }
    ]
  },
  {
    "title": "An exact algorithm for time-dependent variational inference for the dynamic stochastic block model",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.07.014",
    "abstract": "An exact algorithm for estimating the dynamic stochastic block model is proposed. This model assumes a hidden Markov chain for the evolution of the social behavior of a group of individuals at repeated time occasions and may be used to assign these individuals to the latent blocks in a dynamic fashion. For the estimation of this model, the proposed exact algorithm maximizes the target function introduced by Matias and Miele [7]. This function is derived from a variational approximation of the model log-likelihood, based on the assumption that the latent variables identifying the blocks are a posteriori independent across individuals, but not across time occasions. A simulation study is performed to compare the exact algorithm with the approximate maximization algorithm proposed by Matias and Miele [7]. Results show that there is a certain advantage of the first in terms of dynamic assignment of individuals to the latent blocks in comparison to the true blocking structure, as measured by the adjusted Rand index.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302609",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Block (permutation group theory)",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Evolutionary biology",
      "Expectation–maximization algorithm",
      "Function (biology)",
      "Inference",
      "Markov chain",
      "Mathematical optimization",
      "Mathematics",
      "Maximum likelihood",
      "Statistics",
      "Stochastic block model"
    ],
    "authors": [
      {
        "surname": "Bartolucci",
        "given_name": "Francesco"
      },
      {
        "surname": "Pandolfi",
        "given_name": "Silvia"
      }
    ]
  },
  {
    "title": "Robust and precise isotropic scaling registration algorithm using bi-directional distance and correntropy",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.07.026",
    "abstract": "In orthodontics, a patient is collected a lot of 3D oral cavity data, including oral cavity gypsum and scan data sets. To accurately measure the patient's tooth movement, this paper proposes a robust and precise isotropic scaling registration algorithm using bi-directional distance and correntropy. Firstly, because the oral cavity gypsum data sets have a lot of gypsum tumors and bubbles, which can cause the accuracy of registration results to decrease. Then, we introduce the correntropy into the traditional scaling registration model. Secondly, since unconstrained scaling registration is an ill-posed problem, bi-directional distance is used to enhance the robustness. In this way, a registration model using bi-directional distance and correntropy is established. In order to solve this problem, this paper proposes a new registration algorithm with iterative closest point. Moreover, the convergence of the algorithm is proved theoretically. Finally, the proposed algorithm is tested on the orthodontic database, and our experimental results demonstrate that our algorithm performs robust and high accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302725",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Gene",
      "Geometry",
      "Image (mathematics)",
      "Image registration",
      "Iterative closest point",
      "Mathematics",
      "Point cloud",
      "Robustness (evolution)",
      "Scaling"
    ],
    "authors": [
      {
        "surname": "Cui",
        "given_name": "Wenting"
      },
      {
        "surname": "Du",
        "given_name": "Shaoyi"
      },
      {
        "surname": "Wan",
        "given_name": "Teng"
      },
      {
        "surname": "Yao",
        "given_name": "Runzhao"
      },
      {
        "surname": "Liu",
        "given_name": "Yuying"
      },
      {
        "surname": "Han",
        "given_name": "Mengqi"
      },
      {
        "surname": "Mou",
        "given_name": "Qingnan"
      },
      {
        "surname": "Guo",
        "given_name": "Yu-cheng"
      },
      {
        "surname": "Zheng",
        "given_name": "Nanning"
      }
    ]
  },
  {
    "title": "Two-stage knowledge transfer framework for image classification",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107529",
    "abstract": "The two-stage strategy has been widely used in image classification. However, these methods barely take the classification criteria of the first stage into consideration in the second prediction stage. In this paper, we propose a novel Two-Stage Representation method (TSR), and convert it to a Single-Teacher Single-Student (STSS) problem in our two-stage knowledge transfer framework for image classification. Specifically, the first stage classifier is formulated as the teacher, which holds the ‘gate value’ to supervise the student classifier in the second stage. To transfer knowledge from the teacher classifier, we seek the nearest neighbours of the test sample to generate a set of candidate target classes in the first stage. Then, a student classifier learns from the samples belonging to these candidate classes in the second stage. Under the supervision of the teacher classifier, the teacher approves the student only if it obtains a higher score than the ‘gate value’. In actuality, the proposed framework generates a stronger classifier by staging two weaker classifiers in a novel way. The experiments on several databases show that our proposed framework is effective, which outperforms multiple popular classification methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303320",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Contextual image classification",
      "Image (mathematics)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Jianhang"
      },
      {
        "surname": "Zeng",
        "given_name": "Shaoning"
      },
      {
        "surname": "Zhang",
        "given_name": "Bob"
      }
    ]
  },
  {
    "title": "An efficient volume repairing method by using a modified Allen-Cahn equation",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107478",
    "abstract": "Classifying and rendering volumes of the structure are two essential goals of the visualization process. However, loss of some voxels can cause poor visualization results, such as small holes or non-smooth patches in visualized volumes. Beginning with the classified volumes, we propose a modified Allen-Cahn equation, which has the motion of mean curvature, to recover lost voxels and to fill holes. Consequently, a probability function can be obtained, which indicates the probability of each voxel being a volume voxel. Usually, the obtained probability function is smooth due to the motion of the mean curvature flow. Therefore visualization quality of volumes can be significantly improved. The equation is numerically computed by the unconditional stable operator splitting method with a large time step size. Thus the numerical scheme is fast and can be straightforwardly applied to GPU-accelerated DCT implementation that performs up to many times faster than CPU-only alternatives. Many experimental results have been performed to demonstrate the efficiency of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302818",
    "keywords": [
      "Algorithm",
      "Allen–Cahn equation",
      "Applied mathematics",
      "Artificial intelligence",
      "Computer science",
      "Curvature",
      "Geometry",
      "Mathematics",
      "Mean curvature",
      "Mean curvature flow",
      "Rendering (computer graphics)",
      "Visualization",
      "Volume rendering",
      "Voxel"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yibao"
      },
      {
        "surname": "Lan",
        "given_name": "Shouren"
      },
      {
        "surname": "Liu",
        "given_name": "Xin"
      },
      {
        "surname": "Lu",
        "given_name": "Bingheng"
      },
      {
        "surname": "Wang",
        "given_name": "Lisheng"
      }
    ]
  },
  {
    "title": "Complex heterogeneity learning: A theoretical and empirical study",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107519",
    "abstract": "Data heterogeneity such as task heterogeneity, view heterogeneity, and instance heterogeneity often co-exist in many real-world applications including insider threat detection, traffic prediction, brain image analysis, quality control in manufacturing processes, etc. However, most of the existing techniques might not take fully advantage of the rich heterogeneity. To address this problem, we propose a novel graph-based approach named M 3 to simultaneously model triple heterogeneity in a principled framework. The main idea is to employ the hybrid graphs to jointly model the task relatedness, view consistency, and bag-instance correlation by enhancing the labeling consistency between nearby nodes on the graphs. Furthermore, we analyze the generalization performance of the proposed method based on Rademacher complexity, which sheds light on the benefits of jointly modeling multiple types of heterogeneity. The resulting optimization problem is challenging since the objective function is non-smooth and non-convex. We propose an iterative algorithm based on block coordinate descent and bundle method to solve the problem. Experimental results on various datasets demonstrate the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303228",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Consistency (knowledge bases)",
      "Coordinate descent",
      "Data mining",
      "Generalization",
      "Graph",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Pei"
      },
      {
        "surname": "Tan",
        "given_name": "Qi"
      },
      {
        "surname": "He",
        "given_name": "Jingrui"
      }
    ]
  },
  {
    "title": "Dynamic graph convolutional network for multi-video summarization",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107382",
    "abstract": "Multi-video summarization is an effective tool for users to browse multiple videos. In this paper, multi-video summarization is formulated as a graph analysis problem and a dynamic graph convolutional network is proposed to measure the importance and relevance of each video shot in its own video as well as in the whole video collection. Two strategies are proposed to solve the inherent class imbalance problem of video summarization task. Moreover, we propose a diversity regularization to encourage the model to generate a diverse summary. Extensive experiments are conducted, and the comparisons are carried out with the state-of-the-art video summarization methods, the traditional and novel graph models. Our method achieves state-of-the-art performances on two standard video summarization datasets. The results demonstrate the effectiveness of our proposed model in generating a representative summary for multiple videos with good diversity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301850",
    "keywords": [
      "Artificial intelligence",
      "Automatic summarization",
      "Computer science",
      "Graph",
      "Law",
      "Political science",
      "Regularization (linguistics)",
      "Relevance (law)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Jiaxin"
      },
      {
        "surname": "Zhong",
        "given_name": "Sheng-hua"
      },
      {
        "surname": "Liu",
        "given_name": "Yan"
      }
    ]
  },
  {
    "title": "A divide-and-conquer strategy for facial landmark detection using dual-task CNN architecture",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107504",
    "abstract": "In this paper, we propose a novel deep learning-based framework for facial landmark detection. This framework takes as input face image returned by a face detector (Faster R-CNN) and generates as output a set of landmarks positions. Prior CNN-based methods often select randomly small local patches to predict an initial guess of landmarks locations. One issue with these local patches is that the adjacent landmarks might share the same regions due to the overlapping, thus, they might not convey precise information of each individual landmark. By contrast, our approach formulates this problem as a divide-conquer search for facial patches using CNN architecture in a hierarchy, where the input face image is recursively split into two cohesive non-overlapped subparts until each one contains only the region around the expected landmark. To attain better division of face topology, the search is carried out in a structured coarse-to-fine manner, where a learned hierarchical model of the face defining the granularity of each division level is introduced. We also propose a cascaded regressor to detect and refine the position of the individual landmark in each predicted non-overlapped patch. We adopt a carefully designed shallow CNN architecture so that to improve real-time performance. In addition, unlike previous cascaded methods, our regressor does not require auxiliary input such as initial landmarks locations. Extensive experiments on several challenging datasets (including MTFL, AFW, AFLW, COFW, 300W, and 300VW) show that our approach is particularly impressive in the unconstrained scenarios where it outperforms prior arts in both accuracy and efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303071",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Divide and conquer algorithms",
      "Division (mathematics)",
      "Economics",
      "Face (sociological concept)",
      "Face detection",
      "Facial recognition system",
      "Hierarchy",
      "Image (mathematics)",
      "Landmark",
      "Management",
      "Market economy",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Social science",
      "Sociology",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Hannane",
        "given_name": "Rachida"
      },
      {
        "surname": "Elboushaki",
        "given_name": "Abdessamad"
      },
      {
        "surname": "Afdel",
        "given_name": "Karim"
      }
    ]
  },
  {
    "title": "Semi-supervised learning framework based on statistical analysis for image set classification",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107500",
    "abstract": "Statistical models have been widely adopted for image set classification owing to their capacity in characterizing the data distribution more flexibly and faithfully. However, these methods typically suffer from the problem that the query image set has weak statistical correlations with the training sets, which leads to larger fluctuations in performance. To address this problem, we propose a semi-supervised fuzzy discriminative learning framework based on Log-Euclidean multivariate Gaussians descriptor to facilitate more robust image set classification. Specifically, by using the semi-supervised setting which definitely has access to the labeled training data and the available unlabeled testing data, we adopt manifold distance metric to construct a “fully trusted” graph and derive two new data dependent probabilistic kernels to strongly reflect the underlying connection relationships between the training and query Gaussian manifold components. The resulted kernel representations are eventually integrated into a kernel fuzzy discriminant framework to enhance the compactness of intra-class Gaussian components and enlarge the margin for inter-class Gaussian components. Thus, more discriminating power of our learning machine is obtained for the classification of the query image set. Extensive experiments on several datasets well demonstrate the effectiveness of the proposed method compared with other image set algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303034",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Contextual image classification",
      "Discriminative model",
      "Economics",
      "Image (mathematics)",
      "Kernel (algebra)",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Wenzhu"
      },
      {
        "surname": "Sun",
        "given_name": "Quansen"
      },
      {
        "surname": "Sun",
        "given_name": "Huaijiang"
      },
      {
        "surname": "Li",
        "given_name": "Yanmeng"
      }
    ]
  },
  {
    "title": "Covariance descriptors on a Gaussian manifold and their application to image set classification",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107463",
    "abstract": "Covariance descriptors (CovDs) for image set classification have been widely studied recently. Different from the conventional CovDs, which describe similarities between pixels at different locations, we focus more on similarities between regions that convey more comprehensive information. In this paper, we extract pixel-wise features of image regions and represent them by Gaussian models. We extend the conventional covariance computation onto a special type of Riemannian manifold, namely a Gaussian manifold, so that it is applicable to our image set data representation provided in terms of Gaussian models. We present two methods to calculate a Riemannian local difference vector on the Gaussian manifold (RieLDV-G) and generate our proposed Riemannian covariance descriptors (RieCovDs) using the resulting RieLDV-G. By measuring the recognition accuracy achieved on benchmarking datasets, we demonstrate experimentally the superior performance of our proposed RieCovDs descriptors, as compared with state-of-the-art methods. (The code is available at: https://github.com/Kai-Xuan/RiemannianCovDs)",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302661",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmarking",
      "Business",
      "Computation",
      "Computer science",
      "Covariance",
      "Covariance function",
      "Covariance intersection",
      "Engineering",
      "Gaussian",
      "Image (mathematics)",
      "Law",
      "Manifold (fluid mechanics)",
      "Marketing",
      "Mathematics",
      "Mechanical engineering",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel",
      "Political science",
      "Politics",
      "Programming language",
      "Pure mathematics",
      "Quantum mechanics",
      "Representation (politics)",
      "Riemannian manifold",
      "Set (abstract data type)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Kai-Xuan"
      },
      {
        "surname": "Ren",
        "given_name": "Jie-Yi"
      },
      {
        "surname": "Wu",
        "given_name": "Xiao-Jun"
      },
      {
        "surname": "Kittler",
        "given_name": "Josef"
      }
    ]
  },
  {
    "title": "Decoding non-linearity for effective extraction of the eye-blink artifact pattern from EEG recordings",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.01.022",
    "abstract": "Electroencephalogram (EEG) is a non-invasive measurement of electrical signal on the scalp originated due to neuronal activity. EEG signals associated with cortical activity are orders of magnitude lower in amplitude compared to other parasitic signals such as eye-blink artifacts, which contaminate the recorded EEG data making it imperative for the investigators to adopt an effective artifact suppression strategy. In all the previous studies, suppression of the pattern related to the artifacts was performed based on the assumption of a linear interaction between the source of artifact (eye-blink) and the brain signal (EEG data). This paper presents a novel methodology by considering the non-linear interaction between the artifact signal associated with the eye-blink and the contaminated EEG data using kernel functions. In the present work, adaptive data-driven approach called Ensemble Empirical Mode Decomposition (EEMD) is hybridized with kernel Principal Component Analysis (kPCA) to decode the non-linear interaction. The contaminated segment of EEG data is decomposed by the EEMD technique into a series of basic building blocks called intrinsic mode functions (IMFs). The features of the eye-blink signals are captured by some of these IMFs; subsequently, effective extraction of the ocular artifact from the IMFs is performed by kPCA using non-linear kernel functions (radial basis function, second and third-order polynomial function). In the present study, technique used for artifact suppression relies on the extraction of ocular artifact signal from IMFs based on optimizing the different parameters of the kPCA. Compared with other techniques used in previous studies, the proposed method (based on third-order polynomial kernel function) is capable of effectively extracting the pattern related to the ocular artifacts from the single channel contaminated EEG data with low distortion of the EEG signal.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518300291",
    "keywords": [
      "Artifact (error)",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Electroencephalography",
      "Feature extraction",
      "Filter (signal processing)",
      "Hilbert–Huang transform",
      "Kernel (algebra)",
      "Kernel method",
      "Kernel principal component analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Psychiatry",
      "Psychology",
      "SIGNAL (programming language)",
      "Speech recognition",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Patel",
        "given_name": "Rajesh"
      },
      {
        "surname": "Gireesan",
        "given_name": "K."
      },
      {
        "surname": "Sengottuvel",
        "given_name": "S."
      }
    ]
  },
  {
    "title": "Depth image super-resolution using correlation-controlled color guidance and multi-scale symmetric network",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107513",
    "abstract": "Depth image super-resolution (DISR) is an effective solution to improve the quality of depth images captured by real world low-cost cameras. In this paper, we propose a multi-scale symmetric network with the correlation-controlled color guidance block (CCGB) for DISR. The proposed network consists of two multi-scale sub-networks to respectively provide guidance and estimate depth. A symmetric unit (SU), which is a mini-encoder-decoder structure with residual learning, is designed and used as a basic network atom. The encoder part in SU aims to extract essential features, while the decoder part works to restore edge details. The way the SU processes information matches well with the textureless and sharp-edge characteristics of depth images. The two sub-networks present a high-level symmetric structure connected by dense guidance links in between. Based on the correlation analyses between the two sub-networks, each guidance link will transfer information trough a CCGB designed to implement channel-wise re-weighting mechanism. The accurate color guidance from CCGB helps avoiding artifacts introduced by non-co-occurrence of depth discontinuities and color edges. Experimental results demonstrate the superiority of the proposed method over several state-of-the-art DISR works.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303162",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Classification of discontinuities",
      "Computer science",
      "Computer vision",
      "Encoder",
      "Enhanced Data Rates for GSM Evolution",
      "Geometry",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Radiology",
      "Residual",
      "Scale (ratio)",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Tao"
      },
      {
        "surname": "Lin",
        "given_name": "Hongwei"
      },
      {
        "surname": "Dong",
        "given_name": "Xiucheng"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaohua"
      }
    ]
  },
  {
    "title": "Classification of patients with tumor using MR FLAIR images",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2017.10.037",
    "abstract": "Magnetic Resonance Imaging (MRI) is a fast growing imaging tool for neurodiagnosis. The radiologists time is at a premium due to increase in patient studies each having a large data set. This can be aided by classification using machine learning techniques. This paper evaluates its utility for accurate and rapid diagnosis of cerebral tumors. Two hundred subjects were classified into normal and abnormal using volumetric Fluid Attenuated Inversion Recovery (FLAIR) acquisition. The images are normalized to obtain 12 useful slices to be considered as the patient feature set for classification. Discrete Wavelet Transform (DWT) is used for feature extraction and Principal Component Analysis (PCA) is used for feature selection. Various classifiers like Support Vector Machine (SVM), k-Nearest Neighbor (k-NN), CART (Classification and Regression Tree) and Random forest are tested. Applying K-fold cross validation in each train-test ratios, we obtained ceiling level classification accuracy with coherent sensitivity and specificity using only linear SVM (negating the use of PCA). An accuracy of 88% is obtained with a sensitivity of 84% and specificity of 92% with 62.28 s computation time. The algorithm is robust to be tested in clinical settings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865517304051",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Cross-validation",
      "Discrete wavelet transform",
      "Feature extraction",
      "Feature selection",
      "Fluid-attenuated inversion recovery",
      "Magnetic resonance imaging",
      "Medicine",
      "Pattern recognition (psychology)",
      "Principal component analysis",
      "Radiology",
      "Random forest",
      "Support vector machine",
      "Wavelet",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Gupta",
        "given_name": "Tanvi"
      },
      {
        "surname": "Gandhi",
        "given_name": "Tapan K."
      },
      {
        "surname": "Gupta",
        "given_name": "R.K."
      },
      {
        "surname": "Panigrahi",
        "given_name": "B.K."
      }
    ]
  },
  {
    "title": "Multi angle optimal pattern-based deep learning for automatic facial expression recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2017.06.025",
    "abstract": "Facial Expression Recognition (FER) plays the vital role in the Human Computer Interface (HCI) applications. The illumination and pose variations affect the FER adversely. The projection of complex 3D actions on the image plane and the inaccurate alignment are the major issues in the FER process. This paper presents the novel Multi-Angle Optimal Pattern-based Deep Learning (MAOP-DL) method to rectify the problem from sudden illumination changes, find the proper alignment of a feature set by using multi-angle-based optimal configurations. The proposed method includes the five major processes as Extended Boundary Background Subtraction (EBBS), Multi-Angle Texture Pattern+STM, Densely Extracted SURF+Local Occupancy Pattern (LOP), Priority Particle Cuckoo Search Optimization (PPCSO) and Long Short-Term Memory -Convolutional Neural Network (LSTM-CNN). Initially, the EBBS algorithm subtracts the background and isolates the foreground from the images which overcome the illumination and pose variation. Then, the MATP-STM extracts the texture patterns and DESURF-LOP extracts the relevant key features of the facial points. The PPCSO algorithm selects the relevant features from the MATP-STM feature set to speed up the classification. The employment of LSTM-CNN predicts the required label for the facial expressions.The major key findings of the proposed work are clear image analysis, effective handling of pose/illumination variations and the facial alignment. The proposed MAOP-DL validates its effectiveness on two standard databases such as CK+ and MMI regarding various metrics and confirm their assurance of wide applicability in recent applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865517302313",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Background subtraction",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Cuckoo search",
      "Facial expression",
      "Feature (linguistics)",
      "Histogram",
      "Image (mathematics)",
      "Linguistics",
      "Local binary patterns",
      "Particle swarm optimization",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Jain",
        "given_name": "Deepak Kumar"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhang"
      },
      {
        "surname": "Huang",
        "given_name": "Kaiqi"
      }
    ]
  },
  {
    "title": "Multi-head enhanced self-attention network for novelty detection",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107486",
    "abstract": "One-class classification (OCC) is a classical problem in computer vision that can be described as the task of classifying outlier class samples (OC samples) from the OCC model trained on inlier class samples (IC samples) when datasets are highly biased toward one class due to the insufficient sample size of the other class. Currently, the adversarial learning OCC (ALOCC) method has been proven to significantly improve OCC performance. However, its drawbacks include instability issues and non-evident reconstruction between the IC and OC samples. Therefore, we propose multihead enhanced self-attention in the ALOCC network, thereby increasing the difference between the IC and OC samples and significantly increasing OCC accuracy compared with ALOCC accuracy. For training, we propose a new loss, called adversarial-balance loss, that effectively solves the training instability problem, further increasing OCC accuracy. The experiments show the effectiveness of the proposed method compared with state-of-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302892",
    "keywords": [
      "Adversarial system",
      "Anomaly detection",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Economics",
      "Machine learning",
      "Management",
      "Novelty",
      "Novelty detection",
      "Outlier",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Task (project management)",
      "Theology"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yingying"
      },
      {
        "surname": "Gong",
        "given_name": "Yuxin"
      },
      {
        "surname": "Zhu",
        "given_name": "Haogang"
      },
      {
        "surname": "Bai",
        "given_name": "Xiao"
      },
      {
        "surname": "Tang",
        "given_name": "Wenzhong"
      }
    ]
  },
  {
    "title": "Efficient sampling-based energy function evaluation for ensemble optimization using simulated annealing",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107510",
    "abstract": "In this study, we attempted to develop a method for accelerating parameter optimization of an object detector ensemble over large image datasets by using simulated annealing. We propose a novel sampling-based evaluation method that considers the minimum portion of the dataset required in each iteration to maintain solution quality. This approach can be considered a noisy evaluation of the energy. The sample sizes required during the search process are theoretically determined by adapting the convergence results for noisy evaluation. To determine applicability, we prepared and optimized two ensembles for diabetic retinopathy pre-screening based on microaneurysm detection with convolutional neural network-based and traditional object detectors. Our experimental results indicate that the proposed sampling-based evaluation method substantially reduced the computational time required for optimizing the parameters of the ensembles while preserving solution quality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303137",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convergence (economics)",
      "Convolutional neural network",
      "Detector",
      "Economic growth",
      "Economics",
      "Mathematical optimization",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Sampling (signal processing)",
      "Simulated annealing",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Tóth",
        "given_name": "János"
      },
      {
        "surname": "Tomán",
        "given_name": "Henrietta"
      },
      {
        "surname": "Hajdu",
        "given_name": "András"
      }
    ]
  },
  {
    "title": "Low-rank quaternion tensor completion for recovering color videos and images",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107505",
    "abstract": "Low-rank quaternion tensor completion method, a novel approach to recovery color videos and images, is proposed in this paper. We respectively reconstruct a color image and a color video as a quaternion matrix (second-order tensor) and a third-order quaternion tensor by encoding the red, green, and blue channel pixel values on the three imaginary parts of a quaternion. Different from some traditional models which treat color pixel as a scalar and represent color channels separately, whereas, during the quaternion-based reconstruction, it is significant that the inherent color structures of color images and color videos can be completely preserved. Under the definition of Tucker rank, the global low-rank prior to quaternion tensor is encoded as the nuclear norm of unfolding quaternion matrices. Then, by applying the ADMM framework, we provide the tensor completion algorithm for any order ( ≥ 2) quaternion tensors, which theoretically can be well used to recover missing entries of any multidimensional data with color structures. Simulation results for color videos and color images recovery show the superior performance and efficiency of the proposed method over some state-of-the-art existing ones.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303083",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Color balance",
      "Color histogram",
      "Color image",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Eigenvalues and eigenvectors",
      "Geometry",
      "Image (mathematics)",
      "Image processing",
      "Mathematics",
      "Matrix norm",
      "Physics",
      "Pixel",
      "Pure mathematics",
      "Quantum mechanics",
      "Quaternion",
      "Rank (graph theory)",
      "Tensor (intrinsic definition)"
    ],
    "authors": [
      {
        "surname": "Miao",
        "given_name": "Jifei"
      },
      {
        "surname": "Kou",
        "given_name": "Kit Ian"
      },
      {
        "surname": "Liu",
        "given_name": "Wankai"
      }
    ]
  },
  {
    "title": "Zero-shot Handwritten Chinese Character Recognition with hierarchical decomposition embedding",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107488",
    "abstract": "Handwritten Chinese Character Recognition (HCCR) is a challenging topic in the field of pattern recognition due to large-scale character vocabulary, complex hierarchical structure, various writing styles, and scarce training samples. In this paper, we explored the hierarchical knowledge of Chinese characters and presented a novel zero-shot HCCR method. First, we handled the relations between the characters and their primitives, such as radicals and structures, to obtain a tree layout of primitives. Then, we presented a novel zero-shot hierarchical decomposition embedding method to encode the tree layout into a semantic vector. Next, we devised a Convolutional Neural Network (CNN) based framework to learn both radicals and structures of characters via the semantic vector. As different Chinese characters share some common radicals and structures, our method is able to recognize new categories without any labeled samples from them. Moreover, our method is effective in both traditional HCCR and zero-shot HCCR tasks. It achieves competitive performance on the traditional experiment setting and significantly surpasses the state-of-the-art methods on the zero-shot experiment setting.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302910",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Binary tree",
      "Character (mathematics)",
      "Computer science",
      "Convolutional neural network",
      "Embedding",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Tree structure",
      "Zero (linguistics)"
    ],
    "authors": [
      {
        "surname": "Cao",
        "given_name": "Zhong"
      },
      {
        "surname": "Lu",
        "given_name": "Jiang"
      },
      {
        "surname": "Cui",
        "given_name": "Sen"
      },
      {
        "surname": "Zhang",
        "given_name": "Changshui"
      }
    ]
  },
  {
    "title": "Localization of eye Saccadic signatures in Electrooculograms using sparse representations with data driven dictionaries",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2017.11.001",
    "abstract": "In this paper, we propose two methods for localizing saccadic eye movement signatures from Electrooculograms (EOG). The first approach uses a sparse representation of data-driven dictionaries of saccadic movements. In this approach, we match the EOG subsequence with the dictionary element using distance metrics to identify the saccades. The second approach is to compare a saccadic signature template with the EOG subsequence using Dynamic Time Warping (DTW). We find that the proposed methods have advantages over one another in context specific applications. The first method is significantly faster with considerable accuracy, while the second approach is more accurate.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865517304075",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Bounded function",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Dynamic time warping",
      "Eye movement",
      "Geography",
      "Law",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Saccadic masking",
      "Subsequence"
    ],
    "authors": [
      {
        "surname": "Chakraborty",
        "given_name": "Suvodip"
      },
      {
        "surname": "Dasgupta",
        "given_name": "Anirban"
      },
      {
        "surname": "Routray",
        "given_name": "Aurobinda"
      }
    ]
  },
  {
    "title": "Multi-Label classification of multi-modality skin lesion via hyper-connected convolutional neural network",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107502",
    "abstract": "Objective Clinical and dermoscopy images (multi-modality image pairs) are routinely used sequentially in the assessment of skin lesions. Clinical images characterize a lesion's geometry and color; dermoscopy depicts vascularity, dots and globules from the sub-surface of the lesion. Together these modalities provide labels to characterize a skin lesion. Recently, convolutional neural networks (CNNs), due to the ability to learn low-level features and high-level semantic information in an end-to-end architecture, have been shown to be the state-of-the-art in skin lesion classification. Most of the CNN methods have relied on dermoscopy alone. In the few published papers that support multi-modalities, the methods are based on ‘late-fusion’ to integrate extracted clinical and dermoscopy image features separately. These late-fusion methods tend to ignore the accessible complementary image features between the paired images at the early stage of the CNN architecture. Methods We propose a hyper-connected CNN (HcCNN) to classify skin lesions. Compared to existing multi-modality CNNs, our HcCNN has an additional hyper-branch that integrates intermediary image features in a hierarchical manner. The hyper-branch enables the network to learn more complex combinations between the images at all, early and late, stages of the network. We also coupled the HcCNN with a multi-scale attention block (MsA) to prioritize semantically important subtle regions in the two modalities across various image scales. Results Our HcCNN achieved an average accuracy of 74.9% for multi-label classification on the 7-point Checklist dataset, which is a well-benchmarked public dataset. Conclusions: Our method is more accurate than the state-of-the-art methods and, in particular, our method achieved consistent and the best results in datasets with imbalanced label distributions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303058",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Lesion",
      "Medicine",
      "Modality (human–computer interaction)",
      "Pathology",
      "Pattern recognition (psychology)",
      "Vascularity"
    ],
    "authors": [
      {
        "surname": "Bi",
        "given_name": "Lei"
      },
      {
        "surname": "Feng",
        "given_name": "David Dagan"
      },
      {
        "surname": "Fulham",
        "given_name": "Michael"
      },
      {
        "surname": "Kim",
        "given_name": "Jinman"
      }
    ]
  },
  {
    "title": "On-body wearable device localization with a fast and memory efficient SVM-kNN using GPUs",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2017.10.005",
    "abstract": "Biomedical and inertial sensors can be used to generate a complete view of complicated physiological changes in a continuous, real-time, and non-invasive manner. As inappropriate on-body attachments may result in errors, the localization information requires calibration. In most previous studies of on-body wearable device localization, the run-time performance was not evaluated in terms of execution time and memory usage. Recently, smartphones have been used as data processing centers and recognition systems for wearable devices. However, the hardware disadvantages of phone devices have a negative effect on both real-time response and system accuracy. In this study, we focus on a high-performance localization system for the mobile environment and propose a memory efficient algorithm that is accelerated by using graphics processing units (GPUs). The algorithm combines a k-nearest neighbor classifier with a support vector machine (SVM)-based decision-maker to refine the classification outcome. This contributes to a faster and more accurate on-body localization for wearable devices. Experiment results demonstrated that the proposed method achieves an accuracy of 92.94%% and is faster than Serial-SVM and GPU-SVM by a factor of 6.81×and 3.87×, respectively. Therefore, the proposed technique can offer a comparable accuracy to SVM-based methods with faster recognition in the mobile environment.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865517303677",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Embedded system",
      "Focus (optics)",
      "Inertial measurement unit",
      "Mobile device",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Response time",
      "Support vector machine",
      "Wearable computer",
      "Wearable technology"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Quanzhe"
      },
      {
        "surname": "Shin",
        "given_name": "SaeByuk"
      },
      {
        "surname": "Hong",
        "given_name": "Chung-Pyo"
      },
      {
        "surname": "Kim",
        "given_name": "Shin-Dug"
      }
    ]
  },
  {
    "title": "Classification of gait signals into different neurodegenerative diseases using statistical analysis and recurrence quantification analysis",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.05.006",
    "abstract": "Among all the biological signals, gait signal is one of the better features to detect movement disorders caused by a malfunction in parts of the brain and nervous system. Usually, identifying and evaluating movement disorders caused due to neurodegenerative diseases solely depends on a physicians experience. Different diseases having gait abnormalities generate a unique gait characteristic. Traditionally, Fourier analysis is used to understand the gait characteristic, thereby predicting potential diseases. Fourier analysis assumes the gait signal to be stationary, linear and noiseless which is not a reality. To overcome this, Recurrence Quantification Analysis (RQA) is used in this study to quantify gait parameters. RQA has proved to be one of the best tools for non-linear, non-stationary and short length data. It is used to quantify heart rate variability, ventricular fibrillation, wrist pulse and growth of bladder. This paper uses RQA in understanding the dynamics of human gait and the parameters obtained are used as a feature for classification using Support Vector Machine (SVM) and Probabilistic Neural Network (PNN). This study considered thirteen subjects for the classification of gait signals of patients with Neurodegenerative diseases (Amyotrophic Lateral Sclerosis, Huntington and Parkinson) and thirteen healthy control subjects using two different classification models like Support Vector Machine (SVM) and Probabilistic Neural Network (PNN). Features were extracted after statistical analysis and RQA, and Hill-climbing feature selection method was used to optimize the feature set. The accuracy deduced after binary classification using SVM and PNN ranged from 96% to 100%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518301727",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Disease",
      "Feature (linguistics)",
      "Feature selection",
      "Feature vector",
      "Gait",
      "Gait analysis",
      "Linear discriminant analysis",
      "Linguistics",
      "Medicine",
      "Nonlinear system",
      "Parkinson's disease",
      "Pathology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physical medicine and rehabilitation",
      "Physics",
      "Probabilistic logic",
      "Probabilistic neural network",
      "Quantum mechanics",
      "Recurrence quantification analysis",
      "Support vector machine",
      "Time delay neural network"
    ],
    "authors": [
      {
        "surname": "Prabhu",
        "given_name": "Pooja"
      },
      {
        "surname": "Karunakar",
        "given_name": "A.K."
      },
      {
        "surname": "Anitha",
        "given_name": "H."
      },
      {
        "surname": "Pradhan",
        "given_name": "N."
      }
    ]
  },
  {
    "title": "Self-supervised deep reconstruction of mixed strip-shredded text documents",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107535",
    "abstract": "The reconstruction of shredded documents consists of coherently arranging fragments of paper (shreds) to recover the original document(s). A great challenge in computational reconstruction is to properly evaluate the compatibility between the shreds. While traditional pixel-based approaches are not robust to real shredding, more sophisticated solutions compromise significantly time performance. The solution presented in this work extends our previous deep learning method for single-page reconstruction to a more realistic/complex scenario: the reconstruction of several mixed shredded documents at once. In our approach, the compatibility evaluation is modeled as a two-class (valid or invalid) pattern recognition problem. The model is trained in a self-supervised manner on samples extracted from simulated-shredded documents, which obviates manual annotation. Experimental results on three datasets – including a new collection of 100 strip-shredded documents produced for this work – have shown that the proposed method outperforms the competing ones on complex scenarios, achieving accuracy superior to 90%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303381",
    "keywords": [
      "Annotation",
      "Artificial intelligence",
      "Compatibility (geochemistry)",
      "Computer science",
      "Data mining",
      "Geochemistry",
      "Geology",
      "Information retrieval",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Pixel"
    ],
    "authors": [
      {
        "surname": "Paixão",
        "given_name": "Thiago M."
      },
      {
        "surname": "Berriel",
        "given_name": "Rodrigo F."
      },
      {
        "surname": "Boeres",
        "given_name": "Maria C.S."
      },
      {
        "surname": "Koerich",
        "given_name": "Alessandro L."
      },
      {
        "surname": "Badue",
        "given_name": "Claudine"
      },
      {
        "surname": "De Souza",
        "given_name": "Alberto F."
      },
      {
        "surname": "Oliveira-Santos",
        "given_name": "Thiago"
      }
    ]
  },
  {
    "title": "A framework for offline signature verification system: Best features selection approach",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.01.021",
    "abstract": "Biometric verification is a method of identifying the persons by their individualities or traits. Signature verification is the most generally used biometric to maintain human privacy. It is used in many areas as banking, access control, e-business etc. and equally important in financial transactions. Research has progressed greatly in the area of signature verification but still, it is hard to discriminate between genuine signatures and skilled forgeries. Based on the idea of best features selection, a novel technique is introduced in this article foran offline verification system. The presented system consists of four major steps: preprocessing, features extraction, features selection, and feature verification. Global features in the proposed work comprise of aspect ratio, the area of signature, pure width, pure height and normalized actual signature height. Local features consist of signature centroid, slope, angle, and distance. In features selection component, a genetic algorithm is utilized to find appropriate features set which are later on given to support vector machine for verification. For experimental analysis, the selected datasets are CEDAR, MCYT and GPDS synthetic. The performance of proposed algorithm is based on three accuracy measures as FAR, FRR and AER.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786551830028X",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Centroid",
      "Component (thermodynamics)",
      "Computer science",
      "Data mining",
      "Feature extraction",
      "Feature selection",
      "Geometry",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Preprocessor",
      "Programming language",
      "Selection (genetic algorithm)",
      "Set (abstract data type)",
      "Signature (topology)",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Sharif",
        "given_name": "Muhammad"
      },
      {
        "surname": "Khan",
        "given_name": "Muhammad Attique"
      },
      {
        "surname": "Faisal",
        "given_name": "Muhammad"
      },
      {
        "surname": "Yasmin",
        "given_name": "Mussarat"
      },
      {
        "surname": "Fernandes",
        "given_name": "Steven Lawrence"
      }
    ]
  },
  {
    "title": "Dirichlet Variational Autoencoder",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107514",
    "abstract": "This paper proposes Dirichlet Variational Autoencoder (DirVAE) using a Dirichlet prior. To infer the parameters of DirVAE, we utilize the stochastic gradient method by approximating the inverse cumulative distribution function of the Gamma distribution, which is a component of the Dirichlet distribution. This approximation on a new prior led an investigation on the component collapsing, and DirVAE revealed that the component collapsing originates from two problem sources: decoder weight collapsing and latent value collapsing. The experimental results show that 1) DirVAE generates the result with the best log-likelihood compared to the baselines; 2) DirVAE produces more interpretable latent values with no collapsing issues which the baselines suffer from; 3) the latent representation from DirVAE achieves the best classification accuracy in the (semi-)supervised classification tasks on MNIST, OMNIGLOT, COIL-20, SVHN, and CIFAR-10 compared to the baseline VAEs; and 4) the DirVAE augmented topic models show better performances in most cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303174",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Autoencoder",
      "Bayesian probability",
      "Boundary value problem",
      "Categorical variable",
      "Component (thermodynamics)",
      "Computer science",
      "Deep learning",
      "Dirichlet distribution",
      "Gaussian",
      "Latent Dirichlet allocation",
      "Latent variable",
      "Law",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Prior probability",
      "Quantum mechanics",
      "Representation (politics)",
      "Thermodynamics",
      "Topic model"
    ],
    "authors": [
      {
        "surname": "Joo",
        "given_name": "Weonyoung"
      },
      {
        "surname": "Lee",
        "given_name": "Wonsung"
      },
      {
        "surname": "Park",
        "given_name": "Sungrae"
      },
      {
        "surname": "Moon",
        "given_name": "Il-Chul"
      }
    ]
  },
  {
    "title": "Visual tracking by dynamic matching-classification network switching",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107419",
    "abstract": "Existing deep trackers can be roughly divided into either matching-based or classification-based methods. The formers are fast but not very robust; while the latter ones introduce more discriminative information but often very slow. In this work, we present a novel real-time robust tracking method to take full use of the benefits from both kinds of networks. First, we propose a matching-classification network switching (MCS) framework to integrate the matching, classification, verification networks and conduct dynamic switching among them. Second, to speed up online update, we devlop a meta learning method as a critical component in our classification network. The meta classifier is trained offline to obtain general discriminative ability and updated online to the current frame just through one iteration. Extensive experiments are conducted on two popular benchmark datasets. Both qualitative and quantitative evaluations show that our tracker performs favorably against other state-of-the-art trackers with real-time performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302223",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Eye tracking",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Psychology",
      "Statistics",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Peixia"
      },
      {
        "surname": "Chen",
        "given_name": "Boyu"
      },
      {
        "surname": "Wang",
        "given_name": "Dong"
      },
      {
        "surname": "Lu",
        "given_name": "Huchuan"
      }
    ]
  },
  {
    "title": "The benefits of target relations: A comparison of multitask extensions and classifier chains",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107507",
    "abstract": "Multitask (multi-target or multi-output) learning (MTL) deals with simultaneous prediction of several outputs. MTL approaches rely on the optimization of a joint score function over the targets. However, defining a joint score in global models is problematic when the target scales are different. To address such problems, single target (i.e. local) learning strategies are commonly employed. Here we propose alternative tree-based learning strategies to handle the issue with target scaling in global models, and to identify the learning order for chaining operations in local models. In the first proposal, the problems with target scaling are resolved using alternative splitting strategies which consider the learning tasks in a multi-objective optimization framework. The second proposal deals with the problem of ordering in the chaining strategies. We introduce an alternative estimation strategy, minimum error chain policy, that gradually expands the input space using the estimations that approximate to true characteristics of outputs, namely out-of-bag estimations in tree-based ensemble framework. Our experiments on benchmark datasets illustrate the success of the proposed multitask extension of trees compared to the decision trees with de facto design especially for datasets with large number of targets. In line with that, minimum error chain policy improves the performance of the state-of-the-art chaining policies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303101",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Chaining",
      "Classifier (UML)",
      "Computer science",
      "Economics",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Multi-task learning",
      "Psychology",
      "Psychotherapist",
      "Task (project management)",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Adıyeke",
        "given_name": "Esra"
      },
      {
        "surname": "Baydoğan",
        "given_name": "Mustafa Gökçe"
      }
    ]
  },
  {
    "title": "Semi-supervised elastic manifold embedding with deep learning architecture",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107425",
    "abstract": "Graph-based embedding aims to reduce the dimension of high dimensional data and to extract relevant features for learning tasks. In this letter, we propose an Elastic graph-based embedding with deep architecture which deeply explores the structural information of the data. We introduce a flexible deep learning that can overcome the limitations and weaknesses of single-layer learning models. The proposed deep architecture incorporates the geometrical manifold structure of the data. The resulting framework can be used for semi-supervised and supervised settings. Besides, the resulting optimization problems can be solved efficiently. We apply the algorithm on five public image datasets including scene, face and object datasets. These experiments demonstrate the effectiveness of the proposed embedding method, and also show that the proposed method compares favorably with many competing state-of-the-art graph-based methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302284",
    "keywords": [
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Dimensionality reduction",
      "Embedding",
      "Engineering",
      "Graph",
      "Graph embedding",
      "Machine learning",
      "Manifold (fluid mechanics)",
      "Mechanical engineering",
      "Nonlinear dimensionality reduction",
      "Pattern recognition (psychology)",
      "Semi-supervised learning",
      "Theoretical computer science",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "R."
      },
      {
        "surname": "Dornaika",
        "given_name": "F."
      },
      {
        "surname": "Ruichek",
        "given_name": "Y."
      }
    ]
  },
  {
    "title": "A distinctive approach in brain tumor detection and classification using MRI",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2017.10.036",
    "abstract": "A very exigent task for radiologists is early brain tumor detection. Brain tumor raises very fast, its average size doubles in just twenty-five days. If not treated properly, the survival rate of the patient is normally not more than half a year. It can rapidly lead to death. For this reason, an automatic system is required for brain tumor detection at an early stage. In this paper, an automated method is proposed to easily differentiate between cancerous and non-cancerous Magnetic Resonance Imaging (MRI) of the brain. Different techniques have been applied for the segmentation of candidate lesion. Then a features set is chosen for every applicant lesion using shape, texture, and intensity. At that point, Support Vector Machine (SVM) classifier is applied with different cross validations on the features set to compare the precision of proposed framework. The proposed method is validated on three benchmark datasets such as Harvard, RIDER and Local. The method achieved average 97.1% accuracy, 0.98 area under curve, 91.9% sensitivity and 98.0% specificity. It can be used to identify the tumor more accurately in less processing time as compared to existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786551730404X",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Brain tumor",
      "Classifier (UML)",
      "Computer science",
      "Geodesy",
      "Geography",
      "Magnetic resonance imaging",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)",
      "Radiology",
      "Segmentation",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Amin",
        "given_name": "Javeria"
      },
      {
        "surname": "Sharif",
        "given_name": "Muhammad"
      },
      {
        "surname": "Yasmin",
        "given_name": "Mussarat"
      },
      {
        "surname": "Fernandes",
        "given_name": "Steven Lawrence"
      }
    ]
  },
  {
    "title": "Graph-based parallel large scale structure from motion",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107537",
    "abstract": "While Structure from Motion achieves great success in 3D reconstruction, it still meets challenges on large scale scenes. Incremental SfM approaches are robust to outliers, but are limited by low efficiency and easy suffer from drift problem. Though Global SfM methods are more efficient than incremental approaches, they are sensitive to outliers, and would also meet memory limitation and time bottleneck. In this work, large scale SfM is deemed as a graph problem, where graph are respectively constructed in image clustering step and local reconstructions merging step. By leveraging the graph structure, we are able to handle large scale dataset in divide-and-conquer manner. Firstly, images are modelled as graph nodes, with edges are retrieved from geometric information after feature matching. Then images are divided into independent clusters by a image clustering algorithm, and followed by a subgraph expansion step, the connection and completeness of scenes are enhanced by walking along a maximum spanning tree, which is utilized to construct overlapping images between clusters. Secondly, Image clusters are distributed into servers to execute SfM in parallel mode. Thirdly, after local reconstructions complete, we construct a minimum spanning tree to find accurate similarity transformations. Then the minimum spanning tree is transformed into a Minimum Height Tree to find a proper anchor node, and is further utilized to prevent error accumulation. We evaluate our approach on various kinds of datasets and our approach shows superiority over the state-of-the-art in accuracy and efficiency. Our algorithm is open-sourced in https://github.com/AIBluefisher/GraphSfM.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030340X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Data structure",
      "Graph",
      "Matching (statistics)",
      "Mathematics",
      "Minimum spanning tree",
      "Motion estimation",
      "Outlier",
      "Pattern recognition (psychology)",
      "Programming language",
      "Spanning tree",
      "Statistics",
      "Structure from motion",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yu"
      },
      {
        "surname": "Shen",
        "given_name": "Shuhan"
      },
      {
        "surname": "Chen",
        "given_name": "Yisong"
      },
      {
        "surname": "Wang",
        "given_name": "Guoping"
      }
    ]
  },
  {
    "title": "Deep Matching Network for Handwritten Chinese Character Recognition",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107471",
    "abstract": "Just like its remarkable achievements in many computer vision tasks, the convolutional neural networks (CNN) provide an end-to-end solution in handwritten Chinese character recognition (HCCR) with great success. However, the process of learning discriminative features for image recognition is difficult in cases where little data is available. In this paper, we propose a matching network which builds a connection between template characters and handwritten characters inspired by the human learning process of writing Chinese characters. The matching network replaces the parameters in the softmax regression layer with the features extracted from the template character images. After the training process has been finished, the powerful discriminative features help us to generalize the predictive power not just to new data, but to entire new Chinese characters that never appear in the training set before. Experiments performed on the ICDAR-2013 offline HCCR datasets have shown that the proposed method achieves a comparable performance to current CNN-based classifiers. Besides, the matching network has a very promising generalization ability to new Chinese characters that never appear in the existing training set.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302740",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Character (mathematics)",
      "Character recognition",
      "Chinese characters",
      "Computer science",
      "Convolutional neural network",
      "Discriminative model",
      "Generalization",
      "Geometry",
      "Image (mathematics)",
      "Matching (statistics)",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Programming language",
      "Set (abstract data type)",
      "Softmax function",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Zhiyuan"
      },
      {
        "surname": "Wu",
        "given_name": "Qi"
      },
      {
        "surname": "Xiao",
        "given_name": "Yi"
      },
      {
        "surname": "Jin",
        "given_name": "Min"
      },
      {
        "surname": "Lu",
        "given_name": "Huaxiang"
      }
    ]
  },
  {
    "title": "Nonlinear dimensionality reduction for clustering",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107508",
    "abstract": "We introduce an approach to divisive hierarchical clustering that is capable of identifying clusters in nonlinear manifolds. This approach uses the isometric mapping (Isomap) to recursively embed (subsets of) the data in one dimension, and then performs a binary partition designed to avoid the splitting of clusters. We provide a theoretical analysis of the conditions under which contiguous and high-density clusters in the original space are guaranteed to be separable in the one-dimensional embedding. To the best of our knowledge there is little prior work that studies this problem. Extensive experiments on simulated and real data sets show that hierarchical divisive clustering algorithms derived from this approach are effective.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303113",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Dimensionality reduction",
      "Geometry",
      "Mathematics",
      "Nonlinear system",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Reduction (mathematics)"
    ],
    "authors": [
      {
        "surname": "Tasoulis",
        "given_name": "Sotiris"
      },
      {
        "surname": "Pavlidis",
        "given_name": "Nicos G."
      },
      {
        "surname": "Roos",
        "given_name": "Teemu"
      }
    ]
  },
  {
    "title": "Skeleton-based action recognition with hierarchical spatial reasoning and temporal stack learning network",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107511",
    "abstract": "Skeleton-based action recognition aims to recognize human actions by exploring the inherent characteristics from the given skeleton sequences and has attracted far more attention due to its great important potentials in practical applications. Previous methods have illustrated that learning discriminative spatial and temporal features from the skeleton sequences is a crucial factor to recognize human actions. Nevertheless, how to model spatio-temporal evolutions is still a challenging problem. In this work, we propose a novel model with hierarchical spatial reasoning and temporal stack learning network (HSR-TSL) to explore the discriminative spatial and temporal features for human action recognition, which consists of a hierarchical spatial reasoning network (HSRN) and a temporal stack learning network (TSLN). Specifically, the HSRN employs a hierarchical residual graph neural network to capture two-level spatial features: intra spatial information of each part and body-level structural information between each part. The TSLN models the detailed temporal dynamics of skeleton sequences by a composition of multiple skip-clip LSTMs. During training, we develop a clip-based incremental loss to effectively optimize the model. We perform extensive experiments on five challenging benchmarks to verify the effectiveness of each component of our model. The comparison results illustrate that our approach significantly boosts the performances for skeleton-based action recognition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303149",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Discriminative model",
      "Feature learning",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Programming language",
      "Skeleton (computer programming)",
      "Spatial intelligence",
      "Spatial relation"
    ],
    "authors": [
      {
        "surname": "Si",
        "given_name": "Chenyang"
      },
      {
        "surname": "Jing",
        "given_name": "Ya"
      },
      {
        "surname": "Wang",
        "given_name": "Wei"
      },
      {
        "surname": "Wang",
        "given_name": "Liang"
      },
      {
        "surname": "Tan",
        "given_name": "Tieniu"
      }
    ]
  },
  {
    "title": "An improved nonlocal maximum likelihood estimation method for denoising magnetic resonance images with spatially varying noise levels",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.02.007",
    "abstract": "Magnetic resonance images (MRI) reconstructed with parallel MRI (pMRI) techniques generally have spatially varying (non-stationary) noise levels. However, most of the existing MRI denoising methods rely on a stationary noise model and end with suboptimal results when applied to pMRI images. To address this problem, this paper proposes an improved nonlocal maximum likelihood (NLML) estimation method. In the proposed method, a noise map is computed with a robust noise estimator before the ML estimation of the underlying signal. Also, a similarity measure based on local frequency descriptors (LFD) is introduced to find the nonlocal samples for ML estimation. The experiments on simulated and real magnetic resonance (MR) data demonstrate that the proposed technique has superior filtering capabilities in terms of subjective and quantitative assessments when compared with other state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518300424",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Estimation theory",
      "Estimator",
      "Image (mathematics)",
      "Mathematics",
      "Maximum likelihood",
      "Noise (video)",
      "Noise reduction",
      "Pattern recognition (psychology)",
      "Signal-to-noise ratio (imaging)",
      "Similarity (geometry)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Sudeep",
        "given_name": "P.V."
      },
      {
        "surname": "Palanisamy",
        "given_name": "P."
      },
      {
        "surname": "Kesavadas",
        "given_name": "Chandrasekharan"
      },
      {
        "surname": "Rajan",
        "given_name": "Jeny"
      }
    ]
  },
  {
    "title": "Binary coyote optimization algorithm for feature selection",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107470",
    "abstract": "The Coyote Optimization Algorithm (COA) is a bio-inspired optimization algorithm based on the intelligent behavior of coyotes. COA was proposed recently and it considers the social organization of the coyotes and its adaptation to the environment in order to solve continuous optimization problems. In addition, it is a population-based algorithm and it can be classified as both, swarm intelligence and evolutionary heuristics, because contributes with a different algorithmic structure. This paper proposes a binary version of the COA, named Binary COA (BCOA) applying to select the optimal feature subset for classification, based on the hyperbolic transfer function in a wrapper model. By this way, the features are selected based on the performance evaluation of a classification algorithm. We tested the effectiveness of the BCOA wrapper with the Naïve Bayes classifier and were used seven public domain benchmark datasets to compare the proposed approach in terms of classification accuracy, number of selected features and computational cost with other state-of-art algorithms of the literature. The results shown that BCOA was able to find subsets with few features while it still performs well in terms of classification accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302739",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Binary classification",
      "Binary number",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Demography",
      "Feature selection",
      "Geodesy",
      "Geography",
      "Heuristics",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Naive Bayes classifier",
      "Operating system",
      "Optimization algorithm",
      "Pattern recognition (psychology)",
      "Population",
      "Sociology",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Thom de Souza",
        "given_name": "Rodrigo Clemente"
      },
      {
        "surname": "de Macedo",
        "given_name": "Camila Andrade"
      },
      {
        "surname": "dos Santos Coelho",
        "given_name": "Leandro"
      },
      {
        "surname": "Pierezan",
        "given_name": "Juliano"
      },
      {
        "surname": "Mariani",
        "given_name": "Viviana Cocco"
      }
    ]
  },
  {
    "title": "Adaptive core fusion-based density peak clustering for complex data with arbitrary shapes and densities",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107452",
    "abstract": "A challenging issue of clustering in real-word application is to detect clusters with arbitrary shapes and densities in complex data. Many conventional clustering algorithms are capable of detecting non-spherical clusters, but their performance is limited when processing data with complex shapes and multiple density peaks in a cluster without knowing the number of clusters. This paper proposes an adaptive core fusion-based density peak clustering (CFDPC) for detecting clusters in any shape and density adaptively. An initial clustering based on automatic finding of density peaks is proposed first. An adaptive searching approach is then proposed to find core points, and a within-cluster similarity-based core fusion strategy is proposed to obtain the final clustering results. The CFDPC where the number of clusters arises intuitively is simple and efficient. The performance of CFDPC is successfully verified in clustering several benchmark complex datasets with diverse shapes and densities.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302557",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "CURE data clustering algorithm",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Computer science",
      "Core (optical fiber)",
      "Correlation clustering",
      "DBSCAN",
      "Data mining",
      "Determining the number of clusters in a data set",
      "Fusion",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Similarity (geometry)",
      "Single-linkage clustering",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Fang",
        "given_name": "Fang"
      },
      {
        "surname": "Qiu",
        "given_name": "Lei"
      },
      {
        "surname": "Yuan",
        "given_name": "Shenfang"
      }
    ]
  },
  {
    "title": "Dynamic time warping-based imputation for univariate time series data",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2017.08.019",
    "abstract": "Time series with missing values occur in almost any domain of applied sciences. Ignoring missing values can lead to a loss of efficiency and unreliable results, especially for large missing sub-sequence(s). This paper proposes an approach to fill in large gap(s) within time series data under the assumption of effective information. To obtain the imputation of missing values, we find the most similar sub-sequence to the sub-sequence before (resp. after) the missing values, then complete the gap by the next (resp. previous) sub-sequence of the most similar one. Dynamic Time Warping algorithm is applied to compare sub-sequences, and combined with the shape-feature extraction algorithm for reducing insignificant solutions. Eight well-known and real-world data sets are used for evaluating the performance of the proposed approach in comparison with five other methods on different indicators. The obtained results proved that the performance of our approach is the most robust one in case of time series data having high auto-correlation and cross-correlation, strong seasonality, large gap(s), and complex distribution.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865517302751",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Dynamic time warping",
      "Genetics",
      "Imputation (statistics)",
      "Mathematics",
      "Missing data",
      "Multivariate statistics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Sequence (biology)",
      "Series (stratigraphy)",
      "Statistics",
      "Time domain",
      "Time series",
      "Univariate"
    ],
    "authors": [
      {
        "surname": "Phan",
        "given_name": "Thi-Thu-Hong"
      },
      {
        "surname": "Poisson Caillault",
        "given_name": "Émilie"
      },
      {
        "surname": "Lefebvre",
        "given_name": "Alain"
      },
      {
        "surname": "Bigand",
        "given_name": "André"
      }
    ]
  },
  {
    "title": "Generative adversarial classifier for handwriting characters super-resolution",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107453",
    "abstract": "Generative Adversarial Networks (GAN) receive great attention recently due to its excellent performance in image generation, transformation, and super-resolution. However, less emphasis or study has been put on GAN for classification with super-resolution. Moreover, though GANs may fabricate images which perceptually looks realistic, they usually fabricate some fake details especially in character data; this would impose further difficulties when they are input for classification. In this paper, we propose a novel Generative Adversarial Classifier (GAC) for low-resolution handwriting character recognition. Specifically, we design an additional classifier component in GAC, leading to a novel three-player GAN model which is not only able to generate high-quality super-resolved images, but also favorable for classification. Experimental results show that our proposed method can obtain remarkable performance in handwriting characters with 8 × super-resolution, achieving new state-of-the-art on benchmark dataset CASIA-HWDB1.1, and MNIST.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302569",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Character (mathematics)",
      "Chemistry",
      "Classifier (UML)",
      "Computer science",
      "Deep learning",
      "Gene",
      "Generative adversarial network",
      "Generative grammar",
      "Geodesy",
      "Geography",
      "Geology",
      "Geometry",
      "Handwriting",
      "High resolution",
      "Low resolution",
      "MNIST database",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Remote sensing",
      "Salient",
      "Speech recognition",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Qian",
        "given_name": "Zhuang"
      },
      {
        "surname": "Huang",
        "given_name": "Kaizhu"
      },
      {
        "surname": "Wang",
        "given_name": "Qiu-Feng"
      },
      {
        "surname": "Xiao",
        "given_name": "Jimin"
      },
      {
        "surname": "Zhang",
        "given_name": "Rui"
      }
    ]
  },
  {
    "title": "Facial expressions classification and false label reduction using LDA and threefold SVM",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2017.06.021",
    "abstract": "Representation and classification of multi-dimensional data are current key research areas. The representation of data in two classes is more feasible than multi-class representations because of the inherent quadratic complexity in existing techniques. Erroneous assignment of class labels affects separation boundary and training time complexity. In this paper, multi-dimensional data is handled using linear discriminant analysis (LDA) and threefold support vector machine (SVM) techniques to reduce the complexity and minimize false labeling. A facial expression application is proposed in which six natural expressions are used as multi-class data. Face image is divided into seven triangles on the basis of two focal points. A combined local and global feature descriptor is generated. Discrete Fourier transform is applied and processed with LDA to obtain discriminant features and accurately map an input feature space to an output space. To evaluate the system performance, Japanese Female Facial Expression, FER-2013 and Cohn–Kanade DFAT datasets are used. The obtained results show that multi-class data hyper plane using LDA and threefold SVM approach is effective and simple for quadratic data analysis",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865517302271",
    "keywords": [
      "Artificial intelligence",
      "Boundary (topology)",
      "Computer science",
      "Decision boundary",
      "Dimensionality reduction",
      "Discriminant",
      "Face (sociological concept)",
      "Facial expression",
      "Facial recognition system",
      "Feature (linguistics)",
      "Feature vector",
      "Geometry",
      "Linear discriminant analysis",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Quadratic equation",
      "Reduction (mathematics)",
      "Social science",
      "Sociology",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Shah",
        "given_name": "Jamal Hussain"
      },
      {
        "surname": "Sharif",
        "given_name": "Muhammad"
      },
      {
        "surname": "Yasmin",
        "given_name": "Mussarat"
      },
      {
        "surname": "Fernandes",
        "given_name": "Steven Lawrence"
      }
    ]
  },
  {
    "title": "Cost-sensitive deep forest for price prediction",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107499",
    "abstract": "For many real-world applications, predicting a price range is more practical and desirable than predicting a concrete value. In this case, price prediction can be regarded as a classification problem. Although deep forest is recognized as the best solution to many classification problems, a crucial issue limits its direct application to price prediction, i.e., it treated all the misclassifications equally no matter how far away they are from the real classes, since their impacts on the accuracy are the same. This is unreasonable to price prediction as the misclassification should be as close to the real price range as possible even if they have to be wrongly classified. To address this issue, we propose a cost-sensitive deep forest for price prediction, which maintains the high accuracy of deep forest, and propels the misclassifications to be closer to the real price range to reduce the cost of misclassifications. To make the classification more meaningful, we develop a discretization method to pre-define the classes of price, by modifying the conventional K-means method. The experimental results based on multiple real-world datasets (i.e., car sharing, house renting and real estate selling) show that, the cost-sensitive deep forest can significantly reduce the cost in comparison with the conventional deep forest and other baselines, while keeping satisfactory accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303022",
    "keywords": [
      "Artificial intelligence",
      "Composite material",
      "Computer science",
      "Data mining",
      "Econometrics",
      "Economics",
      "Finance",
      "Machine learning",
      "Materials science",
      "Random forest",
      "Range (aeronautics)",
      "Real estate"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Chao"
      },
      {
        "surname": "Liu",
        "given_name": "Zhenbing"
      },
      {
        "surname": "Cao",
        "given_name": "Zhiguang"
      },
      {
        "surname": "Song",
        "given_name": "Wen"
      },
      {
        "surname": "Zhang",
        "given_name": "Jie"
      },
      {
        "surname": "Zeng",
        "given_name": "Weiliang"
      }
    ]
  },
  {
    "title": "Learning discriminative features via weights-biased softmax loss",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107405",
    "abstract": "Loss functions play a key role in training superior deep neural networks. In convolutional neural networks (CNNs), the popular cross entropy loss together with softmax does not explicitly guarantee minimization of intra-class variance or maximization of inter-class variance. In the early studies, there is no theoretical analysis and experiments explicitly indicating how to choose the number of units in fully connected layer. To help CNNs learn features more fast and discriminative, there are two contributions in this paper. First, we determine the minimum number of units in FC layer by rigorous theoretical analysis and extensive experiment, which reduces CNNs’ parameter memory and training time. Second, we propose a negative-focused weights-biased softmax (W-Softmax) loss to help CNNs learn more discriminative features. The proposed W-Softmax loss not only theoretically formulates the intra-class compactness and inter-class separability, but also can avoid overfitting by enlarging decision margins. Moreover, the size of decision margins can be flexibly controlled by adjusting a hyperparameter α. Extensive experimental results on several benchmark datasets show the superiority of W-Softmax in image classification tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302089",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Discriminative model",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Softmax function"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xiaobin"
      },
      {
        "surname": "Wang",
        "given_name": "Weiqiang"
      }
    ]
  },
  {
    "title": "Multiple strong and balanced cluster-based ensemble of deep learners",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107420",
    "abstract": "Convolutional Neural Networks (CNNs), also known as deep learners have seen much success in the last few years due to the availability of large amounts of data and high-performance computational resources. A CNN can be trained effectively if large amounts of data are available as it enables a CNN to find the optimal set of features and weights that can achieve the highest generalization performance. However, due to the requirement of large data size, CNNs require a lot of resources for example running time and computational resources to achieve a reasonable performance. Additionally, unbalanced data makes it difficult to train a CNN effectively that can achieve good generalization performance. In order to alleviate these limitations, in this paper, we propose a novel ensemble of deep learners that learns by combining multiple deep learners trained on small strongly class associated input data effectively. We propose a novel methodology of generating random subspace through clustering input data and propose a measure which can classify each cluster as a strong data cluster and a balanced data cluster. A methodology is also proposed that balances all strong data clusters in the pool so that an architecturally simple CNN can be trained on all balanced data clusters simultaneously. Classification decisions on all trained CNNs are then fused through majority voting to generate class decisions of the ensemble. The performance of the proposed ensemble approach is evaluated on UCI benchmark datasets, and results are compared with existing state-of-the-art ensemble approaches. Significance testing was conducted to further validate the efficacy of the results and a significance test analysis is presented.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302235",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Class (philosophy)",
      "Cluster analysis",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Data set",
      "Deep learning",
      "Ensemble forecasting",
      "Ensemble learning",
      "Generalization",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Jan",
        "given_name": "Zohaib"
      },
      {
        "surname": "Verma",
        "given_name": "Brijesh"
      }
    ]
  },
  {
    "title": "Learning to complete partial observations from unpaired prior knowledge",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107426",
    "abstract": "We present a novel training strategy that allows convolutional encoder-decoder networks, to complete partially observed data by means of hallucination. As input, it takes data from a partially observed domain, for which no complete ground truth is available, and data from an unpaired prior knowledge domain and trains the network in an end-to-end manner. This strategy is demonstrated for the task of completing 2-D road layouts as well as 3-D vehicle shapes. In contrast to alternative approaches, our strategy is compatible with networks that use skip connections, to improve detail in the completed output, while not requiring adversarial supervision. To demonstrate its benefits, our training strategy is benchmarked against two state-of-the-art baselines, one using a two-step auto-encoder training strategy and one using an adversarial strategy. Our novel strategy achieves an improvement up to +12% F-measure on the Cityscapes dataset. The learned network intrinsically generalizes better than the baselines on unseen datasets, which is demonstrated by an improvement up to +24% F-measure on the unseen KITTI dataset. Moreover, our approach outperforms the baselines using the same backbone network on the 3-D shape completion benchmark by reducing the Hamming distance with 15%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302296",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Backbone network",
      "Benchmark (surveying)",
      "Computer network",
      "Computer science",
      "Data mining",
      "Domain (mathematical analysis)",
      "Domain knowledge",
      "Economics",
      "Encoder",
      "Encoding (memory)",
      "Geodesy",
      "Geography",
      "Ground truth",
      "Hamming distance",
      "Image (mathematics)",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Measure (data warehouse)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Chenyang"
      },
      {
        "surname": "Dubbelman",
        "given_name": "Gijs"
      }
    ]
  },
  {
    "title": "Attention and boundary guided salient object detection",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107484",
    "abstract": "In recent years, fully convolutional neural network (FCN) has broken all records in various vision task. It also achieves great performance in salient object detection. However, most of the state-of-the-art methods have suffered from the challenge of precisely segmenting the entire salient object with uniform region and explicit boundary and effectively suppressing the backgrounds on complex images. There is still a large room for improvement over the FCN-based saliency detection approaches. In this paper, we propose an attention and boundary guided deep neural network for salient object detection to better locate and segment the salient objects with uniform interior and explicit boundary. A channel-wise attention module is utilized to emphasize the important regions, which selects the important feature channels and assigns large weights to them. A boundary information localization module is proposed for suppressing the irrelevant boundary information to better locate and explore the useful structure of objects. The proposed approach achieves state-of-the-art performance on four well-known benchmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302879",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Boundary (topology)",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Engineering",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Salient",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Qing"
      },
      {
        "surname": "Shi",
        "given_name": "Yanjiao"
      },
      {
        "surname": "Zhang",
        "given_name": "Xueqin"
      }
    ]
  },
  {
    "title": "Automated diagnosis of epilepsy from EEG signals using ensemble learning approach",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2017.05.021",
    "abstract": "Epilepsy is generally considered as a collection of neurological disorders manifested by epileptic seizures. It is often confirmed with an electroencephalogram (EEG). EEG signals are non stationary, nonlinear and non Gaussian. In this paper, in order to tackle this problem, we have used three different methods for feature extraction namely wavelet based entropy (approximation entropy, sample entropy, permutation entropy), nonlinear features (Hurst exponent, Higuchi Fractal Dimension) and higher order spectra (mean, normalized entropy-1 and normalized entropy-2). Further multiclass classification using indirect approach with One vs One method is employed using heterogenous ensemble learning approach. Entropy features are used for classifying normal and interictal class using k-Nearest Neighbor. Higher Order Spectra features are used for classifying normal and ictal class using Support Vector Machine with Radial basis function as kernel and non linear features are used for classifying interictal and ictal class using Naive Bayes. Final verdict is taken by meta classifier with meta learning algorithm Stacking Correspondence Analysis and Nearest Neighbor (SCANN). The proposed method surpasses the existing methods in literature in terms of sensitivity and specificity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865517301691",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Electroencephalography",
      "Entropy (arrow of time)",
      "Hurst exponent",
      "Ictal",
      "Mathematics",
      "Naive Bayes classifier",
      "Pattern recognition (psychology)",
      "Physics",
      "Psychiatry",
      "Psychology",
      "Quantum mechanics",
      "Sample entropy",
      "Statistics",
      "Support vector machine",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Abdulhay",
        "given_name": "Enas"
      },
      {
        "surname": "V.",
        "given_name": "Elamaran"
      },
      {
        "surname": "M.",
        "given_name": "Chandrasekar"
      },
      {
        "surname": "V.S.",
        "given_name": "Balaji"
      },
      {
        "surname": "K.",
        "given_name": "Narasimhan"
      }
    ]
  },
  {
    "title": "Self-adaptive manifold discriminant analysis for feature extraction from hyperspectral imagery",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107487",
    "abstract": "Traditional manifold learning methods generally include a single projection stage that maps high-dimensional data into lower-dimensional space. However, these methods cannot guarantee that the projection matrix is optimal for classification, which limits their practical application. To address this issue, we propose a two-stage projection matrix optimization model termed self-adaptive manifold discriminant analysis (SAMDA). In pre-training projection stage, SAMDA obtains an initial projection matrix by constructing an interclass graph and an intraclass graph under the graph embedding (GE) framework. In weight optimization stage, a maximal manifold margin criterion is developed to further optimize the weights of projection matrix by feature similarity. A self-adaptive optimization process is introduced to increase the margins among different manifolds in low-dimensional space and extract discriminant features that are beneficial to classification. Experimental results on PaviaU, Indian Pines and Heihe data sets demonstrate that the proposed SAMDA method can achieve better classification results than some state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302909",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Dimensionality reduction",
      "Discriminant",
      "Feature extraction",
      "Feature vector",
      "Linear discriminant analysis",
      "Mathematics",
      "Nonlinear dimensionality reduction",
      "Pattern recognition (psychology)",
      "Projection (relational algebra)"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Hong"
      },
      {
        "surname": "Li",
        "given_name": "Zhengying"
      },
      {
        "surname": "He",
        "given_name": "Haibo"
      },
      {
        "surname": "Duan",
        "given_name": "Yule"
      },
      {
        "surname": "Yang",
        "given_name": "Song"
      }
    ]
  },
  {
    "title": "Neural network with multiple connection weights",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107481",
    "abstract": "Biological studies have shown that the interaction between neurons are based on neurotransmitters, which transmit signals between neurons, and that one neuron sends information to another neuron by releasing a number of different neurotransmitters, which play different roles. Motivated by this biological discovery, a novel neural networks model is proposed by extending the dimension of connection weights from one to multiple, i.e. there are multiple not only one connections between each two units. The number of dimensions of connection weight represents the number of categories of neurotransmitters and different components of the weight correspond to different neurotransmitters. In order to make these neurotransmitters collaborate and compete appropriately, the input and output for each unit in our proposed model have been heuristically defined. From the biological perspective, the proposed neural network is much closer to biological neural network. From the viewpoint of new model structure, the characteristic that the activation of each hidden unit is based on several filters, can improve the interpretability of features learned by the proposed neural network. Experimental results on MNIST, NORB and several other data sets have demonstrated that the performances of traditional neural networks can be improved by extending the dimension of connection weight between units, and the idea of multiple connection weights provides a new paradigm for the design of neural networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302843",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biological neural network",
      "Computer science",
      "Connection (principal bundle)",
      "Dimension (graph theory)",
      "Geometry",
      "Interpretability",
      "MNIST database",
      "Machine learning",
      "Mathematics",
      "Pure mathematics"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Jiangshe"
      },
      {
        "surname": "Hu",
        "given_name": "Junying"
      },
      {
        "surname": "Liu",
        "given_name": "Junmin"
      }
    ]
  },
  {
    "title": "3DSymm: Robust and Accurate 3D Reflection Symmetry Detection",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107483",
    "abstract": "Reflection symmetry is a very commonly occurring feature in both natural and man-made objects, which helps in understanding objects better and makes them visually pleasing. Detection of reflection symmetry is a fundamental problem in the field of computer vision and computer graphics which aids in understanding and representing reflective symmetric objects. In this work, we attempt the problem of detecting the 3D global reflection symmetry of a 3D object represented as a point cloud. The main challenge is to handle outliers, missing parts, and perturbations from the perfect reflection symmetry. We propose a descriptor-free approach, in which, we pose the problem of reflection symmetry detection as an optimization problem and provide a closed-form solution. We show that the proposed method achieves state-of-the-art performance on the standard dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302867",
    "keywords": [
      "Artificial intelligence",
      "Computer graphics",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Object (grammar)",
      "Outlier",
      "Philosophy",
      "Point (geometry)",
      "Point cloud",
      "Programming language",
      "Reflection (computer programming)",
      "Reflection symmetry",
      "Symmetry (geometry)"
    ],
    "authors": [
      {
        "surname": "Nagar",
        "given_name": "Rajendra"
      },
      {
        "surname": "Raman",
        "given_name": "Shanmuganathan"
      }
    ]
  },
  {
    "title": "A novel hybrid approach for crack detection",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107474",
    "abstract": "Vision-based crack detection is of crucial importance in various industries, and it is very challenging due to weak signals in noisy backgrounds. In this paper, we propose a novel hybrid approach for crack detection in raw images, which combines deep learning models and Bayesian probabilistic analysis for robust crack detection. First, we re-train a state-of-the-art object detector (e.g. a Faster R-CNN) to detect crack patches of suitable SNR (signal-noise-ratio). We design a semi-automatic method to generate ground truths of crack patches along crack lines for training. To further improve the accuracy of crack detections over the whole image, we propose a Bayesian integration algorithm to suppress false detections. Specifically, we use a deep CNN to recognize the orientation of the crack segment in each detected patch. Then, a Bayesian probability is computed on the accumulated evidence from detected adjacent patches within a neighborhood based on spatial proximity, orientation consistency and alignment consistency. The patch which lacks local supports is suppressed as false detection. An algorithm to learn the parameters of Bayesian integration is also derived. Extensive experiments and evaluations are performed on a new comprehensive dataset of crack images. The results show that our approach outperforms the state-of-the-art baseline approach on deep CNN classifier. Ablation experiments are also conducted to show the effectiveness of proposed techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302776",
    "keywords": [
      "Artificial intelligence",
      "Bayesian probability",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Geometry",
      "Mathematics",
      "Object detection",
      "Orientation (vector space)",
      "Pattern recognition (psychology)",
      "Probabilistic logic"
    ],
    "authors": [
      {
        "surname": "Fang",
        "given_name": "Fen"
      },
      {
        "surname": "Li",
        "given_name": "Liyuan"
      },
      {
        "surname": "Gu",
        "given_name": "Ying"
      },
      {
        "surname": "Zhu",
        "given_name": "Hongyuan"
      },
      {
        "surname": "Lim",
        "given_name": "Joo-Hwee"
      }
    ]
  },
  {
    "title": "Counter-examples generation from a positive unlabeled image dataset",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107527",
    "abstract": "This paper considers the problem of positive unlabeled (PU) learning. In this context, we propose a two-stage GAN-based model. More specifically, the main contribution is to incorporate a biased PU risk within the standard GAN discriminator loss function. In this manner, the discriminator is constrained to steer the generator to converge towards the unlabeled samples distribution while diverging from the positive samples distribution. Consequently, the proposed model, referred to as D-GAN, exclusively learns the counter-examples distribution without prior knowledge. Experimental results on simple and complex image datasets demonstrate that our approach outperforms state-of-the-art PU methods without prior by overcoming issues such as sensitivity to prior knowledge or first-stage overfitting.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303307",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Detector",
      "Discriminator",
      "Epistemology",
      "Generator (circuit theory)",
      "Image (mathematics)",
      "Machine learning",
      "Overfitting",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Simple (philosophy)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Chiaroni",
        "given_name": "Florent"
      },
      {
        "surname": "Khodabandelou",
        "given_name": "Ghazaleh"
      },
      {
        "surname": "Rahal",
        "given_name": "Mohamed-Cherif"
      },
      {
        "surname": "Hueber",
        "given_name": "Nicolas"
      },
      {
        "surname": "Dufaux",
        "given_name": "Frederic"
      }
    ]
  },
  {
    "title": "A robust matching pursuit algorithm using information theoretic learning",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107415",
    "abstract": "Current orthogonal matching pursuit (OMP) algorithms calculate the correlation between two vectors using the inner product operation and minimize the mean square error, which are both suboptimal when there are non-Gaussian noises or outliers in the observation data. To overcome these problems, a new OMP algorithm is developed based on information theoretic learning (ITL), which is built on the following new techniques: (1) an ITL-based correlation (ITL-Correlation) is developed as a new similarity measure which can better exploit higher-order statistics of the data, and is robust against many different types of noise and outliers in a sparse representation framework; (2) a non-second order statistic measurement and minimization method is developed to improve the robustness of OMP by overcoming the limitation of Gaussianity inherent in a cost function based on second-order moments. The experimental results on both simulated and real-world data consistently demonstrate the superiority of the proposed OMP algorithm in data recovery, image reconstruction, and classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302181",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Compressed sensing",
      "Computer science",
      "Gaussian",
      "Gene",
      "Matching pursuit",
      "Mathematical optimization",
      "Mathematics",
      "Minification",
      "Outlier",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Robustness (evolution)",
      "Sparse approximation",
      "Statistic",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Miaohua"
      },
      {
        "surname": "Gao",
        "given_name": "Yongsheng"
      },
      {
        "surname": "Sun",
        "given_name": "Changming"
      },
      {
        "surname": "Blumenstein",
        "given_name": "Michael"
      }
    ]
  },
  {
    "title": "Marker controlled watershed transform for intra-retinal cysts segmentation from optical coherence tomography B-scans",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2017.12.019",
    "abstract": "Retinal cysts have pathological significance in several eye disorders. Detecting and quantifying such cysts from optical coherence tomography (OCT) scans is currently tedious and requires expertise. To aid the diagnostic process, an automatic intra-retinal cyst segmentation method using marker-controlled watershed transform on OCT B-scans is proposed in this paper. The proposed method is based on two stages – k-means clustering technique is used to identify cysts in the form of markers, followed by topographical based watershed transform for final segmentation. Qualitative and quantitative evaluation of proposed method was carried out against ground truth obtained from two graders on OPTIMA cyst segmentation challenge dataset. This method efficiently segments cystic structures with mean recall and precision rate 0.67 and 0.78, respectively, while preserving high correlation coefficient of 0.95 against ground truth obtained from both graders. Obtained results show that the proposed method outperformed other existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865517304658",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Ground truth",
      "Medicine",
      "Ophthalmology",
      "Optical coherence tomography",
      "Pattern recognition (psychology)",
      "Precision and recall",
      "Radiology",
      "Retinal",
      "Segmentation",
      "Watershed"
    ],
    "authors": [
      {
        "surname": "Girish",
        "given_name": "G.N."
      },
      {
        "surname": "R. Kothari",
        "given_name": "Abhishek"
      },
      {
        "surname": "Rajan",
        "given_name": "Jeny"
      }
    ]
  },
  {
    "title": "On the stability of persistent entropy and new summary functions for topological data analysis",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107509",
    "abstract": "Persistent homology and persistent entropy have recently become useful tools for patter recognition. In this paper, we find requirements under which persistent entropy is stable to small perturbations in the input data and scale invariant. In addition, we describe two new stable summary functions combining persistent entropy and the Betti curve. Finally, we use the previously defined summary functions in a material classification task to show their usefulness in machine learning and pattern recognition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303125",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Betti number",
      "Combinatorics",
      "Computer science",
      "Discrete mathematics",
      "Entropy (arrow of time)",
      "Invariant (physics)",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Persistent homology",
      "Physics",
      "Principle of maximum entropy",
      "Quantum mechanics",
      "Topological data analysis",
      "Topological entropy",
      "Topology (electrical circuits)",
      "Transfer entropy"
    ],
    "authors": [
      {
        "surname": "Atienza",
        "given_name": "Nieves"
      },
      {
        "surname": "Gonzalez-Díaz",
        "given_name": "Rocio"
      },
      {
        "surname": "Soriano-Trigueros",
        "given_name": "Manuel"
      }
    ]
  },
  {
    "title": "Hierarchical dense recursive network for image super-resolution",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107475",
    "abstract": "Image super-resolution (SR) techniques with deep convolutional network (CNN) have achieved significant improvements compared to previous shallow-learning-based methods. Especially for dense connection based networks, these methods have yielded unprecedented achievements but bring the higher complexity and more parameters. To this end, this paper considers both reconstruction performance and efficiency, and advocates a novel hierarchical dense connection network (HDN) for image SR. First of all, we construct a hierarchical dense residual block (HDB) to promote the feature representation while saving the memory footprint with a hierarchical matrix structure design. In this way, it can provide additional interleaved pathways for information fusion and gradient optimization but with a shallower depth compare to the previous networks. In particular, a group of convolutional layers with small size (1 × 1) are embedded in HDB, releasing the computational burden and parameters by rescaling the feature dimensions. Furthermore, HDBs are connected to each other in a sharing manner, thereby allowing the network to fuse the features in different stages. At the final, the multi-scale features from these HDBs are integrated into global fusion module (GFM) for a global fusion and representation, and then the final profile-enriched residual map is obtained by realigning and sub-pixel upsampling the fusion maps. Extensive experimental results on benchmark datasets and really degraded images show that our model outperforms the state-of-the-art methods in terms of quantitative indicators and realistic visual effects, as well as enjoys a fast and accurate reconstruction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302788",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Block (permutation group theory)",
      "Computer science",
      "Convolutional neural network",
      "Electrical engineering",
      "Engineering",
      "Feature (linguistics)",
      "Fuse (electrical)",
      "Geodesy",
      "Geography",
      "Geometry",
      "Image (mathematics)",
      "Law",
      "Linguistics",
      "Mathematics",
      "Memory footprint",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Residual",
      "Upsampling"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Kui"
      },
      {
        "surname": "Wang",
        "given_name": "Zhongyuan"
      },
      {
        "surname": "Yi",
        "given_name": "Peng"
      },
      {
        "surname": "Jiang",
        "given_name": "Junjun"
      }
    ]
  },
  {
    "title": "Gesture recognition based on deep deformable 3D convolutional neural networks",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107416",
    "abstract": "Dynamic gesture recognition, which plays an essential role in human-computer interaction, has been widely investigated but not yet fully addressed. The challenge mainly lies in three folders: 1) to model both of the spatial appearance and the temporal evolution simultaneously; 2) to address the interference from the varied and complex background; 3) the requirement of real-time processing. In this paper, we address the above challenges by proposing a novel deep deformable 3D convolutional neural network for end-to-end learning, which not only gains impressive accuracy in challenging datasets but also can meet the requirement of the real-time processing. We propose three types of very deep 3D CNNs for gesture recognition, which can directly model the spatiotemporal information with their inherent hierarchical structure. To eliminate the background interference, a light-weight spatiotemporal deformable convolutional module is specially designed to augment the spatiotemporal sampling locations of the 3D convolution by learning additional offsets according to the preceding feature map. It can not only diversify the shape of the convolution kernel to better fit the appearance of the hands and arms, but also help the models pay more attention to the discriminative frames in the video sequence. The proposed method is evaluated on three challenging datasets, EgoGesture, Jester and Chalearn-IsoGD, and achieves the state-of-the-art performance on all of them. Our model ranked first on Jester’s official leader-board until the submission time. The code and the trained models are released for better communication and future works 1 1 https://github.com/lshiwjx/deform_conv3d_pytorch_op .",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302193",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Deep learning",
      "Discriminative model",
      "Feature (linguistics)",
      "Gesture",
      "Gesture recognition",
      "Kernel (algebra)",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yifan"
      },
      {
        "surname": "Shi",
        "given_name": "Lei"
      },
      {
        "surname": "Wu",
        "given_name": "Yi"
      },
      {
        "surname": "Cheng",
        "given_name": "Ke"
      },
      {
        "surname": "Cheng",
        "given_name": "Jian"
      },
      {
        "surname": "Lu",
        "given_name": "Hanqing"
      }
    ]
  },
  {
    "title": "A novel segmentation technique for online handwritten Bangla words",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.02.008",
    "abstract": "In the present work, we have proposed a novel Bangla word segmentation technique that is based on stroke-level busy zone formation procedure. In an unconstrained domain, people often write text where strokes may be poorly aligned (due to multi-directional skewness) and varied combination of strokes with various types of joining between them are possible while forming the words. Hence, a segmentation approach for stroke extraction is pertinent for any stroke-based word recognition system. The presence of a large volume of symbols set (58 basic symbols with more than 280 compound characters) in Bangla script makes the task more challenging. In the current experiment, our stroke-level segmentation approach effectively handles such type of Bangla words. A sub-zoning scheme within busy zone followed by a modified Down->Up->Down (DUD) concept within these sub-zones has been used to find valid segmentation points. This scheme avoids over and under-segmentation issues caused by either inherent writing pattern or due to writing style variations up to certain extent. The proposed segmentation approach has been tested on 6500 online handwritten Bangla word samples with 98.45% correct segmentation accuracy (compared with manually generated ground truth of the same database).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518300436",
    "keywords": [
      "Artificial intelligence",
      "Bengali",
      "Computer science",
      "Domain (mathematical analysis)",
      "Economics",
      "Geometry",
      "Image segmentation",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Scheme (mathematics)",
      "Segmentation",
      "Speech recognition",
      "Task (project management)",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Sen",
        "given_name": "Shibaprasad"
      },
      {
        "surname": "Chowdhury",
        "given_name": "Shubham"
      },
      {
        "surname": "Mitra",
        "given_name": "Mridul"
      },
      {
        "surname": "Schwenker",
        "given_name": "Friedhelm"
      },
      {
        "surname": "Sarkar",
        "given_name": "Ram"
      },
      {
        "surname": "Roy",
        "given_name": "Kaushik"
      }
    ]
  },
  {
    "title": "Experiments using deep learning for dermoscopy image analysis",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2017.11.005",
    "abstract": "Skin cancer is a major public health problem, as is the most common type of cancer and represents more than half of cancer diagnosed worldwide. Early detection influences the outcome of the disease and motivates the research presented in this paper. Recent results show that deep learning based approaches learn from data, and can outperform human specialists in a set of tasks when large databases are available for training. This research investigates the scenario where the amount of data available for training is small. It obtains relevant results for the ISBI 2016 melanoma classification challenge (named Skin Lesion Analysis for Melanoma Detection) facing the peculiarities of dealing with such a small and unbalanced biological database. To do this, it explores committees of Deep Convolutional Neural Networks (DCNN), the augmentation of the training data set by image processing classical transforms and by deformations guided by expert knowledge about the lesion axis, and it introduces a third class aiming to improve the classifiers’ distinction of the region of interest of the lesion. The experiments show that the proposed approach improves the final classifier invariance for common melanoma variations, common skin patterns and markers, and dermatoscope capturing conditions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865517304117",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Cancer",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Contextual image classification",
      "Convolutional neural network",
      "Deep learning",
      "Image (mathematics)",
      "Internal medicine",
      "Machine learning",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Skin cancer",
      "Skin lesion",
      "Training set"
    ],
    "authors": [
      {
        "surname": "Vasconcelos",
        "given_name": "Cristina Nader"
      },
      {
        "surname": "Vasconcelos",
        "given_name": "Bárbara Nader"
      }
    ]
  },
  {
    "title": "Cylinders extraction in non-oriented point clouds as a clustering problem",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107443",
    "abstract": "Finding geometric primitives in 3D point clouds is a fundamental task in many engineering applications such as robotics, autonomous-vehicles and automated industrial inspection. Among all solid shapes, cylinders are frequently found in a variety of scenes, comprising natural or man-made objects. Despite their ubiquitous presence, automated extraction and fitting can become challenging if performed ”in-the-wild”, when the number of primitives is unknown or the point cloud is noisy and not oriented. In this paper we pose the problem of extracting multiple cylinders in a scene by means of a Game-Theoretic inlier selection process exploiting the geometrical relations between pairs of axis candidates. First, we formulate the similarity between two possible cylinders considering the rigid motion aligning the two axes to the same line. This motion is represented with a unitary dual-quaternion so that the distance between two cylinders is induced by the length of the shortest geodesic path in SE(3). Then, a Game-Theoretical process exploits such similarity function to extract sets of primitives maximizing their inner mutual consensus. The outcome of the evolutionary process consists in a probability distribution over the sets of candidates (ie axes), which in turn is used to directly estimate the final cylinder parameters. An extensive experimental section shows that the proposed algorithm offers a high resilience to noise, since the process inherently discards inconsistent data. Compared to other methods, it does not need point normals and does not require a fine tuning of multiple parameters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302466",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Geodesic",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Operating system",
      "Point (geometry)",
      "Point cloud",
      "Process (computing)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Bergamasco",
        "given_name": "Filippo"
      },
      {
        "surname": "Pistellato",
        "given_name": "Mara"
      },
      {
        "surname": "Albarelli",
        "given_name": "Andrea"
      },
      {
        "surname": "Torsello",
        "given_name": "Andrea"
      }
    ]
  },
  {
    "title": "Semantic segmentation using stride spatial pyramid pooling and dual attention decoder",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107498",
    "abstract": "Semantic segmentation is an end-to-end task that requires both semantic and spatial accuracy. It is important for deep learning-based segmentation methods to effectively utilize the high-level feature map whose semantic information is abundant and the low-level feature map whose spatial information is accurate. However, existing segmentation networks typically cannot take full advantage of these two kinds of feature maps, leading to inferior performance. This paper attempts to overcome this challenge by introducing two novel structures. On the one hand, we propose a structure called stride spatial pyramid pooling (SSPP) to capture multiscale semantic information from the high-level feature map. Compared with existing pyramid pooling methods based on the atrous convolution, the SSPP structure is able to gather more information from the high-level feature map with faster inference speed, which improves the utilization efficiency of the high-level feature map significantly. On the other hand, we propose a dual attention decoder consisting of a channel attention branch and a spatial attention branch to make full use of the high- and low-level feature maps simultaneously. The dual attention decoder can result in a more “semantic” low-level feature map and a high-level feature map with more accurate spatial information, which bridges the gap between these two kinds of feature maps and benefits their fusion. We evaluate the proposed model on several publicly available semantic image segmentation benchmarks including PASCAL VOC 2012, Cityscapes and COCO-Stuff. The qualitative and quantitative results demonstrate that our method can achieve the state-of-the-art performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303010",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Dual (grammatical number)",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pooling",
      "Pyramid (geometry)",
      "STRIDE",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Chengli"
      },
      {
        "surname": "Ma",
        "given_name": "Jiayi"
      }
    ]
  },
  {
    "title": "A deep inference learning framework for healthcare",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.02.009",
    "abstract": "Deep learning, which is a promising end-to-end learning method for accurate health diagnosis, can extract effective representations directly from high-dimensional sensory inputs without manually feature engineering. However, it is still not enough for the end-to-end dynamic process of healthcare, which needs to derive effective representations of human body from miscellaneous observations and generalize past experience to new situations. This paper proposes an end-to-end framework that emulates the process of healthcare. There are two major modules in our framework. The first one is the deep recognition module to diagnose the health states based on the deep neural networks (DNNs). The second module is the action evaluation module based on the Bayesian inference graphs. A simulation environment is designed in order to evaluate the framework. It includes a body simulator to produce the body instance that can receive treatments, and the latent health state of the simulated patient will be changed by different interventions. It also includes a deep recognition module and an action evaluation module to nurse the body. The experiments demonstrate that the healthcare process under our framework can become more and more efficient as the time goes on with the increasing statistic data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518300448",
    "keywords": [
      "Action (physics)",
      "Artificial intelligence",
      "Bayesian inference",
      "Bayesian probability",
      "Computer science",
      "Deep learning",
      "Deep neural networks",
      "Economic growth",
      "Economics",
      "Feature (linguistics)",
      "Feature engineering",
      "Health care",
      "Inference",
      "Linguistics",
      "Machine learning",
      "Operating system",
      "Philosophy",
      "Physics",
      "Process (computing)",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Dai",
        "given_name": "Yinglong"
      },
      {
        "surname": "Wang",
        "given_name": "Guojun"
      }
    ]
  },
  {
    "title": "Fast sparse coding networks for anomaly detection in videos",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107515",
    "abstract": "The semi-supervised video anomaly detection assumes that only normal video clips are available for training. Therefore, the intuitive idea is either to learn a dictionary by sparse coding or to train encoding-decoding neural networks by minimizing the reconstruction errors. For the former, the optimization of sparse coefficients is extremely time-consuming. For the latter, this manner cannot guarantee that an abnormal data corresponds to a larger reconstruction error due to the strong generalization of neural networks. To remedy their weaknesses and leverage their strengths, we propose a Fast Sparse Coding Network (FSCN) based on High-level Features. First, we propose a two-stream neural network to extract Spatial-Temporal Fusion Features (STFF) in hidden layers. With the STFF at hand, we use a Fast Sparse Coding Network to build a normal dictionary. By leveraging the predictor to produce approximate sparse coefficients, our FSCN generates sparse coefficients within a forward pass, which is simple and computationally efficient. Compared with traditional sparse coding based methods, FSCN is hundreds of or even thousands of times faster at the test stage. Extensive experiments on benchmark datasets demonstrate that our method reaches the state-of-the-art level. 1 1 Code will be released at https://github.com/Roc-Ng/FSCN_AnomalyDetection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303186",
    "keywords": [
      "Algorithm",
      "Anomaly detection",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Coding (social sciences)",
      "Computer science",
      "Decoding methods",
      "Gaussian",
      "Geodesy",
      "Geography",
      "Leverage (statistics)",
      "Mathematics",
      "Neural coding",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Sparse approximation",
      "Sparse matrix",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Peng"
      },
      {
        "surname": "Liu",
        "given_name": "Jing"
      },
      {
        "surname": "Li",
        "given_name": "Mingming"
      },
      {
        "surname": "Sun",
        "given_name": "Yujia"
      },
      {
        "surname": "Shen",
        "given_name": "Fang"
      }
    ]
  },
  {
    "title": "Point attention network for semantic segmentation of 3D point clouds",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107446",
    "abstract": "Convolutional Neural Networks (CNNs) have performed extremely well on data represented by regularly arranged grids such as images. However, directly leveraging the classic convolution kernels or parameter sharing mechanisms on sparse 3D point clouds is inefficient due to their irregular and unordered nature. We propose a point attention network that learns rich local shape features and their contextual correlations for 3D point cloud semantic segmentation. Since the geometric distribution of the neighboring points is invariant to the point ordering, we propose a Local Attention-Edge Convolution (LAE-Conv) to construct a local graph based on the neighborhood points searched in multi-directions. We assign attention coefficients to each edge and then aggregate the point features as a weighted sum of its neighbors. The learned LAE-Conv layer features are then given to a point-wise spatial attention module to generate an interdependency matrix of all points regardless of their distances, which captures long-range spatial contextual features contributing to more precise semantic information. The proposed point attention network consists of an encoder and decoder which, together with the LAE-Conv layers and the point-wise spatial attention modules, make it an end-to-end trainable network for predicting dense labels for 3D point cloud segmentation. Experiments on challenging benchmarks of 3D point clouds show that our algorithm can perform at par or better than the existing state of the art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302491",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Encoder",
      "Geometry",
      "Invariant (physics)",
      "Mathematical physics",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Point (geometry)",
      "Point cloud",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Feng",
        "given_name": "Mingtao"
      },
      {
        "surname": "Zhang",
        "given_name": "Liang"
      },
      {
        "surname": "Lin",
        "given_name": "Xuefei"
      },
      {
        "surname": "Gilani",
        "given_name": "Syed Zulqarnain"
      },
      {
        "surname": "Mian",
        "given_name": "Ajmal"
      }
    ]
  },
  {
    "title": "Learning spatial-temporal deformable networks for unconstrained face alignment and tracking in videos",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107354",
    "abstract": "In this paper, we propose a spatial-temporal deformable networks approach to investigate both problems of face alignment in static images and face tracking in videos under unconstrained environments. Unlike conventional feature extractions which cannot explicitly exploit augmented spatial geometry for various facial shapes, in our approach, we propose a deformable hourglass networks (DHGN) method, which aims to learn a deformable mask to reduce the variances of facial deformation and extract attentional facial regions for robust feature representation. However, our DHGN is limited to extract only spatial appearance features from static facial images, which cannot explicitly exploit the temporal consistency information across consecutive frames in videos. For efficient temporal modeling, we further extend our DHGN to a temporal DHGN (T-DHGN) paradigm particularly for video-based face alignment. To this end, our T-DHGN principally incorporates with a temporal relational reasoning module, so that the temporal order relationship among frames is encoded in the relational feature. By doing this, our T-DHGN reasons about the temporal offsets to select a subset of discriminative frames over time steps, thus allowing temporal consistency information memorized to flow across frames for stable landmark tracking in videos. Compared with most state-of-the-art methods, our approach achieves superior performance on folds of widely-evaluated benchmarking datasets. Code will be made publicly available upon publication.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301576",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Discriminative model",
      "Exploit",
      "Face (sociological concept)",
      "Feature (linguistics)",
      "Landmark",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Philosophy",
      "Psychology",
      "Social science",
      "Sociology",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Hongyu"
      },
      {
        "surname": "Liu",
        "given_name": "Hao"
      },
      {
        "surname": "Zhu",
        "given_name": "Congcong"
      },
      {
        "surname": "Deng",
        "given_name": "Zongyong"
      },
      {
        "surname": "Sun",
        "given_name": "Xuehong"
      }
    ]
  },
  {
    "title": "Detecting distraction of drivers using Convolutional Neural Network",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2017.12.023",
    "abstract": "With the intervention of social media and internet technology, people are getting more and more careless and distracted while driving which is having a severe detrimental effect on the safety of the driver and his fellow passengers. To provide an effective solution, this paper puts forward a Machine Learning model using Convolutional Neural Networks to not only detect the distracted driver but also identify the cause of his distraction by analyzing the images obtained using the camera module installed inside the vehicle. Convolutional Neural Networks are known to learn spatial features from images, which can be further examined by fully connected neural networks. The experimental results show a 99% average accuracy in distraction recognition and hence strongly support that our Convolutional Neural Networks model can be used to identify distraction among the drivers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865517304695",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Distracted driving",
      "Distraction",
      "Machine learning",
      "Neuroscience",
      "Psychology",
      "The Internet",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Masood",
        "given_name": "Sarfaraz"
      },
      {
        "surname": "Rai",
        "given_name": "Abhinav"
      },
      {
        "surname": "Aggarwal",
        "given_name": "Aakash"
      },
      {
        "surname": "Doja",
        "given_name": "M.N."
      },
      {
        "surname": "Ahmad",
        "given_name": "Musheer"
      }
    ]
  },
  {
    "title": "Active contour model for inhomogenous image segmentation based on Jeffreys divergence",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107520",
    "abstract": "Inhomogenous image segmentation has been a research challenge in recent years. To deal with this difficulty, we propose a new local and global active contour model based on Jeffreys divergence. First, unlike the local data fitting energy of the region-scalable fitting model, a new local data fitting energy based on Jeffreys divergence is proposed instead of Euclidean distance, which achieves relatively better segmentation. Second, to improve the versatility of the model, a new global data fitting energy based on Jeffreys divergence is proposed. Finally, the adaptive weights of the local and global data fitting energies are developed to increase the robustness to the initial curve. Experiments on real-world and medical images with inhomogeneities indicate that the proposed model can obtain accurate segmentation results efficiently and is not strictly dependent on setting up initial curves.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030323X",
    "keywords": [
      "Active contour model",
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Curve fitting",
      "Divergence (linguistics)",
      "Euclidean distance",
      "Euclidean geometry",
      "Gene",
      "Geometry",
      "Image segmentation",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Robustness (evolution)",
      "Scale-space segmentation",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Han",
        "given_name": "Bin"
      },
      {
        "surname": "Wu",
        "given_name": "Yiquan"
      }
    ]
  },
  {
    "title": "Modality adversarial neural network for visible-thermal person re-identification",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107533",
    "abstract": "Existing Visible-Thermal Person Re-identification (VT-REID) methods usually adopt two-stream networks for cross-modality images. The two streams are trained to extract features from different modality images respectively. In contrast, we design a Modality Adversarial Neural Network (MANN) to solve VT-REID problem. Our proposed MANN includes a one-stream feature extractor and a modality discriminator. The heterogeneous images are processed by the feature extractor to generate modality-invariant features. And the designed modality discriminator aims to distinguish whether the extracted features are from visible or thermal modality. Moreover, our advanced dual-constrained triplet loss is introduced for better cross-modality matching performance. The experiments on two cross-modality person re-identification datasets show that MANN can effectively learn modality-invariant features and outperform state-of-the-art methods by a large margin.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303368",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Botany",
      "Computer science",
      "Computer vision",
      "Detector",
      "Discriminator",
      "Engineering",
      "Extractor",
      "Feature (linguistics)",
      "Identification (biology)",
      "Linguistics",
      "Machine learning",
      "Margin (machine learning)",
      "Modality (human–computer interaction)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process engineering",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Hao",
        "given_name": "Yi"
      },
      {
        "surname": "Li",
        "given_name": "Jie"
      },
      {
        "surname": "Wang",
        "given_name": "Nannan"
      },
      {
        "surname": "Gao",
        "given_name": "Xinbo"
      }
    ]
  },
  {
    "title": "A linear multivariate binary decision tree classifier based on K-means splitting",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107521",
    "abstract": "A novel linear multivariate decision tree classifier, Binary Decision Tree based on K-means Splitting (BDTKS), is presented in this paper. The unsupervised K-means clustering is recursively integrated into the binary tree, building a hierarchical classifier. The introduction of the unsupervised K-means clustering provides the powerful generalization ability for the resulting BDTKS model. Then, the good generalization ability of BDTKS ensures the classification performance. A novel non-split condition with an easy-setting hyperparameter which focuses more on minority classes of the current node is proposed and applied in the BDTKS model, avoiding ignoring the minority classes in the class imbalance cases. Furthermore, the K-means centroid based BDTKS model is converted into the hyperplane based decision tree, speeding up the process of classification. Extensive experiments on the publicly available data sets have demonstrated that the proposed BDTKS matches or outperforms the previous decision trees.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303241",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Binary classification",
      "Binary tree",
      "Centroid",
      "Classifier (UML)",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Decision tree",
      "Decision tree learning",
      "Hyperplane",
      "Incremental decision tree",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Fei"
      },
      {
        "surname": "Wang",
        "given_name": "Quan"
      },
      {
        "surname": "Nie",
        "given_name": "Feiping"
      },
      {
        "surname": "Li",
        "given_name": "Zhongheng"
      },
      {
        "surname": "Yu",
        "given_name": "Weizhong"
      },
      {
        "surname": "Ren",
        "given_name": "Fuji"
      }
    ]
  },
  {
    "title": "Generative attention adversarial classification network for unsupervised domain adaptation",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107440",
    "abstract": "Domain adaptation is a significant and popular issue of solving distribution discrepancy among different domains in computer vision. Generally, previous works proposed are mainly devoted to reducing domain shift between source domain with labeled data and target domain without labels. Adversarial learning in deep networks has already been widely applied to learn disentangled and transferable features between two different domains to minimize domains distribution discrepancy. However, these methods rarely consider class distributions among source data during adversarial learning, and they pay little attention to these transferable regions among source and target domains images. In this paper, we propose a Generative Attention Adversarial Classification Network (GAACN) model for unsupervised domain adaptation. To learn a joint feature distribution between source and target domains, we present an improved generative adversarial network (GAN) following the feature extractor. Firstly, the discriminator of GAN discriminates the distribution of domains and the classes distribution among source data during adversarial learning, so that our feature extractor can learn a joint feature distribution between source and target domains and maintain the classes consistent simultaneously. Secondly, we present an attention module embedded in GAN, which allows the discriminator to discriminate the transferable regions among the images of source and target domains. Lastly, we propose a simple and efficient method which allocates pseudo-labels for unlabeled target data, and it can improve the performance of our model GAACN while mitigating negative transfer. Extensive experiments demonstrate that our proposed model achieves perfect results on several standard domain adaptation datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302430",
    "keywords": [
      "Adaptation (eye)",
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Detector",
      "Discriminator",
      "Domain (mathematical analysis)",
      "Feature (linguistics)",
      "Feature extraction",
      "Generative grammar",
      "Joint probability distribution",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Wendong"
      },
      {
        "surname": "Hu",
        "given_name": "Haifeng"
      }
    ]
  },
  {
    "title": "Deep co-training for semi-supervised image segmentation",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107269",
    "abstract": "In this paper, we aim to improve the performance of semantic image segmentation in a semi-supervised setting where training is performed with a reduced set of annotated images and additional non-annotated images. We present a method based on an ensemble of deep segmentation models. Models are trained on subsets of the annotated data and use non-annotated images to exchange information with each other, similar to co-training. Diversity across models is enforced with the use of adversarial samples. We demonstrate the potential of our method on three challenging image segmentation problems, and illustrate its ability to share information between simultaneously trained models, while preserving their diversity. Results indicate clear advantages in terms of performance compared to recently proposed semi-supervised methods for segmentation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320300741",
    "keywords": [
      "Artificial intelligence",
      "Co-training",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Image (mathematics)",
      "Image segmentation",
      "Machine learning",
      "Meteorology",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Scale-space segmentation",
      "Segmentation",
      "Semi-supervised learning",
      "Set (abstract data type)",
      "Training (meteorology)",
      "Training set"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Jizong"
      },
      {
        "surname": "Estrada",
        "given_name": "Guillermo"
      },
      {
        "surname": "Pedersoli",
        "given_name": "Marco"
      },
      {
        "surname": "Desrosiers",
        "given_name": "Christian"
      }
    ]
  },
  {
    "title": "Precise detection of Chinese characters in historical documents with deep reinforcement learning",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107503",
    "abstract": "The decision-making ability of deep reinforcement learning has been proved successfully in a variety of fields. Here, we use this method for precise character detection by making tight bounding boxes around the Chinese characters in historical documents. An agent is trained to learn the control policy of fine-tuning a bounding box step-by-step through a Markov Decision Process. We introduce a novel fully convolutional network with position-sensitive Region-of-Interest (RoI) pooling (FCPN). The network receives character patches as input without fixed size, and it can fuse position information into the features of actions. Besides, we propose a dense reward function (DRF) that provides excellent rewards according to different actions and environment states, improving the decision-making ability of the agent. Our approach is designed as a universal method that can be applied to the output of all character-level or word-level text detectors to obtain more precise detection results. Application to the Tripitaka Koreana in Han (TKH) and Multiple Tripitaka in Han (MTH) datasets confirm the very promising performance of this method. In particular, our approach yields a significant improvement under a large Intersection over Union (IoU) of 0.8. The robustness and generality are also proved by experiments on the scene text datasets ICDAR2013 and ICDAR2015.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030306X",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Bounding overwatch",
      "Character (mathematics)",
      "Chemistry",
      "Computer science",
      "Electrical engineering",
      "Engineering",
      "Fuse (electrical)",
      "Gene",
      "Generality",
      "Geometry",
      "Image (mathematics)",
      "Machine learning",
      "Mathematics",
      "Minimum bounding box",
      "Pattern recognition (psychology)",
      "Pooling",
      "Psychology",
      "Psychotherapist",
      "Reinforcement learning",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Sihang",
        "given_name": "Wu"
      },
      {
        "surname": "Jiapeng",
        "given_name": "Wang"
      },
      {
        "surname": "Weihong",
        "given_name": "Ma"
      },
      {
        "surname": "Lianwen",
        "given_name": "Jin"
      }
    ]
  },
  {
    "title": "An improved block based joint reversible data hiding in encrypted images by symmetric cryptosystem",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.01.014",
    "abstract": "Earlier joint EIRDH (Encrypted Image Reversible Data Hiding) techniques divided cover image into equal sized blocks and embedded one secret bit per block and after decryption of stego image with same key as encryption key, secret message is extracted through data hiding key and original cover image is recovered using spatial correlation of the image. After carrying out a feasibility analysis, improved block based joint EIRDH algorithm is employed in this paper which embedded n ( n = 2 , 3 , 4 . . . . . ) secret bits per block by dividing blocks of same size (4 × 4, 8 × 8, 16 × 16, 32 × 32.....)into n sub-blocks. Optimal visual quality and enhanced embedding rate are acquired by the proposed scheme. Test results and correlations are exhibited to outline the viability and focal points of the proposed technique with conventional ones.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518300205",
    "keywords": [
      "Algorithm",
      "Architectural engineering",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Block size",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Cover (algebra)",
      "Cryptosystem",
      "Embedding",
      "Encryption",
      "Engineering",
      "Geometry",
      "Image (mathematics)",
      "Image quality",
      "Information hiding",
      "Joint (building)",
      "Key (lock)",
      "Mathematics",
      "Mechanical engineering",
      "Operating system"
    ],
    "authors": [
      {
        "surname": "Bhardwaj",
        "given_name": "Rupali"
      },
      {
        "surname": "Aggarwal",
        "given_name": "Ashutosh"
      }
    ]
  },
  {
    "title": "Defocus map estimation from a single image using improved likelihood feature and edge-based basis",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107485",
    "abstract": "Defocus map estimation (DME) is very useful in many computer vision applications and has drawn much attention in recent years. Edge-based DME methods can generate sharp defocus discontinuities but usually suffer from textures of the input image. Region-based methods are free of textures but cannot catch the defocus discontinuities very well. In this paper, we propose a DME method combining edge-based and region-based methods together to keep their respective advantages while eliminating the shortcomings. The combination is achieved via regression tree fields (RTF). In an RTF, the input feature and the linear basis are of vital importance. For our RTF, they are obtained as follows. (i) Two orthogonal gradient operators with the corresponding subsets of Gabor filters are employed in localized 2D frequency analysis to generate accurate likelihood, and the first K highest local maximums of likelihood are sent to an RTF as input feature. (ii) At the same time, the input image is processed by three edge-based methods and the results serve as the linear basis of RTF. The experiments demonstrate that the proposed method outperforms state-of-the-art DME methods. Moreover, the proposed method can be readily applied to defocused image deblurring and defocus blur detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302880",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Basis (linear algebra)",
      "Classification of discontinuities",
      "Computer science",
      "Computer vision",
      "Deblurring",
      "Edge detection",
      "Enhanced Data Rates for GSM Evolution",
      "Feature (linguistics)",
      "Geometry",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Shaojun"
      },
      {
        "surname": "Liao",
        "given_name": "Qingmin"
      },
      {
        "surname": "Xue",
        "given_name": "Jing-Hao"
      },
      {
        "surname": "Zhou",
        "given_name": "Fei"
      }
    ]
  },
  {
    "title": "Filtering impulse noise in medical images using information sets",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.06.002",
    "abstract": "An efficient filtering algorithm is required to remove noise and simultaneously protect fine details and important features in the medical images. In this paper, a noise adaptive information set based switching median (NAISM) filter is proposed for the removal of impulse noise. NAISM filter is inspired from fuzzy switching median filter and works on the concept of information sets. Information sets are derived from fuzzy sets to deal with the uncertainty. It works in two phases; first phase identifies noisy pixels and second applies filtering based on an adaptive switching criterion. It is by virtue of this switching criterion and the local effective information surrounding the noisy pixel, the best calculated value replaces the noisy pixel in the selected window. The proposed information set based filter is capable of removing both low and high noise densities and can preserve image details better than the fuzzy filter. The applicability of the proposed filter is demonstrated on different datasets including Berkeley Segmentation Dataset (BSD), medical and real images. The qualitative and quantitative results demonstrate the effectiveness of the proposed approach in suppressing noise over the existing approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518302253",
    "keywords": [
      "Adaptive filter",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Filter (signal processing)",
      "Fuzzy logic",
      "Fuzzy set",
      "Image (mathematics)",
      "Image processing",
      "Impulse noise",
      "Median filter",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Pixel"
    ],
    "authors": [
      {
        "surname": "Arora",
        "given_name": "Shaveta"
      },
      {
        "surname": "Hanmandlu",
        "given_name": "Madasu"
      },
      {
        "surname": "Gupta",
        "given_name": "Gaurav"
      }
    ]
  },
  {
    "title": "Accelerating information entropy-based feature selection using rough set theory with classified nested equivalence classes",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107517",
    "abstract": "Feature selection effectively reduces the dimensionality of data. For feature selection, rough set theory offers a systematic theoretical framework based on consistency measures, of which information entropy is one of the most important significance measures of attributes. However, an information-entropy-based significance measure is computationally expensive and requires repeated calculations. Although many accelerating strategies have been proposed thus far, there remains a bottleneck when using an information-entropy-based feature selection algorithm to handle large-scale datasets with high dimensions. In this study, we introduce a classified nested equivalence class (CNEC)-based approach to calculate the information-entropy-based significance for feature selection using rough set theory. The proposed method extracts knowledge of the reducts of a decision table to reduce the universe and construct CNECs. By exploring the properties of different types of CNECs, we can not only accelerate both outer and inner significance calculation by discarding useless CNECs but also effectively decrease the number of inner significance calculations by using one type of CNECs. The use of CNECs is shown to significantly enhance three representative entropy-based feature selection algorithms that use rough set theory. The feature subset selected by the CNEC-based algorithms is the same as that selected by algorithms using the original definition of information entropies. Experiments conducted using 31 datasets from multiple sources, such as the UCI repository and KDD Cup competition, including large-scale and high-dimensional datasets, confirm the efficiency and effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303204",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Entropy (arrow of time)",
      "Feature selection",
      "Information theory",
      "Joint entropy",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Principle of maximum entropy",
      "Quantum mechanics",
      "Rough set",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Jie"
      },
      {
        "surname": "Liang",
        "given_name": "Jia-ming"
      },
      {
        "surname": "Dong",
        "given_name": "Zhen-ning"
      },
      {
        "surname": "Tang",
        "given_name": "De-yu"
      },
      {
        "surname": "Liu",
        "given_name": "Zhen"
      }
    ]
  },
  {
    "title": "Play and rewind: Context-aware video temporal action proposals",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107477",
    "abstract": "In this paper, we investigate the problem of Temporal Action Proposal (TAP) generation, which plays a fundamental role in large-scale untrimmed video analysis but remains largely unsolved. Most of the prior works proposed the temporal actions by predicting the temporal boundaries or actionness scores of video units. Nevertheless, context information among surrounding video units has not been adequately explored, which may result in severe loss of information. In this work, we propose a context-aware temporal action proposal network which makes full use of the contextual information in two aspects: 1) To generate initial proposals, we design a Bi-directional Parallel LSTMs to extract the visual features of a video unit by considering its contextual information. Therefore, the prediction of temporal boundaries and actionness scores will be more accurate because it knows what happened in the past and what will happen in the future; and 2) To refine the initial proposals, we design an action-attention based re-ranking network which considers both surrounding proposal and initial actionness scores to assign true action proposals with high confidence scores. Extensive experiments are conducted on two challenging datasets for both temporal action proposal generation and detection tasks, demonstrating the effectiveness of the proposed approach. In particular, on THUMOS’14 dataset, our method significantly surpasses state-of-the-art methods by 7.73% on AR@50. Our code is released at: https://github.com/Rheelt/TAPG.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302806",
    "keywords": [
      "Action (physics)",
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "History",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Lianli"
      },
      {
        "surname": "Li",
        "given_name": "Tao"
      },
      {
        "surname": "Song",
        "given_name": "Jingkuan"
      },
      {
        "surname": "Zhao",
        "given_name": "Zhou"
      },
      {
        "surname": "Shen",
        "given_name": "Heng Tao"
      }
    ]
  },
  {
    "title": "Constraint saliency based intelligent camera for enhancing viewers attention towards intended face",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2018.01.002",
    "abstract": "Visual Saliency decides the focus of attention towards a region in a scene. When we talk about attending faces in a crowd or set of multiple faces, our focus of attention does not go equal for all the faces. The biasness of human's visual system towards a particular face, occurs due to some dominant features of it, over rest of the faces. So, for the faces which are not intrinsically salient in a scene, there is a requirement of increment of attentiveness. The current study effort to improve the saliency of a face (in a set of multiple faces), which is not significantly salient. It can be achieved by enhancing the contrast of the intended face with its surrounding faces, in terms of visual low-level features like intensity, color, etc. Modification of such feature values will change its potential to attract observer's gaze. But excesses change will destroy the originality of the image. Therefore, the problem of enhancing saliency of a target face is framed as an optimization (maximization) problem under some constraints. This concept can be applied to develop a saliency based intelligent camera having the power of enhancing the attractiveness of a particular face in the crowd and in the taken photograph, the enhanced face may give more attention to the viewers. Experiment has been conducted on grayscale as well as the colour images. Moreover, effect of saliency on faces wearing jewellery, has also been measured.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865518300011",
    "keywords": [
      "Artificial intelligence",
      "Attractiveness",
      "Computer science",
      "Computer vision",
      "Constraint (computer-aided design)",
      "Face (sociological concept)",
      "Feature (linguistics)",
      "Focus (optics)",
      "Gaze",
      "Geometry",
      "Grayscale",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Observer (physics)",
      "Optics",
      "Philosophy",
      "Physics",
      "Programming language",
      "Psychoanalysis",
      "Psychology",
      "Quantum mechanics",
      "Salient",
      "Set (abstract data type)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Kumar",
        "given_name": "Ravi Kant"
      },
      {
        "surname": "Garain",
        "given_name": "Jogendra"
      },
      {
        "surname": "Kisku",
        "given_name": "Dakshina Ranjan"
      },
      {
        "surname": "Sanyal",
        "given_name": "Goutam"
      }
    ]
  },
  {
    "title": "AutoPruner: An end-to-end trainable filter pruning method for efficient deep model inference",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107461",
    "abstract": "Channel pruning is an important method to speed up CNN model’s inference. Previous filter pruning algorithms regard importance evaluation and model fine-tuning as two independent steps. This paper argues that combining them into a single end-to-end trainable system will lead to better results. We propose an efficient channel selection layer, namely AutoPruner, to find less important filters automatically in a joint training manner. Our AutoPruner takes previous activation responses as an input and generates a true binary index code for pruning. Hence, all the filters corresponding to zero index values can be removed safely after training. By gradually erasing several unimportant filters, we can prevent an excessive drop in model accuracy. Compared with previous state-of-the-art pruning algorithms (including training from scratch), AutoPruner achieves significantly better performance. Furthermore, ablation experiments show that the proposed novel mini-batch pooling and binarization operations are vital for the success of model pruning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302648",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "End-to-end principle",
      "Filter (signal processing)",
      "Inference",
      "Pruning"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Jian-Hao"
      },
      {
        "surname": "Wu",
        "given_name": "Jianxin"
      }
    ]
  },
  {
    "title": "A novel strategy to balance the results of cross-modal hashing",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107523",
    "abstract": "Hashing methods for cross-modal retrieval has drawn increasing research interests and has been widely studied in recent years due to the explosive growth of multimedia big data. However, a significant phenomenon which has been ignored is that there is a large gap between the results of cross-modal hashing in most cases. For example, the results of Text-to-Image frequently outperform that of Image-to-Text with a large margin. In this paper, we propose a strategy named semantic augmentation to improve and balance the results of cross-modal hashing. An intermediate semantic space is constructed to re-align the feature representations that embedded with weak semantic information. By using the intermediate semantic space, the semantic information of visual features can be further augmented before being sent to cross-modal hashing algorithms. Extensive experiments are carried out on four datasets via seven state-of-the-art cross-modal hashing methods. Compared against the results without semantic augmentation, the Image-to-Text results of these methods with semantic augmentation are improved considerably, which demonstrates the effectiveness of the proposed semantic augmentation strategy in bridging the gap between the results of cross-modal retrieval. Additional experiments are conducted on the real-valued, semi-supervised, semi-paired, partial-paired, and unpaired cross-modal retrieval methods, the results further indicates the effectiveness of our strategy in improving performance of cross-modal retrieval.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303265",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Hash function",
      "Image (mathematics)",
      "Image retrieval",
      "Information retrieval",
      "Machine learning",
      "Margin (machine learning)",
      "Modal",
      "Pattern recognition (psychology)",
      "Polymer chemistry",
      "Semantic gap"
    ],
    "authors": [
      {
        "surname": "Zhong",
        "given_name": "Fangming"
      },
      {
        "surname": "Chen",
        "given_name": "Zhikui"
      },
      {
        "surname": "Min",
        "given_name": "Geyong"
      },
      {
        "surname": "Xia",
        "given_name": "Feng"
      }
    ]
  },
  {
    "title": "A new unified method for detecting text from marathon runners and sports players in video (PR-D-19-01078R2)",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107476",
    "abstract": "Detecting text located on the torsos of marathon runners and sports players in video is a challenging issue due to poor quality and adverse effects caused by flexible/colorful clothing, and different structures of human bodies or actions. This paper presents a new unified method for tackling the above challenges. The proposed method fuses gradient magnitude and direction coherence of text pixels in a new way for detecting candidate regions. Candidate regions are used for determining the number of temporal frame clusters obtained by K-means clustering on frame differences. This process in turn detects key frames. The proposed method explores Bayesian probability for skin portions using color values at both pixel and component levels of temporal frames, which provides fused images with skin components. Based on skin information, the proposed method then detects faces and torsos by finding structural and spatial coherences between them. We further propose adaptive pixels linking a deep learning model for text detection from torso regions. The proposed method is tested on our own dataset collected from marathon/sports video and three standard datasets, namely, RBNR, MMM and R-ID of marathon images, to evaluate the performance. In addition, the proposed method is also tested on the standard natural scene datasets, namely, CTW1500 and MS-COCO text datasets, to show the objectiveness of the proposed method. A comparative study with the state-of-the-art methods on bib number/text detection of different datasets shows that the proposed method outperforms the existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030279X",
    "keywords": [
      "Anatomy",
      "Artificial intelligence",
      "Bayesian probability",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Frame (networking)",
      "Image (mathematics)",
      "Medicine",
      "Operating system",
      "Pattern recognition (psychology)",
      "Pixel",
      "Process (computing)",
      "Telecommunications",
      "Text detection",
      "Torso"
    ],
    "authors": [
      {
        "surname": "Nag",
        "given_name": "Sauradip"
      },
      {
        "surname": "Shivakumara",
        "given_name": "Palaiahnakote"
      },
      {
        "surname": "Pal",
        "given_name": "Umapada"
      },
      {
        "surname": "Lu",
        "given_name": "Tong"
      },
      {
        "surname": "Blumenstein",
        "given_name": "Michael"
      }
    ]
  },
  {
    "title": "Detection of rotational symmetry in curves represented by the slope chain code",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107421",
    "abstract": "We present a new approach based on the Slope Chain Code to determine whether a curve is rotational symmetrical and its order of symmetry. The proposed approach works for open and closed perfectly symmetrical or quasi-symmetrical 2D curves. Simple operations on the SCC and its invariant properties are central to our methodology. To evaluate the proposed methodology, we use 1400 curves from a public database. For the symmetrical/asymmetrical classification task, a recall (R) of 0.86, a balanced accuracy (BA) of 0.92, and a precision (P) of 0.87 were obtained. For the quasi-symmetrical/quasi-asymmetrical classification task, R=0.77, BA=0.83, and P=0.70 were obtained. For the order of rotational symmetry detection task, the following performance was achieved: R=0.97, BA=0.98, and P=0.95 for a symmetrical set of curves, and R=0.98, BA=0.98, and P=0.90 for a quasi-symmetrical set of curves. We conclude our presentation demonstrating the usefulness of our methodology with three practical applications",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302247",
    "keywords": [
      "Algorithm",
      "Code (set theory)",
      "Computer science",
      "Engineering",
      "Geometry",
      "Invariant (physics)",
      "Mathematical analysis",
      "Mathematical physics",
      "Mathematics",
      "Programming language",
      "Rotational invariance",
      "Set (abstract data type)",
      "Symmetry (geometry)",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Aguilar",
        "given_name": "Wendy"
      },
      {
        "surname": "Alvarado-Gonzalez",
        "given_name": "Montserrat"
      },
      {
        "surname": "Garduño",
        "given_name": "Edgar"
      },
      {
        "surname": "Velarde",
        "given_name": "Carlos"
      },
      {
        "surname": "Bribiesca",
        "given_name": "Ernesto"
      }
    ]
  },
  {
    "title": "Enhanced automatic twin support vector machine for imbalanced data classification",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107442",
    "abstract": "Most of the classification approaches assume that the sample distribution among classes is balanced. Still, such an assumption leads to biased performance over the majority class. This paper proposes an enhanced automatic twin support vector machine – (EATWSVM) to deal with imbalanced data, which incorporates a kernel representation within a TWSVM-based optimization. To learn the kernel function, we impose a Gaussian similarity, ruled by a Mahalanobis distance, and couple a centered kernel alignment-based approach to improving the data separability. Besides, we suggest a suitable range to fix the regularization parameters concerning both the dataset’ imbalance ratio and overlap. Lastly, we adopt One-vs-One and One-vs-Rest frameworks to extend our EATWSVM formulation for multi-class tasks. Obtained results on synthetic and real-world datasets show that our approach outperforms state-of-the-art methods concerning classification performance and training time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302454",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Gaussian",
      "Gaussian function",
      "Kernel (algebra)",
      "Kernel method",
      "Machine learning",
      "Mahalanobis distance",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Regularization (linguistics)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Jimenez-Castaño",
        "given_name": "C."
      },
      {
        "surname": "Alvarez-Meza",
        "given_name": "A."
      },
      {
        "surname": "Orozco-Gutierrez",
        "given_name": "A."
      }
    ]
  },
  {
    "title": "Handling incomplete heterogeneous data using VAEs",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107501",
    "abstract": "Variational autoencoders (VAEs), as well as other generative models, have been shown to be efficient and accurate for capturing the latent structure of vast amounts of complex high-dimensional data. However, existing VAEs can still not directly handle data that are heterogenous (mixed continuous and discrete) or incomplete (with missing data at random), which is indeed common in real-world applications. In this paper, we propose a general framework to design VAEs suitable for fitting incomplete heterogenous data. The proposed HI-VAE includes likelihood models for real-valued, positive real valued, interval, categorical, ordinal and count data, and allows accurate estimation (and potentially imputation) of missing data. Furthermore, HI-VAE presents competitive predictive performance in supervised tasks, outperforming supervised models when trained on incomplete data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303046",
    "keywords": [],
    "authors": [
      {
        "surname": "Nazábal",
        "given_name": "Alfredo"
      },
      {
        "surname": "Olmos",
        "given_name": "Pablo M."
      },
      {
        "surname": "Ghahramani",
        "given_name": "Zoubin"
      },
      {
        "surname": "Valera",
        "given_name": "Isabel"
      }
    ]
  },
  {
    "title": "Density peaks clustering based on density backbone and fuzzy neighborhood",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107449",
    "abstract": "Density peaks clustering (DPC) is as an efficient clustering algorithm due for using a non-iterative process. However, DPC and most of its improvements suffer from the following shortcomings: (1) highly sensitive to its cutoff distance parameter, (2) ignoring the local structure of data in computing local densities, (3) using a crisp kernel to calculate local densities, and (4) suffering from the cause of chain reaction. To address these issues, in this paper a new method called DPC-DBFN is proposed. The proposed method uses a fuzzy kernel for improving separability of clusters and reducing the impact of outliers. DPC-DBFN uses a density-based kNN graph for labeling backbones. This strategy prevents the chain reaction and effectively assigns true labels to those instances located on the border regions to effectively cluster data with various shapes and densities. The DPC-DBFN is evaluated on some real-world and synthetic datasets. The experimental results show the effectiveness and robustness of the proposed algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302521",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Estimator",
      "Fuzzy clustering",
      "Fuzzy logic",
      "Gene",
      "Kernel (algebra)",
      "Kernel density estimation",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Programming language",
      "Robustness (evolution)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Lotfi",
        "given_name": "Abdulrahman"
      },
      {
        "surname": "Moradi",
        "given_name": "Parham"
      },
      {
        "surname": "Beigy",
        "given_name": "Hamid"
      }
    ]
  },
  {
    "title": "A novel nonintrusive decision support approach for heart rate measurement",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2017.07.002",
    "abstract": "In the last few years researchers have focused more on non-intrusive based frameworks to measure Heart Rate. This is because of the ease and lack of expense in utilization. RGB videos are used by most of the non-intrusive based systems as it is appropriate for experiments. However, there exists some challenges before they can be implemented in real time applications. Heart rate monitoring using RGB videos is inefficient in outdoor environment because light has significant impact on RGB videos. This approach introduces a heart rate measuring strategy using LAB color facial video. Here we have to note that blood circulation causes variation in facial skin color and heart rate can be extracted through these variations. Heart rate is subsequently measured and compared with a reference measurement. This technique has noteworthy potential for advancing telemedicine, health of a person and numerous applications where information is needed on a real time basis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865517302349",
    "keywords": [
      "Artificial intelligence",
      "Blood pressure",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Heart rate",
      "Heart rate monitor",
      "Internal medicine",
      "Measure (data warehouse)",
      "Medicine",
      "RGB color model"
    ],
    "authors": [
      {
        "surname": "Fernandes",
        "given_name": "Steven Lawrence"
      },
      {
        "surname": "Gurupur",
        "given_name": "Varadraj Prabhu"
      },
      {
        "surname": "Sunder",
        "given_name": "Nayak Ramesh"
      },
      {
        "surname": "Arunkumar",
        "given_name": "N"
      },
      {
        "surname": "Kadry",
        "given_name": "Seifedine"
      }
    ]
  },
  {
    "title": "SSFNET-VOS: Semantic segmentation and fusion network for video object segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.09.028",
    "abstract": "Most of the recent successful approaches for video object segmentation are highly complex, heavily rely on fine-tuning of the first frame, are slow, and henceforth, are of constrained practical use. In this work, we introduce a novel approach of video object segmentation using unsupervised learning. The complete process is divided into two phases where base frame and current frame are considered for segmentation. In the first phase, we generate the coarse region proposals, bounding boxes and scores, then in the next phase, the feature extraction process is carried out where attention network is incorporated for feature encoding. Finally, these features are scaled and fused using Softmax operation to generate the object segmentation. The performance of proposed approach is compared with several state-of-art techniques on challenging DAVIS 2016 & 2017 datasets. The experimental study shows that the proposed semantic segmentation and fusion network for video object segmentation (SSFNET-VOS) achieves better segmentation with less error in terms of segmentation accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303470",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Feature (linguistics)",
      "Frame (networking)",
      "Image segmentation",
      "Linguistics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Scale-space segmentation",
      "Segmentation",
      "Segmentation-based object categorization",
      "Softmax function",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Sharma",
        "given_name": "Vipal Kumar"
      },
      {
        "surname": "Mir",
        "given_name": "Roohie Naaz"
      }
    ]
  },
  {
    "title": "An experimental study on classification of thyroid histopathology images using transfer learning",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.09.020",
    "abstract": "CAD systems for histopathology image analysis using machine learning is a well researched subject. Deep learning is playing a major role in advancing this research in the recent years. This paper presents an automated thyroid histopathology image classification system with deep neural networks using the theory of transfer learning and popular pre-trained convolutional neural networks (CNNs). In this experiment-based study, two forms of transfer learning namely feature extraction and fine tuning are applied on popular state-of-the-art CNN architectures such as VGGNet, ResNet, InceptionNet and DenseNet to classify thyroid histopathology images. Accuracy, precision, sensitivity, specificity, area under receiver operating characteristic (AUROC) analysis and F1-score are used to evaluate the performance of the architectures. The results are promising and demonstrate the feasibility of transfer learning for thyroid histopathology image analysis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303573",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Feature extraction",
      "Histopathology",
      "Machine learning",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Buddhavarapu",
        "given_name": "Vijaya Gajanan"
      },
      {
        "surname": "J",
        "given_name": "Angel Arul Jothi"
      }
    ]
  },
  {
    "title": "Transductive semi-supervised metric learning for person re-identification",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107569",
    "abstract": "Semi-supervised learning is important and has become more widespread because obtaining labeled data is expensive and labor-intensive. In this paper, we focus on the challenging semi-supervised person Re-identification (ReID) task, which is a metric learning problem based on the assumption that unlabeled data is open-set. To address this problem, we propose the Transductive Semi-Supervised Metric Learning (TSSML) framework. In TSSML, we propose a graph-based transductive hard mining method for deeply mining hard triplets in unlabeled data and a degree-based relationship confidence scoring method for further reducing incorrect triplets. Moreover, we investigate the feature consistency loss (FCL) and adopt the curriculum learning strategy to improve the representation learning for semi-supervised ReID. Extensive experiments have been conducted on three large-scale ReID datasets and demonstrate the effectiveness of our TSSML framework.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303721",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Botany",
      "Computer science",
      "Consistency (knowledge bases)",
      "Economics",
      "Feature learning",
      "Graph",
      "Identification (biology)",
      "Law",
      "Machine learning",
      "Management",
      "Metric (unit)",
      "Operations management",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Semi-supervised learning",
      "Set (abstract data type)",
      "Supervised learning",
      "Task (project management)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Chang",
        "given_name": "Xinyuan"
      },
      {
        "surname": "Ma",
        "given_name": "Zhiheng"
      },
      {
        "surname": "Wei",
        "given_name": "Xing"
      },
      {
        "surname": "Hong",
        "given_name": "Xiaopeng"
      },
      {
        "surname": "Gong",
        "given_name": "Yihong"
      }
    ]
  },
  {
    "title": "Thermal infrared pedestrian tracking using joint siamese network and exemplar prediction model",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.09.022",
    "abstract": "Tracking pedestrian targets over a thermal infrared (TIR) image sequence is a hot topic in visual tracking. The imagery characteristics of TIR targets such as low target-background contrast and far imaging distance make TIR object tracking very difficult. In this paper, based on a convolutional neural network (CNN) and the siamese region proposal network (SiamRPN), we design an improved TIR pedestrian tracker. By fully considering the temporal and spatial information around an object, we firstly construct a CNN-based prediction model to produce the exemplar of a pedestrian target. Then the predicted exemplar is combined with SiamRPN to form an improved real-time TIR pedestrian tracker. The proposed tracker is evaluated on the TIR pedestrian tracking benchmark dataset PTB-TIR. Our experimental results demonstrate that the proposed tracker achieves promising tracking performance. In terms of tracking success rate and precision, our tracker outperforms traditional trackers such as KCF, and state-of-the-art trackers such as SiamRPN, SRDCF, and DSST. Moreover, similar to other siamese-network-based trackers, our tracker runs in real-time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303585",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "BitTorrent tracker",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Engineering",
      "Eye tracking",
      "Geodesy",
      "Geography",
      "Kalman filter",
      "Object (grammar)",
      "Pedagogy",
      "Pedestrian",
      "Pedestrian detection",
      "Psychology",
      "Tracking (education)",
      "Tracking system",
      "Transport engineering",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Liying"
      },
      {
        "surname": "Zhao",
        "given_name": "Shuo"
      },
      {
        "surname": "Zhang",
        "given_name": "Yanbo"
      },
      {
        "surname": "Yu",
        "given_name": "Lei"
      }
    ]
  },
  {
    "title": "Reconfigurable cyber-physical system for critical infrastructure protection in smart cities via smart video-surveillance",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.11.004",
    "abstract": "Automated surveillance is essential for the protection of Critical Infrastructures (CIs) in future Smart Cities. The dynamic environments and bandwidth requirements demand systems that adapt themselves to react when events of interest occur. We present a reconfigurable Cyber Physical System for the protection of CIs using distributed cloud-edge smart video surveillance. Our local edge nodes perform people detection via Deep Learning. Processing is embedded in high performance SoCs (System-on-Chip) achieving real-time performance ( ≈ 100 fps - frames per second) which enables efficiently managing video streams of more cameras source at lower frame rate. Cloud server gathers results from nodes to carry out biometric facial identification, tracking, and perimeter monitoring. A Quality and Resource Management module monitors data bandwidth and triggers reconfiguration adapting the transmitted video resolution. This also enables a flexible use of the network by multiple cameras while maintaining the accuracy of biometric identification. A real-world example shows a reduction of ≈ 75% bandwidth use with respect to the no-reconfiguration scenario.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520304098",
    "keywords": [
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Biometrics",
      "Cloud computing",
      "Computer network",
      "Computer science",
      "Computer security",
      "Control reconfiguration",
      "Edge computing",
      "Embedded system",
      "Operating system",
      "Real-time computing",
      "Smart camera"
    ],
    "authors": [
      {
        "surname": "Isern",
        "given_name": "Juan"
      },
      {
        "surname": "Barranco",
        "given_name": "Francisco"
      },
      {
        "surname": "Deniz",
        "given_name": "Daniel"
      },
      {
        "surname": "Lesonen",
        "given_name": "Juho"
      },
      {
        "surname": "Hannuksela",
        "given_name": "Jari"
      },
      {
        "surname": "Carrillo",
        "given_name": "Richard R."
      }
    ]
  },
  {
    "title": "A new machine learning based approach to predict Freezing of Gait",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.09.011",
    "abstract": "Freezing of Gait (FoG) is a motor symptom of Parkinson's disease (PD) that frequently occurs in the long-term sufferers of the disease. FoG may result to nursing home admission as it can lead to falls, and therefore, it impacts negatively on the quality of life. The focus of this study is the systematic evaluation of machine learning techniques in conjunction with varying size time windows and time/frequency domain feature sets in predicting a FoG event before its onset. In the experiments, the Daphnet FoG dataset is used to benchmark performance. This consists of accelerometer signals obtained from sensors mounted on the ankle, thigh and trunk of the PD patients. The dataset is annotated with instances of normal activity events, and FoG events. To predict the onset of FoG, the dataset is augmented with an additional class, termed ‘transition’, which relates to a manually defined period prior to the occurrence of a FoG episode. In this research, five machine learning models are used, namely, Random Forest, Extreme Gradient Boosting, Gradient Boosting, Support Vector Machines using Radial Basis Functions, and Neural Networks. Support Vector Machines with Radial Basis kernels provided the best performance achieving sensitivity values of 72.34%, 91.49%, 75.00%, and specificity values of 87.36%, 88.51% and 93.62%, for the FoG, transition and normal activity classes, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303524",
    "keywords": [
      "Accelerometer",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Boosting (machine learning)",
      "Computer science",
      "Gait",
      "Geodesy",
      "Geography",
      "Gradient boosting",
      "Machine learning",
      "Medicine",
      "Operating system",
      "Physical medicine and rehabilitation",
      "Random forest",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Kleanthous",
        "given_name": "Natasa"
      },
      {
        "surname": "Hussain",
        "given_name": "Abir Jaafar"
      },
      {
        "surname": "Khan",
        "given_name": "Wasiq"
      },
      {
        "surname": "Liatsis",
        "given_name": "Panos"
      }
    ]
  },
  {
    "title": "Editorial of the special issue DLHI: Deep learning in medical imaging and healthinformatics",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.09.033",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303652",
    "keywords": [
      "Artificial intelligence",
      "Boundary (topology)",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Image processing",
      "Image segmentation",
      "Mathematical analysis",
      "Mathematics",
      "Object (grammar)",
      "Operating system",
      "Pixel",
      "Process (computing)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Martis",
        "given_name": "Roshan Joy"
      },
      {
        "surname": "Lin",
        "given_name": "Hong"
      },
      {
        "surname": "Javadi",
        "given_name": "Bahman"
      },
      {
        "surname": "Fernandes",
        "given_name": "Steven Lawrence"
      },
      {
        "surname": "Yasmin",
        "given_name": "Mussarat"
      }
    ]
  },
  {
    "title": "SEMEDA: Enhancing segmentation precision with semantic edge aware loss",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107557",
    "abstract": "Per-Pixel Cross entropy (PPCE) is a commonly used loss on semantic segmentation tasks. However, it suffers from a number of drawbacks. Firstly, PPCE only depends on the probability of the ground truth class since the latter is usually one-hot encoded. Secondly, PPCE treats all pixels independently and does not take the local structure into account. While perceptual losses (e.g. matching prediction and ground truth in the embedding space of a pre-trained VGG network) would theoretically address these concerns, it does not constitute a practical solution as segmentation masks follow a distribution that differs largely from natural images. In this paper, we introduce a SEMantic EDge-Aware strategy (SEMEDA) to solve these issues. Inspired by perceptual losses, we propose to match the ’probability texture’ of predicted segmentation mask and ground truth through a proxy network trained for semantic edge detection on the ground truth masks. Through thorough experimental validation on several datasets, we show that SEMEDA steadily improves the segmentation accuracy with negligible computational overhead and can be added with any popular segmentation networks in an end-to-end training framework.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303605",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Cross entropy",
      "Embedding",
      "Enhanced Data Rates for GSM Evolution",
      "Entropy (arrow of time)",
      "Figure–ground",
      "Ground truth",
      "Image segmentation",
      "Matching (statistics)",
      "Mathematics",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Perception",
      "Physics",
      "Pixel",
      "Quantum mechanics",
      "Segmentation",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yifu"
      },
      {
        "surname": "Dapogny",
        "given_name": "Arnaud"
      },
      {
        "surname": "Cord",
        "given_name": "Matthieu"
      }
    ]
  },
  {
    "title": "Multi-view subspace learning via bidirectional sparsity",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107524",
    "abstract": "With the improvement of multi-view data collection technology, multi-view learning has become a hot research area. How to deal with diverse and complex data is one of the challenging problems in multi-view learning. However, it is hard for traditional multi-view subspace learning methods to find an effective subspace dimension and deal with outliers simultaneously. In this paper, we propose a novel method, named as Multi-view Subspace Learning via Bidirectional Sparsity(SLBS), which is effective to overcome the above difficulties and learn a better representation. Specifically, we divide the shared subspace into two parts. One is a row sparse matrix to do a secondary extraction of features and the other is a column sparse matrix to reduce the influence of outliers. The proposed model is a non-convex problem which is difficult to be solved. To address this problem, we develop an efficient algorithm and analyze its convergence and computational complexity. Finally, compared with other multi-view subspace learning methods, the extensive experimental results on real-world datasets present the effectiveness of our SLBS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303277",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Column (typography)",
      "Computer science",
      "Convergence (economics)",
      "Dimension (graph theory)",
      "Economic growth",
      "Economics",
      "Frame (networking)",
      "Law",
      "Machine learning",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Pure mathematics",
      "Representation (politics)",
      "Sparse approximation",
      "Subspace topology",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "Ruidong"
      },
      {
        "surname": "Luo",
        "given_name": "Tingjin"
      },
      {
        "surname": "Zhuge",
        "given_name": "Wenzhang"
      },
      {
        "surname": "Qiang",
        "given_name": "Sheng"
      },
      {
        "surname": "Hou",
        "given_name": "Chenping"
      }
    ]
  },
  {
    "title": "A classification method for brain MRI via MobileNet and feedforward network with random weights",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.10.017",
    "abstract": "Computer aided diagnosis systems are playing an important part in clinical treatment. They can help the doctors and physicians to verify the diagnosis decisions. In this study, a new classification algorithm for the brain magnetic resonance image is proposed. Initially, we utilized a MobileNetV2 to extract features from the input brain images, which was pre-trained on ImageNet dataset. Instead of training the deep network, we simply calculate the output of its certain layer to form the feature vector. The optimal feature layer is obtained by the experiment. Then, three different feedforward networks: extreme learning machine, Schmidt neural network and random vector functional-link net, are trained for classification. Chaotic bat algorithm was proposed to optimize the weights and biases in the three randomized neural networks to boost their classification accuracy. The result from 5×hold-out validation reveals that our method can achieve good generalization performance which is comparable to state-of-the-art pathological brain detection methods. The trained model can serve as a visual question answering system and produce accurate results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520304049",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Chaotic",
      "Computer science",
      "Contextual image classification",
      "Control engineering",
      "Engineering",
      "Extreme learning machine",
      "Feature (linguistics)",
      "Feature vector",
      "Feed forward",
      "Feedforward neural network",
      "Generalization",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Si-Yuan"
      },
      {
        "surname": "Wang",
        "given_name": "Shui-Hua"
      },
      {
        "surname": "Zhang",
        "given_name": "Yu-Dong"
      }
    ]
  },
  {
    "title": "Freely typed keystroke dynamics-based user authentication for mobile devices based on heterogeneous features",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107556",
    "abstract": "Keystroke dynamics-based authentication (KDA) is one of the human behavioral biometric-based user authentication methods based on the unique typing pattern of a person. Previous KDA studies on mobile devices primarily focused on fixed-length text-based KDA, such as passwords and personal identification numbers. This can strengthen the login system and prevent abnormal usage of impostors based on certain attack methods, such as shoulder surfing and smudge attacks. However, this method possesses a limitation that continuous monitoring is not possible after login. To solve this problem, KDA based on freely typed text was studied; however, there are only a few studies on this technique. Further, the performance authentication based on these studies is insufficient for a real-world implementation. In this paper, we propose a novel freely typed text-based KDA method for mobile devices named FACT, i.e., user authentication on mobile devices based on free text, accelerator, coordinate, and time. We collected data from three different smartphone sensors while typing in two languages (English and Korean), and 17 variables were extracted for a set of keystroke data. A total of six authentication methods were employed and the proposed FACT yielded an equal error rate lower than 1% with only one reference keystroke set; moreover, it demonstrated a perfect protection capability while using Korean when more than four reference keystroke sets were used. To contribute to the research and industrial community, we have publicized our collected keystroke dataset so that anyone who conducts a KDA study or develops a KDA-related mobile service can use the dataset without any restrictions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303599",
    "keywords": [
      "Artificial intelligence",
      "Authentication (law)",
      "Biometrics",
      "Computer science",
      "Computer security",
      "Human–computer interaction",
      "Keystroke dynamics",
      "Keystroke logging",
      "Login",
      "Mobile device",
      "Password",
      "Programming language",
      "S/KEY",
      "Set (abstract data type)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Junhong"
      },
      {
        "surname": "Kang",
        "given_name": "Pilsung"
      }
    ]
  },
  {
    "title": "UAV image analysis for leakage detection in district heating systems using machine learning",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.05.024",
    "abstract": "In this paper, we propose automatic energy leakage detection in underground pipes of district heating systems based on Infrared (IR) images, captured by an Unmanned Aerial Vehicle (UAV). Hot water or steam is distributed to homes and industries through underground pipes from a central power plan. Leakages in underground pipes pose a very common problem, which can occur for many reasons, e.g. unprofessional installation and end of service life. Potentially, a leakage remains undiscovered for a very long period of time. Therefore, it is of great interest for power supply companies to monitor district heating networks to identify leakages. In this paper, the original IR images are captured in a 16 bit format by a UAV. On ground, potential leakages are extracted using a region extraction algorithm. Thereafter a Convolutional Neural Network (CNN) as well as eight conventional Machine Learning (ML) classifiers are applied on these regions to classify whether or not it is a leakage. In total, twelve UAV sequences are captured at different cities in Denmark. Based on these, around 13.4 million samples of image patches of district heating systems are extracted. Eleven sequences are used for training and the remaining one for testing. This was performed on all splits in the leave-one-out testing. The deep learning CNN achieved an average weighted accuracy of 0.872 with a false positive and negative rate of 12.7 % and 10.4 %, respectively. This CNN model detected around 98.6 % of the true leakages. In comparison, conventional ML classifiers, i.e. Adaboost (AB), Random Forest (RF), etc. provide lower average weighted accuracy, but on the other hand they require less computational resources. We have compared our method with a state-of-art method and the result shows that the proposed method is very competitive.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302038",
    "keywords": [
      "AdaBoost",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Economics",
      "Leakage (economics)",
      "Machine learning",
      "Macroeconomics",
      "Pattern recognition (psychology)",
      "Real-time computing",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Hossain",
        "given_name": "Kabir"
      },
      {
        "surname": "Villebro",
        "given_name": "Frederik"
      },
      {
        "surname": "Forchhammer",
        "given_name": "Søren"
      }
    ]
  },
  {
    "title": "Online kernel classification with adjustable bandwidth using control-based learning approach",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107566",
    "abstract": "In this paper, a novel control-based kernel learning approach is proposed for inferring online binary classification tasks. Following a carefully designed alternating optimization scheme, the learning problems are transformed into two optimal feedback control problems for a series of linear, controllable systems. Model parameters including weights and kernel bandwidth can be efficiently updated by solving the control problems. These consequently lead to our control-based adaptive online kernel classification algorithm (CAOKC). The bandwidth, although nonlinear in our model, can still be updated accurately after linearization. Thus, compared with the existing benchmark algorithms with fixed kernels, the CAOKC algorithm is able to achieve a more adaptive, robust classification performance with better prediction accuracy by regarding the bandwidth as an adjustable parameter. The results presented in this paper also demonstrate how optimal control can provide novel insights and be an effective approach for addressing various learning tasks. Numerical results on benchmark synthetic and realistic datasets are provided to illustrate our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303691",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Benchmark (surveying)",
      "Binary classification",
      "Combinatorics",
      "Computer network",
      "Computer science",
      "Geodesy",
      "Geography",
      "Kernel (algebra)",
      "Kernel method",
      "Linearization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Nonlinear system",
      "Physics",
      "Quantum mechanics",
      "Radial basis function kernel",
      "Scheme (mathematics)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Jiaming"
      },
      {
        "surname": "Ning",
        "given_name": "Hanwen"
      }
    ]
  },
  {
    "title": "Towards capsule routing as reconstruction with sparsity constraints",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.10.011",
    "abstract": "The most recently-proposed concept of capsule network consists of capsules, a structural group of neurons with activation, as building blocks, and dynamic routing between them as connections. Semantic information for final tasks can be extracted by stacking such capsule layers as construction of deep models. In this paper, we formulate the dynamic routing problem from the perspective of feature compression and reconstruction, proposing a novel routing algorithm which explicitly minimizes the reconstruction error with sparsity constraints. By alternately updating poses and routing weights with closed-form solutions, the method ensures a theoretical convergence of the dynamic routing. Benefitting from the strict convergence, we further prompt the efficiency of routing with the use of an adaptive number of iterations. Experimental results on different datasets prove that our proposed routing method outperforms existing routing methods on both performance as well as efficiency with obvious margin. Furthermore, we empirically show that the reconstruction based routing method can be embedded into backbone networks as a representative compression module. The integration of convolutional networks and capsule routing delivers both reduction of model parameters and improvement of performance to original backbones.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303974",
    "keywords": [
      "Adaptive routing",
      "Computer network",
      "Computer science",
      "Convergence (economics)",
      "Destination-Sequenced Distance Vector routing",
      "Economic growth",
      "Economics",
      "Link-state routing protocol",
      "Mathematical optimization",
      "Mathematics",
      "Multipath routing",
      "Routing (electronic design automation)",
      "Routing protocol",
      "Static routing"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Suofei"
      },
      {
        "surname": "Fan",
        "given_name": "Wenhao"
      },
      {
        "surname": "Wu",
        "given_name": "Xiaofu"
      }
    ]
  },
  {
    "title": "Multi-region saliency-aware learning for cross-domain placenta image segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.10.004",
    "abstract": "We propose a multi-region saliency-aware learning (MSL) method for cross-domain placenta image segmentation. Unlike most existing image-level transfer learning methods that fail to preserve the semantics of paired regions, our MSL incorporates the attention mechanism and a saliency constraint into the adversarial translation process, which can realize multi-region mappings in the semantic level. Specifically, the built-in attention module serves to detect the most discriminative semantic regions that the generator should focus on. Then we use the attention consistency as another guidance for retaining semantics after translation. Furthermore, we exploit the specially designed saliency-consistent constraint to enforce the semantic consistency by requiring the saliency regions unchanged. We conduct experiments using two real-world placenta datasets we have collected. We examine the efficacy of this approach in (1) segmentation and (2) prediction of the placental diagnoses of fetal and maternal inflammatory response (FIR, MIR). Experimental results show the superiority of the proposed approach over the state of the art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303780",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Consistency (knowledge bases)",
      "Constraint (computer-aided design)",
      "Discriminative model",
      "Domain (mathematical analysis)",
      "Focus (optics)",
      "Gene",
      "Generator (circuit theory)",
      "Geometry",
      "Image (mathematics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Messenger RNA",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Power (physics)",
      "Programming language",
      "Quantum mechanics",
      "Segmentation",
      "Semantics (computer science)",
      "Translation (biology)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zhuomin"
      },
      {
        "surname": "Davaasuren",
        "given_name": "Dolzodmaa"
      },
      {
        "surname": "Wu",
        "given_name": "Chenyan"
      },
      {
        "surname": "Goldstein",
        "given_name": "Jeffery A."
      },
      {
        "surname": "Gernand",
        "given_name": "Alison D."
      },
      {
        "surname": "Wang",
        "given_name": "James Z."
      }
    ]
  },
  {
    "title": "Speech emotion recognition model based on Bi-GRU and Focal Loss",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.11.009",
    "abstract": "For the problems of inconsistent sample duration and unbalance of sample categories in the speech emotion corpus, this paper proposes a speech emotion recognition model based on Bi-GRU (Bidirection Gated Recurrent Unit) and Focal Loss. The model has been improved on the basis of learning CRNN (Convolutional Recurrent Neural Network) deeply. In CRNN, Bi-GRU is used to effectively lengthen the samples of the speech with short duration, and Focal Loss function is used to deal with the difficulties in classification caused by the imbalance of emotional categories of the samples. Through different methods for experimental comparison, weighted average recall (WAR), unweighted average recall (UAR) and confusion matrix (CM) are used as evaluation index of the algorithm. The experimental results show that the speech emotion recognition model proposed in this paper improves the recognition accuracy and the imbalance of IEMOCAP database samples, and can effectively prove that the improvement of speech emotion recognition performance is not due to the adjustment of model parameters or the change of the model topology.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520304141",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Chromatography",
      "Cognitive psychology",
      "Computer science",
      "Confusion",
      "Convolutional neural network",
      "Emotion recognition",
      "Pattern recognition (psychology)",
      "Psychoanalysis",
      "Psychology",
      "Recall",
      "Recurrent neural network",
      "Sample (material)",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Zijiang"
      },
      {
        "surname": "Dai",
        "given_name": "Weihuang"
      },
      {
        "surname": "Hu",
        "given_name": "Yi"
      },
      {
        "surname": "Li",
        "given_name": "Junshan"
      }
    ]
  },
  {
    "title": "End-to-end trainable network for superpixel and image segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.09.016",
    "abstract": "Image segmentation and superpixel generation have been studied for many years, and they are still active research topics in computer vision. Although many advanced computer vision algorithms have been used for image segmentation and superpixel generation, there is no end-to-end trainable algorithm that generates superpixels and segment images simultaneously. We propose an end-to-end trainable network to solve this problem. We train a differentiable clustering algorithm module to produce accurate superpixels. Based on the generated superpixels, the superpixel pooling operation is performed to obtain superpixel features, and then we calculate the similarity of two adjacent superpixels. If the similarity is greater than the preset threshold, we merge the two superpixels. Finally, we get the segmented image. We conduct our experiments in the BSDS500 dataset and get good results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303421",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "End-to-end principle",
      "Image (mathematics)",
      "Image segmentation",
      "Information retrieval",
      "Merge (version control)",
      "Pattern recognition (psychology)",
      "Pooling",
      "Segmentation",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Kai"
      },
      {
        "surname": "Li",
        "given_name": "Liang"
      },
      {
        "surname": "Zhang",
        "given_name": "Jiawan"
      }
    ]
  },
  {
    "title": "One-vs-One classification for deep neural networks",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107528",
    "abstract": "For performing multi-class classification, deep neural networks almost always employ a One-vs-All (OvA) classification scheme with as many output units as there are classes in a dataset. The problem of this approach is that each output unit requires a complex decision boundary to separate examples from one class from all other examples. In this paper, we propose a novel One-vs-One (OvO) classification scheme for deep neural networks that trains each output unit to distinguish between a specific pair of classes. This method increases the number of output units compared to the One-vs-All classification scheme but makes learning correct decision boundaries much easier. In addition to changing the neural network architecture, we changed the loss function, created a code matrix to transform the one-hot encoding to a new label encoding, and changed the method for classifying examples. To analyze the advantages of the proposed method, we compared the One-vs-One and One-vs-All classification methods on three plant recognition datasets (including a novel dataset that we created) and a dataset with images of different monkey species using two deep architectures. The two deep convolutional neural network (CNN) architectures, Inception-V3 and ResNet-50, are trained from scratch or pre-trained weights. The results show that the One-vs-One classification method outperforms the One-vs-All method on all four datasets when training the CNNs from scratch. However, when using the two classification schemes for fine-tuning pre-trained CNNs, the One-vs-All method leads to the best performances, which is presumably because the CNNs had been pre-trained using the One-vs-All scheme.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303319",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Contextual image classification",
      "Convolutional neural network",
      "Decision boundary",
      "Deep learning",
      "Encoding (memory)",
      "Evolutionary biology",
      "Function (biology)",
      "Image (mathematics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Multiclass classification",
      "Operating system",
      "Pattern recognition (psychology)",
      "Scheme (mathematics)",
      "Scratch",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Pawara",
        "given_name": "Pornntiwa"
      },
      {
        "surname": "Okafor",
        "given_name": "Emmanuel"
      },
      {
        "surname": "Groefsema",
        "given_name": "Marc"
      },
      {
        "surname": "He",
        "given_name": "Sheng"
      },
      {
        "surname": "Schomaker",
        "given_name": "Lambert R.B."
      },
      {
        "surname": "Wiering",
        "given_name": "Marco A."
      }
    ]
  },
  {
    "title": "Learning deep kernels in the space of monotone conjunctive polynomials",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.10.013",
    "abstract": "Dot-product kernels is a large family of kernel functions based on dot-product between examples. A recent result states that any dot-product kernel can be decomposed as a non-negative linear combination of homogeneous polynomial kernels of different degrees, and it is possible to learn the coefficients of the combination by exploiting the Multiple Kernel Learning (MKL) paradigm. In this paper it is proved that, under mild conditions, any homogeneous polynomial kernel defined on binary valued data can be decomposed in a parametrized finite linear non-negative combination of monotone conjunctive kernels. MKL has been employed to learn the parameters of the combination. Furthermore, we show that our solution produces a deep kernel whose feature space consists of hierarchically organized features of increasing complexity. We also emphasize the connection between our solution and existing deep kernel learning frameworks. A wide empirical assessment is presented to evaluate the proposed framework, and to compare it against the baselines on several categorical and binary datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520304013",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Categorical variable",
      "Combinatorics",
      "Computer science",
      "Connection (principal bundle)",
      "Discrete mathematics",
      "Dot product",
      "Geometry",
      "Homogeneous",
      "Kernel (algebra)",
      "Kernel method",
      "Mathematical analysis",
      "Mathematics",
      "Monotone polygon",
      "Multiple kernel learning",
      "Polynomial",
      "Polynomial kernel",
      "Product (mathematics)",
      "Statistics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Lauriola",
        "given_name": "Ivano"
      },
      {
        "surname": "Polato",
        "given_name": "Mirko"
      },
      {
        "surname": "Aiolli",
        "given_name": "Fabio"
      }
    ]
  },
  {
    "title": "Listening-oriented response generation by exploiting user responses",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.10.007",
    "abstract": "Although listening to a conversation partner is a key factor in the success of dialogue systems or conversational agents, recent neural conversation systems have no interest in generating listening-oriented responses. In this paper, we propose an end-to-end dialogue system that generates listening-oriented responses, which make users disclose themselves and feel positive emotions. Our model uses ‘self-disclosure’ and ‘positiveness’ as listening features and generate responses in an appropriate manner to the features. Furthermore, the model infers a user response that will be brought out at the end of the dialogue and uses the inferred user response for generating a system response. By utilizing both listening features and user responses, our model becomes capable of generating listening-oriented responses. In quantitative and qualitative experiments, our model turned out to be capable of generating listening-oriented responses that induce users to disclose themselves and talk positively. The results also show that the model utilizing user responses generates more listening-oriented responses than those only using listening features.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303809",
    "keywords": [
      "Active listening",
      "Appreciative listening",
      "Communication",
      "Computer science",
      "Conversation",
      "Human–computer interaction",
      "Informational listening",
      "Listening comprehension",
      "Psychology",
      "Reflective listening",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Bang",
        "given_name": "Jeesoo"
      },
      {
        "surname": "Han",
        "given_name": "Sangdo"
      },
      {
        "surname": "Lee",
        "given_name": "Jong-Hyeok"
      }
    ]
  },
  {
    "title": "Cross-modal knowledge reasoning for knowledge-based visual question answering",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107563",
    "abstract": "Knowledge-based Visual Question Answering (KVQA) requires external knowledge beyond the visible content to answer questions about an image. This ability is challenging but indispensable to achieve general VQA. One limitation of existing KVQA solutions is that they jointly embed all kinds of information without fine-grained selection, which introduces unexpected noises for reasoning the correct answer. How to capture the question-oriented and information-complementary evidence remains a key challenge to solve the problem. Inspired by the human cognition theory, in this paper, we depict an image by multiple knowledge graphs from the visual, semantic and factual views. Thereinto, the visual graph and semantic graph are regarded as image-conditioned instantiation of the factual graph. On top of these new representations, we re-formulate Knowledge-based Visual Question Answering as a recurrent reasoning process for obtaining complementary evidence from multimodal information. To this end, we decompose the model into a series of memory-based reasoning steps, each performed by a Graph-based Read, Update, and Control (GRUC) module that conducts parallel reasoning over both visual and semantic information. By stacking the modules multiple times, our model performs transitive reasoning and obtains question-oriented concept representations under the constrain of different modalities. Finally, we perform graph neural networks to infer the global-optimal answer by jointly considering all the concepts. We achieve a new state-of-the-art performance on three popular benchmark datasets, including FVQA, Visual7W-KB and OK-VQA, and demonstrate the effectiveness and interpretability of our model with extensive experiments. The source code is available at: https://github.com/astro-zihao/gruc",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303666",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Cognition",
      "Combinatorics",
      "Computer science",
      "Graph",
      "Interpretability",
      "Mathematics",
      "Neuroscience",
      "Question answering",
      "Semantic memory",
      "Theoretical computer science",
      "Transitive relation",
      "Visual reasoning"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Jing"
      },
      {
        "surname": "Zhu",
        "given_name": "Zihao"
      },
      {
        "surname": "Wang",
        "given_name": "Yujing"
      },
      {
        "surname": "Zhang",
        "given_name": "Weifeng"
      },
      {
        "surname": "Hu",
        "given_name": "Yue"
      },
      {
        "surname": "Tan",
        "given_name": "Jianlong"
      }
    ]
  },
  {
    "title": "Graph attention network for detecting license plates in crowded street scenes",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.09.018",
    "abstract": "Detecting multiple license plate numbers in crowded street scenes is challenging and requires the attention of researchers. In contrast to existing methods that focus on images that are not crowded with vehicles, in this work we aim at situations that are common and complex, for example, in city environments where numerous vehicles of different types like cars, trucks, motorbike etc. may present in a single image. In such cases, one can expect large variations in license plates in terms of quality, backgrounds, and various forms of occlusion. To address these challenges, we explore Adaptive Progressive Scale Expansion based Graph Attention Network (APSEGAT). This approach extracts local information which represents the license plates irrespective of vehicle types and numbers because it works at the pixel level in a progressive way, and identifies the dominant information in the image. This may include other parts of vehicles, drivers and pedestrians, and various other background objects. To overcome this problem, we integrate concepts of graph attention networks with progressive scale expansion networks. For evaluating the proposed method, we use our own dataset, named as AMLPR, which contains images captured in different crowded street scenes in different time span, and the benchmark dataset namely, UFPR-ALPR, which provides images of a single vehicle, and another benchmark dataset called, UCSD, which contains images of cars with different orientations. Experimental results on these datasets show that the method outperforms existing methods and is effective in detecting license plate numbers in crowded street scenes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303548",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Geography",
      "Graph",
      "License",
      "Operating system",
      "Pixel",
      "Street network",
      "Theoretical computer science",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Chowdhury",
        "given_name": "Pinaki Nath"
      },
      {
        "surname": "Shivakumara",
        "given_name": "Palaiahnakote"
      },
      {
        "surname": "Kanchan",
        "given_name": "Swati"
      },
      {
        "surname": "Raghavendra",
        "given_name": "Ramachandra"
      },
      {
        "surname": "Pal",
        "given_name": "Umapada"
      },
      {
        "surname": "Lu",
        "given_name": "Tong"
      },
      {
        "surname": "Lopresti",
        "given_name": "Daniel"
      }
    ]
  },
  {
    "title": "Cross-resolution face recognition adversarial attacks",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.10.008",
    "abstract": "Face Recognition is among the best examples of computer vision problems where the supremacy of deep learning techniques compared to standard ones is undeniable. Unfortunately, it has been shown that they are vulnerable to adversarial examples - input images to which a human imperceptible perturbation is added to lead a learning model to output a wrong prediction. Moreover, in applications such as biometric systems and forensics, cross-resolution scenarios are easily met with a non-negligible impact on the recognition performance and adversary’s success. Despite the existence of such vulnerabilities set a harsh limit to the spread of deep learning-based face recognition systems to real-world applications, a comprehensive analysis of their behavior when threatened in a cross-resolution setting is missing in the literature. In this context, we posit our study, where we harness several of the strongest adversarial attacks against deep learning-based face recognition systems considering the cross-resolution domain. To craft adversarial instances, we exploit attacks based on three different metrics, i.e., L 1, L 2, and L ∞, and we study the resilience of the models across resolutions. We then evaluate the performance of the systems against the face identification protocol, open- and close-set. In our study, we find that the deep representation attacks represents a much dangerous menace to a face recognition system than the ones based on the classification output independently from the used metric. Furthermore, we notice that the input image’s resolution has a non-negligible impact on an adversary’s success in deceiving a learning model. Finally, by comparing the performance of the threatened networks under analysis, we show how they can benefit from a cross-resolution training approach in terms of resilience to adversarial attacks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303950",
    "keywords": [
      "Adversarial system",
      "Adversary",
      "Artificial intelligence",
      "Biology",
      "Biometrics",
      "Computer science",
      "Computer security",
      "Context (archaeology)",
      "Deep learning",
      "Exploit",
      "Face (sociological concept)",
      "Facial recognition system",
      "Machine learning",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Massoli",
        "given_name": "Fabio Valerio"
      },
      {
        "surname": "Falchi",
        "given_name": "Fabrizio"
      },
      {
        "surname": "Amato",
        "given_name": "Giuseppe"
      }
    ]
  },
  {
    "title": "A sub-pixel image registration algorithm based on SURF and M-estimator sample consensus",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.09.031",
    "abstract": "Due to the influence of various conditions and uncertain difficulties for remote sensing images, image registration is still a challenging task. Considering the registration accuracy of pixel level cannot satisfy the requirements of some related applications, we put forward a sub-pixel image registration method based on speeded up robust features and M-estimator sample consensus. It mainly involves four aspects. At first, extract sub-pixel level feature points based on SURF algorithm. Next, obtain the initial matching point pairs based on Sum of Squared Difference and Fast Library for Approximate Nearest Neighbors algorithms. And then, remove the mismatched pair of points based on M-estimator sample consensus algorithm. Finally, calculate geometric transformation matrix based on purified matching points to reach sub-pixel accuracy image registration. Experimental results for several remote sensing image pairs with displacement, noise added, rotation, and different sensors, times and sizes, show that the proposed method can get more anti-interference matches than other methods, and take smaller computational cost in registration process.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303639",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Computer vision",
      "Displacement (psychology)",
      "Estimator",
      "Feature (linguistics)",
      "Gene",
      "Geometry",
      "Image (mathematics)",
      "Image registration",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Point (geometry)",
      "Point set registration",
      "Process (computing)",
      "Psychology",
      "Psychotherapist",
      "Rotation (mathematics)",
      "Sample (material)",
      "Statistics",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Shulei"
      },
      {
        "surname": "Zeng",
        "given_name": "Wankang"
      },
      {
        "surname": "Chen",
        "given_name": "Huandong"
      }
    ]
  },
  {
    "title": "Diverse training dataset generation based on a multi-objective optimization for semi-Supervised classification",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107543",
    "abstract": "The self-labeled technique is a type of semi-supervised classification that can be used when labeled data are lacking. Although existing self-labeled techniques show promise in many areas of classification and pattern recognition, they commonly incorrectly label data. The reasons for this problem are the shortage of labeled data and the inappropriate distribution of data in problem space. To deal with this problem, we propose in this paper a synthetic, labeled data generation method based on accuracy and density. Positions of generated data are improved through a multi-objective evolutionary algorithm with two objectives – accuracy and density. The density function generates data with an appropriate distribution and diversity in feature space, whereas the accuracy function eliminates outlier data. The advantage of the proposed method over existing ones is that it simultaneously considers accuracy and distribution of generated data in feature space. We have applied the new proposed method on four self-labeled techniques with different features, i.e., Democratic-co, Tri-training, Co-forest, and Co-bagging. The results show that the proposed method is superior to existing methods in terms of classification accuracy. Also, the superiority of the proposed method is confirmed over other data generation methods such as SMOTE, Borderline SMOTE, Safe-level SMOTE and SMOTE-RSB.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303460",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Data mining",
      "Economic shortage",
      "Evolutionary biology",
      "Feature (linguistics)",
      "Feature vector",
      "Function (biology)",
      "Government (linguistics)",
      "Linguistics",
      "Machine learning",
      "Outlier",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Donyavi",
        "given_name": "Zahra"
      },
      {
        "surname": "Asadi",
        "given_name": "Shahrokh"
      }
    ]
  },
  {
    "title": "Bringing semantics into word image representation",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107542",
    "abstract": "The shift from one-hot to distributed representation, popularly referred to as word embedding has changed the landscape of natural language processing (nlp) and information retrieval (ir) communities. In the domain of document images, we have always appreciated the need for learning a holistic word image representation which is popularly used for the task of word spotting. The representations proposed for word spotting is different from word embedding in text since the later captures the semantic aspects of the word which is a crucial ingredient to numerous nlp and ir tasks. In this work, we attempt to encode the notion of semantics into word image representation by bringing the advancements from the textual domain. We propose two novel forms of representations where the first form is designed to be inflection invariant by focusing on the approximate linguistic root of the word, while the second form is built along the lines of recent textual word embedding techniques such as Word2Vec. We observe that such representations are useful for both traditional word spotting and also enrich the search results by accounting the semantic nature of the task. We conduct our experiments on the challenging document images taken from historical-modern collections, handwritten-printed domains, and Latin-Indic scripts. For the purpose of semantic evaluation, we have prepared a large synthetic word image dataset and report interesting results for the standard semantic evaluation metrics such as word analogy and word similarity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303459",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Embedding",
      "Information retrieval",
      "Law",
      "Linguistics",
      "Natural language processing",
      "Philosophy",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Semantics (computer science)",
      "Word (group theory)",
      "Word embedding"
    ],
    "authors": [
      {
        "surname": "Krishnan",
        "given_name": "Praveen"
      },
      {
        "surname": "Jawahar",
        "given_name": "C.V."
      }
    ]
  },
  {
    "title": "Density peak clustering based on relative density relationship",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107554",
    "abstract": "The density peak clustering algorithm treats local density peaks as cluster centers, and groups non-center data points by assuming that one data point and its nearest higher-density neighbor are in the same cluster. While this algorithm is shown to be promising in some applications, its clustering results are found to be sensitive to density kernels, and large density differences across clusters tend to result in wrong cluster centers. In this paper we attribute these problems to the inconsistency between the assumption and implementation adopted in this algorithm. While the assumption is based totally on relative density relationship, this algorithm adopts absolute density as one criterion to identify cluster centers. This observation prompts us to present a cluster center identification criterion based only on relative density relationship. Specifically, we define the concept of subordinate to describe the relative density relationship, and use the number of subordinates as a criterion to identify cluster centers. Our approach makes use of only relative density relationship and is less influenced by density kernels and density differences across clusters. In addition, we discuss the problems of two existing density kernels, and present an average-distance based kernel. In data clustering experiments we validate the new criterion and density kernel respectively, and then test the whole algorithm and compare with some other clustering algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303575",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Density estimation",
      "Estimator",
      "Kernel (algebra)",
      "Kernel density estimation",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Hou",
        "given_name": "Jian"
      },
      {
        "surname": "Zhang",
        "given_name": "Aihua"
      },
      {
        "surname": "Qi",
        "given_name": "Naiming"
      }
    ]
  },
  {
    "title": "A mathematical programming approach for integrated multiple linear regression subset selection and validation",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107565",
    "abstract": "Subset selection for multiple linear regression aims to construct a regression model that minimizes errors by selecting a small number of explanatory variables. Once a model is built, various statistical tests and diagnostics are conducted to validate the model and to determine whether the regression assumptions are met. Most traditional approaches require human decisions at this step. For example, the user may repeat adding or removing a variable until a satisfactory model is obtained. However, this trial-and-error strategy cannot guarantee that a subset that minimizes the errors while satisfying all regression assumptions will be found. In this paper, we propose a fully automated model building procedure for multiple linear regression subset selection that integrates model building and validation based on mathematical programming. The proposed model minimizes mean squared errors while ensuring that the majority of the important regression assumptions are met. We also propose an efficient constraint to approximate the constraint for the coefficient t-test. When no subset satisfies all of the considered regression assumptions, our model provides an alternative subset that satisfies most of these assumptions. Computational results show that our model yields better solutions (i.e., satisfying more regression assumptions) compared to the state-of-the-art benchmark models while maintaining similar explanatory power.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030368X",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Constraint (computer-aided design)",
      "Feature selection",
      "Geodesy",
      "Geography",
      "Geometry",
      "Linear model",
      "Linear predictor function",
      "Linear programming",
      "Linear regression",
      "Mathematical optimization",
      "Mathematics",
      "Model selection",
      "Polynomial regression",
      "Proper linear model",
      "Regression",
      "Regression analysis",
      "Regression diagnostic",
      "Selection (genetic algorithm)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Chung",
        "given_name": "Seokhyun"
      },
      {
        "surname": "Park",
        "given_name": "Young Woong"
      },
      {
        "surname": "Cheong",
        "given_name": "Taesu"
      }
    ]
  },
  {
    "title": "Simultaneous 3D hand detection and pose estimation using single depth images",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.09.026",
    "abstract": "In this paper, we investigate 3D hand pose estimation using single depth images. On the one hand, accurate hand localization is a crucial factor for pose estimation. On the other hand, multi-task learning methods have achieved great success in visual recognition tasks. Therefore, in this paper we proposed to simultaneously detect the hand location and estimate its 3D pose in a multi-task learning framework. We used 3D region proposal for 3D pose estimation, which searches possible hand locations in the 3D space. In the experimental part, the proposed method is evaluated on several benchmark datasets and shown it is comparable to most existing 3D hand pose estimation methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303482",
    "keywords": [
      "3D pose estimation",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Estimation",
      "Geodesy",
      "Geography",
      "Pattern recognition (psychology)",
      "Pose",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yu"
      },
      {
        "surname": "Mi",
        "given_name": "Siya"
      },
      {
        "surname": "Wu",
        "given_name": "Jianxin"
      },
      {
        "surname": "Geng",
        "given_name": "Xin"
      }
    ]
  },
  {
    "title": "Few-shot activity recognition with cross-modal memory network",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107348",
    "abstract": "Deep learning based action recognition methods require large amount of labelled training data. However, labelling large-scale video data is time consuming and tedious. In this paper, we consider a more challenging few-shot action recognition problem where the training samples are few and rare. To solve this problem, memory network has been designed to use an external memory to remember the experience learned in training and then apply it to few-shot prediction during testing. However, existing memory-based methods just update the visual information with fixed label embeddings in the memory, which cannot adapt well to novel activities during testing. To alleviate the issue, we propose a novel end-to-end cross-modal memory network for few-shot activity recognition. Specifically, the proposed memory architecture stores the dynamic visual and textual semantics for some high-level attributes related to human activities. And the learned memory can provide effective multi-modal information for new activity recognition in the testing stage. Extensive experimental results on two video datasets, including HMDB51 and UCF101, indicate that our method could achieve significant improvements over other previous methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301515",
    "keywords": [
      "Action recognition",
      "Artificial intelligence",
      "Auxiliary memory",
      "Biology",
      "Chemistry",
      "Class (philosophy)",
      "Cognition",
      "Computer hardware",
      "Computer science",
      "Machine learning",
      "Modal",
      "Neuroscience",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Polymer chemistry",
      "Programming language",
      "Semantics (computer science)",
      "Shot (pellet)",
      "Visual memory"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Lingling"
      },
      {
        "surname": "Chang",
        "given_name": "Xiaojun"
      },
      {
        "surname": "Liu",
        "given_name": "Jun"
      },
      {
        "surname": "Luo",
        "given_name": "Minnan"
      },
      {
        "surname": "Prakash",
        "given_name": "Mahesh"
      },
      {
        "surname": "Hauptmann",
        "given_name": "Alexander G."
      }
    ]
  },
  {
    "title": "Post-comparison mitigation of demographic bias in face recognition using fair score normalization",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.11.007",
    "abstract": "Current face recognition systems achieve high progress on several benchmark tests. Despite this progress, recent works showed that these systems are strongly biased against demographic sub-groups. Consequently, an easily integrable solution is needed to reduce the discriminatory effect of these biased systems. Previous work mainly focused on learning less biased face representations, which comes at the cost of a strongly degraded overall recognition performance. In this work, we propose a novel unsupervised fair score normalization approach that is specifically designed to reduce the effect of bias in face recognition and subsequently lead to a significant overall performance boost. Our hypothesis is built on the notation of individual fairness by designing a normalization approach that leads to treating “similar” individuals “similarly”. Experiments were conducted on three publicly available datasets captured under controlled and in-the-wild circumstances. Results demonstrate that our solution reduces demographic biases, e.g. by up to 82.7% in the case when gender is considered. Moreover, it mitigates the bias more consistently than existing works. In contrast to previous works, our fair normalization approach enhances the overall performance by up to 53.2% at false match rate of 10 − 3 and up to 82.9% at a false match rate of 10 − 5 . Additionally, it is easily integrable into existing recognition systems and not limited to face biometrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520304128",
    "keywords": [],
    "authors": [
      {
        "surname": "Terhörst",
        "given_name": "Philipp"
      },
      {
        "surname": "Kolf",
        "given_name": "Jan Niklas"
      },
      {
        "surname": "Damer",
        "given_name": "Naser"
      },
      {
        "surname": "Kirchbuchner",
        "given_name": "Florian"
      },
      {
        "surname": "Kuijper",
        "given_name": "Arjan"
      }
    ]
  },
  {
    "title": "Adaptive volumetric texture segmentation based on Gaussian Markov random fields features",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.09.035",
    "abstract": "An adaptive method based on three dimensional Gaussian Markov Random fields (3D-GMRF) is proposed in this paper for volumetric texture segmentation. A feature vector is extracted for each voxel in a given volumetric texture image using an estimation cube. However, the selection of the size for this estimation cube causes some fundamental issues related to the uncertainty principle and the inability of the model to capture different texture patterns. These issues are tackled here by employing an adaptive method where the size of the estimation cube is adaptively varying to capture different patterns and also minimize the number of voxels that are related to different texture classes inside the estimation cube. The feature vectors that consist of the estimated parameters of the GMRF and form the parameter volume are hence employed to segment volumetric textures. These features are smoothed by applying an averaging filter using an adaptive averaging technique. Such an averaging filter improves the segmentation results considerably. Our method proposed here is evaluated on a synthetic volumetric texture dataset and compared with other methods to demonstrate the superiority of our segmentation method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303676",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Cube (algebra)",
      "Feature (linguistics)",
      "Filter (signal processing)",
      "Gaussian",
      "Image (mathematics)",
      "Image segmentation",
      "Image texture",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Scale-space segmentation",
      "Segmentation",
      "Texture (cosmology)",
      "Voxel"
    ],
    "authors": [
      {
        "surname": "Almakady",
        "given_name": "Yasseen"
      },
      {
        "surname": "Mahmoodi",
        "given_name": "Sasan"
      },
      {
        "surname": "Bennett",
        "given_name": "Michael"
      }
    ]
  },
  {
    "title": "Identifying noisy labels with a transductive semi-supervised leave-one-out filter",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.09.024",
    "abstract": "Obtaining data with meaningful labels is often costly and error-prone. In this situation, semi-supervised learning (SSL) approaches are interesting, as they leverage assumptions about the unlabeled data to make up for the limited amount of labels. However, in real-world situations, we cannot assume that the labeling process is infallible, and the accuracy of many SSL classifiers decreases significantly in the presence of label noise. In this work, we introduce the LGC _ LVOf , a leave-one-out filtering approach based on the Local and Global Consistency (LGC) algorithm. Our method aims to detect and remove wrong labels, and thus can be used as a preprocessing step to any SSL classifier. Given the propagation matrix, detecting noisy labels takes O ( c l ) per step, with c the number of classes and l the number of labels. Moreover, one does not need to compute the whole matrix, but only a l × l submatrix corresponding to interactions between labeled instances. As a result, our approach is best suited to datasets with a large amount of unlabeled data but not many labels. Results are provided for a number of datasets, including MNIST and ISOLET. LGC _ LVOf appears to be equally or more precise than the adapted gradient-based filter, and thus can be used in practice for active learning, where it may iteratively send labels for re-evaluation. We show that the best-case accuracy of the embedding of LGC _ LVOf into LGC yields performance comparable to the best-case of ℓ1-based classifiers designed to be robust to label noise.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303603",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Deep learning",
      "Filter (signal processing)",
      "Labeled data",
      "Leverage (statistics)",
      "MNIST database",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Preprocessor"
    ],
    "authors": [
      {
        "surname": "de Aquino Afonso",
        "given_name": "Bruno Klaus"
      },
      {
        "surname": "Berton",
        "given_name": "Lilian"
      }
    ]
  },
  {
    "title": "A lower bound for generalized median based consensus learning using kernel-induced distance functions",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.11.003",
    "abstract": "Computing a consensus object from a set of given objects is a core problem in machine learning and pattern recognition. One commonly used approach is to formulate it as an optimization problem using the generalized median. However, in many domains the construction of a median object is NP -hard, requiring approximate solutions instead. In these cases a lower bound is helpful to assess the quality of an approximate median without the need of a ground-truth median result. In this work we introduce a domain-independent lower bound formulation for the generalized median using kernel-induced distances. As kernel functions induce a scalar product in a high-dimensional vector space, they can be used to construct new nonlinear distance functions in the original space with desirable properties. Using the properties of the kernel functions at the basis of the kernel-induced distances, this kernel-based lower bound formulation is shown to be considerably tighter than existing lower bounds on a number of datasets in four different domains, while requiring a lower computational time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520304086",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Domain (mathematical analysis)",
      "Geometry",
      "Kernel (algebra)",
      "Kernel embedding of distributions",
      "Kernel method",
      "Mathematical analysis",
      "Mathematics",
      "Polynomial kernel",
      "Product (mathematics)",
      "Radial basis function kernel",
      "Scalar (mathematics)",
      "Support vector machine",
      "Upper and lower bounds"
    ],
    "authors": [
      {
        "surname": "Nienkötter",
        "given_name": "Andreas"
      },
      {
        "surname": "Jiang",
        "given_name": "Xiaoyi"
      }
    ]
  },
  {
    "title": "Self-supervised pain intensity estimation from facial videos via statistical spatiotemporal distillation",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.09.012",
    "abstract": "Recently, automatic pain assessment technology, in particular automatically detecting pain from facial expressions, has been developed to improve the quality of pain management, and has attracted increasing attention. In this paper, we propose self-supervised learning for automatic yet efficient pain assessment, in order to reduce the cost of collecting large amount of labeled data. To achieve this, we introduce a novel similarity function to learn generalized representations using a Siamese network in the pretext task. The learned representations are finetuned in the downstream task of pain intensity estimation. To make the method computationally efficient, we propose Statistical Spatiotemporal Distillation (SSD) to encode the spatiotemporal variations underlying the facial video into a single RGB image, enabling the use of less complex 2D deep models for video representation. Experiments on two publicly available pain datasets and cross-dataset evaluation demonstrate promising results, showing the good generalization ability of the learned representations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303457",
    "keywords": [
      "Artificial intelligence",
      "Categorization",
      "Computer science",
      "Deep learning",
      "Economics",
      "Generalization",
      "Landmark",
      "Law",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "RGB color model",
      "Representation (politics)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Tavakolian",
        "given_name": "Mohammad"
      },
      {
        "surname": "Bordallo Lopez",
        "given_name": "Miguel"
      },
      {
        "surname": "Liu",
        "given_name": "Li"
      }
    ]
  },
  {
    "title": "A light CNN for detecting COVID-19 from CT scans of the chest",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.10.001",
    "abstract": "Computer Tomography (CT) imaging of the chest is a valid diagnosis tool to detect COVID-19 promptly and to control the spread of the disease. In this work we propose a light Convolutional Neural Network (CNN) design, based on the model of the SqueezeNet, for the efficient discrimination of COVID-19 CT images with respect to other community-acquired pneumonia and/or healthy CT images. The architecture allows to an accuracy of 85.03% with an improvement of about 3.2% in the first dataset arrangement and of about 2.1% in the second dataset arrangement. The obtained gain, though of low entity, can be really important in medical diagnosis and, in particular, for Covid-19 scenario. Also the average classification time on a high-end workstation, 1.25 s, is very competitive with respect to that of more complex CNN designs, 13.41 s, witch require pre-processing. The proposed CNN can be executed on medium-end laptop without GPU acceleration in 7.81 s: this is impossible for methods requiring GPU acceleration. The performance of the method can be further improved with efficient pre-processing strategies for witch GPU acceleration is not necessary.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303688",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Coronavirus disease 2019 (COVID-19)",
      "Deep learning",
      "Disease",
      "Human–computer interaction",
      "Image (mathematics)",
      "Image processing",
      "Infectious disease (medical specialty)",
      "Internal medicine",
      "Medical imaging",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)",
      "Pneumonia",
      "Usability"
    ],
    "authors": [
      {
        "surname": "Polsinelli",
        "given_name": "Matteo"
      },
      {
        "surname": "Cinque",
        "given_name": "Luigi"
      },
      {
        "surname": "Placidi",
        "given_name": "Giuseppe"
      }
    ]
  },
  {
    "title": "Boosting gender identification using author preference",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.10.002",
    "abstract": "Predicting the gender of a text document’s author, also known as gender identification, is a well-studied authorship categorization task in the literature. A common theme in gender identification studies is that gender is considered a binary task. However, digital communications provide users with the ability to select virtual genders leveraging physical anonymity. In this study, the additional duality on gender due to author preferences is examined along with the biological gender. Formally, the objective of this paper is to investigate whether the gender preference of an author contains any additional linguistic information. Furthermore, we explore whether this information can be exploited to improve the author characterization task. In particular, the self-assigned gender, i.e., virtual gender, of the users in text-based real-time online messaging services, along with the biological sex, is evaluated quantitatively via comparing/assessing the gender prediction performance under various settings. Experiment results show that by integrating the virtual gender into the binary classification problem of predicting an author’s gender, it is possible to further improve the prediction performance by 2.6%, up to 85.4%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303767",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Categorization",
      "Computer science",
      "Economics",
      "Gender bias",
      "Gender identity",
      "Identification (biology)",
      "Machine learning",
      "Management",
      "Mathematics",
      "Natural language processing",
      "Preference",
      "Psychology",
      "Social psychology",
      "Statistics",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Kucukyilmaz",
        "given_name": "Tayfun"
      },
      {
        "surname": "Deniz",
        "given_name": "Ayça"
      },
      {
        "surname": "Kiziloz",
        "given_name": "Hakan Ezgi"
      }
    ]
  },
  {
    "title": "A single shot multibox detector based on welding operation method for biometrics recognition in smart cities",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.10.016",
    "abstract": "As enhance of safety requirement in the smart cities, biometrics recognition, as an approach for society safety, has been greatly researched and developed. The identification of working status of welders will help judge whether they are wearing personal protective equipment correctly. We proposed an improved algorithm based on SSD (Single Shot Multibox Detector) that can identify three mainstream manual welding methods including SMAW (shielded metal arc welding), GMAW (gas metal arc welding) and TIG (tungsten inert gas), which has never been researched before and can promote the intelligentization of welding monitoring to construct smart cities. The improvement includes two parts. Firstly, the backbone of SSD is replaced with MobileNetV3. Then, a feature fusion module is added to enhance the information of low-level feature maps to improve detection accuracy. The experimental results of our welding behavior detector show that, the mAP is 87.45%, detection speed is 25FPS, and parameter memory is 87.5 MB, which has a relatively excellent performance considering speed, accuracy and memory.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520304037",
    "keywords": [
      "Arc welding",
      "Artificial intelligence",
      "Biology",
      "Biometrics",
      "Botany",
      "Computer science",
      "Computer vision",
      "Detector",
      "Engineering",
      "Feature (linguistics)",
      "Gas metal arc welding",
      "Gas tungsten arc welding",
      "Identification (biology)",
      "Linguistics",
      "Mechanical engineering",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Shielded metal arc welding",
      "Telecommunications",
      "Welding"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Hongzhi"
      },
      {
        "surname": "Li",
        "given_name": "Changfan"
      },
      {
        "surname": "Chen",
        "given_name": "Weiming"
      },
      {
        "surname": "Jiang",
        "given_name": "Zijie"
      }
    ]
  },
  {
    "title": "Rich heterogeneous information preserving network representation learning",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107564",
    "abstract": "Network representation learning has attracted increasing attention recently due to its applicability in network analysis. However, most existing network representation learning models only focus on preserving fragmentary aspects of network information, either node proximities or fixed semantic information. In this paper, we propose a novel algorithm named Rich Heterogeneous Information Preserving Network Representation Learning (HIRL), which integrates the high-order proximity among nodes and semantic information into a generic framework by exploiting a flexible autoencoder network. Based on the proposed algorithm, we can explore the hidden information in heterogeneous information networks through any custom form of path schema, and represents different types of nodes in a continuous and common vector space. Moreover, the proposed HIRL is applicable to homogeneous information networks. Extensive experimental results demonstrate that our approach can effectively preserve the information in networks under various path schemas, and performs better on real-world applications such as network reconstruction, link prediction, and node classification compared with the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303678",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Engineering",
      "Feature learning",
      "Heterogeneous network",
      "Law",
      "Machine learning",
      "Node (physics)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Schema (genetic algorithms)",
      "Structural engineering",
      "Telecommunications",
      "Theoretical computer science",
      "Wireless",
      "Wireless network"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Bin"
      },
      {
        "surname": "Hu",
        "given_name": "Jinzhi"
      },
      {
        "surname": "Xie",
        "given_name": "Yu"
      },
      {
        "surname": "Zhang",
        "given_name": "Chen"
      },
      {
        "surname": "Tang",
        "given_name": "Zhouhua"
      }
    ]
  },
  {
    "title": "Enhanced factorization machine via neural pairwise ranking and attention networks",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.11.010",
    "abstract": "The factorization machine models attract significant attention nowadays since they improve recommendation performance by incorporating context information into recommendation modeling. However, traditional factorization machine models often adopt the point-wise learning method for model parameter learning, as well as only model the linear interactions between features. They substantially fail to capture the complex interactions among features, which degrades the performance of factorization machine models. In this research, we propose a neural pairwise ranking factorization machine for item recommendation, namely NPRFM, which integrates the multi-layer perceptual neural networks into the pairwise ranking factorization machine model. Specifically, to capture the high-order and nonlinear interactions among features, we stack a multi-layer perceptual neural network over the bi-interaction layer, which encodes the second-order interactions between features. Moreover, instead of the prediction of the absolute scores, the pair-wise ranking model is adopted to learn the relative preferences of users. Since NPRFM does not take into account the importance of feature interactions, we propose a new variant of NPRFM, which learns the importance of feature interactions by introducing the attention mechanism. The empirical results on real-world datasets indicate that the proposed neural pairwise ranking factorization machine outperforms the traditional factorization machine models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520304153",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Eigenvalues and eigenvectors",
      "Factorization",
      "Feature (linguistics)",
      "Linguistics",
      "Machine learning",
      "Matrix decomposition",
      "Pairwise comparison",
      "Paleontology",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Ranking (information retrieval)"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Yonghong"
      },
      {
        "surname": "Jiao",
        "given_name": "Lihong"
      },
      {
        "surname": "Zhou",
        "given_name": "Ningning"
      },
      {
        "surname": "Zhang",
        "given_name": "Li"
      },
      {
        "surname": "Yin",
        "given_name": "Hongzhi"
      }
    ]
  },
  {
    "title": "Geometric rectification of document images using adversarial gated unwarping network",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107576",
    "abstract": "Document images captured in natural scenes with a hand-held camera often suffer from geometric distortions and cluttered backgrounds. In this paper, we propose a simple yet efficient deep model named Adversarial Gated Unwarping Network (AGUN) to rectify these images. In this model, the rectification task is recast as a dense grid prediction problem. We thereby develop a pyramid encoder-decoder architecture to predict the unwarping grid at multiple resolutions in a coarse-to-fine fashion. Based on the observation that the structural visual cues, e.g., text-lines, text blocks, lines in tables, which are critical for the estimation of unwarping mapping, are non-uniformly distributed in the images, three gated modules are introduced to guide the network focusing on these informative cues rather than other interferences such as blank areas and complex backgrounds. To generate more visually pleasing rectification results, we further adopt adversarial training mechanism to implicitly constrain the unwarping grid estimation. Our model can rectify arbitrarily distorted document images with complicated page layouts and cluttered backgrounds. Experiments on the public benchmark dataset and the synthetic dataset demonstrate that our approach outperforms the state-of-the-art methods in terms of OCR accuracy and several widely used quantitative evaluation metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303794",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Encoder",
      "Geodesy",
      "Geography",
      "Geometry",
      "Grid",
      "Image (mathematics)",
      "Image rectification",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Power (physics)",
      "Pyramid (geometry)",
      "Quantum mechanics",
      "Rectification"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Xiyan"
      },
      {
        "surname": "Meng",
        "given_name": "Gaofeng"
      },
      {
        "surname": "Fan",
        "given_name": "Bin"
      },
      {
        "surname": "Xiang",
        "given_name": "Shiming"
      },
      {
        "surname": "Pan",
        "given_name": "Chunhong"
      }
    ]
  },
  {
    "title": "Relational graph neural network for situation recognition",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107544",
    "abstract": "Recently, situation recognition as a new challenging task for image understanding has gained great attention, which needs to simultaneously predict the main activity (verb) and its associated objects (noun entities) in a structured and detailed way. Several methods have been proposed to handle this task, but usually they cannot effectively model the relationships between the activity and the objects. In this paper, we propose a Relational Graph Neural Network (RGNN) for situation recognition, which builds a neural graph on the activity and the objects, and models the triplet relationships between the activity and pairs of objects through message passing between graph nodes. Moreover, we propose a two-stage training strategy to optimize the model. A progressive supervised learning is first adopted to obtain an initial prediction for the activity and the objects. Then, the initial predictions are refined by using a policy-gradient method to directly optimize the non-differentiable value-all metric. To verify the effectiveness of our method, we perform extensive experiments on the Imsitu dataset which is currently the only available dataset for situation recognition. Experimental results show that our approach outperforms the state-of-the-art methods on verb and value metrics, and demonstrates better relationships between the activity and the objects.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303472",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Economics",
      "Graph",
      "Machine learning",
      "Management",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Task (project management)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Jing",
        "given_name": "Ya"
      },
      {
        "surname": "Wang",
        "given_name": "Junbo"
      },
      {
        "surname": "Wang",
        "given_name": "Wei"
      },
      {
        "surname": "Wang",
        "given_name": "Liang"
      },
      {
        "surname": "Tan",
        "given_name": "Tieniu"
      }
    ]
  },
  {
    "title": "SCUT-HCCDoc: A new benchmark dataset of handwritten Chinese text in unconstrained camera-captured documents",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107559",
    "abstract": "In this paper, we introduce a large-scale dataset, called SCUT-HCCDoc, to address challenging detection and recognition problems of handwritten Chinese text (HCT) in the camera-captured documents. Despite extensive studies of optical character recognition (OCR) and offline handwriting recognition for document images, text detection and recognition in the camera-captured documents remains an unsolved problem that is worth for extensive study and investigation. With recent advances in deep learning, researchers have proposed useful architectures for feature learning, detection, and recognition for the scene text. However, the performance of deep learning methods highly depends on the amount and diversity of training data. Previous OCR and offline HCT datasets were built under specific constraints, and most of the recent scene text datasets are for non-handwritten text. Hence, there is a lack of a comprehensive scene handwritten text benchmark. This study focuses on scenes with handwritten Chinese text. We introduce the SCUT-HCCDoc database for HCT detection, recognition and spotting. SCUT-HCCDoc contains 12,253 camera-captured document images with 116,629 text lines and 1,155,801 characters. The diversity of SCUT-HCCDoc can be described at three levels: (1) image-level diversity: image appearance and geometric variances caused by camera-captured settings (such as perspective, background, and resolution) and different applications (such as note-taking, test papers, and homework); (2) text-level diversity: variances of text line length, rotation, etc.; (3) character-level diversity: variances of character categories (up to 6109 classes with additional English letters, and digits), character size, individual writing style, etc. Three kinds of baseline experiments were conducted, where we used several popular text detection methods for text line detection, CTC-based/attention-based methods for text line recognition, and combine text detectors with CTC-based recognizer to achieve end-to-end text spotting. The results indicate the diversity of SCUT-HCCDoc and the challenges of HCT understanding in document images. The dataset is available at https://github.com/HCIILAB/SCUT-HCCDoc_Dataset_Release.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303629",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Character (mathematics)",
      "Computer science",
      "Deep learning",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Geometry",
      "Handwriting",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Natural language processing",
      "Optical character recognition",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Speech recognition",
      "Text detection",
      "Text recognition"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Hesuo"
      },
      {
        "surname": "Liang",
        "given_name": "Lingyu"
      },
      {
        "surname": "Jin",
        "given_name": "Lianwen"
      }
    ]
  },
  {
    "title": "Abnormal event detection in surveillance videos based on low-rank and compact coefficient dictionary learning",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107355",
    "abstract": "In this paper, a novel approach to abnormal event detection in crowded scenes is presented based on a new low-rank and compact coefficient dictionary learning (LRCCDL) algorithm. First, based on the background subtraction and binarization of surveillance videos, we construct a feature space by extracting the histogram of maximal optical flow projection (HMOFP) feature of the foreground from a normal training frame set. Second, in the training stage, a new joint optimization of the nuclear-norm and l 2, 1-norm is applied to obtain a compact coefficient low-rank dictionary. Third, in the detection stage, l 2, 1-norm optimization is utilized to obtain the reconstruction coefficient vectors of the testing samples. Note that the l 2, 1-norm forces the reconstruction vectors of all the testing samples to compactly surround the same center in the training stage, such that the reconstruction errors of abnormal testing samples are different from those of normal ones. Finally, a reconstruction cost (RC) is introduced to detect abnormal frames. Experimental results on both global and local abnormal event detection show the effectiveness of our algorithm. Based on comparisons with state-of-the-art methods employing various criteria, the proposed algorithm achieves comparable detection results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320301588",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Background subtraction",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Eigenvalues and eigenvectors",
      "Feature vector",
      "Frame (networking)",
      "Histogram",
      "Image (mathematics)",
      "Law",
      "Mathematics",
      "Matrix norm",
      "Norm (philosophy)",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel",
      "Political science",
      "Projection (relational algebra)",
      "Quantum mechanics",
      "Rank (graph theory)",
      "Subtraction",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Ang"
      },
      {
        "surname": "Miao",
        "given_name": "Zhenjiang"
      },
      {
        "surname": "Cen",
        "given_name": "Yigang"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiao-Ping"
      },
      {
        "surname": "Zhang",
        "given_name": "Linna"
      },
      {
        "surname": "Chen",
        "given_name": "Shiming"
      }
    ]
  },
  {
    "title": "Discovering and incorporating latent target-domains for domain adaptation",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107536",
    "abstract": "In this paper, we aim to address the unsupervised domain adaptation problem where the data in the target domain are much more diverse compared with the data in the source domain. In particular, this problem is formulated as discovering and incorporating latent domains underlying target data of interest for unsupervised domain adaptation. More specifically, the discovery of the latent target domains is based on three criteria, including the maximization of compactness and distinctiveness of the data in the individual latent target-domain, as well as the minimization of total divergence from the latent target-domains to the source domain. For each pair formed by a latent target domain and the source domain, we learn a feature space where the discrepancy between the source domain and the specific latent target domain is shrunk. Finally, we consider the projected source domain data on the learned latent feature spaces as different views of the source domain, and propose an extended multiple kernel learning algorithm to train a more robust and precise classifier for predicting the unlabeled target data. The effectiveness of our proposed method is demonstrated on various benchmark datasets for object recognition and human activity recognition. Moreover, we also show that our proposed method can be treated as an effective complement to the deep learning based unsupervised domain adaptation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303393",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Domain (mathematical analysis)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Haoliang"
      },
      {
        "surname": "Li",
        "given_name": "Wen"
      },
      {
        "surname": "Wang",
        "given_name": "Shiqi"
      }
    ]
  },
  {
    "title": "Y-Autoencoders: Disentangling latent representations via sequential encoding",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.09.025",
    "abstract": "In the last few years there have been important advancements in disentangling latent representations using generative models, with the two dominant approaches being Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs). However, standard Autoencoders (AEs) and closely related structures have remained popular because they are easy to train and adapt to different tasks. An interesting question is if we can achieve state-of-the-art latent disentanglement with AEs while retaining their good properties. We propose an answer to this question by introducing a new model called Y-Autoencoder (Y-AE). The structure and training procedure of a Y-AE enclose a representation into an implicit and an explicit part. The implicit part is similar to the output of an AE and the explicit part is strongly correlated with labels in the training set. The two parts are separated in the latent space by splitting the output of the encoder into two paths (forming a Y shape) before decoding and re-encoding. We then impose a number of losses, such as reconstruction loss, and a loss on dependence between the implicit and explicit parts. Additionally, the projection in the explicit manifold is monitored by a predictor, that is embedded in the encoder and trained end-to-end with no adversarial losses. We provide significant experimental results on various domains, such as separation of style and content, image-to-image translation, and inverse graphics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303494",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Computer science",
      "Decoding methods",
      "Encoder",
      "Encoding (memory)",
      "Generative grammar",
      "Generative model",
      "Image (mathematics)",
      "Image translation",
      "Law",
      "Operating system",
      "Orthographic projection",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Projection (relational algebra)",
      "Representation (politics)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Patacchiola",
        "given_name": "Massimiliano"
      },
      {
        "surname": "Fox-Roberts",
        "given_name": "Patrick"
      },
      {
        "surname": "Rosten",
        "given_name": "Edward"
      }
    ]
  },
  {
    "title": "Spectral clustering via ensemble deep autoencoder learning (SC-EDAE)",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107522",
    "abstract": "Several works have studied clustering strategies that combine classical clustering algorithms and deep learning methods. These strategies generally improve clustering performance, however deep autoencoder setting issues impede the robustness of these approaches. To alleviate the impact of hyperparameters setting, we propose a model which combines spectral clustering and deep autoencoder strengths in an ensemble framework. Our proposal does not require any pretraining and includes the three following steps: generating various deep embeddings from the original data, constructing a sparse and low-dimensional ensemble affinity matrix based on anchors strategy and applying spectral clustering to obtain the common space shared by multiple deep representations. While the anchors strategy ensures an efficient merging of the encodings, the fusion of various deep representations enables to mitigate the deep networks setting issues. Experiments on various benchmark datasets demonstrate the potential and robustness of our approach compared to state-of-the-art deep clustering methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303253",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Cluster analysis",
      "Computer science",
      "Deep learning",
      "Feature learning",
      "Gene",
      "Geodesy",
      "Geography",
      "Hyperparameter",
      "Initialization",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Programming language",
      "Robustness (evolution)",
      "Spectral clustering"
    ],
    "authors": [
      {
        "surname": "Affeldt",
        "given_name": "Séverine"
      },
      {
        "surname": "Labiod",
        "given_name": "Lazhar"
      },
      {
        "surname": "Nadif",
        "given_name": "Mohamed"
      }
    ]
  },
  {
    "title": "Faster ILOD: Incremental learning for object detectors based on faster RCNN",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.09.030",
    "abstract": "The human vision and perception system is inherently incremental where new knowledge is continually learned over time whilst existing knowledge is retained. On the other hand, deep learning networks are ill-equipped for incremental learning. When a well-trained network is adapted to new categories, its performance on the old categories will dramatically degrade. To address this problem, incremental learning methods have been explored which preserve the old knowledge of deep learning models. However, the state-of-the-art incremental object detector employs an external fixed region proposal method that increases overall computation time and reduces accuracy comparing to Region Proposal Network (RPN) based object detectors such as Faster RCNN. The purpose of this paper is to design an efficient end-to-end incremental object detector using knowledge distillation. We first evaluate and analyze the performance of the RPN-based detector with classic distillation on incremental detection tasks. Then, we introduce multi-network adaptive distillation that properly retains knowledge from the old categories when fine-tuning the model for new task. Experiments on the benchmark datasets, PASCAL VOC and COCO, demonstrate that the proposed incremental detector based on Faster RCNN is more accurate as well as being 13 times faster than the baseline detector.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303627",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Deep learning",
      "Detector",
      "Engineering",
      "Geodesy",
      "Geography",
      "Incremental learning",
      "Machine learning",
      "Object (grammar)",
      "Object detection",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Systems engineering",
      "Task (project management)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Can"
      },
      {
        "surname": "Zhao",
        "given_name": "Kun"
      },
      {
        "surname": "Lovell",
        "given_name": "Brian C."
      }
    ]
  },
  {
    "title": "Sensor-based and vision-based human activity recognition: A comprehensive survey",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107561",
    "abstract": "Human activity recognition (HAR) technology that analyzes data acquired from various types of sensing devices, including vision sensors and embedded sensors, has motivated the development of various context-aware applications in emerging domains, e.g., the Internet of Things (IoT) and healthcare. Even though a considerable number of HAR surveys and review articles have been conducted previously, the major/overall HAR subject has been ignored, and these studies only focus on particular HAR topics. Therefore, a comprehensive review paper that covers major subjects in HAR is imperative. This survey analyzes the latest state-of-the-art research in HAR in recent years, introduces a classification of HAR methodologies, and shows advantages and weaknesses for methods in each category. Specifically, HAR methods are classified into two main groups, which are sensor-based HAR and vision-based HAR, based on the generated data type. After that, each group is divided into subgroups that perform different procedures, including the data collection, pre-processing methods, feature engineering, and the training process. Moreover, an extensive review regarding the utilization of deep learning in HAR is also conducted. Finally, this paper discusses various challenges in the current HAR topic and offers suggestions for future research.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303642",
    "keywords": [
      "Activity recognition",
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Context (archaeology)",
      "Data collection",
      "Data science",
      "Deep learning",
      "Feature engineering",
      "Geography",
      "Internet of Things",
      "Machine learning",
      "Mathematics",
      "Psychology",
      "Social psychology",
      "Statistics",
      "Strengths and weaknesses",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Minh Dang",
        "given_name": "L."
      },
      {
        "surname": "Min",
        "given_name": "Kyungbok"
      },
      {
        "surname": "Wang",
        "given_name": "Hanxiang"
      },
      {
        "surname": "Jalil Piran",
        "given_name": "Md."
      },
      {
        "surname": "Hee Lee",
        "given_name": "Cheol"
      },
      {
        "surname": "Moon",
        "given_name": "Hyeonjoon"
      }
    ]
  },
  {
    "title": "Graph-based non-maximal suppression for detecting products on the rack",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.09.023",
    "abstract": "Identification of stacked retail products from the images of racks of supermarket is a challenging computer vision problem. Regions with convolutional neural network features (R-CNN) generate (mostly overlapped) region proposals around the products on the rack. Subsequently, the region proposals are classified using a convolutional neural network. In the end, R-CNN implements a greedy non-maximal suppression (greedy-NMS) for disambiguating the overlapping proposals. Greedy-NMS discards the proposals (with lower classification scores) that are overlapped with the proposal with higher classification score. This greedy approach often eliminates the (geometrically) better fitted region proposals with (marginally) lower classification scores. This paper introduces a novel graph-based non-maximal suppression (G-NMS) that removes this critical bottleneck of greedy-NMS by looking not only at the classification scores but also at the product classes of the overlapping region proposals. G-NMS first determines the potential confidence scores (pc-scores) of the region proposals by defining the groups of overlapping regions. Subsequently, a directed acyclic graph (DAG) is strategically constructed with the proposals utilizing their pc-scores and overlapping groups. Eventually the maximum weighted path of the DAG provides the products that are present in the rack. The results of our extensive experiments confirm that the proposed scheme is better up to around 7% on one large In-house and three benchmark datasets of retail products. Additionally, the efficacy of our proposed GNMS is also analyzed on four benchmark datasets for detecting generic objects.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303597",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Bottleneck",
      "Computer science",
      "Convolutional neural network",
      "Embedded system",
      "Geodesy",
      "Geography",
      "Graph",
      "Greedy algorithm",
      "Law",
      "Pattern recognition (psychology)",
      "Political science",
      "Rack",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Santra",
        "given_name": "Bikash"
      },
      {
        "surname": "Shaw",
        "given_name": "Avishek Kumar"
      },
      {
        "surname": "Mukherjee",
        "given_name": "Dipti Prasad"
      }
    ]
  },
  {
    "title": "Class mean vector component and discriminant analysis",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.10.014",
    "abstract": "The kernel matrix used in kernel methods encodes all the information required for solving complex nonlinear problems defined on data representations in the input space using simple, but implicitly defined, solutions. Spectral analysis on the kernel matrix defines an explicit nonlinear mapping of the input data representations to a subspace of the kernel space, which can be used for directly applying linear methods. However, the selection of the kernel subspace is crucial for the performance of the proceeding processing steps. We propose a new optimization criterion, leading to a new component analysis method for kernel-based dimensionality reduction that optimally preserves the pair-wise distances of the class means in the feature space. This leads to efficient kernel subspace learning, which is crucial for kernel-based machine learning solutions. We provide extensive analysis on the connections and differences between the proposed criterion and the criteria used in kernel Principal Component Analysis, kernel Entropy Component Analysis and Kernel Discriminant Analysis, leading to a discriminant analysis version of the proposed method. Our theoretical analysis also provides more insights on the properties of the feature spaces obtained by applying these methods. Results on a variety of visual classification problems illustrate the properties of the proposed methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520304001",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Dimensionality reduction",
      "Discrete mathematics",
      "Feature vector",
      "Kernel (algebra)",
      "Kernel Fisher discriminant analysis",
      "Kernel embedding of distributions",
      "Kernel method",
      "Kernel principal component analysis",
      "Linear discriminant analysis",
      "Mathematics",
      "Optimal discriminant analysis",
      "Pattern recognition (psychology)",
      "Polynomial kernel",
      "Principal component analysis",
      "Radial basis function kernel",
      "String kernel",
      "Subspace topology",
      "Support vector machine",
      "Variable kernel density estimation"
    ],
    "authors": [
      {
        "surname": "Iosifidis",
        "given_name": "Alexandros"
      }
    ]
  },
  {
    "title": "3D PostureNet: A unified framework for skeleton-based posture recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.09.029",
    "abstract": "Image-based posture recognition is a very challenging problem as it is difficult to acquire rich 3D information from postures in 2D images. Existing methods founded on 3D skeleton cues could alleviate this issue, but they are not particularly efficient due to the application of handcrafted features and traditional classifiers. This paper presents a novel and unified framework for skeleton-based posture recognition, applying powerful 3D Convolutional Neural Network (CNN) to this issue. Technically, bounding-box-based normalization for the raw skeleton data is proposed to eliminate the coordinate differences caused by diverse recording environments and posture displacements. Moreover, Gaussian voxelization for the skeleton is employed to expressively represent the posture configuration. Thereby, an end-to-end framework based on 3D CNN, called 3D PostureNet, is developed for robust posture recognition. To verify its effectiveness, a large-scale writing posture dataset is created and released in this work, including 113,400 samples of 30 subjects with 15 postures. Extensive experiments on the public MSRA hand gesture dataset, body pose dataset and the proposed writing posture dataset demonstrate that 3D PostureNet achieves significantly superior performance on both skeleton-based human posture and hand posture recognition tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303615",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Gesture",
      "Human skeleton",
      "Image (mathematics)",
      "Minimum bounding box",
      "Normalization (sociology)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Skeleton (computer programming)",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Jianbo"
      },
      {
        "surname": "Wang",
        "given_name": "Ying"
      },
      {
        "surname": "Liu",
        "given_name": "Yongcheng"
      },
      {
        "surname": "Xiang",
        "given_name": "Shiming"
      },
      {
        "surname": "Pan",
        "given_name": "Chunhong"
      }
    ]
  },
  {
    "title": "Revisiting hierarchy: Deep learning with orthogonally constrained prior for classification",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.10.006",
    "abstract": "Deep learning has attracted significant attention for its applications to a variety of classification problems, such as handwritten recognition, image classification and document categorization. One reason behind the success of deep learning can be attributed to its strong representation power with multiple layers of hidden variables. However, complex models are often encompassed with overfitting problems when limited training data is available. In this paper, we are interested in deep learning for classification with prior, where the set of labels are expressed in a hierarchy. In particular, we attempt to leverage knowledge transfer and parameter sharing among classes. We introduce the orthogonally constrained prior into deep learning, which exploits the following information among different classes: (1) introduce weight decomposition to model parameter sharing along the path from the root to leafs; (2) incorporate the orthogonal restrictions among adjacent nodes in the hierarchy. We test our method on challenge datasets in the case of a few training examples available, and show promising results compared to support vector machines and other deep learning methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303810",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Categorization",
      "Computer science",
      "Contextual image classification",
      "Deep learning",
      "Economics",
      "Feature learning",
      "Hierarchy",
      "Image (mathematics)",
      "Leverage (statistics)",
      "Machine learning",
      "Market economy",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Test set",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Gang"
      },
      {
        "surname": "Srihari",
        "given_name": "Sargur N."
      }
    ]
  },
  {
    "title": "Pupil size as a soft biometrics for age and gender classification",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.10.009",
    "abstract": "The pupil size is a soft biometric trait but in-depth study to analyze it for biometric purposes is lacking in the literature, as well as datasets focused on this field of research are missing. On the basis of these observations, we present an extensive study with the objective of demonstrating that pupil size and dilation over time can be potentially used to classify people by age and by gender. To do this, 14 supervised classifiers were applied on a dataset meant for gaze analysis. Measuring the right and left pupil individually and also simultaneously, the performances of the classifiers have been compared and the worst and best performing selected to support potential fusion strategies. If good results have been obtained for age classification, ranging in 79%–82% of accuracy, it cannot be said the same for gender. This is due to the dual nature of this biometric trait. Pupil size can be considered as a physical trait for the age classification and behavioral trait for the gender. The results achieved in this study suggest the need for more balanced and reliable datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303962",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Field (mathematics)",
      "Gaze",
      "Mathematics",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Programming language",
      "Psychology",
      "Pupil",
      "Pupil size",
      "Pupillary response",
      "Pupillometry",
      "Pure mathematics",
      "Trait"
    ],
    "authors": [
      {
        "surname": "Cascone",
        "given_name": "Lucia"
      },
      {
        "surname": "Medaglia",
        "given_name": "Carlo"
      },
      {
        "surname": "Nappi",
        "given_name": "Michele"
      },
      {
        "surname": "Narducci",
        "given_name": "Fabio"
      }
    ]
  },
  {
    "title": "Rank consistent ordinal regression for neural networks with application to age estimation",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.11.008",
    "abstract": "In many real-world prediction tasks, class labels include information about the relative ordering between labels, which is not captured by commonly-used loss functions such as multi-category cross-entropy. Recently, the deep learning community adopted ordinal regression frameworks to take such ordering information into account. Neural networks were equipped with ordinal regression capabilities by transforming ordinal targets into binary classification subtasks. However, this method suffers from inconsistencies among the different binary classifiers. To resolve these inconsistencies, we propose the COnsistent RAnk Logits (CORAL) framework with strong theoretical guarantees for rank-monotonicity and consistent confidence scores. Moreover, the proposed method is architecture-agnostic and can extend arbitrary state-of-the-art deep neural network classifiers for ordinal regression tasks. The empirical evaluation of the proposed rank-consistent method on a range of face-image datasets for age prediction shows a substantial reduction of the prediction error compared to the reference ordinal regression network.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552030413X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Binary classification",
      "Combinatorics",
      "Computer science",
      "Cross entropy",
      "Entropy (arrow of time)",
      "Machine learning",
      "Mathematics",
      "Ordinal data",
      "Ordinal optimization",
      "Ordinal regression",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Rank (graph theory)",
      "Regression",
      "Statistics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Cao",
        "given_name": "Wenzhi"
      },
      {
        "surname": "Mirjalili",
        "given_name": "Vahid"
      },
      {
        "surname": "Raschka",
        "given_name": "Sebastian"
      }
    ]
  },
  {
    "title": "AutoRSISC: Automatic design of neural architecture for remote sensing image scene classification",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.09.034",
    "abstract": "Remote sensing image scene classification (RSISC) is one of the bases of visual question answering for remote sensing data (RSVQA). In the conventional deep learning based solutions of RSISC, domain experts have to manually design the neural architecture, which is costly in both time and manpower. In this paper, AutoRSISC, a high resolution remote sensing image scene classification method based on neural architecture search (NAS) is proposed. AutoRSISC algorithm samples the neural architecture in a certain proportion to reduce the redundancy in the search space which is based on the continuous relaxation of the neural architecture representation. Edge normalization is introduced to make architecture search more efficient. The conventional deep learning approaches take days or more just to manually design the appropriate neural architecture according to the current data set. The experimental results show that our method saves time and manpower compared with the existing manually designed RSISC methods. Our AutoRSISC algorithm took 7.6 h from the automatic design of the most appropriate neural architecture to the end of all experiments according to the UCM data set, and the classification accuracy rate of the final test set reached 97.85%. And our AutoRSISC algorithm took 101.7 h from the automatic design of the most appropriate neural architecture to the end of all experiments according to the NWPU45 data set, and the classification accuracy rate of the final test set reached 94.66%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303664",
    "keywords": [
      "Anthropology",
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Contextual image classification",
      "Data mining",
      "Data set",
      "Image (mathematics)",
      "Normalization (sociology)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Programming language",
      "Redundancy (engineering)",
      "Set (abstract data type)",
      "Sociology",
      "Test data",
      "Test set",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Jing",
        "given_name": "Weipeng"
      },
      {
        "surname": "Ren",
        "given_name": "Quanlin"
      },
      {
        "surname": "Zhou",
        "given_name": "Jun"
      },
      {
        "surname": "Song",
        "given_name": "Houbing"
      }
    ]
  },
  {
    "title": "Fast and incremental algorithms for exponential semi-supervised discriminant embedding",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107530",
    "abstract": "In various pattern classification problems, semi-supervised learning methods have shown its effectiveness in utilizing unlabeled data to yield better performance than some supervised and unsupervised learning methods. Semi-supervised discriminant embedding (SDE) is a semi-supervised extension of local discriminant embedding (LDE). However, when dealing with high dimensional data, SDE often suffers from the small-sample-size (SSS) problem. In order to settle this problem, an exponential semi-supervised discriminant embedding (ESDE) method was proposed in [F. Dornaika, Y. EI Traboulsi. Matrix exponential based semi-supervised discriminant embedding for image classification, Pattern Recognition, 61 (2017): 92–103], which makes use of the tool of matrix exponential. Despite its high discriminative ability, the computational overhead of ESDE is very large for high dimensional data. In order to cure this drawback, the first contribution of this paper is to propose a fast implementation on the ESDE method. The key is to equivalently transform the large matrix problem of size d into a much smaller one of size n, where d is the data dimension and n is the number of training samples, with d ≫ n in practice. On the other hand, in many real world applications, it is likely that whole labeled training set is unavailable beforehand, and the training data is obtained incrementally. Many incremental semi-supervised learning methods have been proposed to deal with this problem, to the best of our knowledge, however, there are no incremental algorithms for matrix exponential discriminant methods till now. To fill in this gap, the second contribution of this paper is to propose incremental ESDE algorithms for incremental learning problems. Numerical experiments on some real-world data sets show the numerical behavior of the proposed algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303332",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Composite material",
      "Computer science",
      "Dimension (graph theory)",
      "Discriminant",
      "Discriminative model",
      "Embedding",
      "Linear discriminant analysis",
      "Machine learning",
      "Materials science",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Optimal discriminant analysis",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Supervised learning"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Yingdi"
      },
      {
        "surname": "Wu",
        "given_name": "Gang"
      }
    ]
  },
  {
    "title": "View-specific subspace learning and re-ranking for semi-supervised person re-identification",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107568",
    "abstract": "Person re-identification (re-ID) focuses on matching the same person across non-overlapping camera views. Most existing methods require tedious manual annotation and can only learn a unitary transformation for images across views, which severely lack of scalability and suffer from view-specific biases. To address these issues, we put forward a View-Specific Semi-supervised Subspace Learning (VS-SSL) approach that can learn specific projections for each view, utilizing limited labeled data to guide the training while leveraging abundant unlabeled data simultaneously. Moreover, a novel re-ranking strategy is proposed to boost the performance further, which re-estimates the similarity between probe and galleries according to the overlap ratio between their expanded neighbors and their position in each other’s ranking list. The effectiveness of the proposed framework is evaluated on several widely-used datasets (VIPeR, PRID450S, PRID2011, CUHK01 and Market-1501), yielding superior performance for both semi-supervised and supervised re-ID.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032030371X",
    "keywords": [
      "Annotation",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Biology",
      "Botany",
      "Chemistry",
      "Computer science",
      "Database",
      "Gene",
      "Identification (biology)",
      "Image (mathematics)",
      "Machine learning",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Ranking (information retrieval)",
      "Scalability",
      "Similarity (geometry)",
      "Statistics",
      "Subspace topology",
      "Supervised learning",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Jia",
        "given_name": "Jieru"
      },
      {
        "surname": "Ruan",
        "given_name": "Qiuqi"
      },
      {
        "surname": "Jin",
        "given_name": "Yi"
      },
      {
        "surname": "An",
        "given_name": "Gaoyun"
      },
      {
        "surname": "Ge",
        "given_name": "Shiming"
      }
    ]
  },
  {
    "title": "Multilevel fusion of multimodal deep features for porn streamer recognition in live video",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.09.027",
    "abstract": "Live video hosted by streamers is being sought after by an increasing number of Internet users. Some streamers mix pornographic content with live video for profit and popularity, but this greatly harms the network environment. To effectively identify porn streamers, a multilevel fusion method of multimodal deep features for porn streamer recognition in live video is proposed in this paper. (1) Visual and audio features including spatial, audio, motion, and temporal context in live video are extracted by a multimodal deep network. (2) Audio-visual attention features are obtained by fusing visual and audio features at the feature level based on a multimodal attention mechanism. (3) Text features are extracted by using the bullet screen text network based on the BERT (bidirectional encoder representations from transformers) model after collecting text information from the viewers’ bullet screen comments. (4) The prediction results of the audio-visual deep network and the bullet screen text network are fused at the decision level to improve the porn streamer recognition accuracy. We build a real-world dataset of porn streamers and conduct experiments and demonstrate that our method can improve the porn streamer recognition accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303469",
    "keywords": [
      "Artificial intelligence",
      "Audio visual",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Encoder",
      "Feature (linguistics)",
      "Linguistics",
      "Multimedia",
      "Operating system",
      "Paleontology",
      "Philosophy",
      "Popularity",
      "Psychology",
      "Social psychology"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Liyuan"
      },
      {
        "surname": "Zhang",
        "given_name": "Jing"
      },
      {
        "surname": "Wang",
        "given_name": "Meng"
      },
      {
        "surname": "Tian",
        "given_name": "Jimiao"
      },
      {
        "surname": "Zhuo",
        "given_name": "Li"
      }
    ]
  },
  {
    "title": "Mutual information based feature subset selection in multivariate time series classification",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107525",
    "abstract": "This paper deals with supervised classification of multivariate time series. In particular, the goal is to propose a filter method to select a subset of time series. Consequently, we adopt the framework proposed by Brown al. [1]. The key point in this framework is the computation of the mutual information between the features, which allows us to measure the relevance of each feature subset. In our case, where the features are a time series, we use an adaptation of existing nonparametric mutual information estimators based on the k-nearest neighbor. Specifically, for the purpose of bringing these methods to the time series scenario, we rely on the use of dynamic time warping dissimilarity. Our experimental results show that our method is able to strongly reduce the number of time series while keeping or increasing the classification accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303289",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Data mining",
      "Dynamic time warping",
      "Estimator",
      "Feature (linguistics)",
      "Feature selection",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Measure (data warehouse)",
      "Multivariate statistics",
      "Mutual information",
      "Nonparametric statistics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Series (stratigraphy)",
      "Statistics",
      "Time series",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Ircio",
        "given_name": "Josu"
      },
      {
        "surname": "Lojo",
        "given_name": "Aizea"
      },
      {
        "surname": "Mori",
        "given_name": "Usue"
      },
      {
        "surname": "Lozano",
        "given_name": "Jose A."
      }
    ]
  },
  {
    "title": "Generating adversarial examples with elastic-net regularized boundary equilibrium generative adversarial network",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.10.018",
    "abstract": "To improve the attack success rate and image perceptual quality of adversarial examples against deep neural networks(DNNs), we propose a new Generative Adversarial Network (GAN) based attacker, named Elastic-net Regularized Boundary Equilibrium Generative Adversarial Network(ERBEGAN). Recent studies have shown that DNNs are easy to attack by adversarial examples(AEs) where benign images with small-magnitude perturbations mislead DNNs to incorrect results. A number of methods are proposed to generate AEs, but how to generate them with high attack success rate and perceptual quality needs more effort. Most attackers generate AEs by restricting L 2 -norm and L ∞ -norm of adversarial perturbations. However, very few works have been developed on L 1 distortion matrix which encourages sparsity in the perturbation. In this paper, we penalize both L 2 -norm and L 1 -norm of perturbation as Elastic-Net regularization to improve the diversity and robustness of AEs. We further improve GAN by minimizing the additional pixel-wise loss derived from the Wasserstein distance between benign and adversarial auto-encoder loss distributions. Extensive experiments and visualizations on several datasets show that the proposed ERBEGAN can yield higher attack success rates than the state-of-the-art GAN-based attacker AdvGAN under the semi-whitebox and black-box attack settings. Besides, our method efficiently generates diverse adversarial examples that are more perceptually realistic.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520304050",
    "keywords": [
      "Adversarial system",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Deep neural networks",
      "Encoder",
      "Generative adversarial network",
      "Generative grammar",
      "Image (mathematics)",
      "Law",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Norm (philosophy)",
      "Operating system",
      "Perturbation (astronomy)",
      "Physics",
      "Political science",
      "Quantum mechanics",
      "Regularization (linguistics)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Cong"
      },
      {
        "surname": "Wu",
        "given_name": "Xiao-Jun"
      },
      {
        "surname": "Li",
        "given_name": "Zuo-Yong"
      }
    ]
  },
  {
    "title": "A new fuzzy k-nearest neighbor classifier based on the Bonferroni mean",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.10.005",
    "abstract": "We present a new generalized version of the fuzzy k-nearest neighbor (FKNN) classifier that uses local mean vectors and utilizes the Bonferroni mean. We call the proposed new method Bonferroni-mean based fuzzy k-nearest neighbor (BM-FKNN) classifier. The BM-FKNN classifier can be easily fitted for various contexts and applications, because the parametric Bonferroni mean allows for problem-based parameter value fitting. The BM-FKNN classifier can perform well also in situations where clear imbalances in class distributions of data are found. The performance of the proposed classifier is tested with six real-world data sets and with one artificial data set. The results are benchmarked with classification results obtained with the classical k-nearest neighbor-, the local mean-based k-nearest neighbor-, the fuzzy k-nearest neighbor- and other three selected classifiers. In addition to this, an enhancement of the local mean-based k-nearest neighbor classifier by using the Bonferroni means is also proposed and tested. The results show that the proposed new BM-FKNN classifier has the potential to outperform the benchmarks in classification accuracy and confirm the usefulness of using the Bonferroni mean in the learning part of classifiers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303792",
    "keywords": [
      "Artificial intelligence",
      "Bonferroni correction",
      "Classifier (UML)",
      "Computer science",
      "Fuzzy logic",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Statistics",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Mailagaha Kumbure",
        "given_name": "Mahinda"
      },
      {
        "surname": "Luukka",
        "given_name": "Pasi"
      },
      {
        "surname": "Collan",
        "given_name": "Mikael"
      }
    ]
  },
  {
    "title": "Visual manipulation relationship recognition in object-stacking scenes",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.09.014",
    "abstract": "Object manipulation in object-stacking scenes is a significant but challenging skill for intelligent robots. In most cases, the relationships among objects should be considered before manipulation to prevent chaos and damages. However, the analysis of object relationships in object-stacking scenes, especially for robotic manipulation, remains to be unsolved. To this end, this paper presents a new convolutional neural network (CNN) architecture, called Visual Manipulation Relationship Network (VMRN), to recognize the visual manipulation relationships (VMR) between objects in real-time. By considering the manipulation relationships in object-stacking scenes, it ensures that the robot can complete manipulation tasks safely and reliably. The core of our model is the Object Pairing Pooling Layer (OP2L), which makes it possible to recognize objects and all possible VMRs in one forward process. Moreover, to train VMRN, we contribute a dataset named Visual Manipulation Relationship Dataset (VMRD) consisting of 4683 images with more than 16,000 object instances and the VMRs between each object pair. The experimental results show that the proposed network architecture can detect objects and predict VMRs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303445",
    "keywords": [
      "Artificial intelligence",
      "Cognitive neuroscience of visual object recognition",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Nuclear magnetic resonance",
      "Object (grammar)",
      "Operating system",
      "Physics",
      "Pooling",
      "Process (computing)",
      "Robot",
      "Stacking"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Hanbo"
      },
      {
        "surname": "Lan",
        "given_name": "Xuguang"
      },
      {
        "surname": "Zhou",
        "given_name": "Xinwen"
      },
      {
        "surname": "Tian",
        "given_name": "Zhiqiang"
      },
      {
        "surname": "Zhang",
        "given_name": "Yang"
      },
      {
        "surname": "Zheng",
        "given_name": "Nanning"
      }
    ]
  },
  {
    "title": "Pixel re-representations for better classification of images",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.04.027",
    "abstract": "Conventional image capture approaches usually try the best to obtain high-resolution images to allow people to see details of the objects. Still, the high-resolution images do not always imply efficient classification accuracy. Holistic information of images has a more critical influence on the identification of the corresponding objects than detailed information of images. In this paper, we propose to attain optimal pixel re-representations of images and exploit them to perform classification. The merits of the proposed approach are as follows. (1) It automatically selects optimal re-representations for original images from a number of candidates by virtue of a feasible procedure. (2) Maximizing the ratio of the between-class distance to the within-class distance not only allows the optimal re-representations to be attained but also can adequately prompt sparse representation and collaborative representation to attain a better classification accuracy. The procedure of maximizing the ratio makes the obtained optimal re-representations of all objects as diverse as possible. Therefore, the attained linear combination of all training samples, in the form of vectors, of all objects can better express the test sample and has less expression error in comparison with the corresponding case based on original training samples of all objects. The experiments also prove that the optimal pixel re-representations are helpful to improve not only the sparse representation and collaborative representation but also other classification image classification approaches, to obtain more satisfactory classification accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520301495",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Class (philosophy)",
      "Computer science",
      "Computer security",
      "Contextual image classification",
      "Exploit",
      "Identification (biology)",
      "Image (mathematics)",
      "Law",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pixel",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Sparse approximation"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Junqian"
      },
      {
        "surname": "Zhang",
        "given_name": "Hanyu"
      },
      {
        "surname": "Han",
        "given_name": "Peiyi"
      },
      {
        "surname": "Liu",
        "given_name": "Chuanyi"
      },
      {
        "surname": "Xu",
        "given_name": "Yong"
      }
    ]
  },
  {
    "title": "Accurate, data-efficient, unconstrained text recognition with convolutional neural networks",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107482",
    "abstract": "Unconstrained text recognition is an important computer vision task, featuring a wide variety of different sub-tasks, each with its own set of challenges. One of the biggest promises of deep neural networks has been the convergence and automation of feature extractors from input raw signals, allowing for the highest possible performance with minimum required domain knowledge. To this end, we propose a data-efficient, end-to-end neural network model for generic, unconstrained text recognition. In our proposed architecture we strive for simplicity and efficiency without sacrificing recognition accuracy. Our proposed architecture is a fully convolutional network without any recurrent connections trained with the CTC loss function. Thus it operates on arbitrary input sizes and produces strings of arbitrary length in a very efficient and parallelizable manner. We show the generality and superiority of our proposed text recognition architecture by achieving state-of-the-art results on seven public benchmark datasets, covering a wide spectrum of text recognition tasks, namely: Handwriting Recognition, CAPTCHA recognition, OCR, License Plate Recognition, and Scene Text Recognition. Our proposed architecture has won the ICFHR2018 Competition on Automated Text Recognition on a READ Dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320302855",
    "keywords": [],
    "authors": [
      {
        "surname": "Yousef",
        "given_name": "Mohamed"
      },
      {
        "surname": "Hussain",
        "given_name": "Khaled F."
      },
      {
        "surname": "Mohammed",
        "given_name": "Usama S."
      }
    ]
  },
  {
    "title": "On unsupervised simultaneous kernel learning and data clustering",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107518",
    "abstract": "A novel optimization framework for joint unsupervised clustering and kernel learning is derived. Sparse nonnegative matrix factorization of kernel covariance matrices is utilized to categorize data according to their information content. It is demonstrated that a pertinent kernel covariance matrix for clustering can be constructed such that it is block diagonal within arbitrary row and column permutations, while each diagonal block has rank one. To achieve this, a linear combination of a dictionary of kernels is sought such that it has rank equal to the number of clusters while a certain kernel eigenvalue is maximized by a novel difference of convex functions formulation. We establish that the proposed algorithm converges to a stationary solution. Numerical tests with different datasets demonstrate the effectiveness of the proposed scheme whose performance is very close to supervised methods, and performs better than unsupervised alternatives without the need of painstaking parameter tuning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303216",
    "keywords": [
      "Artificial intelligence",
      "Block matrix",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Covariance",
      "Diagonal",
      "Eigenvalues and eigenvectors",
      "Geometry",
      "Kernel (algebra)",
      "Kernel embedding of distributions",
      "Kernel method",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Spectral clustering",
      "Statistics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Malhotra",
        "given_name": "Akshay"
      },
      {
        "surname": "Schizas",
        "given_name": "Ioannis D."
      }
    ]
  },
  {
    "title": "From 3D to 2D: Transferring knowledge for rib segmentation in chest X-rays",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.09.021",
    "abstract": "Chest X-rays are the most common type of biomedical radiologic exam, being widely adopted for the diagnosis of a myriad of illnesses in the thoracic region. Computed Tomography – even though being more expensive and rare – is also a useful tool for the detection of several illnesses and surgery planning, providing volumetric information. This paper proposes a methodology aiming to leverage the larger amounts of spatial information and lack of occlusion in tomographic images to aid in the rib segmentation of 2D X-ray images by means of Domain Adaptation. We perform extensive quantitative and qualitative experiments to test the capabilities of this methodology in segmenting ribs in 7 X-ray datasets with distinct visual features, using 6 different metrics and without any use of rib segmentation labels from the target image sets. In order to encourage reproducibility, all data and code used in this research is publicly available online, including a new 2D Digitally Reconstructed Radiograph generated from tomographic data and a new pixel-level label map for the JSRT Chest X-ray dataset. We also publicize our generalizable pretrained models for both rib segmentation in Chest X-rays and lung field segmentation in Digitally Reconstructed Radiographs. Results show that the proposed pipeline outperforms shallow rib segmentation baselines in almost all quantitative metrics and produce higher fidelity pixel-map predictions than simply using the pretrained Neural Networks on the flattened 3D data, mainly in datasets where domain shift is more pronounced. The use of Conditional Domain Adaptation also allows the method to perform inference on all 7 X-ray datasets using one single model, achieving over 0.856 of AUC on OpenIST and 0.934 of AUC on JSRT, with Dice scores of 0.68 and 0.69 in these two datasets, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303561",
    "keywords": [
      "Anatomy",
      "Artificial intelligence",
      "Chest radiograph",
      "Computer science",
      "Computer vision",
      "Leverage (statistics)",
      "Medicine",
      "Pattern recognition (psychology)",
      "Pipeline (software)",
      "Pixel",
      "Programming language",
      "Radiography",
      "Radiology",
      "Rib cage",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Oliveira",
        "given_name": "Hugo"
      },
      {
        "surname": "Mota",
        "given_name": "Virginia"
      },
      {
        "surname": "Machado",
        "given_name": "Alexei M.C."
      },
      {
        "surname": "dos Santos",
        "given_name": "Jefersson A."
      }
    ]
  },
  {
    "title": "A promotion method for generation error-based video anomaly detection",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.09.019",
    "abstract": "Surveillance video anomaly detection is to detect events that rarely or never happened in a certain scene. The generation error (GE)-based methods exhibit excellent performances on this task. They firstly train a generative neural network (GNN) to generate normal samples, then judge the samples with large GEs as anomalies. Almost all the GE-based methods utilize frame-level GEs to detect anomalies. However, anomalies generally occur in local areas, the frame-level GE introduces GEs of normal areas to anomaly detection, that brings two problems: i) The GEs of normal areas reduce the anomaly saliency of the anomalous frame. ii) Different videos have different normal-GE-levels, thus it is hard to set a uniform threshold for different videos to detect anomalies. To address these problems, we propose a promotion method: utilize the maximum of block-level GEs on the frame to detect anomalies. Firstly, we calculate the block-level GEs at each position on the frame. Then, we utilize the maximum of the block-level GEs on the frame to detect anomalies. Based on the existing GNN models, we carry out experiments on multiple datasets. The results demonstrate the effectiveness of the proposed method and achieve state-of-the-art performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552030355X",
    "keywords": [
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Computer vision",
      "Condensed matter physics",
      "Economics",
      "Finance",
      "Frame (networking)",
      "Generative grammar",
      "Geometry",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Position (finance)",
      "Programming language",
      "Set (abstract data type)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zhiguo"
      },
      {
        "surname": "Yang",
        "given_name": "Zhongliang"
      },
      {
        "surname": "Zhang",
        "given_name": "Yu-Jin"
      }
    ]
  },
  {
    "title": "HCNN-PSI: A hybrid CNN with partial semantic information for space target recognition",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107531",
    "abstract": "Space target recognition is the basic task of space situational awareness and has developed significantly in the last decade. This paper proposes a hybrid convolutional neural network with partial semantic information for space target recognition, which joints the global features and partial semantic information. Firstly, we propose a two-stage target detection network based on the characteristics of deep space targets. Secondly, we use the Mask R-CNN to segment the main components of the detected satellite. Thirdly, the recognized target and the segmented components are sent to the hybrid extractor to train the hybrid network. What we have done is to find the proper weights of the partial semantic information that plays different importance. The loss function of the hybrid network integrates the global-based and component-based loss with different weights. In comparison with several sets of comparative experiments, the proposed method has achieved a satisfactory result. Besides, we have simulated some real space target images by data processing and achieved a competitive performance in both the simulated dataset and the public dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303344",
    "keywords": [
      "Aerospace engineering",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Engineering",
      "Feature extraction",
      "Hybrid neural network",
      "Operating system",
      "Pattern recognition (psychology)",
      "Programming language",
      "Semantic space",
      "Semantics (computer science)",
      "Situation awareness",
      "Space (punctuation)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Xi"
      },
      {
        "surname": "Wu",
        "given_name": "Tan"
      },
      {
        "surname": "Wang",
        "given_name": "Nannan"
      },
      {
        "surname": "Huang",
        "given_name": "Yan"
      },
      {
        "surname": "Song",
        "given_name": "Bin"
      },
      {
        "surname": "Gao",
        "given_name": "Xinbo"
      }
    ]
  },
  {
    "title": "Generalized low-rank approximation of matrices based on multiple transformation pairs",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107545",
    "abstract": "Dimensionality reduction is a critical step in the learning process that plays an essential role in various applications. The most popular methods for dimensionality reduction, SVD and PCA, for instance, only work on one-dimensional data. This means that for higher-order data like matrices or more generally tensors, data should be fold to the vector format. Thus, this approach ignores the spatial relationships of features and increases the probability of overfitting as well. Due to the mentioned issues, several methods like Generalized Low-Rank Approximation of Matrices (GLRAM) and Multilinear PCA (MPCA) proposed to deal with multi-dimensional data in their original format. Consequently, the spatial relationships of features preserved and the probability of overfitting diminished. Besides, the time and space complexity in such methods are less than vector-based ones. However, since the multilinear approach needs fewer parameters, its search space is much smaller than that of the vector-based one. To solve the previous problems of multilinear methods like GLRAM, we proposed a novel extension of GLRAM in which instead one transformation pair use multiple left and right transformation pairs on the projected data. Consequently, this provides the problem with a larger feasible region and smaller reconstruction error. This article provides several analytical discussions and experimental results that confirm the quality of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303484",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Curse of dimensionality",
      "Dimensionality reduction",
      "Gene",
      "Geometry",
      "Mathematics",
      "Multilinear map",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Rank (graph theory)",
      "Reduction (mathematics)",
      "Singular value decomposition",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Ahmadi",
        "given_name": "Soheil"
      },
      {
        "surname": "Rezghi",
        "given_name": "Mansoor"
      }
    ]
  },
  {
    "title": "Image segmentation using dense and sparse hierarchies of superpixels",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107532",
    "abstract": "We investigate the intersection between hierarchical and superpixel image segmentation. Two strategies are considered: (i) the classical region merging, that creates a dense hierarchy with a higher number of levels, and (ii) the recursive execution of some superpixel algorithm, which generates a sparse hierarchy with fewer levels. We show that, while dense methods can capture more intermediate or higher-level object information, sparse methods are considerably faster and usually with higher boundary adherence at finer levels. We first formalize the two strategies and present a sparse method, which is faster than its superpixel algorithm and with similar boundary adherence. We then propose a new dense method to be used as post-processing from the intermediate level, as obtained by our sparse method, upwards. This combination results in a unique strategy and the most effective hierarchical segmentation method among the compared state-of-the-art approaches, with efficiency comparable to the fastest superpixel algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303356",
    "keywords": [
      "Aerospace engineering",
      "Algorithm",
      "Artificial intelligence",
      "Boundary (topology)",
      "Computer science",
      "Economics",
      "Engineering",
      "Gaussian",
      "Hierarchy",
      "Image (mathematics)",
      "Image segmentation",
      "Intersection (aeronautics)",
      "Market economy",
      "Mathematical analysis",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Segmentation",
      "Sparse approximation",
      "Sparse matrix"
    ],
    "authors": [
      {
        "surname": "Galvão",
        "given_name": "Felipe Lemes"
      },
      {
        "surname": "Guimarães",
        "given_name": "Silvio Jamil Ferzoli"
      },
      {
        "surname": "Falcão",
        "given_name": "Alexandre Xavier"
      }
    ]
  },
  {
    "title": "Improved prototypical networks for few-Shot learning",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.07.015",
    "abstract": "Few-Shot Learning (FSL) aims at recognizing the target classes that only a few samples are available for training. The current approaches mostly address FSL by learning a generalized class-level metric while neglect the intra-class distribution information. In this work, we propose Improved Prototypical Networks (IPN) to address this issue. Inspired by the observation that the intra-class samples differ greatly in revealing the class distribution, we first propose an attention-analogous strategy to explore the class distribution information by distributing different weights to samples based on their representativeness. Besides, to further explore the discriminative information across classes, we propose a distance scaling strategy to reduce the intra-class difference while enlarge the inter-class difference. The experimental results on two benchmark datasets show the superiority of the proposed model against the state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520302610",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Class (philosophy)",
      "Computer science",
      "Discriminative model",
      "Economics",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Representativeness heuristic",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Ji",
        "given_name": "Zhong"
      },
      {
        "surname": "Chai",
        "given_name": "Xingliang"
      },
      {
        "surname": "Yu",
        "given_name": "Yunlong"
      },
      {
        "surname": "Pang",
        "given_name": "Yanwei"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhongfei"
      }
    ]
  },
  {
    "title": "NAPS: Non-adversarial polynomial synthesis",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.11.006",
    "abstract": "Generative Adversarial Nets (GANs) are currently the dominant model for high fidelity image synthesis. GANs suffer from two major drawbacks: complicated dynamics and the requirement for an auxiliary network for training (discriminator). However, if we train a decoder-only network we circumvent both drawbacks. To achieve that, the decoder should capture high-order correlations that exist between the variables. We demonstrate this is possible by designing a high-order polynomial generator using tensorial factors. We implement two variants of the model, which we call NAPS. We experiment with both MNIST and CelebA and showcase that our model captures the data distribution and synthesizes new images with significantly less parameters than the corresponding baseline.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520304116",
    "keywords": [
      "Adversarial system",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Detector",
      "Discriminator",
      "Electrical engineering",
      "Engineering",
      "Fidelity",
      "Generative adversarial network",
      "Generative grammar",
      "Generator (circuit theory)",
      "High fidelity",
      "Image (mathematics)",
      "Image synthesis",
      "MNIST database",
      "Mathematical analysis",
      "Mathematics",
      "Physics",
      "Polynomial",
      "Power (physics)",
      "Quantum mechanics",
      "Telecommunications",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Chrysos",
        "given_name": "Grigorios G"
      },
      {
        "surname": "Panagakis",
        "given_name": "Yannis"
      }
    ]
  },
  {
    "title": "Camera identification of multi-format devices",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.10.010",
    "abstract": "Photo Response Non-Uniformity (PRNU) based source camera attribution is an effective method to determine an image or a video’s origin camera. However, modern devices, especially smartphones, capture images and videos at different resolutions using the same sensor array, PRNU attribution can become ineffective as the camera fingerprint and query object can be misaligned. While capturing visual objects (either image or video), cameras may use different in-camera operations as well as they may use different parts of the sensor. In this paper, we investigate the problem of source camera attribution of a visual object by doing a thorough investigation of a comprehensive dataset, NYUAD Mixed Media Dataset. This investigation takes many factors into accounts, such as the fact that visual objects may have been captured using different resolution and aspect ratios. Furthermore, the visual objects may use different regions of the sensor, including the usage of boundary pixels for videos. Taking these various cases into account, we propose an efficient search which not only gives the state-of-the-art results but also performs significantly faster compared to existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303998",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Computer vision",
      "Fingerprint (computing)",
      "Identification (biology)",
      "Image sensor",
      "Object (grammar)",
      "Pixel"
    ],
    "authors": [
      {
        "surname": "Taspinar",
        "given_name": "Samet"
      },
      {
        "surname": "Mohanty",
        "given_name": "Manoranjan"
      },
      {
        "surname": "Memon",
        "given_name": "Nasir"
      }
    ]
  },
  {
    "title": "Grabber: A tool to improve convergence in interactive image segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2020",
    "doi": "10.1016/j.patrec.2020.10.012",
    "abstract": "Interactive image segmentation has considerably evolved from techniques that do not learn the parameters of the model to methods that pre-train a model and adapt it from user inputs during the process. However, user control over segmentation still requires significant improvements to avoid that corrections in one part of the object cause errors in other parts. We address this problem by presenting Grabber — a tool to improve convergence (user control) in interactive image segmentation. Grabber is thought to complete segmentation of some other initial method. From a given segmentation mask, Grabber quickly estimates anchor points in one orientation along the boundary of the mask and delineates an optimum contour constrained to pass through those points. The user can control the process by adding, removing, and moving anchor points. Grabber can also explore object properties from the initial coarse segmentation to improve boundary delineation. We integrate Grabber with two recent methods, a region-based approach and a pixel classification method based on deep neural networks. Extensive experiments with robot users on two datasets show in both cases that Grabber can significantly improve convergence, with faster delineation, higher effectiveness, and less user effort. The code of Grabber is available at https://github.com/LIDS-UNICAMP/grabber.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865520303986",
    "keywords": [
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Image processing",
      "Image segmentation",
      "Object (grammar)",
      "Operating system",
      "Pixel",
      "Process (computing)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Bragantini",
        "given_name": "Jordão"
      },
      {
        "surname": "Moura",
        "given_name": "Bruno"
      },
      {
        "surname": "Falcão",
        "given_name": "Alexandre X."
      },
      {
        "surname": "Cappabianco",
        "given_name": "Fábio A.M."
      }
    ]
  },
  {
    "title": "MuLTReNets: Multilingual text recognition networks for simultaneous script identification and handwriting recognition",
    "journal": "Pattern Recognition",
    "year": "2020",
    "doi": "10.1016/j.patcog.2020.107555",
    "abstract": "Multilingual handwritten text recognition is often accomplished in two cascaded steps: script identification and handwriting recognition. However, this scheme is not optimal due to error accumulation. To perform simultaneous script identification and handwriting recognition, in this paper, we propose a new framework named multilingual text recognition networks (MuLTReNets). Specifically, the system has four major modules: feature extractor, script identifier, handwriting recognizer and auto-weighter. The feature extractor integrates both spatial and temporal knowledge to encode text images into features shared by the script identifier and recognizer. The script identifier predicts script category from a variable-length sequence incorporating an auto-weighter for balancing different scripts, while the handwriting recognizer adopts long-short term memory (LSTM) and Connectionist Temporal Classification (CTC) to accomplish sequence decoding. Via multi-task learning, the proposed framework can benefit both two multilingual recognition schemes: unified recognition with merged alphabet (MuLTReNetV1) and cascaded script identification-single script recognition with joint training (MuLTReNetV2). We evaluated the performance of the proposed method on handwritten text databases of five languages, which are English, French, Kannada, Urdu, and Bangla. Experimental results demonstrate that our method performs superiorly for both script identification and handwriting recognition. The accuracy of script identification reaches 99.9%. While in handwriting recognition, the proposed system not only outperforms cascade systems but also surpasses systems particularly designed for specific scripts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320320303587",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Character recognition",
      "Computer science",
      "Feature (linguistics)",
      "Feature extraction",
      "Handwriting",
      "Handwriting recognition",
      "Identification (biology)",
      "Identifier",
      "Image (mathematics)",
      "Intelligent character recognition",
      "Linguistics",
      "Natural language processing",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Scripting language",
      "Speech recognition",
      "Transliteration"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Zhuo"
      },
      {
        "surname": "Yin",
        "given_name": "Fei"
      },
      {
        "surname": "Zhang",
        "given_name": "Xu-Yao"
      },
      {
        "surname": "Yang",
        "given_name": "Qing"
      },
      {
        "surname": "Liu",
        "given_name": "Cheng-Lin"
      }
    ]
  }
]