[
  {
    "title": "Pattern detection in the activation space for identifying synthesized content",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.12.007",
    "abstract": "Generative Adversarial Networks (GANs) have recently achieved unprecedented success in photo-realistic image synthesis from low-dimensional random noise. The ability to synthesize high-quality content at a large scale brings potential risks as the generated samples may lead to misinformation that can create severe social, political, health, and business hazards. We propose SubsetGAN to identify generated content by detecting a subset of anomalous node-activations in the inner layers of pre-trained neural networks. These nodes, as a group, maximize a non-parametric measure of divergence away from the expected distribution of activations created from real data. This enable us to identify synthesised images without prior knowledge of their distribution. SubsetGAN efficiently scores subsets of nodes and returns the group of nodes within the pre-trained classifier that contributed to the maximum score. The classifier can be a general fake classifier trained over samples from multiple sources or the discriminator network from different GANs. Our approach shows consistently higher detection power than existing detection methods across several state-of-the-art GANs (PGGAN, StarGAN, and CycleGAN) and over different proportions of generated content.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521004372",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Detector",
      "Discriminator",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Cintas",
        "given_name": "Celia"
      },
      {
        "surname": "Speakman",
        "given_name": "Skyler"
      },
      {
        "surname": "Tadesse",
        "given_name": "Girmaw Abebe"
      },
      {
        "surname": "Akinwande",
        "given_name": "Victor"
      },
      {
        "surname": "McFowland",
        "given_name": "Edward"
      },
      {
        "surname": "Weldemariam",
        "given_name": "Komminist"
      }
    ]
  },
  {
    "title": "Tracking more than 100 arbitrary objects at 25 FPS through deep learning",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108205",
    "abstract": "Most video analytics applications rely on object detectors to localize objects in frames. However, when real-time is a requirement, running the detector at all the frames is usually not possible. This is somewhat circumvented by instantiating visual object trackers between detector calls, but this does not scale with the number of objects. To tackle this problem, we present SiamMT, a new deep learning multiple visual object tracking solution that applies single-object tracking principles to multiple arbitrary objects in real-time. To achieve this, SiamMT reuses feature computations, implements a novel crop-and-resize operator, and defines a new and efficient pairwise similarity operator. SiamMT naturally scales up to several dozens of targets, reaching 25 fps with 122 simultaneous objects for VGA videos, or up to 100 simultaneous objects in HD720 video. SiamMT has been validated on five large real-time benchmarks, achieving leading performance against current state-of-the-art trackers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003861",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "BitTorrent tracker",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Detector",
      "Eye tracking",
      "Feature (linguistics)",
      "Frame (networking)",
      "Gene",
      "Image (mathematics)",
      "Linguistics",
      "Object (grammar)",
      "Object detection",
      "Operator (biology)",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Philosophy",
      "Programming language",
      "Psychology",
      "Repressor",
      "Similarity (geometry)",
      "Software",
      "Telecommunications",
      "Tracking (education)",
      "Transcription factor",
      "Video Graphics Array",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Vaquero",
        "given_name": "Lorenzo"
      },
      {
        "surname": "Brea",
        "given_name": "Víctor M."
      },
      {
        "surname": "Mucientes",
        "given_name": "Manuel"
      }
    ]
  },
  {
    "title": "PEDENet: Image anomaly localization via patch embedding and density estimation",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.11.030",
    "abstract": "A neural network targeting at unsupervised image anomaly localization, called the PEDENet, is proposed in this work. PEDENet contains a patch embedding (PE) network, a density estimation (DE) network, and an auxiliary network called the location prediction (LP) network. The PE network takes local image patches as input and performs dimension reduction to get low-dimensional patch embeddings via a deep encoder structure. Being inspired by the Gaussian Mixture Model (GMM), the DE network takes those patch embeddings, and then predicts the cluster membership of an embedded patch. The sum of membership probabilities is used as a loss term to guide the learning process. The LP network is a Multi-layer Perception (MLP), which takes embeddings from two neighboring patches as input and predicts their relative location. The performance of the proposed PEDENet is evaluated extensively and benchmarked with that of state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552100427X",
    "keywords": [
      "Algorithm",
      "Anomaly detection",
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Combinatorics",
      "Computer science",
      "Density estimation",
      "Dimension (graph theory)",
      "Embedding",
      "Encoder",
      "Estimator",
      "Gaussian",
      "Image (mathematics)",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Pure mathematics",
      "Quantum mechanics",
      "Statistics",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Kaitai"
      },
      {
        "surname": "Wang",
        "given_name": "Bin"
      },
      {
        "surname": "Kuo",
        "given_name": "C.-C. Jay"
      }
    ]
  },
  {
    "title": "Joint multi-label learning and feature extraction for temporal link prediction",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108216",
    "abstract": "Networks derived from various disciplinary of sociality and nature are dynamic and incomplete, and temporal link prediction has wide applications in recommendation system and data mining system, etc. The current algorithms first obtain features by exploiting the topological or latent structure of networks, and then predict temporal links based on the obtained features. These algorithms are criticized by the separation of feature extraction and link prediction, which fails to fully characterize the dynamics of networks, resulting in undesirable performance. To overcome this problem, we propose a novel algorithm by joint multi-label learning and feature extraction (called MLjFE), where temporal link prediction and feature extraction are integrated into an overall objective function. The main advantage of MLjFE is that the features and parameter matrix for temporal link prediction are simultaneously learned during optimization procedure, which is more precise to capture dynamics of networks, improving the performance of algorithms. The experimental results on a number of artificial and real-world temporal networks demonstrate that the proposed algorithm significantly outperforms state-of-the-art methods, showing joint learning with feature extraction and temporal link prediction is promising.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003976",
    "keywords": [
      "Architectural engineering",
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Data mining",
      "Engineering",
      "Feature (linguistics)",
      "Feature extraction",
      "Joint (building)",
      "Linguistics",
      "Link (geometry)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Xiaoke"
      },
      {
        "surname": "Tan",
        "given_name": "Shiyin"
      },
      {
        "surname": "Xie",
        "given_name": "Xianghua"
      },
      {
        "surname": "Zhong",
        "given_name": "Xiaoxiong"
      },
      {
        "surname": "Deng",
        "given_name": "Jingjing"
      }
    ]
  },
  {
    "title": "Preference prediction based on a photo gallery analysis with scene recognition and object detection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108248",
    "abstract": "In this paper, a user modeling task is examined by processing mobile device gallery of photos and videos. We propose a novel engine for preferences prediction based on scene recognition, object detection and facial analysis. At first, all faces in a gallery are clustered, and all private photos and videos with faces from large clusters are processed on the embedded system in offline mode. Other photos may be sent to the remote server to be analyzed by very deep sophisticated neural networks. The visual features of each photo are obtained from scene recognition and object detection models. These features are aggregated into a single descriptor in the neural attention unit. The proposed pipeline is implemented in mobile Android application. Experimental results for the Photo Event Collection, Web Image Dataset for Event Recognition and Amazon Fashion data demonstrate the possibility to efficiently process images without significant accuracy degradation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004283",
    "keywords": [
      "Artificial intelligence",
      "Cognitive neuroscience of visual object recognition",
      "Computer science",
      "Computer vision",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Preference",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Savchenko",
        "given_name": "A.V."
      },
      {
        "surname": "Demochkin",
        "given_name": "K.V."
      },
      {
        "surname": "Grechikhin",
        "given_name": "I.S."
      }
    ]
  },
  {
    "title": "Beyond neighbourhood-preserving transformations for quantization-based unsupervised hashing",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.11.007",
    "abstract": "An effective unsupervised hashing algorithm leads to compact binary codes preserving the neighborhood structure of data as much as possible. One of the most established schemes for unsupervised hashing is to reduce the dimensionality of data and then find a rigid (neighborhood-preserving) transformation that reduces the quantization error. Although employing rigid transformations is effective, we may not reduce quantization loss to the ultimate limits. As well, reducing dimensionality and quantization loss in two separate steps seems to be sub-optimal. Motivated by these shortcomings, we propose to employ both rigid and non-rigid transformations to reduce quantization error and dimensionality simultaneously. We relax the orthogonality constraint on the projection in a PCA-formulation and regularize this by a quantization term. We show that both the non-rigid projection matrix and rotation matrix contribute towards minimizing quantization loss but in different ways. A scalable nested coordinate descent approach is proposed to optimize this mixed-integer optimization problem. We evaluate the proposed method on five public benchmark datasets providing almost half a million images. Comparative results indicate that the proposed method mostly outperforms state-of-art linear methods and competes with end-to-end deep solutions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003974",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Coordinate descent",
      "Curse of dimensionality",
      "Hash function",
      "Mathematics",
      "Quantization (signal processing)"
    ],
    "authors": [
      {
        "surname": "Hemati",
        "given_name": "Sobhan"
      },
      {
        "surname": "Tizhoosh",
        "given_name": "H.R."
      }
    ]
  },
  {
    "title": "Oil palm tree counting in drone images",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.11.016",
    "abstract": "When the images are captured by drones, the effect of oblique angles, distance variations and open environment are the main challenges for successful palm tree detection. This paper presents a method towards palm tree counting in Drone images using a novel idea of detecting dominant points by exploring Generalized Gradient Vector Flow, which defines symmetry based on gradient direction of the pixels. For each dominant point, we use angle information for classifying diagonal dominant points. It is intuition that the direction of the branches of tree converges at center of tree irrespective of the type of tree and plants. This observation motivated us to expand the direction of diagonal dominant points until it finds intersection point with another diagonal dominant point and this results in candidate points. For each candidate point, the proposed method constructs the ring by considering the distance between the intersection point and nearest neighbor candidate point as radius. This outputs region of interest and it includes center of each tree in the image. To ease the effect of complex background, we explore YOLOv5 architecture to remove false region of interests. This step results in counting oil palm trees in the mages irrespective of tree type of palm family. Experimental results on our dataset of the images captured by drones and standard dataset of coconut images captured by unmanned aerial vehicle of different trees show that the proposed method is effective and performs better than SOTA methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521004074",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Cartography",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Diagonal",
      "Drone",
      "Genetics",
      "Geography",
      "Geometry",
      "Image (mathematics)",
      "Intersection (aeronautics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Point (geometry)",
      "Tree (set theory)",
      "Vanishing point"
    ],
    "authors": [
      {
        "surname": "Chowdhury",
        "given_name": "Pinaki Nath"
      },
      {
        "surname": "Shivakumara",
        "given_name": "Palaiahnakote"
      },
      {
        "surname": "Nandanwar",
        "given_name": "Lokesh"
      },
      {
        "surname": "Samiron",
        "given_name": "Faizal"
      },
      {
        "surname": "Pal",
        "given_name": "Umapada"
      },
      {
        "surname": "Lu",
        "given_name": "Tong"
      }
    ]
  },
  {
    "title": "Unified learning approach for egocentric hand gesture recognition and fingertip detection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108200",
    "abstract": "Head-mounted device-based human-computer interaction often requires egocentric recognition of hand gestures and fingertips detection. In this paper, a unified approach of egocentric hand gesture recognition and fingertip detection is introduced. The proposed algorithm uses a single convolutional neural network to predict the probabilities of finger class and positions of fingertips in one forward propagation. Instead of directly regressing the positions of fingertips from the fully connected layer, the ensemble of the position of fingertips is regressed from the fully convolutional network. Subsequently, the ensemble average is taken to regress the final position of fingertips. Since the whole pipeline uses a single network, it is significantly fast in computation. Experimental results show that the proposed method outperforms the existing fingertip detection approaches including the Direct Regression and the Heatmap-based framework. The effectiveness of the proposed method is also shown in-the-wild scenario as well as in a use-case of virtual reality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003824",
    "keywords": [],
    "authors": [
      {
        "surname": "Alam",
        "given_name": "Mohammad Mahmudul"
      },
      {
        "surname": "Islam",
        "given_name": "Mohammad Tariqul"
      },
      {
        "surname": "Rahman",
        "given_name": "S.M. Mahbubur"
      }
    ]
  },
  {
    "title": "Graph convolutional autoencoders with co-learning of graph structure and node attributes",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108215",
    "abstract": "Recently, graph representation learning based on autoencoders has received much attention. However, these methods suffer from two limitations. First, most graph autoencoders ignore the reconstruction of either the graph structure or the node attributes, which often leads to a poor latent representation of the graph-structured data. Second, for existing graph autoencoders models, the encoder and decoder are mainly composed of an initial graph convolutional network (GCN) or its variants. These traditional GCN-based graph autoencoders more or less encounter the problem of incomplete filtering, which causes these models to be unstable in practical applications. To address the above issues, this paper proposes the Graph convolutional Autoencoders with co-learning of graph Structure and Node attributes (GASN) based on variational autoencoders. Specifically, the proposed GASN encodes and decodes the node attributes and graph structure comprehensively in the graph-structured data. Furthermore, we design a completely low-pass graph encoder and a high-pass graph decoder. The experimental results on real-world datasets demonstrate that the proposed GASN achieves state-of-the-art performance on node clustering, link prediction, and visualization tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003964",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Graph",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jie"
      },
      {
        "surname": "Liang",
        "given_name": "Jiye"
      },
      {
        "surname": "Yao",
        "given_name": "Kaixuan"
      },
      {
        "surname": "Liang",
        "given_name": "Jianqing"
      },
      {
        "surname": "Wang",
        "given_name": "Dianhui"
      }
    ]
  },
  {
    "title": "Learning scale awareness in keypoint extraction and description",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108221",
    "abstract": "To recover relative camera motion accurately and robustly, establishing a set of point-to-point correspondences in the pixel space is an essential yet challenging task in computer vision. Even though multi-scale design philosophy has been used with significant success in computer vision tasks, such as object detection and semantic segmentation, learning-based image matching has not been fully exploited. In this work, we explore a scale awareness learning approach in finding pixel-level correspondences based on the intuition that keypoints need to be extracted and described on an appropriate scale. With that insight, we propose a novel scale-aware network and then develop a new fusion scheme that derives high-consistency response maps and high-precision descriptions. We also revise the Second Order Similarity Regularization (SOSR) to make it more effective for the end-to-end image matching network, which leads to significant improvement in local feature descriptions. Experimental results run on multiple datasets demonstrate that our approach performs better than state-of-the-art methods under multiple criteria.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004027",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Epistemology",
      "Image (mathematics)",
      "Image processing",
      "Intuition",
      "Machine learning",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Regularization (linguistics)",
      "Scale space",
      "Segmentation",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Shen",
        "given_name": "Xuelun"
      },
      {
        "surname": "Wang",
        "given_name": "Cheng"
      },
      {
        "surname": "Li",
        "given_name": "Xin"
      },
      {
        "surname": "Peng",
        "given_name": "Yifan"
      },
      {
        "surname": "He",
        "given_name": "Zijian"
      },
      {
        "surname": "Wen",
        "given_name": "Chenglu"
      },
      {
        "surname": "Cheng",
        "given_name": "Ming"
      }
    ]
  },
  {
    "title": "Blitz-SLAM: A semantic SLAM in dynamic environments",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108225",
    "abstract": "Static environment is a prerequisite for most of visual simultaneous localization and mapping systems. Such a strong assumption limits the practical application of most existing SLAM systems. When moving objects enter the camera’s view field, dynamic matching points will directly interrupt the camera localization, and the noise blocks formed by moving objects will contaminate the constructed map. In this paper, a semantic SLAM system working in indoor dynamic environments named Blitz-SLAM is proposed. The noise blocks in the local point cloud are removed by combining the advantages of semantic and geometric information of mask, RGB and depth images. The global point cloud map can be obtained by merging the local point clouds. We evaluate Blitz-SLAM on the TUM RGB-D dataset and in the real-world environment. The experimental results demonstrate that Blitz-SLAM can work robustly in dynamic environments and generate a clean and accurate global point cloud map simultaneously.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004064",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Geometry",
      "Image (mathematics)",
      "Matching (statistics)",
      "Mathematics",
      "Mobile robot",
      "Noise (video)",
      "Point (geometry)",
      "Point cloud",
      "RGB color model",
      "Robot",
      "Semantic mapping",
      "Simultaneous localization and mapping",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "Yingchun"
      },
      {
        "surname": "Zhang",
        "given_name": "Qichi"
      },
      {
        "surname": "Tang",
        "given_name": "Yuliang"
      },
      {
        "surname": "Liu",
        "given_name": "Shaofen"
      },
      {
        "surname": "Han",
        "given_name": "Hong"
      }
    ]
  },
  {
    "title": "Spatiotemporal consistency-enhanced network for video anomaly detection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108232",
    "abstract": "Video anomaly detection aims to detect abnormal segments in a video sequence, which is a key problem in video surveillance. Based on deep prediction methods, we propose a spatiotemporal consistency-enhanced network to generate spatiotemporal consistency predictions. A 3D CNN-based encoder and 2D CNN-based decoder constitute the main part of our model. A resampling strategy is applied to the latent space vector when the model is trained by the normal data, yet this can cause the model to perform poorly if the data include abnormal data. Moreover, we combine an input clip with a generated frame into a reformed video clip, which is then fed into a discriminator that is constructed by the 3D CNN to evaluate the consistency of the input clip. Owing to the adversarial training between the generator and discriminator, the spatiotemporal consistency of the generated results is enhanced. During the testing stage, the abnormal data generates a different appearance and motion changes, which affect the ability of our model to predict spatiotemporal consistency in future images. Then, the prediction quality gap between normal and anomalous contents is used to infer whether anomalies occur. Extensive experiments confirm that the proposed method achieves state-of-the-art performance on three benchmark datasets, including ShanghaiTech, CUHK Avenue, and UCSD Ped2.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004131",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Data consistency",
      "Detector",
      "Discriminator",
      "Encoder",
      "Frame (networking)",
      "Generator (circuit theory)",
      "Geodesy",
      "Geography",
      "Key (lock)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Resampling",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Hao",
        "given_name": "Yi"
      },
      {
        "surname": "Li",
        "given_name": "Jie"
      },
      {
        "surname": "Wang",
        "given_name": "Nannan"
      },
      {
        "surname": "Wang",
        "given_name": "Xiaoyu"
      },
      {
        "surname": "Gao",
        "given_name": "Xinbo"
      }
    ]
  },
  {
    "title": "Towards open-set touchless palmprint recognition via weight-based meta metric learning",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108247",
    "abstract": "Touchless biometrics has become significant in the wake of novel coronavirus 2019 (COVID-19). Due to the convenience, user-friendly, and high-accuracy, touchless palmprint recognition shows great potential when the hygiene issues are considered during COVID-19. However, previous palmprint recognition methods are mainly focused on close-set scenario. In this paper, a novel Weight-based Meta Metric Learning (W2ML) method is proposed for accurate open-set touchless palmprint recognition, where only a part of categories is seen during training. Deep metric learning-based feature extractor is learned in a meta way to improve the generalization ability. Multiple sets are sampled randomly to define support and query sets, which are further combined into meta sets to constrain the set-based distances. Particularly, hard sample mining and weighting are adopted to select informative meta sets to improve the efficiency. Finally, embeddings with obvious inter-class and intra-class differences are obtained as features for palmprint identification and verification. Experiments are conducted on four palmprint benchmarks including fourteen constrained and unconstrained palmprint datasets. The results show that our W2ML method is more robust and efficient in dealing with open-set palmprint recognition issue as compared to the state-of-the-arts, where the accuracy is increased by up to 9.11% and the Equal Error Rate (EER) is decreased by up to 2.97%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004271",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Class (philosophy)",
      "Computer science",
      "Engineering",
      "Feature (linguistics)",
      "Generalization",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Radiology",
      "Set (abstract data type)",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Shao",
        "given_name": "Huikai"
      },
      {
        "surname": "Zhong",
        "given_name": "Dexing"
      }
    ]
  },
  {
    "title": "Learning to schedule multi-NUMA virtual machines via reinforcement learning",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108254",
    "abstract": "With the rapid development of cloud computing, the importance of dynamic virtual machine scheduling is increasing. Existing works formulate the VM scheduling as a bin-packing problem and design greedy methods to solve it. However, cloud service providers widely adopt multi-NUMA architecture servers in recent years, and existing methods do not consider the architecture. This paper formulates the multi-NUMA VM scheduling into a novel structured combinatorial optimization and transforms it into a reinforcement learning problem. We propose a reinforcement learning algorithm called SchedRL with a delta reward scheme and an episodic guided sampling strategy to solve the problem efficiently. Evaluating on a public dataset of Azure under two different scenarios, our SchedRL outperforms FirstFit and BestFit on the fulfill number and allocation rate.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004349",
    "keywords": [
      "Algorithm",
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Bin",
      "Bin packing problem",
      "Cloud computing",
      "Computer science",
      "Distributed computing",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Reinforcement learning",
      "Schedule",
      "Scheduling (production processes)",
      "Server",
      "Virtual machine",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Sheng",
        "given_name": "Junjie"
      },
      {
        "surname": "Hu",
        "given_name": "Yiqiu"
      },
      {
        "surname": "Zhou",
        "given_name": "Wenli"
      },
      {
        "surname": "Zhu",
        "given_name": "Lei"
      },
      {
        "surname": "Jin",
        "given_name": "Bo"
      },
      {
        "surname": "Wang",
        "given_name": "Jun"
      },
      {
        "surname": "Wang",
        "given_name": "Xiangfeng"
      }
    ]
  },
  {
    "title": "SparseShift-GCN: High precision skeleton-based action recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.12.005",
    "abstract": "Skeleton-based action recognition is widely used due to its advantages of lightweight and strong anti-interference. Recently, graph convolutional networks (GCNs) have been applied to action recognition and have made breakthrough progress. The shift convolution operator can effectively replace the spatial convolution and greatly reduce the computational complexity of the algorithm. This article first applies the Conv-Shift-Conv (CSC) module and the Shift-Conv-Shift-Conv (SC2 ) module to replace the Shift-Conv-Shift (SCS) module in spatial graph convolution of Shift-GCN respectively. This design can reorder the shifted channels more effectively. The experimental results show that the CSC module has the best effect and effectively improves accuracy of model. After that, this article proposes to replace the shift module in the original Shift-GCN with a sparse shift module and named SparseShift-GCN. This structure can reduce the redundancy of features, prevent overfitting and improve the generality of the model. Based on the improvement in the previous step, better results have been achieved. Finally, this paper uses OHEM Loss and Weighted Loss to carefully design the loss function of the model and introduces it into the model proposed in this paper. Experimental results show that OHEM Loss further improves the accuracy of algorithm. After a series of improvements, our proposed model has improved the accuracy of 4 different streams to varying degrees, which improves the overall performance of the network.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521004360",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolution (computer science)",
      "Graph",
      "Operating system",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Redundancy (engineering)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zang",
        "given_name": "Ying"
      },
      {
        "surname": "Yang",
        "given_name": "Dongsheng"
      },
      {
        "surname": "Liu",
        "given_name": "Tianjiao"
      },
      {
        "surname": "Li",
        "given_name": "Hui"
      },
      {
        "surname": "Zhao",
        "given_name": "Shuguang"
      },
      {
        "surname": "Liu",
        "given_name": "Qingshan"
      }
    ]
  },
  {
    "title": "A discriminative channel diversification network for image classification",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.12.004",
    "abstract": "Channel attention mechanisms in convolutional neural networks have been proven to be effective in various computer vision tasks. However, the performance improvement comes with additional model complexity and computation cost. In this paper, we propose a light-weight and effective attention module, called channel diversification block, to enhance the global context by establishing the channel relationship at the global level. Unlike other channel attention mechanisms, the proposed module focuses on the most discriminative features by giving more attention to the spatially distinguishable channels while taking account of the channel activation. Different from other attention models that plugin the module in between several intermediate layers, the proposed module is embedded at the end of the backbone networks, making it easy to implement. Extensive experiments on CIFAR-10, SVHN, and Tiny-ImageNet datasets demonstrate that the proposed module improves the performance of the baseline networks by a margin of 3% on average.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521004311",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Block (permutation group theory)",
      "Channel (broadcasting)",
      "Computation",
      "Computer network",
      "Computer science",
      "Context (archaeology)",
      "Convolutional neural network",
      "Discriminative model",
      "Geometry",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Plug-in",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Patel",
        "given_name": "Krushi"
      },
      {
        "surname": "Wang",
        "given_name": "Guanghui"
      }
    ]
  },
  {
    "title": "Universal multi-Source domain adaptation for image classification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108238",
    "abstract": "Unsupervised domain adaptation (DA) enables intelligent models to learn transferable knowledge from a labeled source domain and adapt to a similar but unlabeled target domain. Studies showed that knowledge could be transferred from one source domain to another unknown target domain, called Universal DA (UDA). However, there is often more than one source domain in the real-world application to be exploited for DA. In this paper, we formally propose a more general domain adaptation setting for image classification, universal multi-source DA (UMDA), where the label sets of multiple source domains can be different, and the label set of the target domain is completely unknown. The main challenge in UMDA is to identify the common label set among each source and target domain and keep the model scalable as the number of source domains increases. In the face of this challenge, we propose a universal multi-source adaptation network (UMAN) to solve the DA problem without increasing the complexity of the model in various UMDA settings. In UMAN, the reliability of each known class belonging to the common label set is estimated via a novel pseudo-margin vector and its weighted form, which helps adversarial training better align the distributions of multiple source domains and target domain. Moreover, the theoretical guarantee for UMAN is also provided. Massive experimental results show that existing UDA and multi-source DA (MDA) methods cannot be directly deployed to UMDA, and the proposed UMAN achieves the state-of-the-art performance in various UMDA settings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004192",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Database",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematical analysis",
      "Mathematics",
      "Multi-source",
      "Pattern recognition (psychology)",
      "Programming language",
      "Scalability",
      "Set (abstract data type)",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Yin",
        "given_name": "Yueming"
      },
      {
        "surname": "Yang",
        "given_name": "Zhen"
      },
      {
        "surname": "Hu",
        "given_name": "Haifeng"
      },
      {
        "surname": "Wu",
        "given_name": "Xiaofu"
      }
    ]
  },
  {
    "title": "Novel fuzzy clustering algorithm with variable multi-pixel fitting spatial information for image segmentation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108201",
    "abstract": "Spatial information is often used to enhance the robustness of traditional fuzzy c-means (FCM) clustering algorithms. Although some recently emerged improvements are remarkable, the computational complexity of these algorithms is high, which may lead to lack of practicability. To address this problem, an efficient variant named the fuzzy clustering algorithm with variable multi-pixel fitting spatial information (FCM-VMF) is presented. First, a fuzzy clustering algorithm with multi-pixel fitting spatial information (FCM-MF) is developed. Specifically, by dividing the input image into several filter windows, the spatial information of all pixels in each filter window can be obtained simultaneously by fitting the pixels in its corresponding neighbourhood window, which enormously reduces the computational complexity. However, the FCM-MF may result in the loss of edge information. Therefore, the FCM-VMF integrates a variable window strategy with FCM-MF. In this strategy, to preserve more edge information, the sizes of the filter window and generalized neighbourhood window are adaptively reduced. The experimental results show that FCM-VMF is as effective as some recent algorithms. Notably, the FCM-VMF has extremely high efficiency, which means it has a better prospect of application.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003836",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Filter (signal processing)",
      "Fuzzy clustering",
      "Fuzzy logic",
      "Gene",
      "Image segmentation",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Pixel",
      "Robustness (evolution)",
      "Segmentation",
      "Spatial analysis",
      "Statistics",
      "Window (computing)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Hang"
      },
      {
        "surname": "Li",
        "given_name": "Haili"
      },
      {
        "surname": "Chen",
        "given_name": "Ning"
      },
      {
        "surname": "Chen",
        "given_name": "Shengfeng"
      },
      {
        "surname": "Liu",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "Part-based annotation-free fine-grained classification of images of retail products",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108257",
    "abstract": "We propose a novel solution that classifies very similar images (fine-grained classification) of variants of retail products displayed on the racks of supermarkets. The proposed scheme simultaneously captures object-level and part-level cues of the product images. The object-level cues of the product images are captured with our novel reconstruction-classification network (RC-Net). For annotation-free modeling of part-level cues, the discriminatory parts of the product images are identified around the keypoints. The ordered sequences of these discriminatory parts, encoded using convolutional LSTM, describe the products uniquely. Finally, the part-level and object-level models jointly determine the products explicitly explaining coarse to finer descriptions of the products. This bi-level architecture is embedded in R-CNN for recognizing variants of retail products on the rack. We perform extensive experiments on one In-house and three benchmark datasets. The proposed scheme outperforms competing methods in almost all the evaluations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004374",
    "keywords": [
      "Annotation",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Geodesy",
      "Geography",
      "Geometry",
      "Mathematical analysis",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Product (mathematics)",
      "Scheme (mathematics)"
    ],
    "authors": [
      {
        "surname": "Santra",
        "given_name": "Bikash"
      },
      {
        "surname": "Shaw",
        "given_name": "Avishek Kumar"
      },
      {
        "surname": "Mukherjee",
        "given_name": "Dipti Prasad"
      }
    ]
  },
  {
    "title": "Why is this an anomaly? Explaining anomalies using sequential explanations",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108227",
    "abstract": "In most applications, anomaly detection operates in an unsupervised mode by looking for outliers hoping that they are anomalies. Unfortunately, most anomaly detectors do not come with explanations about which features make a detected outlier point anomalous. Therefore, it requires human analysts to manually browse through each detected outlier point’s feature space to obtain the subset of features that will help them determine whether they are genuinely anomalous or not. This paper introduces sequential explanation (SE) methods that sequentially explain to the analyst which features make the detected outlier anomalous. We present two methods for computing SEs called the outlier and sample-based SE that will work alongside any anomaly detector. The outlier-based SE methods use an anomaly detector’s outlier scoring measure guided by a search algorithm to compute the SEs. Meanwhile, the sample-based SE methods employ sampling to turn the problem into a classical feature selection problem. In our experiments, we compare the performances of the different outlier- and sample-based SEs. Our results show that both the outlier and sample-based methods compute SEs that perform well and outperform sequential feature explanations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004088",
    "keywords": [
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Computer science",
      "Condensed matter physics",
      "Data mining",
      "Detector",
      "Feature (linguistics)",
      "Feature selection",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Point (geometry)",
      "Sample (material)",
      "Telecommunications",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Mokoena",
        "given_name": "Tshepiso"
      },
      {
        "surname": "Celik",
        "given_name": "Turgay"
      },
      {
        "surname": "Marivate",
        "given_name": "Vukosi"
      }
    ]
  },
  {
    "title": "Learning sequentially diversified representations for fine-grained categorization",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108219",
    "abstract": "Learning representation carrying rich local information is essential for recognizing fine-grained objects. Existing methods to this task resort to multi-stage frameworks to capture fine-grained information. However, they usually require multiple forward passes of the backbone network, resulting in efficiency deterioration. In this paper, we propose Sequentially Diversified Networks (SDNs) that enrich representation by promoting their diversity while maintaining the extraction efficiency. Specifically, we construct multiple lightweight sub-networks to model mutually different scales of discriminative patterns. The design of these sub-networks follows the sequentially diversified constraint, encouraging them to be varied in spatial attention. By inserting these sub-networks into a single backbone network, SDNs enable information interaction among local regions of the fine-grained image. In this way, SDNs jointly promote diversity in terms of scale and spatial attention in the one-stage pipeline, thereby facilitating the learning of diversified representation efficiently. We evaluate our proposed method on three challenging datasets, namely CUB-200-2011, Stanford-Cars, and FGVC-Aircraft. Experiments demonstrate its effectiveness in learning diversified information. Moreover, our method achieves state-of-the-art performance, only requiring a single forward pass of the backbone network, which reduces inference time noticeably.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004003",
    "keywords": [
      "Artificial intelligence",
      "Backbone network",
      "Categorization",
      "Computer network",
      "Computer science",
      "Constraint (computer-aided design)",
      "Construct (python library)",
      "Discriminative model",
      "Economics",
      "Feature learning",
      "Geometry",
      "Inference",
      "Law",
      "Machine learning",
      "Management",
      "Mathematics",
      "Pipeline (software)",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Lianbo"
      },
      {
        "surname": "Huang",
        "given_name": "Shaoli"
      },
      {
        "surname": "Liu",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Anomaly detection in streaming data with gaussian process based stochastic differential equations",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.12.017",
    "abstract": "This paper characterises streaming data as the evolution of a stochastic differential equation, with the aim of extracting information that can be used to detect anomalies in the stream. Gaussian process regression provides a flexible approach to approximating components of the stochastic differential equation, allowing for complex modelling of underlying data generation dynamics. The proposed algorithm displays superior discriminative power over different time-series anomaly detection methods for both synthetic and NYC taxi datasets, whilst the introduced bootstrapping method for setting the detection threshold provides control over the false alarm rate of the anomaly detector.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521004487",
    "keywords": [
      "Algorithm",
      "Anomaly (physics)",
      "Anomaly detection",
      "Applied mathematics",
      "Artificial intelligence",
      "Bootstrapping (finance)",
      "Computer science",
      "Condensed matter physics",
      "Constant false alarm rate",
      "Data mining",
      "Data stream",
      "Differential equation",
      "Discriminative model",
      "Econometrics",
      "False alarm",
      "Gaussian",
      "Gaussian process",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Physics",
      "Process (computing)",
      "Quantum mechanics",
      "Statistics",
      "Stochastic differential equation",
      "Stochastic process",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Glyn-Davies",
        "given_name": "Alex"
      },
      {
        "surname": "Girolami",
        "given_name": "Mark"
      }
    ]
  },
  {
    "title": "Edge computing enabled video segmentation for real-time traffic monitoring in internet of vehicles",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108146",
    "abstract": "In the Internet of Things enabled intelligent transportation systems, a huge amount of vehicle video data has been generated and real-time and accurate video analysis are very important and challenging work, especially in situations with complex street scenes. Therefore, we propose edge computing based video pre-processing to eliminate the redundant frames, so that we migrate the partial or all the video processing task to the edge, thereby diminishing the computing, storage and network bandwidth requirements of the cloud center, and enhancing the effectiveness of video analyzes. To eliminate the redundancy of the traffic video, the magnitude of motion detection based on spatio-temporal interest points (STIP) and the multi-modal linear features combination are presented which splits a video into super frame segments of interests. After that, we select the key frames from these interesting segments of the long videos with the design and detection of the prominent region. Finally, the extensive numerical experimental verification results show our methods are superior to the previous algorithms for different stages of the redundancy elimination, video segmentation, key frame selection and vehicle detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003332",
    "keywords": [
      "Artificial intelligence",
      "Cloud computing",
      "Computer network",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Edge computing",
      "Enhanced Data Rates for GSM Evolution",
      "Frame (networking)",
      "Key (lock)",
      "Motion compensation",
      "Operating system",
      "Real-time computing",
      "Redundancy (engineering)",
      "Segmentation",
      "The Internet",
      "Video processing",
      "Video tracking",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Wan",
        "given_name": "Shaohua"
      },
      {
        "surname": "Ding",
        "given_name": "Songtao"
      },
      {
        "surname": "Chen",
        "given_name": "Chen"
      }
    ]
  },
  {
    "title": "GLMNet: Graph learning-matching convolutional networks for feature matching",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108167",
    "abstract": "Recently, graph convolutional networks (GCNs) have been employed for graph matching problem. It can integrate graph node feature embedding, node-wise affinity learning and matching optimization together in a unified end-to-end model. However, first, the matching graphs feeding to existing graph matching networks are generally fixed and independent of graph matching task, which thus are not guaranteed to be optimal for the graph matching task. Second, existing methods generally employ smoothing-based graph convolution to generate graph node embeddings, in which extensive smoothing convolution operation may dilute the desired discriminatory information of graph nodes. To overcome these issues, we propose a novel Graph Learning-Matching Network (GLMNet) for graph matching problem. GLMNet has three main aspects. (1) It integrates graph learning into graph matching which thus adaptively learns a pair of optimal graphs for graph matching task. (2) It further employs a Laplacian sharpening graph convolution to generate more discriminative node embeddings for graph matching. (3) A new constraint regularized loss is designed for GLMNet training which can encode the desired one-to-one matching constraints in matching optimization. Experiments demonstrate the effectiveness of GLMNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100354X",
    "keywords": [
      "3-dimensional matching",
      "Algorithm",
      "Bipartite graph",
      "Computer science",
      "Factor-critical graph",
      "Graph",
      "Line graph",
      "Matching (statistics)",
      "Mathematics",
      "Statistics",
      "Theoretical computer science",
      "Voltage graph"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Bo"
      },
      {
        "surname": "Sun",
        "given_name": "Pengfei"
      },
      {
        "surname": "Luo",
        "given_name": "Bin"
      }
    ]
  },
  {
    "title": "Syntactic data generation for handwritten mathematical expression recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.12.002",
    "abstract": "This paper proposes tree-based decomposition and sub-expression interchange for generating new syntactically valid handwritten mathematical expressions (HMEs) from a given set of HMEs to train an HME recognition model and a mathematical language model (LM). The recognition model is dual trained using weakly supervised learning and encoder-decoder attention loss on the generated samples. Recognition experiments indicate that the proposed data generation method is superior to other such methods for offline HMEs. The HME recognition model increases the expression recognition rates by 1.47, 2.88, and 2.67 points on the Competition on Recognition of Online Handwritten Mathematical Expressions (CROHME) 2014, 2016, and 2019 testing sets, respectively. The LM also increases them by 8.92, 6.88, and 2.59 points on the testing sets. Further adding extra LaTeX sequences is cost effective in strengthening the LM for the expression recognition rates being 2.54, 2.8, and 1.25 points higher than without them on the CROHME testing sets, respectively. Among academic systems, the trained HME recognition system achieves the best performance with 64.60% and 66.08% expression recognition rates on the CROHME 2014 and 2016 testing sets and a comparable expression recognition rate of 58.72% on the CROHME 2019 testing set, respectively. Comparison with top systems from companies on CROHME 2019 suggests that more real and/or generated HME patterns will improve the performance of HME recognition models as well as mathematical language models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521004293",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data set",
      "Expression (computer science)",
      "Language model",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Truong",
        "given_name": "Thanh-Nghia"
      },
      {
        "surname": "Nguyen",
        "given_name": "Cuong Tuan"
      },
      {
        "surname": "Nakagawa",
        "given_name": "Masaki"
      }
    ]
  },
  {
    "title": "Estimating fund-raising performance for start-up projects from a market graph perspective",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108204",
    "abstract": "In the online innovation market, the fund-raising performance of the start-up project is a concerning issue for creators, investors and platforms. Unfortunately, existing studies always focus on modeling the fund-raising process after the publishment of a project but the predicting of a project attraction in the market before setting up is largely unexploited. Usually, this prediction is always with great challenges to making a comprehensive understanding of both the start-up project and market environment. To that end, in this paper, we present a focused study on this important problem from a market graph perspective. Specifically, we propose a Graph-based Market Environment (GME) model for predicting the fund-raising performance of the unpublished project by exploiting the market environment. In addition, we discriminatively model the project competitiveness and market preferences by designing two graph-based neural network architectures and incorporating them into a joint optimization stage. Furthermore, to explore the information propagation problem with dynamic environment in a large-scale market graph, we extend the GME model with parallelizing competitiveness quantification and hierarchical propagation algorithm. Finally, we conduct extensive experiments on real-world data. The experimental results clearly demonstrate the effectiveness of our proposed model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100385X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Geometry",
      "Graph",
      "Machine learning",
      "Mathematics",
      "Perspective (graphical)",
      "Raising (metalworking)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Likang"
      },
      {
        "surname": "Li",
        "given_name": "Zhi"
      },
      {
        "surname": "Zhao",
        "given_name": "Hongke"
      },
      {
        "surname": "Liu",
        "given_name": "Qi"
      },
      {
        "surname": "Chen",
        "given_name": "Enhong"
      }
    ]
  },
  {
    "title": "Lifelong robotic visual-tactile perception learning",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108176",
    "abstract": "Lifelong machine learning can learn a sequence of consecutive robotic perception tasks via transferring previous experiences. However, 1) most existing lifelong learning based perception methods only take advantage of visual information for robotic tasks, while neglecting another important tactile sensing modality to capture discriminative material properties; 2) Meanwhile, they cannot explore the intrinsic relationships across different modalities and the common characterization among different tasks of each modality, due to the distinct divergence between heterogeneous feature distributions. To address above challenges, we propose a new Lifelong Visual-Tactile Learning (LVTL) model for continuous robotic visual-tactile perception tasks, which fully explores the latent correlations in both intra-modality and cross-modality aspects. Specifically, a modality-specific knowledge library is developed for each modality to explore common intra-modality representations across different tasks, while narrowing intra-modality mapping divergence between semantic and feature spaces via an auto-encoder mechanism. Moreover, a sparse constraint based modality-invariant space is constructed to capture underlying cross-modality correlations and identify the contributions of each modality for new coming visual-tactile tasks. We further propose a modality consistency regularizer to efficiently align the heterogeneous visual and tactile samples, which ensures the semantic consistency between different modality-specific knowledge libraries. After deriving an efficient model optimization strategy, we conduct extensive experiments on several representative datasets to demonstrate the superiority of our LVTL model. Evaluation experiments show that our proposed model significantly outperforms existing state-of-the-art methods with about 1.16% ∼ 15.36% improvement under different lifelong visual-tactile perception scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003630",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Encoder",
      "Feature (linguistics)",
      "Lifelong learning",
      "Linguistics",
      "Modalities",
      "Modality (human–computer interaction)",
      "Neuroscience",
      "Operating system",
      "Pedagogy",
      "Perception",
      "Philosophy",
      "Psychology",
      "Social science",
      "Sociology",
      "Stimulus modality",
      "Visual perception"
    ],
    "authors": [
      {
        "surname": "Dong",
        "given_name": "Jiahua"
      },
      {
        "surname": "Cong",
        "given_name": "Yang"
      },
      {
        "surname": "Sun",
        "given_name": "Gan"
      },
      {
        "surname": "Zhang",
        "given_name": "Tao"
      }
    ]
  },
  {
    "title": "HMFCA-Net: Hierarchical multi-frequency based Channel attention net for mobile phone surface defect detection",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.11.029",
    "abstract": "The surface defect detection is an important process in the production of mobile phones. To detect various mobile phone surface defects and acquire detailed features of tiny defects, this paper proposes a Hierarchical Multi-Frequency based Channel Attention Net (HMFCA-Net). In particular, an attention mechanism that uses multi-frequency information and local cross-channel interaction is proposed to represent the weighted defect features. A deformable convolution based ResNeSt network is introduced to handle various defect shapes. Besides, to overcome the extreme aspect ratio problem caused by the tiny phone surface defects, a RoI Align is introduced to decrease localization error. Experiments on the public DAGM dataset and a self-collected dataset named MPSSD shows that the proposed method achieves promising performance on defect detection task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521004268",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Channel (broadcasting)",
      "Computer science",
      "Convolution (computer science)",
      "Engineering",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Mobile phone",
      "Net (polyhedron)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Phone",
      "Process (computing)",
      "Surface (topology)",
      "Systems engineering",
      "Task (project management)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Ying"
      },
      {
        "surname": "Ding",
        "given_name": "Runwei"
      },
      {
        "surname": "Huang",
        "given_name": "Weibo"
      },
      {
        "surname": "Wei",
        "given_name": "Peng"
      },
      {
        "surname": "Yang",
        "given_name": "Ge"
      },
      {
        "surname": "Wang",
        "given_name": "Yong"
      }
    ]
  },
  {
    "title": "Transtrack: Online meta-transfer learning and Otsu segmentation enabled wireless gesture tracking",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108157",
    "abstract": "Individual diversity poses a cross-user performance variance challenge that stumbles the practicality, especially for the wireless gesture tracking systems. Since the difficulty of annotating low-semantic wireless data limits constructing a big dataset, the recognizer should quickly adjust to different individuals via small datasets. To this end, we present TransTrack, an accurate wireless indoor gesture tracking system that can adjust to different users quickly. The key insight is that each unlabeled gesture contains learnable individual features that can help the gesture tracking model learning how to adapt to different users. Specifically, TransTrack uses recursive Otsu segmentation to separate gesture-induced signals with the background noise inspired by image segmentation. It then augments training data to learn the transferable features by leveraging the redundant information. A datum-based alignment method is proposed to unlock the limitation of classifier selection without distortion. Finally, TransTrack proposes an online meta-transfer learning method that collects unlabeled data transparently to train the tracking model for different tasks. Extensive experiments show that TransTrack can quickly adapt to different users and conditions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003447",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Gesture",
      "Gesture recognition",
      "Machine learning",
      "Segmentation",
      "Telecommunications",
      "Transfer of learning",
      "Wireless"
    ],
    "authors": [
      {
        "surname": "Xiao",
        "given_name": "Jiang"
      },
      {
        "surname": "Li",
        "given_name": "Huichuwu"
      },
      {
        "surname": "Jin",
        "given_name": "Hai"
      }
    ]
  },
  {
    "title": "Learning common and label-specific features for multi-Label classification with correlation information",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108259",
    "abstract": "In multi-label classification, many existing works only pay attention to the label-specific features and label correlation while they ignore the common features and instance correlation, which are also essential for building a competitive classifier. Besides, existing works usually depend on the assumption that they tend to have the similar label-specific features if two labels are correlated. However, this assumption cannot always hold in some cases. Therefore, in this paper, we propose a new approach of learning common and label-specific features for multi-label classification using the correlation information from labels and instances. First, we introduce l 2 , 1 -norm and l 1 -norm regularizers to learn common and label-specific features simultaneously. Second, we use a regularizer to constrain label correlations on label outputs instead of coefficient matrix. Finally, instance correlations are also considered through the k-nearest neighbor mechanism. Comprehensive experiments manifest the superiority of our proposed approach against other well-established multi-label learning algorithms for label-specific features.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004398",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Correlation",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Multi-label classification",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Junlong"
      },
      {
        "surname": "Li",
        "given_name": "Peipei"
      },
      {
        "surname": "Hu",
        "given_name": "Xuegang"
      },
      {
        "surname": "Yu",
        "given_name": "Kui"
      }
    ]
  },
  {
    "title": "Learning hybrid ranking representation for person re-identification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108239",
    "abstract": "Contemporary person re-identification (re-id) methods mostly compute independentlya feature representation of each person image in the query set and the gallery set. This strategy fails to consider any ranking context information of each probe image in the query set represented implicitly by the whole gallery set. Some recent re-ranking re-id methods therefore propose to take a post-processing strategy to exploit such contextual information for improving re-id matching performance. However, post-processing is independent of model training without jointly optimising the re-id feature and the ranking context information for better compatibility. In this work, for the first time, we show that the appearance feature and the ranking context information can be jointly optimised for learning more discriminative representations and achieving superior matching accuracy. Specifically, we propose to learn a hybrid ranking representation for person re-id with a two-stream architecture: (1) In the external stream, we use the ranking list of each probe image to learn plausible visual variations among the top ranks from the gallery as the external ranking information; (2) In the internal stream, we employ the part-based fine-grained feature as the internal ranking information, which mitigates the harm of incorrect matches in the ranking list. Assembling these two streams generates a hybrid ranking representation for person matching. Extensive experiments demonstrate the superiority of our method over the state-of-the-art methods on four large-scale re-id benchmarks (Market-1501, DukeMTMC-ReID, CUHK03 and MSMT17), under both supervised and unsupervised settings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004209",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer security",
      "Context (archaeology)",
      "Data mining",
      "Discriminative model",
      "Exploit",
      "Feature (linguistics)",
      "Feature learning",
      "Information retrieval",
      "Law",
      "Learning to rank",
      "Linguistics",
      "Machine learning",
      "Matching (statistics)",
      "Mathematics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Ranking (information retrieval)",
      "Ranking SVM",
      "Representation (politics)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Guile"
      },
      {
        "surname": "Zhu",
        "given_name": "Xiatian"
      },
      {
        "surname": "Gong",
        "given_name": "Shaogang"
      }
    ]
  },
  {
    "title": "Application of CycleGAN and transfer learning techniques for automated detection of COVID-19 using X-ray images",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.11.020",
    "abstract": "Coronavirus (which is also known as COVID-19) is severely impacting the wellness and lives of many across the globe. There are several methods currently to detect and monitor the progress of the disease such as radiological image from patients’ chests, measuring the symptoms and applying polymerase chain reaction (RT-PCR) test. X-ray imaging is one of the popular techniques used to visualise the impact of the virus on the lungs. Although manual detection of this disease using radiology images is more popular, it can be time-consuming, and is prone to human errors. Hence, automated detection of lung pathologies due to COVID-19 utilising deep learning (Bowles et al.) techniques can assist with yielding accurate results for huge databases. Large volumes of data are needed to achieve generalizable DL models; however, there are very few public databases available for detecting COVID-19 disease pathologies automatically. Standard data augmentation method can be used to enhance the models’ generalizability. In this research, the Extensive COVID-19 X-ray and CT Chest Images Dataset has been used and generative adversarial network (GAN) coupled with trained, semi-supervised CycleGAN (SSA- CycleGAN) has been applied to augment the training dataset. Then a newly designed and finetuned Inception V3 transfer learning model has been developed to train the algorithm for detecting COVID-19 pandemic. The obtained results from the proposed Inception-CycleGAN model indicated Accuracy = 94.2%, Area under Curve = 92.2%, Mean Squared Error = 0.27, Mean Absolute Error = 0.16. The developed Inception-CycleGAN framework is ready to be tested with further COVID-19 X-Ray images of the chest.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521004128",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Deep learning",
      "Disease",
      "Generalizability theory",
      "Infectious disease (medical specialty)",
      "Machine learning",
      "Mathematics",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)",
      "Statistics",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Bargshady",
        "given_name": "Ghazal"
      },
      {
        "surname": "Zhou",
        "given_name": "Xujuan"
      },
      {
        "surname": "Barua",
        "given_name": "Prabal Datta"
      },
      {
        "surname": "Gururajan",
        "given_name": "Raj"
      },
      {
        "surname": "Li",
        "given_name": "Yuefeng"
      },
      {
        "surname": "Acharya",
        "given_name": "U. Rajendra"
      }
    ]
  },
  {
    "title": "4D computed tomography super-resolution reconstruction based on tensor product and nuclear norm optimization",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108150",
    "abstract": "Four-dimensional computed tomography (4D-CT) has been widely used in preoperative evaluation and radiotherapy planning of lung tumors. To reduce the damage to healthy tissue, it is a better way to limit the scan time and the number of CT slices. Yet, it leads to the reduction of CT image resolution in the superior-inferior direction. To improve the resolution of the 4D-CT image, we propose a super-resolution (SR) algorithm based on tensor product and nuclear norm optimization. The proposed cost function includes a tensor fidelity term and a nuclear norm regularization term. The tensor fidelity term consists of low-resolution (LR) and high-resolution (HR) image tensors, as well as SR operators. The nuclear norm regularization term is used to preserve the operators’ low-rank. The optimization problem can be effectively solved by an alternative direction method of the multipliers (ADMM) technique. The SR operators can extract useful information from each dimension of LR image tensors to enhance the equality of 4D-CT SR reconstruction. Experimental results show that the proposed method can preserve the edge details of the 4D-CT image. Moreover, quantitative comparisons show that the proposed method increases peak signal-to-noise ratio from 1.5 dB to 5.5 dB, structural similarity index from 2 % to 11 % , visual information fidelity from 6 % to 20 % , edge model-based blur metric from 5 % to 15 % , and decreases the spatial-spectral entropy-based quality index from 1 % to 5 % , compared with conventional 4D-CT SR algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100337X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Diffusion MRI",
      "Eigenvalues and eigenvectors",
      "Image (mathematics)",
      "Image quality",
      "Image resolution",
      "Iterative reconstruction",
      "Magnetic resonance imaging",
      "Mathematical optimization",
      "Mathematics",
      "Matrix norm",
      "Medicine",
      "Optics",
      "Physics",
      "Quantum mechanics",
      "Radiology",
      "Regularization (linguistics)",
      "Structure tensor",
      "Tomography"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Shu"
      },
      {
        "surname": "Xia",
        "given_name": "Youshen"
      }
    ]
  },
  {
    "title": "The residual generator: An improved divergence minimization framework for GAN",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108222",
    "abstract": "GAN is a generative modelling framework which has been proven as able to minimise various types of divergence measures under an optimal discriminator. However, there is a gap between the loss function of GAN used in theory and in practice. In theory, the proof of the Jensen divergence minimisation involves the min-max criterion, but in practice the non-saturating criterion is instead used to avoid gradient vanishing. We argue that the formulation of divergence minimization via GAN is biased and may yield a poor convergence of the algorithm. In this paper, we propose the Residual Generator for GAN (Rg-GAN), which is inspired by the closed-loop control theory, to bridge the gap between theory and practice. Rg-GAN minimizes the residual between the loss of the generated data to be real and the loss of the generated data to be fake from the perspective of the discriminator. In this setting, the loss terms of the generator depend only on the generated data and therefore contribute to the optimisation of the model. We formulate the residual generator for standard GAN and least-squares GAN and show that they are equivalent to the minimisation of reverse-KL divergence and a novel instance of f-divergence, respectively. Furthermore, we prove that Rg-GAN can be reduced to Integral Probability Metrics (IPMs) GANs (e.g., Wasserstein GAN) and bridge the gap between IPMs and f-divergence. Additionally, we further improve on Rg-GAN by proposing a loss function for the discriminator that has a better discrimination ability. Experiments on synthetic and natural images data sets show that Rg-GAN is robust to mode collapse, and improves the generation quality of GAN in terms of FID and IS scores.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004039",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Detector",
      "Discriminator",
      "Divergence (linguistics)",
      "Generator (circuit theory)",
      "Linguistics",
      "Mathematical optimization",
      "Mathematics",
      "Minification",
      "Minimisation (clinical trials)",
      "Philosophy",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Residual",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Gnanha",
        "given_name": "Aurele Tohokantche"
      },
      {
        "surname": "Cao",
        "given_name": "Wenming"
      },
      {
        "surname": "Mao",
        "given_name": "Xudong"
      },
      {
        "surname": "Wu",
        "given_name": "Si"
      },
      {
        "surname": "Wong",
        "given_name": "Hau-San"
      },
      {
        "surname": "Li",
        "given_name": "Qing"
      }
    ]
  },
  {
    "title": "Expression of concern: “Deep Adaptive Feature Embedding with Local Sample Distributions for Person Re-identification” [Pattern Recognition, Volume 73, January 2018, Pages 275-288]",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108133",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003204",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Embedding",
      "Feature (linguistics)",
      "Identification (biology)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Sample (material)",
      "Volume (thermodynamics)"
    ],
    "authors": []
  },
  {
    "title": "Consistent and diverse multi-View subspace clustering with structure constraint",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108196",
    "abstract": "Multi-view subspace clustering algorithms have recently been developed to process multi-view dataset clustering by accurately depicting the essential characteristics of multi-view data. Most existing methods focus on conduct self-representation property using a consistent representation and a set of specific representations with well-designed regularization to learn the common and specific knowledge among different views. However, specific representations only contain the unique information of each individual view, which limits their ability to fully excavate the diversity of multi-view data to enhance the complementarity among different views. Moreover, when conducting multi-view subspace clustering, the learned subspace self-representation and clustering are sequential and independent, which lacks consideration of the interaction between representation learning and the final clustering calculation. In this paper, a novel method termed consistent and diverse multi-view subspace clustering with structure constraint (CDMSC 2 ) is proposed to overcome the above-described deficiencies. (1) An exclusivity constraint term is employed to enhance the diversity of specific representations among different views for modeling consistency and diversity in a unified framework. (2) A clustering structure constraint is imposed on the subspace self-representation by factorizing the learned subspace self-representation into the cluster centroids and the cluster assignments with the goal of obtaining a clustering-oriented subspace self-representation. In addition, we carefully designed an efficient optimization algorithm to solve the objective function through relaxation and alternating minimization. Extensive experiments on five benchmark datasets in terms of six evaluation metrics demonstrate that our method outperforms the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003782",
    "keywords": [
      "Artificial intelligence",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Constrained clustering",
      "Constraint (computer-aided design)",
      "Correlation clustering",
      "Data mining",
      "Geometry",
      "Law",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Si",
        "given_name": "Xiaomeng"
      },
      {
        "surname": "Yin",
        "given_name": "Qiyue"
      },
      {
        "surname": "Zhao",
        "given_name": "Xiaojie"
      },
      {
        "surname": "Yao",
        "given_name": "Li"
      }
    ]
  },
  {
    "title": "Pedestrian trajectory prediction with convolutional neural networks",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108252",
    "abstract": "Predicting the future trajectories of pedestrians is a challenging problem that has a range of application, from crowd surveillance to autonomous driving. In literature, methods to approach pedestrian trajectory prediction have evolved, transitioning from physics-based models to data-driven models based on recurrent neural networks. In this work, we propose a new approach to pedestrian trajectory prediction, with the introduction of a novel 2D convolutional model. This new model outperforms recurrent models, and it achieves state-of-the-art results on the ETH and TrajNet datasets. We also present an effective system to represent pedestrian positions and powerful data augmentation techniques, such as the addition of Gaussian noise and the use of random rotations, which can be applied to any model. As an additional exploratory analysis, we present experimental results on the inclusion of occupancy methods to model social information, which empirically show that these methods are ineffective in capturing social interaction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004325",
    "keywords": [
      "Aerospace engineering",
      "Artificial intelligence",
      "Astronomy",
      "Computer science",
      "Convolutional neural network",
      "Engineering",
      "Gaussian",
      "Machine learning",
      "Pedestrian",
      "Physics",
      "Quantum mechanics",
      "Range (aeronautics)",
      "Trajectory",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Zamboni",
        "given_name": "Simone"
      },
      {
        "surname": "Kefato",
        "given_name": "Zekarias Tilahun"
      },
      {
        "surname": "Girdzijauskas",
        "given_name": "Sarunas"
      },
      {
        "surname": "Norén",
        "given_name": "Christoffer"
      },
      {
        "surname": "Dal Col",
        "given_name": "Laura"
      }
    ]
  },
  {
    "title": "Delving deep into spatial pooling for squeeze-and-excitation networks",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108159",
    "abstract": "Squeeze-and-Excitation (SE) blocks have demonstrated significant accuracy gains for state-of-the-art deep architectures by re-weighting channel-wise feature responses. The SE block is an architecture unit that integrates two operations: a squeeze operation that employs global average pooling to aggregate spatial convolutional features into a channel feature, and an excitation operation that learns instance-specific channel weights from the squeezed feature to re-weight each channel. In this paper, we revisit the squeeze operation in SE blocks, and shed lights on why and how to embed rich (both global and local) information into the excitation module at minimal extra costs. In particular, we introduce a simple but effective two-stage spatial pooling process: rich descriptor extraction and information fusion. The rich descriptor extraction step aims to obtain a set of diverse (i.e., global and especially local) deep descriptors that contain more informative cues than global average-pooling. While, absorbing more information delivered by these descriptors via a fusion step can aid the excitation operation to return more accurate re-weight scores in a data-driven manner. We validate the effectiveness of our method by extensive experiments on ImageNet for image classification and on MS-COCO for object detection and instance segmentation. For these experiments, our method achieves consistent improvements over the SENets on all tasks, in some cases, by a large margin.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003460",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Feature (linguistics)",
      "Feature extraction",
      "Geometry",
      "Linguistics",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematics",
      "Medicine",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pooling",
      "Radiology",
      "Segmentation",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Jin",
        "given_name": "Xin"
      },
      {
        "surname": "Xie",
        "given_name": "Yanping"
      },
      {
        "surname": "Wei",
        "given_name": "Xiu-Shen"
      },
      {
        "surname": "Zhao",
        "given_name": "Bo-Rui"
      },
      {
        "surname": "Chen",
        "given_name": "Zhao-Min"
      },
      {
        "surname": "Tan",
        "given_name": "Xiaoyang"
      }
    ]
  },
  {
    "title": "Cloud based scalable object recognition from video streams using orientation fusion and convolutional neural networks",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108207",
    "abstract": "Object recognition from live video streams comes with numerous challenges such as the variation in illumination conditions and poses. Convolutional neural networks (CNNs) have been widely used to perform intelligent visual object recognition. Yet, CNNs still suffer from severe accuracy degradation, particularly on illumination-variant datasets. To address this problem, we propose a new CNN method based on orientation fusion for visual object recognition. The proposed cloud-based video analytics system pioneers the use of bi-dimensional empirical mode decomposition to split a video frame into intrinsic mode functions (IMFs). We further propose these IMFs to endure Reisz transform to produce monogenic object components, which are in turn used for the training of CNNs. Past works have demonstrated how the object orientation component may be used to pursue accuracy levels as high as 93%. Herein we demonstrate how a feature-fusion strategy of the orientation components leads to further improving visual recognition accuracy to 97%. We also assess the scalability of our method, looking at both the number and the size of the video streams under scrutiny. We carry out extensive experimentation on the publicly available Yale dataset, including also a self generated video datasets, finding significant improvements (both in accuracy and scale), in comparison to AlexNet, LeNet and SE-ResNeXt, which are three most commonly used deep learning models for visual object recognition and classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003885",
    "keywords": [
      "Artificial intelligence",
      "Cloud computing",
      "Cognitive neuroscience of visual object recognition",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Database",
      "Geometry",
      "Mathematics",
      "Object (grammar)",
      "Operating system",
      "Orientation (vector space)",
      "Pattern recognition (psychology)",
      "Scalability"
    ],
    "authors": [
      {
        "surname": "Usman Yaseen",
        "given_name": "Muhammad"
      },
      {
        "surname": "Anjum",
        "given_name": "Ashiq"
      },
      {
        "surname": "Fortino",
        "given_name": "Giancarlo"
      },
      {
        "surname": "Liotta",
        "given_name": "Antonio"
      },
      {
        "surname": "Hussain",
        "given_name": "Amir"
      }
    ]
  },
  {
    "title": "Variable weight algorithm for convolutional neural networks and its applications to classification of seizure phases and types",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108226",
    "abstract": "Deep learning techniques have recently achieved impressive results and raised expectations in the domains of medical diagnosis and physiological signal processing. The widely adopted methods include convolutional neural networks (CNNs) and recurrent neural networks (RNNs). However, the existing models possess static connection weights between layers, which might limit the generalization capability and the classification performance of the models as the weights of different layers are fixed after training. Furthermore, to deal with a large amount of data, a neural network with a sufficiently large size is required. This paper proposes the variable weight convolutional neural networks (VWCNNs), which are a type of network structure employing dynamic weights instead of static weights in their convolutional layers and fully-connected layers. VWCNNs are able to adapt to different characteristics of input data and can be viewed as an infinite number of traditional, fixed-weight CNNs. We will show that the proposed VWCNN structure outperforms the conventional CNN in terms of the classification accuracy, generalization capability, and robustness when the inputs are contaminated by noise. In this paper, VWCNNs are applied to the classification of three seizure phases (seizure-free, pre-seizure and seizure) based on measured electroencephalography (EEG) data. VWCNNs achieve 100% test accuracy and show strong robustness in the classification of the three seizure phases, and thus show the potential to be a useful classification tool for medical diagnosis. Furthermore, the classification of seven types of seizures is investigated in this paper using the world’s largest open source database of seizure recordings, TUH EEG seizure corpus. Comparisons with conventional CNNs, RNN, MobileNet, ResNet, DenseNet and traditional machine learning methods including random forest, decision tree, support vector machine, K-nearest neighbours, standard neural networks, and Naïve Bayes are being conducted using realistic test data sets. The results demonstrate that VWCNNs have advantages over other classifiers in terms of classification accuracy and robustness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004076",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Electroencephalography",
      "Epileptic seizure",
      "Gene",
      "Generalization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Psychiatry",
      "Psychology",
      "Recurrent neural network",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Jia",
        "given_name": "Guangyu"
      },
      {
        "surname": "Lam",
        "given_name": "Hak-Keung"
      },
      {
        "surname": "Althoefer",
        "given_name": "Kaspar"
      }
    ]
  },
  {
    "title": "Discriminative and semantic feature selection for place recognition towards dynamic environments",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.11.014",
    "abstract": "Features play an important role in various visual tasks, especially in visual place recognition applied to perceptually changing environments. We address challenges in place recognition due to dynamic and confusable patterns by proposing a discriminative and semantic feature selection network named DSFeat in this study. With supervision of both semantic information and attention mechanism, the pixel-wise stability of features can be estimated, which indicates the probability of a static and discriminative region where features are extracted. We can then select features that are insensitive to dynamic interference and distinguishable for correct matching. The designed feature selection model is evaluated in place recognition and SLAM system using several public datasets with varying appearances and viewpoints. Experimental results demonstrate the effectiveness of the proposed method. Note that our proposed method can be easily integrated into any feature-based SLAM system.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521004050",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature selection",
      "Linguistics",
      "Machine learning",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Selection (genetic algorithm)",
      "Semantic feature",
      "Statistics",
      "Viewpoints",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Tian",
        "given_name": "Yuxin"
      },
      {
        "surname": "Miao",
        "given_name": "Jinyu"
      },
      {
        "surname": "Wu",
        "given_name": "Xingming"
      },
      {
        "surname": "Yue",
        "given_name": "Haosong"
      },
      {
        "surname": "Liu",
        "given_name": "Zhong"
      },
      {
        "surname": "Chen",
        "given_name": "Weihai"
      }
    ]
  },
  {
    "title": "BiconNet: An edge-preserved connectivity-based approach for salient object detection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108231",
    "abstract": "Salient object detection (SOD) is viewed as a pixel-wise saliency modeling task by traditional deep learning-based methods. A limitation of current SOD models is insufficient utilization of inter-pixel information, which usually results in imperfect segmentation near edge regions and low spatial coherence. As we demonstrate, using a saliency mask as the only label is suboptimal. To address this limitation, we propose a connectivity-based approach called bilateral connectivity network (BiconNet), which uses connectivity masks together with saliency masks as labels for effective modeling of inter-pixel relationships and object saliency. Moreover, we propose a bilateral voting module to enhance the output connectivity map, and a novel edge feature enhancement method that efficiently utilizes edge-specific features. Through comprehensive experiments on five benchmark datasets, we demonstrate that our proposed method can be plugged into any existing state-of-the-art saliency-based SOD framework to improve its performance with negligible parameter increase.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100412X",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Enhanced Data Rates for GSM Evolution",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Salient",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Ziyun"
      },
      {
        "surname": "Soltanian-Zadeh",
        "given_name": "Somayyeh"
      },
      {
        "surname": "Farsiu",
        "given_name": "Sina"
      }
    ]
  },
  {
    "title": "A robust image segmentation framework based on total variation spectral transform",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.12.001",
    "abstract": "Image segmentation is a critical process that influences subsequent tasks in computer vision. Despite years of research, popular segmentation methods usually cause over-segmentation or under-segmentation when segmenting images with noises. To address the issue, this paper proposes an image segmentation framework with the help of total variation (TV) spectral transform technique. Firstly, we impose the TV spectral transform on an image with noises to get basic elements that preserve shape structure. Secondly, the object structure is roughly separated from the background by a separation surface, which is fitted using the maximal response time of each pixel. Thirdly, the roughly generated object structure is refined with a guided filter whose guidance image is calculated through a band-pass filter and inverse transform in the TV spectral domain. Finally, binary segmentation and morphological operations are conducted on the refined object structure to obtain the segmentation result. Experiment results indicate that our method can achieve high segmentation accuracy and be robust to noise compared with other classical approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521004281",
    "keywords": [
      "Artificial intelligence",
      "Astrophysics",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Image segmentation",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Segmentation",
      "Variation (astronomy)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Jianwei"
      },
      {
        "surname": "Qi",
        "given_name": "Jing"
      },
      {
        "surname": "Zheng",
        "given_name": "Zhaohui"
      },
      {
        "surname": "Sun",
        "given_name": "Le"
      }
    ]
  },
  {
    "title": "Design of Blockchain enabled intrusion detection model for detecting security attacks using deep learning",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.11.023",
    "abstract": "Cyber-attacks are getting more sophisticated and nuanced. Intrusion Detection Systems (IDSs) are commonly used in a variety of networks to assist in the timely detection of intrusions. In recent years, blockchain technology has got a lot of attention as a way to share data without the need for a trusted third party. In particular, data recorded in a single block cannot be modified without impacting all subsequent blocks. For an effective update, an attacker will need to monitor the majority of network nodes, which is not feasible given the current network size. This work aims to create a deep learning-based IDS model with the potential of integrating blockchain technology with intrusion detection, inspired by the ability to apply blockchain in all fields. The proposed model outperforms the conventional systems with respect to accuracy in detecting the security attacks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521004116",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Blockchain",
      "Computer science",
      "Computer security",
      "Data mining",
      "Deep learning",
      "Geometry",
      "Intrusion detection system",
      "Mathematics",
      "Network security",
      "Variety (cybernetics)"
    ],
    "authors": [
      {
        "surname": "Saveetha",
        "given_name": "D."
      },
      {
        "surname": "Maragatham",
        "given_name": "G."
      }
    ]
  },
  {
    "title": "Semi-supervised student-teacher learning for single image super-resolution",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108206",
    "abstract": "Most existing approaches for single image super-resolution (SISR) resort to quality low-high resolution (LR-HR) pairs and available degradation kernels to train networks for a specific task in hand in a fully supervised manner. Labeled data used for training are, however, usually limited in terms of the quantity and the diversity degradation kernels. The learned SR networks with one degradation kernel (e.g., bicubic) do not generalize well and their performance sharply deteriorates on other kernels (e.g., blurred or noise). In this paper, we address the critical challenge for SISR: limited labeled LR images and degradation kernels. We propose a novel Semi-supervised Student-Teacher Super-Resolution approach called S 2 TSR that super-resolves both labelled and unlabeled LR images via adversarial learning. To better exploit the information from labeled LR images, we propose a student-teacher framework (S-T) via knowledge transfer from supervised learning (T) to unsupervised learning (S). Specifically, the S-T knowledge transfer is based on a shared SR network, partial weight sharing of dual discriminators, and a pair matching network which also plays as a ‘latent discriminator’. Lastly, to learn better features from the limited labeled LR images, we propose a new SR network via non-local and attention mechanisms. Experiments demonstrate that our approach substantially improves unsupervised methods and performs favorably over fully supervised methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003873",
    "keywords": [
      "Artificial intelligence",
      "Bicubic interpolation",
      "Combinatorics",
      "Computer science",
      "Detector",
      "Discriminator",
      "Image (mathematics)",
      "Kernel (algebra)",
      "Linear interpolation",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Telecommunications",
      "Transfer of learning",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Lin"
      },
      {
        "surname": "Yoon",
        "given_name": "Kuk-Jin"
      }
    ]
  },
  {
    "title": "Pedestrian attribute recognition: A survey",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108220",
    "abstract": "Pedestrian Attribute Recognition (PAR) is an important task in computer vision community and plays an important role in practical video surveillance. The goal of this paper is to review existing works using traditional methods or based on deep learning networks. Firstly, we introduce the background of pedestrian attribute recognition, including the fundamental concepts and formulation of pedestrian attributes and corresponding challenges. Secondly, we analyze popular solutions for this task from eight perspectives. Thirdly, we discuss the specific attribute recognition, then, give a comparison between deep learning and traditional algorithm based PAR methods. After that, we show the connections between PAR and other computer vision tasks. Fourthly, we introduce the benchmark datasets, evaluation metrics in this community, and give a brief performance comparison. Finally, we summarize this paper and give several possible research directions for PAR. The project page of this paper can be found at: https://sites.google.com/view/ahu-pedestrianattributes/.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004015",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Deep learning",
      "Economics",
      "Engineering",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Management",
      "Pedestrian",
      "Pedestrian detection",
      "Task (project management)",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xiao"
      },
      {
        "surname": "Zheng",
        "given_name": "Shaofei"
      },
      {
        "surname": "Yang",
        "given_name": "Rui"
      },
      {
        "surname": "Zheng",
        "given_name": "Aihua"
      },
      {
        "surname": "Chen",
        "given_name": "Zhe"
      },
      {
        "surname": "Tang",
        "given_name": "Jin"
      },
      {
        "surname": "Luo",
        "given_name": "Bin"
      }
    ]
  },
  {
    "title": "Constrained mutual convex cone method for image set based recognition",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108190",
    "abstract": "In this paper, we propose convex cone-based frameworks for image-set classification. Image-set classification aims to classify a set of images, usually obtained from video frames or multi-view cameras, into a target object. To accurately and stably classify a set, it is essential to accurately represent structural information of the set. There are various image features, such as histogram-based features and convolutional neural network features. We should note that most of them have non-negativity and thus can be effectively represented by a convex cone. This leads us to introduce the convex cone representation to image-set classification. To establish a convex cone-based framework, we mathematically define multiple angles between two convex cones, and then use the angles to define the geometric similarity between them. Moreover, to enhance the framework, we introduce two discriminant spaces. We first propose a discriminant space that maximizes gaps between cones and minimizes the within-class variance. We then extend it to a weighted discriminant space by introducing weights on the gaps to deal with complicated data distribution. In addition, to reduce the computational cost of the proposed methods, we develop a novel strategy for fast implementation. The effectiveness of the proposed methods is demonstrated experimentally by using five databases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003502",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convex cone",
      "Convex optimization",
      "Convex set",
      "Geometry",
      "Image (mathematics)",
      "Linear discriminant analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Regular polygon",
      "Set (abstract data type)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Sogi",
        "given_name": "Naoya"
      },
      {
        "surname": "Zhu",
        "given_name": "Rui"
      },
      {
        "surname": "Xue",
        "given_name": "Jing-Hao"
      },
      {
        "surname": "Fukui",
        "given_name": "Kazuhiro"
      }
    ]
  },
  {
    "title": "Graph variational auto-encoder for deriving EEG-based graph embedding",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108202",
    "abstract": "Graph embedding is an effective method for deriving low-dimensional representations of graph data. The power of graph deep learning methods to characterize electroencephalogram (EEG) graph embedding is still in question. We designed a novel graph variational auto-encoder (GVAE) method to extract nodal features of brain functional connections. A new decoder model for the GVAEs network is proposed, which considers the node neighborhood of the reconstructed adjacency matrix. The GVAE is applied and tested on 3 biometric databases which contain 64 to 9 channels’ EEG recordings. For all datasets, promising results with more than 95% accuracy and considerably low computational cost are achieved compared to state-of-the-art user identification methods. The proposed GVAE is robust to a limited number of nodes and stable to users’ task performance. Moreover, we developed a traditional variational auto-encoder to demonstrate that more accurate features can be obtained when observing EEG-based brain connectivity from a graph perspective.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003848",
    "keywords": [
      "Adjacency list",
      "Adjacency matrix",
      "Algorithm",
      "Artificial intelligence",
      "Autoencoder",
      "Computer science",
      "Deep learning",
      "Electroencephalography",
      "Embedding",
      "Encoder",
      "Graph",
      "Graph embedding",
      "Operating system",
      "Pattern recognition (psychology)",
      "Psychiatry",
      "Psychology",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Behrouzi",
        "given_name": "Tina"
      },
      {
        "surname": "Hatzinakos",
        "given_name": "Dimitrios"
      }
    ]
  },
  {
    "title": "Creating synthetic minority class samples based on autoencoder extreme learning machine",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108191",
    "abstract": "This paper reports a new method (simplified as AE-ELM-SynMin) to create the Synthetic Minority class samples for imbalanced classification based on AutoEncoder Extreme Learning Machine (AE-ELM). AE-ELM-SynMin first trains an AE-ELM which is a special ELM with the same input and output, i.e., the original minority class samples. Second, the crossover, mutation and filtration operations are conducted on the hidden-layer output of AE-ELM and then the synthetic hidden-layer output is obtained. Third, the synthetic minority class samples are created by decoding the synthetic hidden-layer output with output-layer weights of AE-ELM. AE-ELM-SynMin guarantees that the synthetic minority class has the higher information amount than original minority class and meanwhile keeps the consistent probability distribution with the original minority class. The experimental results demonstrate the better imbalanced classification performances of AE-ELM-SynMin in comparison with the regular synthetic minority over-sampling technique (Regular-SMOTE) and its variants, e.g., Borderline-SMOTE, Random-SMOTE, and SMOTE-IPF.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003472",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Chemistry",
      "Class (philosophy)",
      "Computer science",
      "Crossover",
      "Decoding methods",
      "Extreme learning machine",
      "Layer (electronics)",
      "Organic chemistry",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Yu-Lin"
      },
      {
        "surname": "Xu",
        "given_name": "Sheng-Sheng"
      },
      {
        "surname": "Huang",
        "given_name": "Joshua Zhexue"
      }
    ]
  },
  {
    "title": "SVDN: A spatially variant degradation network for blind image super-resolution",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.12.009",
    "abstract": "Blind super-resolution (SR) aims at generating a high-resolution image from a low-resolution image where the degradation kernels are unknown. Most existing blind super-resolution approaches apply an estimated blur kernel as a degradation prior to the entire image. However, due to the intrinsic degradation characteristics of camera lens or different depth-of-filed in the image, the degradation kernels in different regions are not exactly the same. To address this issue, we propose a spatially variant degradation network (SVDN) for blind SR with a pixel-wise kernel estimation block to handle the complex and unknown degradations in blind SR tasks. Moreover, to fully exploit the estimated degradation information, the output of network is influenced by the estimated kernel via an affine transformation applied to the feature maps in each middle layer. The proposed method permits a lot of freedom in adapting the image degradation locally. Extensive experiments on synthetic and real-world images demonstrate that the proposed method generates more accurate SR results under the complex degradation settings and provide naturally visual results both on the in-focus and out-of-focus regions of the images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521004414",
    "keywords": [
      "Affine transformation",
      "Artificial intelligence",
      "Biochemistry",
      "Block (permutation group theory)",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Degradation (telecommunications)",
      "Estimator",
      "Feature (linguistics)",
      "Focus (optics)",
      "Gene",
      "Geometry",
      "Image (mathematics)",
      "Image processing",
      "Image resolution",
      "Image restoration",
      "Kernel (algebra)",
      "Kernel density estimation",
      "Linguistics",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pixel",
      "Pure mathematics",
      "Statistics",
      "Telecommunications",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Shiyan"
      },
      {
        "surname": "Zhang",
        "given_name": "Jiangshan"
      },
      {
        "surname": "Yu",
        "given_name": "Xiang"
      },
      {
        "surname": "Shi",
        "given_name": "Fan"
      }
    ]
  },
  {
    "title": "Object-based cluster validation with densities",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108223",
    "abstract": "Clustering validity indices are typically used as tools to find the correct number of clusters in a data set and/or to evaluate the quality of the clusters formed by clustering algorithms. Clustering validity indices measure separation and compactness of clusters. Typically, when applying a clustering algorithm, the input includes the number of clusters. After applying the algorithm with several different numbers of clusters, we determine the number of clusters to be the one with the best validity index. There are two types of clustering validity indices: external indices that are supervised, and internal indices that are unsupervised. The focus of this paper is on internal validity indices. Some existing internal validity indices capture the properties of the clusters by using representative statistics such as mean, variance, diameter, etc., however, these do not perform well when clusters have arbitrary shapes. One approach to overcome this issue is to use the density of the data objects in each cluster. That provides the advantage of capturing the full characteristics of the cluster which is most beneficial when there are clusters with arbitrary shapes. In the literature, a few density-based clustering validity indices have been proposed. However, some of them show poor performance when the clusters are not perfectly separated. Some others perform poorly because they use only representative objects from each cluster instead of all objects. The contribution of this paper is an internal validity index named the object-based clustering validity index with densities (OCVD). OCVD is a single number that averages the density-based contribution of individual data objects to both separation and compactness of clusters. The methodology behind calculating the density-based contributions of the objects is kernel density estimation. We show through several experiments that OCVD performs well in detecting the correct number of clusters in data sets with different cluster shapes including arbitrary shapes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004040",
    "keywords": [
      "Accounting",
      "Artificial intelligence",
      "Business",
      "CURE data clustering algorithm",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Determining the number of clusters in a data set",
      "Fuzzy clustering",
      "Mathematics",
      "Measure (data warehouse)",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Single-linkage clustering",
      "Variance (accounting)",
      "k-medians clustering"
    ],
    "authors": [
      {
        "surname": "Tavakkol",
        "given_name": "Behnam"
      },
      {
        "surname": "Choi",
        "given_name": "Jeongsub"
      },
      {
        "surname": "Jeong",
        "given_name": "Myong Kee"
      },
      {
        "surname": "Albin",
        "given_name": "Susan L."
      }
    ]
  },
  {
    "title": "Biomedical image segmentation based on full-Resolution network",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.11.017",
    "abstract": "Convolutional neural networks (CNN) has been widely used in biomedical image segmentation (BIS) tasks for its remarkable feature representation capability, and most of existing CNN-based segmentation networks leverage a down-sampling operation to achieve larger acceptance domain. However, down-sampling operations could inevitably loss the detailed information of images which is very important for the BIS task. In this paper, we propose a full-resolution biomedical image segmentation network(FRNet) that could maintain the integrated detailed information of image while keeping sufficient semantic information and large receptive field. Specifically, the basic semantic feature and non-destructive feature are employed to represent the semantic and detailed information of images, respectively. A backbone network and a new full-feature extraction branch are conducted to extract those two kinds of complementary features. Furthermore, a novel feature fusion module is designed to integrate those complementary features to achieve non-destructive description of images. Finally, in order to further improve the description ability of the integrated feature, a Densely connected Atrous Spatial Pyramid Pooling(DenseASPP) module is arranged at the end of our proposed FRNet to extract the multiscale information of images. Thorough experimental results on several available databases demonstrate the effectiveness and advancement of FRNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521004086",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Feature extraction",
      "Geometry",
      "Leverage (statistics)",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pooling",
      "Pyramid (geometry)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Qu",
        "given_name": "Lei"
      },
      {
        "surname": "Wang",
        "given_name": "Meng"
      },
      {
        "surname": "Guo",
        "given_name": "Kaixuan"
      },
      {
        "surname": "Wan",
        "given_name": "Wan"
      },
      {
        "surname": "Liu",
        "given_name": "Yu"
      },
      {
        "surname": "Tang",
        "given_name": "Jun"
      },
      {
        "surname": "Wu",
        "given_name": "Jun"
      },
      {
        "surname": "Duan",
        "given_name": "Peng"
      }
    ]
  },
  {
    "title": "MTHetGNN: A heterogeneous graph embedding framework for multivariate time series forecasting",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.12.008",
    "abstract": "Multivariate time series forecasting, which analyzes historical time series to predict future trends, can effectively help decision-making. Complex relations among variables in MTS, including static, dynamic, predictable, and latent relations, have made it possible to mining more features of MTS. Modeling complex relations are not only essential in characterizing latent dependency as well as modeling temporal dependence, but also brings great challenges in the MTS forecasting task. However, existing methods mainly focus on modeling certain relations among MTS variables. In this paper, we propose a novel end-to-end deep learning model, termed Multivariate Time Series Forecasting via Heterogeneous Graph Neural Networks (MTHetGNN). To characterize complex relations among variables, a relation embedding module is designed in MTHetGNN, where each variable is regarded as a graph node, and each type of edge represents a specific static or dynamic relationship. Meanwhile, a temporal embedding module is introduced for time series features extraction, where involving convolutional neural network (CNN) filters with different perception scales. Finally, a heterogeneous graph embedding module is adopted to handle the complex structural information generated by the two modules. Three benchmark datasets from the real world are used to evaluate the proposed MTHetGNN. The comprehensive experiments show that MTHetGNN achieves state-of-the-art results in the MTS forecasting task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521004396",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Embedding",
      "Geodesy",
      "Geography",
      "Graph",
      "Latent variable",
      "Law",
      "Machine learning",
      "Multivariate statistics",
      "Paleontology",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Series (stratigraphy)",
      "Theoretical computer science",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yueyang"
      },
      {
        "surname": "Duan",
        "given_name": "Ziheng"
      },
      {
        "surname": "Huang",
        "given_name": "Yida"
      },
      {
        "surname": "Xu",
        "given_name": "Haoyan"
      },
      {
        "surname": "Feng",
        "given_name": "Jie"
      },
      {
        "surname": "Ren",
        "given_name": "Anni"
      }
    ]
  },
  {
    "title": "An adaptive index smoothing loss for face anti-spoofing",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.12.006",
    "abstract": "Face anti-spoofing is the security guarantee of facial recognition technology. Situations exist extensively in which the half total error rate (HTER) is used as evaluation, and the cross-entropy is utilized as a loss function for face anti-spoofing. Though the reduction of cross-entropy can cause the change of HTER, their relationship is not strictly monotone. Note that when HTER is feedback to the network, a better model can be learned by optimization. However, HTER is a discrete performance metric and cannot be used as a loss function. In this paper, we propose an adaptive index smoothing loss (AISL) for face anti-spoofing. Firstly, we smoothly approximate HTER to make it a derivable loss function. Then, we reshape the smoothed HTER as the desired loss and assign different weights to the false acceptance rate (FAR) and the false rejection rate (FRR). Finally, we introduce the prior knowledge into the loss. Extensive experimental examples of four benchmark datasets are given to illustrate the effectiveness of the proposed method. Actual results show that the proposed method can significantly improve the accuracy and stability of the model and enhance the security of face recognition systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521004384",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Computer vision",
      "Entropy (arrow of time)",
      "Evolutionary biology",
      "Face (sociological concept)",
      "Function (biology)",
      "Geodesy",
      "Geography",
      "Geometry",
      "Mathematics",
      "Physics",
      "Quantum mechanics",
      "Reduction (mathematics)",
      "Smoothing",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Caixun"
      },
      {
        "surname": "Zhou",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "Automated visual stimuli evoked multi-channel EEG signal classification using EEGCapsNet",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.11.019",
    "abstract": "Automated visual stimuli evoked multi-channel electroencephalograph (EEG) signals classification is in a nascent stage but is receiving progressive attention from researchers. The conventional techniques existing for EEG classification tasks overlook the spatial attributes of EEG signals, which contain spatial data information to characterize an image from visual stimuli evoked EEG signal. In this paper, a Deep learning structure implemented on STFT (Short Term Fourier Transform) generated spectrogram images and a Capsule Network (EEGCapsNet) is proposed. In this architecture, the time and frequency domain as well as, spatial attributes of the multi-channel EEG signals are extracted to build the spectrogram image and are fed to the proposed EEGCapsNet for classifying those EEG signals which are acquired from the stimuli evoked visual experience while seeing an image. For this purpose, two different EEG datasets (namely Perceive and MindBig) are trained on the proposed network. The highest average accuracy of 81.59% and 84.62% is reported for the proposed EEGCapsNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521004104",
    "keywords": [
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Electroencephalography",
      "Feature extraction",
      "Fourier analysis",
      "Fourier transform",
      "Mathematical analysis",
      "Mathematics",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Programming language",
      "Psychology",
      "SIGNAL (programming language)",
      "Short-time Fourier transform",
      "Spectrogram",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Kumari",
        "given_name": "Nandini"
      },
      {
        "surname": "Anwar",
        "given_name": "Shamama"
      },
      {
        "surname": "Bhattacharjee",
        "given_name": "Vandana"
      }
    ]
  },
  {
    "title": "Financial time series forecasting with multi-modality graph neural network",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108218",
    "abstract": "Financial time series analysis plays a central role in hedging market risks and optimizing investment decisions. This is a challenging task as the problems are always accompanied by multi-modality streams and lead-lag effects. For example, the price movements of stock are reflections of complicated market states in different diffusion speeds, including historical price series, media news, associated events, etc. Furthermore, the financial industry requires forecasting models to be interpretable and compliant. Therefore, in this paper, we propose a multi-modality graph neural network (MAGNN) to learn from these multimodal inputs for financial time series prediction. The heterogeneous graph network is constructed by the sources as nodes and relations in our financial knowledge graph as edges. To ensure the model interpretability, we leverage a two-phase attention mechanism for joint optimization, allowing end-users to investigate the importance of inner-modality and inter-modality sources. Extensive experiments on real-world datasets demonstrate the superior performance of MAGNN in financial market prediction. Our method provides investors with a profitable as well as interpretable option and enables them to make informed investment decisions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100399X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Economics",
      "Finance",
      "Financial market",
      "Graph",
      "Interpretability",
      "Investment strategy",
      "Leverage (statistics)",
      "Machine learning",
      "Market liquidity",
      "Modality (human–computer interaction)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Cheng",
        "given_name": "Dawei"
      },
      {
        "surname": "Yang",
        "given_name": "Fangzhou"
      },
      {
        "surname": "Xiang",
        "given_name": "Sheng"
      },
      {
        "surname": "Liu",
        "given_name": "Jin"
      }
    ]
  },
  {
    "title": "Deep neural networks-based relevant latent representation learning for hyperspectral image classification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108224",
    "abstract": "The classification of hyperspectral image is a challenging task due to the high dimensional space, with large number of spectral bands, and low number of labeled training samples. To overcome these challenges, we propose a novel methodology for hyperspectral image classification based on multi-view deep neural networks which fuses both spectral and spatial features by using only a small number of labeled samples. Firstly, we process the initial hyperspectral image in order to extract a set of spectral and spatial features. Each spectral vector is the spectral signature of each pixel of the image. The spatial features are extracted using a simple deep autoencoder, which seeks to reduce the high dimensionality of data taking into account the neighborhood region for each pixel. Secondly, we propose a multi-view deep autoencoder model which allows fusing the spectral and spatial features extracted from the hyperspectral image into a joint latent representation space. Finally, a semi-supervised graph convolutional network is trained based on thee fused latent representation space to perform the hyperspectral image classification. The main advantage of the proposed approach is to allow the automatic extraction of relevant information while preserving the spatial and spectral features of data, and improve the classification of hyperspectral images even when the number of labeled samples is low. Experiments are conducted on three real hyperspectral images respectively Indian Pines, Salinas, and Pavia University datasets. Results show that the proposed approach is competitive in classification performances compared to state-of-the-art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004052",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Feature learning",
      "Hyperspectral imaging",
      "Pattern recognition (psychology)",
      "Pixel"
    ],
    "authors": [
      {
        "surname": "Sellami",
        "given_name": "Akrem"
      },
      {
        "surname": "Tabbone",
        "given_name": "Salvatore"
      }
    ]
  },
  {
    "title": "Action recognition via pose-based graph convolutional networks with intermediate dense supervision",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108170",
    "abstract": "Pose-based action recognition has drawn considerable attention recently. Existing methods exploit the joint position to extract body-part features from the activation maps of the backbone CNN to assist human action recognition. However, there are two limitations: (1) the body-part features are independently used or simply concatenated to obtain a representation, where the prior knowledge about the structured correlations between body parts are not fully exploited; (2) the backbone CNN, from which the body-part features are extracted, is “lazy”. It always contents itself with identifying patterns from the most discriminative areas of the input, which causes no information on the features extracted from other areas. This consequently hampers the performance of the followed aggregation process and makes the model easy to be misled by the training data bias. To address these problems, we encode the body-part features into a human-based spatiotemporal graph and employ a light-weight graph convolutional module to explicitly model the dependencies between body parts. Besides, we introduce a novel intermediate dense supervision to promote the backbone CNN to treat all regions equally, which is simple and effective, without extra parameters and computations. The proposed approach, namely, the pose-based graph convolutional network (PGCN), is evaluated on three popular benchmarks, where our approach significantly outperforms the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003575",
    "keywords": [
      "Action recognition",
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Class (philosophy)",
      "Computation",
      "Computer science",
      "Computer security",
      "Convolutional neural network",
      "Discriminative model",
      "ENCODE",
      "Exploit",
      "Feature learning",
      "Gene",
      "Graph",
      "Law",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Lei"
      },
      {
        "surname": "Zhang",
        "given_name": "Yifan"
      },
      {
        "surname": "Cheng",
        "given_name": "Jian"
      },
      {
        "surname": "Lu",
        "given_name": "Hanqing"
      }
    ]
  },
  {
    "title": "A novel GCN-based point cloud classification model robust to pose variances",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108251",
    "abstract": "Point cloud data can be produced by many depth sensors, such as Light Detection and Ranging (LIDAR) and RGB-D cameras, and they are widely used in broad applications of robotic navigation and remote-sensing for the understanding of environment. Hence, new techniques for object representation and classification based on 3D point cloud are becoming increasingly in high demand. Due to the irregularity of the object shape, the point cloud-based object recognition is a very challenging task, especially the pose variances of a point cloud will impose many difficulties. In this paper, we tackle the challenge of pose variances in object classification based on point cloud by developing a novel end-to-end pose robust graph convolutional network. Technically, we first represent the point cloud using the spherical system instead of the traditional Cartesian system for simplicity of computation and representation. Then a pose auxiliary network is constructed with an aim to estimate the pose changes in terms of rotation angles. Finally, a graph convolutional network is constructed for object classification against the pose variations of point cloud. The experimental results show the new model outperforms the existing approaches (such as PointNet and PointNet++) on the classification task when conducting experiments on both the ModelNet40 and the ShapeNetCore dataset with a series of random rotations of a 3D point cloud. Specifically, we obtain 73.02% accuracy for classification task on the ModelNet40 with delaunay triangulation algorithm, which is much better than the state of the art algorithms, such as PointNet and PointCNN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004313",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Delaunay triangulation",
      "Graph",
      "Pattern recognition (psychology)",
      "Point cloud",
      "Pose",
      "RGB color model",
      "Ranging",
      "Telecommunications",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Huafeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Yaming"
      },
      {
        "surname": "Liu",
        "given_name": "Wanquan"
      },
      {
        "surname": "Gu",
        "given_name": "Xianfeng"
      },
      {
        "surname": "Jing",
        "given_name": "Xin"
      },
      {
        "surname": "Liu",
        "given_name": "Zicheng"
      }
    ]
  },
  {
    "title": "Pareto optimization of deep networks for COVID-19 diagnosis from chest X-rays",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108242",
    "abstract": "The year 2020 was characterized by the COVID-19 pandemic that has caused, by the end of March 2021, more than 2.5 million deaths worldwide. Since the beginning, besides the laboratory test, used as the gold standard, many applications have been applying deep learning algorithms to chest X-ray images to recognize COVID-19 infected patients. In this context, we found out that convolutional neural networks perform well on a single dataset but struggle to generalize to other data sources. To overcome this limitation, we propose a late fusion approach where we combine the outputs of several state-of-the-art CNNs, introducing a novel method that allows us to construct an optimum ensemble determining which and how many base learners should be aggregated. This choice is driven by a two-objective function that maximizes, on a validation set, the accuracy and the diversity of the ensemble itself. A wide set of experiments on several publicly available datasets, accounting for more than 92,000 images, shows that the proposed approach provides average recognition rates up to 93.54% when tested on external datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004234",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Construct (python library)",
      "Context (archaeology)",
      "Convolutional neural network",
      "Coronavirus disease 2019 (COVID-19)",
      "Deep learning",
      "Disease",
      "Geography",
      "Gold standard (test)",
      "Infectious disease (medical specialty)",
      "Machine learning",
      "Mathematics",
      "Medicine",
      "Pareto principle",
      "Pathology",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Statistics",
      "Test set"
    ],
    "authors": [
      {
        "surname": "Guarrasi",
        "given_name": "Valerio"
      },
      {
        "surname": "D’Amico",
        "given_name": "Natascha Claudia"
      },
      {
        "surname": "Sicilia",
        "given_name": "Rosa"
      },
      {
        "surname": "Cordelli",
        "given_name": "Ermanno"
      },
      {
        "surname": "Soda",
        "given_name": "Paolo"
      }
    ]
  },
  {
    "title": "WITS: Weakly-supervised individual tooth segmentation model trained on box-level labels",
    "journal": "Pattern Recognition",
    "year": "2023",
    "doi": "10.1016/j.patcog.2022.108974",
    "abstract": "Accurately and automatically segmenting teeth from cone-beam computed tomography (CBCT) images plays an essential role in dental disease diagnosis and treatment. This paper presents an automatic tooth segmentation model that combines deep learning methods and level-set approaches. The proposed model uses a deep learning method to detect each tooth’s location and size and generates prior ellipses from those detected boundary boxes. Calculating each point’s signed distance to the prior edge and using them as prior weights, the restriction term can constrain the evolution of level set functions according to the distance to the prior ellipses. Then, we use the curvature direction to find out joint points of teeth and employ a variational model to separate them to get individual results. By quantitative evaluation, we show that the proposed model can accurately segment teeth. The performance is more accurate and stable than those of classical level-set models and deep-learning models. For example, the Dice coefficient is increased by 7 % than that of the U-Net model. Besides, we will release the code on https://github.com/ruicx/Individual-Tooth-Segmentation-with-Rectangle-Labels.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200454X",
    "keywords": [
      "Artificial intelligence",
      "Boundary (topology)",
      "Computer science",
      "Computer vision",
      "Curvature",
      "Deep learning",
      "Ellipse",
      "Enhanced Data Rates for GSM Evolution",
      "Geometry",
      "Image segmentation",
      "Level set (data structures)",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Point (geometry)",
      "Programming language",
      "Rectangle",
      "Segmentation",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Xie",
        "given_name": "Ruicheng"
      },
      {
        "surname": "Yang",
        "given_name": "Yunyun"
      },
      {
        "surname": "Chen",
        "given_name": "Zhaoyang"
      }
    ]
  },
  {
    "title": "ANCES: A novel method to repair attribute noise in classification problems",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108198",
    "abstract": "Noise negatively affects the complexity and performance of models built in classification problems. The most common approach to mitigate its consequences is the usage of preprocessing techniques, known as noise filters, which are designed to remove noisy samples from the training data. Nevertheless, they are specifically oriented to deal with errors affecting class labels. Their employment may not always result in an improvement when noise affects attribute values. In these cases, correcting the errors is an interesting alternative to traditional noise filtering that has not been enough studied so far in the specialized literature. This research proposes an attribute noise correction method with the final aim of increasing the performance of the classification algorithms used later. The identification of noisy data is based on an error score assigned to each one of the attribute values in the dataset, which are then passed through an optimization process to correct their potential noise. The validity of the proposed method is studied in an exhaustive experimental study, in which it is compared to several well-known preprocessing methods to deal with noisy datasets. The results obtained show the suitability of attribute noise correction with respect to the other alternatives when data suffer from attribute noise.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003800",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Class (philosophy)",
      "Computer science",
      "Data mining",
      "Data pre-processing",
      "Identification (biology)",
      "Image (mathematics)",
      "Machine learning",
      "Noise (video)",
      "Noise measurement",
      "Noise reduction",
      "Noisy data",
      "Operating system",
      "Pattern recognition (psychology)",
      "Preprocessor",
      "Process (computing)"
    ],
    "authors": [
      {
        "surname": "Sáez",
        "given_name": "José A."
      },
      {
        "surname": "Corchado",
        "given_name": "Emilio"
      }
    ]
  },
  {
    "title": "Indefinite twin support vector machine with DC functions programming",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108195",
    "abstract": "Twin support vector machine (TWSVM) is an efficient algorithm for binary classification. However, the lack of the structural risk minimization principle restrains the generalization of TWSVM and the guarantee of convex optimization constraints TWSVM to only use positive semi-definite kernels (PSD). In this paper, we propose a novel TWSVM for indefinite kernel called indefinite twin support vector machine with difference of convex functions programming (ITWSVM-DC). The indefinite TWSVM (ITWSVM) leverages a maximum margin regularization term to improve the generalization of TWSVM and a smooth quadratic hinge loss function to make the model continuously differentiable. The representer theorem is applied to the ITWSVM and the convexity of the ITWSVM is analyzed. In order to address the non-convex optimization problem when the kernel is indefinite, a difference of convex functions (DC) is used to decompose the non-convex objective function into the subtraction of two convex functions and a line search method is applied in the DC algorithm to accelerate the convergence rate. A theoretical analysis illustrates that ITWSVM-DC can converge to a local optimum and extensive experiments on indefinite and positive semi-definite kernels show the superiority of ITWSVM-DC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003770",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Convex function",
      "Convex optimization",
      "Convexity",
      "Differentiable function",
      "Discrete mathematics",
      "Economics",
      "Financial economics",
      "Geometry",
      "Hilbert space",
      "Hinge loss",
      "Kernel (algebra)",
      "Mathematical optimization",
      "Mathematics",
      "Pure mathematics",
      "Quadratic programming",
      "Regular polygon",
      "Regularization (linguistics)",
      "Reproducing kernel Hilbert space",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "An",
        "given_name": "Yuexuan"
      },
      {
        "surname": "Xue",
        "given_name": "Hui"
      }
    ]
  },
  {
    "title": "A hierarchical model for learning to understand head gesture videos",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108256",
    "abstract": "Head gesture videos recorded of a person bear rich information about the individual. Automatically understanding these videos can empower many useful human-centered applications in areas such as smart health, education, work safety and security. To understand a video’s content, low-level head gesture signals carried in the video that capture characteristics of both human postures and motions need to be translated into high-level semantic labels. To meet this aim, we propose a hierarchical model for learning to understand head gesture videos. Given a head gesture video of an arbitrary length, the model first segments the full-length video into multiple short clips for clip-based feature extraction. Multiple base feature extraction procedures are then independently tuned via a set of peripheral learning tasks without consuming any labels of the goal task. These independently derived base features are subsequently aggregated through a multi-task learning framework, coupled with a feature dimensionality reduction module, to optimally learn to accomplish the end video understanding task in an weakly supervised manner, utilizing the limited amount of video labels available of the goal task. Experimental results show that the hierarchical model is superior to multiple state-of-the-art peer methods in tackling versatile video understanding tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004362",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Economics",
      "Feature (linguistics)",
      "Feature extraction",
      "Gesture",
      "Gesture recognition",
      "Linguistics",
      "Machine learning",
      "Management",
      "Philosophy",
      "Programming language",
      "Set (abstract data type)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Jiachen"
      },
      {
        "surname": "Xu",
        "given_name": "Songhua"
      },
      {
        "surname": "Qin",
        "given_name": "Xueying"
      }
    ]
  },
  {
    "title": "Unsupervised thermal-to-visible domain adaptation method for pedestrian detection",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.11.024",
    "abstract": "Pedestrian detection is a common task in the research area of video analysis and its results lay the foundations of a wide range of applications. It is commonly known that under challenging illumination and weather conditions, conventional visible cameras perform poorly and this limitation could be catered using thermal imagery. But, due to the fact that annotated thermal datasets are less available compared to the visible ones, in this paper we emphasis the need for leveraging information from the visible domain to perform detection in the thermal domain at no additional annotation cost. Precisely, we propose a domain adaptation method by incorporating feature distribution alignments into Faster R-CNN architecture at different levels and at two phases of the network. The resulting proposed adaptive detector has the advantage of covering different aspects of the domain shift in order to improve the overall performance. The proposed detector is evaluated on KAIST multispectral dataset and the obtained results demonstrate its effectiveness by improving the adaptability in the thermal domain. Also, by means of comparisons to other existing works, better results are obtained. Additional experiments are conducted on other datasets to further justify the obtained results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521004153",
    "keywords": [
      "Adaptability",
      "Adaptation (eye)",
      "Annotation",
      "Archaeology",
      "Artificial intelligence",
      "Biology",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Detector",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Ecology",
      "Economics",
      "Feature (linguistics)",
      "Geography",
      "Linguistics",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Multispectral image",
      "Object detection",
      "Optics",
      "Pattern recognition (psychology)",
      "Pedestrian",
      "Pedestrian detection",
      "Philosophy",
      "Physics",
      "Task (project management)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Marnissi",
        "given_name": "Mohamed Amine"
      },
      {
        "surname": "Fradi",
        "given_name": "Hajer"
      },
      {
        "surname": "Sahbani",
        "given_name": "Anis"
      },
      {
        "surname": "Essoukri Ben Amara",
        "given_name": "Najoua"
      }
    ]
  },
  {
    "title": "Category attention transfer for efficient fine-grained visual categorization",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.11.015",
    "abstract": "Fine-Grained Visual Categorization (FGVC) aims at distinguishing subordinate-level categories with subtle interclass differences. Although previous research shows the impressive effectiveness of the recurrent multi-attention models and the second-order feature encoding, they often require an enormous amount of both computation and memory space, making them inadequate for mobile applications. This paper proposed a Category Attention Transfer CNN (CAT-CNN) to address the efficiency issue in solving FGVC problems. We transfer part attention knowledge from a very large-scale FGVC network to a small but efficient network to significantly improve its presentation ability. Using the proposed CAT-CNN, the accuracy of the efficient networks, such as ShuffleNet, MobilieNet, and EfficientNet, can be improved by up to 5.7% on the CUB-2011-200 dataset without increasing computation complexity or memory cost. Our experiments show that the proposed CAT-CNN can be applied to multiple structures to enhance their performance. With a single efficient network structure and single inference, the proposed CAT-MobileNet-large-1.0 and the CAT-EfficientNet-b0 can achieve accuracies of 86.5% and 86.7%, respectively, on the CUB-2011-200 dataset, which is close to or better than the results from state-of-the-art methods using large scale networks and multiple inferences, and make FGVC feasible on mobile devices.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521004062",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Categorization",
      "Computation",
      "Computer science",
      "Encoding (memory)",
      "Feature (linguistics)",
      "Inference",
      "Linguistics",
      "Machine learning",
      "Mobile device",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Transfer of learning",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Liao",
        "given_name": "Qiyu"
      },
      {
        "surname": "Wang",
        "given_name": "Dadong"
      },
      {
        "surname": "Xu",
        "given_name": "Min"
      }
    ]
  },
  {
    "title": "Small object detection in remote sensing images based on super-resolution",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.11.027",
    "abstract": "Accurate objects detection in remote sensing images is very important, because security, transportation, and rescue applications in military and civilian fields require fully analyzing and using these images. To address the problem that many small-sized objects in remote sensing images are difficult to detect, this paper proposes an improved S 2 ANET-SR model based on S 2 A-NET network. In this paper, the original and reduced image are fed to the detection network at the same time, and then a super-resolution enhancement module for the reduced image is designed to enhance the feature extraction of small objects, after that, the perceptual loss and texture matching loss is proposed as supervision. Extensional experiments are conducted to evaluate the performance on the general remote sensing dataset DOTA, and the results show that our proposed method can achieve 74.47% mAP, which is 0.79% better than the accuracy of S 2 A-NET.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521004244",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Feature extraction",
      "Geography",
      "Image (mathematics)",
      "Image resolution",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Remote sensing",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Xiaolin",
        "given_name": "Fang"
      },
      {
        "surname": "Fan",
        "given_name": "Hu"
      },
      {
        "surname": "Ming",
        "given_name": "Yang"
      },
      {
        "surname": "Tongxin",
        "given_name": "Zhu"
      },
      {
        "surname": "Ran",
        "given_name": "Bi"
      },
      {
        "surname": "Zenghui",
        "given_name": "Zhang"
      },
      {
        "surname": "Zhiyuan",
        "given_name": "Gao"
      }
    ]
  },
  {
    "title": "Mixture factor analysis with distance metric constraint for dimensionality reduction",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108156",
    "abstract": "Dimensionality reduction (DR) is a key preprocessing stage in high-dimensional data classification. Traditional linear DR algorithms, e.g., Linear Discriminant Analysis, transform the original data into a low-dimensional subspace with a linear transformation matrix. However, these methods cannot handle complex nonlinearly separable data. Although some nonlinear DR methods, e.g., Locally Linear Embedding, are proposed to solve this problem, most of them are unsupervised, which only focus on the data structure hidden in the original high-dimensional space, rather than maximizing the inter-class separability of the transformed data, thus reducing the classification accuracy. To tackle this challenge, a novel supervised nonlinear DR algorithm, distance metric restricted mixture factor analysis (DMR-MFA), is proposed for high-dimensional data classification. In DMR-MFA, the original data is divided into several clusters, and the generation of original data in each cluster is described via a factor analysis model. Meanwhile, the distance metric constraint (DMC) is used for maximizing the separability of transformed low-dimensional data from different classes. Moreover, the optimal model parameters are learned via the joint optimization of log-likelihood function and DMC loss function, which makes the DMR-MFA possible to obtain the more separable low-dimensional embeddings while accurately describing the original data. Experimental results on synthetic data, benchmark datasets and high-resolution range profile data demonstrate that our method can handle nonlinearly separable data and improves the classification accuracy of data with high dimensionality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003435",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Computer science",
      "Dimensionality reduction",
      "Economics",
      "Linear discriminant analysis",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Jian"
      },
      {
        "surname": "Liao",
        "given_name": "Leiyao"
      },
      {
        "surname": "Zhang",
        "given_name": "Wei"
      },
      {
        "surname": "Du",
        "given_name": "Lan"
      }
    ]
  },
  {
    "title": "SAR-to-optical image translation based on improved CGAN",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108208",
    "abstract": "SAR images have the advantages of being less susceptible to clouds and light, while optical images conform to the human vision system. Both of them are widely applied in the field of scene classification, natural environment monitoring, disaster warning, etc. However, due to the speckle noise caused by the SAR imaging principle, it is difficult for people to distinguish the ground objects from complex background without professional knowledge. One commonly used solution is to exploit Generative Adversarial Networks (GAN) to translate SAR images to optical images which is able to clearly present ground objects with rich color information, i.e., SAR-to-optical image translation. Traditional GAN-based translation methods are apt to cause blurring of contour, disappearance of texture and inconsistency of color. To this end, we propose an improved conditional GAN (ICGAN) method. Compared with the basic CGAN model, the translation ability of our method is improved in the following three aspects. (1) Contour sharpness. We utilize the parallel branches to combine low-level and high-level features, and thus the image contour information is improved without the influence of noise. (2) Texture fine-grainedness. We discriminate the image using multi-scale receptive fields to enrich the local and global texture features of the image. (3) Color fidelity. We use the chromatic aberration loss which is based on Gaussian blur convolution to reduce the color gap between the generated image and the real optical image. Our method considers both the visual layer and the conceptual layer of the image to complete the SAR-to-optical image translation task. The model is able to preserve the contours and textures of the SAR image, while more closely approximates the colors of the ground truth. The experimental results show that the generated image not only has preferable results in visual effects and favorable evaluation metrics (subjective and objective), but also achieves outstanding classification accuracy, which proves the superiority of our method over the state-of-the-arts in the SAR-to-optical image translation task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003897",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Chromatic scale",
      "Color image",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Gene",
      "Image (mathematics)",
      "Image processing",
      "Image translation",
      "Messenger RNA",
      "Noise (video)",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Speckle noise",
      "Speckle pattern",
      "Translation (biology)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Xi"
      },
      {
        "surname": "Zhao",
        "given_name": "Jingyi"
      },
      {
        "surname": "Wei",
        "given_name": "Ziyu"
      },
      {
        "surname": "Wang",
        "given_name": "Nannan"
      },
      {
        "surname": "Gao",
        "given_name": "Xinbo"
      }
    ]
  },
  {
    "title": "A Unified Hierarchical XGBoost model for classifying priorities for COVID-19 vaccination campaign",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108197",
    "abstract": "The current ML approaches do not fully focus to answer a still unresolved and topical challenge, namely the prediction of priorities of COVID-19 vaccine administration. Thus, our task includes some additional methodological challenges mainly related to avoiding unwanted bias while handling categorical and ordinal data with a highly imbalanced nature. Hence, the main contribution of this study is to propose a machine learning algorithm, namely Hierarchical Priority Classification eXtreme Gradient Boosting for priority classification for COVID-19 vaccine administration using the Italian Federation of General Practitioners dataset that contains Electronic Health Record data of 17k patients. We measured the effectiveness of the proposed methodology for classifying all the priority classes while demonstrating a significant improvement with respect to the state of the art. The proposed ML approach, which is integrated into a clinical decision support system, is currently supporting General Pracitioners in assigning COVID-19 vaccine administration priorities to their assistants.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003794",
    "keywords": [
      "Artificial intelligence",
      "Boosting (machine learning)",
      "Categorical variable",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Data mining",
      "Disease",
      "Infectious disease (medical specialty)",
      "Machine learning",
      "Medicine",
      "Pathology"
    ],
    "authors": [
      {
        "surname": "Romeo",
        "given_name": "Luca"
      },
      {
        "surname": "Frontoni",
        "given_name": "Emanuele"
      }
    ]
  },
  {
    "title": "Recursive multi-model complementary deep fusion for robust salient object detection via parallel sub-networks",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108212",
    "abstract": "Fully convolutional networks have shown outstanding performance in the salient object detection (SOD) field. The state-of-the-art (SOTA) methods have a tendency to become deeper and more complex, which easily homogenize their learned deep features, resulting in a clear performance bottleneck. In sharp contrast to the conventional “deeper” schemes, this paper proposes a “wider” network architecture which consists of parallel sub-networks with totally different network architectures. In this way, those deep features obtained via these two sub-networks will exhibit large diversity, which will have large potential to be able to complement with each other. However, a large diversity may easily lead to the feature conflictions, thus we use the dense short-connections to enable a recursively interaction between the parallel sub-networks, pursuing an optimal complementary status between multi-model deep features. Finally, all these complementary multi-model deep features will be selectively fused to make high-performance salient object detections. Extensive experiments on several famous benchmarks clearly demonstrate the superior performance, good generalization, and powerful learning ability of the proposed wider framework.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003939",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Bottleneck",
      "Chemistry",
      "Complement (music)",
      "Complementation",
      "Computer science",
      "Computer security",
      "Contrast (vision)",
      "Convolutional neural network",
      "Deep learning",
      "Embedded system",
      "Feature (linguistics)",
      "Field (mathematics)",
      "Gene",
      "Generalization",
      "Key (lock)",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Network architecture",
      "Pattern recognition (psychology)",
      "Phenotype",
      "Philosophy",
      "Pure mathematics",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Zhenyu"
      },
      {
        "surname": "Li",
        "given_name": "Shuai"
      },
      {
        "surname": "Chen",
        "given_name": "Chenglizhao"
      },
      {
        "surname": "Hao",
        "given_name": "Aimin"
      },
      {
        "surname": "Qin",
        "given_name": "Hong"
      }
    ]
  },
  {
    "title": "Central hubs prediction for bio networks by directed hypergraph - GA with validation to COVID-19 PPI",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.12.015",
    "abstract": "Network structures have attracted much interest and have been rigorously studied in the past two decades. Researchers used many mathematical tools to represent these networks, and in recent days, hypergraphs play a vital role in this analysis. This paper presents an efficient technique to find the influential nodes using centrality measure of weighted directed hypergraph. Genetic Algorithm is exploited for tuning the weights of the node in the weighted directed hypergraph through which the characterization of the strength of the nodes, such as strong and weak ties by statistical measurements (mean, standard deviation, and quartiles) is identified effectively. Also, the proposed work is applied to various biological networks for identification of influential nodes and results shows the prominence the work over the existing measures. Furthermore, the technique has been applied to COVID-19 viral protein interactions. The proposed algorithm identified some critical human proteins that belong to the enzymes TMPRSS2, ACE2, and AT-II, which have a considerable role in hosting COVID-19 viral proteins and causes for various types of diseases. Hence these proteins can be targeted in drug design for an effective therapeutic against COVID-19.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521004463",
    "keywords": [
      "Biological network",
      "Biology",
      "Botany",
      "Centrality",
      "Combinatorics",
      "Computational biology",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Data mining",
      "Disease",
      "Engineering",
      "Hypergraph",
      "Identification (biology)",
      "Infectious disease (medical specialty)",
      "Mathematics",
      "Medicine",
      "Node (physics)",
      "Pathology",
      "Structural engineering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Gopalakrishnan",
        "given_name": "Sathyanarayanan"
      },
      {
        "surname": "Sridharan",
        "given_name": "Supriya"
      },
      {
        "surname": "Nayak",
        "given_name": "Soumya Ranjan"
      },
      {
        "surname": "Nayak",
        "given_name": "Janmenjoy"
      },
      {
        "surname": "Venkataraman",
        "given_name": "Swaminathan"
      }
    ]
  },
  {
    "title": "Image encryption based on logistic chaotic systems and deep autoencoder",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.11.025",
    "abstract": "In this paper, we propose a novel image encryption method based on logistic chaotic systems and deep autoencoder. In the encryption phase, first, the plaintext image is randomly scrambled by a logistic chaotic system. Then, the random scrambled image is encoded by a deep autoencoder to generate the ciphertext image. In order to obtain the ciphertext image with uniform distribution, we incorporated the uniform distribution constraint into the training of the deep autoencoder. The resulting ciphertext image contains high randomness, which is critical for an excellent image encryption algorithm. Histogram analysis, information entropy analysis, key space analysis, key sensitivity analysis, correlation analysis, and ablation experiments show that the proposed encryption algorithm can effectively resist attacks and has excellent encryption performance while providing high security.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521004220",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Chaotic",
      "Ciphertext",
      "Computer science",
      "Computer security",
      "Cryptography",
      "Encryption",
      "Histogram",
      "Image (mathematics)",
      "Key space",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Plaintext",
      "Randomness",
      "Scrambling",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Sang",
        "given_name": "Yongpeng"
      },
      {
        "surname": "Sang",
        "given_name": "Jun"
      },
      {
        "surname": "Alam",
        "given_name": "Mohammad S."
      }
    ]
  },
  {
    "title": "A directed graph convolutional neural network for edge-structured signals in link-fault detection",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.12.003",
    "abstract": "The growing interest in graph deep learning has led to a surge of research focusing on learning various characteristics of graph-structured data. Directed graphs have generally been treated as incidental to definitions on the more general class of undirected graphs. The implicit class imbalance in some graph problems also proves difficult to tackle. Moreover, a body of work has begun to grow that considers how to learn signals structured on the edges of graphs. In this paper, we propose the directed graph convolutional neural network (DGCNN), and describe a simple way to mitigate the inherent class imbalance in graphs. The model is applied to edge-structured signals from datacenter simulations using the structure of a directed linegraph to represent the second-order structure of its underlying graph. We demonstrate that the DGCNN’s improves over undirected models and other directed models by applying our model to locating link-faults in a datacenter simulation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552100430X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Directed graph",
      "Enhanced Data Rates for GSM Evolution",
      "Graph",
      "Link (geometry)",
      "Theoretical computer science",
      "Undirected graph"
    ],
    "authors": [
      {
        "surname": "Kenning",
        "given_name": "Michael"
      },
      {
        "surname": "Deng",
        "given_name": "Jingjing"
      },
      {
        "surname": "Edwards",
        "given_name": "Michael"
      },
      {
        "surname": "Xie",
        "given_name": "Xianghua"
      }
    ]
  },
  {
    "title": "Context-aware co-supervision for accurate object detection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108199",
    "abstract": "State-of-the-art object detection approaches are often composed of two stages, namely, proposing a number of regions on an image and classifying each of them into one class. Both stages share a network backbone which builds visual features in a bottom-up manner. In this paper, we advocate the importance of equipping two-stage detectors with top-down signals, in order to which provides high-level contextual cues to complement low-level features. In practice, this is implemented by adding a side path in the detection head to predict all object classes in the image, which is co-supervised by image-level semantics and requires little extra overheads. Our approach is easily applied to two popular object detection algorithms, and achieves consistent performance gain in the MS-COCO dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003812",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Class (philosophy)",
      "Complement (music)",
      "Complementation",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Detector",
      "Gene",
      "Image (mathematics)",
      "Machine learning",
      "Object (grammar)",
      "Object detection",
      "Paleontology",
      "Path (computing)",
      "Pattern recognition (psychology)",
      "Phenotype",
      "Programming language",
      "Semantics (computer science)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Junran"
      },
      {
        "surname": "Wang",
        "given_name": "Haoquan"
      },
      {
        "surname": "Yue",
        "given_name": "Shaolong"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhaoxiang"
      }
    ]
  },
  {
    "title": "Towards robust explanations for deep neural networks",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108194",
    "abstract": "Explanation methods shed light on the decision process of black-box classifiers such as deep neural networks. But their usefulness can be compromised because they are susceptible to manipulations. With this work, we aim to enhance the resilience of explanations. We develop a unified theoretical framework for deriving bounds on the maximal manipulability of a model. Based on these theoretical insights, we present three different techniques to boost robustness against manipulation: training with weight decay, smoothing activation functions, and minimizing the Hessian of the network. Our experimental results confirm the effectiveness of these approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003769",
    "keywords": [
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Black box",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Deep neural networks",
      "Gene",
      "Hessian matrix",
      "Machine learning",
      "Mathematics",
      "Robustness (evolution)",
      "Smoothing"
    ],
    "authors": [
      {
        "surname": "Dombrowski",
        "given_name": "Ann-Kathrin"
      },
      {
        "surname": "Anders",
        "given_name": "Christopher J."
      },
      {
        "surname": "Müller",
        "given_name": "Klaus-Robert"
      },
      {
        "surname": "Kessel",
        "given_name": "Pan"
      }
    ]
  },
  {
    "title": "Deep anomaly detection with self-supervised learning and adversarial training",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108234",
    "abstract": "Deep anomaly detection, which utilizes neural networks to discover anomalies, is a vital research topic in pattern recognition. With the burgeoning of inference mechanism, inference-based methods show the promising performance. However, inference-based methods have two limitations: (1) they use an adversarial training way to learn data features. Such training way fails to learn task-specific features which can be conducive to capture the difference between normal and anomaly data. (2) The structure of detection network cannot capture the marginal distributions of normal data and corresponding features, which influences on the performance of anomaly detection. To overcome these limitations, this paper proposes a deep adversarial anomaly detection (DAAD) method. Specifically, an auxiliary task with self-supervised learning is first designed to learn task-specific features. Then a deep adversarial training (DAT) model is constructed to capture marginal distributions of normal data in different spaces. In addition, a majority voting strategy is applied to obtain reliable detection results. Experimental results on image and sequence datasets show that proposed method performs significantly better than many strong baselines.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004155",
    "keywords": [
      "Adversarial system",
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Condensed matter physics",
      "Deep learning",
      "Engineering",
      "Inference",
      "Machine learning",
      "Meteorology",
      "Pattern recognition (psychology)",
      "Physics",
      "Systems engineering",
      "Task (project management)",
      "Training (meteorology)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Xianchao"
      },
      {
        "surname": "Mu",
        "given_name": "Jie"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaotong"
      },
      {
        "surname": "Liu",
        "given_name": "Han"
      },
      {
        "surname": "Zong",
        "given_name": "Linlin"
      },
      {
        "surname": "Li",
        "given_name": "Yuangang"
      }
    ]
  },
  {
    "title": "SAM-Net: Semantic probabilistic and attention mechanisms of dynamic objects for self-supervised depth and camera pose estimation in visual odometry applications",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.11.028",
    "abstract": "3D scene understanding is an essential research topic in the field of Visual Odometry (VO). VO is usually built under the assumption of a static environment, which does not always hold in real scenarios. Existing works fail to consider the dynamic objects, leading to poor performance. To tackle the aforementioned issues, we propose a self-supervised learning-based VO framework with Semantic probabilistic and Attention Mechanism, SAM-Net, which can jointly learn the single view depth, the ego motion of camera and object detection. For depth estimation, semantic probabilistic fusion mechanism is employed to detect the dynamic objects and generate the semantic probability map as a prior before feeding it to the network to generate a more refined depth map, and attention mechanism is explored to enhance perception ability in spatial and channel view. For pose estimation, we present a novel PoseNet with the atrous separable convolution to expand receptive field. And the photometric consistency loss is employed to alleviate the impact of large rotations. Intensive experiments on the KITTI dataset demonstrate that the proposed approach achieves excellent performance in terms of pose and depth accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521004256",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Convolutional neural network",
      "Mobile robot",
      "Odometry",
      "Pattern recognition (psychology)",
      "Pose",
      "Probabilistic logic",
      "Robot",
      "Visual odometry"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Binchao"
      },
      {
        "surname": "Xu",
        "given_name": "Xinying"
      },
      {
        "surname": "Ren",
        "given_name": "Jinchang"
      },
      {
        "surname": "Cheng",
        "given_name": "Lan"
      },
      {
        "surname": "Guo",
        "given_name": "Lei"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhe"
      }
    ]
  },
  {
    "title": "Online aggregation of probability forecasts with confidence",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108193",
    "abstract": "The paper presents numerical experiments and some theoretical developments in prediction with expert advice (PEA). One experiment deals with predicting electricity consumption depending on temperature and uses real data. As the pattern of dependence can change with season and time of the day, the domain naturally admits PEA formulation with experts having different “areas of expertise”. We consider the case where several competing methods produce online predictions in the form of probability distribution functions. The dissimilarity between a probability forecast and an outcome is measured by a loss function (scoring rule). A popular example of scoring rule for continuous outcomes is Continuous Ranked Probability Score ( CRPS ). In this paper the problem of combining probabilistic forecasts is considered in the PEA framework. We show that CRPS is a mixable loss function and then the time-independent upper bound for the regret of the Vovk aggregating algorithm using CRPS as a loss function can be obtained. Also, we incorporate a “smooth” version of the method of specialized experts in this scheme which allows us to combine the probabilistic predictions of the specialized experts with overlapping domains of their competence.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003496",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Evolutionary biology",
      "Function (biology)",
      "Machine learning",
      "Mathematical economics",
      "Mathematical optimization",
      "Mathematics",
      "Outcome (game theory)",
      "Probabilistic forecasting",
      "Probabilistic logic",
      "Probability distribution",
      "Regret",
      "Scoring rule",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "V’yugin",
        "given_name": "Vladimir"
      },
      {
        "surname": "Trunov",
        "given_name": "Vladimir"
      }
    ]
  },
  {
    "title": "Head pose estimation using deep neural networks and 3D point clouds",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108210",
    "abstract": "In this paper, we propose head pose estimation using deep neural networks and 3D point cloud. Unlike existing methods that either take 2D RGB image or 2D depth image as input, we adopt 3D point cloud data generated from depth to estimate 3D head poses. To further improve robustness and accuracy of head pose estimation, we classify 3D angles of head poses into 36 classes with 5 ∘ interval and predict the probability of each angle in a class based on multi-layer perceptron (MLP). While traditional iterative methods for head model construction require high computation and memory costs, the proposed method is lightweight and computationally efficient by utilizing a sampled 3D point cloud as input combined with a graph convolutional neural network (GCNN). Experimental results on Biwi Kinect Head Pose dataset show that the proposed method achieves outstanding performance in head pose estimation and outperforms state-of-the-art ones in terms of accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003915",
    "keywords": [
      "3D pose estimation",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computation",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Gene",
      "Geology",
      "Geomorphology",
      "Head (geology)",
      "Iterative closest point",
      "Pattern recognition (psychology)",
      "Perceptron",
      "Point cloud",
      "Pose",
      "RGB color model",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Yuanquan"
      },
      {
        "surname": "Jung",
        "given_name": "Cheolkon"
      },
      {
        "surname": "Chang",
        "given_name": "Yakun"
      }
    ]
  },
  {
    "title": "Personalized recommendation of collective points-of-interest with preference and context awareness",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.11.018",
    "abstract": "With the popularity of mobile devices, location-based services, such as Foursquare and Facebook, have attracted increasing attention in recent years. Users share their interests anytime, anywhere with their friends on the social network. Point-of-Interest (POI) recommendation, as one of key services on Location-Based Social Networks (LBSNs), can effectively enhance users’ experience especially when they travel in a new city. Previous studies have made great success on POI recommendation by employing geographical influence and user preference. However, it is believed that the human decision on where to visit is very complex and involves the comprehensive factors such as POI popularity, POI location, user trajectory, and time context. In this paper, we propose a collective POIs recommendation framework which leverages the individual latent preference and contextual information. Firstly, to recommend top-K initial POIs, a scoring prediction model is constructed, which considers the influence of similarity, popularity and location of POIs. Furthermore, a next POI recommendation model based on personalized transfer probability is proposed, and the initial POI recommendation is combined to calculate the user’s score on the next POI. Extensive experiments based on real datasets collected from Foursquare demonstrate the proposed framework outperforms the state-of-art ones.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521004098",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Context (archaeology)",
      "Data science",
      "Economics",
      "Geography",
      "Geometry",
      "Image (mathematics)",
      "Information retrieval",
      "Key (lock)",
      "Location-based service",
      "Mathematics",
      "Microeconomics",
      "Point (geometry)",
      "Point of interest",
      "Popularity",
      "Preference",
      "Psychology",
      "Recommender system",
      "Similarity (geometry)",
      "Social psychology",
      "Telecommunications",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Dongjin"
      },
      {
        "surname": "Yu",
        "given_name": "Ting"
      },
      {
        "surname": "Wu",
        "given_name": "Yiyu"
      },
      {
        "surname": "Liu",
        "given_name": "Chengfei"
      }
    ]
  },
  {
    "title": "Deep relational self-Attention networks for scene graph generation",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.12.013",
    "abstract": "Scene graph generation (SGG) aims to simultaneously detect objects in an image and predict relations for these detected objects. SGG is challenging that requires modeling the contextualized relationships among objects rather than only considering relationships between paired objects. Most existing approaches address this problem by using a CNN or RNN framework, which can not explicitly and effectively models the dense interactions among objects. In this paper, we exploit the attention mechanism and introduce a relational self-attention (RSA) module to simultaneously model the object and relation contexts. By stacking such RSA modules in depth, we obtain a deep relational self-attention network (RSAN), which is able to characterize complex interactions thus facilitating the understanding of object and relation semantics. Extensive experiments on the benchmark Visual Genome dataset demonstrate the effectiveness of RSAN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552100444X",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer security",
      "Data mining",
      "Exploit",
      "Geodesy",
      "Geography",
      "Graph",
      "Object (grammar)",
      "Programming language",
      "Relation (database)",
      "Rendering (computer graphics)",
      "Scene graph",
      "Semantics (computer science)",
      "Spatial relation",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Ping"
      },
      {
        "surname": "Yu",
        "given_name": "Zhou"
      },
      {
        "surname": "Zhan",
        "given_name": "Yibing"
      }
    ]
  },
  {
    "title": "Lifelong CycleGAN for continual multi-task image restoration",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.12.010",
    "abstract": "Recent years have witnessed the great success of deep learning in the applications of image restoration. However, there are still some challenging problems. First, most deep learning methods rely heavily on paired training images which are difficult to capture in the real world. Second, most existing methods are generally designed for a specific task of image restoration and they suffer from catastrophic forgetting when learn multiple tasks continually. Third, for most multi-task image restoration methods, they learn an individual network for each task and require to know the type of distortion for both training and test, which costs a lot of computational time and memory. To address the above issues, we propose a new lifelong learning framework based on CycleGAN for continual multi-task image restoration, called Lifelong CycleGAN (LCGAN), which can enhance low-light images, deblur and denoise simultaneously. The model utilizes knowledge distillation and memory replay to transfer knowledge and replay information that learned previously to alleviate forgetting. In addition, to regularize the unpaired training, we introduce the local discriminator and feature consistency constraint to preserve the color, edge and texture of input images. The proposed method can continually learn the three tasks using one network model and doesn’t need to prejudge the type of distortion, which has low time and memory requirements. Experimental results demonstrate that LCGAN can achieve better visual and numerical results across the three tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521004402",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Artificial neural network",
      "Bandwidth (computing)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Distortion (music)",
      "Economics",
      "Feature (linguistics)",
      "Forgetting",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Lifelong learning",
      "Linguistics",
      "Machine learning",
      "Management",
      "Pedagogy",
      "Philosophy",
      "Psychology",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yuping"
      },
      {
        "surname": "Nie",
        "given_name": "Xiangli"
      },
      {
        "surname": "Diao",
        "given_name": "Wenhui"
      },
      {
        "surname": "Zheng",
        "given_name": "Suiwu"
      }
    ]
  },
  {
    "title": "Salient object detection by aggregating contextual information",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.12.011",
    "abstract": "Salient object detection is a fundamental computer vision task. Deep learning techniques have been successfully applied for salient object detection, however, the saliency maps still suffer from incomplete predictions owing to the difficulty and complexity of objects. We propose a novel convolutional neural network based on aggregating contextual information that can improve the accuracy of salient object detection. In order to obtain high-level global information for fine-level feature maps, we add a pyramid pooling module to the proposed network. Moreover, building a guiding module that consists of four context-information feature modules, the algorithm can gain more effective information and enrich the details of saliency maps. Extensive experiments conducted using six benchmark datasets demonstrate that the proposed method outperforms existing state-of-the-art algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521004426",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Convolutional neural network",
      "Economics",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Geometry",
      "Linguistics",
      "Management",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pooling",
      "Pyramid (geometry)",
      "Salient",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yan"
      },
      {
        "surname": "Zhang",
        "given_name": "Yunzhou"
      },
      {
        "surname": "Liu",
        "given_name": "Shichang"
      },
      {
        "surname": "Coleman",
        "given_name": "Sonya"
      },
      {
        "surname": "Wang",
        "given_name": "Zhenyu"
      },
      {
        "surname": "Qiu",
        "given_name": "Feng"
      }
    ]
  },
  {
    "title": "Relation-aware dynamic attributed graph attention network for stocks recommendation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108119",
    "abstract": "The inherent properties of the graph structure of the financial market and the correlation attributes that actually exist in the system inspire us to introduce the concept of the graph to solve the problem of prediction and recommendation in the financial sector. In this paper, we are adhering to the idea of recommending high return ratio stocks and put forward an attributed graph attention network model based on the correlation information, with encoded timing characteristics derived from time series module and global information originating from the stacked graph neural network(GNN) based models, which we called Relation-aware Dynamic Attributed Graph Attention Network (RA-AGAT). On this basis, we have verified the practicality and applicability of the application of graph models in finance. Our innovative structure first captures the local correlation topology information and then introduce a stacked graph neural network structure to recommend Top-N return ratio of stock items. Experiments on the real China A-share market demonstrate that the RA-AGAT architecture is capable of surpassing the previously applicable methods in the prediction and recommendation of stock return ratio.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100306X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Correlation",
      "Data mining",
      "Econometrics",
      "Engineering",
      "Geometry",
      "Graph",
      "Horse",
      "Mathematics",
      "Mechanical engineering",
      "Paleontology",
      "Stock (firearms)",
      "Stock market",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Feng",
        "given_name": "Shibo"
      },
      {
        "surname": "Xu",
        "given_name": "Chen"
      },
      {
        "surname": "Zuo",
        "given_name": "Yu"
      },
      {
        "surname": "Chen",
        "given_name": "Guo"
      },
      {
        "surname": "Lin",
        "given_name": "Fan"
      },
      {
        "surname": "XiaHou",
        "given_name": "Jianbing"
      }
    ]
  },
  {
    "title": "WC-KNNG-PC: Watershed clustering based on k-nearest-neighbor graph and Pauta Criterion",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108177",
    "abstract": "Watershed clustering utilizes the concept of watershed algorithm to process clustering or cluster analyzes. The most attractive characteristic of this method is the capability to determine automatically the number of clusters from the data sets. However, in terms of the literature, the purposes of the original watershed clustering algorithm and the improved version are the detection of the clusters within two-dimensional linear data sets. In order to enable watershed clustering to deal with the dataset with multiple dimensions and nonlinear structures, we introduce k-nearest neighbor graph (KNNG), the shared nearest neighbor method and Pauta Criterion into watershed clustering to present a new watershed graph clustering with noise detection, WC-KNNG-PC. This approach first calculates a KNNG for the data sets, and then compute catchment basins (subclusters), basin immersions (connectivity between basins) and outliers. To prevent the merger of illegal subclusters, a maximum normalization stability factor, based on t-nearest neighbors and angle, MNSF, is proposed to detect the invalid basin immersions. Finally, a basin level similarity using median criterion is presented to merge the catchment basins to obtain the final clustering. Experiments on complex synthetic datasets and multidimensional real-world datasets have successfully demonstrated that the performance of the WC-KNNG-PC in clustering some various dimensional and complex datasets with heterogeneous density and diverse shapes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003642",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Fuzzy clustering",
      "Graph",
      "Machine learning",
      "Mathematics",
      "Nearest-neighbor chain algorithm",
      "Normalization (sociology)",
      "Outlier",
      "Pattern recognition (psychology)",
      "Sociology",
      "Theoretical computer science",
      "Watershed",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Xia",
        "given_name": "Jianhua"
      },
      {
        "surname": "Zhang",
        "given_name": "Jinbing"
      },
      {
        "surname": "Wang",
        "given_name": "Yang"
      },
      {
        "surname": "Han",
        "given_name": "Lixin"
      },
      {
        "surname": "Yan",
        "given_name": "Hong"
      }
    ]
  },
  {
    "title": "Data-adaptive binary neural networks for efficient object detection and recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.12.012",
    "abstract": "Binary neural networks (BNNs) are promising for computational resource-limited devices, but the degradation of feature representation capacity stifles performance due to binarization. The reason is that existing methods fail to adapt to their input when approximating full-precision features. In this paper, we introduce the DA-BNN, a data-adaptive amplitude method based on spatial and channel attention. We generate an adaptive amplitude for a better feature approximation and minimize the gap between the real-valued and 1-bit convolution. Our adaptive amplitude introduces negligible storage but can significantly enhance the performance. Extensive experiments on object detection and recognition are conducted for the comprehensive evaluation of our methods. Our method achieves 64.0% on Pascal VOC with saving of the storage and computation by 18.62 × and 15.77 × , respectively. While on ImageNet, compared to the full-precision counterpart, 11.04 × and 10.80 × saving on storage and computation are obtained with just 3% drop on accuracy, demonstrating the effectiveness on both objective detection and recognition tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521004438",
    "keywords": [
      "Algorithm",
      "Amplitude",
      "Arithmetic",
      "Artificial intelligence",
      "Artificial neural network",
      "Binary number",
      "Cognitive neuroscience of visual object recognition",
      "Computation",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Feature extraction",
      "Law",
      "Linguistics",
      "Mathematics",
      "Object detection",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Programming language",
      "Quantum mechanics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Junhe"
      },
      {
        "surname": "Xu",
        "given_name": "Sheng"
      },
      {
        "surname": "Wang",
        "given_name": "Runqi"
      },
      {
        "surname": "Zhang",
        "given_name": "Baochang"
      },
      {
        "surname": "Guo",
        "given_name": "Guodong"
      },
      {
        "surname": "Doermann",
        "given_name": "David"
      },
      {
        "surname": "Sun",
        "given_name": "Dianmin"
      }
    ]
  },
  {
    "title": "Fast transformation of discriminators into encoders using pre-trained GANs",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.11.026",
    "abstract": "A typical generative adversarial network (GAN) consists of a generator and a discriminator. Currently, finely tuned deep GANs can synthesize high-quality (HQ) images via their generators. However, the discriminator in typical GANs is only able to distinguish true or fake images in the training process. Moreover, some synthesized images from GANs are imperfect, and we can not reconstruct images via GANs. In this paper, we revisit pre-trained GANs and offer a self-supervised method to quickly transform GAN’s discriminators into encoders. We reuse parameters of the GAN’s discriminator and replace its output layer, so it can be transformed into an encoder and output reformed latent vectors. The transformation makes GAN architecture more symmetrical and allows for better performance. Based on the method, GANs can be made to reconstruct synthesized images via GAN encoders. Compared to synthesized images, these reconstructions can maintain or even attain higher quality. The code and pre-trained models are available at https://github.com/disanda/GAN-Encoder-Sym.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521004232",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Encoder",
      "Gene",
      "Operating system",
      "Pattern recognition (psychology)",
      "Speech recognition",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Cheng"
      },
      {
        "surname": "Wang",
        "given_name": "Wenmin"
      }
    ]
  },
  {
    "title": "Learning discriminative region representation for person retrieval",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108229",
    "abstract": "Region-level representation learning plays a key role in providing discriminative information for person retrieval. Current methods rely on heuristically coarse-grained region strips or directly borrow pixel-level annotations from pretrained human parsing models for region representation learning. How to learn a discriminative region representation within fine-grained segments while avoiding expensive pixel-level annotations is rarely discussed. To that end, we introduce a novel identity-guided human region segmentation (HRS) method for person retrieval. Via learning a set of distinct region bases that are consistent across a given dataset, HRS can predict informative region segments by grouping intermediate feature vectors based on their similarity to these bases. The predicted segments are iteratively refined for discriminative region representation learning. HRS enjoys two advantages: (1) HRS learns region segmentation using only identity labels, making it a much more practical solution to person retrieval. (2) By jointly learning global appearance and local granularity cues, HRS enables a comprehensive feature representation learning. We verify the effectiveness of the proposed HRS on four challenging benchmark datasets of Market1501, DukeMTMC-reID, CUHK03, and Occluded-DukeMTMC. Extensive experiments demonstrate superior performance over the state-of-the-art region-based methods. For instance, on the CUHK03-labeled dataset, the performance increases from 74.1% mAP and 76.5% rank-1 accuracy to 81.5% ( + 7.4 %) mAP and 83.2% ( + 6.7 %) rank-1 accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004106",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Combinatorics",
      "Computer science",
      "Concatenation (mathematics)",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature learning",
      "Geodesy",
      "Geography",
      "Law",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Yang"
      },
      {
        "surname": "Yu",
        "given_name": "Xiaohan"
      },
      {
        "surname": "Gao",
        "given_name": "Yongsheng"
      },
      {
        "surname": "Shen",
        "given_name": "Chunhua"
      }
    ]
  },
  {
    "title": "Chapter 1 Soft constraints in curve resolution problems",
    "journal": "Machine Learning and Pattern Recognition Methods in Chemistry from Multivariate and Data Driven Modeling",
    "year": "2023",
    "doi": "10.1016/B978-0-323-90408-7.00004-6",
    "abstract": "The reliability of the results of multivariate curve resolution (MCR) is very dependent on applied constraints. These user-defined constraints are based on prior physico-chemical knowledge about the system and can be introduced to the model either as hard constraints or soft constraints. When imposing hard constraints, no deviations are accepted from the known information, while soft constraints are not as strict, and a predefined level of deviations is allowed in this case. Practically, soft constraints are more likely to be useful as they allow for small deviations due to the effects of noise and nonideal responses. In this chapter, after an introduction on the basis of soft modeling curve resolution (SMCR) and the SMCR research hypothesis, a detailed description of all the soft constraints that have been introduced in the literature so far is given. The basic concepts and theory for applying soft constraints in the most used algorithms for applying soft constraints, namely penalty alternating least squares (P-ALS), polygon inflation, and grid search, are reviewed. Moreover, the effect of soft constraints on the range of feasible solutions in several simulated and experimental data sets is investigated.",
    "link": "https://www.sciencedirect.com/science/article/pii/B9780323904087000046",
    "keywords": [
      "Aerospace engineering",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Engineering",
      "Estimator",
      "Frame (networking)",
      "Geometry",
      "Grid",
      "Least-squares function approximation",
      "Mathematical optimization",
      "Mathematics",
      "Polygon (computer graphics)",
      "Range (aeronautics)",
      "Resolution (logic)",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Gemperline",
        "given_name": "Paul J."
      },
      {
        "surname": "Lakeh",
        "given_name": "Mahsa AKbari"
      }
    ]
  },
  {
    "title": "Chapter 2 Multivariate predictive modeling and validation",
    "journal": "Machine Learning and Pattern Recognition Methods in Chemistry from Multivariate and Data Driven Modeling",
    "year": "2023",
    "doi": "10.1016/B978-0-323-90408-7.00001-0",
    "abstract": "Many problems in chemistry involve the prediction of one or more qualitative or quantitative properties based on the experimental data. Examples of such problems involve, for instance, the possibility of predicting protein or lipid content in food matrices based on NIR spectra or of diagnosing the onset of a disease through the MS or NMR analysis of serum samples. In the former case, the property to be predicted is of a quantitative nature, while in the latter, it is discrete (qualitative). This chapter presents the chemometric strategies most commonly used to formulate predictive models, i.e., models that relate one or more dependent variables Y (qualitative or quantitative) to a set of independent variables X.",
    "link": "https://www.sciencedirect.com/science/article/pii/B9780323904087000010",
    "keywords": [
      "Biological system",
      "Biology",
      "Computer science",
      "Data mining",
      "Data set",
      "Econometrics",
      "Epistemology",
      "Mathematics",
      "Multivariate analysis",
      "Multivariate statistics",
      "Philosophy",
      "Predictive modelling",
      "Programming language",
      "Property (philosophy)",
      "Qualitative analysis",
      "Qualitative research",
      "Set (abstract data type)",
      "Social science",
      "Sociology",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Biancolillo",
        "given_name": "Alessandra"
      },
      {
        "surname": "Marini",
        "given_name": "Federico"
      }
    ]
  },
  {
    "title": "Preface",
    "journal": "Machine Learning and Pattern Recognition Methods in Chemistry from Multivariate and Data Driven Modeling",
    "year": "2023",
    "doi": "10.1016/B978-0-323-90408-7.09988-3",
    "abstract": "Unknown",
    "link": "https://www.sciencedirect.com/science/article/pii/B9780323904087099883",
    "keywords": [
      "Materials science"
    ],
    "authors": [
      {
        "surname": "Ghasemi",
        "given_name": "Jahan B."
      }
    ]
  },
  {
    "title": "Chapter 4 Tuning the apparent thermodynamic parameters of chemical systems",
    "journal": "Machine Learning and Pattern Recognition Methods in Chemistry from Multivariate and Data Driven Modeling",
    "year": "2023",
    "doi": "10.1016/B978-0-323-90408-7.00008-3",
    "abstract": "For many important equilibrium systems, it is customary to use a simplified apparent, instead of a complete but rather complex, equilibrium system. The resulting apparent equilibrium constants are altered or controlled by changes in the chemical system, usually modifying the total concentration of auxiliary reagents, but the variation in other parameters such the intensity of light in photo-controlled reactions, types and concentration of solvents or composition of mixed solvents, the ionic strength of the chemical media, etc. is also possible.",
    "link": "https://www.sciencedirect.com/science/article/pii/B9780323904087000083",
    "keywords": [
      "Aqueous solution",
      "Chemical equilibrium",
      "Chemistry",
      "Computer science",
      "Constant (computer programming)",
      "Dynamic equilibrium",
      "Equilibrium constant",
      "Ion",
      "Ionic bonding",
      "Ionic strength",
      "Organic chemistry",
      "Physical chemistry",
      "Physics",
      "Programming language",
      "Reagent",
      "Thermodynamic equilibrium",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Abdollahi",
        "given_name": "Hamid"
      },
      {
        "surname": "Karimvand",
        "given_name": "Somaiyeh Khodadadi"
      },
      {
        "surname": "Zade",
        "given_name": "Somaye Vali"
      }
    ]
  },
  {
    "title": "Chapter 7 Uniqueness in resolving multivariate chemical data",
    "journal": "Machine Learning and Pattern Recognition Methods in Chemistry from Multivariate and Data Driven Modeling",
    "year": "2023",
    "doi": "10.1016/B978-0-323-90408-7.00006-X",
    "abstract": "The most significant challenge of the chemometricians who use the multivariate curve resolution methods is to reduce the range of feasible solutions by applying proper information in the form of constraints. The appropriate constraints can narrow down the band of feasible solutions, even in most favorable cases, can produce unique solutions, and thus they can improve the accuracy of the results of the resolving methods. In this chapter the circumstances and the rules that govern to achieve the unique real solutions will be explained. A collected knowledge in this subject is completely rare in literature.",
    "link": "https://www.sciencedirect.com/science/article/pii/B978032390408700006X",
    "keywords": [
      "Aerospace engineering",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Econometrics",
      "Engineering",
      "Geography",
      "High resolution",
      "Library science",
      "Low resolution",
      "Machine learning",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Multivariate statistics",
      "Range (aeronautics)",
      "Remote sensing",
      "Resolution (logic)",
      "Subject (documents)",
      "Uniqueness"
    ],
    "authors": [
      {
        "surname": "Abdollahi",
        "given_name": "Hamid"
      },
      {
        "surname": "Karimvand",
        "given_name": "Somaiyeh Khodadadi"
      },
      {
        "surname": "Zade",
        "given_name": "Somaye Vali"
      }
    ]
  },
  {
    "title": "Chapter 6 Autoencoders in generative modeling, feature extraction, regression, and classification",
    "journal": "Machine Learning and Pattern Recognition Methods in Chemistry from Multivariate and Data Driven Modeling",
    "year": "2023",
    "doi": "10.1016/B978-0-323-90408-7.00007-1",
    "abstract": "Autoencoders are a family of neural network algorithms with a wide range of applications. Potential uses include but are not limited to feature extraction, sampling, denoising, dimensionality reduction, and generative modeling. Generally speaking, an autoencoder consists of two parts. An Encoder function tries to copy input to output, and the Decoder attempts to reconstruct the input from the output. But, the point is that copying does not take place completely. Forcing some restrictions on encoder and decoder functions, they tend to extract just necessary and salient features. Autoencoders are robust and data-friendly, so they are suitable for big datasets. Autoencoders have been used successfully as generative models for de novo molecular design, chemometric data analysis, robust classification on molecular biology datasets, dimension reduction mechanism for various imaging datasets, etc. This chapter introduces mathematical formulations of well-known autoencoders, training procedures, and some state-of-the-art uses.",
    "link": "https://www.sciencedirect.com/science/article/pii/B9780323904087000071",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Computer science",
      "Curse of dimensionality",
      "Dimensionality reduction",
      "Feature extraction",
      "Generative grammar",
      "Machine learning",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Namazi",
        "given_name": "Mohsen"
      },
      {
        "surname": "Karimi-Jafari",
        "given_name": "Mohammad Hossein"
      },
      {
        "surname": "Qassemi",
        "given_name": "Farzad"
      },
      {
        "surname": "Ghasemi",
        "given_name": "Jahan B."
      }
    ]
  },
  {
    "title": "Chapter 5 The analytical/measurement sources of multivariate errors. A case study: Detecting microplastics in sand",
    "journal": "Machine Learning and Pattern Recognition Methods in Chemistry from Multivariate and Data Driven Modeling",
    "year": "2023",
    "doi": "10.1016/B978-0-323-90408-7.00003-4",
    "abstract": "Recently, the scientific literature has proposed near infrared hyperspectral imaging (NIR-HSI) as a plausible analytical choice to detect microplastics of diverse size (ranging in size from 1 to 1000 µm) in natural samples in real-time. In particular, several factors make NIR-HSI an attractive possibility: the ability to collect precise spectral information on a surface at a suitable spatial level; the utility of appropriate chemometric methods; and the speed of measurement (several orders of magnitude faster than, e.g., Raman imaging or Mid-Infrared). Nevertheless, there are important analytical concerns surrounding NIR-HSI that require a deep understanding of the interactions between NIR radiation, microplastics, and the matrices in which microplastics are located. This book chapter gives some answers to the abovementioned questions and sets the basis for understanding the extent to which HSI-NIR and classification methods can be used to detect microplastics.",
    "link": "https://www.sciencedirect.com/science/article/pii/B9780323904087000034",
    "keywords": [
      "Biological system",
      "Biology",
      "Chemistry",
      "Environmental chemistry",
      "Environmental science",
      "Geography",
      "Hyperspectral imaging",
      "Microplastics",
      "Remote sensing"
    ],
    "authors": [
      {
        "surname": "Goyetche",
        "given_name": "Reaha"
      },
      {
        "surname": "Amigo",
        "given_name": "José Manuel"
      },
      {
        "surname": "Kortazar",
        "given_name": "Leire"
      }
    ]
  },
  {
    "title": "Chapter 3 Multivariate pattern recognition by machine learning methods",
    "journal": "Machine Learning and Pattern Recognition Methods in Chemistry from Multivariate and Data Driven Modeling",
    "year": "2023",
    "doi": "10.1016/B978-0-323-90408-7.00002-2",
    "abstract": "In pattern recognition, the goal is to identify patterns and regularities from data with minimal intervention. Finding the patterns and regularities is done using machine learning algorithms. In other words, pattern recognition is the engineering application, while machine learning origin back in computer science. Machine learning algorithms map the input data to the output which these algorithms have two main steps: the training step and the inference step. In the training step, the training data is utilized to learn a model, and in the inference step, the learned model is used to map unseen data to the output space. Machine learning algorithms are divided into four categories from the view of the available training data: (1) supervised learning, (2) semi-supervised learning, (3) weakly supervised learning, and (4) unsupervised learning. In supervised learning, all the available training data are fully annotated (labeled) compared to unsupervised learning, where none of the training data is labeled. In semi-supervised learning, some portion of training data is labeled, and a large portion is unlabeled. If the whole training data is not fully labeled (i.e., partially labeled), then it is weakly supervised learning. In this chapter, we focus on supervised learning.",
    "link": "https://www.sciencedirect.com/science/article/pii/B9780323904087000022",
    "keywords": [
      "Active learning (machine learning)",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Inference",
      "Instance-based learning",
      "Machine learning",
      "Online machine learning",
      "Pattern recognition (psychology)",
      "Semi-supervised learning",
      "Supervised learning",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Razzaghi",
        "given_name": "Parvin"
      },
      {
        "surname": "Abbasi",
        "given_name": "Karim"
      },
      {
        "surname": "Ghasemi",
        "given_name": "Jahan B."
      }
    ]
  },
  {
    "title": "GraphXCOVID: Explainable deep graph diffusion pseudo-Labelling for identifying COVID-19 on chest X-rays",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108274",
    "abstract": "Can one learn to diagnose COVID-19 under extreme minimal supervision? Since the outbreak of the novel COVID-19 there has been a rush for developing automatic techniques for expert-level disease identification on Chest X-ray data. In particular, the use of deep supervised learning has become the go-to paradigm. However, the performance of such models is heavily dependent on the availability of a large and representative labelled dataset. The creation of which is a heavily expensive and time consuming task, and especially imposes a great challenge for a novel disease. Semi-supervised learning has shown the ability to match the incredible performance of supervised models whilst requiring a small fraction of the labelled examples. This makes the semi supervised paradigm an attractive option for identifying COVID-19. In this work, we introduce a graph based deep semi-supervised framework for classifying COVID-19 from chest X-rays. Our framework introduces an optimisation model for graph diffusion that reinforces the natural relation among the tiny labelled set and the vast unlabelled data. We then connect the diffusion prediction output as pseudo-labels that are used in an iterative scheme in a deep net. We demonstrate, through our experiments, that our model is able to outperform the current leading supervised model with a tiny fraction of the labelled examples. Finally, we provide attention maps to accommodate the radiologist’s mental model, better fitting their perceptual and cognitive abilities. These visualisation aims to assist the radiologist in judging whether the diagnostic is correct or not, and in consequence to accelerate the decision.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004544",
    "keywords": [],
    "authors": [
      {
        "surname": "Aviles-Rivero",
        "given_name": "Angelica I."
      },
      {
        "surname": "Sellars",
        "given_name": "Philip"
      },
      {
        "surname": "Schönlieb",
        "given_name": "Carola-Bibiane"
      },
      {
        "surname": "Papadakis",
        "given_name": "Nicolas"
      }
    ]
  },
  {
    "title": "A cascade reconstruction model with generalization ability evaluation for anomaly detection in videos",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108336",
    "abstract": "Anomaly detection plays an important role in surveillance video since it maintains public safety efficiently with low cost. In current works, anomaly detection methods based on reconstruction with deep learning has been extensively studied for the powerful representation capacity. These methods use convolutional neural networks to learn model for describing normality at training and detect anomalies according to reconstruction error at testing. However, excessive representation capacity of neural networks will also bring disadvantages to anomaly detection when it is powerful enough to reconstruct abnormal information. For this reason, we proposed two solutions; firstly, a cascade model which conducts pixel reconstruction followed by optical flow prediction is designed. The conversion from frame to optical flow learns the correlation between object appearance and motion, while pixel reconstruction enlarges the optical flow prediction error to conduct effective anomaly detection. Secondly, the generalization ability evaluation based on pseudo-anomaly is proposed, which is used to evaluate the ability of model to represent anomaly, thus selecting an optimal model for anomaly detection. The selected model achieves AUC 88.9% on Avenue, 82.6% on Ped1, 97.7% on Ped2, and 70.7% on ShanghaiTech datasets. Extensive ablation experiments have verified the effectiveness of our method. Code will be released at https://github.com/Xia-Chen/Cascade_Reconstruction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005161",
    "keywords": [
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Cascade",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Computer vision",
      "Condensed matter physics",
      "Convolutional neural network",
      "Frame (networking)",
      "Generalization",
      "Image (mathematics)",
      "Law",
      "Mathematical analysis",
      "Mathematics",
      "Optical flow",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zhong",
        "given_name": "Yuanhong"
      },
      {
        "surname": "Chen",
        "given_name": "Xia"
      },
      {
        "surname": "Jiang",
        "given_name": "Jinyang"
      },
      {
        "surname": "Ren",
        "given_name": "Fan"
      }
    ]
  },
  {
    "title": "Contextual ensemble network for semantic segmentation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108290",
    "abstract": "Recently, exploring features from different layers in fully convolutional networks (FCNs) has gained substantial attention to capture context information for semantic segmentation. This paper presents a novel encoder-decoder architecture, called contextual ensemble network (CENet), for semantic segmentation, where the contextual cues are aggregated via densely usampling the convolutional features of deep layer to the shallow deconvolutional layers. The proposed CENet is trained in terms of end-to-end segmentation to match the resolution of input image, and allows us to fully explore contextual features through ensemble of dense deconvolutions. We evaluate our CENet on two widely-used semantic segmentation datasets: PASCAL VOC 2012 and CityScapes. The experimental results demonstrate our CENet achieves superior performance with respect to recent state-of-the-art results. Furthermore, we also evaluate CENet on MS COCO dataset and ISBI 2012 dataset for the task of instance segmentation and biological segmentation, respectively. The experimental results show that CENet obtains promising results on these two datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004702",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Convolutional neural network",
      "Economics",
      "Encoder",
      "Image segmentation",
      "Machine learning",
      "Management",
      "Operating system",
      "Paleontology",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Scale-space segmentation",
      "Segmentation",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Quan"
      },
      {
        "surname": "Wu",
        "given_name": "Xiaofu"
      },
      {
        "surname": "Zhang",
        "given_name": "Suofei"
      },
      {
        "surname": "Kang",
        "given_name": "Bin"
      },
      {
        "surname": "Ge",
        "given_name": "Zongyuan"
      },
      {
        "surname": "Jan Latecki",
        "given_name": "Longin"
      }
    ]
  },
  {
    "title": "High dynamic range imaging via gradient-aware context aggregation network",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108342",
    "abstract": "Obtaining a high dynamic range (HDR) image from multiple low dynamic range images with different exposures is an important step in various computer vision tasks. One of the ongoing challenges in the field is to generate HDR images without ghosting artifacts. Motivated by an observation that such artifacts are particularly noticeable in the gradient domain, in this paper, we propose an HDR imaging approach that aggregates the information from multiple LDR images with guidance from image gradient domain. The proposed method generates artifact-free images by integrating the image gradient information and the image context information in the pixel domain. The context information in a large area helps to reconstruct the contents contaminated by saturation and misalignments. Specifically, an additional gradient stream and the supervision in the gradient domain are applied to incorporate the gradient information in HDR imaging. To use the context information captured from a large area while preserving spatial resolution, we adopt dilated convolutions to extract multi-scale features with rich context information. Moreover, we build a new dataset containing 40 groups of real-world images from diverse scenes with ground truth to validate the proposed model. The samples in the proposed dataset include more challenging moving objects inducing misalignments. Extensive experimental results demonstrate that our proposed model outperforms previous methods on different datasets in terms of both quantitative measure and visual perception quality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005227",
    "keywords": [
      "Artifact (error)",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Dynamic range",
      "Ghosting",
      "Ground truth",
      "High dynamic range",
      "Image resolution",
      "Paleontology",
      "Spatial contextual awareness"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Qingsen"
      },
      {
        "surname": "Gong",
        "given_name": "Dong"
      },
      {
        "surname": "Shi",
        "given_name": "Javen Qinfeng"
      },
      {
        "surname": "den Hengel",
        "given_name": "Anton van"
      },
      {
        "surname": "Sun",
        "given_name": "Jinqiu"
      },
      {
        "surname": "Zhu",
        "given_name": "Yu"
      },
      {
        "surname": "Zhang",
        "given_name": "Yanning"
      }
    ]
  },
  {
    "title": "Adaptive Decision Forest: An incremental machine learning framework",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108345",
    "abstract": "In this study, we present an incremental machine learning framework called Adaptive Decision Forest (ADF), which produces a decision forest to classify new records. Based on our two novel theorems, we introduce a new splitting strategy called iSAT, which allows ADF to classify new records even if they are associated with previously unseen classes. ADF is capable of identifying and handling concept drift; it, however, does not forget previously gained knowledge. Moreover, ADF is capable of handling big data if the data can be divided into batches. We evaluate ADF on nine publicly available natural datasets and one synthetic dataset, and compare the performance of ADF against the performance of eight state-of-the-art techniques. We also examine the effectiveness of ADF in some challenging situations. Our experimental results, including statistical sign test and Nemenyi test analyses, indicate a clear superiority of the proposed framework over the state-of-the-art techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005252",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Big data",
      "Computer science",
      "Concept drift",
      "Data mining",
      "Data set",
      "Data stream mining",
      "Incremental learning",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Programming language",
      "Set (abstract data type)",
      "Sign (mathematics)",
      "State (computer science)"
    ],
    "authors": [
      {
        "surname": "Rahman",
        "given_name": "Md Geaur"
      },
      {
        "surname": "Islam",
        "given_name": "Md Zahidul"
      }
    ]
  },
  {
    "title": "Edge detection with attention: From global view to local focus",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.01.006",
    "abstract": "Recently, edge detection models with convolutional neural networks (CNNs) have achieved significant advances, mostly by convolutional pyramid features and multi-path aggregation to generate accurate boundaries. However, it is still challenging for them when facing complex structure, weak context and dense boundaries. In this paper, we propose an Edge Attention Network (EdgeAtNet) from the viewpoint of attention mechanism. EdgeAtNet is derived from the richer convolutional features (RCF) basic architecture. On the low-level features, a global view attention block is inserted to the bottleneck to capture the long-range dependency of edge features, and on the high-level features, a local focus attention is designed for crisp boundary representation. Using ResNet101 as the backbone, we achieve state-of-the-art (SOTA) performance on several benchmarks. When evaluated on the well-known BSDS500 benchmark dataset, EdgeAtNet achieves the Optimal Dataset Scale (ODS) F-measure of 0.825. On the NYUDv2 and BIPED benchmark datasets, EdgeAtNet obtaines ODS F-measure of 0.764 and 0.868, respectively, and it outperforms the existing most methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552200006X",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Block (permutation group theory)",
      "Bottleneck",
      "Computer science",
      "Context (archaeology)",
      "Convolutional neural network",
      "Data mining",
      "Dependency (UML)",
      "Embedded system",
      "Enhanced Data Rates for GSM Evolution",
      "Feature (linguistics)",
      "Focus (optics)",
      "Geodesy",
      "Geography",
      "Geometry",
      "Law",
      "Linguistics",
      "Mathematics",
      "Measure (data warehouse)",
      "Optics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Pyramid (geometry)",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Huajun"
      },
      {
        "surname": "Yang",
        "given_name": "Zuyuan"
      },
      {
        "surname": "Zhang",
        "given_name": "Haofeng"
      },
      {
        "surname": "Wang",
        "given_name": "Cailing"
      }
    ]
  },
  {
    "title": "Well-calibrated confidence measures for multi-label text classification with a large number of labels",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108271",
    "abstract": "We extend our previous work on Inductive Conformal Prediction (ICP) for multi-label text classification and present a novel approach for addressing the computational inefficiency of the Label Powerset (LP) ICP, arrising when dealing with a high number of unique labels. We present experimental results using the original and the proposed efficient LP-ICP on two English and one Czech language data-sets. Specifically, we apply the LP-ICP on three deep Artificial Neural Network (ANN) classifiers of two types: one based on contextualised (bert) and two on non-contextualised (word2vec) word-embeddings. In the LP-ICP setting we assign nonconformity scores to label-sets from which the corresponding p -values and prediction-sets are determined. Our approach deals with the increased computational burden of LP by eliminating from consideration a significant number of label-sets that will surely have p -values below the specified significance level. This reduces dramatically the computational complexity of the approach while fully respecting the standard CP guarantees. Our experimental results show that the contextualised-based classifier surpasses the non-contextualised-based ones and obtains state-of-the-art performance for all data-sets examined. The good performance of the underlying classifiers is carried on to their ICP counterparts without any significant accuracy loss, but with the added benefits of ICP, i.e. the confidence information encapsulated in the prediction sets. We experimentally demonstrate that the resulting prediction sets can be tight enough to be practically useful even though the set of all possible label-sets contains more than 1 e + 16 combinations. Additionally, the empirical error rates of the obtained prediction-sets confirm that our outputs are well-calibrated.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004519",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Confidence interval",
      "Mathematics",
      "Multi-label classification",
      "Pattern recognition (psychology)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Maltoudoglou",
        "given_name": "Lysimachos"
      },
      {
        "surname": "Paisios",
        "given_name": "Andreas"
      },
      {
        "surname": "Lenc",
        "given_name": "Ladislav"
      },
      {
        "surname": "Martínek",
        "given_name": "Jiří"
      },
      {
        "surname": "Král",
        "given_name": "Pavel"
      },
      {
        "surname": "Papadopoulos",
        "given_name": "Harris"
      }
    ]
  },
  {
    "title": "Leveraging local and global descriptors in parallel to search correspondences for visual localization",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108344",
    "abstract": "Visual localization to compute 6DoF camera pose from a given image has wide applications. Both local and global descriptors are crucial for visual localization. Most of the existing visual localization methods adopt a two-stage strategy: image retrieval first is performed by global descriptors, and then 2D-3D correspondences are made by local descriptors from 2D query image points and its nearest neighbor candidates which are the 3D points visible by these retrieved images. The above two stages are serially performed in these methods. However, due to the fact that 3D points obtained from the retrieval feedback are only rely on global descriptors, these methods cannot fully take the advantages of both local and global descriptors. In this paper, we propose a novel parallel search framework, which fully leverages advantages of both local and global descriptors to get nearest neighbor candidates of a 2D query image point. Specifically, besides using deep learning based global descriptors, we also utilize local descriptors to construct random tree structures for obtaining nearest neighbor candidates of the 2D query image point. We propose a new probability model and a new deep learning based local descriptor when constructing the random trees. In addition, a weighted Hamming regularization term to keep discriminativeness after binarization is given in loss function for the proposed local descriptor. The loss function co-trains both real and binary local descriptors of which the results are integrated into the random trees. Experiments on challenging benchmarks show that the proposed localization method can significantly improve the robustness and accuracy compared with the ones which get nearest neighbor candidates of a query local feature just based on either local or global descriptors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005240",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Gene",
      "Hamming distance",
      "Histogram",
      "Image (mathematics)",
      "Image retrieval",
      "Local binary patterns",
      "Pattern recognition (psychology)",
      "Point cloud",
      "Regularization (linguistics)",
      "Robustness (evolution)",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Pengju"
      },
      {
        "surname": "Zhang",
        "given_name": "Chaofan"
      },
      {
        "surname": "Liu",
        "given_name": "Bingxi"
      },
      {
        "surname": "Wu",
        "given_name": "Yihong"
      }
    ]
  },
  {
    "title": "Graph-based stock correlation and prediction for high-frequency trading systems",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108209",
    "abstract": "In this paper, we have implemented a high-frequency quantitative system that can obtain stable returns for the Chinese A-share market, which has been running for more than 3 months (from March 27, 2020 to June 30, 2020) with the expected results. A number of rules and barriers exist in the Chinese A-share market such as trading restrictions and high fees, as well as scarce and expensive hedging tools. It is difficult to achieve stable absolute returns in such a market. Stock correlation analysis and price prediction play an important role to achieve any profitable trading. The portfolio management and subsequent trading decisions highly depend on the results of stock correlation analysis and price prediction. However, it is nontrivial to analyze and predict any stocks, being time-varying and affected by unlimited factors in a given market. Traditional methods only take some certain factors into consideration but ignore others that may be changed dynamically. In this paper, we propose a novel machine learning model named Graph Attention Long Short-Term Memory (GALSTM) to learn the correlations between stocks and predict their future prices automatically. First, a multi-Hawkes Process is used to initial a correlation graph between stocks. This procedure provides a good training start as the multi-Hawkes Processes will be studied on the most saint feature fluctuations with any correlations being statistically significant. Then an attention-based LSTM is built to learn the weighting matrix underlying the dynamic graph. In addition, we also build matching data process plus portfolio management modules to form a complete system. The proposed GALSTM enables us to expand the scope of stock selection under the premise of controlling risks with limited hedging tools in the A-share market, thereby effectively increasing high-frequency excess returns. We then construct a long and short positions combination, select long positions in the A shares of the entire market, and use stock index futures to short. With GALSTM model, the products managed by our fully automatic quantitative trading system achieved an absolute annual return rate of 44.71% and the standard deviation of daily returns is only 0.42% in three months of operation. Only 1 week loss in 13 weeks of running time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003903",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Econometrics",
      "Economics",
      "Engineering",
      "Financial economics",
      "Graph",
      "Horse",
      "Mechanical engineering",
      "Medicine",
      "Paleontology",
      "Portfolio",
      "Radiology",
      "Stock (firearms)",
      "Stock market",
      "Theoretical computer science",
      "Trading strategy",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Yin",
        "given_name": "Tao"
      },
      {
        "surname": "Liu",
        "given_name": "Chenzhengyi"
      },
      {
        "surname": "Ding",
        "given_name": "Fangyu"
      },
      {
        "surname": "Feng",
        "given_name": "Ziming"
      },
      {
        "surname": "Yuan",
        "given_name": "Bo"
      },
      {
        "surname": "Zhang",
        "given_name": "Ning"
      }
    ]
  },
  {
    "title": "Zero-shot learning via a specific rank-controlled semantic autoencoder",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108237",
    "abstract": "Existing embedding zero-shot learning models usually learn a projection function from the visual feature space to the semantic embedding space, e.g. attribute space or word vector space. However, the projection learned based on seen samples may not generalize well to unseen classes, which is known as the projection domain shift problem in ZSL. To address this issue, we propose a method named Low-rank Semantic Autoencoder (LSA) to consider the low-rank structure of seen samples to maintain the sparse feature of reconstruction error, which can further improve zero-shot learning capability. Moreover, to obtain a more robust projection for unseen classes, we propose a Specific Rank-controlled Semantic Autoencoder (SRSA) to accurately control of the projection’s rank. Extensive experiments on six benchmarks demonstrate the superiority of the proposed models over most existing embedding ZSL models under the standard zero-shot setting and the more realistic generalized zero-shot setting.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004180",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Autoencoder",
      "Combinatorics",
      "Computer science",
      "Deep learning",
      "Embedding",
      "Feature (linguistics)",
      "Feature vector",
      "Linguistics",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Projection (relational algebra)",
      "Rank (graph theory)",
      "Space (punctuation)",
      "Zero (linguistics)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yang"
      },
      {
        "surname": "Gao",
        "given_name": "Xinbo"
      },
      {
        "surname": "Han",
        "given_name": "Jungong"
      },
      {
        "surname": "Liu",
        "given_name": "Li"
      },
      {
        "surname": "Shao",
        "given_name": "Ling"
      }
    ]
  },
  {
    "title": "Graph matching based on fast normalized cut and multiplicative update mapping",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108228",
    "abstract": "Point correspondence is a fundamental problem in pattern recognition and computer vision, which can be tackled by graph matching. Since graph matching is basically an NP-complete problem, some approximate methods are proposed to solve it. Continuous relaxation offers an effective approximate method for graph matching problem. However, the discrete constraint is not taken into consideration in the optimization step. In this paper, a fast normalized cut based graph matching method is proposed, where the discrete constraint is introduced into the optimization step. Specifically, first a semidefinite positive affinity matrix based form objective function is constructed by introducing a regularization term which is related to the discrete constraint. Then the fast normalized cut algorithm is utilized to find the continuous solution. Last, the discrete solution of graph matching is obtained by a multiplicative update algorithm. Experiments on both synthetic points and real-world images validate the effectiveness of the proposed method by comparing it with the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100409X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Graph",
      "Matching (statistics)",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Multiplicative function",
      "Regularization (linguistics)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Jing"
      },
      {
        "surname": "Yang",
        "given_name": "Xu"
      },
      {
        "surname": "Zhou",
        "given_name": "Zhang-Bing"
      },
      {
        "surname": "Liu",
        "given_name": "Zhi-Yong"
      }
    ]
  },
  {
    "title": "Application of optimal clustering and metric learning to patch-based anomaly detection",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.01.017",
    "abstract": "Anomaly detection in computer vision is a process of identifying observations or events deviated from the norm. Pixel- and patch-based approaches for locating abnormal regions suffer from a tremendously expensive computational complexity due to repeated calculations inherently required. The approach introduced in this letter aims to reducing such computational burden by employing an optimal clustering method for features extracted from a deep network-based encoder, and a metric learning which provides a way to bias the clusters found by the clustering algorithm. A set of loss functions is defined and used for training the encoder. After the training steps, the clustering result obtained from the training dataset is saved and used in the inference phase by measuring distances between the features of the input data and the clusters. The proposed approach has been applied to a patch-based anomaly detection, Patch-SVDD, to clarify the effectiveness of the idea. The experiment carried out with the implementation on MVTec-AD dataset results in improved detection speed by 10 ∼ 35% and better detection accuracy for many image classes, as well. In addition, the ablation study conducted proves the validity of the ideas introduced in the proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000216",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Autoencoder",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Economics",
      "Inference",
      "Machine learning",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Ahn",
        "given_name": "Jae-Young"
      },
      {
        "surname": "Kim",
        "given_name": "Gyeonghwan"
      }
    ]
  },
  {
    "title": "An unsupervised approach of colonic polyp segmentation using adaptive markov random fields",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.12.014",
    "abstract": "In this article, an unsupervised image segmentation method is proposed to segment out polyps in endoscopic images. Based on the skeleton of an over-segmented image, an adaptive Markov Random Field (MRF)-based framework is employed. The polyps or abnormal regions show markedly different texture and color characteristics in contrast to normal tissues. In our method, the endoscope images are first over-segmented into superpixels. For final refinement, Local binary pattern (LBP) and color features are used in the adaptive MRF. The experimental results show that the proposed method can perfectly localize polyp regions, and it can give a mean dice value of 60.77% in a test set of 612 images taken from 29 different video sequences. Deep learning models require a huge number of training images for tweaking of network parameters. In this context, it is sometimes challenging to deploy a deep model for unseen medical datasets having many segmentation challenges, like complex background, specularity, non-uniform illuminations, etc. Also, manual labeling of a huge number of images is another challenge. That is why we adopted an unsupervised approach in this paper.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521004451",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Image segmentation",
      "Markov random field",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Sasmal",
        "given_name": "Pradipta"
      },
      {
        "surname": "Bhuyan",
        "given_name": "M.K."
      },
      {
        "surname": "Dutta",
        "given_name": "Soumayan"
      },
      {
        "surname": "Iwahori",
        "given_name": "Yuji"
      }
    ]
  },
  {
    "title": "Discriminative feature generation for classification of imbalanced data",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108302",
    "abstract": "The data imbalance problem is a frequent bottleneck in the classification performance of neural networks. In this paper, we propose a novel supervised discriminative feature generation (DFG) method for a minority class dataset. DFG is based on the modified structure of a generative adversarial network consisting of four independent networks: generator, discriminator, feature extractor, and classifier. To augment the selected discriminative features of the minority class data by adopting an attention mechanism, the generator for the class-imbalanced target task is trained, and the feature extractor and classifier are regularized using the pre-trained features from a large source data. The experimental results show that the DFG generator enhances the augmentation of the label-preserved and diverse features, and the classification results are significantly improved on the target task. The feature generation model can contribute greatly to the development of data augmentation methods through discriminative feature generation and supervised attention methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004829",
    "keywords": [],
    "authors": [
      {
        "surname": "Suh",
        "given_name": "Sungho"
      },
      {
        "surname": "Lukowicz",
        "given_name": "Paul"
      },
      {
        "surname": "Lee",
        "given_name": "Yong Oh"
      }
    ]
  },
  {
    "title": "A black-box adversarial attack for poisoning clustering",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108306",
    "abstract": "Clustering algorithms play a fundamental role as tools in decision-making and sensible automation processes. Due to the widespread use of these applications, a robustness analysis of this family of algorithms against adversarial noise has become imperative. To the best of our knowledge, however, only a few works have currently addressed this problem. In an attempt to fill this gap, in this work, we propose a black-box adversarial attack for crafting adversarial samples to test the robustness of clustering algorithms. We formulate the problem as a constrained minimization program, general in its structure and customizable by the attacker according to her capability constraints. We do not assume any information about the internal structure of the victim clustering algorithm, and we allow the attacker to query it as a service only. In the absence of any derivative information, we perform the optimization with a custom approach inspired by the Abstract Genetic Algorithm (AGA). In the experimental part, we demonstrate the sensibility of different single and ensemble clustering algorithms against our crafted adversarial samples on different scenarios. Furthermore, we perform a comparison of our algorithm with a state-of-the-art approach showing that we are able to reach or even outperform its performance. Finally, to highlight the general nature of the generated noise, we show that our attacks are transferable even against supervised algorithms such as SVMs, random forests and neural networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004866",
    "keywords": [
      "Adversarial system",
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Gene",
      "Machine learning",
      "Minification",
      "Programming language",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Cinà",
        "given_name": "Antonio Emanuele"
      },
      {
        "surname": "Torcinovich",
        "given_name": "Alessandro"
      },
      {
        "surname": "Pelillo",
        "given_name": "Marcello"
      }
    ]
  },
  {
    "title": "Estimating Tukey depth using incremental quantile estimators",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108339",
    "abstract": "Measures of distance or how data points are positioned relative to each other are fundamental in pattern recognition. The concept of depth measures how deep an arbitrary point is positioned in a dataset, and is an interesting concept in this regard. However, while this concept has received a lot of attention in the statistical literature, its application within pattern recognition is still limited. To increase the applicability of the depth concept in pattern recognition, we address the well-known computational challenges associated with the depth concept, by suggesting to estimate depth using incremental quantile estimators. The suggested algorithm can not only estimate depth when the dataset is known in advance, but can also track depth for dynamically varying data streams by using recursive updates. The tracking ability of the algorithm was demonstrated based on a real-life application associated with detecting changes in human activity from real-time accelerometer observations. Given the flexibility of the suggested approach, it can detect virtually any kind of changes in the distributional patterns of the observations, and thus outperforms detection approaches based on the Mahalanobis distance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005197",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Composite material",
      "Computer science",
      "Data mining",
      "Estimator",
      "Geometry",
      "Materials science",
      "Mathematics",
      "Measure (data warehouse)",
      "Pedagogy",
      "Point (geometry)",
      "Psychology",
      "Quantile",
      "Range (aeronautics)",
      "Statistics",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Hammer",
        "given_name": "Hugo L."
      },
      {
        "surname": "Yazidi",
        "given_name": "Anis"
      },
      {
        "surname": "Rue",
        "given_name": "Håvard"
      }
    ]
  },
  {
    "title": "Unsupervised descriptor selection based meta-learning networks for few-shot classification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108304",
    "abstract": "Meta-learning aims to train a classifier on collections of tasks, such that it can recognize new classes given few samples from each. However, current approaches encounter overfitting and poor generalization since the internal representation learning is obstructed by backgrounds and noises in limited samples. To alleviate those issues, we propose the Unsupervised Descriptor Selection (UDS) to tackle few-shot learning tasks. Specifically, a descriptor selection module is proposed to localize and select semantic meaningful regions in feature maps without supervision. The selected features are then mapped into novel vectors by a task-related aggregation module to enhance internal representations. With a simple network structure, UDS makes adaptation between tasks more efficient, and improves the performance in few-shot learning. Extensive experiments with various backbones are conducted on Caltech-UCSD Bird and miniImageNet, indicate that UDS achieves the comparable performance to state-of-the-art methods, and improves the performance of prior meta-learning methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004842",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Classifier (UML)",
      "Computer science",
      "Economics",
      "Feature learning",
      "Feature selection",
      "Generalization",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Multi-task learning",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Selection (genetic algorithm)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Zhengping"
      },
      {
        "surname": "Li",
        "given_name": "Zijun"
      },
      {
        "surname": "Wang",
        "given_name": "Xueyu"
      },
      {
        "surname": "Zheng",
        "given_name": "Saiyue"
      }
    ]
  },
  {
    "title": "JSPNet: Learning joint semantic & instance segmentation of point clouds via feature self-similarity and cross-task probability",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108250",
    "abstract": "In this paper, we propose a novel method named JSPNet, to segment 3D point cloud in semantic and instance simultaneously. First, we analyze the problem in addressing joint semantic and instance segmentation, including the common ground of cooperation of two tasks, conflict of two tasks, quadruplet relation between semantic and instance distributions, and ignorance of existing works. Then we introduce our method to reinforce mutual cooperation and alleviate the essential conflict. Our method has a shared encoder and two decoders to address two tasks. Specifically, to maintain discriminative features and characterize inconspicuous content, a similarity-based feature fusion module is designed to locate the inconspicuous area in the feature of current branch and then select related features from the other branch to compensate for the unclear content. Furthermore, given the salient semantic feature and the salient instance feature, a cross-task probability-based feature fusion module is developed to establish the probabilistic correlation between semantic and instance features. This module could transform features from one branch and further fuse them with the other branch by multiplying probabilistic matrix. Experimental results on a large-scale 3D indoor point cloud dataset S3DIS and a part-segmentation dataset ShapeNet have demonstrated the superiority of our method over existing state-of-the-arts in both semantic and instance segmentation. The proposed method outperforms PointNet with 12% and 26% improvements and outperforms ASIS with 2.7% and 4.3% improvements in terms of mIoU and mPre. Code of this work has been made available at https://github.com/Chenfeng1271/JSPNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004301",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Discriminative model",
      "Economics",
      "Encoder",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Linguistics",
      "Management",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point cloud",
      "Probabilistic logic",
      "Programming language",
      "Segmentation",
      "Semantic feature",
      "Semantics (computer science)",
      "Similarity (geometry)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Feng"
      },
      {
        "surname": "Wu",
        "given_name": "Fei"
      },
      {
        "surname": "Gao",
        "given_name": "Guangwei"
      },
      {
        "surname": "Ji",
        "given_name": "Yimu"
      },
      {
        "surname": "Xu",
        "given_name": "Jing"
      },
      {
        "surname": "Jiang",
        "given_name": "Guo-Ping"
      },
      {
        "surname": "Jing",
        "given_name": "Xiao-Yuan"
      }
    ]
  },
  {
    "title": "Two-step domain adaptation for underwater image enhancement",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108324",
    "abstract": "In recent years, underwater image enhancement methods based on deep learning have achieved remarkable results. Since the images obtained in complex underwater scenarios lack a ground truth, these algorithms mainly train models on underwater images synthesized from in-air images. Synthesized underwater images are different from real-world underwater images; this difference leads to the limited generalizability of the training model when enhancing real-world underwater images. In this work, we present an underwater image enhancement method that does not require training on synthetic underwater images and eliminates the dependence on underwater ground-truth images. Specifically, a novel domain adaptation framework for real-world underwater image enhancement inspired by transfer learning is presented; it transfers in-air image dehazing to real-world underwater image enhancement. The experimental results on different real-world underwater scenes indicate that the proposed method produces visually satisfactory results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005045",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Generalizability theory",
      "Geology",
      "Ground truth",
      "Image (mathematics)",
      "Mathematics",
      "Oceanography",
      "Statistics",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Qun"
      },
      {
        "surname": "Zhang",
        "given_name": "Yunfeng"
      },
      {
        "surname": "Bao",
        "given_name": "Fangxun"
      },
      {
        "surname": "Zhao",
        "given_name": "Xiuyang"
      },
      {
        "surname": "Zhang",
        "given_name": "Caiming"
      },
      {
        "surname": "Liu",
        "given_name": "Peide"
      }
    ]
  },
  {
    "title": "Bayesian compression for dynamically expandable networks",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108260",
    "abstract": "This paper develops Bayesian Compression for Dynamically Expandable Network (BCDEN), which can learn a compact model structure with preserving the accuracy in a continual learning scenarios. Dynamically Expandable Network (DEN) is efficiently trained by performing selective retraining, dynamically expands network capacity with only the necessary number of units, and effectively prevents semantic drift by duplicating and timestamping units in an online manner. Overcoming conventional DEN only giving point estimates, we providing the Bayesian inference under the principle framework. We validate our BCDEN on multiple public datasets under continual learning setting, on which it can outperform existing continual learning methods on a variety of tasks, and with the state-of-the-art compression results, while still maintaining comparable performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004404",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bayesian inference",
      "Bayesian network",
      "Bayesian probability",
      "Business",
      "Composite material",
      "Compression (physics)",
      "Computer science",
      "Geometry",
      "Inference",
      "International trade",
      "Machine learning",
      "Materials science",
      "Mathematics",
      "Point (geometry)",
      "Retraining",
      "State (computer science)",
      "Variety (cybernetics)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Yang"
      },
      {
        "surname": "Chen",
        "given_name": "Bo"
      },
      {
        "surname": "Liu",
        "given_name": "Hongwei"
      }
    ]
  },
  {
    "title": "Divide well to merge better: A novel clustering algorithm",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108305",
    "abstract": "In this paper, a novel non-parametric clustering algorithm which is based on the concept of divide-and-merge is proposed. The proposed algorithm is based on two primary phases, after data cleaning: (i) the Division phase and (ii) the Merging phase. In the initial phase of division, the data is divided into an optimized number of small sub-clusters utilizing all the dimensions of the data. In the second phase of merging, the small sub-clusters obtained as a result of division are merged according to an advanced statistical metric to form the actual clusters in the data. The proposed algorithm has the following merits: (i) ability to discover both convex and non-convex shaped clusters, (ii) ability to discover clusters different in densities, (iii) ability to detect and remove outliers/noise in the data (iv) easily tunable or fixed hyperparameters (v) and its usability for high dimensional data. The proposed algorithm is extensively tested on 20 benchmark datasets including both, the synthetic and the real datasets and is found better/competing to the existing state-of-the-art parametric and non-parametric clustering algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004854",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Determining the number of clusters in a data set",
      "Division (mathematics)",
      "Hyperparameter",
      "Information retrieval",
      "Mathematics",
      "Merge (version control)",
      "Outlier",
      "Parametric statistics",
      "Pattern recognition (psychology)",
      "Single-linkage clustering",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Rehman",
        "given_name": "Atiq Ur"
      },
      {
        "surname": "Belhaouari",
        "given_name": "Samir Brahim"
      }
    ]
  },
  {
    "title": "Coarse-to-fine pseudo supervision guided meta-task optimization for few-shot object classification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108296",
    "abstract": "Few-Shot Learning (FSL) is a challenging and practical learning pattern, aiming to solve a target task which has only a few labeled examples. Currently, the field of FSL has made great progress, but largely in the supervised setting, where a large auxiliary labeled dataset is required for offline training. However, the unsupervised FSL (UFSL) problem where the auxiliary dataset is fully unlabeled has been seldom investigated despite of its significant value. This paper focuses on the more general and challenging UFSL problem and presents a novel method named Coarse-to-Fine Pseudo Supervision-guided Meta-Learning (C2FPS-ML) for unsupervised few-shot object classification. It first obtains prior knowledge from an unlabeled auxiliary dataset during unsupervised meta-training, and then use the prior knowledge to assist the downstream few-shot classification task. Coarse-to-Fine Pseudo Supervisions in C2FPS-ML aim to optimize meta-task sampling process in unsupervised meta-training stage which is one of the dominant factors for improving the performance of meta-learning based FSL algorithms. Human can learn new concepts progressively or hierarchically following the coarse-to-fine manners. By simulating this human’s behaviour, we develop two versions of C2FPS-ML for two different scenarios: one is natural object dataset and another one is other kinds of dataset (e.g., handwritten character dataset). For natural object dataset scenario, we propose to exploit the potential hierarchical semantics of the unlabeled auxiliary dataset to build a tree-like structure of visual concepts. For another scenario, progressive pseudo supervision is obtained by forming clusters in different similarity aspects and is represented by a pyramid-like structure. The obtained structure is applied as the supervision to construct meta-tasks in meta-training stage, and prior knowledge from the unlabeled auxiliary dataset is learned from the coarse-grained level to the fine-grained level. The proposed method sets the new state of the art on the gold-standard miniImageNet and achieves remarkable results on Omniglot while simultaneously increases efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004763",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer security",
      "Economics",
      "Exploit",
      "Machine learning",
      "Management",
      "Meta learning (computer science)",
      "Object (grammar)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Supervised learning",
      "Task (project management)",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Cui",
        "given_name": "Yawen"
      },
      {
        "surname": "Liao",
        "given_name": "Qing"
      },
      {
        "surname": "Hu",
        "given_name": "Dewen"
      },
      {
        "surname": "An",
        "given_name": "Wei"
      },
      {
        "surname": "Liu",
        "given_name": "Li"
      }
    ]
  },
  {
    "title": "Towards automatic threat detection: A survey of advances of deep learning within X-ray security imaging",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108245",
    "abstract": "X-ray security screening is widely used to maintain aviation/transport security, and its significance poses a particular interest in automated screening systems. This paper aims to review computerised X-ray security imaging algorithms by taxonomising the field into conventional machine learning and contemporary deep learning applications. The first part briefly discusses the classical machine learning approaches utilised within X-ray security imaging, while the latter part thoroughly investigates the use of modern deep learning algorithms. The proposed taxonomy sub-categorises the use of deep learning approaches into supervised and unsupervised learning, with a particular focus on object classification, detection, segmentation and anomaly detection tasks. The paper further explores well-established X-ray datasets and provides a performance benchmark. Based on the current and future trends in deep learning, the paper finally presents a discussion and future directions for X-ray security imagery.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004258",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Deep learning",
      "Field (mathematics)",
      "Focus (optics)",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Mathematics",
      "Object detection",
      "Optics",
      "Physics",
      "Pure mathematics",
      "Segmentation",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Akcay",
        "given_name": "Samet"
      },
      {
        "surname": "Breckon",
        "given_name": "Toby"
      }
    ]
  },
  {
    "title": "Efficient COVID-19 testing via contextual model based compressive sensing",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108253",
    "abstract": "The COVID-19 pandemic is threatening billions of people's life all over the world. As of March 6, 2021, covid-19 has confirmed in 115,653,459 people worldwide. It has also a devastating effect on businesses and social activities. Since there is still no definite cure for this disease, extensive testing is the most critical issue to determine the trend of illness, appropriate medical treatment, and make social distancing policies. Besides, testing more people in a shorter time helps to contain the contagion. The PCR-based methods are the most popular tests which take about an hour to make the output result. Obviously, it makes the number of tests highly limited and consequently, hurts the efficiency of pandemic control. In this paper, we propose a new approach to identify affected individuals with a considerably reduced No. of tests. Intuitively, saving time and resources is the main advantage of our approach. We use contextual information to make a graph-based model to be used in model-based compressive sensing (CS). Our proposed model makes the testing with fewer tests required compared to traditional testing methods and even group testing. We embed contextual information such as age, underlying disease, symptoms (i.e. cough, fever, fatigue, loss of consciousness), and social contacts into a graph-based model. This model is used in model-based CS to minimize the required test. We take advantage of Discrete Graph Signal Processing on Graph (DSPG) to generate the model. Our contextual model makes CS more efficient in both the number of samples and the recovery quality. Moreover, it can be applied in the case that group testing is not applicable due to its severe dependency on sparsity. Experimental results show that the overall testing speed (individuals per test ratio) increases more than 15 times compared to the individual testing with the error of less than 5% which is dramatically lower than that of traditional compressive sensing.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004337",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Disease",
      "Environmental health",
      "Epidemic model",
      "Graph",
      "Infectious disease (medical specialty)",
      "Machine learning",
      "Medicine",
      "Pandemic",
      "Pathology",
      "Population",
      "Social distance",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Hasaninasab",
        "given_name": "Mehdi"
      },
      {
        "surname": "Khansari",
        "given_name": "Mohammad"
      }
    ]
  },
  {
    "title": "Compressed feature vector-based effective object recognition model in detection of COVID-19",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.12.016",
    "abstract": "To better understand the structure of the COVID-19, and to improve the recognition speed, an effective recognition model based on compressed feature vector is proposed. Object recognition plays an important role in computer vison aera. To improve the recognition accuracy, most recent approaches always adopt a set of complicated hand-craft feature vectors and build the complex classifiers. Although such approaches achieve the favourable performance on recognition accuracy, they are inefficient. To raise the recognition speed without decreasing the accuracy loss, this paper proposed an efficient recognition modeltrained witha kind of compressed feature vectors. Firstly, we propose a kind of compressed feature vector based on the theory of compressive sensing. A sparse matrix is adopted to compress feature vector from very high dimensions to very low dimensions, which reduces the computation complexity and saves enough information for model training and predicting. Moreover, to improve the inference efficiency during the classification stage, an efficient recognition model is built by a novel optimization approach, which reduces the support vectors of kernel-support vector machine (kernel SVM). The SVM model is established with whether the subject is infected with the COVID-19 as the dependent variable, and the age, gender, nationality, and other factors as independent variables. The proposed approach iteratively builds a compact set of the support vectors from the original kernel SVM, and then the new generated model achieves approximate recognition accuracy with the original kernel SVM. Additionally, with the reduction of support vectors, the recognition time of new generated is greatly improved. Finally, the COVID-19 patients have specific epidemiological characteristics, and the SVM recognition model has strong fitting ability. From the extensive experimental results conducted on two datasets, the proposed object recognition model achieves favourable performance not only on recognition accuracy but also on recognition speed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521004475",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Feature (linguistics)",
      "Feature vector",
      "Kernel (algebra)",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Chao"
      },
      {
        "surname": "Mao",
        "given_name": "Jinhong"
      },
      {
        "surname": "Liu",
        "given_name": "Xinzhi"
      },
      {
        "surname": "Tan",
        "given_name": "Yi"
      },
      {
        "surname": "Abaido",
        "given_name": "Ghada M"
      },
      {
        "surname": "Alsayed",
        "given_name": "Hamdy"
      }
    ]
  },
  {
    "title": "Learning panoptic segmentation through feature discriminability",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108240",
    "abstract": "Panoptic segmentation has attracted increasing attention as a joint task of semantic and instance segmentation. However, previous works have not noticed that the different requirements for semantic and instance segmentation can lead to conflict of feature discriminability. Instance segmentation mainly focuses on the central area of each instance in things regions, while semantic segmentation focuses on the whole region of a specific class. To resolve it, we propose: 1) a Dual-FPN framework which separates the shared Feature Pyramid Network (FPN) in previous works to reduce the conflict of receptive field and meet different requirements of the two tasks; 2) a Region Refinement Module which leverages the prediction of semantic segmentation to refine the result of instance segmentation and resolves the conflict between the things regions and the stuff regions. Experimental results on Cityscapes dataset and Mapillary Vistas dataset show that our proposed method can improve the result of both things and stuff and obtain state-of-the-art performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004210",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Economics",
      "Feature (linguistics)",
      "Field (mathematics)",
      "Geometry",
      "Image segmentation",
      "Linguistics",
      "Management",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pure mathematics",
      "Pyramid (geometry)",
      "Scale-space segmentation",
      "Segmentation",
      "Segmentation-based object categorization",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Chu",
        "given_name": "Tao"
      },
      {
        "surname": "Cai",
        "given_name": "Wenjie"
      },
      {
        "surname": "Liu",
        "given_name": "Qiong"
      }
    ]
  },
  {
    "title": "Gradient-Aligned convolution neural network",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108354",
    "abstract": "Although Convolution Neural Networks (CNN) have achieved great success in many applications of computer vision in recent years, rotation invariance is still a difficult problem for CNN. Especially for some images, the content can appear in the image at any angle of rotation, such as medical images, microscopic images, remote sensing images and astronomical images. In this paper, we propose a novel convolution operation, called Gradient-Aligned Convolution (GAConv), which can help CNN achieve rotation invariance by replacing vanilla convolutions in CNN. GAConv is implemented with a prior pixel-level gradient alignment operation before regular convolution. With GAConv, Gradient-Aligned CNN (GACNN) can achieve rotation invariance without any data augmentation, feature-map augmentation, and filter enrichment. In GACNN, rotation invariance does not learn from the training set, but bases on the network model. Different from the vanilla CNN, GACNN will output invariant results for all rotated versions of an object, no matter whether the network is trained or not. This means that we only need to train the network with one canonical version of the object and all other rotated versions of this object should be recognized with the same accuracy. Classification experiments have been conducted to evaluate GACNN compared with some rotation invariant approaches. GACNN achieved the best results on the 360 ∘ rotated test set of MNIST-rotation, Plankton-sub-rotation, and Galaxy Zoo 2.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005343",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Invariant (physics)",
      "MNIST database",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Rotation (mathematics)"
    ],
    "authors": [
      {
        "surname": "Hao",
        "given_name": "You"
      },
      {
        "surname": "Hu",
        "given_name": "Ping"
      },
      {
        "surname": "Li",
        "given_name": "Shirui"
      },
      {
        "surname": "Udupa",
        "given_name": "Jayaram K."
      },
      {
        "surname": "Tong",
        "given_name": "Yubing"
      },
      {
        "surname": "Li",
        "given_name": "Hua"
      }
    ]
  },
  {
    "title": "Coarse-to-fine-grained method for image splicing region detection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108347",
    "abstract": "In this study, we aim to improve the accuracy of image splicing detection. We propose a progressive image splicing detection method that can detect the position and shape of spliced region. Because image splicing is likely to destroy or change the consistent correlation pattern introduced by color filter array (CFA) interpolation process, we first used a covariance matrix to reconstruct the R, G and B channels of image and utilized the inconsistencies of the CFA interpolation pattern to extract forensics feature. Then, these forensics features were used to perform coarse-grained detection, and texture strength features were used to perform fine-grained detection. Finally, an edge smoothing method was applied to realize precise localization. As compared to the state-of-the-art CFA-based image splicing detection methods, the proposed method has a high-level detection accuracy and strong robustness against content-preserving manipulations and JPEG compression.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005276",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Edge detection",
      "Feature (linguistics)",
      "Gene",
      "Image (mathematics)",
      "Image processing",
      "Image scaling",
      "Interpolation (computer graphics)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Robustness (evolution)",
      "Smoothing"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xiaofeng"
      },
      {
        "surname": "Wang",
        "given_name": "Yan"
      },
      {
        "surname": "Lei",
        "given_name": "Jinjin"
      },
      {
        "surname": "Li",
        "given_name": "Bin"
      },
      {
        "surname": "Wang",
        "given_name": "Qin"
      },
      {
        "surname": "Xue",
        "given_name": "Jianru"
      }
    ]
  },
  {
    "title": "DeepStreamOS: Fast open-Set classification for convolutional neural networks",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.01.011",
    "abstract": "Convolutional Neural Networks (CNNs) can achieve state of the art results for visual recognition problems when the train and test data distributions are the same and when all classes in the test set are present in the training data. This is not representative of the real-world where the data evolves; existing classes change and new classes emerge. A traditional neural network only has the capacity to label instances with classes it has been trained on and cannot identify unknown classes. This can be of serious consequence in safety critical systems. The research field of Open-Set Classification provides potential solutions to overcome the identification of unknown classes in deep neural networks. In safety-critical systems, the speed with which unknown classes can be identified is also essential. Our system, termed DeepStreamOS, brings together the use of deep neural network activations with a stream-based outlier detection method for fast identification of instances that belong to unknown classes. DeepStreamOS uses all layers of a CNN to get a trajectory of the activations and applies a stream-based analysis method to determine if an instance belongs to an unknown class. We use CIFAR-10 and Fashion-MNIST datasets, withholding classes to apply as unknown data on VGG16 and MobileNet deep neural networks. Our system is compared with leading open-set classification methods: OpenMax and EVM. We show that DeepStreamOS outperforms OpenMax and EVM in most open-set classification scenarios and by a large margin on speed in all scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000186",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Botany",
      "Class (philosophy)",
      "Computer science",
      "Contextual image classification",
      "Convolutional neural network",
      "Data mining",
      "Data set",
      "Deep learning",
      "Discrete mathematics",
      "Field (mathematics)",
      "Identification (biology)",
      "Image (mathematics)",
      "MNIST database",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematics",
      "Open set",
      "Outlier",
      "Pattern recognition (psychology)",
      "Programming language",
      "Pure mathematics",
      "Set (abstract data type)",
      "Test set"
    ],
    "authors": [
      {
        "surname": "Chambers",
        "given_name": "Lorraine"
      },
      {
        "surname": "Gaber",
        "given_name": "Mohamed Medhat"
      }
    ]
  },
  {
    "title": "Option compatible reward inverse reinforcement learning",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.01.016",
    "abstract": "Reinforcement learning in complex environments is a challenging problem. In particular, the success of reinforcement learning algorithms depends on a well-designed reward function. Inverse reinforcement learning (IRL) solves the problem of recovering reward functions from expert demonstrations. In this paper, we solve a hierarchical inverse reinforcement learning problem within the options framework, which allows us to utilize intrinsic motivation of the expert demonstrations. A gradient method for parametrized options is used to deduce a defining equation for the Q-feature space, which leads to a reward feature space. Using a second-order optimality condition for option parameters, an optimal reward function is selected. Experimental results in both discrete and continuous domains confirm that our recovered rewards provide a solution to the IRL problem using temporal abstraction, which in turn are effective in accelerating transfer learning tasks. We also show that our method is robust to noises contained in expert demonstrations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000241",
    "keywords": [
      "Abstraction",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Engineering",
      "Epistemology",
      "Evolutionary biology",
      "Feature (linguistics)",
      "Function (biology)",
      "Geometry",
      "Inverse",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Philosophy",
      "Reinforcement",
      "Reinforcement learning",
      "Space (punctuation)",
      "Structural engineering"
    ],
    "authors": [
      {
        "surname": "Hwang",
        "given_name": "Rakhoon"
      },
      {
        "surname": "Lee",
        "given_name": "Hanjin"
      },
      {
        "surname": "Hwang",
        "given_name": "Hyung Ju"
      }
    ]
  },
  {
    "title": "Multinomial random forest",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108331",
    "abstract": "Despite the impressive performance of random forests (RF), its theoretical properties have not been thoroughly understood. In this paper, we propose a novel RF framework, dubbed multinomial random forest (MRF), to analyze its consistency and privacy-preservation. Instead of deterministic greedy split rule or with simple randomness, the MRF adopts two impurity-based multinomial distributions to randomly select a splitting feature and a splitting value, respectively. Theoretically, we prove the consistency of MRF and analyze its privacy-preservation within the framework of differential privacy. We also demonstrate with multiple datasets that its performance is on par with the standard RF. To the best of our knowledge, MRF is the first consistent RF variant that has comparable performance to the standard RF. The code is available at https://github.com/jiawangbai/Multinomial-Random-Forest.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005112",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Code (set theory)",
      "Computer science",
      "Consistency (knowledge bases)",
      "Data mining",
      "Feature (linguistics)",
      "Linguistics",
      "Mathematics",
      "Multinomial distribution",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Random forest",
      "Randomness",
      "Set (abstract data type)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Bai",
        "given_name": "Jiawang"
      },
      {
        "surname": "Li",
        "given_name": "Yiming"
      },
      {
        "surname": "Li",
        "given_name": "Jiawei"
      },
      {
        "surname": "Yang",
        "given_name": "Xue"
      },
      {
        "surname": "Jiang",
        "given_name": "Yong"
      },
      {
        "surname": "Xia",
        "given_name": "Shu-Tao"
      }
    ]
  },
  {
    "title": "Protect, show, attend and tell: Empowering image captioning models with ownership protection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108285",
    "abstract": "By and large, existing Intellectual Property (IP) protection on deep neural networks typically i) focus on image classification task only, and ii) follow a standard digital watermarking framework that was conventionally used to protect the ownership of multimedia and video content. This paper demonstrates that the current digital watermarking framework is insufficient to protect image captioning tasks that are often regarded as one of the frontiers AI problems. As a remedy, this paper studies and proposes two different embedding schemes in the hidden memory state of a recurrent neural network to protect the image captioning model. From empirical points, we prove that a forged key will yield an unusable image captioning model, defeating the purpose of infringement. To the best of our knowledge, this work is the first to propose ownership protection on image captioning task. Also, extensive experiments show that the proposed method does not compromise the original image captioning performance on all common captioning metrics on Flickr30k and MS-COCO datasets, and at the same time it is able to withstand both removal and ambiguity attacks. Code is available at https://github.com/jianhanlim/ipr-imagecaptioning",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004659",
    "keywords": [
      "Ambiguity",
      "Artificial intelligence",
      "Closed captioning",
      "Code (set theory)",
      "Computer science",
      "Computer security",
      "Digital watermarking",
      "Engineering",
      "Image (mathematics)",
      "Key (lock)",
      "Natural language processing",
      "Programming language",
      "Set (abstract data type)",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Lim",
        "given_name": "Jian Han"
      },
      {
        "surname": "Chan",
        "given_name": "Chee Seng"
      },
      {
        "surname": "Ng",
        "given_name": "Kam Woh"
      },
      {
        "surname": "Fan",
        "given_name": "Lixin"
      },
      {
        "surname": "Yang",
        "given_name": "Qiang"
      }
    ]
  },
  {
    "title": "Regularizing deep networks with label geometry for accurate object localization on small training datasets",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.01.004",
    "abstract": "Localization is a critical subtask in object detection, which is closely related to spatial information of objects. Most current detectors simply rely on the fitting ability of deep neural networks to regress towards numerical targets such as coordinates of object boxes. Training deep networks for sufficient fitting ability requires a large number of annotations that are expensive to obtain. In this work, we fully exploit limited annotations by extracting label geometry to improve localization performance on small datasets. We generate distance transform of bounding box edges according to localization labels, with which we supervise intermediate outputs of networks pixel by pixel to reconstruct object geometry for localization. Distance transform is sensitive to box edges and provides geometric gradients flowing into boundaries. We learn such gradients to enhance geometric-aware features through a coupled training with regression, and use it to refine regressed boxes in an evolutionary manner in inference. Extensive experiments are implemented to demonstrate the effectiveness of our method. Our method can be applied in applications that required human-machine interaction, such as the driver-assistance system in autonomous driving, by providing accurate detections to assist humans in making better decisions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000046",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Bounding overwatch",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Deep learning",
      "Deep neural networks",
      "Exploit",
      "Image (mathematics)",
      "Inference",
      "Minimum bounding box",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Pixel"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xiaolian"
      },
      {
        "surname": "Hu",
        "given_name": "Xiyuan"
      },
      {
        "surname": "Chen",
        "given_name": "Chen"
      },
      {
        "surname": "Peng",
        "given_name": "Silong"
      }
    ]
  },
  {
    "title": "A new bayesian Poisson denoising algorithm based on nonlocal means and stochastic distances",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108363",
    "abstract": "Poisson noise is the main cause of degradation of many imaging modalities. However, many of the proposed methods for reducing noise in images lack a formal approach. Our work develops a new, general, formal and computationally efficient bayesian Poisson denoising algorithm, based on the Nonlocal Means framework and replacing the euclidean distance by stochastic distances, which are more appropriate for the denoising problem. It takes advantage of the conjugacy of Poisson and gamma distributions to obtain its computational efficiency. When dealing with low dose CT images, the algorithm operates on the sinogram, modeling the rates of the Poisson noise by the Gamma distribution. Based on the Bayesian formulation and the conjugacy property, the likelihood follows the Poisson distribution, while the a posteriori distribution is also described by the Gamma distribution. The derived algorithm is applied to simulated and real low-dose CT images and compared to several algorithms proposed in the literature, with competitive results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005434",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Bayesian probability",
      "Computer science",
      "Detector",
      "Image (mathematics)",
      "Mathematical optimization",
      "Mathematics",
      "Maximum a posteriori estimation",
      "Maximum likelihood",
      "Noise (video)",
      "Noise reduction",
      "Pattern recognition (psychology)",
      "Poisson distribution",
      "Shot noise",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Evangelista",
        "given_name": "Rodrigo C."
      },
      {
        "surname": "Salvadeo",
        "given_name": "Denis H.P."
      },
      {
        "surname": "Mascarenhas",
        "given_name": "Nelson D.A."
      }
    ]
  },
  {
    "title": "Atom correlation based graph propagation for scene graph generation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108300",
    "abstract": "Long-tailed distribution in the dataset is one of the major problems of the scene graph generation task. Previous methods attempt to alleviate this by introducing human commonsense knowledge in the form of statistical correlations between object pairs. However, the reasoning path they used is usually composable and the prior knowledge they employed is generally image-specific, making the knowledge learning less flexible, stable and holistic. In this paper, we propose Atom Correlation Based Graph Propagation (AC-GP) for the scene graph generation task. Specifically, diverse atom correlations between objects and their relationships are explored by separating relationships to form new semantic nodes and decomposing the compound reasoning paths. Based on these atom correlations, the knowledge graphs are introduced for the feature enhancement by information propagating in the global category space. By exploiting atom correlations, the introduced prior knowledge can be more common and easy to learn. Moreover, propagating the knowledge in the global category space enables the model aware of more comprehensive and holistic knowledge. As a result, the model capacity and stability can be effectively improved to mine infrequent and missed relationships. Experimental results on two benchmark datasets: Visual Relation Detection (VRD) and Visual Genome (VG) show the superiority of the proposed AC-GP over strong baseline methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004805",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Correlation",
      "Economics",
      "Geometry",
      "Graph",
      "Knowledge graph",
      "Machine learning",
      "Management",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Rendering (computer graphics)",
      "Scene graph",
      "Task (project management)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Bingqian"
      },
      {
        "surname": "Zhu",
        "given_name": "Yi"
      },
      {
        "surname": "Liang",
        "given_name": "Xiaodan"
      }
    ]
  },
  {
    "title": "(AD) 2 : Adversarial domain adaptation to defense with adversarial perturbation removal",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108303",
    "abstract": "Deep Neural Networks (DNNs) are demonstrated to be vulnerable to adversarial examples, which are crafted by adding adversarial perturbations to the legitimate examples. To address this issue, some defense methods have been proposed. Among them, the adversarial training (AT) is a popular method to improve the robustness of DNNs. However, theory analysis has shown that in the adversarial training framework, the improvement of the robustness will lead to a decline of standard accuracy. In this paper, we propose a modularized defense framework, namely Adversarial Domain Adaptation to Defense ((AD) 2 ). Different from all adversarial training methods, (AD) 2 detects adversarial example using a generative algorithm and applies the adversarial domain adaptation method to remove adversarial perturbation. Experimental results show that (AD) 2 is effective to remove the adversarial perturbation and mitigate the odds between the robustness and standard accuracy for DNNs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004830",
    "keywords": [
      "Adversarial system",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Classifier (UML)",
      "Computer science",
      "Deep neural networks",
      "Domain adaptation",
      "Gene",
      "Logistic regression",
      "Machine learning",
      "Odds",
      "Perturbation (astronomy)",
      "Physics",
      "Quantum mechanics",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Han",
        "given_name": "Keji"
      },
      {
        "surname": "Xia",
        "given_name": "Bin"
      },
      {
        "surname": "Li",
        "given_name": "Yun"
      }
    ]
  },
  {
    "title": "Deep and interpretable regression models for ordinal outcomes",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108263",
    "abstract": "Outcomes with a natural order commonly occur in prediction problems and often the available input data are a mixture of complex data like images and tabular predictors. Deep Learning (DL) models are state-of-the-art for image classification tasks but frequently treat ordinal outcomes as unordered and lack interpretability. In contrast, classical ordinal regression models consider the outcome’s order and yield interpretable predictor effects but are limited to tabular data. We present ordinal neural network transformation models (ontrams), which unite DL with classical ordinal regression approaches. ontrams are a special case of transformation models and trade off flexibility and interpretability by additively decomposing the transformation function into terms for image and tabular data using jointly trained neural networks. The performance of the most flexible ontram is by definition equivalent to a standard multi-class DL model trained with cross-entropy while being faster in training when facing ordinal outcomes. Lastly, we discuss how to interpret model components for both tabular and image data on two publicly available datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100443X",
    "keywords": [],
    "authors": [
      {
        "surname": "Kook",
        "given_name": "Lucas"
      },
      {
        "surname": "Herzog",
        "given_name": "Lisa"
      },
      {
        "surname": "Hothorn",
        "given_name": "Torsten"
      },
      {
        "surname": "Dürr",
        "given_name": "Oliver"
      },
      {
        "surname": "Sick",
        "given_name": "Beate"
      }
    ]
  },
  {
    "title": "The UU-test for statistical modeling of unimodal data",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108272",
    "abstract": "Deciding on the unimodality of a dataset is an important problem in data analysis and statistical modeling. It allows to obtain knowledge about the structure of the dataset, i.e. whether data points have been generated by a probability distribution with a single or more than one peaks. Such knowledge is very useful for several data analysis problems, such as for deciding on the number of clusters and determining unimodal projections. We propose a technique called UU-test (Unimodal Uniform test) to decide on the unimodality of a one-dimensional dataset. The method operates on the empirical cumulative density function (ecdf) of the dataset. It attempts to build a piecewise linear approximation of the ecdf that is unimodal and models the data sufficiently in the sense that the data corresponding to each linear segment follows the uniform distribution. A unique feature of this approach is that in the case of unimodality, it also provides a statistical model of the data in the form of a Uniform Mixture Model. We present experimental results in order to assess the ability of the method to decide on unimodality and perform comparisons with the well-known dip-test approach. In addition, in the case of unimodal datasets we evaluate the Uniform Mixture Models provided by the proposed method using the test set log-likelihood and the two-sample Kolmogorov-Smirnov (KS) test.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004520",
    "keywords": [
      "Biology",
      "Evolutionary biology",
      "Function (biology)",
      "Geometry",
      "Mathematical analysis",
      "Mathematics",
      "Piecewise",
      "Piecewise linear function",
      "Statistical hypothesis testing",
      "Statistics",
      "Unimodality"
    ],
    "authors": [
      {
        "surname": "Chasani",
        "given_name": "Paraskevi"
      },
      {
        "surname": "Likas",
        "given_name": "Aristidis"
      }
    ]
  },
  {
    "title": "Kernelized support tensor train machines",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108337",
    "abstract": "Tensor, a multi-dimensional data structure, has been exploited recently in the machine learning community. Traditional machine learning approaches are vector- or matrix-based, and cannot handle tensorial data directly. In this paper, we propose a tensor train (TT)-based kernel technique for the first time, and apply it to the conventional support vector machine (SVM) for high-dimensional image classification with very small number of training samples. Specifically, we propose a kernelized support tensor train machine that accepts tensorial input and preserves the intrinsic kernel property. The main contributions are threefold. First, we propose a TT-based feature mapping procedure that maintains the TT structure in the feature space. Second, we demonstrate two ways to construct the TT-based kernel function while considering consistency with the TT inner product and preservation of information. Third, we show that it is possible to apply different kernel functions on different data modes. In principle, our method tensorizes the standard SVM on its input structure and kernel mapping scheme. This reduces the storage and computation complexity of kernel matrix construction from exponential to polynomial. The validity proof and computation complexity of the proposed TT-based kernel functions are provided elaborately. Extensive experiments are performed on high-dimensional fMRI and color images datasets, which demonstrates the superiority of the proposed scheme compared with the state-of-the-art techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005173",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Feature vector",
      "Kernel (algebra)",
      "Kernel embedding of distributions",
      "Kernel method",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Polynomial kernel",
      "Pure mathematics",
      "Radial basis function kernel",
      "String kernel",
      "Support vector machine",
      "Tensor (intrinsic definition)",
      "Tensor product",
      "Tree kernel"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Cong"
      },
      {
        "surname": "Batselier",
        "given_name": "Kim"
      },
      {
        "surname": "Yu",
        "given_name": "Wenjian"
      },
      {
        "surname": "Wong",
        "given_name": "Ngai"
      }
    ]
  },
  {
    "title": "Continuous conditional random field convolution for point cloud segmentation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108357",
    "abstract": "Point cloud segmentation is the foundation of 3D environmental perception for modern intelligent systems. To solve this problem and image segmentation, conditional random fields (CRFs) are usually formulated as discrete models in label space to encourage label consistency, which is actually a kind of postprocessing. In this paper, we reconsider the CRF in feature space for point cloud segmentation because it can capture the structure of features well to improve the representation ability of features rather than simply smoothing. Therefore, we first model the point cloud features with a continuous quadratic energy model and formulate its solution process as a message-passing graph convolution, by which it can be easily integrated into a deep network. We theoretically demonstrate that the message passing in the graph convolution is equivalent to the mean-field approximation of a continuous CRF model. Furthermore, we build an encoder-decoder network based on the proposed continuous CRF graph convolution (CRFConv), in which the CRFConv embedded in the decoding layers can restore the details of high-level features that were lost in the encoding stage to enhance the location ability of the network, thereby benefiting segmentation. Analogous to the CRFConv, we show that the classical discrete CRF can also work collaboratively with the proposed network via another graph convolution to further improve the segmentation results. Experiments on various point cloud benchmarks demonstrate the effectiveness and robustness of the proposed method. Compared with the state-of-the-art methods, the proposed method can also achieve competitive segmentation performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005379",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "CRFS",
      "Computer science",
      "Computer vision",
      "Conditional random field",
      "Convolution (computer science)",
      "Point cloud",
      "Segmentation",
      "Smoothing",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Fei"
      },
      {
        "surname": "Davoine",
        "given_name": "Franck"
      },
      {
        "surname": "Wang",
        "given_name": "Huan"
      },
      {
        "surname": "Jin",
        "given_name": "Zhong"
      }
    ]
  },
  {
    "title": "Efficient k -nearest neighbor search based on clustering and adaptive k values",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108356",
    "abstract": "The k -Nearest Neighbor ( k NN) algorithm is widely used in the supervised learning field and, particularly, in search and classification tasks, owing to its simplicity, competitive performance, and good statistical properties. However, its inherent inefficiency prevents its use in most modern applications due to the vast amount of data that the current technological evolution generates, being thus the optimization of k NN-based search strategies of particular interest. This paper introduces the caKD+ algorithm, which tackles this limitation by combining the use of feature learning techniques, clustering methods, adaptive search parameters per cluster, and the use of pre-calculated K-Dimensional Tree structures, and results in a highly efficient search method. This proposal has been evaluated using 10 datasets and the results show that caKD+ significantly outperforms 16 state-of-the-art efficient search methods while still depicting such an accurate performance as the one by the exhaustive k NN search.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005367",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Machine learning",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Gallego",
        "given_name": "Antonio Javier"
      },
      {
        "surname": "Rico-Juan",
        "given_name": "Juan Ramón"
      },
      {
        "surname": "Valero-Mas",
        "given_name": "Jose J."
      }
    ]
  },
  {
    "title": "Balanced single-shot object detection using cross-context attention-guided network",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108258",
    "abstract": "In real-world application scenarios, object detection usually encounters two technical challenges, i.e., high accuracy and high speed. Although the latest detection frameworks based on anchor-free detection have achieved outstanding performance, they cannot be widely used in real-world scenarios due to their model complexity and slow speed. In this paper, inspired by cross-context attention mechanism of human visual systems, we propose a light but effective single-shot detection framework using Cross-context Attention-guided Network (CCAGNet) to balance the accuracy and speed. CCAGNet uses attention-guided mechanism to highlight the interaction of object-synergy regions, and suppresses non-object-synergy regions by combining Cross-context Attention Mechanism (CCAM), Receptive Field Attention Mechanism (RFAM), and Semantic Fusion Attention Mechanism (SFAM). The main contribution of our work includes establishing a novel attention mechanism that takes the context information of channel, spatial, cross- and adjacent-regions into consideration simultaneously. Extensive experiments demonstrate the feasibility and effectiveness of our method on the public benchmark datasets. To the best of our knowledge, CCAGNet obtains the state-of-the-art performance on both PascalVOC and MSCOCO with the excellent trade-off between accuracy and speed among single-shot detectors. Especially, the Average Precision (AP) metric is significantly improved by 17.0% on small object detection on MSCOCO.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004386",
    "keywords": [
      "Artificial intelligence",
      "Attention network",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Economics",
      "Epistemology",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Mechanism (biology)",
      "Metric (unit)",
      "Object (grammar)",
      "Object detection",
      "Operations management",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Miao",
        "given_name": "Shuyu"
      },
      {
        "surname": "Du",
        "given_name": "Shanshan"
      },
      {
        "surname": "Feng",
        "given_name": "Rui"
      },
      {
        "surname": "Zhang",
        "given_name": "Yuejie"
      },
      {
        "surname": "Li",
        "given_name": "Huayu"
      },
      {
        "surname": "Liu",
        "given_name": "Tianbi"
      },
      {
        "surname": "Zheng",
        "given_name": "Lin"
      },
      {
        "surname": "Fan",
        "given_name": "Weiguo"
      }
    ]
  },
  {
    "title": "Graph Clustering via Variational Graph Embedding",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108334",
    "abstract": "Graph clustering based on embedding aims to divide nodes with higher similarity into several mutually disjoint groups, but it is not a trivial task to maximumly embed the graph structure and node attributes into the low dimensional feature space. Furthermore, most of the current advanced methods of graph nodes clustering adopt the strategy of separating graph embedding technology and clustering algorithm, and ignore the potential relationship between them. Therefore, we propose an innovative end-to-end graph clustering framework with joint strategy to handle the complex problem in a non-Euclidean space. In terms of learning the graph embedding, we propose a new variational graph auto-encoder algorithm based on the Graph Convolution Network (GCN), which takes into account the boosting influence of joint generative model of graph structure and node attributes on the embedding output. On the basis of embedding representation, we implement a self-training mechanism through the construction of auxiliary distribution to further enhance the prediction of node categories, thereby realizing the unsupervised clustering mode. In addition, the loss contribution of each cluster is normalized to prevent large clusters from distorting the embedding space. Extensive experiments on real-world graph datasets validate our design and demonstrate that our algorithm has highly competitive in graph clustering over state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005148",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Clustering coefficient",
      "Computer science",
      "Embedding",
      "Graph",
      "Graph embedding",
      "Line graph",
      "Mathematics",
      "Theoretical computer science",
      "Voltage graph"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Lin"
      },
      {
        "surname": "Dai",
        "given_name": "Qun"
      }
    ]
  },
  {
    "title": "Active recursive Bayesian inference using Rényi information measures",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.01.009",
    "abstract": "Recursive Bayesian inference (RBI) framework provides optimal Bayesian latent variable estimates in real-time settings with streaming noisy observations. Active RBI attempts to effectively select queries that lead to more informative observations to reduce uncertainty until the process is stopped at a certain confidence level. However, the mismatch between querying objective and stopping criterion creates the conundrum of improving the performance of one objective at the expense of deteriorating the other. Moreover, conventional active querying methods stagger in the presence of misleading prior information. Inspired by information theoretic approaches, we propose an active RBI framework where query and stopping criterion are jointly selected through a unified objective based on Rényi information measures. The proposed unified formulation enables us to jointly enhance speed and accuracy in the RBI process. We theoretically demonstrate that the proposed objective encourages exploration in the presence of misleading prior. Furthermore, we motivate our framework by proving a geometrical representation for active querying and decision making on a probability simplex. We provide empirical and experimental studies on two applications including restaurant recommendation and brain-computer interface (BCI) typing systems and demonstrate that our method outperforms comparable approaches by improving both speed and accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000150",
    "keywords": [
      "Artificial intelligence",
      "Bayesian inference",
      "Bayesian probability",
      "Computer science",
      "Inference",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Marghi",
        "given_name": "Yeganeh M."
      },
      {
        "surname": "Koçanaoğulları",
        "given_name": "Aziz"
      },
      {
        "surname": "Akçakaya",
        "given_name": "Murat"
      },
      {
        "surname": "Erdoğmuş",
        "given_name": "Deniz"
      }
    ]
  },
  {
    "title": "SetMargin loss applied to deep keystroke biometrics with circle packing interpretation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108283",
    "abstract": "This work presents a new deep learning approach for keystroke biometrics based on a novel Distance Metric Learning method (DML). DML maps input data into a learned representation space that reveals a “semantic” structure based on distances. In this work, we propose a novel DML method specifically designed to address the challenges associated to free-text keystroke identification where the classes used in learning and inference are disjoint. The proposed SetMargin Loss (SM-L) extends traditional DML approaches with a learning process guided by pairs of sets instead of pairs of samples, as done traditionally. The proposed learning strategy allows to enlarge inter-class distances while maintaining the intra-class structure of keystroke dynamics. We analyze the resulting representation space using the mathematical problem known as Circle Packing, which provides neighbourhood structures with a theoretical maximum inter-class distance. We finally prove experimentally the effectiveness of the proposed approach on a challenging task: keystroke biometric identification over a large set of 78,000 subjects. Our method achieves state-of-the-art accuracy on a comparison performed with the best existing approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004635",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Class (philosophy)",
      "Combinatorics",
      "Computer science",
      "Disjoint sets",
      "Economics",
      "Feature learning",
      "Inference",
      "Keystroke logging",
      "Law",
      "Machine learning",
      "Mathematics",
      "Metric (unit)",
      "Operating system",
      "Operations management",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Morales",
        "given_name": "Aythami"
      },
      {
        "surname": "Fierrez",
        "given_name": "Julian"
      },
      {
        "surname": "Acien",
        "given_name": "Alejandro"
      },
      {
        "surname": "Tolosana",
        "given_name": "Ruben"
      },
      {
        "surname": "Serna",
        "given_name": "Ignacio"
      }
    ]
  },
  {
    "title": "AE-Net: Fine-grained sketch-based image retrieval via attention-enhanced network",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108291",
    "abstract": "In this paper, we investigate the task of Fine-grained Sketch-based Image Retrieval (FG-SBIR), which uses hand-drawn sketches as input queries to retrieve the relevant images at the fine-grained instance level. The sketches and images come from different modalities, thus the similarity computation needs to consider both fine-grained and cross-modal characteristics. Existing solutions only focus on fine-grained details or spatial contexts, while ignoring the channel context and spatial sequence information. To mitigate such challenging problems, we propose a novel deep FG-SBIR model, which aims at inferring attention maps along channel dimension and spatial dimension, improving modules of channel attention and spatial attention, and exploring Transformer to enhance the model’s ability for constructing and understanding spatial sequence information. We focus not only on the correlation information between two modalities of sketch and image, but also on the discrimination information inside the single modality. Mutual Loss is especially proposed to enhance the traditional triplet loss, and promote the internal discrimination ability of the model on a single modality. Extensive experiments show that our AE-Net obtains promising results on Sketchy, which is the largest public dataset available for FG-SBIR at present.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004714",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Context (archaeology)",
      "Dimension (graph theory)",
      "Focus (optics)",
      "Image (mathematics)",
      "Mathematics",
      "Modalities",
      "Modality (human–computer interaction)",
      "Optics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Physics",
      "Pure mathematics",
      "Similarity (geometry)",
      "Sketch",
      "Social science",
      "Sociology",
      "Spatial contextual awareness"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yangdong"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhaolong"
      },
      {
        "surname": "Wang",
        "given_name": "Yanfei"
      },
      {
        "surname": "Zhang",
        "given_name": "Yuejie"
      },
      {
        "surname": "Feng",
        "given_name": "Rui"
      },
      {
        "surname": "Zhang",
        "given_name": "Tao"
      },
      {
        "surname": "Fan",
        "given_name": "Weiguo"
      }
    ]
  },
  {
    "title": "Robust and discrete matrix factorization hashing for cross-modal retrieval",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108343",
    "abstract": "Hashing based methods have gained great success for cross-modal similarity search, due to its fast query speed and low storage cost. However, there are some challenging problems that need to be further solved: 1) Many approaches are sensitive to noises and outliers, because ℓ 2 norm is utilized in the objective function, the error may be amplified. 2) Most existing methods take relaxation or rounding scheme to generate binary codes, causing a large quantization loss. 3) Many supervised cross-media algorithms usually take a large n × n matrix to preserve the similarity relationship, leading to large calculation and making them unscalable. To mitigate these challenges, we develop a novel cross-media search algorithm, i.e., robust and discrete matrix factorization hashing, dubbed RDMH. The method takes a two-step strategy. In the first phase, the ℓ 2 , 1 norm is utilized to improve the robustness, which makes our model not sensitive to noises and outliers. We can learn the hash codes directly by the proposed discrete optimization method instead of relaxation scheme, avoiding the large quantization loss. Moreover, RDMH correlates the hash codes and semantic labels directly instead of manipulating the large similarity matrix. In the second phase, we propose an autoencoder strategy to learn the hash functions, more valuable information can be preserved and making the hash functions more powerful. Comprehensive experiments on several databases demonstrate the superior performance and efficacy of the developed RDMH.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005239",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Data mining",
      "Double hashing",
      "Dynamic perfect hashing",
      "Eigenvalues and eigenvectors",
      "Feature hashing",
      "Hash function",
      "Hash table",
      "K-independent hashing",
      "Matrix decomposition",
      "Non-negative matrix factorization",
      "Operating system",
      "Outlier",
      "Perfect hash function",
      "Physics",
      "Quantum mechanics",
      "Rounding",
      "Theoretical computer science",
      "Universal hashing"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Donglin"
      },
      {
        "surname": "Wu",
        "given_name": "Xiao-Jun"
      }
    ]
  },
  {
    "title": "Video anomaly detection with spatio-temporal dissociation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108213",
    "abstract": "Anomaly detection in videos remains a challenging task due to the ambiguous definition of anomaly and the complexity of visual scenes from real video data. Different from the previous work which utilizes reconstruction or prediction as an auxiliary task to learn the temporal regularity, in this work, we explore a novel convolution autoencoder architecture that can dissociate the spatio-temporal representation to separately capture the spatial and the temporal information, since abnormal events are usually different from the normality in appearance and/or motion behavior. Specifically, the spatial autoencoder models the normality on the appearance feature space by learning to reconstruct the input of the first individual frame (FIF), while the temporal part takes the first four consecutive frames as the input and the RGB difference as the output to simulate the motion of optical flow in an efficient way. The abnormal events, which are irregular in appearance or in motion behavior, lead to a large reconstruction error. To improve detection performance on fast moving outliers, we exploit a variance-based attention module and insert it into the motion autoencoder to highlight large movement areas. In addition, we propose a deep K-means cluster strategy to force the spatial and the motion encoder to extract a compact representation. Extensive experiments on some publicly available datasets have demonstrated the effectiveness of our method which achieves the state-of-the-art performance. The code is publicly released at the link 1 1 https://github.com/ChangYunPeng/VideoAnomalyDetection .",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003940",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Autoencoder",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Image (mathematics)",
      "Optical flow",
      "Outlier",
      "Pattern recognition (psychology)",
      "RGB color model"
    ],
    "authors": [
      {
        "surname": "Chang",
        "given_name": "Yunpeng"
      },
      {
        "surname": "Tu",
        "given_name": "Zhigang"
      },
      {
        "surname": "Xie",
        "given_name": "Wei"
      },
      {
        "surname": "Luo",
        "given_name": "Bin"
      },
      {
        "surname": "Zhang",
        "given_name": "Shifu"
      },
      {
        "surname": "Sui",
        "given_name": "Haigang"
      },
      {
        "surname": "Yuan",
        "given_name": "Junsong"
      }
    ]
  },
  {
    "title": "Feature refinement: An expression-specific feature learning and fusion method for micro-expression recognition",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108275",
    "abstract": "Micro-expression recognition has become challenging, as it is extremely difficult to extract the subtle facial changes of micro-expressions. Recently, several approaches have proposed various expression-shared features algorithms for micro-expression recognition. However, these approaches do not reveal the specific discriminative characteristics, which leads to sub-optimal performance. This paper proposes a novel Feature Refinement (FeatRef) with expression-specific feature learning and fusion for micro-expression recognition that aims to obtain salient and discriminative features for specific expressions and predicts expressions by fusing expression-specific features. FeatRef consists of an expression proposal module with an attention mechanism and a classification branch. First, an inception module is designed based on optical flow to obtain expression-shared features. Second, to extract salient and discriminative features for specific expressions, expression-shared features are fed into an expression proposal module with attention factors and proposal loss. Last, in the classification branch, category labels are predicted via a fusion of expression-specific features. Experiments on three publicly available databases validate the effectiveness of FeatRef under different protocols. The results on public benchmarks demonstrate that FeatRef provides salient and discriminative information for micro-expression recognition. The results also show that FeatRef achieves better or competitive performance with existing state-of-the-art methods on micro-expression recognition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004556",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Discriminative model",
      "Expression (computer science)",
      "Facial expression recognition",
      "Facial recognition system",
      "Feature (linguistics)",
      "Fusion",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Ling"
      },
      {
        "surname": "Mao",
        "given_name": "Qirong"
      },
      {
        "surname": "Huang",
        "given_name": "Xiaohua"
      },
      {
        "surname": "Zhang",
        "given_name": "Feifei"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhihong"
      }
    ]
  },
  {
    "title": "Multi‐frame based adversarial learning approach for video surveillance",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108350",
    "abstract": "Foreground-background segmentation (FBS) is one of the prime tasks for automated video-based applications like traffic analysis and surveillance. The different practical scenarios like weather degraded videos, irregular moving objects, dynamic background, etc., make FBS a challenging task. The existing FBS algorithms mainly depend on one of the three different factors, namely (1) complicated training process, (2) additionally trained modules for other applications, or (3) neglect the inter-frame spatio-temporal structural dependencies. In this paper, a novel multi-frame-based adversarial learning network is proposed with multi-scale inception and residual module for FBS. As, FBS is a temporal enlightenment-based problem, a temporal encoding mechanism with decreasing variable intervals is proposed for the input frame selection. The proposed network comprises multi-scale inception and residual connection-based dense modules to learn prominent features of the foreground object(s). Also, feedback of the estimated foreground map of previous frame is utilized to exhibit more temporal consistency. Learning of the network is concentrated in different ways like cross-data, disjoint, and global training-testing for FBS. The qualitative and quantitative experimental analysis of the proposed approach is done on three benchmark datasets for FBS. Experimental analysis on three benchmark datasets proves the significance of the proposed approach as compared to state-of-the-art FBS approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005306",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Frame (networking)",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Residual",
      "Segmentation",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Patil",
        "given_name": "Prashant W."
      },
      {
        "surname": "Dudhane",
        "given_name": "Akshay"
      },
      {
        "surname": "Chaudhary",
        "given_name": "Sachin"
      },
      {
        "surname": "Murala",
        "given_name": "Subrahmanyam"
      }
    ]
  },
  {
    "title": "Who is closer: A computational method for domain gap evaluation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108293",
    "abstract": "Domain gaps between different datasets limit the generalization ability of CNN models. Precise evaluation on the domain gap has potential to assist the promotion of CNN generalization ability. This paper proposes a computational framework to evaluate gaps between different domains, e.g., judging which one of source domains is closer to the target domain. Our model is based on the observation that, given a well-trained classifier on the source domain, the entropy of its classification scores of the output layer can be used as an indicator of the domain gap. For instance, smaller domain gap generally corresponds to smaller entropy of classification scores. To further boost the discriminative power in distinguishing domain gaps, a novel training strategy is proposed to supervise the model to produce smaller entropy on one source domain and larger entropy on other source domains. This supervision leads to an efficient and discriminative domain gap evaluation model. Extensive experiments on multiple datasets including faces, vehicles, fashions, and persons, etc. show that our method can reasonably measure domain gaps. We further conduct experiments on domain adaptive person ReID task and our method is adopted to pre-trained model selection, pre-trained model fusion, source dataset fusion, and source dataset selection. As shown in the experiments, our method substantially boosts the ReID accuracy. To the best of our knowledge, this is an original work focusing on computational domain gap evaluation. Our code is available at https://github.com/liu-xb/DomainGapEvaluation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004738",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Discriminative model",
      "Domain (mathematical analysis)",
      "Entropy (arrow of time)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Source code"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Xiaobin"
      },
      {
        "surname": "Zhang",
        "given_name": "Shiliang"
      }
    ]
  },
  {
    "title": "Weakly Supervised Segmentation of COVID19 Infection with Scribble Annotation on CT Images",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108341",
    "abstract": "Segmentation of infections from CT scans is important for accurate diagnosis and follow-up in tackling the COVID-19. Although the convolutional neural network has great potential to automate the segmentation task, most existing deep learning-based infection segmentation methods require fully annotated ground-truth labels for training, which is time-consuming and labor-intensive. This paper proposed a novel weakly supervised segmentation method for COVID-19 infections in CT slices, which only requires scribble supervision and is enhanced with the uncertainty-aware self-ensembling and transformation-consistent techniques. Specifically, to deal with the difficulty caused by the shortage of supervision, an uncertainty-aware mean teacher is incorporated into the scribble-based segmentation method, encouraging the segmentation predictions to be consistent under different perturbations for an input image. This mean teacher model can guide the student model to be trained using information in images without requiring manual annotations. On the other hand, considering the output of the mean teacher contains both correct and unreliable predictions, equally treating each prediction in the teacher model may degrade the performance of the student network. To alleviate this problem, the pixel level uncertainty measure on the predictions of the teacher model is calculated, and then the student model is only guided by reliable predictions from the teacher model. To further regularize the network, a transformation-consistent strategy is also incorporated, which requires the prediction to follow the same transformation if a transform is performed on an input image of the network. The proposed method has been evaluated on two public datasets and one local dataset. The experimental results demonstrate that the proposed method is more effective than other weakly supervised methods and achieves similar performance as those fully supervised.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005215",
    "keywords": [
      "Annotation",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Economic shortage",
      "Economics",
      "Gene",
      "Government (linguistics)",
      "Ground truth",
      "Image segmentation",
      "Linguistics",
      "Machine learning",
      "Management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Segmentation",
      "Task (project management)",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Xiaoming"
      },
      {
        "surname": "Yuan",
        "given_name": "Quan"
      },
      {
        "surname": "Gao",
        "given_name": "Yaozong"
      },
      {
        "surname": "He",
        "given_name": "Kelei"
      },
      {
        "surname": "Wang",
        "given_name": "Shuo"
      },
      {
        "surname": "Tang",
        "given_name": "Xiao"
      },
      {
        "surname": "Tang",
        "given_name": "Jinshan"
      },
      {
        "surname": "Shen",
        "given_name": "Dinggang"
      }
    ]
  },
  {
    "title": "Exploring rich intermediate representations for reconstructing 3D shapes from 2D images",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108295",
    "abstract": "Recovering 3D voxelized shapes with fine details from single-view 2D images is an extremely challenging and ill-conditioned problem. Most of the existing methods learn the 3D reconstruction process by encoding the 3D shapes and the 2D images into the same low-dimensional latent vector, which lacks the capacity to capture detailed features in the surface of the 3D object shapes. To address this issue, we propose to explore rich intermediate representation for 3D shape reconstruction by using a newly designed network architecture. We first use a two-steam network to infer the depth map and the topology-specific mean shape from the given 2D image, which forms the intermediate representation prediction branch. The intermediate representations capture the global spatial structure and the visible surface geometric structure, which are important for reconstructing high-quality 3D shapes. Based on the obtained intermediate representation, a novel shape transformation network is then proposed to reconstruct the fine details of the whole 3D object shapes. The experimental results on the challenging ShapeNet and Pix3D datasets show that our approach outperforms the existing state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004751",
    "keywords": [
      "3D reconstruction",
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Encoding (memory)",
      "Gene",
      "Geometry",
      "Image (mathematics)",
      "Law",
      "Mathematics",
      "Object (grammar)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Process (computing)",
      "Representation (politics)",
      "Surface (topology)",
      "Surface reconstruction",
      "Topology (electrical circuits)",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Yang"
      },
      {
        "surname": "Han",
        "given_name": "Junwei"
      },
      {
        "surname": "Zhang",
        "given_name": "Dingwen"
      },
      {
        "surname": "Tian",
        "given_name": "Qi"
      }
    ]
  },
  {
    "title": "Online learnable keyframe extraction in videos and its application with semantic word vector in action recognition",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108273",
    "abstract": "Video processing has become a popular research direction in computer vision due to its various applications such as video summarization, action recognition, etc. Recently, deep learning-based methods have achieved impressive results in action recognition. However, these methods need to process a full video sequence to recognize the action, even though many of the frames in the video sequence are similar and non-essential to recognizing a particular action. Additionally, these non-essential frames increase the computational cost and can confuse a method in action recognition. Instead, the important frames called keyframes not only are helpful in recognizing an action but also can reduce the processing time of each video sequence in classification or in other applications, e.g. summarization. As well, current methods in video processing have not yet been demonstrated in an online fashion. Motivated by the above, we propose an online learnable module for keyframe extraction. This module can be used to select key shots in video and thus, can be applied to video summarization. The extracted keyframes can be used as input to any deep learning-based classification model to recognize action. We also propose a plugin module to use the semantic word vector as input along with keyframes and a novel train/test strategy for the classification models. To our best knowledge, this is the first time such an online module and train/test strategy have been proposed. The experimental results on many commonly used datasets in video summarization and in action recognition have demonstrated the effectiveness of the proposed module.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004532",
    "keywords": [
      "Action (physics)",
      "Artificial intelligence",
      "Automatic summarization",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Feature extraction",
      "Key (lock)",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Semantics (computer science)",
      "Speech recognition",
      "Video processing",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "E Elahi",
        "given_name": "G M Mashrur"
      },
      {
        "surname": "Yang",
        "given_name": "Yee-Hong"
      }
    ]
  },
  {
    "title": "Feature wise normalization: An effective way of normalizing data",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108307",
    "abstract": "This paper presents a novel Feature Wise Normalization approach for the effective normalization of data. In this approach, each feature is normalized independently with one of the methods from the pool of normalization methods. It is in contrast to the conventional approach which normalizes the data with one method only and as a result, yields suboptimal performance. Additionally, generalization and superiority among normalization methods are also not ensured owing to different machine learning mechanisms for solving classification tasks. The proposed approach benefits from the collective response of multiple methods to normalize the data better as individual features become a normalization unit. The selection of methods is a combinatorial problem that can be solved with optimization algorithms. For this purpose, Antlion optimization is considered that combines the search of methods with the fine-tuning of classifier parameters. Twelve methods are used to create the pool beside the original scale, and the obtained data is evaluated on four learning algorithms. Experiments are performed on 18 benchmark datasets to show the efficacy of the proposed approach in contrast to conventional normalization.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004878",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Database normalization",
      "Feature selection",
      "Machine learning",
      "Normalization (sociology)",
      "Pattern recognition (psychology)",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Singh",
        "given_name": "Dalwinder"
      },
      {
        "surname": "Singh",
        "given_name": "Birmohan"
      }
    ]
  },
  {
    "title": "A Novel Quasi-Newton Method for Composite Convex Minimization",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108281",
    "abstract": "A fast parallelable Jacobi iteration type optimization method for non-smooth convex composite optimization is presented. Traditional gradient-based techniques cannot solve the problem. Smooth approximate functions are attempted to be used as a replacement of those non-smooth terms without compromising the accuracy. Recently, proximal mapping concept has been introduced into this field. Techniques which utilize proximal average based proximal gradient have been used to solve the problem. The state-of-art methods only utilize first-order information of the smooth approximate function. We integrate both first and second-order techniques to use both first and second-order information to boost the convergence speed. A convergence rate with a lower bound of O ( 1 k 2 ) is achieved by the proposed method and a super-linear convergence is enjoyed when there is proper second-order information. In experiments, the proposed method converges significantly better than the state of art methods which enjoy O ( 1 k ) convergence.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004611",
    "keywords": [
      "Algorithm",
      "Biology",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Convergence (economics)",
      "Convex function",
      "Convex optimization",
      "Economic growth",
      "Economics",
      "Evolutionary biology",
      "Function (biology)",
      "Geometry",
      "Gradient method",
      "Mathematical optimization",
      "Mathematics",
      "Minification",
      "Proximal Gradient Methods",
      "Rate of convergence",
      "Regular polygon"
    ],
    "authors": [
      {
        "surname": "Chai",
        "given_name": "W.H."
      },
      {
        "surname": "Ho",
        "given_name": "S.S."
      },
      {
        "surname": "Quek",
        "given_name": "H.C."
      }
    ]
  },
  {
    "title": "Nonconvex 3D array image data recovery and pattern recognition under tensor framework",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108311",
    "abstract": "In this paper, we present a weighted tensor Schatten- p quasi-norm ( 0 < p < 1 ) regularizer for 3D array datasets in order to recover the low-rank part and the sparse part, respectively. Corresponding algorithms associated with augmented Lagrangian multipliers are established and the constructed sequence converges to the desirable Karush-Kuhn-Tucker (KKT) point, which is mathematically validated in detail. Although the proposed weighted tensor Schatten- p quasi-norm is non-convex, it appears not only to less penalize the singular values but also to be effective in capturing the low-rank property. The main findings in this paper are the appropriate choice of p depends on specific tasks: low-rank data set recovery usually requires relatively large value of p , while sparse data set recovery needs relatively small value of p . And the weights chosen in our tensor Schatten- p quasi-norm are inversely to the singular values exponentially for promoting the sensitivity to different singular values. Experimental results for video inpainting (tensor completion), image recovery and salient object detection (tensor robust principal component analysis) have been shown that the proposed approach outperforms various latest approaches in literature.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100491X",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Augmented Lagrangian method",
      "Combinatorics",
      "Computer science",
      "Eigenvalues and eigenvectors",
      "Image (mathematics)",
      "Inpainting",
      "Law",
      "Mathematics",
      "Matrix norm",
      "Norm (philosophy)",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Principal component analysis",
      "Pure mathematics",
      "Quantum mechanics",
      "Rank (graph theory)",
      "Robust principal component analysis",
      "Singular value",
      "Singular value decomposition",
      "Statistics",
      "Tensor (intrinsic definition)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Ming"
      },
      {
        "surname": "Luo",
        "given_name": "Qilun"
      },
      {
        "surname": "Li",
        "given_name": "Wen"
      },
      {
        "surname": "Xiao",
        "given_name": "Mingqing"
      }
    ]
  },
  {
    "title": "An effective deep network using target vector update modules for image restoration",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108333",
    "abstract": "Image restoration (IR) has been widely used in many computer vision applications. The model-based IR methods have clear theoretical bases. However, numerous hyper-parameters need to be set empirically, which is often challenging and time-consuming. Because of the powerful nonlinear fitting ability, deep convolutional neural networks (CNNs) have been widely used in IR tasks in recent years. However, it is challenging to design new network architecture to further significantly improve the IR performance. Inspired by the plug and play (P&P) methods, we first decouple the original IR problem into two subproblems with the variable splitting technique. Then, derived from the model-based methods, a novel deep CNN framework in the transformation domain is proposed to mimic the optimization process of the two subproblems. The proposed framework is driven effectively by the target vector update (TVU) module. Extensive experiments demonstrate the effectiveness of our proposed method over other state-of-the-art IR methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005136",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Domain (mathematical analysis)",
      "Gene",
      "Image (mathematics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Nonlinear system",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Plug and play",
      "Process (computing)",
      "Programming language",
      "Quantum mechanics",
      "Set (abstract data type)",
      "Transformation (genetics)",
      "Variable (mathematics)"
    ],
    "authors": [
      {
        "surname": "Zhai",
        "given_name": "Sen"
      },
      {
        "surname": "Ren",
        "given_name": "Chao"
      },
      {
        "surname": "Wang",
        "given_name": "Zhengyong"
      },
      {
        "surname": "He",
        "given_name": "Xiaohai"
      },
      {
        "surname": "Qing",
        "given_name": "Linbo"
      }
    ]
  },
  {
    "title": "Robust patchmatch HDR image reconstruction for deghosting",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.01.012",
    "abstract": "Aiming at the deghosting problem of High Dynamic Range (HDR) imaging technology, different from the existing algorithms of simply eliminating moving objects or only using a separate pre-registration based on homography transformation or optical flow, we propose an integrated HDR imaging scheme including Registration and Matching based on Patch (HDR-RMP) for deghosting in complex dynamic scenes. In our scheme, the combination of Affine Transformation (AT) and Normalization Mutual Information (NMI) has an implicit registration effect on the input images. Furthermore, NMI and Structural Similarity Index Measurement (SSIM) are fit to establish the final optimal function including the strategies of registration and matching, and the registration accuracy is promoted by this way, thereby the ability to remove ghosts is improved and the time complexity is reduced simultaneously. Finally, the accuracy of image matching is enhanced. The results show that the deghosting effect of HDR-RMP algorithm is remarkable, especially in dynamic scenes with large and mass moving objects, which not only improves the fusion quality, but also reduces the algorithmic complexity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000198",
    "keywords": [
      "Affine transformation",
      "Anthropology",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Digitization",
      "Dynamic range",
      "Gene",
      "High dynamic range",
      "High-dynamic-range imaging",
      "Homography",
      "Image (mathematics)",
      "Image registration",
      "Matching (statistics)",
      "Mathematics",
      "Normalization (sociology)",
      "Pattern recognition (psychology)",
      "Projective space",
      "Projective test",
      "Pure mathematics",
      "Sociology",
      "Statistics",
      "Transformation (genetics)",
      "Transformation geometry"
    ],
    "authors": [
      {
        "surname": "Xia",
        "given_name": "Shu-fang"
      },
      {
        "surname": "Guo",
        "given_name": "Song-tao"
      },
      {
        "surname": "Qu",
        "given_name": "Zhong"
      },
      {
        "surname": "Yang",
        "given_name": "Yuan-yuan"
      }
    ]
  },
  {
    "title": "Learning multiscale hierarchical attention for video summarization",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108312",
    "abstract": "In this paper, we propose a multiscale hierarchical attention approach for supervised video summarization. Different from most existing supervised methods which employ bidirectional long short-term memory networks, our method exploits the underlying hierarchical structure of video sequences and learns both the short-range and long-range temporal representations via a intra-block and a inter-block attention. Specifically, we first separate each video sequence into blocks of equal length and employ the intra-block and inter-block attention to learn local and global information, respectively. Then, we integrate the frame-level, block-level, and video-level representations for the frame-level importance score prediction. Next, we conduct shot segmentation and compute shot-level importance scores. Finally, we perform key shot selection to produce video summaries. Moreover, we extend our method into a two-stream framework, where appearance and motion information is leveraged. Experimental results on the SumMe and TVSum datasets validate the effectiveness of our method against state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004921",
    "keywords": [
      "Artificial intelligence",
      "Automatic summarization",
      "Block (permutation group theory)",
      "Chemistry",
      "Composite material",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Exploit",
      "Frame (networking)",
      "Geometry",
      "Machine learning",
      "Materials science",
      "Mathematics",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Range (aeronautics)",
      "Segmentation",
      "Shot (pellet)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Wencheng"
      },
      {
        "surname": "Lu",
        "given_name": "Jiwen"
      },
      {
        "surname": "Han",
        "given_name": "Yucheng"
      },
      {
        "surname": "Zhou",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "Reasoning structural relation for occlusion-robust facial landmark localization",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108325",
    "abstract": "In facial landmark localization tasks, various occlusions heavily degrade the localization accuracy due to the partial observability of facial features. This paper proposes a structural relation network (SRN) for occlusion-robust landmark localization. Unlike most existing methods that simply exploit the shape constraint, the proposed SRN aims to capture the structural relations among different facial components. These relations can be considered a more powerful shape constraint against occlusion. To achieve this, a hierarchical structural relation module (HSRM) is designed to hierarchically reason the structural relations that represent both long- and short-distance spatial dependencies. Compared with existing network architectures,the HSRM can efficiently model the spatial relations by leveraging its geometry-aware network architecture, which reduces the semantic ambiguity caused by occlusion. Moreover, the SRN augments the training data by synthesizing occluded faces. To further extend our SRN for occluded video data, we formulate the occluded face synthesis as a Markov decision process (MDP). Specifically, it plans the movement of the dynamic occlusion based on an accumulated reward associated with the performance degradation of the pre-trained SRN. This procedure augments hard samples for robust facial landmark tracking. Extensive experimental results indicate that the proposed method achieves outstanding performance on occluded and masked faces. Code is available at https://github.com/zhuccly/SRN",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005057",
    "keywords": [
      "Ambiguity",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Constraint (computer-aided design)",
      "Data mining",
      "Face (sociological concept)",
      "Geometry",
      "Landmark",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Relation (database)",
      "Social science",
      "Sociology",
      "Spatial relation"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Congcong"
      },
      {
        "surname": "Li",
        "given_name": "Xiaoqiang"
      },
      {
        "surname": "Li",
        "given_name": "Jide"
      },
      {
        "surname": "Dai",
        "given_name": "Songmin"
      },
      {
        "surname": "Tong",
        "given_name": "Weiqin"
      }
    ]
  },
  {
    "title": "Generalizable model-agnostic semantic segmentation via target-specific normalization",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108292",
    "abstract": "Semantic segmentation in a supervised learning manner has achieved significant progress in recent years. However, its performance usually drops dramatically due to the data-distribution discrepancy between seen and unseen domains when we directly deploy the trained model to segment the images of unseen (or new coming) domains. To this end, we propose a novel domain generalization framework for the generalizable semantic segmentation task, which enhances the generalization ability of the model from two different views, including the training paradigm and the test strategy. Concretely, we exploit the model-agnostic learning to simulate the domain shift problem, which deals with the domain generalization from the training scheme perspective. Besides, considering the data-distribution discrepancy between seen source and unseen target domains, we develop the target-specific normalization scheme to enhance the generalization ability. Furthermore, when images come one by one in the test stage, we design the image-based memory bank (Image Bank in short) with style-based selection policy to select similar images to obtain more accurate statistics of normalization. Extensive experiments highlight that the proposed method produces state-of-the-art performance for the domain generalization of semantic segmentation on multiple benchmark segmentation datasets, i.e., Cityscapes, Mapillary.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004726",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer security",
      "Exploit",
      "Generalization",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Normalization (sociology)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Segmentation",
      "Sociology",
      "Test data"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Jian"
      },
      {
        "surname": "Qi",
        "given_name": "Lei"
      },
      {
        "surname": "Shi",
        "given_name": "Yinghuan"
      },
      {
        "surname": "Gao",
        "given_name": "Yang"
      }
    ]
  },
  {
    "title": "Discriminative unimodal feature selection and fusion for RGB-D salient object detection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108359",
    "abstract": "Most existing RGB-D salient object detectors make use of the complementary information of RGB-D images to overcome the challenging scenarios, e.g., low contrast, clutter backgrounds. However, these models generally neglect the fact that one of the input images may be poor in quality. This will adversely affect the discriminative ability of cross-modal features when the two channels are fused directly. To address this issue, a novel end-to-end RGB-D salient object detection model is proposed in this paper. At the core of our model is a Semantic-Guided Modality-Weight Map Generation (SG-MWMG) sub-network, producing modality-weight maps to indicate which regions on both modalities are high-quality regions, given input RGB-D images and the guidance of their semantic information. Based on it, a Bi-directional Multi-scale Cross-modal Feature Fusion (Bi-MCFF) module is presented, where the interactions of the features across different modalities and scales are exploited by using a novel bi-directional structure for better capturing cross-scale and cross-modal complementary information. The experimental results on several benchmark datasets verify the effectiveness and superiority of the proposed method over some state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005392",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Chemistry",
      "Clutter",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature selection",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Modal",
      "Modality (human–computer interaction)",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Polymer chemistry",
      "Quantum mechanics",
      "RGB color model",
      "Radar",
      "Salient",
      "Scale (ratio)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Nianchang"
      },
      {
        "surname": "Luo",
        "given_name": "Yongjiang"
      },
      {
        "surname": "Zhang",
        "given_name": "Qiang"
      },
      {
        "surname": "Han",
        "given_name": "Jungong"
      }
    ]
  },
  {
    "title": "Exploiting foreground and background separation for prohibited item detection in overlapping X-Ray images",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108261",
    "abstract": "X-ray imagery security screening is an essential component of transportation and logistics. In recent years, some researchers have used computer vision algorithms to replace inefficient and tedious manual baggage inspection. However, X-ray images are complicated, and objects overlap with one another in a semi-transparent state, which underperforms the existing object detection frameworks. To solve the severe overlapping problem of X-ray images, we propose a foreground and background separation (FBS) X-ray prohibited item detection framework, which separates prohibited items from other items to exclude irrelevant information. First, we design a target foreground and use recursive training to adaptively approximate the real foreground. Thereafter, with the constraints of X-ray imaging characteristics, a decoder is employed to separate the prohibited items from other irrelevant items to obtain the foreground and background (FB). Finally, we use the attention module to make the detection framework focus more on the foreground. Our method is evaluated on a synthetic dataset with FB ground truth and two public datasets with only bounding box annotations. Extensive experimental results demonstrate that our method significantly outperforms state-of-the-art solutions. Furthermore, experiments are performed in the case where only a small number of images contain the FB ground truth. The results indicate that our method requires only a small number of FB ground truths to obtain a performance equivalent to that of all FB ground truths.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004416",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Bounding overwatch",
      "Component (thermodynamics)",
      "Computer science",
      "Computer vision",
      "Figure–ground",
      "Focus (optics)",
      "Foreground detection",
      "Ground truth",
      "Image (mathematics)",
      "Minimum bounding box",
      "Neuroscience",
      "Object (grammar)",
      "Object detection",
      "Optics",
      "Pattern recognition (psychology)",
      "Perception",
      "Physics",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Shao",
        "given_name": "Fangtao"
      },
      {
        "surname": "Liu",
        "given_name": "Jing"
      },
      {
        "surname": "Wu",
        "given_name": "Peng"
      },
      {
        "surname": "Yang",
        "given_name": "Zhiwei"
      },
      {
        "surname": "Wu",
        "given_name": "Zhaoyang"
      }
    ]
  },
  {
    "title": "Loss functions for pose guided person image generation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108351",
    "abstract": "Pose guided person image generation aims to transform a source person image to a target pose. It is an ill-posed problem as we often need to generate pixels that are invisible in the source image. Recent works focus on designing new architectures of deep neural networks and show promising performance. However, they simply adopt loss functions widely used in generic image generation tasks, e.g., adversarial loss, L1-norm loss, perceptual loss, and style loss, which fail to consider the unique structural patterns of a person. In addition, it remains unclear how each individual loss and their combinations impact the generated person images. The goal of this paper is to have a comprehensive study of loss functions for pose guided person image generation. After revisiting these generic loss functions, we consider the structural similarity (SSIM) index as a loss function since it is widely used as the evaluation metric and can capture the perceptual quality of generated images. In addition, motivated by the observation that a person can be divided into part regions with homogeneous pixel values or texture, we extend the SSIM loss into a novel Part-based SSIM (PSSIM) loss to explicitly account for the articulated body structure. A new PSSIM metric is then proposed naturally to access the quality of generated person images. In order to have a deep investigation of loss functions, we conduct extensive experiments including single-loss analysis, multi-loss combination analysis, optimal loss combination search, and comparison with state-of-the-art methods. Both quantitative and qualitative results indicate that (1) using different loss functions significantly impacts the generated person images, (2) the combination of adversarial loss, perceptual loss, and PSSIM loss is the optimal choice for person image generation, and (3) the proposed PSSIM loss is complementary to prior losses and helps improve the performance of state-of-the art methods. We have made the source code publicly available at https://github.com/shyern/Pose-Transfer-pSSIM.git.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005318",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Economics",
      "Image (mathematics)",
      "Image quality",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Pixel",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Haoyue"
      },
      {
        "surname": "Wang",
        "given_name": "Le"
      },
      {
        "surname": "Zheng",
        "given_name": "Nanning"
      },
      {
        "surname": "Hua",
        "given_name": "Gang"
      },
      {
        "surname": "Tang",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "AI-Based human audio processing for COVID-19: A comprehensive overview",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108289",
    "abstract": "The Coronavirus (COVID-19) pandemic impelled several research efforts, from collecting COVID-19 patients’ data to screening them for virus detection. Some COVID-19 symptoms are related to the functioning of the respiratory system that influences speech production; this suggests research on identifying markers of COVID-19 in speech and other human generated audio signals. In this article, we give an overview of research on human audio signals using ‘Artificial Intelligence’ techniques to screen, diagnose, monitor, and spread the awareness about COVID-19. This overview will be useful for developing automated systems that can help in the context of COVID-19, using non-obtrusive and easy to use bio-signals conveyed in human non-speech and speech audio productions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004696",
    "keywords": [
      "2019-20 coronavirus outbreak",
      "Artificial intelligence",
      "Audio signal",
      "Audio signal processing",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Coronavirus disease 2019 (COVID-19)",
      "Disease",
      "Infectious disease (medical specialty)",
      "Medicine",
      "Outbreak",
      "Paleontology",
      "Pandemic",
      "Pathology",
      "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)",
      "Speech coding",
      "Speech processing",
      "Speech recognition",
      "Virology"
    ],
    "authors": [
      {
        "surname": "Deshpande",
        "given_name": "Gauri"
      },
      {
        "surname": "Batliner",
        "given_name": "Anton"
      },
      {
        "surname": "Schuller",
        "given_name": "Björn W."
      }
    ]
  },
  {
    "title": "Data synthesis method preserving correlation of features",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108241",
    "abstract": "Abundant data are essential for improving the performance of machine learning algorithms. Thus, if only limited data are available, data synthesis can be used to enlarge datasets. Data synthesis methods based on the covariance matrix are useful because of their fast data synthesis capabilities. However, artificial datasets generated via classical techniques show statistical discrepancies when compared to original datasets. To address this problem, we developed a new data synthesis method that preserves the correlation (between features) observed in the original dataset. This preservation was realized by considering not only the correlation but also the random noises used in data synthesis process. This method was applied to various biosignals (i.e., electrocortiography, electromyogram, and electrocardiogram), wherein data points are insufficient. Several classifiers (i.e., convolutional neural network, support vector machine, and k-nearest neighbor) were used to verify that the classification accuracy can be improved by the proposed data synthesis method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004222",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolutional neural network",
      "Correlation",
      "Covariance",
      "Covariance matrix",
      "Data mining",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Statistics",
      "Support vector machine",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Wonseok"
      },
      {
        "surname": "Nam",
        "given_name": "Woochul"
      }
    ]
  },
  {
    "title": "Multi-task driven explainable diagnosis of COVID-19 using chest X-ray images",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108243",
    "abstract": "With increasing number of COVID-19 cases globally, all the countries are ramping up the testing numbers. While the RT-PCR kits are available in sufficient quantity in several countries, others are facing challenges with limited availability of testing kits and processing centers in remote areas. This has motivated researchers to find alternate methods of testing which are reliable, easily accessible and faster. Chest X-Ray is one of the modalities that is gaining acceptance as a screening modality. Towards this direction, the paper has two primary contributions. Firstly, we present the COVID-19 Multi-Task Network (COMiT-Net) which is an automated end-to-end network for COVID-19 screening. The proposed network not only predicts whether the CXR has COVID-19 features present or not, it also performs semantic segmentation of the regions of interest to make the model explainable. Secondly, with the help of medical professionals, we manually annotate the lung regions and semantic segmentation of COVID19 symptoms in CXRs taken from the ChestXray-14, CheXpert, and a consolidated COVID-19 dataset. These annotations will be released to the research community. Experiments performed with more than 2500 frontal CXR images show that at 90% specificity, the proposed COMiT-Net yields 96.80% sensitivity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004246",
    "keywords": [],
    "authors": [
      {
        "surname": "Malhotra",
        "given_name": "Aakarsh"
      },
      {
        "surname": "Mittal",
        "given_name": "Surbhi"
      },
      {
        "surname": "Majumdar",
        "given_name": "Puspita"
      },
      {
        "surname": "Chhabra",
        "given_name": "Saheb"
      },
      {
        "surname": "Thakral",
        "given_name": "Kartik"
      },
      {
        "surname": "Vatsa",
        "given_name": "Mayank"
      },
      {
        "surname": "Singh",
        "given_name": "Richa"
      },
      {
        "surname": "Chaudhury",
        "given_name": "Santanu"
      },
      {
        "surname": "Pudrod",
        "given_name": "Ashwin"
      },
      {
        "surname": "Agrawal",
        "given_name": "Anjali"
      }
    ]
  },
  {
    "title": "Self-supervised representation learning for detection of ACL tear injury in knee MR videos",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.01.008",
    "abstract": "The success of deep learning based models for computer vision applications requires large scale human annotated data which are often expensive to generate. Self-supervised learning, a subset of unsupervised learning, handles this problem by learning meaningful features from unlabeled image or video data. In this paper, we propose a self-supervised learning approach to learn transferable features from MR video clips by enforcing the model to learn anatomical features. The pretext task models are designed to predict the correct ordering of the jumbled image patches that the MR video frames are divided into. To the best of our knowledge, none of the supervised learning models performing injury classification task from MR video provide any explanation for the decisions made by the models and hence makes our work the first of its kind on MR video data. Experiments on the pretext task show that this proposed approach enables the model to learn spatial context invariant features which help for reliable and explainable performance in downstream tasks like classification of Anterior Cruciate Ligament tear injury from knee MR videos. The efficiency of the novel Convolutional Neural Network proposed in this paper is reflected in the experimental results obtained in the downstream task. The proposed model achieves an accuracy of 76.62% and an AUC score of 0.848 on the Sagittal plane, outperforming the contrastive learning algorithms like PIRL and SimCLR using jigsaw puzzle as a transformation. The proposed model also achieved an AUC score of 0.740 on the KneeMRI dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000149",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Convolutional neural network",
      "Deep learning",
      "Economics",
      "Feature learning",
      "Machine learning",
      "Management",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Supervised learning",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Manna",
        "given_name": "Siladittya"
      },
      {
        "surname": "Bhattacharya",
        "given_name": "Saumik"
      },
      {
        "surname": "Pal",
        "given_name": "Umapada"
      }
    ]
  },
  {
    "title": "Deep neighbor-aware embedding for node clustering in attributed graphs",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108230",
    "abstract": "Node clustering aims to partition the vertices in a graph into multiple groups or communities. Existing studies have mostly focused on developing deep learning approaches to learn a latent representation of nodes, based on which simple clustering methods like k -means are applied. These two-step frameworks for node clustering are difficult to manipulate and usually lead to suboptimal performance, mainly because the graph embedding is not goal-directed, i.e., designed for the specific clustering task. In this paper, we propose a clustering-directed deep learning approach, Deep Neighbor-aware Embedded Node Clustering (DNENC for short) for clustering graph data. Our method focuses on attributed graphs to sufficiently explore the two sides of information in graphs. It encodes the topological structure and node content in a graph into a compact representation via a neighbor-aware graph autoencoder, which progressively absorbs information from neighbors via a convolutional or attentional encoder. Multiple neighbor-aware encoders are stacked to build a deep architecture followed by an inner-product decoder for reconstructing the graph structure. Furthermore, soft labels are generated to supervise a self-training process, which iteratively refines the node clustering results. The self-training process is jointly learned and optimized with the graph embedding in a unified framework, to benefit both components mutually. Experimental results compared with state-of-the-art algorithms demonstrate the good performance of our framework.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004118",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Deep learning",
      "Embedding",
      "Feature learning",
      "Graph",
      "Line graph",
      "Pattern recognition (psychology)",
      "Theoretical computer science",
      "Topological graph theory",
      "Voltage graph"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Chun"
      },
      {
        "surname": "Pan",
        "given_name": "Shirui"
      },
      {
        "surname": "Yu",
        "given_name": "Celina P."
      },
      {
        "surname": "Hu",
        "given_name": "Ruiqi"
      },
      {
        "surname": "Long",
        "given_name": "Guodong"
      },
      {
        "surname": "Zhang",
        "given_name": "Chengqi"
      }
    ]
  },
  {
    "title": "Multi-scale spatial-spectral fusion based on multi-input fusion calculation and coordinate attention for hyperspectral image classification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108348",
    "abstract": "Recently, the deep learning method that integrates image features has gradually become a hot development trend in hyperspectral image classification. However, these studies did not fully consider the fusion of image features, and did not remove the interference to the classification process caused by the difference in the size of the objects. These factors hinder the further improvement of the classification effect. To eliminate these drawbacks, this paper proposes a more effective fusion scheme (MSF-MIF), which realizes the fusion from the perspective of location characteristics and channel characteristics through 3D convolution and spatial feature concatenation. In view of the size discrepancy of the objects to be classified, this method extracts features from several input patches of different scales and uses the novel calculation method proposed to fuse them, which minimizes the interference caused by size differences. In addition, this research also tried to quote the coordinate attention structure for the first time that combines spatial and spectral attention features to further improve the classification performance. Experimental results on three commonly used data sets prove that this framework has achieved a breakthrough in classification accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005288",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Channel (broadcasting)",
      "Combinatorics",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Concatenation (mathematics)",
      "Convolution (computer science)",
      "Electrical engineering",
      "Engineering",
      "Feature (linguistics)",
      "Fuse (electrical)",
      "Fusion",
      "Geometry",
      "Hyperspectral imaging",
      "Image (mathematics)",
      "Image fusion",
      "Interference (communication)",
      "Linguistics",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Philosophy",
      "Process (computing)",
      "Pyramid (geometry)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Lina"
      },
      {
        "surname": "Zhang",
        "given_name": "Fengqi"
      },
      {
        "surname": "Wang",
        "given_name": "Patrick Shen-Pei"
      },
      {
        "surname": "Li",
        "given_name": "Xichun"
      },
      {
        "surname": "Meng",
        "given_name": "Zuqiang"
      }
    ]
  },
  {
    "title": "Multi-attention augmented network for single image super-resolution",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108349",
    "abstract": "How to improve the representational power of visual features extracted by deep convolutional neural networks is of crucial importance for high-quality image super-resolution. To address this issue, we propose a multi-attention augmented network, which mainly consists of content-, orientation- and position-aware modules. Specifically, we develop an attention augmented U-net structure to form the content-aware module in order to learn and combine multi-scale informative features within a large receptive field. To better reconstruct image details in different directions, we design a set of pre-defined sparse kernels to construct the orientation-aware module, which can extract more representative multi-orientation features and enhance the discriminative capacity in stacked convolutional stages. Then these extracted features are adaptively fused through channel attention mechanism. In upscale stage, the position-aware module adopts a novel self-attention to reweight the element-wise value of final low-resolution feature maps, for further suppressing the possible artifacts. Experimental results demonstrate that our method obtains better reconstruction accuracy and perceptual quality against state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100529X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Construct (python library)",
      "Convolutional neural network",
      "Discriminative model",
      "Feature (linguistics)",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Orientation (vector space)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Rui"
      },
      {
        "surname": "Zhang",
        "given_name": "Heng"
      },
      {
        "surname": "Liu",
        "given_name": "Jixin"
      }
    ]
  },
  {
    "title": "Feature flow: In-network feature flow estimation for video object detection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108323",
    "abstract": "Optical flow, which expresses pixel displacement, is widely used in many computer vision tasks to provide pixel-level motion information. However, with the remarkable progress of the convolutional neural network, recent state-of-the-art approaches are proposed to solve problems directly on feature-level. Since the displacement of feature vector is not consistent with the pixel displacement, a common approach is to forward optical flow to a neural network and fine-tune this network on the task dataset. With this method, they expect the fine-tuned network to produce tensors encoding feature-level motion information. In this paper, we rethink about this de facto paradigm and analyze its drawbacks in the video object detection task. To mitigate these issues, we propose a novel network (IFF-Net) with an In-network Feature Flow estimation module (IFF module) for video object detection. Without resorting to pre-training on any additional dataset, our IFF module is able to directly produce feature flow which indicates the feature displacement. Our IFF module consists of a shallow module, which shares the features with the detection branches. This compact design enables our IFF-Net to accurately detect objects, while maintaining a fast inference speed. Furthermore, we propose a transformation residual loss (TRL) based on self-supervision, which further improves the performance of our IFF-Net. Our IFF-Net outperforms existing methods and achieves new state-of-the-art performance on ImageNet VID.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005033",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Flow (mathematics)",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Optical flow",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Jin",
        "given_name": "Ruibing"
      },
      {
        "surname": "Lin",
        "given_name": "Guosheng"
      },
      {
        "surname": "Wen",
        "given_name": "Changyun"
      },
      {
        "surname": "Wang",
        "given_name": "Jianliang"
      },
      {
        "surname": "Liu",
        "given_name": "Fayao"
      }
    ]
  },
  {
    "title": "Gaussian-guided feature alignment for unsupervised cross-subject adaptation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108332",
    "abstract": "Human activities recognition (HAR) and human intent recognition (HIR) are important for medical diagnosis and human-robot interaction. HAR and HIR usually rely on the signals of some wearable sensors, such as inertial measurement unit (IMU), but these signals may be user-dependent, which degrades the performance of the recognition algorithm on new subjects. Traditional supervised learning methods require labeling signals and training specific classifiers for each new subject, which is burdensome. To deal with this problem, this paper proposes a novel non-adversarial cross-subject adaptation method called Gaussian-guided feature alignment (GFA). The proposed GFA metric quantifies the discrepancy between the labeled features of source subjects and the unlabeled features of target subjects so that minimizing the GFA metric leads to the alignment of the source and target features. The GFA metric is estimated by calculating the divergence between the feature distribution and Gaussian distribution, as well as the mean squared error of the mean and variance between source and target features. This paper analytically proves the effect of the GFA metric and validates its performance using three public human activity datasets. Experimental results show that the proposed GFA achieves 1% higher target classification accuracy and 0.5% lower variance than state-of-the-art methods in case of cross-subject validation. These results indicate that the proposed GFA is feasible for improving the generalization of the HAR and HIR.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005124",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Divergence (linguistics)",
      "Economics",
      "Engineering",
      "Feature (linguistics)",
      "Feature vector",
      "Gaussian",
      "Generalization",
      "Inertial measurement unit",
      "Linguistics",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Performance metric",
      "Philosophy",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Kuangen"
      },
      {
        "surname": "Chen",
        "given_name": "Jiahong"
      },
      {
        "surname": "Wang",
        "given_name": "Jing"
      },
      {
        "surname": "Leng",
        "given_name": "Yuquan"
      },
      {
        "surname": "de Silva",
        "given_name": "Clarence W."
      },
      {
        "surname": "Fu",
        "given_name": "Chenglong"
      }
    ]
  },
  {
    "title": "ProCAN: Progressive growing channel attentive non-local network for lung nodule classification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108309",
    "abstract": "Lung cancer classification in screening computed tomography (CT) scans is one of the most crucial tasks for early detection of this disease. Many lives can be saved if we are able to accurately classify malignant/cancerous lung nodules. Consequently, several deep learning based models have been proposed recently to classify lung nodules as malignant or benign. Nevertheless, the large variation in the size and heterogeneous appearance of the nodules makes this task an extremely challenging one. We propose a new Progressive Growing Channel Attentive Non-Local (ProCAN) network for lung nodule classification. The proposed method addresses this challenge from three different aspects. First, we enrich the Non-Local network by adding channel-wise attention capability to it. Second, we apply Curriculum Learning principles, whereby we first train our model on easy examples before hard ones. Third, as the classification task gets harder during the Curriculum learning, our model is progressively grown to increase its capability of handling the task at hand. We examined our proposed method on two different public datasets and compared its performance with state-of-the-art methods in the literature. The results show that the ProCAN model outperforms state-of-the-art methods and achieves an AUC of 98.05% and an accuracy of 95.28% on the LIDC-IDRI dataset. Moreover, we conducted extensive ablation studies to analyze the contribution and effects of each new component of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004891",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Deep learning",
      "Economics",
      "Lung cancer",
      "Machine learning",
      "Management",
      "Medicine",
      "Nodule (geology)",
      "Paleontology",
      "Pathology",
      "Pattern recognition (psychology)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Al-Shabi",
        "given_name": "Mundher"
      },
      {
        "surname": "Shak",
        "given_name": "Kelvin"
      },
      {
        "surname": "Tan",
        "given_name": "Maxine"
      }
    ]
  },
  {
    "title": "Neighborhood preserving embedding on Grassmann manifold for image-set analysis",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108335",
    "abstract": "Modeling image sets as points on Grassmann manifold has attracted increasing interests in computer vision community and has been applied to many applications. However, such approaches have suffered from the limitation that high computational cost on Grassmann manifold must be involved, especially high-dimensional ones. In this paper, we propose an unsupervised robust dimensionality reduction algorithm for Grassmann manifold based on Neighborhood Preserving Embedding (GNPE). We first introduce two strategies to construct the coefficients-based similarity graph to eliminate the effects of errors. Then, a projection is learned from the high-dimensional Grassmann manifold to the relative low-dimensional one with more discriminative capability, where the local neighborhood structure is well preserved. To address the issue that the estimated similarity graph is unreliable with noise and outliers, we further propose a unified learning framework which performs similarity learning and projection learning simultaneously. By leveraging the interactions between these two essential tasks, we can capture accurate structures and learn discriminative projections. The proposed method can be optimized by an efficient iterative algorithm. Experiments on various image set classification and clustering tasks clearly show that our model achieves consistent improvements in terms of both effectiveness and efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100515X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Dimensionality reduction",
      "Discriminative model",
      "Embedding",
      "Engineering",
      "Graph",
      "Graph embedding",
      "Grassmannian",
      "Image (mathematics)",
      "Manifold (fluid mechanics)",
      "Manifold alignment",
      "Mathematics",
      "Mechanical engineering",
      "Nonlinear dimensionality reduction",
      "Outlier",
      "Pattern recognition (psychology)",
      "Projection (relational algebra)",
      "Similarity (geometry)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Dong"
      },
      {
        "surname": "Shen",
        "given_name": "Xiaobo"
      },
      {
        "surname": "Sun",
        "given_name": "Quansen"
      },
      {
        "surname": "Gao",
        "given_name": "Xizhan"
      },
      {
        "surname": "Ren",
        "given_name": "Zhenwen"
      }
    ]
  },
  {
    "title": "Scale-aware heatmap representation for human pose estimation",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.12.018",
    "abstract": "The performance of multi-person pose estimation is seriously affected by scale variation. Extensive works have been devoted to reducing the effect by modifying convolutional network structure or loss function, but little attention has been paid to the problem in the construction of heatmaps. In this paper, we focus on the scale variation of keypoints within heatmap generation and propose a novel method called scale-aware heatmap generator, which constructs a customized heatmap for each type of keypoints based on their relative scales. In addition, we design a weight-redistributed loss function to facilitate the detection of keypoints that are hard to identify. Our approach outperforms the baseline by nearly 2.5% in average precision and performs on par with the state-of-the-art result in bottom-up pose estimation with multi-scale testing (69.4% AP) on the COCO test-dev dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521004499",
    "keywords": [
      "Artificial intelligence",
      "Astrophysics",
      "Biology",
      "Computer science",
      "Convolutional neural network",
      "Engineering",
      "Estimation",
      "Evolutionary biology",
      "Focus (optics)",
      "Function (biology)",
      "Generator (circuit theory)",
      "Law",
      "Machine learning",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Pose",
      "Power (physics)",
      "Quantum mechanics",
      "Representation (politics)",
      "Scale (ratio)",
      "Systems engineering",
      "Variation (astronomy)"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Han"
      },
      {
        "surname": "Du",
        "given_name": "Congju"
      },
      {
        "surname": "Yu",
        "given_name": "Li"
      }
    ]
  },
  {
    "title": "Non-stationary, online variational Bayesian learning, with circular variables",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108340",
    "abstract": "We introduce an online variational Bayesian model for tracking changes in a non-stationary, multivariate, temporal signal, using as an example the changing frequency and amplitude of a noisy sinusoidal signal over time. The model incorporates each observation as it arrives and then discards it, and places priors over precision hyperparameters to ensure that (i) the posterior probability distributions do not become overly tight, which would impede its ability to recognise and track changes, and (ii) no values in the system are able to continuously increase and hence exceed the numerical representation of the programming language. It is thus able to perform truly online processing for an infinitely long set of observations. Only a single round of updates in the variational Bayesian scheme per observation is used, and the complexity of the algorithm is constant in time. The proposed method is demonstrated on a large number of synthetic datasets, comparing the results from the full model (with precision hyperparameters as variables with priors) with those from the base model where the precision hyperparameters are fixed values. The full model is also demonstrated on a set of real climate data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005203",
    "keywords": [
      "Artificial intelligence",
      "Bayesian probability",
      "Computer science",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Christmas",
        "given_name": "J."
      }
    ]
  },
  {
    "title": "Context extraction module for deep convolutional neural networks",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108284",
    "abstract": "Convolutional layers convolve the input feature maps to generate valuable output features, and they help deep learning methods significantly in solving complex problems. In order to tackle problems efficiently, deep learning solutions should ensure that the parameters of the model do not increase significantly with the complexity of the problem. Pointwise convolutions are primarily used for parameter reduction in many deep learning architectures. They are convolutional filters of kernel size 1 × 1 . The pointwise convolution, however, ignores the spatial information around the points it is processing. This design is by choice, in order to reduce the overall parameters and computations. However, we hypothesize that this shortcoming of pointwise convolution has a significant impact on network performance. We propose a novel alternative design for pointwise convolution, which uses spatial information from the input efficiently. Our approach extracts spatial context information from the input at two scales and further refines the extracted context based on the channel importance. Finally, we add the refined context to the output of the pointwise convolution. This is the first work that improves pointwise convolution by incorporating context information. Our design significantly improves the performance of the networks without substantially increasing the number of parameters and computations. We perform experiments on coarse/fine-grained image classification, few-shot fine-grained classification, and on object detection. We further perform various ablation experiments to validate the significance of the different components used in our design. Lastly, we show experimentally that our proposed technique can be combined with existing state-of-the-art network performance improvement approaches to further improve the network performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004647",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Combinatorics",
      "Computation",
      "Computer science",
      "Context (archaeology)",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Deep learning",
      "Geometry",
      "Kernel (algebra)",
      "Mathematical analysis",
      "Mathematics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Pointwise",
      "Reduction (mathematics)"
    ],
    "authors": [
      {
        "surname": "Singh",
        "given_name": "Pravendra"
      },
      {
        "surname": "Mazumder",
        "given_name": "Pratik"
      },
      {
        "surname": "Namboodiri",
        "given_name": "Vinay P."
      }
    ]
  },
  {
    "title": "A multimodal attention fusion network with a dynamic vocabulary for TextVQA",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108214",
    "abstract": "Visual question answering (VQA) is a well-known problem in computer vision. Recently, Text-based VQA tasks are getting more and more attention because text information is very important for image understanding. The key to this task is to make good use of text information in the image. In this work, we propose an attention-based encoder-decoder network that combines the multimodal information of visual, linguistic, and location features together. By using the attention mechanism to focus on key features to the question, our multimodal feature fusion can provide more accurate information to improve the performance. Furthermore, we present a decoder with attention map loss, which can not only predict complex answers but also deal with a dynamic vocabulary to reduce the decoding space. Compared with softmax-based cross entropy loss which can only handle a fixed-length vocabulary, the attention map loss significantly improves the accuracy and efficiency. Our method achieved the first place of all three tasks in the ICDAR2019 robust reading challenge on scene text visual question answering (ST-VQA).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003952",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Decoding methods",
      "Deep learning",
      "Encoder",
      "Focus (optics)",
      "Key (lock)",
      "Linguistics",
      "Machine learning",
      "Natural language processing",
      "Operating system",
      "Optics",
      "Philosophy",
      "Physics",
      "Question answering",
      "Softmax function",
      "Speech recognition",
      "Telecommunications",
      "Vocabulary"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Jiajia"
      },
      {
        "surname": "Du",
        "given_name": "Jun"
      },
      {
        "surname": "Wang",
        "given_name": "Fengren"
      },
      {
        "surname": "Yang",
        "given_name": "Chen"
      },
      {
        "surname": "Jiang",
        "given_name": "Xinzhe"
      },
      {
        "surname": "Hu",
        "given_name": "Jinshui"
      },
      {
        "surname": "Yin",
        "given_name": "Bing"
      },
      {
        "surname": "Zhang",
        "given_name": "Jianshu"
      },
      {
        "surname": "Dai",
        "given_name": "Lirong"
      }
    ]
  },
  {
    "title": "Integrated generalized zero-shot learning for fine-grained classification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108246",
    "abstract": "Embedding learning (EL) and feature synthesizing (FS) are two of the popular categories of fine-grained GZSL methods. EL or FS using global features cannot discriminate fine details in the absence of local features. On the other hand, EL or FS methods exploiting local features either neglect direct attribute guidance or global information. Consequently, neither method performs well. In this paper, we propose to explore global and direct attribute-supervised local visual features for both EL and FS categories in an integrated manner for fine-grained GZSL. The proposed integrated network has an EL sub-network and a FS sub-network. Consequently, the proposed integrated network can be tested in two ways. We propose a novel two-step dense attention mechanism to discover attribute-guided local visual features. We introduce new mutual learning between the sub-networks to exploit mutually beneficial information for optimization. Moreover, we propose to compute source-target class similarity based on mutual information and transfer-learn the target classes to reduce bias towards the source domain during testing. We demonstrate that our proposed method outperforms contemporary methods on benchmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100426X",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer security",
      "Domain (mathematical analysis)",
      "Embedding",
      "Exploit",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Mutual information",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Shermin",
        "given_name": "Tasfia"
      },
      {
        "surname": "Teng",
        "given_name": "Shyh Wei"
      },
      {
        "surname": "Sohel",
        "given_name": "Ferdous"
      },
      {
        "surname": "Murshed",
        "given_name": "Manzur"
      },
      {
        "surname": "Lu",
        "given_name": "Guojun"
      }
    ]
  },
  {
    "title": "Effective and efficient pixel-level detection for diverse video copy-move forgery types",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108286",
    "abstract": "Video copy-move forgery detection (VCMFD) is a significant and greatly challenging task due to a variety of difficulties, including a huge amount of video information, diverse forgery types, rich forgery objects, and homogenous forgery sources. These difficulties raise four unresolved key challenges in VCMFD: i) ineffective detection in some popular forgery cases; ii) inefficient matching in processing numerous video pixels with hundred-dimensional features under dozens of matching iterations; iii) high false positive (FP ) in detecting forgery videos; iv) low trade-off of efficiency and effectiveness in filling forgery region, and even failing in indicating forgeries at the pixel level. In this paper, a novel VCMFD method is proposed to address these issues: i) an innovatively improved SIFT structure that can address the thorough feature extraction in all video copy-move forgery cases; ii) a novel fast keypoint-label matching (FKLM) algorithm is proposed that creates some keypoint-label groups so that every high-dimensional feature is assigned into one of these groups. As a result, matching of video pixels can be directly done on a small number of keypoint-label groups only, leading to a nearly 500% raise in matching efficiency; iii) a new coarse-to-fine filtering relying on intrinsic attributes of exact keypoint-matches is designed to more effectively reduce the false keypoint-matches; iv) the adaptive block filling relying on true keypoint-matches contributes to the accurate and efficient suspicious region filling, even at the pixel level. Finally, the suspicious region locations with the forgery vision persistence concept indicate forgery videos. Compared to the state-of-art methods, the experiments show that our proposed method achieves the best detection accuracy, lowest FP , and improved at least 16% and 8% of F 1 scores on the GRIP 2.0 dataset and a combination of SULFA 2.0 & REWIND datasets. Furthermore, the proposed method is with low computational time (4.45 s/Mpixels), which is about 1/2-1/3 times of the latest DFMI-BM (8.02 s/Mpixels) and PM-2D (13.1 s/Mpixels) methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004660",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Feature (linguistics)",
      "Geometry",
      "Image (mathematics)",
      "Key (lock)",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Scale-invariant feature transform",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Zhong",
        "given_name": "Jun-Liu"
      },
      {
        "surname": "Gan",
        "given_name": "Yan-Fen"
      },
      {
        "surname": "Vong",
        "given_name": "Chi-Man"
      },
      {
        "surname": "Yang",
        "given_name": "Ji-Xiang"
      },
      {
        "surname": "Zhao",
        "given_name": "Jing-Hong"
      },
      {
        "surname": "Luo",
        "given_name": "Jia-Hua"
      }
    ]
  },
  {
    "title": "Support structure representation learning for sequential data clustering",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108326",
    "abstract": "Sequential data clustering is a challenging task in data mining (e.g., motion recognition and video segmentation). For good performance in dealing with complex local correlation and high-dimensional structure of sequential data, representation based methods have become one of the hot topics for sequential data clustering, in which subspace clustering is a representative tool. Subspace clustering methods divide the sequence into disjoint segments according to a locally continuous and connected representation of raw data. Although the subspace clustering methods maintain the successive property of sequential data well, there exist redundant connections in the intersection of two subsequences, which will destroy the integrity of a cluster and easily cause the chained partition of the sequence. So it is necessary to learn a more specific structure representation of a sequence to preserves both sequential information and efficient connections. Besides, the representation that conducive to clustering should have sparsity and connectivity under some assumptions. To this end, we propose a novel method to learn the support structure representation of sequence, which can extract sufficient information about instances and get the compact structure of sequential data. Furthermore, a new subspace clustering method is proposed based on the representation based method. Theoretical analysis and experimental results show the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005069",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "CURE data clustering algorithm",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Computer science",
      "Constrained clustering",
      "Correlation clustering",
      "Data mining",
      "Data stream clustering",
      "Fuzzy clustering",
      "Genetics",
      "Law",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Sequence (biology)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xiumei"
      },
      {
        "surname": "Guo",
        "given_name": "Dingning"
      },
      {
        "surname": "Cheng",
        "given_name": "Peitao"
      }
    ]
  },
  {
    "title": "Discrete online cross-modal hashing",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108262",
    "abstract": "With the prevalence of multimedia content on the Web which usually continuously comes in a stream fashion, online cross-modal hashing methods have attracted extensive interest in recent years. However, most online hashing methods adopt a relaxation strategy or real-valued auxiliary variable strategy to avoid complex optimization of hash codes, leading to large quantization errors. In this paper, based on Discrete Latent Factor model-based cross-modal Hashing (DLFH), we propose a novel cross-modal online hashing method, i.e., Discrete Online Cross-modal Hashing (DOCH). To generate uniform high-quality hash codes of different modal, DOCH not only directly exploits the similarity between newly coming data and old existing data in the Hamming space, but also utilizes the fine-grained semantic information by label embedding. Moreover, DOCH can discretely learn hash codes by an efficient optimization algorithm. Extensive experiments conducted on two real-world datasets demonstrate the superiority of DOCH.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004428",
    "keywords": [
      "Algorithm",
      "Block code",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Decoding methods",
      "Double hashing",
      "Dynamic perfect hashing",
      "Feature hashing",
      "Hamming code",
      "Hamming space",
      "Hash function",
      "Hash table",
      "Locality-sensitive hashing",
      "Modal",
      "Polymer chemistry",
      "Theoretical computer science",
      "Universal hashing"
    ],
    "authors": [
      {
        "surname": "Zhan",
        "given_name": "Yu-Wei"
      },
      {
        "surname": "Wang",
        "given_name": "Yongxin"
      },
      {
        "surname": "Sun",
        "given_name": "Yu"
      },
      {
        "surname": "Wu",
        "given_name": "Xiao-Ming"
      },
      {
        "surname": "Luo",
        "given_name": "Xin"
      },
      {
        "surname": "Xu",
        "given_name": "Xin-Shun"
      }
    ]
  },
  {
    "title": "A black-box adversarial attack strategy with adjustable sparsity and generalizability for deep image classifiers",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108279",
    "abstract": "Constructing adversarial perturbations for deep neural networks is an important direction of research. Crafting image-dependent adversarial perturbations using white-box feedback has hitherto been the norm for such adversarial attacks. However, black-box attacks are much more practical for real-world applications. Universal perturbations applicable across multiple images are gaining popularity due to their innate generalizability. There have also been efforts to restrict the perturbations to a few pixels in the image. This helps to retain visual similarity with the original images making such attacks hard to detect. This paper marks an important step that combines all these directions of research. We propose the DEceit algorithm for constructing effective universal pixel-restricted perturbations using only black-box feedback from the target network. We conduct empirical investigations using the ImageNet validation set on the state-of-the-art deep neural classifiers by varying the number of pixels to be perturbed from a meager 10 pixels to as high as all pixels in the image. We find that perturbing only about 10% of the pixels in an image using DEceit achieves a commendable and highly transferable Fooling Rate while retaining the visual quality. We further demonstrate that DEceit can be successfully applied to image-dependent attacks as well. In both sets of experiments, we outperform several state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004593",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Artificial neural network",
      "Black box",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Deep neural networks",
      "Generalizability theory",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pixel",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Ghosh",
        "given_name": "Arka"
      },
      {
        "surname": "Mullick",
        "given_name": "Sankha Subhra"
      },
      {
        "surname": "Datta",
        "given_name": "Shounak"
      },
      {
        "surname": "Das",
        "given_name": "Swagatam"
      },
      {
        "surname": "Das",
        "given_name": "Asit Kr."
      },
      {
        "surname": "Mallipeddi",
        "given_name": "Rammohan"
      }
    ]
  },
  {
    "title": "Robust face alignment by dual-attentional spatial-aware capsule networks",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108297",
    "abstract": "Face alignment in-the-wild still faces great challenges due to that i) partial occlusion blurs the inter-features spatial relations of faces and ii) traditional CNN makes the network more difficult to capture the spatial positional relations between landmarks. To address the issues above, we propose a face alignment algorithm named Dual-attentional Spatial-aware Capsule Network (DSCN). Firstly, the spatial-aware module builds a more accurate inter-features spatial constrained model with the hourglass capsule network (HGCaps) as the backbone, which can effectively enhance its robustness against occlusions. Then, two sorts of attention mechanisms, namely capsule attention and spatial attention, are added to the attention-guided module to make the network focus more on the advantageous features and suppress other unrelated ones for more effective feature recalibration. Our method achieves 1.08% failure rate on the COFW dataset, which is much lower than the current state-of-the-art algorithms. The mean error under 300W dataset and WFLW dataset are respectively 3.91% and 5.66%, which shows that DSCN is more robust to occlusion and outperforms state-of-the-art methods in the literature.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004775",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Attention network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Dual (grammatical number)",
      "Face (sociological concept)",
      "Focus (optics)",
      "Gene",
      "Literature",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Robustness (evolution)",
      "Social science",
      "Sociology",
      "Spatial relation"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Jinyan"
      },
      {
        "surname": "Li",
        "given_name": "Jing"
      },
      {
        "surname": "Du",
        "given_name": "Bo"
      },
      {
        "surname": "Wu",
        "given_name": "Jia"
      },
      {
        "surname": "Wan",
        "given_name": "Jun"
      },
      {
        "surname": "Xiao",
        "given_name": "Yafu"
      }
    ]
  },
  {
    "title": "Explainable scale distillation for hyperspectral image classification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108316",
    "abstract": "The land-covers within an observed remote sensing scene are usually of different scales; therefore, the ensemble of multi-scale information is a commonly used strategy to achieve more accurate scene interpretation; however, this process suffers from being time-consuming. In terms of this issue, this paper proposes a scale distillation network to explore the possibility that single-scale classification network can achieve the same (or even better) classification performance compared with multi-scale one. The proposed scale distillation network consists of a cumbersome multi-scale teacher network and a lightweight single-scale student network. The former is trained for multi-scale information learning, and the latter improves the classification accuracy by accepting the knowledge from the multi-scale teacher network and its true label. The experimental results show the advantages of scale distillation on hyperspectral image classification. The single-scale student network can even achieve higher evaluation accuracy than the multi-scale teacher network. In addition, a faithful explainable scale network is designed to visually explain the trained scale distillation network. The traditional deep neural network is a black-box and lacks interpretability. The explanation of the trained network can explore more hidden information from the predictions. We visually explain the prediction results of scale distillation network, and the results show that the explainable scale network can more precisely analyze the relationship between the learned scale features and the land-cover categories. Moreover, the possible application of the explainable scale network on classification is further discussed in this study.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004969",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Distillation",
      "Hyperspectral imaging",
      "Interpretability",
      "Machine learning",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Cheng"
      },
      {
        "surname": "Fang",
        "given_name": "Li"
      },
      {
        "surname": "Lv",
        "given_name": "Zhiyong"
      },
      {
        "surname": "Zhao",
        "given_name": "Minghua"
      }
    ]
  },
  {
    "title": "Multi-task framework based on feature separation and reconstruction for cross-modal retrieval",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108217",
    "abstract": "Cross-modal retrieval has become a hot research topic in both computer vision and natural language processing areas. Learning intermediate common space for features of different modalities has become one of mainstream methods. In this paper, we propose a novel multi-task framework based on feature separation and reconstruction (mFSR) for cross-modal retrieval based on common space learning methods, which introduces feature separation module to deal with information asymmetry between different modalities, and introduces image and text reconstruction module to improve the quality of feature separation module. Extensive experiments on MS-COCO and Flickr30K datasets demonstrate that feature separation and specific information reconstruction can significantly improve the baseline performance of cross-modal image-caption retrieval.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003988",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature vector",
      "Information retrieval",
      "Linguistics",
      "Machine learning",
      "Modal",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Polymer chemistry",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Li"
      },
      {
        "surname": "Wu",
        "given_name": "Xiangqian"
      }
    ]
  },
  {
    "title": "Face mask recognition from audio: The MASC database and an overview on the mask challenge",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108361",
    "abstract": "The sudden outbreak of COVID-19 has resulted in tough challenges for the field of biometrics due to its spread via physical contact, and the regulations of wearing face masks. Given these constraints, voice biometrics can offer a suitable contact-less biometric solution; they can benefit from models that classify whether a speaker is wearing a mask or not. This article reviews the Mask Sub-Challenge (MSC) of the INTERSPEECH 2020 COMputational PARalinguistics challengE (ComParE), which focused on the following classification task: Given an audio chunk of a speaker, classify whether the speaker is wearing a mask or not. First, we report the collection of the Mask Augsburg Speech Corpus (MASC) and the baseline approaches used to solve the problem, achieving a performance of 71.8 % Unweighted Average Recall (UAR). We then summarise the methodologies explored in the submitted and accepted papers that mainly used two common patterns: (i) phonetic-based audio features, or (ii) spectrogram representations of audio combined with Convolutional Neural Networks (CNNs) typically used in image processing. Most approaches enhance their models by adapting ensembles of different models and attempting to increase the size of the training data using various techniques. We review and discuss the results of the participants of this sub-challenge, where the winner scored a UAR of 80.1 % . Moreover, we present the results of fusing the approaches, leading to a UAR of 82.6 % . Finally, we present a smartphone app that can be used as a proof of concept demonstration to detect in real-time whether users are wearing a face mask; we also benchmark the run-time of the best models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005410",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Convolutional neural network",
      "Economics",
      "Face (sociological concept)",
      "Facial recognition system",
      "Field (mathematics)",
      "Machine learning",
      "Management",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Social science",
      "Sociology",
      "Spectrogram",
      "Speech recognition",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Mohamed",
        "given_name": "Mostafa M."
      },
      {
        "surname": "Nessiem",
        "given_name": "Mina A."
      },
      {
        "surname": "Batliner",
        "given_name": "Anton"
      },
      {
        "surname": "Bergler",
        "given_name": "Christian"
      },
      {
        "surname": "Hantke",
        "given_name": "Simone"
      },
      {
        "surname": "Schmitt",
        "given_name": "Maximilian"
      },
      {
        "surname": "Baird",
        "given_name": "Alice"
      },
      {
        "surname": "Mallol-Ragolta",
        "given_name": "Adria"
      },
      {
        "surname": "Karas",
        "given_name": "Vincent"
      },
      {
        "surname": "Amiriparian",
        "given_name": "Shahin"
      },
      {
        "surname": "Schuller",
        "given_name": "Björn W."
      }
    ]
  },
  {
    "title": "Fine-grained action recognition using dynamic kernels",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108282",
    "abstract": "Fine-grained action recognition involves comparison of similar actions of variable-length size consisting of subtle interactions between human and specific objects. Hence, we propose a dynamic kernel-based approach to handle the variable-length patterns for effective recognition of fine-grained actions. Initially, we extract local spatio-temporal features for each video to capture appearance and motion information effectively. An action-independent Gaussian mixture model (AIGMM) is trained on the extracted features of all fine-grained actions to analyze spatio-temporal information and preserve the local similarities among fine-grained actions. Then, the statistics of AIGMM, namely, mean, covariance, and posteriors are used to build the kernels for finding the similarity between any two fine-grained actions by mapping statistics to kernel feature space. We demonstrate the effectiveness of proposed approach using three dynamic kernels i.e., GMM mean interval kernel, supervector kernel, intermediate matching kernel on four varieties of fine-grained action datasets, namely, MERL, JIGSAWS, KSCGR, and MPII cooking2",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004623",
    "keywords": [
      "Action (physics)",
      "Action recognition",
      "Artificial intelligence",
      "Class (philosophy)",
      "Combinatorics",
      "Computer science",
      "Covariance",
      "Feature (linguistics)",
      "Gaussian",
      "Image (mathematics)",
      "Interval (graph theory)",
      "Kernel (algebra)",
      "Linguistics",
      "Matching (statistics)",
      "Mathematical analysis",
      "Mathematics",
      "Mixture model",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Similarity (geometry)",
      "Statistics",
      "Variable (mathematics)"
    ],
    "authors": [
      {
        "surname": "Yenduri",
        "given_name": "Sravani"
      },
      {
        "surname": "Perveen",
        "given_name": "Nazil"
      },
      {
        "surname": "Chalavadi",
        "given_name": "Vishnu"
      },
      {
        "surname": "C",
        "given_name": "Krishna Mohan"
      }
    ]
  },
  {
    "title": "Learning-based resilience guarantee for multi-UAV collaborative QoS management",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108166",
    "abstract": "Unmanned and intelligent technologies are the future development trend in the business field. It is of great significance for the connotation analysis and application characterization of massive interactive data. Particularly, during major epidemics or disasters, how to provide business services safely and securely is crucial. Specifically, providing users with resilient and guaranteed communication services is a challenging business task when the communication facilities are damaged. Unmanned aerial vehicles (UAVs), with flexible deployment and high maneuverability, can be used to serve as aerial base stations (BSs) to establish emergency networks. However, it is challenging to control multiple UAVs to provide efficient and fair communication quality of service (QoS) to users due to their limited communication service capabilities. In this paper, we propose a learning-based resilience guarantee framework for multi-UAV collaborative QoS management. We formulate this problem as a partial observable Markov decision process and solve it with proximal policy optimization (PPO), which is a policy-based deep reinforcement learning method. A centralized training and decentralized execution paradigm is used, where the experience collected by all UAVs is used to train the shared control policy. Each UAV takes actions based on the partial environment information it observes. In addition, the design of the reward function considers the average and variance of the communication QoS of all users. Extensive simulations are conducted for performance evaluation. The simulation results indicate that (1) the trained policies can adapt to different scenarios and provide resilient and guaranteed communication QoS to users, (2) increasing the number of UAVs can compensate for the lack of service capabilities of UAVs, (3) when UAVs have local communication service capabilities, the policies trained with PPO have better performance compared with the policies trained with other algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003538",
    "keywords": [
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Distributed computing",
      "Economics",
      "Economy",
      "Engineering",
      "Markov decision process",
      "Markov process",
      "Mathematics",
      "Operating system",
      "Physics",
      "Process (computing)",
      "Quality of service",
      "Reinforcement learning",
      "Resilience (materials science)",
      "Service (business)",
      "Statistics",
      "Systems engineering",
      "Task (project management)",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Bai",
        "given_name": "Chengchao"
      },
      {
        "surname": "Yan",
        "given_name": "Peng"
      },
      {
        "surname": "Yu",
        "given_name": "Xiaoqiang"
      },
      {
        "surname": "Guo",
        "given_name": "Jifeng"
      }
    ]
  },
  {
    "title": "Multi-label sampling based on local label imbalance",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108294",
    "abstract": "Class imbalance is an inherent characteristic of multi-label data that hinders most multi-label learning methods. One efficient and flexible strategy to deal with this problem is to employ sampling techniques before training a multi-label learning model. Although existing multi-label sampling approaches alleviate the global imbalance of multi-label datasets, it is actually the imbalance level within the local neighbourhood of minority class examples that plays a key role in performance degradation. To address this issue, we propose a novel measure to assess the local label imbalance of multi-label datasets, as well as two multi-label sampling approaches, namely Multi-Label Synthetic Oversampling based on Local label imbalance (MLSOL) and Multi-Label Undersampling based on Local label imbalance (MLUL). By considering all informative labels, MLSOL creates more diverse and better labeled synthetic instances for difficult examples, while MLUL eliminates instances that are harmful to their local region. Experimental results on 13 multi-label datasets demonstrate the effectiveness of the proposed measure and sampling approaches for a variety of evaluation metrics, particularly in the case of an ensemble of classifiers trained on repeated samples of the original data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100474X",
    "keywords": [
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Class (philosophy)",
      "Computer network",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Data mining",
      "Filter (signal processing)",
      "Key (lock)",
      "Machine learning",
      "Measure (data warehouse)",
      "Multi-label classification",
      "Oversampling",
      "Pattern recognition (psychology)",
      "Sampling (signal processing)",
      "Undersampling"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Bin"
      },
      {
        "surname": "Blekas",
        "given_name": "Konstantinos"
      },
      {
        "surname": "Tsoumakas",
        "given_name": "Grigorios"
      }
    ]
  },
  {
    "title": "Graph regularized locally linear embedding for unsupervised feature selection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108299",
    "abstract": "As one of the important dimensionality reduction techniques, unsupervised feature selection (UFS) has enjoyed amounts of popularity over the last few decades, which can not only improve learning performance, but also enhance interpretability and reduce computational costs. The existing UFS methods often model the data in the original feature space, which cannot fully exploit the discriminative information. In this paper, to address this issue, we investigate how to strengthen the relationship between UFS and the feature subspace, so as to select relevant features more straightforwardly and effectively. Methodologically, a novel UFS approach, referred to as Graph Regularized Local Linear Embedding (GLLE), is proposed by integrating local linear embedding (LLE) and manifold regularization constrained in feature subspace into a unified framework. To be more specific, we explicitly define a feature selection matrix composed of 0 and 1, which can realize the process of UFS. For the purpose of modelling the feature selection matrix, we propose to preserve the local linear reconstruction relationship among neighboring data points in the feature subspace, which corresponds to LLE constrained in the feature subspace. To make the feature selection matrix more accurate, we propose to use manifold regularization as an assistant of LLE to find the relevant and representative features such that the selected features can make each sample under the feature subspace be accordance with the manifold assumption. A tailored iterative algorithm based on Alternative Direction Method of Multipliers (ADMM) is designed to solve the proposed optimization problem. Extensive experiments on twelve real-world benchmark datasets are conducted, and the more promising results are achieved compared with the state-of-the-arts approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004799",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Dimensionality reduction",
      "Embedding",
      "Feature (linguistics)",
      "Feature selection",
      "Feature vector",
      "Graph",
      "Graph embedding",
      "Interpretability",
      "Laplacian matrix",
      "Linguistics",
      "Mathematics",
      "Nonlinear dimensionality reduction",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Subspace topology",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Miao",
        "given_name": "Jianyu"
      },
      {
        "surname": "Yang",
        "given_name": "Tiejun"
      },
      {
        "surname": "Sun",
        "given_name": "Lijun"
      },
      {
        "surname": "Fei",
        "given_name": "Xuan"
      },
      {
        "surname": "Niu",
        "given_name": "Lingfeng"
      },
      {
        "surname": "Shi",
        "given_name": "Yong"
      }
    ]
  },
  {
    "title": "Supervised discrete hashing for hamming space retrieval",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.01.001",
    "abstract": "Recently, Hamming space retrieval, which realizes the foremost effective constant-time search, is incredibly prevalent in large-scale approximate neighbor retrieval since its high computational potency. However, notwithstanding the exciting progress, the accuracy of extant hashing strategies for Hamming space retrieval continues to be faraway from satisfactory. To handle this issue, this research proposes a novel discrete hashing method named Supervised Hamming Hashing (SHH). Specifically, supervision is carefully tailored to reduce the semantic classification loss, which incorporates label regression strategies and asymmetric similarity preserving scheme. Further, a unique hash code fusion strategy and a customized discrete optimization algorithm are designed for optimizing hash codes, thus heightening the potency and precision of the hash codes. A large number of experiments carried out on three benchmarks have corroborated the advantageous performance of SHH in Hamming space retrieval.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000010",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Block code",
      "Computer science",
      "Computer security",
      "Decoding methods",
      "Double hashing",
      "Dynamic perfect hashing",
      "Hamming code",
      "Hamming distance",
      "Hamming space",
      "Hash function",
      "Hash table",
      "Locality-sensitive hashing",
      "Nearest neighbor search",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Shaohua"
      },
      {
        "surname": "Kang",
        "given_name": "Xiao"
      },
      {
        "surname": "Liu",
        "given_name": "Fasheng"
      },
      {
        "surname": "Nie",
        "given_name": "Xiushan"
      },
      {
        "surname": "Liu",
        "given_name": "Xingbo"
      }
    ]
  },
  {
    "title": "A fuzzy-rough uncertainty measure to discover bias encoded explicitly or implicitly in features of structured pattern classification datasets",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.01.005",
    "abstract": "The need to measure bias encoded in tabular data that are used to solve pattern recognition problems is widely recognized by academia, legislators and enterprises alike. In previous work, we proposed a bias quantification measure, called fuzzy-rough uncertainty, which relies on the fuzzy-rough set theory. The intuition dictates that protected features should not change the fuzzy-rough boundary regions of a decision class significantly. The extent to which this happens is a proxy for bias expressed as uncertainty in a decision-making context. Our measure’s main advantage is that it does not depend on any machine learning prediction model but a distance function. In this paper, we extend our study by exploring the existence of bias encoded implicitly in non-protected features as defined by the correlation between protected and unprotected attributes. This analysis leads to four scenarios that domain experts should evaluate before deciding how to tackle bias. In addition, we conduct a sensitivity analysis to determine the fuzzy operators and distance function that best capture change in the boundary regions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000058",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Decision boundary",
      "Epistemology",
      "Fuzzy logic",
      "Fuzzy set",
      "Intuition",
      "Machine learning",
      "Mathematics",
      "Measure (data warehouse)",
      "Membership function",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Rough set"
    ],
    "authors": [
      {
        "surname": "Nápoles",
        "given_name": "Gonzalo"
      },
      {
        "surname": "Koutsoviti Koumeri",
        "given_name": "Lisa"
      }
    ]
  },
  {
    "title": "DDBN: Dual detection branch network for semantic diversity predictions",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108315",
    "abstract": "It is well known that detail features and context semantics are conducive to improving object detection performance. However, the current single-prediction detectors do not well incorporate these two types of information together. To alleviate the limitation of single-prediction on the use of multiple types of information, we propose a dual detection branch network (DDBN) with adjacent feature compensation and customized training strategy for semantic diversity predictions. Different from the conventional single-prediction models, our DDBN is in the form of a single model with dual different semantic predictions. In particular, two types of adjacent feature compensations are designed to extract detail and context information from different perspectives. Also, a specialized training strategy is customized for our DDBN to well explore the diversity of predictions for improving the performance of object detection. We conduct extensive experiments on three datasets, i.e., DOTA, MS-COCO, and Pascal-VOC, and the experimental results strongly demonstrate the efficacy of our proposed model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004957",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Dual (grammatical number)",
      "Feature (linguistics)",
      "Linguistics",
      "Literature",
      "Machine learning",
      "Object detection",
      "Paleontology",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Semantic feature",
      "Semantics (computer science)"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Qifeng"
      },
      {
        "surname": "Long",
        "given_name": "Chengjiang"
      },
      {
        "surname": "Zhao",
        "given_name": "Jianhui"
      },
      {
        "surname": "Fu",
        "given_name": "Gang"
      },
      {
        "surname": "Yuan",
        "given_name": "Zhiyong"
      }
    ]
  },
  {
    "title": "Effective fake news video detection using domain knowledge and multimodal data fusion on youtube",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.01.007",
    "abstract": "In the digital age, numerous videos are being actively produced and uploaded online. Simultaneously, fake news videos to attract public attention are also on the rise. Therefore, intensive research is being conducted to detect them. Validating video content is critical for all users as the public is exposed to various fake news videos. This study proposes ways to detect fake news videos effectively using domain knowledge and multimodal data fusion. We use domain knowledge to perform learning by reflecting the potential meaning of comments, helping us detecting fake news videos. We also use the linear combination to efficiently adjust the encoding rate for each characteristic of the video and effectively detect fake news videos. In particular, the domain knowledge improves the model performance by approximately 3% for all test datasets. Consequently, we achieve an F1-score of 0.93, which is higher than those of other comparison models in all the test datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000071",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Domain (mathematical analysis)",
      "Domain knowledge",
      "Fake news",
      "Information retrieval",
      "Internet privacy",
      "Mathematical analysis",
      "Mathematics",
      "Multimedia",
      "Online video",
      "Philosophy",
      "Public domain",
      "Theology",
      "Upload",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Choi",
        "given_name": "Hyewon"
      },
      {
        "surname": "Ko",
        "given_name": "Youngjoong"
      }
    ]
  },
  {
    "title": "Biological eagle eye-based method for change detection in water scenes",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108203",
    "abstract": "Change detection (CD) is an important vision task for autonomous landing of unmanned aerial vehicles (UAV) on water. High-density photoreceptors and lateral inhibition mechanisms have inspired a novel biologic computational method based on structure and properties in eagle eyes as proposed for change detection. We call this method “STabCD,” which ensures spatiotemporal distribution consistency to achieve foreground acquisition, noise reduction, and background adaptability. Therefore, our proposed model responds strongly to object information and suppresses noise and wave textures. Then, we present a cloning method to simulate water scenes and collect a new synthetic dataset (called “Synthetic Boat Sequence”) for UAV vision research. Besides, we utilize synthetic datasets and corresponding real datasets to conduct change detection experiments. The experimental results indicate that: 1) the STabCD model achieves the best results in real or synthetic water landing scenes; and 2) change detection models for UAV can be quantitatively analyzed and tested under challenging synthetic scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321003654",
    "keywords": [
      "Adaptability",
      "Artificial intelligence",
      "Biology",
      "Change detection",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Ecology",
      "Image (mathematics)",
      "Noise (video)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Synthetic data"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xuan"
      },
      {
        "surname": "Duan",
        "given_name": "Haibin"
      },
      {
        "surname": "Li",
        "given_name": "Jingchun"
      },
      {
        "surname": "Deng",
        "given_name": "Yimin"
      },
      {
        "surname": "Wang",
        "given_name": "Fei-Yue"
      }
    ]
  },
  {
    "title": "Privacy-aware supervised classification: An informative subspace based multi-objective approach",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108301",
    "abstract": "Sharing the raw or an abstract representation of a labelled dataset on cloud platforms can potentially expose sensitive information of the data to an adversary, e.g., in the case of an emotion classification task from text, an adversary-agnostic abstract representation of the text data may eventually lead an adversary to identify the demographics of the authors, such as their gender and age. In this paper, we propose a universal defense mechanism against such malicious attempts of stealing sensitive information from data shared on cloud platforms. More specifically, our proposed method employs an informative subspace based multi-objective approach to obtain a sensitive information aware encoding of the data representation. A number of experiments conducted on both standard text and image datasets demonstrate that our proposed approach is able to reduce the effectiveness of the adversarial task (i.e., in other words is able to better protect the sensitive information of the data) without significantly reducing the effectiveness of the primary task itself.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004817",
    "keywords": [
      "Adversarial system",
      "Adversary",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Data mining",
      "Economics",
      "Encoding (memory)",
      "Information sensitivity",
      "Law",
      "Machine learning",
      "Management",
      "Political science",
      "Politics",
      "Programming language",
      "Raw data",
      "Representation (politics)",
      "Subspace topology",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Biswas",
        "given_name": "Chandan"
      },
      {
        "surname": "Ganguly",
        "given_name": "Debasis"
      },
      {
        "surname": "Mukherjee",
        "given_name": "Partha Sarathi"
      },
      {
        "surname": "Bhattacharya",
        "given_name": "Ujjwal"
      },
      {
        "surname": "Hou",
        "given_name": "Yufang"
      }
    ]
  },
  {
    "title": "SARS-Net: COVID-19 detection from chest x-rays by combining graph convolutional network and convolutional neural network",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108255",
    "abstract": "COVID-19 has emerged as one of the deadliest pandemics that has ever crept on humanity. Screening tests are currently the most reliable and accurate steps in detecting severe acute respiratory syndrome coronavirus in a patient, and the most used is RT-PCR testing. Various researchers and early studies implied that visual indicators (abnormalities) in a patient's Chest X-Ray (CXR) or computed tomography (CT) imaging were a valuable characteristic of a COVID-19 patient that can be leveraged to find out virus in a vast population. Motivated by various contributions to open-source community to tackle COVID-19 pandemic, we introduce SARS-Net, a CADx system combining Graph Convolutional Networks and Convolutional Neural Networks for detecting abnormalities in a patient's CXR images for presence of COVID-19 infection in a patient. In this paper, we introduce and evaluate the performance of a custom-made deep learning architecture SARS-Net, to classify and detect the Chest X-ray images for COVID-19 diagnosis. Quantitative analysis shows that the proposed model achieves more accuracy than previously mentioned state-of-the-art methods. It was found that our proposed model achieved an accuracy of 97.60% and a sensitivity of 92.90% on the validation set.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004350",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Coronavirus disease 2019 (COVID-19)",
      "Deep learning",
      "Disease",
      "Environmental health",
      "Graph",
      "Infectious disease (medical specialty)",
      "Machine learning",
      "Medicine",
      "Pandemic",
      "Pathology",
      "Pattern recognition (psychology)",
      "Population",
      "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)",
      "Test set",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Kumar",
        "given_name": "Aayush"
      },
      {
        "surname": "Tripathi",
        "given_name": "Ayush R"
      },
      {
        "surname": "Satapathy",
        "given_name": "Suresh Chandra"
      },
      {
        "surname": "Zhang",
        "given_name": "Yu-Dong"
      }
    ]
  },
  {
    "title": "Hyperspectral super-resolution via coupled tensor ring factorization",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108280",
    "abstract": "Hyperspectral super-resolution (HSR) fuses a low-resolution hyperspectral image (HSI) and a high-resolution multispectral image (MSI) to obtain a high-resolution HSI (HR-HSI). In this paper, we propose a new model called coupled tensor ring factorization (CTRF) for HSR. The proposed CTRF approach simultaneously learns the tensor ring core tensors of the HR-HSI from a pair of HSI and MSI. The CTRF model can separately exploit the low-rank property of each class (Section 3.3), which has not been explored in previous coupled tensor models. Meanwhile, the model inherits the simple representation of coupled matrix/canonical polyadic factorization and flexible low-rank exploration of coupled Tucker factorization. We further introduce spectral nuclear norm regularization to explore the global spectral low-rank property. The experiments demonstrated the advantage of the proposed nuclear norm regularized CTRF model compared to previous matrix/tensor and deep learning methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100460X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Eigenvalues and eigenvectors",
      "Factorization",
      "Hyperspectral imaging",
      "Mathematics",
      "Matrix decomposition",
      "Matrix norm",
      "Multispectral image",
      "Pattern recognition (psychology)",
      "Physics",
      "Pure mathematics",
      "Quantum mechanics",
      "Rank (graph theory)",
      "Tensor (intrinsic definition)"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Wei"
      },
      {
        "surname": "Chen",
        "given_name": "Yong"
      },
      {
        "surname": "Yokoya",
        "given_name": "Naoto"
      },
      {
        "surname": "Li",
        "given_name": "Chao"
      },
      {
        "surname": "Zhao",
        "given_name": "Qibin"
      }
    ]
  },
  {
    "title": "GeoConv: Geodesic guided convolution for facial action unit recognition",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108355",
    "abstract": "Automatic facial action unit (AU) recognition has attracted great attention but still remains a challenging task, as subtle changes of local facial muscles are difficult to thoroughly capture. Most existing AU recognition approaches leverage geometry information in a straightforward 2D or 3D manner, which either ignore 3D manifold information or suffer from high computational costs. In this paper, we propose a novel geodesic guided convolution (GeoConv) for AU recognition by embedding 3D manifold information into 2D convolutions. Specifically, the kernel of GeoConv is weighted by our introduced geodesic weights, which are negatively correlated to geodesic distances on a coarsely reconstructed 3D morphable face model. Moreover, based on GeoConv, we further develop an end-to-end trainable framework named GeoCNN for AU recognition. Extensive experiments on BP4D and DISFA benchmarks show that our approach significantly outperforms the state-of-the-art AU recognition methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005355",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Embedding",
      "Engineering",
      "Face (sociological concept)",
      "Facial recognition system",
      "Geodesic",
      "Geometry",
      "Kernel (algebra)",
      "Leverage (statistics)",
      "Manifold (fluid mechanics)",
      "Mathematics",
      "Mechanical engineering",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yuedong"
      },
      {
        "surname": "Song",
        "given_name": "Guoxian"
      },
      {
        "surname": "Shao",
        "given_name": "Zhiwen"
      },
      {
        "surname": "Cai",
        "given_name": "Jianfei"
      },
      {
        "surname": "Cham",
        "given_name": "Tat-Jen"
      },
      {
        "surname": "Zheng",
        "given_name": "Jianmin"
      }
    ]
  },
  {
    "title": "Factored Latent-Dynamic Conditional Random Fields for single and multi-label sequence modeling",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108236",
    "abstract": "Conditional Random Fields (CRF) are frequently applied for labeling and segmenting sequence data. Morency et al. (2007) introduced hidden state variables in a labeled CRF structure in order to model the latent dynamics within class labels, thus improving the labeling performance. Such a model is known as Latent-Dynamic CRF (LDCRF). We present Factored LDCRF (FLDCRF), a structure that allows multiple latent dynamics of the class labels to interact with each other. Including such latent-dynamic interactions leads to improved labeling performance on single-label and multi-label sequence modeling experiments across two different datasets, viz., UCI gesture phase data and UCI opportunity data. FLDCRF outperforms all state-of-the-art sequence models, viz., CRF, LDCRF, LSTM, LSTM-CRF, Factorial CRF, Coupled CRF and a multi-label LSTM model across experiments in this paper. In addition, FLDCRF offers easier model selection and is more consistent across validation and test data than LSTM models. FLDCRF is also much faster to train compared to LSTM, even without a GPU. FLDCRF outshines the best LSTM model by ∼ 4% on a single-label task on the UCI gesture phase data and outperforms LSTM models by ∼ 2% on average on the multi-label sequence tagging experiment on the UCI opportunity data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004179",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Conditional random field",
      "Consistency (knowledge bases)",
      "Economics",
      "Epistemology",
      "Genetics",
      "Intuition",
      "Machine learning",
      "Management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Sequence (biology)",
      "Sequence labeling",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Neogi",
        "given_name": "Satyajit"
      },
      {
        "surname": "Dauwels",
        "given_name": "Justin"
      }
    ]
  },
  {
    "title": "Detecting owner-member relationship in fisheye camera system with graph convolution network",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.01.010",
    "abstract": "The owner-member relationship between wheels and vehicles contributes significantly to the 3D perception of vehicles, especially in embedded environments. However, two major challenges must be addressed to leverage the relationship: i) Standard IoU-based heuristics have difficulty dealing with instances of occluded traffic congestion. ii) It is challenging to demonstrate the solution’s efficacy and applicability in a vehicle-mounted system. To address these issues, we propose a novel technique for relationship prediction called DeepWORD which is based on the architecture of a graph convolutional network (GCN). Specifically, we employ feature maps with local correlation as input to the nodes to improve the information richness. Subsequently, we present a graph attention network (GAT) to rectify the priori estimate bias dynamically. Furthermore, we created a large-scale benchmark dataset called WORD that includes annotated owner-member relationships. Sufficient experiments demonstrate that the proposed method is capable of providing state-of-the-art performance and effects in real-time. The WORD dataset is publicly available at https://github.com/NamespaceMain/ownermember-relationship-dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000174",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Graph",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Zizhang"
      },
      {
        "surname": "Wang",
        "given_name": "Jason"
      },
      {
        "surname": "Xu",
        "given_name": "Tianhao"
      },
      {
        "surname": "Wang",
        "given_name": "Fan"
      }
    ]
  },
  {
    "title": "RectiNet-v2: A stacked network architecture for document image dewarping",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.01.014",
    "abstract": "With the advent of mobile and hand-held cameras, document images have found their way into almost every domain. Dewarping of these images for the removal of perspective distortions and folds is essential so that they can be understood by document recognition algorithms. For this, we propose an end-to-end CNN architecture that can produce distortion-free document images from warped documents it takes as input. We train this model on warped document images simulated synthetically to compensate for the lack of enough natural data. Our method is novel in the use of a bifurcated decoder with shared weights to prevent intermingling of grid coordinates, in the use of residual networks in the U-Net skip connections to allow the flow of data from different receptive fields in the model, and in the use of a gated network to help the model focus on structure and line-level detail of the document image. We evaluate our method on the DocUNet dataset, a benchmark in this domain, and obtain results comparable to state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552200023X",
    "keywords": [
      "Algorithm",
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Benchmark (surveying)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Distortion (music)",
      "Domain (mathematical analysis)",
      "Focus (optics)",
      "Geodesy",
      "Geography",
      "Geometry",
      "Grid",
      "Image (mathematics)",
      "Line (geometry)",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Perspective distortion",
      "Physics",
      "Residual"
    ],
    "authors": [
      {
        "surname": "Bandyopadhyay",
        "given_name": "Hmrishav"
      },
      {
        "surname": "Dasgupta",
        "given_name": "Tanmoy"
      },
      {
        "surname": "Das",
        "given_name": "Nibaran"
      },
      {
        "surname": "Nasipuri",
        "given_name": "Mita"
      }
    ]
  },
  {
    "title": "Systematic generation of moment invariant bases for 2D and 3D tensor fields",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108313",
    "abstract": "Moment invariants have been successfully applied to pattern detection tasks in 2D and 3D scalar, vector, and matrix valued data. However so far no flexible basis of invariants exists, i.e., no set that is optimal in the sense that it is complete and independent for every input pattern. In this paper, we prove that a basis of moment invariants can be generated that consists of tensor contractions of not more than two different moment tensors each under the conjecture of the set of all possible tensor contractions to be complete. This result allows us to derive the first generator algorithm that produces flexible bases of moment invariants with respect to orthogonal transformations by selecting a single non-zero moment to pair with all others in these two-factor products. Since at least one non-zero moment can be found in every non-zero pattern, this approach always generates a complete set of descriptors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321004933",
    "keywords": [
      "Basis (linear algebra)",
      "Classical mechanics",
      "Computer science",
      "Conjecture",
      "Exact solutions in general relativity",
      "Geometry",
      "Invariant (physics)",
      "Linguistics",
      "Mathematical analysis",
      "Mathematical physics",
      "Mathematics",
      "Moment (physics)",
      "Philosophy",
      "Physics",
      "Programming language",
      "Pure mathematics",
      "Scalar (mathematics)",
      "Set (abstract data type)",
      "Tensor (intrinsic definition)",
      "Tensor field",
      "Zero (linguistics)"
    ],
    "authors": [
      {
        "surname": "Bujack",
        "given_name": "Roxana"
      },
      {
        "surname": "Zhang",
        "given_name": "Xinhua"
      },
      {
        "surname": "Suk",
        "given_name": "Tomáš"
      },
      {
        "surname": "Rogers",
        "given_name": "David"
      }
    ]
  },
  {
    "title": "Discrete embedding for attributed graphs",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108368",
    "abstract": "Attributed graphs refer to graphs where both node links and node attributes are observable for analysis. Attributed graph embedding enables joint representation learning of node links and node attributes. Different from classical graph embedding methods such as Deepwalk and node2vec that first project node links into low-dimensional vectors which are then linearly concatenated with node attribute vectors as node representation, attributed graph embedding fully explores data dependence between node links and attributes by either using node attributes as class labels to supervise structure learning from node links, or reversely using node links to supervise the learning from node attributes. However, existing attributed graph embedding models are designed in continuous Euclidean spaces which often introduce data redundancy and impose challenges to storage and computation costs. In this paper, we study a new problem of discrete embedding for attributed graphs that can learn succinct node representations. Specifically, we present a Binarized Attributed Network Embedding model (BANE for short) to learn binary node representation by factorizing a Weisfeiler-Lehman proximity matrix under the constraint of binary node representation. Furthermore, based on BANE, we propose a new Low-bit Quantization for Attributed Network Representation learning model (LQANR for short) to learn even more compact node representation of bit-width values. Theoretical analysis and empirical studies on real-world datasets show that the new discrete embedding models outperform benchmark methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005483",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Computer science",
      "Embedding",
      "Engineering",
      "Feature learning",
      "Graph",
      "Graph embedding",
      "Mathematics",
      "Node (physics)",
      "Structural engineering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Hong"
      },
      {
        "surname": "Chen",
        "given_name": "Ling"
      },
      {
        "surname": "Pan",
        "given_name": "Shirui"
      },
      {
        "surname": "Wang",
        "given_name": "Haishuai"
      },
      {
        "surname": "Zhang",
        "given_name": "Peng"
      }
    ]
  },
  {
    "title": "Fast Camouflaged Object Detection via Edge-based Reversible Re-calibration Network",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108414",
    "abstract": "Camouflaged Object Detection (COD) aims to detect objects with similar patterns (e.g., texture, intensity, colour, etc) to their surroundings, and recently has attracted growing research interest. As camouflaged objects often present very ambiguous boundaries, how to determine object locations as well as their weak boundaries is challenging and also the key to this task. Inspired by the biological visual perception process when a human observer discovers camouflaged objects, this paper proposes a novel edge-based reversible re-calibration network called ERRNet. Our model is characterized by two innovative designs, namely Selective Edge Aggregation (SEA) and Reversible Re-calibration Unit (RRU), which aim to model the visual perception behaviour and achieve effective edge prior and cross-comparison between potential camouflaged regions and background. More importantly, RRU incorporates diverse priors with more comprehensive information comparing to existing COD models. Experimental results show that ERRNet outperforms existing cutting-edge baselines on three COD datasets and five medical image segmentation datasets. Especially, compared with the existing top-1 model SINet, ERRNet significantly improves the performance by ∼ 6% (mean E-measure) with notably high speed (79.3 FPS), showing that ERRNet could be a general and robust solution for the COD task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005902",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Calibration",
      "Computer science",
      "Computer vision",
      "Economics",
      "Edge detection",
      "Enhanced Data Rates for GSM Evolution",
      "Image (mathematics)",
      "Image processing",
      "Management",
      "Mathematics",
      "Neuroscience",
      "Object (grammar)",
      "Object detection",
      "Observer (physics)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Perception",
      "Physics",
      "Process (computing)",
      "Quantum mechanics",
      "Segmentation",
      "Statistics",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Ji",
        "given_name": "Ge-Peng"
      },
      {
        "surname": "Zhu",
        "given_name": "Lei"
      },
      {
        "surname": "Zhuge",
        "given_name": "Mingchen"
      },
      {
        "surname": "Fu",
        "given_name": "Keren"
      }
    ]
  },
  {
    "title": "A coarse-to-fine approach for dynamic-to-static image translation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108373",
    "abstract": "Dynamic-to-static image translation aims to convert the dynamic scene into static so that dynamic elements are eliminated from the image. Recent works typically see the problem as an image-to-image translation task, and perform the learned feature mapping over the whole dynamic image to synthesize the static image, which leads to unnecessary detail loss in original static regions. To that end, we delicately formulate it as an image inpainting-like problem to fill the missing static pixels in dynamic regions while retaining original static regions. We achieve this by proposing a coarse-to-fine framework. At coarse stage, we utilize a simple encoder-decoder network to rough out the static image. Using the coarse predicted image, we explicitly infer a more accurate dynamic mask to identify both dynamic objects and their shadows, so that the task could be effectively converted to an image inpainting problem. At fine stage, we recover the missing static pixels in the estimated dynamic regions on the basis of their coarse predictions. We enhance the coarse predicted contents by proposing a mutual texture-structure attention module, which enables the dynamic regions to borrow textures and structures separately from distant locations based on contextual similarity. Several losses are combined as the training objective function to generate excellent results with global consistency and fine details. Qualitative and quantitative experiments verify the superiority of our method in restoring high-quality static contents over state-of-the-art models. In addition, we evaluate the usefulness of the recovered static images by using them as query images to improve visual place recognition in dynamic scenes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005537",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Economics",
      "Encoder",
      "Feature (linguistics)",
      "Gene",
      "Image (mathematics)",
      "Image editing",
      "Inpainting",
      "Linguistics",
      "Management",
      "Messenger RNA",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Task (project management)",
      "Translation (biology)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Teng"
      },
      {
        "surname": "Wu",
        "given_name": "Lin"
      },
      {
        "surname": "Sun",
        "given_name": "Changyin"
      }
    ]
  },
  {
    "title": "CERN: Compact facial expression recognition net",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.01.013",
    "abstract": "Since the renaissance of deep learning (DL), facial expression recognition (FER) has received a lot of interest, with continual improvement in the performance. Hand-in-hand with performance, new challenges have come up. Modern FER systems deal with face images captured under uncontrolled conditions (also called in-the-wild scenario) including occlusions and pose variations. They successfully handle such conditions using deep networks that come with various components like transfer learning, attention mechanism and local-global context extractor. However, these deep networks are highly complex with large number of parameters, making them unfit to be deployed in real scenarios. Is it possible to build a light-weight network that can still show significantly good performance on FER under in-the-wild scenario? In this work, we methodically build such a network and call it as Compact Expression Recognition Net (CERN). We leverage on the aforementioned components of deep networks for FER, and analyse, and appropriately fit them to arrive at CERN. Our CERN is a low-calorie net with only 1.45M parameters, which is almost 50x less than that of a state-of-the-art (SOTA) architecture. It requires only 17MB of storage. Further, during inference, it can process at the real time rate of 40 frames per second (fps) in an intel-i7 cpu. Though it is low-calorie, it is still power-packed in its performance, overpowering other light-weight architectures, and even few high capacity architectures. Specifically, CERN reports 87.09%, 88.17% and 62.06% accuracies on in-the-wild datasets RAFDB, FERPlus and AffectNet respectively. It also exhibits superior robustness under occlusions and pose variations in comparison to other light-weight architectures from the literature. Codes are publicly available at https://github.com/1980x/CFERNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000204",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Deep learning",
      "Inference",
      "Large Hadron Collider",
      "Leverage (statistics)",
      "Paleontology",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Gera",
        "given_name": "Darshan"
      },
      {
        "surname": "Balasubramanian",
        "given_name": "S."
      },
      {
        "surname": "Jami",
        "given_name": "Anwesh"
      }
    ]
  },
  {
    "title": "Kernelized Supervised Laplacian Eigenmap for Visualization and Classification of Multi-Label Data",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108399",
    "abstract": "We had previously proposed a supervised Laplacian eigenmap for visualization (SLE-ML) that can handle multi-label data. In addition, SLE-ML can control the trade-off between the class separability and local structure by a single trade-off parameter. However, SLE-ML cannot transform new data, that is, it has the “out-of-sample” problem. In this paper, we show that this problem is solvable, that is, it is possible to simulate the same transformation perfectly using a set of linear sums of reproducing kernels (KSLE-ML) with a nonsingular Gram matrix. We experimentally showed that the difference between training and testing is not large; thus, a high separability of classes in a low-dimensional space is realizable with KSLE-ML by assigning an appropriate value to the trade-off parameter. This offers the possibility of separability-guided feature extraction for classification. In addition, to optimize the performance of KSLE-ML, we conducted both kernel selection and parameter selection. As a result, it is shown that parameter selection is more important than kernel selection. We experimentally demonstrated the advantage of using KSLE-ML for visualization and for feature extraction compared with a few typical algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005756",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Feature extraction",
      "Feature selection",
      "Gene",
      "Invertible matrix",
      "Kernel (algebra)",
      "Laplace operator",
      "Laplacian matrix",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Selection (genetic algorithm)",
      "Transformation (genetics)",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Tai",
        "given_name": "Mariko"
      },
      {
        "surname": "Kudo",
        "given_name": "Mineichi"
      },
      {
        "surname": "Tanaka",
        "given_name": "Akira"
      },
      {
        "surname": "Imai",
        "given_name": "Hideyuki"
      },
      {
        "surname": "Kimura",
        "given_name": "Keigo"
      }
    ]
  },
  {
    "title": "PUNet: Novel and efficient deep neural network architecture for handwritten documents word spotting",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.01.019",
    "abstract": "Ancient scripts, documents and photos are valuable supports that opened a window to humanity’s past, its social and cultural treasure. However, due to the inevitable deterioration, seeking information in these documents remains a key challenge. Word Spotting remains an attractive alternative to Optical Character Recognition systems for information retrieval, especially in the case of old documents. The quantity of research work on this particular topic continues growing by extracting relevant words as an alternative method of recognizing text content. Here, we propose a new robust Deep Learning based framework for data exploration of ancient documents by applying Transfer Learning from the U-Net Network and using recent Pyramidal Histogram of Character Encryption: no special preprocessing or Data augmentation optimizers are required. Experimental results reveal its accuracy and high capacity to well classify words of various handwritten datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000253",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Character (mathematics)",
      "Computer science",
      "Computer security",
      "Deep learning",
      "Geometry",
      "Information retrieval",
      "Key (lock)",
      "Linguistics",
      "Mathematics",
      "Natural language processing",
      "Operating system",
      "Philosophy",
      "Preprocessor",
      "Scripting language",
      "Spotting",
      "Theology",
      "Treasure",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Boudraa",
        "given_name": "Omar"
      },
      {
        "surname": "Michelucci",
        "given_name": "Dominique"
      },
      {
        "surname": "Hidouci",
        "given_name": "Walid Khaled"
      }
    ]
  },
  {
    "title": "Advances in human action, activity and gesture recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.11.003",
    "abstract": "A set of advanced approaches and models on human action, activity, gesture, and behavior recognition related aspects along with associated challenges are summarized in this note. Notably, Video-based human activity recognition, sensor-based activity analysis, skeleton-based activity recognition, assisted daily living, anomaly detection, facial expression and emotion analysis, gesture and sign language are highlighted in the works. This special issue also introduced six new datasets, while exploring a total of 55 different datasets in its 21 research articles. Apart from the the areas covered in this issue, research on multi-modal analysis, action quality assessment, real-time applications, and activity and behavior computing on edge devices are some of the dominant future challenges to deal with. We firmly believe that the original works and ideas presented in this special issue will serve as helpful references for the relevant research communities in the journey towards a brighter future.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003949",
    "keywords": [
      "Action (physics)",
      "Action recognition",
      "Activity recognition",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Epistemology",
      "Facial expression",
      "Gesture",
      "Gesture recognition",
      "Human–computer interaction",
      "Linguistics",
      "Philosophy",
      "Physics",
      "Programming language",
      "Quality (philosophy)",
      "Quantum mechanics",
      "Set (abstract data type)",
      "Sign language"
    ],
    "authors": [
      {
        "surname": "Mahbub",
        "given_name": "Upal"
      },
      {
        "surname": "Ahad",
        "given_name": "Md Atiqur Rahman"
      }
    ]
  },
  {
    "title": "Description and recognition of complex spatial configurations of object pairs with Force Banner 2D features",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108410",
    "abstract": "A major challenge in scene understanding is the handling of spatial relations between objects or object parts. Several descriptors dedicated to this task already exist, such as the force histogram which is a typical example of relative position descriptor. By computing the interaction between two objects for a given force in all the directions, it gives a good overview of the configuration, and it has useful properties that can make it invariant to the 2D viewpoint. Considering that using complementary forces (negative for repulsion, positive for attraction) should improve the description of complex spatial configurations, we propose to extend the force histogram to a panel of forces so as to make it a more complete descriptor. This gives a 2D descriptor that we called “(discrete) Force Banner” and which can be used as input of a classical Convolutional Neural Network (CNN), benefiting from their powerful performances, and reduced into more compact spatial features to use them in another system. As an illustration of its ability to describe spatial configurations, we used it to solve a classification problem aiming to discriminate simple spatial relations, but with variable configuration complexities. Experimental results obtained on datasets of synthetic and natural images with various shapes highlight the interest of this approach, in particular for complex spatial configurations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005860",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Economics",
      "Finance",
      "Histogram",
      "Image (mathematics)",
      "Invariant (physics)",
      "Mathematical physics",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Position (finance)",
      "Spatial analysis",
      "Spatial relation",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Deléarde",
        "given_name": "Robin"
      },
      {
        "surname": "Kurtz",
        "given_name": "Camille"
      },
      {
        "surname": "Wendling",
        "given_name": "Laurent"
      }
    ]
  },
  {
    "title": "Selective Nearest Neighbors Clustering",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.10.005",
    "abstract": "In this paper, we propose a simple data clustering technique that uses the idea of creating a graph on the data points based on nearest neighbors and identifying clusters by finding connected components of the graph thus constructed. The algorithm forms the graph based on a border detection and an outlier detection technique. We also propose a novel outlier detection technique that is suitable for our implementation. We compare our method against state-of-the-art clustering techniques and perform experiments to analyze the effects of various aspects of a dataset on the proposed scheme.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003639",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Graph",
      "Nearest neighbor graph",
      "Nearest-neighbor chain algorithm",
      "Outlier",
      "Pattern recognition (psychology)",
      "Theoretical computer science",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Sengupta",
        "given_name": "Souhardya"
      },
      {
        "surname": "Das",
        "given_name": "Swagatam"
      }
    ]
  },
  {
    "title": "Robust multi-feature collective non-negative matrix factorization for ECG biometrics",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108376",
    "abstract": "The field of electrocardiogram (ECG) biometrics has received considerable attention in recent years. Although some promising methods have been proposed, it is challenging to design a robust and precise method to improve the recognition performance of ECG signals with noise and sample variation. While the advantage of improved local binary pattern (LBP) for establishing identities has been widely recognized, extracting the latent semantics from multiple LBP features has attracted little attention. We propose a robust multi-feature collective non-negative matrix factorization (RMCNMF) model to handle noise and sample variation in ECG Biometrics. We extract multiple LBP histograms as feature descriptors from segmented ECG signals, and propose a multi-feature learning framework that learns unified representations in the shared latent semantic space via collective non-negative matrix factorization. To further enhance the discrimination of learned representations, we integrate label information and multiple norms in the proposed model, which not only preserves intra- and inter-subject similarities but also mitigates the influence of noise and sample variation. RMCNMF can be solved by an efficient iteration method, for which we provide a convergence analysis in detail. Extensive experiments on four ECG databases show that it performs competitively with state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005562",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Astrophysics",
      "Binary number",
      "Biometrics",
      "Computer science",
      "Data mining",
      "Eigenvalues and eigenvectors",
      "Feature (linguistics)",
      "Feature vector",
      "Histogram",
      "Image (mathematics)",
      "Linguistics",
      "Local binary patterns",
      "Machine learning",
      "Mathematics",
      "Matrix decomposition",
      "Noise (video)",
      "Non-negative matrix factorization",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Variation (astronomy)"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Yuwen"
      },
      {
        "surname": "Yang",
        "given_name": "Gongping"
      },
      {
        "surname": "Wang",
        "given_name": "Kuikui"
      },
      {
        "surname": "Liu",
        "given_name": "Haiying"
      },
      {
        "surname": "Yin",
        "given_name": "Yilong"
      }
    ]
  },
  {
    "title": "Feature-Induced Label Distribution for Learning with Noisy Labels",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.02.011",
    "abstract": "It is challenging to train deep neural networks robustly with noisy labels, since the deep neural networks can totally over-fit on these noisy labels. In this paper, motivated by label distribution learning, we propose a novel method named Feature-Induced Label Distribution (FILD) to deal with noisy labels. Specifically, FILD recovers label distributions by leveraging the topological structure information of feature space, where the feature representation adjusts alternately by fitting the predictive model on the recovered label distributions. Extensive experiments on CIFAR-10, CIFAR-100, and Clothing1M clearly validate the effectiveness of FILD against other compared approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552200054X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Deep neural networks",
      "Distribution (mathematics)",
      "Feature (linguistics)",
      "Feature learning",
      "Feature vector",
      "Law",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Minxue"
      },
      {
        "surname": "Xu",
        "given_name": "Ning"
      },
      {
        "surname": "Geng",
        "given_name": "Xin"
      }
    ]
  },
  {
    "title": "A Bayesian evaluation framework for subjectively annotated visual recognition tasks",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108395",
    "abstract": "An interesting development in automatic visual recognition has been the emergence of tasks where it is not possible to assign objective labels to images, yet still feasible to collect annotations that reflect human judgements about them. Machine learning-based predictors for these tasks rely on supervised training that models the behavior of the annotators, i.e., what would the average person’s judgement be for an image? A key open question for this type of work, especially for applications where inconsistency with human behavior can lead to ethical lapses, is how to evaluate the epistemic uncertainty of trained predictors, i.e., the uncertainty that comes from the predictor’s model. We propose a Bayesian framework for evaluating black box predictors in this regime, agnostic to the predictor’s internal structure. The framework specifies how to estimate the epistemic uncertainty that comes from the predictor with respect to human labels by approximating a conditional distribution and producing a credible interval for the predictions and their measures of performance. The framework is successfully applied to four image classification tasks that use subjective human judgements: facial beauty assessment, social attribute assignment, apparent age estimation, and ambiguous scene labeling.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005604",
    "keywords": [],
    "authors": [
      {
        "surname": "Prijatelj",
        "given_name": "Derek S."
      },
      {
        "surname": "McCurrie",
        "given_name": "Mel"
      },
      {
        "surname": "Anthony",
        "given_name": "Samuel E."
      },
      {
        "surname": "Scheirer",
        "given_name": "Walter J."
      }
    ]
  },
  {
    "title": "Semi-supervised Active Salient Object Detection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108364",
    "abstract": "In this paper, we propose a novel semi-supervised active salient object detection (SOD) method that actively acquires a small subset of the most discriminative and representative samples for labeling. Two main contributions have been made to prevent the method from being overwhelmed by labeling similar distributed samples. First, we design a saliency encoder-decoder with adversarial discriminator to generate a confidence map, representing the network uncertainty on the current prediction. Then, we select the least confident (discriminative) samples from the unlabeled pool to form the “candidate labeled pool”. Second, we train a Variational Auto-Encoder (VAE) to select and add the most representative data from the “candidate labeled pool” into the labeled pool by comparing their corresponding features in the latent space. Within our framework, these two networks are optimized conditioned on the states of each other progressively. Experimental results on six benchmarking SOD datasets demonstrate that our annotation-efficient learning based salient object detection method, reaching to 14% labeling budget, can be on par with the state-of-the-art fully-supervised deep SOD models. The source code is publicly available via our project page: https://github.com/JingZhang617/Semi-sup-active-self-sup-Learning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005446",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Lv",
        "given_name": "Yunqiu"
      },
      {
        "surname": "Liu",
        "given_name": "Bowen"
      },
      {
        "surname": "Zhang",
        "given_name": "Jing"
      },
      {
        "surname": "Dai",
        "given_name": "Yuchao"
      },
      {
        "surname": "Li",
        "given_name": "Aixuan"
      },
      {
        "surname": "Zhang",
        "given_name": "Tong"
      }
    ]
  },
  {
    "title": "Incomplete multiview nonnegative representation learning with multiple graphs",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108412",
    "abstract": "Multiview clustering has become an important research topic during the past decade. However, partial views of many data instances are missing in some realistic multiview learning scenarios. To handle this problem, we develop an effective incomplete multiview nonnegative representation learning (IMNRL) framework, which is suitable for incomplete multiview clustering in various situations. The IMNRL framework performs matrix factorization on multiple incomplete graphs and decomposes these incomplete graphs into a consensus nonnegative representation and view-specific spectral representations, which integrates the advantages of multiview nonnegative representation learning and graph learning. The proposed framework has the following merits: (1) it learns a consensus nonnegative embedding and view-specific embeddings simultaneously; (2) the nonnegative embedding satisfies the neighbor constraint on each incomplete view, which directly reveals the multiview clustering results. Experimental results show that the proposed framework outperforms other state-of-the-art incomplete multiview clustering algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005884",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Constraint (computer-aided design)",
      "Eigenvalues and eigenvectors",
      "Embedding",
      "Geometry",
      "Graph",
      "Law",
      "Mathematics",
      "Matrix decomposition",
      "Non-negative matrix factorization",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Representation (politics)",
      "Spectral clustering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Nan"
      },
      {
        "surname": "Sun",
        "given_name": "Shiliang"
      }
    ]
  },
  {
    "title": "Unsupervised deep clustering via contractive feature representation and focal loss",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108386",
    "abstract": "Deep clustering aims to promote clustering tasks by combining deep learning and clustering together to learn the clustering-oriented representation, and many approaches have shown their validity. However, the feature learning modules in existing methods hardly learn a discriminative representation. In addition, the label assignment mechanism becomes inefficient when dealing with some hard samples. To address these issues, a new joint optimization clustering framework is proposed through introducing the contractive representation in feature learning and utilizing focal loss in the clustering layer. The contractive penalty term added in feature learning would cause the local feature space contraction, resulting in learning more discriminative features. To our certain knowledge, this is also the first work to utilize the focal loss to improve the label assignment in deep clustering method. Moreover, the construction of the joint optimization framework enables the proposed method to learn feature representation and label assignment simultaneously in an end-to-end way. Finally, we comprehensively compare with some state-of-the-art clustering approaches on several clustering tasks to demonstrate the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005665",
    "keywords": [
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Constrained clustering",
      "Correlation clustering",
      "Data stream clustering",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature learning",
      "Feature vector",
      "Law",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Cai",
        "given_name": "Jinyu"
      },
      {
        "surname": "Wang",
        "given_name": "Shiping"
      },
      {
        "surname": "Xu",
        "given_name": "Chaoyang"
      },
      {
        "surname": "Guo",
        "given_name": "Wenzhong"
      }
    ]
  },
  {
    "title": "Enhance to read better: A Multi-Task Adversarial Network for Handwritten Document Image Enhancement",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108370",
    "abstract": "Handwritten document images can be highly affected by degradation for different reasons: Paper ageing, daily-life scenarios (wrinkles, dust, etc.), bad scanning process and so on. These artifacts raise many readability issues for current Handwritten Text Recognition (HTR) algorithms and severely devalue their efficiency. In this paper, we propose an end to end architecture based on Generative Adversarial Networks (GANs) to recover the degraded documents into a c l e a n and r e a d a b l e form. Unlike the most well-known document binarization methods, which try to improve the visual quality of the degraded document, the proposed architecture integrates a handwritten text recognizer that promotes the generated document image to be more readable. To the best of our knowledge, this is the first work to use the text information while binarizing handwritten documents. Extensive experiments conducted on degraded Arabic and Latin handwritten documents demonstrate the usefulness of integrating the recognizer within the GAN architecture, which improves both the visual quality and the readability of the degraded document images. Moreover, we outperform the state of the art in H-DIBCO challenges, after fine tuning our pre-trained model with synthetically degraded Latin handwritten images, on this task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005501",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Document processing",
      "Economics",
      "Handwriting",
      "Image (mathematics)",
      "Management",
      "Natural language processing",
      "Operating system",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Programming language",
      "Readability",
      "Speech recognition",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Khamekhem Jemni",
        "given_name": "Sana"
      },
      {
        "surname": "Souibgui",
        "given_name": "Mohamed Ali"
      },
      {
        "surname": "Kessentini",
        "given_name": "Yousri"
      },
      {
        "surname": "Fornés",
        "given_name": "Alicia"
      }
    ]
  },
  {
    "title": "Low-rank inter-class sparsity based semi-flexible target least squares regression for feature representation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108346",
    "abstract": "Least squares regression (LSR) is an important machine learning method for feature extraction, feature selection, and image classification. For the training samples, there are correlations among samples from the same class. Therefore, many LSR-based methods utilize this property to pursue discriminative representation. However, if the training samples contain noise or outliers, it will be hard to obtain the exact inter-class correlation. To address this problem, in this paper, a novel LSR-based method is proposed, named low-rank inter-class sparsity based semi-flexible target least squares regression (LIS_StLSR). Firstly, the low-rank representation method is utilized to achieve the intrinsic characteristics of the training samples. Afterwards, the low-rank inter-class sparsity constraint is used to force the projected data to have an exact common sparsity structure in each class, which will be robust to noise and outliers in the training samples. This step can also reduce margins of samples from the same class and enlarge margins of samples from different classes to make the projection matrix discriminative. The low-rank representation and the discriminative projection matrix are jointly learned such that they can be boosted mutually. Moreover, a semi-flexible regression target matrix is introduced to measure the regression error more accurately, thus the regression performance can be enhanced to improve the classification accuracy. Experiments are implemented on the different databases of Yale B, AR, LFW, CASIA NIR-VIS, 15-Scene SPF, COIL-20, and Caltech 101, illustrating that the proposed LIS_StLSR outperforms many state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005264",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Discriminative model",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Law",
      "Linguistics",
      "Mathematics",
      "Noise (video)",
      "Outlier",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Projection (relational algebra)",
      "Rank (graph theory)",
      "Regression",
      "Representation (politics)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Shuping"
      },
      {
        "surname": "Wu",
        "given_name": "Jigang"
      },
      {
        "surname": "Zhang",
        "given_name": "Bob"
      },
      {
        "surname": "Fei",
        "given_name": "Lunke"
      }
    ]
  },
  {
    "title": "DLA-Net: Learning dual local attention features for semantic segmentation of large-scale building facade point clouds",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108372",
    "abstract": "The semantic segmentation of building facades is critical for various construction applications, such as urban building reconstruction and damage assessments. As there is a lack of 3D point cloud datasets related to fine-grained building facades, in this work we construct the first large-scale point cloud benchmark dataset for building facade semantic segmentation. In terms of the characteristics of building facade dataset, the existing methods of semantic segmentation cannot fully mine the local neighborhood information of point clouds; therefore, we propose an attention module that learns Dual Local Attention features, called DLA in this paper. The proposed DLA module consists of two blocks, a self-attention block and an attentive pooling block, which both embed an enhanced position encoding block. The DLA module can be easily embedded into various network architectures for point cloud segmentation, naturally resulting in a new 3D semantic segmentation network with an encoder-decoder architecture; we called this network the DLA-Net. Extensive experimental results on our constructed building facade dataset demonstrate that the proposed DLA-Net achieves better performance than the state-of-the-art methods for semantic segmentation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005525",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Block (permutation group theory)",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Construct (python library)",
      "Encoder",
      "Engineering",
      "Facade",
      "Geography",
      "Geometry",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Point cloud",
      "Pooling",
      "Programming language",
      "Scale (ratio)",
      "Segmentation",
      "Structural engineering"
    ],
    "authors": [
      {
        "surname": "Su",
        "given_name": "Yanfei"
      },
      {
        "surname": "Liu",
        "given_name": "Weiquan"
      },
      {
        "surname": "Yuan",
        "given_name": "Zhimin"
      },
      {
        "surname": "Cheng",
        "given_name": "Ming"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhihong"
      },
      {
        "surname": "Shen",
        "given_name": "Xuelun"
      },
      {
        "surname": "Wang",
        "given_name": "Cheng"
      }
    ]
  },
  {
    "title": "A novel bio-inspired texture descriptor based on biodiversity and taxonomic measures",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108382",
    "abstract": "Texture can be defined as the change of image intensity that forms repetitive patterns resulting from the physical properties of an object’s roughness or differences in a reflection on the surface. Considering that texture forms a system of patterns in a non-deterministic way, biodiversity concepts can help its characterization from an image. This paper proposes a novel approach to quantify such a complex system of diverse patterns through species diversity, richness, and taxonomic distinctiveness. The proposed approach considers each image channel as a species ecosystem and computes species diversity and richness as well as taxonomic measures to describe the texture. Furthermore, the proposed approach takes advantage of ecological patterns’ invariance characteristics to build a permutation, rotation, and translation invariant descriptor. Experimental results on three datasets of natural texture images and two datasets of histopathological images have shown that the proposed texture descriptor has advantages over several texture descriptors and deep methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005628",
    "keywords": [],
    "authors": [
      {
        "surname": "Ataky",
        "given_name": "Steve Tsham Mpinda"
      },
      {
        "surname": "Lameiras Koerich",
        "given_name": "Alessandro"
      }
    ]
  },
  {
    "title": "SAF-Net: A spatio-temporal deep learning method for typhoon intensity prediction",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.11.012",
    "abstract": "A typhoon is a destructive weather system that can cause severe casualties and economic losses. Typhoon intensity (TI) is a measurement to evaluate its ruinous degree. Hence, typhoon intensity prediction is an important research problem and many methods have been proposed. However, most of the existing approaches have very limited capability to combine the 2D Typhoon Structure Domain-expert Knowledge (2D-TSDK) and the 3D Typhoon Structure Data-driven Knowledge (3D-TSDK) for the TI prediction. To address this issue, this paper proposes a spatio-temporal deep learning method named Spatial Attention Fusing Network (SAF-Net). The designed model aims to fuse the 2D-TSDK and the 3D-TSDK by developing a specific Wide & Deep framework. In the data-driven component, a special Spatial Attention (SA) module is designed to automatically select high-response wind speed areas and embedded into a three-branch CNN to exploit the 3D-TSDK. Then, the Wide & Deep framework integrates the 2D-TSDK and the 3D-TSDK for the TI prediction. Comprehensive experiments have been conducted on a real-world dataset, and the result shows that the proposed method outperforms state-of-the-art typhoon intensity prediction methods. The code is available in GitHub: https://github.com/xuguangning1218/TI_Prediction",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521004037",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Code (set theory)",
      "Computer science",
      "Computer security",
      "Data mining",
      "Deep learning",
      "Domain (mathematical analysis)",
      "Electrical engineering",
      "Engineering",
      "Exploit",
      "Fuse (electrical)",
      "Intensity (physics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Meteorology",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Set (abstract data type)",
      "Typhoon",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Guangning"
      },
      {
        "surname": "Lin",
        "given_name": "Kenghong"
      },
      {
        "surname": "Li",
        "given_name": "Xutao"
      },
      {
        "surname": "Ye",
        "given_name": "Yunming"
      }
    ]
  },
  {
    "title": "Cross-domain structure preserving projection for heterogeneous domain adaptation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108362",
    "abstract": "Heterogeneous Domain Adaptation (HDA) addresses the transfer learning problems where data from the source and target domains are of different modalities (e.g., texts and images) or feature dimensions (e.g., features extracted with different methods). It is useful for multi-modal data analysis. Traditional domain adaptation algorithms assume that the representations of source and target samples reside in the same feature space, hence are likely to fail in solving the heterogeneous domain adaptation problem. Contemporary state-of-the-art HDA approaches are usually composed of complex optimization objectives for favourable performance and are therefore computationally expensive and less generalizable. To address these issues, we propose a novel Cross-Domain Structure Preserving Projection (CDSPP) algorithm for HDA. As an extension of the classic LPP to heterogeneous domains, CDSPP aims to learn domain-specific projections to map sample features from source and target domains into a common subspace such that the class consistency is preserved and data distributions are sufficiently aligned. CDSPP is simple and has deterministic solutions by solving a generalized eigenvalue problem. It is naturally suitable for supervised HDA but has also been extended for semi-supervised HDA where the unlabelled target domain samples are available. Extensive experiments have been conducted on commonly used benchmark datasets (i.e. Office-Caltech, Multilingual Reuters Collection, NUS-WIDE-ImageNet) for HDA as well as the Office-Home dataset firstly introduced for HDA by ourselves due to its significantly larger number of classes than the existing ones (65 vs 10, 6 and 8). The experimental results of both supervised and semi-supervised HDA demonstrate the superior performance of our proposed method against contemporary state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005422",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Classifier (UML)",
      "Computer science",
      "Consistency (knowledge bases)",
      "Data mining",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Projection (relational algebra)",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Qian"
      },
      {
        "surname": "Breckon",
        "given_name": "Toby P."
      }
    ]
  },
  {
    "title": "Real masks and spoof faces: On the masked face presentation attack detection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108398",
    "abstract": "Face masks have become one of the main methods for reducing the transmission of COVID-19. This makes face recognition (FR) a challenging task because masks hide several discriminative features of faces. Moreover, face presentation attack detection (PAD) is crucial to ensure the security of FR systems. In contrast to the growing number of masked FR studies, the impact of face masked attacks on PAD has not been explored. Therefore, we present novel attacks with real face masks placed on presentations and attacks with subjects wearing masks to reflect the current real-world situation. Furthermore, this study investigates the effect of masked attacks on PAD performance by using seven state-of-the-art PAD algorithms under different experimental settings. We also evaluate the vulnerability of FR systems to masked attacks. The experiments show that real masked attacks pose a serious threat to the operation and security of FR systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005744",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Coronavirus disease 2019 (COVID-19)",
      "Discriminative model",
      "Disease",
      "Engineering",
      "Face (sociological concept)",
      "Face masks",
      "Facial recognition system",
      "Infectious disease (medical specialty)",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)",
      "Social science",
      "Sociology",
      "Systems engineering",
      "Task (project management)",
      "Vulnerability (computing)"
    ],
    "authors": [
      {
        "surname": "Fang",
        "given_name": "Meiling"
      },
      {
        "surname": "Damer",
        "given_name": "Naser"
      },
      {
        "surname": "Kirchbuchner",
        "given_name": "Florian"
      },
      {
        "surname": "Kuijper",
        "given_name": "Arjan"
      }
    ]
  },
  {
    "title": "Data-aware relation learning-based graph convolution neural network for facial action unit recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.02.010",
    "abstract": "In the paper, we propose a novel Data-aware relation graph convolutional neural network (DAR-GCN) for AU recognition. With learning and updating the relation dynamically, it facilitates modeling the potential dynamic individual facial expressing manner and accordingly improves the AU recognition under the unconstrained environment. Taking the psychological research knowledge of AUs as a reference, we adopt the consensus widely-used AUs and six basic emotions as vertexes, and their co-occurrence or ex-occurrence relations between AUs and the emotion dependent relation as the edges to construct the graph. Moreover, the Data-aware relation Graph Generator (DAR-GG) module is proposed to learn the relations with data-driven metric learning. This proposed scheme is benefit for calculating and updating AU relations from data, which facilitates to extract specific relations causing by individual expressing characteristics as well as inherent relations due to facial anatomical structure. Comparative experiments are done on three public datasets: CK+, RAF-AU and DISFA. Experimental results demonstrate that our proposed method achieves a higher AU recognition accuracy rate than the baseline based on the graph with fixed AU relations defined from the psychological knowledge. Additionally, our proposed approach outperforms several existing state-of-the-art AU recognition method by utilizing GCN-based dynamic AU relations learning strategies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000538",
    "keywords": [
      "Action recognition",
      "Artificial intelligence",
      "Artificial neural network",
      "Class (philosophy)",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Data mining",
      "Economics",
      "Emotion recognition",
      "Generator (circuit theory)",
      "Graph",
      "Machine learning",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Relation (database)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Jia",
        "given_name": "Xibin"
      },
      {
        "surname": "Zhou",
        "given_name": "Yuhan"
      },
      {
        "surname": "Li",
        "given_name": "Weiting"
      },
      {
        "surname": "Li",
        "given_name": "Jinghua"
      },
      {
        "surname": "Yin",
        "given_name": "Baocai"
      }
    ]
  },
  {
    "title": "A robust and efficient fingerprint image restoration method based on a phase-field model",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108405",
    "abstract": "In this study, we present a robust and efficient fingerprint image restoration algorithm using the nonlocal Cahn–Hilliard (CH) equation, which was proposed for modeling the microphase separation of diblock copolymers. We take a small local region embedding the damaged domain and solve the nonlocal CH equation to restore the fingerprint image. A Gauss–Seidel type iterative method, which is efficient and simple to implement, is used. The proposed method has the advantage in that the pixel values in the damaged fingerprint domain can be obtained using the image information from the outside of the damaged fingerprint region. Fingerprint restoration based on adjacent pixel information can ensure the accuracy of the fingerprint information with a low computational cost. Computational experiments demonstrated the superior performance of the proposed fingerprint restoration algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005811",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Domain (mathematical analysis)",
      "Embedding",
      "Fingerprint (computing)",
      "Fingerprint recognition",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pixel"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yibao"
      },
      {
        "surname": "Xia",
        "given_name": "Qing"
      },
      {
        "surname": "Lee",
        "given_name": "Chaeyoung"
      },
      {
        "surname": "Kim",
        "given_name": "Sangkwon"
      },
      {
        "surname": "Kim",
        "given_name": "Junseok"
      }
    ]
  },
  {
    "title": "Majorities help minorities: Hierarchical structure guided transfer learning for few-shot fault recognition",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108383",
    "abstract": "To ensure the operational safety and reliability, fault recognition of complex systems is becoming an essential process in industrial systems. However, the existing recognition methods mainly focus on common faults with enough data, which ignore that many faults are lack of samples in engineering practice. Transfer learning can be helpful, but irrelevant knowledge transfer can cause performance degradation, especially in complex systems. To address the above problem, a hierarchy guided transfer learning framework (HGTL) is proposed in this paper for fault recognition with few-shot samples. Firstly, we fuse domain knowledge, label semantics and inter-class distance to calculate the affinity between categories, based on which a category hierarchical tree is constructed by hierarchical clustering. Then, guided by the hierarchical structure, the samples in most similar majority classes are selected from the source domain to pre-train the hierarchical feature learning network (HFN) and extract the transferable fault information. For the fault knowledge extracted from the child nodes of one parent node are similar and can be transferred with each other, so the trained HFN can extract better features of few samples classes with the help of the information from similar faults, and used to address few-shot fault recognition problems. Finally, a dataset of a nuclear power system with 65 categories and the widely used Tennessee Eastman dataset are analyzed respectively via the proposed method, as well as state-of-the-art recognition methods for comparison. The experimental results demonstrate the effectiveness and superiority of the proposed method in fault recognition with few-shot problem.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100563X",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Class hierarchy",
      "Computer science",
      "Data mining",
      "Domain knowledge",
      "Economics",
      "Fault (geology)",
      "Feature (linguistics)",
      "Geology",
      "Hierarchy",
      "Linguistics",
      "Machine learning",
      "Market economy",
      "Object-oriented programming",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Seismology",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Hao"
      },
      {
        "surname": "Liu",
        "given_name": "Ruonan"
      },
      {
        "surname": "Xie",
        "given_name": "Zongxia"
      },
      {
        "surname": "Hu",
        "given_name": "Qinghua"
      },
      {
        "surname": "Dai",
        "given_name": "Jianhua"
      },
      {
        "surname": "Zhai",
        "given_name": "Junhai"
      }
    ]
  },
  {
    "title": "Co-attentive multi-task convolutional neural network for facial expression recognition",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108401",
    "abstract": "Previous research on Facial Expression Recognition (FER) assisted by facial landmarks mainly focused on single-task learning or hard-parameter sharing based multi-task learning. However, soft-parameter sharing based methods have not been explored in this area. Therefore, this paper adopts Facial Landmark Detection (FLD) as the auxiliary task and explores new multi-task learning strategies for FER. First, three classical multi-task structures, including Hard-Parameter Sharing (HPS), Cross-Stitch Network (CSN), and Partially Shared Multi-task Convolutional Neural Network (PS-MCNN), are used to verify the advantages of multi-task learning for FER. Then, we propose a new end-to-end Co-attentive Multi-task Convolutional Neural Network (CMCNN), which is composed of the Channel Co-Attention Module (CCAM) and the Spatial Co-Attention Module (SCAM). Functionally, the CCAM generates the channel co-attention scores by capturing the inter-dependencies of different channels between FER and FLD tasks. The SCAM combines the max- and average-pooling operations to formulate the spatial co-attention scores. Finally, we conduct extensive experiments on four widely used benchmark facial expression databases, including RAF, SFEW2, CK+, and Oulu-CASIA. Extensive experimental results show that our approach achieves better performance than single-task and multi-task baselines, fully validating multi-task learning’s effectiveness and generalizability 1 1 Codes and detailed instructions can be available at https://github.com/thuiar/cmcnn. .",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100577X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Economics",
      "Facial expression",
      "Facial expression recognition",
      "Facial recognition system",
      "Management",
      "Pattern recognition (psychology)",
      "Speech recognition",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Wenmeng"
      },
      {
        "surname": "Xu",
        "given_name": "Hua"
      }
    ]
  },
  {
    "title": "Incomplete multi-view clustering with cosine similarity",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108371",
    "abstract": "Incomplete multi-view clustering partitions multi-view data suffering from missing views, for which matrix factorization approaches seek the latent representation of incomplete multi-view data and constitute one effective category of methods. To exploit data properties further, manifold structure preserving is also incorporated into matrix factorization. However, previous methods optimized the data similarity matrix in the manifold structure preserving term as an unknown variable, which is not guaranteed to faithfully represent the similarities of the original multi-view data and also increases the computational difficulty. To overcome these drawbacks, in this paper, we propose Incomplete Multi-view Clustering with Cosine Similarity (IMCCS). In IMCCS, we directly calculate the cosine similarity in the original multi-view space to strengthen the ability of preserving the manifold structure of the original multi-view data. There is no need to introduce the additional variable. The manifold structure preserving term with cosine similarity and the matrix factorization term are integrated into a unified objective function. An iterative algorithm with gradient descent is designed to solve this objective. Extensive experiments on multi-view datasets show that IMCCS outperforms state-of-the-art incomplete multi-view clustering methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005513",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Cosine similarity",
      "Eigenvalues and eigenvectors",
      "Engineering",
      "Geometry",
      "Image (mathematics)",
      "Law",
      "Manifold (fluid mechanics)",
      "Mathematics",
      "Matrix decomposition",
      "Mechanical engineering",
      "Non-negative matrix factorization",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Representation (politics)",
      "Similarity (geometry)",
      "Trigonometric functions"
    ],
    "authors": [
      {
        "surname": "Yin",
        "given_name": "Jun"
      },
      {
        "surname": "Sun",
        "given_name": "Shiliang"
      }
    ]
  },
  {
    "title": "Orthogonal least squares based fast feature selection for linear classification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108419",
    "abstract": "An Orthogonal Least Squares (OLS) based feature selection method is proposed for both binomial and multinomial classification. The novel Squared Orthogonal Correlation Coefficient (SOCC) is defined based on Error Reduction Ratio (ERR) in OLS and used as the feature ranking criterion. The equivalence between the canonical correlation coefficient, Fisher’s criterion, and the sum of the SOCCs is revealed, which unveils the statistical implication of ERR in OLS for the first time. It is also shown that the OLS based feature selection method has speed advantages when applied for greedy search. The proposed method is comprehensively compared with the mutual information based feature selection methods and the embedded methods using both synthetic and real world datasets. The results show that the proposed method is always in the top 5 among the 12 candidate methods. Besides, the proposed method can be directly applied to continuous features without discretisation, which is another significant advantage over mutual information based methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005951",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Canonical correlation",
      "Computer science",
      "Correlation coefficient",
      "Feature (linguistics)",
      "Feature selection",
      "Linguistics",
      "Mathematics",
      "Mutual information",
      "Ordinary least squares",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Selection (genetic algorithm)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Sikai"
      },
      {
        "surname": "Lang",
        "given_name": "Zi-Qiang"
      }
    ]
  },
  {
    "title": "Rain-component-aware capsule-GAN for single image de-raining",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108377",
    "abstract": "Images taken in the rain usually have poor visual quality, which may cause difficulties for vision-based analysis systems. The research aims to recover clean image content from a single rainy image by removing rain components without introducing any artifacts. Existing rain removal methods often model the rain component as noise, but it obviously has clear patterns instead of random noise. Motivated by this, we raise the idea to build modules to capture rain patterns for de-raining. A Rain-Component-Aware ( R C A ) network is proposed to capture the characteristics of the rain. We then integrate it into an image-conditioned generative adversarial network (image-cGAN) as a R C A loss to guide the generation of rainless images. This results in the proposed two-branch cGAN, where one branch aims at improving the image visual quality after de-raining, and the other aims at extracting rain patterns so that the rain could be effectively removed. To better capture the spatial relationship of different objects within an image, we incorporate the capsule structure in both generator and discriminator of cGAN, which further improves the quality of generated images. The proposed approach is hence named as RCA-cGAN. Benefited by the RCA loss based two-branch optimization and the capsule structure, RCA-cGAN achieves good de-raining effect. Extensive experimental results on several benchmark datasets show that the R C A network is effective to capture rain patterns and the proposed approach could produce much better de-raining images in terms of both subjective visual quality inspection and objective quantitative assessment.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005574",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Component (thermodynamics)",
      "Computer science",
      "Computer vision",
      "Detector",
      "Discriminator",
      "Generative adversarial network",
      "Geodesy",
      "Geology",
      "Image (mathematics)",
      "Image quality",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Physics",
      "Telecommunications",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Fei"
      },
      {
        "surname": "Ren",
        "given_name": "Jianfeng"
      },
      {
        "surname": "Lu",
        "given_name": "Zheng"
      },
      {
        "surname": "Zhang",
        "given_name": "Jialu"
      },
      {
        "surname": "Zhang",
        "given_name": "Qian"
      }
    ]
  },
  {
    "title": "Unified curiosity-Driven learning with smoothed intrinsic reward estimation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108352",
    "abstract": "In reinforcement learning (RL), the intrinsic reward estimation is necessary for policy learning when the extrinsic reward is sparse or absent. To this end, Unified Curiosity-driven Learning with Smoothed intrinsic reward Estimation (UCLSE) is proposed to address the sparse extrinsic reward problem from the perspective of completeness of intrinsic reward estimation. We further propose state distribution-aware weighting method and policy-aware weighting method to dynamically unify two mainstream intrinsic reward estimation methods. In this way, the agent can explore the environment more effectively and efficiently. Under this framework, we propose to employ an attention module to extract task-relevant features for a more precise estimation of intrinsic reward. Moreover, we propose to improve the robustness of policy learning by smoothing the intrinsic reward with a batch of transitions close to the current transition. Extensive experimental results on Atari games demonstrate that our method outperforms the state-of-the-art approaches in terms of both score and training efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100532X",
    "keywords": [
      "A priori and a posteriori",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Curiosity",
      "Economics",
      "Epistemology",
      "Estimation",
      "Gene",
      "Heuristics",
      "Machine learning",
      "Management",
      "Medicine",
      "Operating system",
      "Philosophy",
      "Psychology",
      "Radiology",
      "Reinforcement learning",
      "Robustness (evolution)",
      "Smoothing",
      "Social psychology",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Fuxian"
      },
      {
        "surname": "Li",
        "given_name": "Weichao"
      },
      {
        "surname": "Cui",
        "given_name": "Jiabao"
      },
      {
        "surname": "Fu",
        "given_name": "Yongjian"
      },
      {
        "surname": "Li",
        "given_name": "Xi"
      }
    ]
  },
  {
    "title": "Learning directly from synthetic point clouds for “in-the-wild” 3D face recognition",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108394",
    "abstract": "Point clouds-based networks have achieved great attention in 3D object classification, segmentation, and indoor scene semantic parsing, but its application to 3D face recognition is still underdeveloped owing to two main reasons: lack of large-scale 3D facial data and absence of deep neural network that can directly extract discriminative face representations from point clouds. To address these two problems, a PointNet++ based network is proposed in this paper to extract face features directly from point clouds facial scans and a statistical 3D Morphable Model based 3D face synthesizing strategy is established to generate large-scale unreal facial scans to train the proposed network from scratch. A curvature-aware point sampling technique is proposed to hierarchically down-sample feature-sensitive points which are crucial to pass and aggregate discriminative facial features deeply. In addition, a novel 3D face transfer learning method is proposed to ease the domain discrepancy between synthetic and ‘in-the-wild’ faces. Experimental results on two public 3D face benchmarks show that the network trained only on synthesized data can also be well generalized to ‘in-the-wild’ 3D face recognition. Our method achieves the state-of-the-art results by achieving an overall rank-1 identification rate of 99.46% and 99.65% on FRGCv2 and Bosphorus, respectively. Further, we evaluate on a self-collected dataset to demonstrate the robustness and application potential of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005732",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Discriminative model",
      "Face (sociological concept)",
      "Facial recognition system",
      "Feature (linguistics)",
      "Gene",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point cloud",
      "Robustness (evolution)",
      "Segmentation",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Ziyu"
      },
      {
        "surname": "Da",
        "given_name": "Feipeng"
      },
      {
        "surname": "Yu",
        "given_name": "Yi"
      }
    ]
  },
  {
    "title": "GAN-MVAE: A discriminative latent feature generation framework for generalized zero-shot learning",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.02.002",
    "abstract": "Generalized zero-shot learning (GZSL) is a challenging task that aims to recognize both seen and unseen classes. It is achieved by transferring knowledge from seen classes to unseen classes via a shared semantic space (e.g. attribute space). Recently, Generative adversarial network (GAN) have gained considerable attention in GZSL. GAN can generate missing unseen classes samples from class-specific semantic embedding for training, thereby transforming GZSL into a traditional classification task and achieving impressive results. However, due to the instability during training and the complexity of data distribution, a simple GAN framework cannot capture the real data distribution perfectly, and there is still a large gap between the generated and real sample distributions, which severely limits the performance of GZSL. Therefore, the proposed GAN-MVAE further aligns the real and generated samples by mapping them into the latent space of multi-modal reconstruction variational autoencoder (MVAE), while preserving discriminative semantic information through cross-modal reconstruction. GAN-MVAE provides some inspiration for the study of multi-modal alignment and asymmetry VAE. Extensive experiments on four GZSL benchmark datasets show that GAN-MVAE significantly outperforms the state of the arts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000368",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Benchmark (surveying)",
      "Chemistry",
      "Class (philosophy)",
      "Computer science",
      "Deep learning",
      "Discriminative model",
      "Economics",
      "Embedding",
      "Feature vector",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Management",
      "Margin (machine learning)",
      "Modal",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Polymer chemistry",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Peirong"
      },
      {
        "surname": "Lu",
        "given_name": "Hong"
      },
      {
        "surname": "Yang",
        "given_name": "Bohong"
      },
      {
        "surname": "Ran",
        "given_name": "Wu"
      }
    ]
  },
  {
    "title": "AdaNFF: A new method for adaptive nonnegative multi-feature fusion to scene classification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108402",
    "abstract": "Scene classification is an important basis for many modern intelligent applications, however the performance of pattern recognition or deep learning-based methods are still not sufficient since complicated structure and context of scene images. In this paper, we propose a novel fusion framework of adaptive nonnegative feature fusion (AdaNFF) for scene classification. The AdaNFF integrates nonnegative matrix factorization, adaptive feature fusion and feature fusion boosting into an end-to-end process. Firstly, feature fusion is known as a general strategy to strengthen weak features, and we observe that pixel values and most hand-craft features of the scene image are naturally nonnegative. Therefore we are motivated to build a fusion method based on nonnegative matrix factorization, which can preserve features nonnegative properties and improve their representation performance. Secondly, with the results of fused single or multiple features fusion, we develop an adaptive feature fusion and boosting algorithm to improve the efficiency of image features. Finally, a normalized l 2 -norm classifier and a deep-learning like multilayer perceptron (MLP) classifier are trained to predict label of scene image. Under this framework, there are two versions of the proposed feature fusion method for nonnegative single-feature fusion and multi-feature fusion. All methods were validated on scene classification benchmarks. Experiment results suggest that the proposed methods can deal with multi-class scene problems and achieve remarkable classification performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005781",
    "keywords": [
      "Artificial intelligence",
      "Boosting (machine learning)",
      "Classifier (UML)",
      "Computer science",
      "Contextual image classification",
      "Eigenvalues and eigenvectors",
      "Feature (linguistics)",
      "Feature extraction",
      "Fusion",
      "Image (mathematics)",
      "Image fusion",
      "Linguistics",
      "Matrix decomposition",
      "Non-negative matrix factorization",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Zou",
        "given_name": "Zhiyuan"
      },
      {
        "surname": "Liu",
        "given_name": "Weibin"
      },
      {
        "surname": "Xing",
        "given_name": "Weiwei"
      }
    ]
  },
  {
    "title": "Multi-level adversarial network for domain adaptive semantic segmentation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108384",
    "abstract": "Recent progresses in domain adaptive semantic segmentation demonstrate the effectiveness of adversarial learning (AL) in unsupervised domain adaptation. However, most adversarial learning based methods align source and target distributions at a global image level but neglect the inconsistency around local image regions. This paper presents a novel multi-level adversarial network (MLAN) that aims to address inter-domain inconsistency at both global image level and local region level optimally. MLAN has two novel designs, namely, region-level adversarial learning (RL-AL) and co-regularized adversarial learning (CR-AL). Specifically, RL-AL models prototypical regional context-relations explicitly in the feature space of a labelled source domain and transfers them to an unlabelled target domain via adversarial learning. CR-AL fuses region-level AL and image-level AL optimally via mutual regularization. In addition, we design a multi-level consistency map that can guide domain adaptation in both input space (i.e., image-to-image translation) and output space (i.e., self-training) effectively. Extensive experiments show that MLAN outperforms the state-of-the-art with a large margin consistently across multiple datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005641",
    "keywords": [
      "Adversarial system",
      "Archaeology",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Consistency (knowledge bases)",
      "Context (archaeology)",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Geography",
      "Image (mathematics)",
      "Image translation",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Jiaxing"
      },
      {
        "surname": "Guan",
        "given_name": "Dayan"
      },
      {
        "surname": "Xiao",
        "given_name": "Aoran"
      },
      {
        "surname": "Lu",
        "given_name": "Shijian"
      }
    ]
  },
  {
    "title": "Visual vs internal attention mechanisms in deep neural networks for image classification and object detection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108411",
    "abstract": "The so-called “attention mechanisms” in Deep Neural Networks (DNNs) denote an automatic adaptation of DNNs to capture representative features given a specific classification task and related data. Such attention mechanisms perform both globally by reinforcing feature channels and locally by stressing features in each feature map. Channel and feature importance are learnt in the global end-to-end DNNs training process. In this paper, we present a study and propose a method with a different approach, adding supplementary visual data next to training images. We use human visual attention maps obtained independently with psycho-visual experiments, both in task-driven or in free viewing conditions, or powerful models for prediction of visual attention maps. We add visual attention maps as new data alongside images, thus introducing human visual attention into the DNNs training and compare it with both global and local automatic attention mechanisms. Experimental results show that known attention mechanisms in DNNs work pretty much as human visual attention, but still the proposed approach allows a faster convergence and better performance in image classification tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005872",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Cognitive neuroscience of visual object recognition",
      "Computer science",
      "Computer vision",
      "Contextual image classification",
      "Image (mathematics)",
      "Neuroscience",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Perception",
      "Psychology",
      "Visual attention"
    ],
    "authors": [
      {
        "surname": "Obeso",
        "given_name": "Abraham Montoya"
      },
      {
        "surname": "Benois-Pineau",
        "given_name": "Jenny"
      },
      {
        "surname": "García Vázquez",
        "given_name": "Mireya Saraí"
      },
      {
        "surname": "Acosta",
        "given_name": "Alejandro Álvaro Ramírez"
      }
    ]
  },
  {
    "title": "Improvement in design and training of feature pyramid network for contour refinement",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.01.015",
    "abstract": "Semantic edge detection is a challenging problem which aims to produce multiple edge maps corresponding to different object categories. The design and the training of the related network are the two most powerful levers to optimize its accuracy. To this end, we first refine existing semantic edge detection architectures with low and high level features inside our Edge Pyramid Network, which already outperforms state-of-the-art networks on popular benchmark datasets. Then, we apply an ingenious fine-tuning strategy, which incrementally improves edge detection accuracy through a few alternated training cycles, using various loss functions to perform thick and thin edge detection. We observe that applying separately any of these refinements outperforms such networks as SEAL and STEAL for category-aware edge detection. More remarkable is the fact that these two refinements provide accumulated benefits on the edge detection performance (+0.85% and +1.78% on MF-scores compared to STEAL on SBD and Cityscapes datasets respectively). This improvement is visible in the edge maps in which EPN architecture results in smoother edges and the fine-tuning strategy is responsible for thinner edges. Furthermore, we show that EPN architecture achieves competitive performance (ODS F-measure of 0.828) against the state-of-the-art category-agnostic edge detection networks on the BSDS500 dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000228",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Meteorology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pyramid (geometry)",
      "Training (meteorology)"
    ],
    "authors": [
      {
        "surname": "Pham",
        "given_name": "Van Trung"
      },
      {
        "surname": "Lucas",
        "given_name": "Yves"
      },
      {
        "surname": "Treuillet",
        "given_name": "Sylvie"
      },
      {
        "surname": "Debraux",
        "given_name": "Laurent"
      }
    ]
  },
  {
    "title": "Adversarial learning and decomposition-based domain generalization for face anti-spoofing",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.10.014",
    "abstract": "Face anti-spoofing (FAS) plays a critical role in the face recognition community for securing the face presentation attacks. Many works have been proposed to regard FAS as a domain generalization problem for robust deployment in real-world scenarios. However, existing methods focus on extracting intrinsic spoofing cues to improve the generalization ability, yet neglect to train a robust classifier. In this paper, we propose a framework to improve the generalization ability of face anti-spoofing in two folds:) a generalized feature space is obtained via aggregation of all live faces while dispersing each domain’s spoof faces; and) a domain agnostic classifier is trained through low-rank decomposition. Specifically, a Common Specific Decomposition for Specific (CSD-S) layer is deployed in the last layer of the network to select common features while discarding domain-specific ones among multiple source domains. The above-mentioned two components are integrated into an end-to-end framework, ensuring the generalization ability to unseen scenarios. The extensive experiments demonstrate that the proposed method achieves state-of-the-art results on four public datasets, including CASIA-MFSD, MSU-MFSD, Replay-Attack, and OULU-NPU.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552100372X",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer security",
      "Face (sociological concept)",
      "Facial recognition system",
      "Generalization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Social science",
      "Sociology",
      "Spoofing attack"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Mingxin"
      },
      {
        "surname": "Mu",
        "given_name": "Jiong"
      },
      {
        "surname": "Yu",
        "given_name": "Zitong"
      },
      {
        "surname": "Ruan",
        "given_name": "Kun"
      },
      {
        "surname": "Shu",
        "given_name": "Baiyi"
      },
      {
        "surname": "Yang",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "Fitbeat: COVID-19 estimation based on wristband heart rate using a contrastive convolutional auto-encoder",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108403",
    "abstract": "This study proposes a contrastive convolutional auto-encoder (contrastive CAE), a combined architecture of an auto-encoder and contrastive loss, to identify individuals with suspected COVID-19 infection using heart-rate data from participants with multiple sclerosis (MS) in the ongoing RADAR-CNS mHealth research project. Heart-rate data was remotely collected using a Fitbit wristband. COVID-19 infection was either confirmed through a positive swab test, or inferred through a self-reported set of recognised symptoms of the virus. The contrastive CAE outperforms a conventional convolutional neural network (CNN), a long short-term memory (LSTM) model, and a convolutional auto-encoder without contrastive loss (CAE). On a test set of 19 participants with MS with reported symptoms of COVID-19, each one paired with a participant with MS with no COVID-19 symptoms, the contrastive CAE achieves an unweighted average recall of 95.3 % , a sensitivity of 100 % and a specificity of 90.6 % , an area under the receiver operating characteristic curve (AUC-ROC) of 0.944, indicating a maximum successful detection of symptoms in the given heart rate measurement period, whilst at the same time keeping a low false alarm rate.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005793",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Coronavirus disease 2019 (COVID-19)",
      "Disease",
      "Encoder",
      "Infectious disease (medical specialty)",
      "Internal medicine",
      "Medicine",
      "Operating system",
      "Pattern recognition (psychology)",
      "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Shuo"
      },
      {
        "surname": "Han",
        "given_name": "Jing"
      },
      {
        "surname": "Puyal",
        "given_name": "Estela Laporta"
      },
      {
        "surname": "Kontaxis",
        "given_name": "Spyridon"
      },
      {
        "surname": "Sun",
        "given_name": "Shaoxiong"
      },
      {
        "surname": "Locatelli",
        "given_name": "Patrick"
      },
      {
        "surname": "Dineley",
        "given_name": "Judith"
      },
      {
        "surname": "Pokorny",
        "given_name": "Florian B."
      },
      {
        "surname": "Costa",
        "given_name": "Gloria Dalla"
      },
      {
        "surname": "Leocani",
        "given_name": "Letizia"
      },
      {
        "surname": "Guerrero",
        "given_name": "Ana Isabel"
      },
      {
        "surname": "Nos",
        "given_name": "Carlos"
      },
      {
        "surname": "Zabalza",
        "given_name": "Ana"
      },
      {
        "surname": "Sørensen",
        "given_name": "Per Soelberg"
      },
      {
        "surname": "Buron",
        "given_name": "Mathias"
      },
      {
        "surname": "Magyari",
        "given_name": "Melinda"
      },
      {
        "surname": "Ranjan",
        "given_name": "Yatharth"
      },
      {
        "surname": "Rashid",
        "given_name": "Zulqarnain"
      },
      {
        "surname": "Conde",
        "given_name": "Pauline"
      },
      {
        "surname": "Stewart",
        "given_name": "Callum"
      },
      {
        "surname": "Folarin",
        "given_name": "Amos A"
      },
      {
        "surname": "Dobson",
        "given_name": "Richard JB"
      },
      {
        "surname": "Bailón",
        "given_name": "Raquel"
      },
      {
        "surname": "Vairavan",
        "given_name": "Srinivasan"
      },
      {
        "surname": "Cummins",
        "given_name": "Nicholas"
      },
      {
        "surname": "Narayan",
        "given_name": "Vaibhav A"
      },
      {
        "surname": "Hotopf",
        "given_name": "Matthew"
      },
      {
        "surname": "Comi",
        "given_name": "Giancarlo"
      },
      {
        "surname": "Schuller",
        "given_name": "Björn"
      },
      {
        "surname": "Consortium",
        "given_name": "RADAR-CNS"
      }
    ]
  },
  {
    "title": "Multi-scale signed recurrence plot based time series classification using inception architectural networks",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108385",
    "abstract": "Inspired by the great success of deep neural networks in image classification, recent works use Recurrence Plots (RP) to encode time series as images for classification. RP provide rich texture information and construct long-term time correlations, which are effective supplements to the networks. However, RP cannot handle the scale and length variability of sequences. Moreover, RP have serious tendency confusion problem. They cannot represent the upward and downward trends of sequences effectively. In addition to the defects of RP, existing time series classification (TSC) networks cannot adapt to the various scales of discriminative regions of time series effectively. To tackle these problems, this paper proposes a method, named MSRP-IFCN. It is composed of two submodules, the Multi-scale Signed RP (MSRP) and the Inception Fully Convolutional Network (IFCN). MSRP are proposed to handle the defects of RP. They comprise three components, namely the multi-scale RP, the asymmetric RP and the signed RP. We first use the multi-scale RP to enrich the scales of images. Then, the asymmetric RP are constructed to represent long sequences. Finally, the signed RP images are obtained by multiplying the designed sign masks to remove the tendency confusion. Besides, IFCN is proposed to enhance the existing TSC networks in multi-scale feature extraction. By introducing the modified Inception modules, IFCN obtains extensive receptive fields and better extracts multi-scale features from the MSRP images. Experimental results on 85 UCR datasets indicate the superior performance of MSRP-IFCN. The visualization results further demonstrate the effectiveness of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005653",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Confusion",
      "Convolutional neural network",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature extraction",
      "Geography",
      "Linguistics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Psychoanalysis",
      "Psychology",
      "Scale (ratio)",
      "Series (stratigraphy)",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Ye"
      },
      {
        "surname": "Hou",
        "given_name": "Yi"
      },
      {
        "surname": "OuYang",
        "given_name": "Kewei"
      },
      {
        "surname": "Zhou",
        "given_name": "Shilin"
      }
    ]
  },
  {
    "title": "A unified deep sparse graph attention network for scene graph generation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108367",
    "abstract": "Scene graph generation (SGG) plays an important role in deep understanding of the visual scene. Despite the empirical success of traditional methods in many applications, they still have several challenges in the high computational complexity of dense graph and the inaccurate pruning of sparse graph. To tackle these problems, we propose a novel deep sparse graph attention network to mine the rich contextual clues and simultaneously preserve the statistical co-occurrence knowledge of SGG. Specifically, our Relationship Measurement Network (RelMN) is adapted to first classify all object pairs in dense graph as the foreground and background categories to filter the false relationships and then construct a sparse graph efficiently. Meanwhile, we design a novel feature aggregation and update method via graphical message passing to jointly learn the node and edge features for object recognition and relationship classification in the graph attention network. Extensive experimental results on the large scale VG and VRD datasets demonstrate our proposed method outperforms several state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005471",
    "keywords": [
      "1-planar graph",
      "Artificial intelligence",
      "Attention network",
      "Computer science",
      "Dense graph",
      "Graph",
      "Line graph",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Hao"
      },
      {
        "surname": "Yang",
        "given_name": "Yazhou"
      },
      {
        "surname": "Luo",
        "given_name": "Tingjin"
      },
      {
        "surname": "Zhang",
        "given_name": "Jun"
      },
      {
        "surname": "Li",
        "given_name": "Shuohao"
      }
    ]
  },
  {
    "title": "Defect attention template generation cycleGAN for weakly supervised surface defect segmentation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108396",
    "abstract": "Surface defect segmentation is very important for the quality inspection of industrial production and is an important pattern recognition problem. Although deep learning (DL) has achieved remarkable results in surface defect segmentation, most of these results have been obtained by using massive images with pixel-level annotations, which are difficult to obtain at industrial sites. This paper proposes a weakly supervised defect segmentation method based on the dynamic templates generated by an improved cycle-consistent generative adversarial network (CycleGAN) trained by image-level annotations. To generate better templates for defects with weak signals, we propose a defect attention module by applying the defect residual for the discriminator to strengthen the elimination of defect regions and suppress changes in the background. A defect cycle-consistent loss is designed by adding structural similarity (SSIM) to the original L1 loss to include the grayscale and structural features; the proposed loss can better model the inner structure of defects. After obtaining the defect-free template, a defect segmentation map can easily be obtained through a simple image comparison and threshold segmentation. Experiments show that the proposed method is both efficient and effective, significantly outperforms other weakly supervised methods, and achieves performance that is comparable or even superior to that of supervised methods on three industrial datasets (intersection over union (IoU) on the DAGM 2007, KSD and CCSD datasets of 78.28%, 59.43%,and 68.83%, respectively). The proposed method can also be employed as a semiautomatic annotation tool combined with active learning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005616",
    "keywords": [
      "Aerospace engineering",
      "Artificial intelligence",
      "Computer science",
      "Detector",
      "Discriminator",
      "Engineering",
      "Grayscale",
      "Image (mathematics)",
      "Intersection (aeronautics)",
      "Pattern recognition (psychology)",
      "Pixel",
      "Programming language",
      "Segmentation",
      "Similarity (geometry)",
      "Telecommunications",
      "Template"
    ],
    "authors": [
      {
        "surname": "Niu",
        "given_name": "Shuanlong"
      },
      {
        "surname": "Li",
        "given_name": "Bin"
      },
      {
        "surname": "Wang",
        "given_name": "Xinggang"
      },
      {
        "surname": "He",
        "given_name": "Songping"
      },
      {
        "surname": "Peng",
        "given_name": "Yaru"
      }
    ]
  },
  {
    "title": "GL-GAN: Adaptive global and local bilevel optimization for generative adversarial network",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108375",
    "abstract": "Although Generative Adversarial Networks (GAN) have shown remarkable performance in image generation, there exist some challenges in instability and convergence speed. During the training, the results of some models display the imbalances of quality within a generated image, in which some defective parts appear compared with other regions. Different from general single global optimization methods, we introduce an adaptive global and local bilevel optimization model (GL-GAN). The model achieves the generation of high-resolution images in a complementary and promoting way, where global optimization is to optimize the whole images and local is only to optimize the low-quality areas. Based on DCGAN, GL-GAN is able to effectively avoid the nature of imbalance by local bilevel optimization, which is accomplished by first locating low-quality areas and then optimizing them. Moreover, through feature map cues from discriminator output, we propose the adaptive local and global optimization method (Ada-OP) for interactive optimization and observe that it boosts the convergence speed. Compared with the current GAN methods, our model has shown impressive performance on CelebA, Oxford Flowers, CelebA-HQ and LSUN datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005550",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bilevel optimization",
      "Computer science",
      "Convergence (economics)",
      "Detector",
      "Discriminator",
      "Economic growth",
      "Economics",
      "Epistemology",
      "Feature (linguistics)",
      "Global optimization",
      "Image (mathematics)",
      "Linguistics",
      "Local search (optimization)",
      "Mathematical optimization",
      "Mathematics",
      "Optimization problem",
      "Philosophy",
      "Quality (philosophy)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Ying"
      },
      {
        "surname": "Fan",
        "given_name": "Heng"
      },
      {
        "surname": "Yuan",
        "given_name": "Xiaohui"
      },
      {
        "surname": "Xiang",
        "given_name": "Jinhai"
      }
    ]
  },
  {
    "title": "Video anomaly detection using deep residual-spatiotemporal translation network",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.11.001",
    "abstract": "Video anomaly detection has gained significant attention in the current intelligent surveillance systems. We propose Deep Residual Spatiotemporal Translation Network (DR-STN), a novel unsupervised Deep Residual conditional Generative Adversarial Network (DR-cGAN) model with an Online Hard Negative Mining (OHNM) approach. The proposed DR-cGAN provides a wider network to learn a mapping from spatial to temporal representations and enhance the perceptual quality of synthesized images from a generator. During DR-cGAN training, we take only the frames of normal events to produce their corresponding dense optical flow. At testing time, we compute the reconstruction error in local pixels between the synthesized and the real dense optical flow and then apply OHNM to remove false-positive detection results. Finally, a semantic region merging is introduced to integrate the intensities of all the individual abnormal objects into a full output frame. The proposed DR-STN has been extensively evaluated on publicly available benchmarks, including UCSD, UMN, and CUHK Avenue, demonstrating superior results over other state-of-the-art methods both in frame-level and pixel-level evaluations. The average Area Under the Curve (AUC) value of the frame-level evaluation for the three benchmarks is 96.73%. The improvement ratio of AUC in the frame level between DR-STN and state-of-the-art methods is 7.6%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003925",
    "keywords": [
      "Algorithm",
      "Anomaly detection",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Frame (networking)",
      "Gene",
      "Generative adversarial network",
      "Image (mathematics)",
      "Messenger RNA",
      "Optical flow",
      "Pattern recognition (psychology)",
      "Pixel",
      "Residual",
      "Telecommunications",
      "Translation (biology)"
    ],
    "authors": [
      {
        "surname": "Ganokratanaa",
        "given_name": "Thittaporn"
      },
      {
        "surname": "Aramvith",
        "given_name": "Supavadee"
      },
      {
        "surname": "Sebe",
        "given_name": "Nicu"
      }
    ]
  },
  {
    "title": "Mixing up contrastive learning: Self-supervised representation learning for time series",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.02.007",
    "abstract": "The lack of labeled data is a key challenge for learning useful representation from time series data. However, an unsupervised representation framework that is capable of producing high quality representations could be of great value. It is key to enabling transfer learning, which is especially beneficial for medical applications, where there is an abundance of data but labeling is costly and time consuming. We propose an unsupervised contrastive learning framework that is motivated from the perspective of label smoothing. The proposed approach uses a novel contrastive loss that naturally exploits a data augmentation scheme in which new samples are generated by mixing two data samples with a mixing component. The task in the proposed framework is to predict the mixing component, which is utilized as soft targets in the loss function. Experiments demonstrate the framework’s superior performance compared to other representation learning approaches on both univariate and multivariate time series and illustrate its benefits for transfer learning for clinical time series.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000502",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Component (thermodynamics)",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Feature learning",
      "Key (lock)",
      "Law",
      "Machine learning",
      "Mixing (physics)",
      "Multivariate statistics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Representation (politics)",
      "Series (stratigraphy)",
      "Smoothing",
      "Supervised learning",
      "Thermodynamics",
      "Time series",
      "Transfer of learning",
      "Univariate",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Wickstrøm",
        "given_name": "Kristoffer"
      },
      {
        "surname": "Kampffmeyer",
        "given_name": "Michael"
      },
      {
        "surname": "Mikalsen",
        "given_name": "Karl Øyvind"
      },
      {
        "surname": "Jenssen",
        "given_name": "Robert"
      }
    ]
  },
  {
    "title": "A fusion representation for face learning by low-rank constrain and high-frequency texture components",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.01.022",
    "abstract": "Face image processing has always been an important field of artificial intelligence. In machine learning tasks, samples are vectorized into a data matrix. Hence, different faces lie in several respective low-rank subspaces, which can be effectively captured by Low-Rank Representation (LRR) learning. However, most technologies usually use the raw data directly but ignore the weight among different information of the image itself. Following the intuition that human beings distinguish different people through the lineaments and wrinkles of faces which are indeed the texture components of face images, it is believed that representation learning will benefit from introducing the texture components of samples into the information fusion. Inspired by Multi-view learning models which can reconcile the knowledge gained from different views, a fusion framework is proposed in this letter, named High-Frequency texture components Low-Rank Representation (HFLRR). In HFLRR, the high-frequency texture components of samples will be extracted by Fast Fourier Transformation and Butterworth filtering firstly. Subsequently, the texture information will be introduced into the subspace learning procedure. An effective optimization solution is presented for this framework and the experimental results show that the proposed algorithm outperforms the state-of-art methods on several real-world face datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000290",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Facial recognition system",
      "Geometry",
      "Image (mathematics)",
      "Law",
      "Linear subspace",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Rank (graph theory)",
      "Representation (politics)",
      "Subspace topology",
      "Texture (cosmology)"
    ],
    "authors": [
      {
        "surname": "Liang",
        "given_name": "Zexiao"
      },
      {
        "surname": "Zeng",
        "given_name": "Deyu"
      },
      {
        "surname": "Guo",
        "given_name": "Shaozhi"
      },
      {
        "surname": "Li",
        "given_name": "Jianzhong"
      },
      {
        "surname": "Wu",
        "given_name": "Zongze"
      }
    ]
  },
  {
    "title": "Transformer-based approach for joint handwriting and named entity recognition in historical document",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.11.010",
    "abstract": "The extraction of relevant information carried out by named entities in handwriting documents is still a challenging task. Unlike traditional information extraction approaches that usually face text transcription and named entity recognition as separate subsequent tasks, we propose in this paper an end-to-end transformer-based approach to jointly perform these two tasks. The proposed approach operates at the paragraph level, which brings two main benefits. First, it allows the model to avoid unrecoverable early errors due to line segmentation. Second, it allows the model to exploit larger bi-dimensional context information to identify the semantic categories, reaching a higher final prediction accuracy. We also explore different training scenarios to show their effect on the performance and we demonstrate that a two-stage learning strategy can make the model reach a higher final prediction accuracy. As far as we know, this work presents the first approach that adopts the transformer networks for named entity recognition in handwritten documents. We achieve the new state-of-the-art performance in the ICDAR 2017 Information Extraction competition using the Esposalles database, for the complete task, even though the proposed technique does not use any dictionaries, language modeling, or post-processing.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521004013",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Economics",
      "Exploit",
      "Feature extraction",
      "Handwriting",
      "Handwriting recognition",
      "Information extraction",
      "Language model",
      "Management",
      "Named-entity recognition",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Segmentation",
      "Speech recognition",
      "Task (project management)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Rouhou",
        "given_name": "Ahmed Cheikh"
      },
      {
        "surname": "Dhiaf",
        "given_name": "Marwa"
      },
      {
        "surname": "Kessentini",
        "given_name": "Yousri"
      },
      {
        "surname": "Salem",
        "given_name": "Sinda Ben"
      }
    ]
  },
  {
    "title": "Query Pixel Guided Stroke Extraction with Model-Based Matching for Offline Handwritten Chinese Characters",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108416",
    "abstract": "Stroke extraction and matching are critical for structural interpretation based applications of handwritten Chinese characters, such as Chinese character education and calligraphy analysis. Stroke extraction from offline handwritten Chinese characters is difficult because of the missing of temporal information, the multi-stroke structures and the distortion of handwritten shapes. In this paper, we propose a comprehensive scheme for solving the stroke extraction problem for handwritten Chinese characters. The method consists of three main steps: (1) fully convolutional network (FCN) based skeletonization; (2) query pixel guided stroke extraction; (3) model-based stroke matching. Specifically, based on a recently proposed architecture of FCN, both the stroke skeletons and cross regions are firstly extracted from the character image by the proposed SkeNet and CrossNet, respectively. Stroke extraction is solved by simulating the human perception that once given a certain pixel from non-cross region of a stroke, the whole stroke containing the pixel can be traced. To realize this idea, we formulate stroke extraction as a problem of pairing and connecting skeleton-wise stroke segments which are adjacent to the same cross region, where the pairing consistency between stroke segments is measured using a PathNet [1]. To reduce the ambiguity of stroke extraction, the extracted candidate strokes are matched with a character model consisting of standard strokes by tree search to identify the correct strokes. For verifying the effectiveness of the proposed method, we train and test our models on character images with stroke segmentation annotations generated from the online handwriting datasets CASIA-OLHWDB and ICDAR13-Online, as well as a dataset of Regularly-Written online handwritten characters (RW-OLHWDB). The experimental results demonstrate the effectiveness of the proposed method and provide several benchmarks. Particularly, the precisions of stroke extraction for ICDAR13-Online and RW-OLHWDB are 89.0% and 94.9%, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005926",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Character (mathematics)",
      "Chinese characters",
      "Computer network",
      "Computer science",
      "Convolutional neural network",
      "Distortion (music)",
      "Engineering",
      "Feature extraction",
      "Geometry",
      "Handwriting",
      "Handwriting recognition",
      "Matching (statistics)",
      "Mathematics",
      "Mechanical engineering",
      "Pattern recognition (psychology)",
      "Pixel",
      "Segmentation",
      "Skeletonization",
      "Statistics",
      "Stroke (engine)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Tie-Qiang"
      },
      {
        "surname": "Jiang",
        "given_name": "Xiaoyi"
      },
      {
        "surname": "Liu",
        "given_name": "Cheng-Lin"
      }
    ]
  },
  {
    "title": "Semi-supervised extensions of multi-task tree ensembles",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108393",
    "abstract": "Scale inconsistency is a widely encountered issue in multi-output learning problems. Specifically, target sets with multiple real valued or a mixture of categorical and real valued variables require addressing the scale differences to obtain predictive models with sufficiently good performance. Data transformation techniques are often employed to solve that problem. However, these operations are susceptible to different shortcomings such as changing the statistical properties of the data and increase the computational burden. Scale differences also pose problem in semi-supervised learning (SSL) models as they require processing of unsupervised information where distance measures are commonly employed. Classical distance metrics can be criticized as they lose efficiency when variables exhibit type or scale differences, too. Besides, in higher dimensions distance metrics cause problems due to loss of discriminative power. This paper introduces alternative semi-supervised tree-based strategies that are robust to scale differences both in terms of feature and target variables. We propose use of a scale-invariant proximity measure by means of tree-based ensembles to preserve the original characteristics of the data. We update classical tree derivation procedure to a multi-criteria form to resolve scale inconsistencies. We define proximity based clustering indicators and extend the supervised model with unsupervised criteria. Our experiments show that proposed method significantly outperforms its benchmark learning model that is predictive clustering trees.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005549",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Biochemistry",
      "Categorical variable",
      "Chemistry",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Decision tree",
      "Discriminative model",
      "Gene",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)",
      "Supervised learning",
      "Transformation (genetics)",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Adıyeke",
        "given_name": "Esra"
      },
      {
        "surname": "Baydoğan",
        "given_name": "Mustafa Gökçe"
      }
    ]
  },
  {
    "title": "Localized multiple kernel learning using graph modularity",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.01.018",
    "abstract": "Multiple kernel learning (MKL) algorithms exploit information from multiple feature representations by assigning weights to each representation in the kernel space, and later combining them. However, this ignores the fact that data points exhibit locally varying characteristics. To address this problem, localized MKL algorithms learn locality-specific kernel weights which determine each base kernel’s influence in the locality under consideration. Here, we relate the problem of determining the relevance of base kernels for classification to that of quantifying community structure in graphs. Next, we derive sample-specific kernel weights using graph modularity. Through experiments on publicly available datasets, we show that the proposed approach offers a viable alternative to state-of-the-art MKL approaches while being computationally inexpensive.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000265",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Combinatorics",
      "Computer science",
      "Computer security",
      "Exploit",
      "Genetics",
      "Graph",
      "Graph kernel",
      "Kernel (algebra)",
      "Kernel embedding of distributions",
      "Kernel method",
      "Law",
      "Linguistics",
      "Locality",
      "Machine learning",
      "Mathematics",
      "Modularity (biology)",
      "Multiple kernel learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "String kernel",
      "Support vector machine",
      "Theoretical computer science",
      "Tree kernel"
    ],
    "authors": [
      {
        "surname": "Chamakura",
        "given_name": "Lily"
      },
      {
        "surname": "Saha",
        "given_name": "Goutam"
      }
    ]
  },
  {
    "title": "Learning image aesthetic subjectivity from attribute-aware relational reasoning network",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.02.008",
    "abstract": "People’s various visual preferences lead to subjectivity in their aesthetic perception of images. Hence, learning image aesthetic subjectivity has attracted great interest in the computer vision community. People with different subjectivity often have huge uncertainty in their aesthetic ratings, which is affected by diversified aesthetic attributes in images. Although existing studies leverage aesthetic attributes to infer image aesthetic scores, the influence mechanism of diversified aesthetic attributes on people’s aesthetic ratings has not yet been revealed. Because of this, this paper proposes an attribute-aware relational reasoning network to learn image aesthetic distribution rated by people with different subjectivity. Specifically, we embed the relationship between different aesthetic attributes into a deep network for reasoning the uncertainty in image aesthetic distribution. Besides, an efficient distribution loss function is introduced to intensively learn image aesthetics with high uncertainty. Experimental results show that our method is superior to state-of-the-art methods in learning image aesthetic distribution.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000514",
    "keywords": [
      "Aesthetics",
      "Art",
      "Artificial intelligence",
      "Biology",
      "Cognitive psychology",
      "Computer science",
      "Computer vision",
      "Epistemology",
      "Evolutionary biology",
      "Function (biology)",
      "Image (mathematics)",
      "Leverage (statistics)",
      "Neuroscience",
      "Perception",
      "Philosophy",
      "Psychology",
      "Subjectivity"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Hancheng"
      },
      {
        "surname": "Zhou",
        "given_name": "Yong"
      },
      {
        "surname": "Yao",
        "given_name": "Rui"
      },
      {
        "surname": "Wang",
        "given_name": "Guangcheng"
      },
      {
        "surname": "Yang",
        "given_name": "Yuzhe"
      }
    ]
  },
  {
    "title": "Big data directed acyclic graph model for real-time COVID-19 twitter stream detection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108404",
    "abstract": "Every day, large-scale data are continuously generated on social media as streams, such as Twitter, which inform us about all events around the world in real-time. Notably, Twitter is one of the effective platforms to update countries leaders and scientists during the coronavirus (COVID-19) pandemic. Other people have also used this platform to post their concerns about the spread of this virus and a rapid increase of death cases globally. The aim of this work is to detect anomalous events associated with COVID-19 from Twitter. To this end, we propose a distributed Directed Acyclic Graph topology framework to aggregate and process large-scale real-time tweets related to COVID-19. The core of our system is a novel lightweight algorithm that can automatically detect anomaly events. In addition, our system can also identify, cluster, and visualize important keywords in tweets. On 18 August 2020, our model detected the highest anomaly since many tweets mentioned the casualties’ updates and the debates on the pandemic that day. We obtained the three most commonly listed terms on Twitter: “covid”, “death”, and “Trump” (21,566, 11,779, and 4761 occurrences, respectively), with the highest TF-IDF score for these terms: “people” (0.63637), “school” (0.5921407) and “virus” (0.57385). From our clustering result, the word “death”, “corona”, and “case” are grouped into one cluster, where the word “pandemic”, “school”, and “president” are grouped as another cluster. These terms were located near each other on vector space so that they were clustered, indicating people’s most concerned topics on Twitter.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100580X",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Cartography",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Computer network",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Data mining",
      "Data stream mining",
      "Disease",
      "Geography",
      "Graph",
      "Infectious disease (medical specialty)",
      "Medicine",
      "Pandemic",
      "Pathology",
      "Scale (ratio)",
      "Social media",
      "Theoretical computer science",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Amen",
        "given_name": "Bakhtiar"
      },
      {
        "surname": "Faiz",
        "given_name": "Syahirul"
      },
      {
        "surname": "Do",
        "given_name": "Thanh-Toan"
      }
    ]
  },
  {
    "title": "Learning to select cuts for efficient mixed-integer programming",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108353",
    "abstract": "Cutting plane methods play a significant role in modern solvers for tackling mixed-integer programming (MIP) problems. Proper selection of cuts would remove infeasible solutions in the early stage, thus largely reducing the computational burden without hurting the solution accuracy. However, the major cut selection approaches heavily rely on heuristics, which strongly depend on the specific problem at hand and thus limit their generalization capability. In this paper, we propose a data-driven and generalizable cut selection approach, named Cut Ranking, in the settings of multiple instance learning. To measure the quality of the candidate cuts, a scoring function, which takes the instance-specific cut features as inputs, is trained and applied in cut ranking and selection. In order to evaluate our method, we conduct extensive experiments on both synthetic datasets and real-world datasets. Compared with commonly used heuristics for cut selection, the learning-based policy has shown to be more effective, and is capable of generalizing over multiple problems with different properties. Cut Ranking has been deployed in an industrial solver for large-scale MIPs. In the online A/B testing of the product planning problems with more than 10 7 variables and constraints daily, Cut Ranking has achieved the average speedup ratio of 12.42% over the production solver without any accuracy loss of solution.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005331",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Cutting-plane method",
      "Generalization",
      "Heuristic",
      "Heuristics",
      "Integer programming",
      "Limit (mathematics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Ranking (information retrieval)",
      "Selection (genetic algorithm)",
      "Solver",
      "Speedup"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Zeren"
      },
      {
        "surname": "Wang",
        "given_name": "Kerong"
      },
      {
        "surname": "Liu",
        "given_name": "Furui"
      },
      {
        "surname": "Zhen",
        "given_name": "Hui-Ling"
      },
      {
        "surname": "Zhang",
        "given_name": "Weinan"
      },
      {
        "surname": "Yuan",
        "given_name": "Mingxuan"
      },
      {
        "surname": "Hao",
        "given_name": "Jianye"
      },
      {
        "surname": "Yu",
        "given_name": "Yong"
      },
      {
        "surname": "Wang",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "InfraGAN: A GAN architecture to transfer visible images to infrared domain",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.01.026",
    "abstract": "Utilizing both visible and infrared (IR) images in various deep learning based computer vision tasks has been a recent trend. Consequently, datasets having both visible and IR image pairs are desired in many applications. However, while large image datasets taken at the visible spectrum can be found in many domains, large IR-based datasets are not easily available in many domains. The lack of IR counterparts of the available visible image datasets limits existing deep algorithms to perform on IR images effectively. In this paper, to overcome with that challenge, we introduce a generative adversarial network (GAN) based solution and generate the IR equivalent of a given visible image by training our deep network to learn the relation between visible and IR modalities. In our proposed GAN architecture (InfraGAN), we introduce using structural similarity as an additional loss function. Furthermore, in our discriminator, we do not only consider the entire image being fake or real but also each pixel being fake or real. We evaluate our comparative results on three different datasets and report the state of the art results over five metrics when compared to Pix2Pix and ThermalGAN architectures from the literature. We report up to +16% better performance in Structural Similarity Index Measure (SSIM) over Pix2Pix and +8% better performance over ThermalGAN for VEDAI dataset. Further gains on different metrics and on different datasets are also reported in our experiments section.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000332",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Detector",
      "Discriminator",
      "Image (mathematics)",
      "Pattern recognition (psychology)",
      "Pixel",
      "Similarity (geometry)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Özkanoğlu",
        "given_name": "Mehmet Akif"
      },
      {
        "surname": "Ozer",
        "given_name": "Sedat"
      }
    ]
  },
  {
    "title": "ADCNN: Towards learning adaptive dilation for convolutional neural networks",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108369",
    "abstract": "Dilated convolution kernels are constrained by their shared dilation, keeping them from being aware of diverse spatial contents at different locations. We address such limitations by formulating the dilation as trainable weights with respect to individual positions. We propose Adaptive Dilation Convolutional Neural Networks (ADCNN), a light-weighted extension that allows convolutional kernels to adjust their dilation value based on different contents at the pixel level. Unlike previous content-adaptive models, ADCNN dynamically infers pixel-wise dilation via modeling feed-forward inter-patterns, which provides a new perspective for developing adaptive network structures other than sampling kernel spaces. Our evaluation results indicate ADCNNs can be easily integrated into various backbone networks and consistently outperform their regular counterparts on various visual tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005495",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Dilation (metric space)",
      "Discrete mathematics",
      "Kernel (algebra)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pixel"
    ],
    "authors": [
      {
        "surname": "Yao",
        "given_name": "Jie"
      },
      {
        "surname": "Wang",
        "given_name": "Dongdong"
      },
      {
        "surname": "Hu",
        "given_name": "Hao"
      },
      {
        "surname": "Xing",
        "given_name": "Weiwei"
      },
      {
        "surname": "Wang",
        "given_name": "Liqiang"
      }
    ]
  },
  {
    "title": "Iris R-CNN: Accurate iris segmentation and localization in non-cooperative environment with visible illumination",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.10.031",
    "abstract": "Despite the significant advances in iris segmentation, accomplishing accurate iris segmentation and localization in non-cooperative environment with visible illumination remains a grand challenge. In this paper, we present an end-to-end multi-task deep neural network, referred to as Iris R-CNN, to achieve superior accuracy for iris segmentation and localization. Inspired by instance segmentation, Iris R-CNN seamlessly integrates segmentation and localization in a unified framework and generates a normalized iris image/mask required for iris recognition. It is composed of several novel techniques to carefully explore the unique characteristics of iris. First, we propose two novel networks, (i) Double-Circle Region Proposal Network (DC-RPN) and (ii) Double-Circle Classification and Regression Network (DC-CRN), to efficiently capture pupil and iris circles and enhance the accuracy for iris localization. Second, we propose a novel normalization scheme for Regions of Interest (RoIs) to enable a radically new pooling operation over a double-circle region. To facilitate accurate training and validation, we annotate two public datasets in non-cooperative environment with visible illumination: NICE-II and MICHE. Experimental results on these two challenging datasets demonstrate the superior accuracy of our proposed approach over other state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003743",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Computer vision",
      "IRIS (biosensor)",
      "Iris recognition",
      "Normalization (sociology)",
      "Pattern recognition (psychology)",
      "Pooling",
      "Segmentation",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Feng",
        "given_name": "Xin"
      },
      {
        "surname": "Liu",
        "given_name": "Wenxing"
      },
      {
        "surname": "Li",
        "given_name": "Jiangang"
      },
      {
        "surname": "Meng",
        "given_name": "Zhiying"
      },
      {
        "surname": "Sun",
        "given_name": "Yufeng"
      },
      {
        "surname": "Feng",
        "given_name": "Chunyang"
      }
    ]
  },
  {
    "title": "Super-resolution guided knowledge distillation for low-resolution image classification",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.02.006",
    "abstract": "With the development of deep convolutional neural networks, the high-resolution image classification has achieved excellent classification results. However, in natural scenes, low-resolution images are very common, such as images taken by a webcam or images taken with a lens far away from the target object. Low-resolution image classification is a very difficult problem, because low-resolution images have small size and contain fewer discriminative features, which lead to a sharp decline in classification performance. In order to solve the above problem, this paper proposes a Super-Resolution guided Knowledge Distillation (SRKD) framework, which consists of two sub-networks: one is the super-resolution sub-network used to enhance the features of low-resolution images, and the other is the knowledge distillation sub-network used to minimize the difference between the features of high-resolution images and the features of the images output by the super-resolution sub-network. Extensive experiments on the Pascal VOC 2007 and CUB-200-2011 datasets show that the proposed method has a great improvement compared to the benchmark which is trained on high-resolution images. Especially in the case of very low resolution, the proposed method improves the mAP on Pascal VOC 2007 test set by 30.4% and improves the classification accuracy on CUB-200-2011 test set by 60.37% compared with the benchmark model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000496",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Contextual image classification",
      "Convolutional neural network",
      "Discriminative model",
      "Geodesy",
      "Geography",
      "Geology",
      "High resolution",
      "Image (mathematics)",
      "Image resolution",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Remote sensing",
      "Resolution (logic)",
      "Test set"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Hongyuan"
      },
      {
        "surname": "Pei",
        "given_name": "Yanting"
      },
      {
        "surname": "Zhao",
        "given_name": "Hongwei"
      },
      {
        "surname": "Huang",
        "given_name": "Yaping"
      }
    ]
  },
  {
    "title": "Driving behavior explanation with multi-level fusion",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108421",
    "abstract": "In this era of active development of autonomous vehicles, it becomes crucial to provide driving systems with the capacity to explain their decisions. In this work, we focus on generating high-level driving explanations as the vehicle drives. We present BEEF, for BEhavior Explanation with Fusion, a deep architecture which explains the behavior of a trajectory prediction model. Supervised by annotations of human driving decisions justifications, BEEF learns to fuse features from multiple levels. Leveraging recent advances in the multi-modal fusion literature, BEEF is carefully designed to model the correlations between high-level decisions features and mid-level perceptual features. The flexibility and efficiency of our approach are validated with extensive experiments on the HDD and BDD-X datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005975",
    "keywords": [
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Astronomy",
      "Chemistry",
      "Computer science",
      "Economics",
      "Electrical engineering",
      "Engineering",
      "Flexibility (engineering)",
      "Focus (optics)",
      "Fuse (electrical)",
      "Fusion",
      "Linguistics",
      "Machine learning",
      "Management",
      "Modal",
      "Neuroscience",
      "Optics",
      "Perception",
      "Philosophy",
      "Physics",
      "Polymer chemistry",
      "Psychology",
      "Trajectory",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Ben-Younes",
        "given_name": "Hédi"
      },
      {
        "surname": "Zablocki",
        "given_name": "Éloi"
      },
      {
        "surname": "Pérez",
        "given_name": "Patrick"
      },
      {
        "surname": "Cord",
        "given_name": "Matthieu"
      }
    ]
  },
  {
    "title": "Effective extraction of ventricles and myocardium objects from cardiac magnetic resonance images with a multi-task learning U-Net",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.10.025",
    "abstract": "Accurate extraction of semantic objects such as ventricles and myocardium from magnetic resonance (MR) images is one essential but very challenging task for the diagnosis of the cardiac diseases. To tackle this problem, in this paper, an automatic end-to-end supervised deep learning framework is proposed, using a multi-task learning based U-Net (MTL-UNet). Specifically, an edge extraction module and a fusion-based module are introduced for effectively capturing the contextual information such as continuous edges and consistent spatial patterns in terms of intensity and texture features. With a weighted triple loss including the dice loss, the cross-entropy loss and the edge loss, the accuracy of object segmentation and extraction has been effectively improved. Extensive experiments on the publicly available ACDC 2017 dataset have validated the efficacy and efficiency of the proposed MTL-UNet model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003846",
    "keywords": [
      "Artificial intelligence",
      "Cardiac Ventricle",
      "Cardiology",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Economics",
      "Entropy (arrow of time)",
      "Image segmentation",
      "Magnetic resonance imaging",
      "Management",
      "Medicine",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Radiology",
      "Segmentation",
      "Task (project management)",
      "Ventricle"
    ],
    "authors": [
      {
        "surname": "Ren",
        "given_name": "Jinchang"
      },
      {
        "surname": "Sun",
        "given_name": "He"
      },
      {
        "surname": "Zhao",
        "given_name": "Huimin"
      },
      {
        "surname": "Gao",
        "given_name": "Hao"
      },
      {
        "surname": "Maclellan",
        "given_name": "Calum"
      },
      {
        "surname": "Zhao",
        "given_name": "Sophia"
      },
      {
        "surname": "Luo",
        "given_name": "Xiaoyu"
      }
    ]
  },
  {
    "title": "Neighborhood linear discriminant analysis",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108422",
    "abstract": "Linear Discriminant Analysis (LDA) assumes that all samples from the same class are independently and identically distributed (i.i.d.). LDA may fail in the cases where the assumption does not hold. Particularly when a class contains several clusters (or subclasses), LDA cannot correctly depict the internal structure as the scatter matrices that LDA relies on are defined at the class level. In order to mitigate the problem, this paper proposes a neighborhood linear discriminant analysis (nLDA) in which the scatter matrices are defined on a neighborhood consisting of reverse nearest neighbors. Thus, the new discriminator does not need an i.i.d. assumption. In addition, the neighborhood can be naturally regarded as the smallest subclass, for which it is easier to be obtained than subclass without resorting to any clustering algorithms. The projected directions are sought to make sure that the within-neighborhood scatter as small as possible and the between-neighborhood scatter as large as possible, simultaneously. The experimental results show that nLDA performs significantly better than previous discriminators, such as LDA, LFDA, ccLDA, LM-NNDA, and l 2 , 1 -RLDA.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005987",
    "keywords": [
      "Algorithm",
      "Antibody",
      "Artificial intelligence",
      "Biology",
      "Class (philosophy)",
      "Cluster analysis",
      "Computer science",
      "Covariance matrix",
      "Detector",
      "Discriminant",
      "Discriminator",
      "Estimation of covariance matrices",
      "Immunology",
      "Linear discriminant analysis",
      "Mathematics",
      "Optimal discriminant analysis",
      "Pattern recognition (psychology)",
      "Scatter matrix",
      "Statistics",
      "Subclass",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Fa"
      },
      {
        "surname": "Gao",
        "given_name": "Junbin"
      },
      {
        "surname": "Yang",
        "given_name": "Jian"
      },
      {
        "surname": "Ye",
        "given_name": "Ning"
      }
    ]
  },
  {
    "title": "GEVE: A generative adversarial network for extremely dark image/video enhancement",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.10.030",
    "abstract": "A variety of information of the real-time scenes is carried by the images and videos. Processing these images and videos in an intelligent way helps in many domains such as computer vision, object detection, deep learning and 3D reconstruction leaving its large usage in applications such as auto pilots, augmented reality, smart vehicles, etc. The quality of image and videos plays a vital role in case of real-time systems. One such scenario is where the images are captured without sufficient illumination. Images captured in cameras where sufficient light is not present, leads to noisy and information-loss images. It is a fact that, dark images have mainly two aspects which make its study a difficult task. They are at its low dynamic range and high propensity for generating high noise levels. Hence, an approach based on deep learning based system is adopted. For this purpose, Generative Adversarial Network (GAN) based Extremely Dark Video Enhancement Network (GEVE) model is proposed. The main objective of GEVE is to team the model with low /normal- light image pairs. Thus, GAN network learns the translation from light feeble images and images captured under normal illumination and automatically translate original images taken under extremely low light conditions into images of quality. It is clearly observed that the proposed GEVE outperforms the known state-of-art techniques. We are the view that the proposed system is an ideal candidate to handle dark image/video frames.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003895",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Generative adversarial network",
      "Generative grammar",
      "Image (mathematics)"
    ],
    "authors": [
      {
        "surname": "Anitha",
        "given_name": "C."
      },
      {
        "surname": "Kumar",
        "given_name": "R.Mathusoothana S."
      }
    ]
  },
  {
    "title": "OCmst: One-class novelty detection using convolutional neural network and minimum spanning trees",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.11.013",
    "abstract": "We present a novel model called One Class Minimum Spanning Tree (OCmst) for novelty detection problem that uses a Convolutional Neural Network (CNN) as deep feature extractor and graph-based model based on Minimum Spanning Tree (MST). In a novelty detection scenario, the training data is no polluted by outliers (abnormal class) and the goal is to recognize if a test instance belongs to the normal class or the abnormal class. Our approach uses the deep features from CNN to feed a pair of MSTs built starting from each test instance. To cut down the computational time we use a parameter γ to specify the size of the MST’s starting to the neighbours from the test instance. To prove the effectiveness of the proposed approach we conducted experiments on two publicly available datasets, well-known in literature and we achieved the state-of-the-art results on the CIFAR10 dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521004049",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Class (philosophy)",
      "Combinatorics",
      "Computer science",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Graph",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Minimum spanning tree",
      "Novelty",
      "Novelty detection",
      "Outlier",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Spanning tree",
      "Theology",
      "Theoretical computer science",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "La Grassa",
        "given_name": "Riccardo"
      },
      {
        "surname": "Gallo",
        "given_name": "Ignazio"
      },
      {
        "surname": "Landro",
        "given_name": "Nicola"
      }
    ]
  },
  {
    "title": "Robust experience replay sampling for multi-agent reinforcement learning",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2021.11.006",
    "abstract": "Learning from the relevant experiences leads to fast convergence if the experiences provide useful information. We present the new and simple yet efficient technique to find suitable samples of experiences to train the agents in a given state of an environment. We intended to increase the number of states visited and unique sequences that efficiently reduce the number of states the agents have to explore or exploit. Our technique implicitly introduces additional strength to the exploration-exploitation trade-off. It filters the samples of experiences that can benefit more than half the number of agents and then utilizes the experiences to extract useful information for decision making. First, we compute the similarities between the observed state and previous states in the experiences to achieve this filtering. Then, we filter the samples using the hyper-parameter, z , to decide which experiences will be suitable. We found out that agents learn quickly and efficiently since sampled experiences provide useful information that speeds up convergence. In every episode, most agents learn or contribute to improve the total expected future return. We further study our approaches’ generalization ability and present different settings to show significant improvements in diverse experiment environments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865521003986",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Exploit",
      "Filter (signal processing)",
      "Generalization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Reinforcement learning",
      "Sampling (signal processing)",
      "State (computer science)"
    ],
    "authors": [
      {
        "surname": "Nicholaus",
        "given_name": "Isack Thomas"
      },
      {
        "surname": "Kang",
        "given_name": "Dae-Ki"
      }
    ]
  },
  {
    "title": "Rapid trajectory clustering based on neighbor spatial analysis",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.03.010",
    "abstract": "The existing trajectory clustering algorithms only use the position information in trajectory segmentation, which makes the selection of segment points unreliable. Meanwhile, the adopted distance metrics are originally designed to compare the whole trajectory, leads to the inaccuracy similarity of trajectory segments, and hence causes subsequent clustering risks. Moreover, the execution time of clustering significantly increases with the amount of data. To address these issues, the direction of velocity is introduced for improving the discrimination of segment points. Next, the shared nearest neighbor ( S N N ) similarity and Trajectory-Hausdorff distance are combined to construct the similarity matrix for overcoming the limitations of existing distance measures. Then, based on the R-tree index strategy, the neighbored trajectory segments are extracted and stored for fastening segment indexing. Finally, the Atlantic hurricane and elk datasets verify that the proposed algorithm can not only improve the clustering efficiency but also extract the trajectory model accurately.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000769",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Astronomy",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Distance matrix",
      "Economics",
      "Finance",
      "Fuzzy clustering",
      "Hausdorff distance",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Nearest neighbor search",
      "Pattern recognition (psychology)",
      "Physics",
      "Position (finance)",
      "Search engine indexing",
      "Segmentation",
      "Similarity (geometry)",
      "Trajectory",
      "Tree (set theory)",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Qiao",
        "given_name": "Dianfeng"
      },
      {
        "surname": "Yang",
        "given_name": "Xinyu"
      },
      {
        "surname": "Liang",
        "given_name": "Yan"
      },
      {
        "surname": "Hao",
        "given_name": "Xiaohui"
      }
    ]
  },
  {
    "title": "Self-supervised 2D face presentation attack detection via temporal sequence sampling",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.03.001",
    "abstract": "Conventional 2D face biometric systems are vulnerable to presentation attacks performed with different face artefacts, e.g., printouts, video-replays and wearable 3D masks. The research focus in face presentation attack detection (PAD) has been recently shifting towards end-to-end learning of deep representations directly from annotated data rather than designing hand-crafted (low-level) features. However, even the state-of-the-art deep learning based face PAD models have shown unsatisfying generalization performance when facing unknown attacks or acquisition conditions due to lack of representative training and tuning data available in the existing public benchmarks. To alleviate this issue, we propose a video pre-processing technique called Temporal Sequence Sampling (TSS) for 2D face PAD by removing the estimated inter-frame 2D affine motion in the view and encoding the appearance and dynamics of the resulting smoothed video sequence into a single RGB image. Furthermore, we leverage the features of a Convolutional Neural Network (CNN) by introducing a self-supervised representation learning scheme, where the labels are automatically generated by the TSS method as the stabilized frames accumulated over video clips of different temporal lengths provide the supervision. The learnt feature representations are then fine-tuned for the downstream task using labelled face PAD data. Our extensive experiments on four public benchmarks, namely Replay-Attack, MSU-MFSD, CASIA-FASD and OULU-NPU, demonstrate that the proposed framework provides promising generalization capability and encourage further study in this domain.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000605",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Discriminative model",
      "Face (sociological concept)",
      "Facial recognition system",
      "Feature learning",
      "Generalization",
      "Leverage (statistics)",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "RGB color model",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Muhammad",
        "given_name": "Usman"
      },
      {
        "surname": "Yu",
        "given_name": "Zitong"
      },
      {
        "surname": "Komulainen",
        "given_name": "Jukka"
      }
    ]
  },
  {
    "title": "Semi-supervised node classification via adaptive graph smoothing networks",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108492",
    "abstract": "Inspections on current graph neural networks suggest us to reconsider the computational aspect of the final aggregation. We consider that such aggregations perform a prediction smoothing and impute their potential drawbacks to be the inter-class interference implied by the underlying graphs. We aim at weakening the inter-class connections so that aggregations focus more on intra-class relations and producing smooth predictions according to weakening results. We apply a metric learning module to learn new edge weights and combine entropy losses to ensure the correspondence between the predictions and the learnt distances so that the weights of inter-class edges are reduced and predictions are smoothed according to the modified graph. Experiments on four citation networks and a Wiki network show that in comparison with other state-of-the-art graph neural networks, the proposed algorithm can improve the classification accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006683",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Entropy (arrow of time)",
      "Graph",
      "Machine learning",
      "Physics",
      "Quantum mechanics",
      "Smoothing",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Ruigang"
      },
      {
        "surname": "Chen",
        "given_name": "Weifu"
      },
      {
        "surname": "Feng",
        "given_name": "Guocan"
      }
    ]
  },
  {
    "title": "Velocity-to-velocity human motion forecasting",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108424",
    "abstract": "Forecasting human motion from a sequence of human poses is an important problem in the fields of computer vision and robotics. Most previous approaches merely consider learning the temporal dynamics of body joints or joint angles, while neglect derivatives of body joints (i.e., pose velocities) which could reasonably reduce noise impact and improve stability. To exploit the benefits of pose velocities, we propose the velocity-to-velocity learning paradigm for human motion prediction which attempts to directly build the sequence-to-sequence model in the velocity space. Two variant architectures based on recurrent encoder-decoder networks are introduced under this paradigm. Considering human motion as kinematics of rigid bodies, joint angles which denote transformation are the computations of inverse kinematics. Accordingly, a novel loss function in terms of rotation matrices is designed during training for human motion prediction through a rotation matrix transformation (RMT) layer. Finally, we present an effective training algorithm which exploits sequence transformation to improve model generalization. Our approaches substantially outperform state-of-the-art approaches on two large-scale datasets, Human3.6M and CMU Motion Capture, for both short-term prediction and long-term prediction. In particular, our model can competently forecast human-like and meaningful poses up to 1000 milliseconds. The code is available on GitHub: https://github.com/hongsong-wang/RNN_based_human_motion_prediction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006002",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Classical mechanics",
      "Computer science",
      "Computer vision",
      "Gene",
      "Generalization",
      "Genetics",
      "Kinematics",
      "Mathematical analysis",
      "Mathematics",
      "Motion (physics)",
      "Physics",
      "Robot",
      "Robotics",
      "Rotation (mathematics)",
      "Sequence (biology)",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Hongsong"
      },
      {
        "surname": "Wang",
        "given_name": "Liang"
      },
      {
        "surname": "Feng",
        "given_name": "Jiashi"
      },
      {
        "surname": "Zhou",
        "given_name": "Daquan"
      }
    ]
  },
  {
    "title": "Sketches by MoSSaRT: Representative selection from manifolds with gross sparse corruptions",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108454",
    "abstract": "Conventional sampling techniques fall short of selecting representatives that encode the underlying conformation of non-linear manifolds. The problem is exacerbated if the data is contaminated with gross sparse corruptions. In this paper, we present a data selection approach, dubbed MoSSaRT, which draws robust and descriptive sketches of grossly corrupted manifold structures. Built upon an explicit randomized transformation, we obtain a judiciously designed representation of the data relations, which facilitates a versatile selection approach accounting for robustness to gross corruption, descriptiveness and novelty of the chosen representatives, simultaneously. Our model lends itself to a convex formulation with an efficient parallelizable algorithm, which coupled with our randomized matrix structures gives rise to a highly scalable implementation. Theoretical analysis guarantees probabilistic convergence of the approximate function to the desired objective function and reveals insightful geometrical characterization of the chosen representatives. Finally, MoSSaRT substantially outperforms the state-of-the-art algorithms as demonstrated by experiments conducted on both real and synthetic data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006300",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Engineering",
      "Gene",
      "Manifold (fluid mechanics)",
      "Mathematical optimization",
      "Mathematics",
      "Mechanical engineering",
      "Robustness (evolution)",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Sedghi",
        "given_name": "Mahlagha"
      },
      {
        "surname": "Georgiopoulos",
        "given_name": "Michael"
      },
      {
        "surname": "Atia",
        "given_name": "George K."
      }
    ]
  },
  {
    "title": "Spatial-driven features based on image dependencies for person re-identification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108462",
    "abstract": "Person re-identification (Re-ID) aims to search for the same pedestrian in different cameras, which is a crucial research direction in pattern recognition. Recent deep learning methods have advanced the development of Re-ID. However, the existing approaches easily result in performance degradation in the case of larger scene data because they do not adequately consider the spatial dependencies of both the inter-image and the intra-image. The paper proposes a novel Spatial-Driven Network (SDN) to learn particularly discriminative features with abundant semantic information from both the inter-image and the intra-image dependencies for person Re-ID. Firstly, we design a global-correlation attention module to capture the inter-image dependencies among a series of different pedestrian images. Secondly, we present a local-correlation attention module to compute the intra-image dependencies from any pair of pixels within each pedestrian image. Furthermore, we propose a specific network integration mechanism, which carefully combines the above two complementary modules to match well the solution of the spatial dependency problem. We implement numerous experiments to assess the proposed SDN on mainstream person Re-ID databases. The results demonstrate that the proposed SDN outperforms most of the state-of-the-art methods in typical key criteria.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006385",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Data mining",
      "Dependency (UML)",
      "Discriminative model",
      "Engineering",
      "Identification (biology)",
      "Image (mathematics)",
      "Key (lock)",
      "Pattern recognition (psychology)",
      "Pedestrian",
      "Pedestrian detection",
      "Pixel",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Si",
        "given_name": "Tongzhen"
      },
      {
        "surname": "He",
        "given_name": "Fazhi"
      },
      {
        "surname": "Wu",
        "given_name": "Haoran"
      },
      {
        "surname": "Duan",
        "given_name": "Yansong"
      }
    ]
  },
  {
    "title": "Learning to rectify for robust learning with noisy labels",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108467",
    "abstract": "Label noise significantly degrades the generalization ability of deep models in applications. Effective strategies and approaches (e.g., re-weighting or loss correction) are designed to alleviate the negative impact of label noise when training a neural network. Those existing works usually rely on the pre-specified architecture and manually tuning the additional hyper-parameters. In this paper, we propose warped probabilistic inference (WarPI) to achieve adaptively rectifying the training procedure for the classification network within the meta-learning scenario. In contrast to the deterministic models, WarPI is formulated as a hierarchical probabilistic model by learning an amortization meta-network, which can resolve sample ambiguity and be therefore more robust to serious label noise. Unlike the existing approximated weighting function of directly generating weight values from losses, our meta-network is learned to estimate a rectifying vector from the input of the logits and labels, which has the capability of leveraging sufficient information lying in them. The procedure provides an effective way to rectify the learning procedure for the classification network, demonstrating a significant improvement of the generalization ability. Besides, modeling the rectifying vector as a latent variable and learning the meta-network can be seamlessly integrated into the SGD optimization of the classification network. We evaluate WarPI on four benchmarks of robust learning with noisy labels and achieve the new state-of-the-art under variant noise types. Extensive study and analysis also demonstrate the effectiveness of our model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006439",
    "keywords": [
      "Ambiguity",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer security",
      "Economics",
      "Generalization",
      "Image (mathematics)",
      "Inference",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Meta learning (computer science)",
      "Network architecture",
      "Noise (video)",
      "Probabilistic logic",
      "Programming language",
      "Radiology",
      "Subnetwork",
      "Task (project management)",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Haoliang"
      },
      {
        "surname": "Guo",
        "given_name": "Chenhui"
      },
      {
        "surname": "Wei",
        "given_name": "Qi"
      },
      {
        "surname": "Han",
        "given_name": "Zhongyi"
      },
      {
        "surname": "Yin",
        "given_name": "Yilong"
      }
    ]
  },
  {
    "title": "Source data-free domain adaptation for a faster R-CNN",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108436",
    "abstract": "The existing domain adaptive object detection methods often need to carry a large number of source domain samples for domain adaptation, which is not realistic due to GPU limitations, privacy and physical memory in practical applications. To solve this problem, we propose a source data-free domain adaptive object detection method. Only unlabeled target domain data is used to optimize the source domain model so that it can work better in the target domain. Our method takes Faster R-CNN as baseline. Specifically, we first construct global class prototypes which will be updated in batch iteratively. Then based on the global class prototypes, more accurate pseudo-labels are generated for training the target model. In this way, the source and target domains are also implicitly aligned. Our contributions are 1) a prototype guided domain adaptation method which uses prototypes to mine the semantic category information without accessing the source dataset; 2) a scheme of iteratively updating global class prototype which can handle the class and sample imbalances in the training procedure and 3) a more accurate pseudo-label generation method combining semantic information and image information. On multiple public domain adaptive scenarios, our method achieves the state-of-the-art results in terms of accuracy compared with the Faster R-CNN model and some domain adaptive methods with source datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006129",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Construct (python library)",
      "Data mining",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Image (mathematics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Scheme (mathematics)"
    ],
    "authors": [
      {
        "surname": "Xiong",
        "given_name": "Lin"
      },
      {
        "surname": "Ye",
        "given_name": "Mao"
      },
      {
        "surname": "Zhang",
        "given_name": "Dan"
      },
      {
        "surname": "Gan",
        "given_name": "Yan"
      },
      {
        "surname": "Liu",
        "given_name": "Yiguang"
      }
    ]
  },
  {
    "title": "Weakly-supervised semantic segmentation with superpixel guided local and global consistency",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108504",
    "abstract": "Weakly supervised semantic segmentation task aims to learn a segmentation model with only image-level annotations. Existing methods generally refine the initial seeds to obtain pseudo labels for training a fully supervised model. In recent years, some affinity-based methods perform well in this task. However, most of these methods only focus on the localization information from class activation map, while ignoring rule-based appearance information. In this paper, we find that the superpixel guidance is helpful for mining semantic affinities between pixels because pixels belonging to the same superpixel often have the same class label. As such, we propose a Superpixel Guided Weakly Segmentation framework, which alternately learns two modules to fuse superpixel information and localization information. The semantic segmentation results are more consistent with the image’s local and global consistency through our framework. Experiments show that the proposed method achieves state-of-the-art performance, with mIoU at 70.5% on the PASCAL VOC 2012 test set and mIoU at 34.4% on the MS-COCO 2014 val set.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006804",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Consistency (knowledge bases)",
      "Image segmentation",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Pixel",
      "Programming language",
      "Segmentation",
      "Set (abstract data type)",
      "Test set"
    ],
    "authors": [
      {
        "surname": "Yi",
        "given_name": "Sheng"
      },
      {
        "surname": "Ma",
        "given_name": "Huimin"
      },
      {
        "surname": "Wang",
        "given_name": "Xiang"
      },
      {
        "surname": "Hu",
        "given_name": "Tianyu"
      },
      {
        "surname": "Li",
        "given_name": "Xi"
      },
      {
        "surname": "Wang",
        "given_name": "Yu"
      }
    ]
  },
  {
    "title": "Face photo-sketch synthesis via full-scale identity supervision",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108446",
    "abstract": "Face photo-sketch synthesis refers transforming a face image between photo domain and sketch domain. It plays a crucial role in law enforcement and digital entertainment. A great deal of efforts have been devoted on face photo-sketch synthesis. However, limited by the weak identity supervision, existing methods mostly yield indistinct details or great deformation, resulting in poor perceptual appearance or low recognition accuracy. In the past several years, face identification achieved great progress, which represents the face images much more precisely than before. Considering the face image translation is also a type of face image re-representation, we attempt to introduce face recognition models to improve the synthesis performance. First, we applied existing synthesis models to augment the training set. Then, we proposed a full-scale identity supervision method to reduce redundant information introduced by these pseudo samples and take the valid information to enhance the intra-class variations. The proposed framework consists of two sub-networks: cross-domain translation (CT) network and intra-domain adaptation (IA) network. The CT network translates the input image from source domain to latent image of target domain, which overcomes the great gap between two domains with less structural deformation. The IA network adapts the perceptual appearance of latent image to target image by adversarial learning. Experimental results on CUHK Face Sketch Database and CUHK Face Sketch FERET Database demonstrate the proposed method preserved best perceptual appearance and more distinct details with less deformation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006221",
    "keywords": [
      "Acoustics",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Domain (mathematical analysis)",
      "Face (sociological concept)",
      "Facial recognition system",
      "Identity (music)",
      "Image (mathematics)",
      "Image translation",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Sketch",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Cao",
        "given_name": "Bing"
      },
      {
        "surname": "Wang",
        "given_name": "Nannan"
      },
      {
        "surname": "Li",
        "given_name": "Jie"
      },
      {
        "surname": "Hu",
        "given_name": "Qinghua"
      },
      {
        "surname": "Gao",
        "given_name": "Xinbo"
      }
    ]
  },
  {
    "title": "TradeBot: Bandit learning for hyper-parameters optimization of high frequency trading strategy",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108490",
    "abstract": "Quantitative trading takes advantage of mathematical functions for automatically making stock or futures trading decisions. Specifically, various trading strategies that proposed by human-experts are associated with weight hyper-parameters to determine the probability of selecting a specific strategy according to market conditions. Prior work manually adjusting the weight hyper-parameters is error-prone, because the essential advantage of quantitative trading, i.e., automation, is lost. In this paper, we propose a dynamic parameter tuning algorithm, i.e., TradeBot, based on bandit learning for quantitative trading. We consider sequentially selecting hyper-parameters of rules for trading as a bandit game, where a set of hyper-parameters of trading rule is considered as an action. A novel reward-agnostic Upper Confidence Bound bandit method is proposed to solve the automatically trading problem with a reward function estimated by inverse reinforcement learning. Experimental results on China Commodity Futures Market Data show state-of-the-art performance. To our best knowledge, this is one of the first work deployed in the online trading system via reinforcement learning, in published literature.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100666X",
    "keywords": [
      "Algorithmic trading",
      "Artificial intelligence",
      "Computer science",
      "Econometrics",
      "Economics",
      "Finance",
      "Financial economics",
      "Futures contract",
      "High-frequency trading",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Programming language",
      "Reinforcement learning",
      "Set (abstract data type)",
      "Trading strategy"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Weipeng"
      },
      {
        "surname": "Wang",
        "given_name": "Lu"
      },
      {
        "surname": "Xie",
        "given_name": "Liang"
      },
      {
        "surname": "Feng",
        "given_name": "Ke"
      },
      {
        "surname": "Liu",
        "given_name": "Xiang"
      }
    ]
  },
  {
    "title": "Self-restrained triplet loss for accurate masked face recognition",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108473",
    "abstract": "Using the face as a biometric identity trait is motivated by the contactless nature of the capture process and the high accuracy of the recognition algorithms. After the current COVID-19 pandemic, wearing a face mask has been imposed in public places to keep the pandemic under control. However, face occlusion due to wearing a mask presents an emerging challenge for face recognition systems. In this paper, we present a solution to improve masked face recognition performance. Specifically, we propose the Embedding Unmasking Model (EUM) operated on top of existing face recognition models. We also propose a novel loss function, the Self-restrained Triplet (SRT), which enabled the EUM to produce embeddings similar to these of unmasked faces of the same identities. The achieved evaluation results on three face recognition models, two real masked datasets, and two synthetically generated masked face datasets proved that our proposed approach significantly improves the performance in most experimental settings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100649X",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Computer vision",
      "Embedding",
      "Face (sociological concept)",
      "Face detection",
      "Facial recognition system",
      "Identity (music)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Process (computing)",
      "Social science",
      "Sociology",
      "Speech recognition",
      "Three-dimensional face recognition"
    ],
    "authors": [
      {
        "surname": "Boutros",
        "given_name": "Fadi"
      },
      {
        "surname": "Damer",
        "given_name": "Naser"
      },
      {
        "surname": "Kirchbuchner",
        "given_name": "Florian"
      },
      {
        "surname": "Kuijper",
        "given_name": "Arjan"
      }
    ]
  },
  {
    "title": "Action Transformer: A self-attention model for short-time pose-based human action recognition",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108487",
    "abstract": "Deep neural networks based purely on attention have been successful across several domains, relying on minimal architectural priors from the designer. In Human Action Recognition (HAR), attention mechanisms have been primarily adopted on top of standard convolutional or recurrent layers, improving the overall generalization capability. In this work, we introduce Action Transformer (AcT), a simple, fully, self-attentional architecture that consistently outperforms more elaborated networks that mix convolutional, recurrent, and attentive layers. In order to limit computational and energy requests, building on previous human action recognition research, the proposed approach exploits 2D pose representations over small temporal windows, providing a low latency solution for accurate and effective real-time performance. Moreover, we open-source MPOSE2021, a new large-scale dataset, as an attempt to build a formal training and evaluation benchmark for real-time, short-time HAR. The proposed methodology was extensively tested on MPOSE2021 and compared to several state-of-the-art architectures, proving the effectiveness of the AcT model and laying the foundations for future work on HAR.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006634",
    "keywords": [
      "Action (physics)",
      "Action recognition",
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Class (philosophy)",
      "Computer science",
      "Computer security",
      "Convolutional neural network",
      "Deep learning",
      "Electrical engineering",
      "Engineering",
      "Exploit",
      "Generalization",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Physics",
      "Quantum mechanics",
      "Transformer",
      "Visual arts",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Mazzia",
        "given_name": "Vittorio"
      },
      {
        "surname": "Angarano",
        "given_name": "Simone"
      },
      {
        "surname": "Salvetti",
        "given_name": "Francesco"
      },
      {
        "surname": "Angelini",
        "given_name": "Federico"
      },
      {
        "surname": "Chiaberge",
        "given_name": "Marcello"
      }
    ]
  },
  {
    "title": "Revisiting convexity-preserving signal recovery with the linearly involved GMC penalty",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.02.004",
    "abstract": "The generalized minimax concave (GMC) penalty is a newly proposed regularizer that can maintain the convexity of the objective function. This paper deals with signal recovery with the linearly involved GMC penalty. First, we propose a new method to set the matrix parameter in the penalty via solving a feasibility problem. The new method possesses appealing advantages over the existing method. Second, we recast the linearly involved GMC model as a saddle-point problem and use the primal-dual hybrid gradient (PDHG) algorithm to compute the solution. Another important work in this paper is that we provide guidance on the tuning parameter selection by proving desirable properties of the solution path. Finally, we apply the linearly involved GMC penalty to 1-D signal recovery and matrix regression. Numerical results show that the linearly involved GMC penalty can obtain better recovery performance and preserve the signal structure more successfully in comparison with the total variation (TV) regularizer.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000381",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Convexity",
      "Economics",
      "Financial economics",
      "Mathematical optimization",
      "Mathematics",
      "Programming language",
      "Radar",
      "SIGNAL (programming language)",
      "Signal processing",
      "Speech recognition",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Xiaoqian"
      },
      {
        "surname": "Chi",
        "given_name": "Eric C."
      }
    ]
  },
  {
    "title": "Multi-complementary and unlabeled learning for arbitrary losses and models",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108447",
    "abstract": "A weakly-supervised learning framework named as complementary-label learning has been proposed recently, where each sample is equipped with a single complementary label that denotes one of the classes the sample does not belong to. However, the existing complementary-label learning methods cannot learn from the easily accessible unlabeled samples and samples with multiple complementary labels, which are more informative. In this paper, to remove these limitations, we propose the novel multi-complementary and unlabeled learning framework that allows unbiased estimation of classification risk from samples with any number of complementary labels and unlabeled samples, for arbitrary loss functions and models. We first give an unbiased estimator of the classification risk from samples with multiple complementary labels, and then further improve the estimator by incorporating unlabeled samples into the risk formulation. The estimation error bounds show that the proposed methods are in the optimal parametric convergence rate. We also propose a risk correction scheme for alleviating over-fitting caused by negative empirical risk. Finally, the experiments on both linear and deep models show the effectiveness of our proposed methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006233",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Estimator",
      "Machine learning",
      "Mathematics",
      "Parametric statistics",
      "Pattern recognition (psychology)",
      "Sample (material)",
      "Statistics",
      "Supervised learning"
    ],
    "authors": [
      {
        "surname": "Cao",
        "given_name": "Yuzhou"
      },
      {
        "surname": "Liu",
        "given_name": "Shuqi"
      },
      {
        "surname": "Xu",
        "given_name": "Yitian"
      }
    ]
  },
  {
    "title": "Accelerated sparse nonnegative matrix factorization for unsupervised feature learning",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.01.020",
    "abstract": "Sparse Nonnegative Matrix Factorization (SNMF) is a fundamental unsupervised representation learning technique, and it represents low-dimensional features of a data set and lends itself to a clustering interpretation. However, the model and algorithm of SNMF have some shortcomings. In this work, we created a clustering method by improving the SNMF model and its Alternating Direction Multiplier Method acceleration algorithm. A novel, fast and closed-form iterative solution is proposed for SNMF with implicit sparse constraints which are L 1 and L 2 norms of the coefficient and basis matrixes, respectively. A low-dimensional feature space is also proposed as result of the closed-form iteration formats of each sub-problem obtained by variable splitting. In addition, the convergence points of the presented iterative algorithms are stationary points of the model. Finally, numerical experiments show that the improved algorithm is comparable to the sate-of-the-art methods in data clustering.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000277",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Coefficient matrix",
      "Composite material",
      "Computer science",
      "Eigenvalues and eigenvectors",
      "Feature (linguistics)",
      "Feature vector",
      "Gaussian",
      "Iterative method",
      "Linguistics",
      "Materials science",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Matrix decomposition",
      "Non-negative matrix factorization",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Sparse approximation",
      "Sparse matrix"
    ],
    "authors": [
      {
        "surname": "Xie",
        "given_name": "Ting"
      },
      {
        "surname": "Zhang",
        "given_name": "Hua"
      },
      {
        "surname": "Liu",
        "given_name": "Ruihua"
      },
      {
        "surname": "Xiao",
        "given_name": "Hanguang"
      }
    ]
  },
  {
    "title": "SC-LPR: Spatiotemporal context based LiDAR place recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.03.012",
    "abstract": "3D LiDAR-based place recognition remains challenging due to ambiguities and dynamics of scenes. Place representation can be learned effectively by deep learning, but perceptual confusion is inevitable based on single-scan. To alleviate the problem, we propose a new place recognition method named SC-LPR. In this method, spatiotemporal contextual information from LiDAR scans is used to increase the capacity of features’ representation. Firstly, a semantic graph is constructed to represent the topological geometric map of each LiDAR scan. Then, an end-to-end network is designed to predict similarity, in which GRU-EdgeConv++ is proposed to learn discriminative spatiotemporal feature representation and a novel C&TNT to predict a score. We evaluate our approach on the KITTI odometry benchmark. The experimental results show that our method can effectively fuse spatiotemporal information for place recognition. Compared with the state-of-the-art methods, our approach is superior on the KITTI dataset and can achieve competitive performance. To benefit the community by serving as a benchmark for place recognition, the code of our method will be made open-source on Github 1 1 https://github.com/Daideyun/SC-LPR .",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000782",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Geography",
      "Lidar",
      "Pattern recognition (psychology)",
      "Remote sensing"
    ],
    "authors": [
      {
        "surname": "Dai",
        "given_name": "Deyun"
      },
      {
        "surname": "Wang",
        "given_name": "Jikai"
      },
      {
        "surname": "Chen",
        "given_name": "Zonghai"
      },
      {
        "surname": "Bao",
        "given_name": "Peng"
      }
    ]
  },
  {
    "title": "Scene-specific crowd counting using synthetic training images",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108484",
    "abstract": "Crowd counting is a computer vision task on which considerable progress has recently been made thanks to convolutional neural networks. However, it remains a challenging task even in scene-specific settings, in real-world application scenarios where no representative images of the target scene are available, not even unlabelled, for training or fine-tuning a crowd counting model. Inspired by previous work in other computer vision tasks, we propose a simple but effective solution for the above application scenario, which consists of automatically building a scene-specific training set of synthetic images. Our solution does not require from end-users any manual annotation effort nor the collection of representative images of the target scene. Extensive experiments on several benchmark data sets show that the proposed solution can improve the effectiveness of existing crowd counting methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006609",
    "keywords": [
      "Annotation",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Economics",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Management",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Task (project management)",
      "Training set"
    ],
    "authors": [
      {
        "surname": "Delussu",
        "given_name": "Rita"
      },
      {
        "surname": "Putzu",
        "given_name": "Lorenzo"
      },
      {
        "surname": "Fumera",
        "given_name": "Giorgio"
      }
    ]
  },
  {
    "title": "Semantic clustering based deduction learning for image recognition and classification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108440",
    "abstract": "The paper proposes a semantic clustering based deduction learning by mimicking the learning and thinking process of human brains. Human beings can make judgments based on experience and cognition, and as a result, no one would recognize an unknown animal as a car. Inspired by this observation, we propose to train deep learning models using the clustering prior that can guide the models to learn with the ability of semantic deducing and summarizing from classification attributes, such as a cat belonging to animals while a car pertaining to vehicles. The proposed approach realizes the high-level clustering in the semantic space, enabling the model to deduce the relations among various classes during the learning process. In addition, the paper introduces a semantic prior based random search for the opposite labels to ensure the smooth distribution of the clustering and the robustness of the classifiers. The proposed approach is supported theoretically and empirically through extensive experiments. We compare the performance across state-of-the-art classifiers on popular benchmarks, and the generalization ability is verified by adding noisy labeling to the datasets. Experimental results demonstrate the superiority of the proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006166",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Cluster analysis",
      "Computer science",
      "Gene",
      "Generalization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Wenchi"
      },
      {
        "surname": "Tu",
        "given_name": "Xuemin"
      },
      {
        "surname": "Luo",
        "given_name": "Bo"
      },
      {
        "surname": "Wang",
        "given_name": "Guanghui"
      }
    ]
  },
  {
    "title": "Triangle-based outlier detection",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.03.008",
    "abstract": "For the last decades, anomaly detection has been one of the most common problems in data mining and computer science projects. The scientific community has made a great effort to develop methods and techniques for the detection of elements that deviate from the norm. Many of these techniques follow an unsupervised approach since anomalies are scarce and not easily reproducible. In this paper, a novel unsupervised anomaly detection method based on geometric reasoning is proposed. Triangle-based Outlier Detection (TOD) falls within the group of distance-based anomaly detection techniques and has been compared to state of the art methods for unsupervised anomaly detection. Results achieved prove that TOD offers greater robustness than alternative methods for the selection of the decision thresholds that condition the detection of outliers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000757",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Gene",
      "Machine learning",
      "Outlier",
      "Pattern recognition (psychology)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Navarro",
        "given_name": "Jorge"
      },
      {
        "surname": "Martín de Diego",
        "given_name": "Isaac"
      },
      {
        "surname": "Fernández",
        "given_name": "Rubén R."
      },
      {
        "surname": "Moguerza",
        "given_name": "Javier M."
      }
    ]
  },
  {
    "title": "MTCNet: Multi-task collaboration network for rotation-invariance face detection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108425",
    "abstract": "Detecting rotated faces is a challenging task with images from uncontrolled environments. The use of deep convolutional neural networks have greatly improved detection performance, but these methods still do not fully exploit face structure information. This leaves faces with more extreme rotation angles undetectable. In this paper, we present a novel Multi-Task Collaboration Network (MTCNet) for rotation-invariance face detection that fully uses facial landmarks to improve the detection performance by means of collaboration between face detection and face alignment. Differing from previous methods that predict rotation angles in a single step, MTCNet employs a cascaded architecture with three stages to predict faces with gradually decreasing rotation-in-plane ranges in a coarse-to-fine process. Accurate facial landmarks further facilitate face detection. We also introduce a new training loss by integrating the geometric angle into the penalization process, which is much more reasonable than measuring the differences of training samples roughly. Our approach also explores contextual information to distinguish challenging faces from unconstrained scenarios. Extensive experimental results were conducted to demonstrate the effectiveness of MTCNet on both the multiple orientation and rotation datasets. Empirical studies show that MTCNet achieves results competitive with state-of-the-art face detectors while being time-efficient.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006014",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Convolutional neural network",
      "Engineering",
      "Exploit",
      "Face (sociological concept)",
      "Face detection",
      "Facial recognition system",
      "Geometry",
      "Mathematics",
      "Operating system",
      "Orientation (vector space)",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Rotation (mathematics)",
      "Social science",
      "Sociology",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Lifang"
      },
      {
        "surname": "Zhao",
        "given_name": "Hui"
      },
      {
        "surname": "Leng",
        "given_name": "Jiaxu"
      }
    ]
  },
  {
    "title": "Detection and rectification of arbitrary shaped scene texts by using text keypoints and links",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108494",
    "abstract": "Detection and recognition of scene texts of arbitrary shapes remain a grand challenge due to the super-rich text shape variation in text line orientations, lengths, curvatures, etc. This paper presents a mask-guided multi-task network that detects and rectifies scene texts of arbitrary shapes reliably. Three types of keypoints are detected which specify the centre line and so the shape of text instances accurately. In addition, four types of keypoint links are detected of which the horizontal links associate the detected keypoints of each text instance and the vertical links predict a pair of landmark points (for each keypoint) along the upper and lower text boundary, respectively. Scene texts can be located and rectified by linking up the associated landmark points (giving localization polygon boxes) and transforming the polygon boxes via thin plate spline, respectively. Extensive experiments over several public datasets show that the use of text keypoints is tolerant to the variation in text orientations, lengths, and curvatures, and it achieves competitive scene text detection and rectification performance as compared with state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006701",
    "keywords": [
      "Artificial intelligence",
      "Astrophysics",
      "Boundary (topology)",
      "Computer science",
      "Computer vision",
      "Frame (networking)",
      "Geometry",
      "Landmark",
      "Line (geometry)",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Polygon (computer graphics)",
      "Power (physics)",
      "Quantum mechanics",
      "Rectification",
      "Telecommunications",
      "Variation (astronomy)"
    ],
    "authors": [
      {
        "surname": "Xue",
        "given_name": "Chuhui"
      },
      {
        "surname": "Lu",
        "given_name": "Shijian"
      },
      {
        "surname": "Hoi",
        "given_name": "Steven"
      }
    ]
  },
  {
    "title": "Deep collaborative multi-task network: A human decision process inspired model for hierarchical image classification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108449",
    "abstract": "Hierarchical classification is significant for big data, where the original task is divided into several sub-tasks to provide multi-granularity predictions based on a tree-shape label structure. Obviously, these sub-tasks are highly correlated: results of the coarser-grained sub-tasks can reduce the candidates for the fine-grained sub-tasks, while results of the fine-grained sub-tasks provide attributes describing the coarser-grained classes. A human can integrate feedbacks from all the related sub-tasks instead of considering each sub-task independently. Therefore, we propose a deep collaborative multi-task network for hierarchical image classification. Specifically, we first extract the relationship matrix between every two sub-tasks defined by the hierarchical label structure. Then, the information of each sub-task is broadcasted to all the related sub-tasks through the relationship matrix. Finally, to combine this information, a novel fusion function based on the task evaluation and the decision uncertainty is designed. Extensive experimental results demonstrate that our model can achieve state-of-the-art performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006257",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Decision tree",
      "Economics",
      "Granularity",
      "Hierarchical database model",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Task (project management)",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Yu"
      },
      {
        "surname": "Li",
        "given_name": "Xiaoni"
      },
      {
        "surname": "Zhou",
        "given_name": "Yucan"
      },
      {
        "surname": "Wang",
        "given_name": "Yu"
      },
      {
        "surname": "Hu",
        "given_name": "Qinghua"
      },
      {
        "surname": "Wang",
        "given_name": "Weiping"
      }
    ]
  },
  {
    "title": "Robust Gaussian process regression with a bias model",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108444",
    "abstract": "This paper presents a new approach to a robust Gaussian process regression, creating a non-parametric Bayesian regression estimate robust to outliers. Most existing approaches replace an outlier-prone Gaussian likelihood with a non-Gaussian likelihood induced from a heavy tail distribution, such as the Laplace distribution and Student-t distribution. However, the use of a non-Gaussian likelihood would incur the need for a computationally expensive Bayesian approximate computation in the posterior inferences. The proposed approach models an outlier as a noisy and biased observation of an unknown regression function, and accordingly, the likelihood contains bias terms to explain the degree of deviations from the regression function. We introduce two bias models that handle the bias terms differently, treating a bias as an unknown and fixed quantity or treating a bias as a random quantity. We entail how the biases can be estimated accurately with other hyperparameters by a regularized maximum likelihood estimation. Conditioned on the bias estimates, the robust GP regression can be reduced to a standard GP regression problem with analytical forms of the predictive mean and variance estimates. Therefore, the proposed approach is simple and very computationally attractive. It also gives a very robust and accurate GP estimate for many tested scenarios. For the numerical evaluation, we perform a comprehensive simulation study to evaluate the proposed approach with the comparison to the existing robust GP approaches under various simulated scenarios of different outlier proportions and different noise levels. The approach is applied to data from two measurement systems, where the predictors are based on robust environmental parameter measurements and the response variables utilize more complex chemical sensing methods that contain a certain percentage of outliers. The utility of the measurement systems and value of the environmental data are improved through the computationally efficient GP regression and bias model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006208",
    "keywords": [
      "Algorithm",
      "Bayesian inference",
      "Bayesian linear regression",
      "Bayesian probability",
      "Computer science",
      "Estimation theory",
      "Gaussian",
      "Gaussian process",
      "Hyperparameter",
      "Kriging",
      "Laplace's method",
      "Likelihood function",
      "Mathematics",
      "Outlier",
      "Physics",
      "Quantum mechanics",
      "Regression",
      "Regression analysis",
      "Robust regression",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Park",
        "given_name": "Chiwoo"
      },
      {
        "surname": "Borth",
        "given_name": "David J."
      },
      {
        "surname": "Wilson",
        "given_name": "Nicholas S."
      },
      {
        "surname": "Hunter",
        "given_name": "Chad N."
      },
      {
        "surname": "Friedersdorf",
        "given_name": "Fritz J."
      }
    ]
  },
  {
    "title": "HITS: Binarizing physiological time series with deep hashing neural network",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.03.003",
    "abstract": "In this paper, we aim to transform numerical physiological time series into binary hash codes, that can be further used for indexing large-scale dataset, or accelerating downstream tasks such as instance-based classification. We propose HITS to learn binary Hash codes from physIological Time Series. HITS first builds a very deep one-dimensional convolutional neural network to learn lower-dimensional representations from raw physiological time series. Then, HITS jointly learns high utility, similarity preserving, and temporal related compact binary codes by corresponding objectives with imposed. Finally, given a new physiological time series, HITS can encode it to a binary hash code. Experiments are performed on two real-world Electrocardiogram and Electroencephalogram datasets. The accuracy of a k-nearest classifier is used to evaluate the quality of codes. HITS outperforms the second-best baseline 7.42% on average of k = { 1 , 2 , 4 , 8 , 16 , 32 } with code length c = 48 , while reducing 45.51% searching time than the same length numerical vectors on average of c = { 16 , 24 , 32 , 48 , 64 } . HITS consistently achieve higher classification accuracy than compared methods using k-nearest classifier varying different k and code length. Thus, HITS learns better binary hash codes than compared methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000629",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Binary code",
      "Binary number",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Classifier (UML)",
      "Code (set theory)",
      "Computer science",
      "Computer security",
      "Convolutional neural network",
      "Data mining",
      "ENCODE",
      "Gene",
      "Hash function",
      "Hash table",
      "Mathematics",
      "Nearest neighbor search",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Programming language",
      "Search engine indexing",
      "Series (stratigraphy)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Fu",
        "given_name": "Zhaoji"
      },
      {
        "surname": "Wang",
        "given_name": "Can"
      },
      {
        "surname": "Wei",
        "given_name": "Guodong"
      },
      {
        "surname": "Zhang",
        "given_name": "Wenrui"
      },
      {
        "surname": "Du",
        "given_name": "Shaofu"
      },
      {
        "surname": "Hong",
        "given_name": "Shenda"
      }
    ]
  },
  {
    "title": "IoU-Balanced loss functions for single-stage object detection",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.01.021",
    "abstract": "Single-stage object detectors have been widely applied in computer vision applications due to their high efficiency. However, the loss functions adopted by single-stage detectors hurt the localization accuracy seriously. Firstly, the cross-entropy loss for classification is independent of the localization task and drives all the positive examples to learn as high classification scores as possible regardless of localization accuracy. Thus, there exist many detections with high classification scores but low IoU or detections with low classification scores but high IoU. Secondly, for the smooth L1 loss, the gradient is dominated by the outliers with poor localization accuracy. In this work, IoU-balanced loss functions consisting of IoU-balanced classification loss and IoU-balanced localization loss are proposed to solve these problems. IoU-balanced classification loss pays more attention to positive examples with high IoU and enhances the correlation between classification and localization tasks. IoU-balanced localization loss decreases the gradient of examples with low IoU and increases the gradient of examples with high IoU, which improves the localization accuracy of models. Extensive experiments on MS COCO, PASCAL VOC, Cityscapes and WIDERFace demonstrate that IoU-balanced losses can substantially improve the popular single-stage detectors, especially the localization accuracy. On COCO test-dev, the proposed methods can substantially improve AP by 1.0 % ∼ 1.7 % and AP 75 by 1.0 % ∼ 2.4 % . On PASCAL VOC, Cityscape and WIDERFace, it can also substantially improve AP by 1.0 % ∼ 1.5 % and A P 80 , A P 90 by ∼ 3.9 % . The source code will be made publicly available.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000289",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Detector",
      "Object detection",
      "Outlier",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Shengkai"
      },
      {
        "surname": "Yang",
        "given_name": "Jinrong"
      },
      {
        "surname": "Wang",
        "given_name": "Xinggang"
      },
      {
        "surname": "Li",
        "given_name": "Xiaoping"
      }
    ]
  },
  {
    "title": "Text-instance graph: Exploring the relational semantics for text-based visual question answering",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108455",
    "abstract": "It is time to stop neglecting the text around your world. In VQA, the surrounding text helps humans to understand complete visual scenes and reason question semantics efficiently. Here, we address the challenging Text-based Visual Question Answering (TextVQA) problem, which requires a model to answer the VQA questions with text reading ability. Existing TextVQA methods mainly focus on the latent relationships between detected object instances and scene texts with the given question, but ignore spatial location relationships and complex relational semantics between visual object instances and OCR texts (e.g. the A of B on C). To deal with these challenges, we propose a novel Text-Instance Graph (TIG) network for TextVQA. The TIG builds an OCR-OBJ graph for overlapping relationships modeling, where each node of graph is updated by utilizing relative objects or OCR texts. To deal with the question with complex logic, we propose a dynamic OCR-OBJ graph network to extend the perception space of graph nodes, which grasps the information of non-directly adjacent node features. Considering a scene about “the brand of the computer on the table”, the model would build correlations between “brand” and “table” using “the computer” node as the intermediate node. Extensive experiments on three benchmarks demonstrate the effectiveness and superiority of the proposed method. In addition, our TIG achieves 0.505 ANLS on ST-VQA challenge leaderboard and sets a new state-of-the-art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006312",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Graph",
      "Graph database",
      "Information retrieval",
      "Natural language processing",
      "Programming language",
      "Question answering",
      "Semantics (computer science)",
      "Theoretical computer science",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xiangpeng"
      },
      {
        "surname": "Wu",
        "given_name": "Bo"
      },
      {
        "surname": "Song",
        "given_name": "Jingkuan"
      },
      {
        "surname": "Gao",
        "given_name": "Lianli"
      },
      {
        "surname": "Zeng",
        "given_name": "Pengpeng"
      },
      {
        "surname": "Gan",
        "given_name": "Chuang"
      }
    ]
  },
  {
    "title": "Semi-supervised robust training with generalized perturbed neighborhood",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108472",
    "abstract": "Adversarial examples have been shown to be a severe threat to deep neural networks (DNNs). One of the most effective adversarial defense methods is adversarial training (AT) through minimizing the adversarial risk R a d v , which encourages both the benign example x and its adversarially perturbed neighborhoods within the ℓ p -ball to be predicted as the ground-truth label. In this paper, we propose a novel defense method, the robust training (RT), by jointly minimizing two separated risks ( i . e . , R s t a n d and R r o b ), which are with respect to the benign example and its neighborhoods, respectively. The motivation is to explicitly and jointly enhance the accuracy and the adversarial robustness. We prove that R a d v is upper-bounded by R s t a n d + R r o b , which implies that RT has similar effect as AT. Intuitively, minimizing the standard risk enforces the benign example to be correctly predicted, while the robust risk minimization encourages the predictions of the neighbor examples to be consistent with the prediction of the benign example. Besides, since R r o b is independent of the ground-truth label, RT is naturally extended to the semi-supervised mode ( i . e . , SRT), to further enhance its effectiveness. Moreover, we extend the ℓ p -bounded neighborhood to a general case, which covers different types of perturbations, such as the pixel-wise ( i . e . , x + δ ) or the spatial perturbation ( i . e . , A x + b ). Extensive experiments on benchmark datasets not only verify the superiority of the proposed SRT to state-of-the-art methods for defending pixel-wise or spatial perturbations separately but also demonstrate its robustness to both perturbations simultaneously. Our work may shed the light on the understanding of universal model robustness and the potential of unlabeled samples. The code for reproducing main results is available at https://github.com/THUYimingLi/Semi-supervised_Robust_Training.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006488",
    "keywords": [
      "Adversarial system",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Bounded function",
      "Chemistry",
      "Computer science",
      "Deep neural networks",
      "Gene",
      "Ground truth",
      "Machine learning",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Minification",
      "Pattern recognition (psychology)",
      "Pixel",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yiming"
      },
      {
        "surname": "Wu",
        "given_name": "Baoyuan"
      },
      {
        "surname": "Feng",
        "given_name": "Yan"
      },
      {
        "surname": "Fan",
        "given_name": "Yanbo"
      },
      {
        "surname": "Jiang",
        "given_name": "Yong"
      },
      {
        "surname": "Li",
        "given_name": "Zhifeng"
      },
      {
        "surname": "Xia",
        "given_name": "Shu-Tao"
      }
    ]
  },
  {
    "title": "Unsupervised cross-domain person re-identification by instance and distribution alignment",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108514",
    "abstract": "Most existing person re-identification (re-id) methods assume supervised model training on a separate large set of training samples from the target domain. While performing well in the training domain, such trained models are seldom generalisable to a new independent unsupervised target domain without further labelled training data from the target domain. To solve this scalability limitation, we develop a novel Hierarchical Unsupervised Domain Adaptation (HUDA) method. It can transfer labelled information of an existing dataset (a source domain) to an unlabelled target domain for unsupervised person re-id. Specifically, HUDA is designed to model jointly global distribution alignment and local instance alignment in a two-level hierarchy for discovering transferable source knowledge in unsupervised domain adaptation. Crucially, this approach aims to overcome the under-constrained learning problem of existing unsupervised domain adaptation methods. Extensive evaluations show the superiority of HUDA for unsupervised cross-domain person re-id over a wide variety of state-of-the-art methods on four re-id benchmarks: Market-1501, DukeMTMC, MSMT17 and CUHK03.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006907",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Classifier (UML)",
      "Computer science",
      "Database",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Economics",
      "Hierarchy",
      "Identification (biology)",
      "Machine learning",
      "Market economy",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Scalability",
      "Set (abstract data type)",
      "Transfer of learning",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Lan",
        "given_name": "Xu"
      },
      {
        "surname": "Zhu",
        "given_name": "Xiatian"
      },
      {
        "surname": "Gong",
        "given_name": "Shaogang"
      }
    ]
  },
  {
    "title": "Sparse attention block: Aggregating contextual information for object detection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108418",
    "abstract": "It is well recognized that the contextual information of surrounding objects is beneficial for object detection. Such contextual information can often be obtained from long-range dependencies. This paper proposes a sparse attention block to capture long-range dependencies in an efficient way. Unlike the conventional non-local block, which generates a dense attention map to characterize the dependency between any two positions of the input feature map, our sparse attention block samples the most representative positions for contextual information aggregation. After searching for local peaks in a heat map of the given input feature map, it adaptively selects a sparse set of positions to represent the relationship between query and key elements. With the obtained sparse positions, our sparse attention block can well model long-range dependencies, and greatly improve the object detection performance at the additional cost of < 2% GPU memory and computation of the conventional non-local block. This sparse attention block can be easily plugged into various object detection frameworks, such as Faster R-CNN, RetinaNet and Mask R-CNN. Experiments on COCO benchmark confirm that our sparse attention block can boost the detection accuracy with significant gains ranging from 1.4% to 1.9% and negligible overhead of computation and memory usage.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100594X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Block (permutation group theory)",
      "Composite material",
      "Computation",
      "Computer science",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Geometry",
      "Linguistics",
      "Materials science",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Operating system",
      "Overhead (engineering)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Range (aeronautics)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Chunlin"
      },
      {
        "surname": "Yu",
        "given_name": "Jun"
      },
      {
        "surname": "Ling",
        "given_name": "Qiang"
      }
    ]
  },
  {
    "title": "CANet: Co-attention network for RGB-D semantic segmentation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108468",
    "abstract": "Incorporating the depth (D) information to RGB images has proven the effectiveness and robustness in semantic segmentation. However, the fusion between them is not trivial due to their inherent physical meaning discrepancy, in which RGB represents RGB information but D depth information. In this paper, we propose a co-attention network (CANet) to build sound interaction between RGB and depth features. The key part in the CANet is the co-attention fusion part. It includes three modules. Specifically, the position and channel co-attention fusion modules adaptively fuse RGB and depth features in spatial and channel dimensions. An additional fusion co-attention module further integrates the outputs of the position and channel co-attention fusion modules to obtain a more representative feature which is used for the final semantic segmentation. Extensive experiments witness the effectiveness of the CANet in fusing RGB and depth features, achieving state-of-the-art performance on two challenging RGB-D semantic segmentation datasets, i.e., NYUDv2 and SUN-RGBD.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006440",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Channel (broadcasting)",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Fusion",
      "Gene",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "RGB color model",
      "Robustness (evolution)",
      "Segmentation",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Hao"
      },
      {
        "surname": "Qi",
        "given_name": "Lu"
      },
      {
        "surname": "Huang",
        "given_name": "Hai"
      },
      {
        "surname": "Yang",
        "given_name": "Xu"
      },
      {
        "surname": "Wan",
        "given_name": "Zhaoliang"
      },
      {
        "surname": "Wen",
        "given_name": "Xianglong"
      }
    ]
  },
  {
    "title": "A survey on biometric recognition using wearable devices",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.03.002",
    "abstract": "Thanks to their ability to monitor physical activity and health-related parameters, wearable devices are becoming more and more popular. In addition to what they already offer, an interesting capability achievable through such devices is biometric recognition. The physiological traits recorded by wearable devices may in fact possess distinctive properties which could allow to recognize their legitimate users, and detect unauthorized usage. In this paper, the most recent advances accomplished in this field are reviewed, and a critical analysis on the current state of the art, as well as on the issues still open, is provided.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000617",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Embedded system",
      "Field (mathematics)",
      "Human–computer interaction",
      "Mathematics",
      "Pure mathematics",
      "Wearable computer",
      "Wearable technology"
    ],
    "authors": [
      {
        "surname": "Maiorana",
        "given_name": "Emanuele"
      }
    ]
  },
  {
    "title": "SibNet: Food instance counting and segmentation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108470",
    "abstract": "Food computing has recently attracted considerable research attention due to its significance for health risk analysis. In the literature, the majority of research efforts are dedicated to food recognition. Relatively few works are conducted for food counting and segmentation, which are essential for portion size estimation. This paper presents a deep neural network, named SibNet, for simultaneous counting and extraction of food instances from an image. The problem is challenging due to varying size and shape of food as well as arbitrary viewing angle of camera, not to mention that food instances often occlude each other. SibNet is novel for proposal of learning seed map to minimize the overlap between instances. The map facilitates counting and can be completed as an instance segmentation map that depicts the arbitrary shape and size of individual instance under occlusion. To this end, a novel sibling relation sub-network is proposed for pixel connectivity analysis. Along with this paper, three new datasets covering Western, Chinese and Japanese food are also constructed for performance evaluation. The three datasets and SibNet source code are publicly available.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006464",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Code (set theory)",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Image segmentation",
      "Pattern recognition (psychology)",
      "Pixel",
      "Programming language",
      "Relation (database)",
      "Segmentation",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Nguyen",
        "given_name": "Huu-Thanh"
      },
      {
        "surname": "Ngo",
        "given_name": "Chong-Wah"
      },
      {
        "surname": "Chan",
        "given_name": "Wing-Kwong"
      }
    ]
  },
  {
    "title": "COVID-MTL: Multitask learning with Shift3D and random-weighted loss for COVID-19 diagnosis and severity assessment",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108499",
    "abstract": "There is an urgent need for automated methods to assist accurate and effective assessment of COVID-19. Radiology and nucleic acid test (NAT) are complementary COVID-19 diagnosis methods. In this paper, we present an end-to-end multitask learning (MTL) framework (COVID-MTL) that is capable of automated and simultaneous detection (against both radiology and NAT) and severity assessment of COVID-19. COVID-MTL learns different COVID-19 tasks in parallel through our novel random-weighted loss function, which assigns learning weights under Dirichlet distribution to prevent task dominance; our new 3D real-time augmentation algorithm (Shift3D) introduces space variances for 3D CNN components by shifting low-level feature representations of volumetric inputs in three dimensions; thereby, the MTL framework is able to accelerate convergence and improve joint learning performance compared to single-task models. By only using chest CT scans, COVID-MTL was trained on 930 CT scans and tested on separate 399 cases. COVID-MTL achieved AUCs of 0.939 and 0.846, and accuracies of 90.23% and 79.20% for detection of COVID-19 against radiology and NAT, respectively, which outperformed the state-of-the-art models. Meanwhile, COVID-MTL yielded AUC of 0.800 ± 0.020 and 0.813 ± 0.021 (with transfer learning) for classifying control/suspected, mild/regular, and severe/critically-ill cases. To decipher the recognition mechanism, we also identified high-throughput lung features that were significantly related (P < 0.001) to the positivity and severity of COVID-19.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006750",
    "keywords": [
      "2019-20 coronavirus outbreak",
      "Artificial intelligence",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Disease",
      "Economics",
      "Infectious disease (medical specialty)",
      "Machine learning",
      "Management",
      "Medicine",
      "Multi-task learning",
      "Outbreak",
      "Pathology",
      "Pattern recognition (psychology)",
      "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)",
      "Task (project management)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Bao",
        "given_name": "Guoqing"
      },
      {
        "surname": "Chen",
        "given_name": "Huai"
      },
      {
        "surname": "Liu",
        "given_name": "Tongliang"
      },
      {
        "surname": "Gong",
        "given_name": "Guanzhong"
      },
      {
        "surname": "Yin",
        "given_name": "Yong"
      },
      {
        "surname": "Wang",
        "given_name": "Lisheng"
      },
      {
        "surname": "Wang",
        "given_name": "Xiuying"
      }
    ]
  },
  {
    "title": "Progressive polarization based reflection removal via realistic training data generation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108497",
    "abstract": "The reflection effect is unavoidable when taking photos through glasses or other transparent materials, which introduces undesired information into pictures. Hence, removing the influence of reflection becomes a key problem in computer vision. One of the main obstacles of recent learning based approaches is the lacking of realistic training data. To address this issue, we introduce a new dataset synthesis method as well as a novel neural network architecture for single image reflection removal. First, we make use of the polarization characteristics of light into the synthesis of datasets, so as to obtain more realistic and diversified training dataset POL. Then, we design a novel Progressive Polarization based Reflection Removal Network ( P 2 R 2 Net), which preliminary estimates the coarse background layer to guide the final reflection removal. We demonstrate that our method performs better than the state-of-the-art single image reflection removal methods through quantitative and qualitative experimental comparisons. Specifically, the average PSNR of our restored images selected from three representative benchmark datesets: “Real20”, “ SI R 2 ” and “Nature” is improved at least 0.49 compared with existing methods and reaches to 24.52.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006737",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Meteorology",
      "Optics",
      "Physical chemistry",
      "Physics",
      "Polarization (electrochemistry)",
      "Programming language",
      "Reflection (computer programming)",
      "Training (meteorology)",
      "Training set"
    ],
    "authors": [
      {
        "surname": "Pang",
        "given_name": "Youxin"
      },
      {
        "surname": "Yuan",
        "given_name": "Mengke"
      },
      {
        "surname": "Fu",
        "given_name": "Qiang"
      },
      {
        "surname": "Ren",
        "given_name": "Peiran"
      },
      {
        "surname": "Yan",
        "given_name": "Dong-Ming"
      }
    ]
  },
  {
    "title": "A new framework of designing iterative techniques for image deblurring",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108463",
    "abstract": "In this work we present a framework of designing iterative techniques for image deblurring in inverse problem. The new framework is based on two observations about existing methods. We used Landweber method as the basis to develop and present the new framework but note that the framework is applicable to other iterative techniques. First, we observed that the iterative steps of Landweber method consist of a constant term, which is a low-pass filtered version of the already blurry observation. We proposed a modification to use the observed image directly. Second, we observed that Landweber method uses an estimate of the true image as the starting point. This estimate, however, does not get updated over iterations. We proposed a modification that updates this estimate as the iterative process progresses. We integrated the two modifications into one framework of iteratively deblurring images. Finally, we tested the new method and compared its performance with several existing techniques, including Landweber method, Van Cittert method, GMRES (generalized minimal residual method), and LSQR (least square), to demonstrate its superior performance in image deblurring.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006397",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deblurring",
      "Generalized minimal residual method",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Iterative method",
      "Mathematical optimization",
      "Mathematics",
      "Residual"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Min"
      },
      {
        "surname": "Young",
        "given_name": "Geoffrey S."
      },
      {
        "surname": "Tie",
        "given_name": "Yanmei"
      },
      {
        "surname": "Gu",
        "given_name": "Xianfeng"
      },
      {
        "surname": "Xu",
        "given_name": "Xiaoyin"
      }
    ]
  },
  {
    "title": "Global models for time series forecasting: A Simulation study",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108441",
    "abstract": "The recent advances in Big Data have opened up the opportunity to develop competitive Global Forecasting Models (GFM) that simultaneously learn from many time series. Although, the concept of series relatedness has been heavily exploited with GFMs to explain their superiority over local statistical benchmarks, this concept remains largely under-investigated in an empirical setting. Hence, this study attempts to explore the factors that affect GFM performance, by simulating a number of datasets having controllable characteristics. The factors being controlled are along the homogeneity/heterogeneity of series, the complexity of patterns in the series, the complexity of forecasting models, and the lengths/number of series. We simulate time series from simple Data Generating Processes (DGP), such as Auto Regressive (AR), Seasonal AR and Fourier Terms to complex DGPs, such as Chaotic Logistic Map, Self-Exciting Threshold Auto-Regressive and Mackey-Glass Equations. We perform experiments on these datasets using Recurrent Neural Networks (RNN), Feed-Forward Neural Networks, Pooled Regression models and Light Gradient Boosting Models (LGBM) built as GFMs, and compare their performance against standard statistical forecasting techniques. Our experiments demonstrate that with respect to GFM performance, relatedness is closely associated with other factors such as the availability of data, complexity of data and the complexity of the forecasting technique used. Also, techniques such as RNNs and LGBMs having complex non-linear modelling capabilities, when built as GFMs are competitive methods under challenging forecasting scenarios such as short series, heterogeneous series and having minimal prior knowledge of the data patterns.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006178",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Artificial neural network",
      "Autoregressive model",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Econometrics",
      "Geography",
      "Machine learning",
      "Mathematics",
      "Multivariate statistics",
      "Paleontology",
      "Series (stratigraphy)",
      "Time series",
      "Univariate"
    ],
    "authors": [
      {
        "surname": "Hewamalage",
        "given_name": "Hansika"
      },
      {
        "surname": "Bergmeir",
        "given_name": "Christoph"
      },
      {
        "surname": "Bandara",
        "given_name": "Kasun"
      }
    ]
  },
  {
    "title": "Network optimization using defender system in cloud computing security based intrusion detection system withgame theory deep neural network (IDSGT-DNN)",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.02.013",
    "abstract": "Cloud computing leads an organization for data storage, sharing, processing, and other services. It was subjected to several challenges insecurity due to the presence of regular attacks. These security challenges are worsened due to the presence of various attack environments. The conventional techniques adopted in cloud security are Intrusion Detection System (IDS). However, the IDS system requires an efficient security model for improving security in the cloud. In this paper, to improve the security in the cloud IDS system developed a framework stated as IDSGT-DNN. The proposed model incorporates an attacker and a defender mechanism for attack and normal data processing. The developed game theory is implemented in the DNN model with IWA for the identification of optimal solutions. The estimation is based on the identification of minimal and maximal optimal points in the dataset. The proposed IDSGT-DNN model is evaluated for CICIDS - 2017 dataset. The performance of proposed IDSGT-DNN is evaluated with existing technique is performed. Simulation analysis exhibited that proposed IDSGT-DNN exhibits enhanced performance in terms of accuracy, detection rate, and precision, F-Score, AUC, and FPR.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000551",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Botany",
      "Cloud computing",
      "Computer science",
      "Computer security",
      "Data mining",
      "Identification (biology)",
      "Intrusion detection system",
      "Network security",
      "Operating system",
      "Real-time computing"
    ],
    "authors": [
      {
        "surname": "Balamurugan",
        "given_name": "E"
      },
      {
        "surname": "Mehbodniya",
        "given_name": "Abolfazl"
      },
      {
        "surname": "Kariri",
        "given_name": "Elham"
      },
      {
        "surname": "Yadav",
        "given_name": "Kusum"
      },
      {
        "surname": "Kumar",
        "given_name": "Anil"
      },
      {
        "surname": "Anul Haq",
        "given_name": "Mohd"
      }
    ]
  },
  {
    "title": "CR-GAN: Automatic craniofacial reconstruction for personal identification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108400",
    "abstract": "Craniofacial reconstruction is applied to identify human remains in the absence of determination data (e.g., fingerprinting, dental records, radiological materials, or DNA), by predicting the likeness of the unidentified remains based on the internal relationship between the skull and face. Conventional 3D methods are usually based on statistical models with poor capacity, which limit the description of such complex relationship. Moreover, the required high-quality data are difficult to collect. In this study, we present a novel craniofacial reconstruction paradigm that synthesize craniofacial images from 2D computed tomography scan of skull data. The key idea is to recast craniofacial reconstruction as an image translation task, with the goal of generating corresponding craniofacial images from 2D skull images. To this end, we design an automatic skull-to-face transformation system based on deep generative adversarial nets. The system was trained on 4551 paired skull-face images obtained from 1780 CT head scans of the Han Chinese population. To the best of our knowledge, this is the only database of this magnitude in the literature. Finally, to accurately evaluate the performance of the model, a face recognition task employing five existing deep learning algorithms, —FaceNet, —SphereFace, —CosFace, —ArcFace, and —MagFace, was tested on 102 reconstruction cases in a face pool composed of 1744 CT-scan face images. The experimental results demonstrate that the proposed method can be used as an effective forensic tool.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005768",
    "keywords": [
      "Anatomy",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Computer vision",
      "Craniofacial",
      "Environmental health",
      "Face (sociological concept)",
      "Identification (biology)",
      "Medicine",
      "Pattern recognition (psychology)",
      "Population",
      "Psychiatry",
      "Skull",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yuan"
      },
      {
        "surname": "Wang",
        "given_name": "Jian"
      },
      {
        "surname": "Liang",
        "given_name": "Weibo"
      },
      {
        "surname": "Xue",
        "given_name": "Hui"
      },
      {
        "surname": "He",
        "given_name": "Zhenan"
      },
      {
        "surname": "Lv",
        "given_name": "Jiancheng"
      },
      {
        "surname": "Zhang",
        "given_name": "Lin"
      }
    ]
  },
  {
    "title": "Exploiting appearance transfer and multi-scale context for efficient person image generation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108451",
    "abstract": "Pose guided person image generation means to generate a photo-realistic person image conditioned on an input person image and a desired pose. This task requires spatial manipulation of the source image according to the target pose. However, convolutional neural networks (CNNs) are inherently limited to geometric transformations due to the fixed geometric structures in their building modules, i.e., convolution, pooling and unpooling, which cannot handle large motion and occlusions caused by large pose transform. This paper introduces a novel two-stream context-aware appearance transfer network to address these challenges. It is a three-stage architecture consisting of a source stream and a target stream. Each stage features an appearance transfer module, a multi-scale context module and two-stream feature fusion modules. The appearance transfer module handles large motion by finding the dense correspondence between the two-stream feature maps and then transferring the appearance information from the source stream to the target stream. The multi-scale context module handles occlusion via contextual modeling, which is achieved by atrous convolutions of different sampling rates. Both quantitative and qualitative results indicate the proposed network can effectively handle challenging cases of large pose transform while retaining the appearance details. Compared with state-of-the-art approaches, it achieves comparable or superior performance using much fewer parameters while being significantly faster.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006270",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Linguistics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pooling",
      "Pose"
    ],
    "authors": [
      {
        "surname": "Shen",
        "given_name": "Chengkang"
      },
      {
        "surname": "Wang",
        "given_name": "Peiyan"
      },
      {
        "surname": "Tang",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Introduction to the special section on intelligent systems and pattern recognition (SS:ISPR20)",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.03.013",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000794",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Graph",
      "Machine learning",
      "Naive Bayes classifier",
      "Pattern recognition (psychology)",
      "Probabilistic classification",
      "Probabilistic logic",
      "Support vector machine",
      "Theoretical computer science",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Cheddad",
        "given_name": "Abbas"
      },
      {
        "surname": "Bennour",
        "given_name": "Akram"
      },
      {
        "surname": "Kessentini",
        "given_name": "Yousri"
      }
    ]
  },
  {
    "title": "FW-SMOTE: A feature-weighted oversampling approach for imbalanced classification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108511",
    "abstract": "The Synthetic Minority Over-sampling Technique (SMOTE) is a well-known resampling strategy that has been successfully used for dealing with the class-imbalance problem, one of the most challenging pattern recognition tasks in the last two decades. In this work, we claim that SMOTE has an important issue when defining the neighborhood in order to create new minority samples: the use of the Euclidean distance may not be suitable in high-dimensional settings. Our hypothesis is that the use of a weighted metric that does not assume that all features are equally important could improve performance in the presence of noisy/redundant variables. In this line, we present a novel SMOTE-like method that uses the weighted Minkowski distance for defining the neighborhood for each example of the minority class. This methodology leads to a better definition of the neighborhood since it prioritizes those features that are more relevant for the classification task. A complementary advantage of the proposal is performing feature selection since attributes can be discarded when their corresponding weights are below a given threshold. Our experiments on 42 class-imbalance datasets show the virtues of the proposed SMOTE variant, achieving the best predictive performance when compared with the traditional SMOTE approach and other recent variants on low- and high-dimensional settings, handling issues such as class overlap and hubness adequately without increasing the complexity of the method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006877",
    "keywords": [
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Class (philosophy)",
      "Computer network",
      "Computer science",
      "Data mining",
      "Economics",
      "Euclidean distance",
      "Feature (linguistics)",
      "Feature selection",
      "Linguistics",
      "Machine learning",
      "Metric (unit)",
      "Operations management",
      "Oversampling",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Resampling",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Maldonado",
        "given_name": "Sebastián"
      },
      {
        "surname": "Vairetti",
        "given_name": "Carla"
      },
      {
        "surname": "Fernandez",
        "given_name": "Alberto"
      },
      {
        "surname": "Herrera",
        "given_name": "Francisco"
      }
    ]
  },
  {
    "title": "Segmentation information with attention integration for classification of breast tumor in ultrasound image",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108427",
    "abstract": "Breast cancer is one of the most common forms of cancer among women worldwide. The development of computer-aided diagnosis (CAD) technology based on ultrasound imaging to promote the diagnosis of breast lesions has attracted the attention of researchers and deep learning is a popular and effective method. However, most of the deep learning based CAD methods neglect the relationship between two vision tasks tumor region segmentation and classification. In this paper, taking into account some prior knowledges of medicine, we propose a novel segmentation-to-classification scheme by adding the segmentation-based attention (SBA) information to the deep convolution network (DCNN) for breast tumors classification. A segmentation network is trained to generate tumor segmentation enhancement images. Then two parallel networks extract features for the original images and segmentation enhanced images and one channel attention based feature aggregation network is to automatically integrate the features extracted from two feature networks to improve the performance of recognizing malignant tumors in the breast ultrasound images. To validate our method, experiments have been conducted on breast ultrasound datasets. The classification results of our method have been compared with those obtained by eleven existing approaches. The experimental results show that the proposed method achieves the highest Accuracy (90.78%), Sensitivity (91.18%), Specificity (90.44%), F1-score (91.46%), and AUC (0.9549).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006038",
    "keywords": [
      "Artificial intelligence",
      "Breast cancer",
      "Breast ultrasound",
      "CAD",
      "Cancer",
      "Computer science",
      "Computer-aided diagnosis",
      "Convolutional neural network",
      "Deep learning",
      "Engineering",
      "Engineering drawing",
      "Feature (linguistics)",
      "Image segmentation",
      "Internal medicine",
      "Linguistics",
      "Mammography",
      "Medicine",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Yaozhong"
      },
      {
        "surname": "Huang",
        "given_name": "Qinghua"
      },
      {
        "surname": "Li",
        "given_name": "Xuelong"
      }
    ]
  },
  {
    "title": "Instance-based learning using the half-space proximal graph",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.01.025",
    "abstract": "The primary example of instance-based learning is the k -nearest neighbor rule (kNN), praised for its simplicity and the capacity to adapt to new unseen data and toss away old data. The main disadvantages often mentioned are the classification complexity, which is O ( n ) , and the estimation of the parameter k , the number of nearest neighbors to be used. The use of indexes at classification time lifts the former disadvantage, while there is no conclusive method for the latter. This paper presents a parameter-free instance-based learning algorithm using the Half-Space Proximal (HSP) graph. The HSP neighbors simultaneously possess proximity and variety concerning the center node. To classify a given query, we compute its HSP neighbors and apply a simple majority rule over them. In our experiments, the resulting classifier bettered K N N for any k in a battery of datasets. This improvement sticks even when applying weighted majority rules to both kNN and HSP classifiers. Surprisingly, when using a probabilistic index to approximate the HSP graph and consequently speeding-up the classification task, our method could improve its accuracy in stark contrast with the kNN classifier, which worsens with a probabilistic index.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000320",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Graph",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Talamantes",
        "given_name": "Ariana"
      },
      {
        "surname": "Chavez",
        "given_name": "Edgar"
      }
    ]
  },
  {
    "title": "HashWalk: An efficient node classification method based on clique-compressed graph embedding",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.02.001",
    "abstract": "In recent years, random walk based embedding has become a popular method for node classification. However, current methods still require a huge computational cost to obtain the representation of a large number of nodes. In addition, walking methods cannot adapt well to diverse network structures. Hence, this paper proposes HashWalk to generate a clique-compressed graph that can be used in random walk based embedding for node classification. Specifically, HashWalk compresses cliques into single nodes, and these single nodes are able to inherit neighbors of cliques. As a result, HashWalk can significantly reduce the number of training nodes and computational cost. Besides, HashWalk uses the random walk mapping method to obtain walking sequences of the clique-compressed graph, which makes random walk adapt to network structure. The experimental results prove that HashWalk provides faster time efficiency and lower space complexity while ensuring the accuracy. In summary, this paper provides a fast and effective method using clique-compressed graph embedding for node classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000356",
    "keywords": [
      "Artificial intelligence",
      "Block graph",
      "Clique",
      "Clique graph",
      "Clique problem",
      "Combinatorics",
      "Computer science",
      "Embedding",
      "Engineering",
      "Graph",
      "Line graph",
      "Mathematics",
      "Node (physics)",
      "Pathwidth",
      "Simplex graph",
      "Structural engineering",
      "Voltage graph"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Shuliang"
      },
      {
        "surname": "Qin",
        "given_name": "Xiaorui"
      },
      {
        "surname": "Chi",
        "given_name": "Lianhua"
      }
    ]
  },
  {
    "title": "Brain tumor segmentation based on the dual-path network of multi-modal MRI images",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108434",
    "abstract": "Because of the tumor with infiltrative growth, the glioma boundary is usually fused with the brain tissue, which leads to the failure of accurately segmenting the brain tumor structure through single-modal images. The multi-modal ones are relatively complemented to the inherent heterogeneity and external boundary, which provide complementary features and outlines. Besides, it can retain the structural characteristics of brain diseases from multi angles. However, due to the particularity of multi-modal medical image sampling that increases uneven data density and dense structural vascular tumor mitosis, the glioma may have atypical boundary fuzzy and more noise. To solve this problem, in this paper, the dual-path network based on multi-modal feature fusion (MFF-DNet) is proposed. Firstly, the proposed network uses different kernels multiplexing methods to realize the combination of the large-scale perceptual domain and the non-linear mapping features, which effectively enhances the coherence of information flow. Then, the over-lapping frequency and the vanishing gradient phenomenon are reduced by the residual connection and the dense connection, which alleviate the mutual influence of multi-modal channels. Finally, a dual-path model based on the DenseNet network and the feature pyramid networks (FPN) is established to realize the fusion of low-level, middle-level, and high-level features. Besides, it increases the diversification of glioma non-linear structural features and improves the segmentation precision. A large number of ablation experiments show the effectiveness of the proposed model. The precision of the whole brain tumor and the core tumor can reach 0.92 and 0.90, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006105",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Linguistics",
      "Modal",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Polymer chemistry",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Fang",
        "given_name": "Lingling"
      },
      {
        "surname": "Wang",
        "given_name": "Xin"
      }
    ]
  },
  {
    "title": "Infinite-dimensional feature aggregation via a factorized bilinear model",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108397",
    "abstract": "Aggregating infinite-dimensional features has demonstrated superiority compared with their finite-dimensional counterparts. However, most existing methods approximate infinite-dimensional features with finite-dimensional representations, which inevitably results in approximation error and inferior performance. In this paper, we propose a non-approximate aggregation method that directly aggregates infinite-dimensional features rather than relying on approximation strategies. Specifically, since infinite-dimensional features are infeasible to store, represent and compute explicitly, we introduce a factorized bilinear model to capture pairwise second-order statistics of infinite-dimensional features as a global descriptor. It enables the resulting aggregation formulation to only involve the inner product in an infinite-dimensional space. The factorized bilinear model is calculated by a Sigmoid kernel to generate informative features containing infinite order statistics. Experiments on four visual tasks including the fine-grained, indoor scene, texture, and material classification, demonstrate that our method consistently achieves the state-of-the-art performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005598",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Bilinear interpolation",
      "Computer science",
      "Computer vision",
      "Discrete mathematics",
      "Feature (linguistics)",
      "Geometry",
      "Inner product space",
      "Kernel (algebra)",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Product (mathematics)"
    ],
    "authors": [
      {
        "surname": "Dai",
        "given_name": "Jindou"
      },
      {
        "surname": "Wu",
        "given_name": "Yuwei"
      },
      {
        "surname": "Gao",
        "given_name": "Zhi"
      },
      {
        "surname": "Jia",
        "given_name": "Yunde"
      }
    ]
  },
  {
    "title": "Super-resolution semantic segmentation with relation calibrating network",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108501",
    "abstract": "To achieve high-resolution segmentation results, typical semantic segmentation models often require high-resolution inputs. However, high-resolution inputs inevitably bring high cost on computation, which limits its application seriously in realistic scenarios. To address the problem, we propose to predict a high-resolution semantic segmentation result with a degraded low-resolution image as input, which is called super-resolution semantic segmentation in this paper. We further propose a Relation Calibrating Network (RCNet) for this task. Specifically, we propose two modules, namely Relation Upsampling Module (RUM) and Feature Calibrating Module (FCM). In RUM, the input feature map generates the relation map of pixels in low-resolution, which is then gradually upsampled to high-resolution. Meanwhile, FCM takes the input feature map and the relation map from RUM as inputs, gradually calibrating the feature. Finally, the last FCM outputs the high-resolution segmentation results. We conduct extensive experiments to verify the effectiveness of our method. Specially, we achieve a comparable segmentation result (from 70.01% to 70.90%) with only 1/4 of the computational cost (from 1107.57 to 255.72 GFLOPs) based on FCN on Cityscapes dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006774",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computation",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Image resolution",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Relation (database)",
      "Resolution (logic)",
      "Segmentation",
      "Upsampling"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Jie"
      },
      {
        "surname": "Liu",
        "given_name": "Jing"
      },
      {
        "surname": "Fu",
        "given_name": "Jun"
      },
      {
        "surname": "Wang",
        "given_name": "Weining"
      },
      {
        "surname": "Lu",
        "given_name": "Hanqing"
      }
    ]
  },
  {
    "title": "Signature barcodes for online verification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108426",
    "abstract": "As a sub-branch of behavioral biometrics, online signature verification systems deal with unique signing characteristics, which could be better differentiated by extraction of habitual singing styles instead of geometric features in case of perfect forgery. Even if the signatures are geometrically identical, speed and frequency components of the signing process might significantly vary. Therefore, a novel framework is introduced as a new signature verification protocol for touchscreen devices using barcodes containing the dominant frequency component of the speed signals. A special interface is designed as signature tracker to extract the displacement data sampled from the signing process. The speed signals are interpolated from the displacement data and the frequency components of the signals are computed by scalograms analysis governed by continuous wavelet transformations (CWT). The signature barcodes are generated as 4-scale scalograms and classified by support vector machines (SVM). Among several compatible wavelets, Gaussian derivative wavelet is selected for generating scalograms and the results of the process are calculated as 2.25% FAR, 2.75% FRR and 2.81%EER for our dataset. The framework is also tested with SVC2004 data that we achieved 0% FAR, 9.33% FRR and 8%EER, also with SUSIG-Visual, SUSIG-Blind, MOBISIG databases and we reached between 1.22%-3.62% average EERs, which are competitive among the relevant results. Given the promising outcomes, the signature barcoding is very reliable method which could be executed by a simple touchscreen interface collecting the barcodes for storing and benchmarking when needed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006026",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Bubble",
      "Computer hardware",
      "Computer science",
      "Computer vision",
      "Geometry",
      "Interface (matter)",
      "Mathematics",
      "Maximum bubble pressure method",
      "Operating system",
      "Parallel computing",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Signature (topology)",
      "Speech recognition",
      "Support vector machine",
      "Touchscreen",
      "Wavelet"
    ],
    "authors": [
      {
        "surname": "Alpar",
        "given_name": "Orcan"
      }
    ]
  },
  {
    "title": "An analysis of heuristic metrics for classifier ensemble pruning based on ordered aggregation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108493",
    "abstract": "Classifier ensemble pruning is a strategy through which a subensemble can be identified via optimizing a predefined performance criterion. Choosing the optimum or suboptimum subensemble decreases the initial ensemble size and increases its predictive performance. In this article, a set of heuristic metrics will be analyzed to guide the pruning process. The analyzed metrics are based on modifying the order of the classifiers in the bagging algorithm, with selecting the first set in the queue. Some of these criteria include general accuracy, the complementarity of decisions, ensemble diversity, the margin of samples, minimum redundancy, discriminant classifiers, and margin hybrid diversity. The efficacy of those metrics is affected by the original ensemble size, the required subensemble size, the kind of individual classifiers, and the number of classes. While the efficiency is measured in terms of the computational cost and the memory space requirements. The performance of those metrics is assessed over fifteen binary and fifteen multiclass benchmark classification tasks, respectively. In addition, the behavior of those metrics against randomness is measured in terms of the distribution of their accuracy around the median. Results show that ordered aggregation is an efficient strategy to generate subensembles that improve both predictive performance as well as computational and memory complexities of the whole bagging ensemble.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006695",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Ensemble learning",
      "Heuristic",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Random subspace method",
      "Randomness",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Mohammed",
        "given_name": "Amgad M."
      },
      {
        "surname": "Onieva",
        "given_name": "Enrique"
      },
      {
        "surname": "Woźniak",
        "given_name": "Michał"
      },
      {
        "surname": "Martínez-Muñoz",
        "given_name": "Gonzalo"
      }
    ]
  },
  {
    "title": "3D pose estimation and future motion prediction from 2D images",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108439",
    "abstract": "This paper considers to jointly tackle the highly correlated tasks of estimating 3D human body poses and predicting future 3D motions from RGB image sequences. Based on Lie algebra pose representation, a novel self-projection mechanism is proposed that naturally preserves human motion kinematics. This is further facilitated by a sequence-to-sequence multi-task architecture based on an encoder-decoder topology, which enables us to tap into the common ground shared by both tasks. Finally, a global refinement module is proposed to boost the performance of our framework. The effectiveness of our approach, called PoseMoNet, is demonstrated by ablation tests and empirical evaluations on Human3.6M and HumanEva-I benchmark, where competitive performance is obtained comparing to the state-of-the-arts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006154",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Classical mechanics",
      "Computer science",
      "Computer vision",
      "Economics",
      "Encoder",
      "Encoding (memory)",
      "Genetics",
      "Geodesy",
      "Geography",
      "Kinematics",
      "Law",
      "Management",
      "Motion (physics)",
      "Motion capture",
      "Motion estimation",
      "Operating system",
      "Physics",
      "Political science",
      "Politics",
      "Pose",
      "Projection (relational algebra)",
      "RGB color model",
      "Representation (politics)",
      "Sequence (biology)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Ji"
      },
      {
        "surname": "Ma",
        "given_name": "Youdong"
      },
      {
        "surname": "Zuo",
        "given_name": "Xinxin"
      },
      {
        "surname": "Wang",
        "given_name": "Sen"
      },
      {
        "surname": "Gong",
        "given_name": "Minglun"
      },
      {
        "surname": "Cheng",
        "given_name": "Li"
      }
    ]
  },
  {
    "title": "Incremental few-shot object detection via knowledge transfer",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.01.024",
    "abstract": "As a challenging problem in machine learning, incremental few-shot object detection (iFSD) [1] aims to incrementally detect novel classes with few examples, while keeping the previous knowledge without revisiting base classes. Here, we propose two models based on the observation that when new memories come, new connections will be created between memory cells in the brain [2]. The first one, which is called the multi-class head (MCH) model, simulates how humans add new memory-connections that every time novel classes come, a classification branch is added to predict novel classification. And the second model, called the bi-path multi-class head (BPMCH) model, adds a new backbone, which is initialized with the weight of the base class backbone, to transfer more knowledge of the base class to the novel class. Considering accuracy and speed, we choose the Fully Convolutional One-Stage Object Detection (FCOS) [3] + Adaptive Training Sample Selection (ATSS) [4] detector as our baseline. Our models are first trained on the base classes with abundant examples and then finetuned on novel classes with few examples, which not only maintain the knowledge learned from the base class but also transfer the knowledge to the novel class. Extensive experiments show that our models outperform the state-of-the-art model ONCE [1] on the COCO [5] and PASCAL VOC [6] by a large margin.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000319",
    "keywords": [
      "Artificial intelligence",
      "Base (topology)",
      "Class (philosophy)",
      "Computer science",
      "Knowledge base",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematical analysis",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Feng",
        "given_name": "Hangtao"
      },
      {
        "surname": "Zhang",
        "given_name": "Lu"
      },
      {
        "surname": "Yang",
        "given_name": "Xu"
      },
      {
        "surname": "Liu",
        "given_name": "Zhiyong"
      }
    ]
  },
  {
    "title": "GDFormer: A Graph Diffusing Attention based approach for Traffic Flow Prediction",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.03.005",
    "abstract": "In this paper, we propose a novel traffic flow prediction approach, called as Graph Diffusing trans-Former (GDFormer). GDFormer is in architecture of transformer, which is composed by the encoder sequence and decoder sequence. both of the encoder sequence and decoder sequence in GDFormer are constituted by the novel designed Graph Diffusing Attention (GDA) module and the auxiliaries. The GDA module utilizes the query-key-value attention to learn the diffusion parameters for each diffusion step, and dynamically updates the adjacency transition, which reflects the dynamically changing traffic flow between the traffic monitors. To verify the efficiency of our approach, we conduct a lot of experiments on two real-world data sets. With a comparison between our approach and the benchmarks, we find that our approach has achieved state of the art performance. Ablation experiments are conducted to illustrate the effectiveness of the key components in the model. For ease of reproducibility, the code, the processed real-world data sets and the evaluation results are available at https://github.com/dublinsky/GDFormer.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000708",
    "keywords": [
      "Adjacency list",
      "Algorithm",
      "Biology",
      "Computer science",
      "Data mining",
      "Encoder",
      "Genetics",
      "Graph",
      "Operating system",
      "Sequence (biology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Su",
        "given_name": "Jie"
      },
      {
        "surname": "Jin",
        "given_name": "Zhongfu"
      },
      {
        "surname": "Ren",
        "given_name": "Jie"
      },
      {
        "surname": "Yang",
        "given_name": "Jiandang"
      },
      {
        "surname": "Liu",
        "given_name": "Yong"
      }
    ]
  },
  {
    "title": "Physiology-based augmented deep neural network frameworks for ECG biometrics with short ECG pulses considering varying heart rates",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.02.014",
    "abstract": "Electrocardiogram (ECG) has been investigated as promising biometrics with high authentication accuracy, natural liveness test ability, and wearable sensor availability. There have been many algorithms developed for ECG biometric authentication or identification including recent state-of-the-art deep learning (DL) methods that usually yielded excellent performance with real ECG data in ideal conditions. However, one of the challenges against ideal conditions is the intra-personal variability of ECG pulses due to heart beat rate changes. Due to this variability, ECG based biometric methods have experienced significant performance degradation. It is especially challenging when a small number of ECG pulses must be used for biometrics with fast response authentication since there is not enough information available to correct for different heart rates. In this letter, we investigated DL based ECG biometrics with the input of a small number of ECG pulses considering varying heart rates. We propose physiology-based augmented deep neural network (DNN) frameworks for ECG biometric methods that are based on the Hodges’ QT interval correction. Unlike QT interval correction methods, our proposed framework does not require the estimated heart rate. Our proposed training and testing schemes were evaluated with representative DL based biometric methods using CNN and RNN with very short ECG pulses (1 or 3 pulses per authentication) from the public multi-session ECG-ID dataset (83 subjects). We exploited the ECG-ID dataset to simulate the challenging scenario including the enrollment and authentication happening over relatively long time duration so that heart rate variation is likely occurring. Our augmented DNN frameworks yielded significantly better performance than the original DL based biometrics; up to 11.7% improvement in accuracy and 8.6% improvement in sensitivity simultaneously with 99.9% specificity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000575",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Authentication (law)",
      "Biology",
      "Biometrics",
      "Blood pressure",
      "Botany",
      "Computer science",
      "Computer security",
      "Deep learning",
      "Heart rate",
      "Heart rate variability",
      "Identification (biology)",
      "Internal medicine",
      "Liveness",
      "Machine learning",
      "Medicine",
      "Pattern recognition (psychology)",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Hanvit"
      },
      {
        "surname": "Phan",
        "given_name": "Thanh Quoc"
      },
      {
        "surname": "Hong",
        "given_name": "Wonjae"
      },
      {
        "surname": "Chun",
        "given_name": "Se Young"
      }
    ]
  },
  {
    "title": "Exploring semantic segmentation of related subclasses from a superset of classes",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108509",
    "abstract": "Image segmentation is a very important topic in the field of computer vision. We present a method for semantic segmentation of selected stuff classes from a superset of classes. We show that in situations where only select stuff classes are required if we group them as per a strategy then it can attain much higher accuracy than the models trained on the original dataset with all classes intact. The COCO-Stuff Dataset is used for demonstrating the aforesaid strategy. For training purposes, the DeepLabv3+ with Mobilenet-v2 architecture is used. We have achieved an 80.2 percent mean Intersection over Union (mIoU) on these selected classes. We also refine the masks using Learning/Computer Vision (CV) methods and hence obtain better visualization results as compared to the existing DeepLabv3+ results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006853",
    "keywords": [
      "Artificial intelligence",
      "Cartography",
      "Computer science",
      "Field (mathematics)",
      "Geography",
      "Image (mathematics)",
      "Image segmentation",
      "Intersection (aeronautics)",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Segmentation",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Shah",
        "given_name": "Kunjal"
      },
      {
        "surname": "Bhat",
        "given_name": "Gururaj"
      }
    ]
  },
  {
    "title": "An approach to boundary detection for 3D point clouds based on DBSCAN clustering",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108431",
    "abstract": "This paper introduces a new DBSCAN-based method for boundary detection and plane segmentation for 3D point clouds. The proposed method is based on candidate samples selection in 3D space and plane validity detection via revising the classical DBSCAN clustering algorithm to obtain a valid fitting plane. Technically, a coplanar threshold is designed as an additional clustering condition to group 3D points whose distances to the fitting plane satisfy the constraint of the threshold as one cluster. The threshold value is automatically adjusted to fit the local distribution of samples in the input dataset, which is free of parameter tuning. Planar objects can be detected by the proposed method since a cluster contains only data points belonging to one plane, and the boundaries among different planes can be correctly detected. Experimental evaluations are performed on both synthetic and real point cloud datasets. Results show that the proposed approach is effective for planar segmentation and high-quality segmentation of intersection boundaries.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006075",
    "keywords": [
      "Artificial intelligence",
      "Boundary (topology)",
      "CURE data clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "DBSCAN",
      "Fuzzy clustering",
      "Geometry",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Point (geometry)",
      "Point cloud"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Hui"
      },
      {
        "surname": "Liang",
        "given_name": "Man"
      },
      {
        "surname": "Liu",
        "given_name": "Wanquan"
      },
      {
        "surname": "Wang",
        "given_name": "Weina"
      },
      {
        "surname": "Liu",
        "given_name": "Peter Xiaoping"
      }
    ]
  },
  {
    "title": "DE-GAN: Domain Embedded GAN for High Quality Face Image Inpainting",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108415",
    "abstract": "Domain knowledge of face shapes and structures plays an important role in face inpainting. However, general inpainting methods focus mainly on the resolution of generated images without considering the particular structure of human faces and generally produce inharmonious facial parts. Existing face-inpainting methods incorporate only one type of facial feature for face completion, and their results are still undesirable. To improve face inpainting quality, we propose a Domain Embedded Generative Adversarial Network (DE-GAN) for face inpainting. DE-GAN embeds three types of face domain knowledge (i.e., face mask, face part, and landmark image) via a hierarchical variational auto-encoder (HVAE) into a latent variable space to guide face completion. Two adversarial discriminators, a global discriminator and a patch discriminator, are used to judge whether the generated distribution is close to the real distribution or not. Experiments on two public face datasets demonstrate that our proposed method generates higher quality inpainting results with consistent and harmonious facial structures and appearance than existing methods and achieves the state-of-the-art performance, esp. for inpainting under-pose variations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005914",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Detector",
      "Discriminator",
      "Domain (mathematical analysis)",
      "Encoder",
      "Face (sociological concept)",
      "Face detection",
      "Face hallucination",
      "Facial recognition system",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Inpainting",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Social science",
      "Sociology",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Xian"
      },
      {
        "surname": "Wang",
        "given_name": "Xin"
      },
      {
        "surname": "Shi",
        "given_name": "Canghong"
      },
      {
        "surname": "Yan",
        "given_name": "Zhe"
      },
      {
        "surname": "Li",
        "given_name": "Xiaojie"
      },
      {
        "surname": "Kong",
        "given_name": "Bin"
      },
      {
        "surname": "Lyu",
        "given_name": "Siwei"
      },
      {
        "surname": "Zhu",
        "given_name": "Bin"
      },
      {
        "surname": "Lv",
        "given_name": "Jiancheng"
      },
      {
        "surname": "Yin",
        "given_name": "Youbing"
      },
      {
        "surname": "Song",
        "given_name": "Qi"
      },
      {
        "surname": "Wu",
        "given_name": "Xi"
      },
      {
        "surname": "Mumtaz",
        "given_name": "Imran"
      }
    ]
  },
  {
    "title": "Poisson kernel: Avoiding self-smoothing in graph convolutional networks",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108443",
    "abstract": "Graph convolutional network is now an effective tool to deal with non-Euclidean data, such as social behavior analysis, molecular structure analysis, and skeleton-based action recognition. Graph convolutional kernel is one of the most significant factors in graph convolutional networks to extract nodes’ feature, and some variants of it have achieved highly satisfactory performance theoretically and experimentally. However, there was limited research about how exactly different graph structures influence the performance of these kernels. Some existing methods used an adaptive convolutional kernel to deal with a given graph structure, which still not explore the internal reasons. In this paper, we start from theoretical analysis of the spectral graph and study the properties of existing graph convolutional kernels, revealing the self-smoothing phenomenon and its effect in specific structured graphs. After that, we propose the Poisson kernel that can avoid self-smoothing without training any adaptive kernel. Experimental results demonstrate that our Poisson kernel not only works well on the benchmark datasets where state-of-the-art methods work fine, but also is evidently superior to them in synthetic datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006191",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Discrete mathematics",
      "Graph",
      "Graph kernel",
      "Kernel (algebra)",
      "Kernel embedding of distributions",
      "Kernel method",
      "Kernel smoother",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Radial basis function kernel",
      "Smoothing",
      "Support vector machine",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Ziqing"
      },
      {
        "surname": "Han",
        "given_name": "Shoudong"
      },
      {
        "surname": "Zhao",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Knowledge guided learning: Open world egocentric action recognition with zero supervision",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.03.007",
    "abstract": "Advances in deep learning have enabled the development of models that have exhibited a remarkable tendency to recognize and even localize actions in videos. However, they tend to experience errors when faced with scenes or examples beyond their initial training environment. Hence, they fail to adapt to new domains without significant retraining with large amounts of annotated data. In this paper, we propose to overcome these limitations by moving to an open-world setting by decoupling the ideas of recognition and reasoning. Building upon the compositional representation offered by Grenander’s Pattern Theory formalism, we show that attention and commonsense knowledge can be used to enable the self-supervised discovery of novel actions in egocentric videos in an open-world setting, where data from the observed environment (the target domain) is open i.e., the vocabulary is partially known and training examples (both labeled and unlabeled) are not available. We show that our approach can infer and learn novel classes for open vocabulary classification in egocentric videos and novel object detection with zero supervision. Extensive experiments show its competitive performance on two publicly available egocentric action recognition datasets (GTEA Gaze and GTEA Gaze+) under open-world conditions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000733",
    "keywords": [
      "Action (physics)",
      "Action recognition",
      "Artificial intelligence",
      "Business",
      "Class (philosophy)",
      "Computer science",
      "Control engineering",
      "Decoupling (probability)",
      "Engineering",
      "Gaze",
      "Human–computer interaction",
      "International trade",
      "Linguistics",
      "Natural language processing",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Retraining",
      "Vocabulary"
    ],
    "authors": [
      {
        "surname": "Aakur",
        "given_name": "Sathyanarayanan N."
      },
      {
        "surname": "Kundu",
        "given_name": "Sanjoy"
      },
      {
        "surname": "Gunti",
        "given_name": "Nikhil"
      }
    ]
  },
  {
    "title": "Contrastive attention network with dense field estimation for face completion",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108465",
    "abstract": "Most modern face completion approaches adopt an autoencoder or its variants to restore missing regions in face images. Encoders are often utilized to learn powerful representations that play an important role in meeting the challenges of sophisticated learning tasks. Specifically, various kinds of masks are often presented in face images in the wild, forming complex patterns, especially in this hard period of COVID-19. It’s difficult for encoders to capture such powerful representations under this complex situation. To address this challenge, we propose a self-supervised Siamese inference network to improve the generalization and robustness of encoders. It can encode contextual semantics from full-resolution images and obtain more discriminative representations. To deal with geometric variations of face images, a dense correspondence field is integrated into the network. We further propose a multi-scale decoder with a novel dual attention fusion module (DAF), which can combine the restored and known regions in an adaptive manner. This multi-scale architecture is beneficial for the decoder to utilize discriminative representations learned from encoders into images. Extensive experiments clearly demonstrate that the proposed approach not only achieves more appealing results compared with state-of-the-art methods but also improves the performance of masked face recognition dramatically.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006415",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Discriminative model",
      "ENCODE",
      "Encoder",
      "Face (sociological concept)",
      "Field (mathematics)",
      "Gene",
      "Inference",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Robustness (evolution)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Xin"
      },
      {
        "surname": "Zhou",
        "given_name": "Xiaoqiang"
      },
      {
        "surname": "Huang",
        "given_name": "Huaibo"
      },
      {
        "surname": "Jia",
        "given_name": "Gengyun"
      },
      {
        "surname": "Chai",
        "given_name": "Zhenhua"
      },
      {
        "surname": "Wei",
        "given_name": "Xiaolin"
      }
    ]
  },
  {
    "title": "Sparse CapsNet with explicit regularizer",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108486",
    "abstract": "Capsule Network (CapsNet) achieves great improvements in recognizing pose and deformation through a novel encoding mode. However, it carries a large number of parameters, leading to the challenge of heavy memory and computational cost. To solve this problem, we propose sparse CapsNet with an explicit regularizer in this paper. To our knowledge, it’s the first work that utilizes sparse optimization to compress CapsNet. Specifically, to reduce unnecessary weight parameters, we first introduce the component-wise absolute value regularizer into the objective function of CapsNet based on zero-means Laplacian prior. Then, to reduce the computational cost and speed up CapsNet, the weight parameters are further grouped by 2D filters and sparsified by 1-norm regularization. To train our model efficiently, a new stochastic proximal gradient algorithm, which has analytical solutions at each iteration, is presented. Extensive numerical experiments on four commonly used datasets validate the effectiveness and efficiency of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006622",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Mathematical optimization",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Ruiyang"
      },
      {
        "surname": "Niu",
        "given_name": "Lingfeng"
      },
      {
        "surname": "Zhou",
        "given_name": "Ruizhi"
      }
    ]
  },
  {
    "title": "PRNU registration under scale and rotation transform based on convolutional neural networks",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108413",
    "abstract": "Assessing if an image comes from a specific device is fundamental in many application scenarios. The most promising techniques to solve this problem rely on the Photo Response Non Uniformity (PRNU), a unique trace left during image acquisition. A PRNU fingerprint is computed from several images of a given device, then it is compared with the probe residual noise by means of correlation. However, such a comparison requires that PRNUs are synchronized: even small image transformations can spoil this task. Most of the attempts to solve the registration problem rely on time consuming brute-force search, which is prone to missing detections and false positives. In this paper, the problem is addressed from a computer vision perspective, exploiting recent image registration techniques based on deep learning, and focusing on scaling and rotation transformations. Experiments show that the proposed method is both more accurate and faster than state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005896",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "False positive paradox",
      "Fingerprint (computing)",
      "Image (mathematics)",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Physics",
      "Quantum mechanics",
      "Residual",
      "Rotation (mathematics)",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Fanfani",
        "given_name": "Marco"
      },
      {
        "surname": "Piva",
        "given_name": "Alessandro"
      },
      {
        "surname": "Colombo",
        "given_name": "Carlo"
      }
    ]
  },
  {
    "title": "Human object interaction detection using two-direction spatial enhancement and exclusive object prior",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108438",
    "abstract": "Human-Object Interaction (HOI) detection aims to detect visual relations between humans and objects in images. One significant problem of HOI detection is that non-interactive human-object pair can be easily mis-grouped and misclassified as an action, especially when the humans are close and performing similar actions in the scene. To address the mis-grouping problem, we propose a spatial enhancement approach to enforce fine-level spatial constraints in two directions between human body parts and object parts. At inference, we propose a human-object regrouping approach for object-exclusive actions by considering the object-exclusive property of the interactive object, where the target object should not be shared by more than one human. By suppressing non-interactive pairs, our approach can decrease the false positives. Experiments on V-COCO and HICO-DET datasets demonstrate our approach is more robust compared to the existing methods under the presence of multiple humans and objects in the scene.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006142",
    "keywords": [
      "Action (physics)",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Epistemology",
      "Face detection",
      "Facial recognition system",
      "False positive paradox",
      "Inference",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Object-based spatial database",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Property (philosophy)",
      "Quantum mechanics",
      "Spatial analysis",
      "Spatial database",
      "Statistics",
      "Viola–Jones object detection framework"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Lu"
      },
      {
        "surname": "Tan",
        "given_name": "Robby T."
      }
    ]
  },
  {
    "title": "Uncertainty estimation for stereo matching based on evidential deep learning",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108498",
    "abstract": "Although deep learning-based stereo matching approaches have achieved excellent performance in recent years, it is still a non-trivial task to estimate the uncertainty of the produced disparity map. In this paper, we propose a novel approach to estimate both aleatoric and epistemic uncertainties for stereo matching in an end-to-end way. We introduce an evidential distribution, named Normal Inverse-Gamma (NIG) distribution, whose parameters can be used to calculate the uncertainty. Instead of directly regressed from aggregated features, the uncertainty parameters are predicted for each potential disparity and then averaged via the guidance of matching probability distribution. Furthermore, considering the sparsity of ground truth in real scene datasets, we design two additional losses. The first one tries to enlarge uncertainty on incorrect predictions, so uncertainty becomes more sensitive to erroneous regions. The second one enforces the smoothness of the uncertainty in the regions with smooth disparity. Most stereo matching models, such as PSM-Net, GA-Net, and AA-Net, can be easily integrated with our approach. Experiments on multiple benchmark datasets show that our method improves stereo matching results. We prove that both aleatoric and epistemic uncertainties are well-calibrated with incorrect predictions. Particularly, our method can capture increased epistemic uncertainty on out-of-distribution data, making it effective to prevent a system from potential fatal consequences. Code is available at https://github.com/Dawnstar8411/StereoMatching-Uncertainty.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006749",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Code (set theory)",
      "Computer science",
      "Economics",
      "Geodesy",
      "Geography",
      "Ground truth",
      "Machine learning",
      "Management",
      "Matching (statistics)",
      "Mathematical analysis",
      "Mathematics",
      "Measurement uncertainty",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Smoothness",
      "Statistics",
      "Task (project management)",
      "Uncertainty quantification"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Chen"
      },
      {
        "surname": "Wang",
        "given_name": "Xiang"
      },
      {
        "surname": "Zhang",
        "given_name": "Jiawei"
      },
      {
        "surname": "Zhang",
        "given_name": "Liang"
      },
      {
        "surname": "Bai",
        "given_name": "Xiao"
      },
      {
        "surname": "Ning",
        "given_name": "Xin"
      },
      {
        "surname": "Zhou",
        "given_name": "Jun"
      },
      {
        "surname": "Hancock",
        "given_name": "Edwin"
      }
    ]
  },
  {
    "title": "GaitSlice: A gait recognition model based on spatio-temporal slice features",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108453",
    "abstract": "Improving the performance of gait recognition under multiple camera views (i.e., cross-view gait recognition) and various conditions is urgent. From observation, we find that adjacent body parts are inter-related while walking, and each frame in a gait sequence possesses different degrees of semantic information. In this paper, we propose a novel model, GaitSlice, to analyze the human gait based on spatio-temporal slice features. Spatially, we design Slice Extraction Device (SED) to form top-down inter-related slice features. Temporally, we introduce Residual Frame Attention Mechanism (RFAM) to acquire and highlight the key frames. To better simulate reality, GaitSlice combines parallel RFAMs with inter-related slice features to focus on the features’ spatio-temporal information. We evaluate our model on CASIA-B and OU-MVLP gait datasets and compare it with six typical gait recognition models by using rank-1 accuracy. The results show that GaitSlice achieves high accuracy in gait recognition under cross-view and various walking conditions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006294",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Gait",
      "Medicine",
      "Pattern recognition (psychology)",
      "Physical medicine and rehabilitation"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Huakang"
      },
      {
        "surname": "Qiu",
        "given_name": "Yidan"
      },
      {
        "surname": "Zhao",
        "given_name": "Huimin"
      },
      {
        "surname": "Zhan",
        "given_name": "Jin"
      },
      {
        "surname": "Chen",
        "given_name": "Rongjun"
      },
      {
        "surname": "Wei",
        "given_name": "Tuanjie"
      },
      {
        "surname": "Huang",
        "given_name": "Zhihui"
      }
    ]
  },
  {
    "title": "Deep open-set recognition for silicon wafer production monitoring",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108488",
    "abstract": "The chips contained in any electronic device are manufactured over circular silicon wafers, which are monitored by inspection machines at different production stages. Inspection machines detect and locate any defect within the wafer and return a Wafer Defect Map (WDM), i.e., a list of the coordinates where defects lie, which can be considered a huge, sparse, and binary image. In normal conditions, wafers exhibit a small number of randomly distributed defects, while defects grouped in specific patterns might indicate known or novel categories of failures in the production line. Needless to say, a primary concern of semiconductor industries is to identify these patterns and intervene as soon as possible to restore normal production conditions. Here we address WDM monitoring as an open-set recognition problem, where the aim is to classify WDM in known categories and promptly detect novel patterns. In particular, we propose a comprehensive pipeline for wafer monitoring based on a Submanifold Sparse Convolutional Network, a deep architecture designed to process sparse data at an arbitrary resolution, which is trained on the known classes. To detect novelties, we define an outlier detector based on a Gaussian Mixture Model fitted on the latent representation of the classifier. Our experiments on a real dataset of WDMs show that directly processing full-resolution WDMs by Submanifold Sparse Convolutions yields superior classification performance on known classes than traditional Convolutional Neural Networks, which require a preliminary binning to reduce the size of the binary images representing WDMs. Moreover, our solution outperforms state-of-the-art open-set recognition solutions in novelty detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006646",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Discrete mathematics",
      "Economics",
      "Macroeconomics",
      "Materials science",
      "Mathematics",
      "Open set",
      "Optoelectronics",
      "Pattern recognition (psychology)",
      "Production (economics)",
      "Programming language",
      "Set (abstract data type)",
      "Silicon",
      "Wafer"
    ],
    "authors": [
      {
        "surname": "Frittoli",
        "given_name": "Luca"
      },
      {
        "surname": "Carrera",
        "given_name": "Diego"
      },
      {
        "surname": "Rossi",
        "given_name": "Beatrice"
      },
      {
        "surname": "Fragneto",
        "given_name": "Pasqualina"
      },
      {
        "surname": "Boracchi",
        "given_name": "Giacomo"
      }
    ]
  },
  {
    "title": "Multi-label video classification via coupling attentional multiple instance learning with label relation graph",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.01.003",
    "abstract": "Multi-label video classification is a challenging problem in pattern recognition field, as it is difficult to grasp the occurring localizations of a huge number of labels in videos. To solve this problem, we propose a general framework named MALL-CNN, i.e., Multi-Attention Label Relation Learning Convolutional Neural Network. MALL-CNN not only builds the correspondences between labels and videos by an attention mechanism, but also captures label co-occurrence by a graph learning approach. Specifically, we introduce multiple instance learning to composite a set of frame-level features into a video-level feature. Then, video-level feature is mapped into the content-aware category representations in an improved attentional manner. Further, these representations are enhanced by a series of label relation graphs, which transform global label relationships to the label relationships of current video. With the three processes, frame feature aggregation, video feature mapping, and label relationship construction can be achieved in MALL-CNN for multi-label video classification. Extensive experiments on real-world scene benchmark Youtube-8M verify that MALL-CNN with only frame feature surpasses the state of the arts with multi-modal features as well as ensemble models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000034",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Feature (linguistics)",
      "Feature learning",
      "Frame (networking)",
      "Geodesy",
      "Geography",
      "Graph",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Relation (database)",
      "Telecommunications",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xuewei"
      },
      {
        "surname": "Wu",
        "given_name": "Hongjun"
      },
      {
        "surname": "Li",
        "given_name": "Mengzhu"
      },
      {
        "surname": "Liu",
        "given_name": "Hongzhe"
      }
    ]
  },
  {
    "title": "VANT-GAN: Adversarial Learning for Discrepancy-Based Visual Attribution in Medical Imaging",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.02.005",
    "abstract": "Visual attribution (VA) in relation to medical images is an essential aspect of modern automation-assisted diagnosis. Since it is generally not straightforward to obtain pixel-level ground-truth labelling of medical images, classification-based interpretation approaches have become the de facto standard for automated diagnosis, in which the ability of classifiers to make categorical predictions based on class-salient regions is harnessed within the learning algorithm. Such regions, however, typically constitute only a small subset of the full range of features of potential medical interest. They may hence not be useful for VA of medical images where capturing all of the disease evidence is a critical requirement. This hence motivates the proposal of a novel strategy for visual attribution that is not reliant on image classification. We instead obtain normal counterparts of abnormal images and find discrepancy maps between the two. To perform the abnormal-to-normal mapping in unsupervised way, we employ a Cycle-Consistency Generative Adversarial Network, thereby formulating visual attribution in terms of a discrepancy map that, when subtracted from the abnormal image, makes it indistinguishable from the counterpart normal image. Experiments are performed on three datasets including a synthetic, Alzheimer’s disease Neuro imaging Initiative and, BraTS dataset. We outperform baseline and related methods in both experiments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000393",
    "keywords": [
      "Artificial intelligence",
      "Attribution",
      "Categorical variable",
      "Computer science",
      "Consistency (knowledge bases)",
      "Data mining",
      "Ground truth",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Psychology",
      "Relation (database)",
      "Social psychology"
    ],
    "authors": [
      {
        "surname": "Zia",
        "given_name": "Tehseen"
      },
      {
        "surname": "Murtaza",
        "given_name": "Shakeeb"
      },
      {
        "surname": "Bashir",
        "given_name": "Nauman"
      },
      {
        "surname": "Windridge",
        "given_name": "David"
      },
      {
        "surname": "Nisar",
        "given_name": "Zeeshan"
      }
    ]
  },
  {
    "title": "Investigating strategies towards adversarially robust time series classification",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.01.023",
    "abstract": "Deep neural networks have been shown to be vulnerable against specifically-crafted perturbations designed to affect their predictive performance. Such perturbations, formally termed ‘adversarial attacks’ have been designed for various domains in the literature, most prominently in computer vision and more recently, in time series classification. Therefore there is a need to derive robust strategies to defend deep networks from such attacks. In this work we propose to establish axioms of robustness against adversarial attacks in time series classification. We subsequently design a suitable experimental methodology and empirically validate the hypotheses put forth. Results obtained from our investigations confirm the proposed hypotheses, and provide a strong empirical baseline with a view to mitigating the effects of adversarial attacks in deep time series classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000307",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Artificial neural network",
      "Axiom",
      "Baseline (sea)",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Deep neural networks",
      "Gene",
      "Geology",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Oceanography",
      "Paleontology",
      "Robustness (evolution)",
      "Series (stratigraphy)",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Abdu-Aguye",
        "given_name": "Mubarak G."
      },
      {
        "surname": "Gomaa",
        "given_name": "Walid"
      },
      {
        "surname": "Makihara",
        "given_name": "Yasushi"
      },
      {
        "surname": "Yagi",
        "given_name": "Yasushi"
      }
    ]
  },
  {
    "title": "A mixture modeling approach for clustering log files with coreset and user feedback",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.01.027",
    "abstract": "Machine-generated log data can provide valuable insights into many critical areas such as system failures, network security, and performance optimization. The increasing prominence of this data in both volume and complexity requires data mining approaches that are both scalable and flexible. In this paper, we propose a new approach for clustering machine-generated logs which contains a novel combination of the use of the coreset with user feedback. The coreset allows us to efficiently summarize the data in a principled manner such that performance after fitting model parameters on the coreset is similar to the performance that would have been achieved with the full dataset. Furthermore, the formal approach we propose allows users to incorporate two different types of feedback, in the forms of labels and pairwise constraints, to further improve results and better deal with the increasing complexity and variety of log datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000344",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Database",
      "Machine learning",
      "Pairwise comparison",
      "Scalability",
      "Variety (cybernetics)"
    ],
    "authors": [
      {
        "surname": "Bunker",
        "given_name": "Justin"
      },
      {
        "surname": "Curtis",
        "given_name": "Kristal"
      },
      {
        "surname": "Girolami",
        "given_name": "Mark"
      },
      {
        "surname": "Sriharsha",
        "given_name": "Ram"
      }
    ]
  },
  {
    "title": "Improved time series clustering based on new geometric frameworks",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108423",
    "abstract": "Most existing methods for time series clustering rely on distances calculated from the entire raw data using the Euclidean distance or Dynamic Time Warping distance. In this work, we propose to embed the time series onto higher-dimensional spaces to obtain geometric representations of the time series themselves. Particularly, the embedding on R n × p , on the Stiefel manifold and on the unit Sphere are analyzed for their performances with respect to several yet well-known clustering algorithms. The gain brought by the geometrical representation for the time series clustering is illustrated through a large benchmark of databases. We particularly exhibit that, firstly, the embedding of the time series on higher dimensional spaces gives better results than classical approaches and, secondly, that the embedding on the Stiefel manifold - in conjunction with UMAP and HDBSCAN clustering algorithms - is the recommended framework for time series clustering.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005999",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Cluster analysis",
      "Computer science",
      "Dynamic time warping",
      "Embedding",
      "Engineering",
      "Geodesy",
      "Geography",
      "Geometry",
      "Law",
      "Manifold (fluid mechanics)",
      "Mathematics",
      "Mechanical engineering",
      "Paleontology",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Series (stratigraphy)",
      "Stiefel manifold"
    ],
    "authors": [
      {
        "surname": "Péalat",
        "given_name": "Clément"
      },
      {
        "surname": "Bouleux",
        "given_name": "Guillaume"
      },
      {
        "surname": "Cheutet",
        "given_name": "Vincent"
      }
    ]
  },
  {
    "title": "Weighted clustering ensemble: A review",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108428",
    "abstract": "Clustering ensemble, or consensus clustering, has emerged as a powerful tool for improving both the robustness and the stability of results from individual clustering methods. Weighted clustering ensemble arises naturally from clustering ensemble. One of the arguments for weighted clustering ensemble is that elements (clusterings or clusters) in a clustering ensemble are of different quality, or that objects or features are of varying significance. However, it is not possible to directly apply the weighting mechanisms from classification (supervised) domain to clustering (unsupervised) domain, also because clustering is inherently an ill-posed problem. This paper provides an overview of weighted clustering ensemble by discussing different types of weights, major approaches to determining weight values, and applications of weighted clustering ensemble to complex data. The unifying framework presented in this paper will help clustering practitioners select the most appropriate weighting mechanisms for their own problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100604X",
    "keywords": [],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Mimi"
      }
    ]
  },
  {
    "title": "Learning interlaced sparse Sinkhorn matching network for video super-resolution",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108475",
    "abstract": "How to effectively fuse inter- and intra-frame spatio-temporal information plays a key role in video super-resolution (VSR). Most existing works rely heavily on the accuracy of motion estimation and compensation for spatio-temporal feature alignment. However, they cannot perform well when suffering from large-scale and complex motions. To this end, this paper introduces an efficient and effective Interlaced Sparse Sinkhorn Matching (ISSM) network for VSR, which aligns supporting frames with the reference one in the feature space by learning optimal matching between image regions across frames. Specifically, the ISSM divides the input dense affinity matrix into two sparse block matrixes: one can match long-distance regions while the other can match short-distance regions, and then we leverage an efficient Sinkhorn method on each block to learn optimal matching. Moreover, we insert a residual atrous spatial pyramid pooling module before the ISSM, which can flexibly generate multi-scale features frame by frame to capture the multi-scale context information in images. The aligned features of each adjacent frame are then fed to a bidirectional temporal fusion module to capture the rich temporal information. Finally, the fused features are sent into a frame-wise dynamic reconstruction network to produce an HR frame. Extensive evaluations on three benchmark datasets demonstrate the superiority of our method over the state-of-the-art methods in terms of PSNR and SSIM.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006518",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Block (permutation group theory)",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Feature (linguistics)",
      "Frame (networking)",
      "Geodesy",
      "Geography",
      "Geometry",
      "Leverage (statistics)",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pooling",
      "Pyramid (geometry)",
      "Residual",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Huihui"
      },
      {
        "surname": "Jin",
        "given_name": "Yutong"
      },
      {
        "surname": "Cheng",
        "given_name": "Yong"
      },
      {
        "surname": "Liu",
        "given_name": "Bo"
      },
      {
        "surname": "Liu",
        "given_name": "Dong"
      },
      {
        "surname": "Liu",
        "given_name": "Qingshan"
      }
    ]
  },
  {
    "title": "Discriminative deep attributes for generalized zero-shot learning",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108435",
    "abstract": "We indirectly predict a class by deriving user-defined (i.e., existing) attributes (UA) from an image in generalized zero-shot learning (GZSL). High-quality attributes are essential for GZSL, but the existing UAs are sometimes not discriminative. We observe that the hidden units at each layer in a convolutional neural network (CNN) contain highly discriminative semantic information across a range of objects, parts, scenes, textures, materials, and color. The semantic information in CNN features is similar to the attributes that can distinguish each class. Motivated by this observation, we employ CNN features like novel class representative semantic data, i.e., deep attribute (DA). Precisely, we propose three objective functions (e.g., compatible, discriminative, and intra-independent) to inject the fundamental properties into the generated DA. We substantially outperform the state-of-the-art approaches on four challenging GZSL datasets, including CUB, FLO, AWA1, and SUN. Furthermore, the existing UA and our proposed DA are complementary and can be combined to enhance performance further.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006117",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Discriminative model",
      "Machine learning",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Hoseong"
      },
      {
        "surname": "Lee",
        "given_name": "Jewook"
      },
      {
        "surname": "Byun",
        "given_name": "Hyeran"
      }
    ]
  },
  {
    "title": "Supervised dimensionality reduction technology of generalized discriminant component analysis and its kernelization forms",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108450",
    "abstract": "Supervised subspace projection technology is a major method for dimensionality reduction in pattern recognition. At present, most supervised subspace projection algorithms are derived from the multi-dimensional extended version of Fisher linear discriminant analysis (FDA), also known as Multi-dimensional Fisher discriminant analysis (MD-FDA). However, MD-FDA needs to be improved further because the projection vectors in the noise-subspace cannot be sorted and the ill-condition of the within-class scatter matrix may cause severe numerical instabilities. Generalized discriminant component analysis (GDCA), the generalization of MD-FDA, together with its kernelization forms are proposed and correspondingly rigorous mathematical proofs are detailed in this paper. By virtue of 5 validation data sets derived from UCI Machine Learning Repository and our laboratory, the theoretical validity and technical advantages of GDCA as well as its kernelization forms are verified, and the effectiveness of the newly proposed method is demonstrated in comparison with 36 kinds of state-of-the-art dimensionality reduction algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006269",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Dimensionality reduction",
      "Discriminant",
      "Facial recognition system",
      "Geometry",
      "Kernel Fisher discriminant analysis",
      "Kernelization",
      "Linear discriminant analysis",
      "Machine learning",
      "Mathematics",
      "Optimal discriminant analysis",
      "Parameterized complexity",
      "Pattern recognition (psychology)",
      "Principal component analysis",
      "Projection (relational algebra)",
      "Reduction (mathematics)",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Ruixu"
      },
      {
        "surname": "Gao",
        "given_name": "Wensheng"
      },
      {
        "surname": "Ding",
        "given_name": "Dengwei"
      },
      {
        "surname": "Liu",
        "given_name": "Weidong"
      }
    ]
  },
  {
    "title": "Multiple cross-attention for video-subtitle moment retrieval",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.02.016",
    "abstract": "Given a natural language query, video-subtitle moment retrieval (VSMR) aims at localizing a short video moment from a video with subtitles. Different from the extensively studied video moment retrieval task locating vision moments to match the text query, VSMR is a more challenging task because the retrieval results have to contain both vision and subtitle contents, which needs a deep understanding of one more subtitle modality in addition to the query text and the video itself. Towards this end, we design a mutually guided cross-attention block by uniting multiple self-attention units and guided-attention units with successively mutual connections, and therefrom propose a novel Multiple Cross-Attention (MCA) network for multi-modal interaction and matching. Through such an attention interaction among multiple modalities, the proposed MCA can favorably model both the query-video relations and query-subtitle relations in word-by-clip level for VSMR. We quantitatively and qualitatively evaluate our proposed MCA on TVR, which is the most challenging VSMR dataset available. Empirical evidences demonstrate that our method outperforms the state-of-the-art ones.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000587",
    "keywords": [
      "Artificial intelligence",
      "Classical mechanics",
      "Computer science",
      "Economics",
      "Information retrieval",
      "Management",
      "Matching (statistics)",
      "Mathematics",
      "Modalities",
      "Moment (physics)",
      "Operating system",
      "Physics",
      "Social science",
      "Sociology",
      "Statistics",
      "Subtitle",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Fu",
        "given_name": "Hao"
      },
      {
        "surname": "Wang",
        "given_name": "Hongxing"
      }
    ]
  },
  {
    "title": "Adaptive Gabor convolutional networks",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108495",
    "abstract": "Despite the great breakthroughs that deep convolutional neural networks (DCNNs) have achieved on image representation learning in recent years, they lack the ability to extract invariant information from images. On the other hand, several traditional feature extractors like Gabor filters are widely used for invariant information learning from images. In this paper, we propose a new class of DCNNs named adaptive Gabor convolutional networks (AGCNs). In the AGCNs, the convolutional kernels are adaptively multiplied by Gabor filters to construct the Gabor convolutional filters (GCFs), while the parameters in the Gabor functions (i.e., scale and orientation) are learned alongside those in the convolutional kernels. In addition, the GCFs can be regenerated after updating the Gabor filters and convolutional kernels. We evaluate the performance of the proposed AGCNs on image classification using five benchmark image datasets, i.e., MNIST and its rotated version, SVHN, CIFAR-10, CINIC-10, and DogsVSCats. Experimental results show that the AGCNs are robust to spatial transformations and have achieved higher accuracy compared with the DCNNs and other state-of-the-art deep networks. Moreover, the GCFs can be easily embedded into any classical DCNN models (e.g., ResNet) and require fewer parameters than the corresponding DCNNs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006713",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Contextual image classification",
      "Convolutional neural network",
      "Deep learning",
      "Discrete wavelet transform",
      "Feature (linguistics)",
      "Feature extraction",
      "Gabor filter",
      "Gabor wavelet",
      "Geodesy",
      "Geography",
      "Geometry",
      "Image (mathematics)",
      "Invariant (physics)",
      "Linguistics",
      "MNIST database",
      "Mathematical physics",
      "Mathematics",
      "Orientation (vector space)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Wavelet",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Yuan",
        "given_name": "Ye"
      },
      {
        "surname": "Wang",
        "given_name": "Li-Na"
      },
      {
        "surname": "Zhong",
        "given_name": "Guoqiang"
      },
      {
        "surname": "Gao",
        "given_name": "Wei"
      },
      {
        "surname": "Jiao",
        "given_name": "Wencong"
      },
      {
        "surname": "Dong",
        "given_name": "Junyu"
      },
      {
        "surname": "Shen",
        "given_name": "Biao"
      },
      {
        "surname": "Xia",
        "given_name": "Dongdong"
      },
      {
        "surname": "Xiang",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Mask encoding: A general instance mask representation for object segmentation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108505",
    "abstract": "Instance segmentation is one of the most challenging tasks in computer vision, which requires separating each instance in pixels. To date, a low-resolution binary mask is the dominant paradigm for representation of instance mask. For example, the size of the predicted mask in Mask R-CNN is usually 28 × 28 . Generally, a low-resolution mask can not capture the object details well, while a high-resolution mask dramatically increases the training complexity. In this work, we propose a flexible and effective approach to encode the high-resolution structured mask to the compact representation which shares the advantages of high-quality and low-complexity. The proposed mask representation can be easily integrated into two-stage pipelines such as Mask R-CNN, improving mask AP by 0.9% on the COCO dataset, 1.4% on the LVIS dataset, and 2.1% on the Cityscapes dataset. Moreover, a novel single shot instance segmentation framework can be constructed by extending the existing one-stage detector with a mask branch for this instance representation. Our model shows its superiority over the explicit contour-based pipelines in accuracy with similar computational complexity. We also evaluate our method for video instance segmentation, achieving promising results on YouTube-VIS dataset. Code is available at: https://git.io/AdelaiDet",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006816",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Detector",
      "ENCODE",
      "Encoding (memory)",
      "Gene",
      "Law",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Pixel",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Segmentation",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Rufeng"
      },
      {
        "surname": "Kong",
        "given_name": "Tao"
      },
      {
        "surname": "Wang",
        "given_name": "Xinlong"
      },
      {
        "surname": "You",
        "given_name": "Mingyu"
      }
    ]
  },
  {
    "title": "Rapid construction of 4D high-quality microstructural image for cement hydration using partial information registration",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108471",
    "abstract": "Studying on the microstructural evolution of cement paste during hydration is of considerable significance for understanding its mechanism and designing such material in cement industry. With the use of microtomography and image registration, the four-dimensional (4D) microstructure of cement paste can be captured, thereby assisting material scientists in studying the hydration process in situ. However, as a challenging task, the construction of high-quality 4D microstructural image is remarkably impeded by image size, isotropy, and homogeneity. This paper proposes an image processing framework to construct 4D high quality microstructural image rapidly for cement hydration. This framework improves and accelerates microstructural image registration and enhancement by using bias field correction, temporal intensity calibration and fast image registration. Additionally, a partial information registration method adopting partial information on the spatial and phased scales, is proposed to improve the registration speed and accuracy. Furthermore, a multi-factor multi-layer particle swarm optimization is proposed to improve the optimization in registration. Experimental results indicate that the 4D high quality microstructural image can be constructed rapidly with promising precision.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006476",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cement",
      "Composite material",
      "Computer science",
      "Computer vision",
      "Homogeneity (statistics)",
      "Image (mathematics)",
      "Image quality",
      "Image registration",
      "Machine learning",
      "Materials science",
      "Particle swarm optimization"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Liangliang"
      },
      {
        "surname": "Wang",
        "given_name": "Lin"
      },
      {
        "surname": "Yang",
        "given_name": "Bo"
      },
      {
        "surname": "Niu",
        "given_name": "Sijie"
      },
      {
        "surname": "Han",
        "given_name": "Yamin"
      },
      {
        "surname": "Oh",
        "given_name": "Sung-Kwun"
      }
    ]
  },
  {
    "title": "cPCA++: An efficient method for contrastive feature learning",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108378",
    "abstract": "In this work, we propose a new data visualization and clustering technique for discovering discriminative structures in high-dimensional data. This technique, referred to as cPCA++, is motivated by the fact that the interesting features of a “target” dataset may be obscured by high variance components during traditional PCA. By analyzing what is referred to as a “background” dataset (i.e., one that exhibits the high variance principal components but not the interesting structures), our technique is capable of efficiently highlighting the structures that are unique to the “target” dataset. Similar to another recently proposed algorithm called “contrastive PCA” (cPCA), the proposed cPCA++ method identifies important dataset-specific patterns that are not detected by traditional PCA in a wide variety of settings. However, unlike cPCA, the proposed cPCA++ method does not require a parameter sweep, and as a result, it is significantly more efficient. Several experiments were conducted in order to compare the proposed method to state-of-the-art methods. These experiments show that the proposed method achieves performance that is similar to or better than that of the other methods, while being more efficient.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005586",
    "keywords": [
      "Accounting",
      "Artificial intelligence",
      "Business",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Discriminative model",
      "Feature (linguistics)",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Principal component analysis",
      "Variance (accounting)",
      "Variety (cybernetics)",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Salloum",
        "given_name": "Ronald"
      },
      {
        "surname": "Kuo",
        "given_name": "C.-C. Jay"
      }
    ]
  },
  {
    "title": "Combining embedding-based and symbol-based methods for entity alignment",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108433",
    "abstract": "The objective of entity alignment is to judge whether entities refer to the same object in the real world. Methods for entity alignment can be grossly divided into two groups: conventional symbol-based entity alignment methods and embedding-based entity alignment methods. Both groups of methods have advantages and disadvantages (which are detailed in Section 1). Therefore, combining the advantages of both methods might be a promising strategy. However, to the best of our knowledge, only the RTEA algorithm that was proposed in our previous conference paper (Proceeding of Pacific Rim International Conference on Artificial Intelligence, pp. 162–175, 2019) utilizes this strategy for entity alignment. This manuscript is an extended version of that conference paper, in which an improved algorithm, namely, ESEA (combining e mbedding-based and s ymbol-based methods for e ntity a lignment), is proposed based on the following steps. First, a novel method for combining embedding models with symbol-based models is proposed. Entities with high vector similarities are obtained through a hybrid embedding model, and the final aligned entity pairs are calculated via symbol-based methods. Second, a series of symbol-based methods, instead of only the edit distance method in the original version, are combined with embedding-based methods for relation alignment. Third, we combine symbol-based and embedding-based methods in a more complicated framework with the objective of better exploiting the advantages of both methods. The experimental results on real-world datasets demonstrate that the proposed method outperformed several state-of-the-art embedding-based entity alignment approaches and outperformed our previous RTEA method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006099",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Embedding",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Symbol (formal)"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Tingting"
      },
      {
        "surname": "Bu",
        "given_name": "Chenyang"
      },
      {
        "surname": "Zhu",
        "given_name": "Yi"
      },
      {
        "surname": "Wu",
        "given_name": "Xindong"
      }
    ]
  },
  {
    "title": "Calibrating probabilistic predictions of quantile regression forests with conformal predictive systems",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.02.003",
    "abstract": "Quantile regression forests (QRF) is a generalization of random forests for quantile regression, which can also output probabilistic prediction for regression problems. QRF delivers a nonlinear and nonparametric way of probabilistic prediction and is widely used in many applications. To improve the prediction quality of QRF, this paper builds conformal predictive systems (CPSs) on top of QRF to calibrate the probabilistic prediction, which is the first attempt of combining CPSs with QRF to our best knowledge. Conformal predictive systems aim to produce valid probabilistic predictions in machine learning community, whose predictive distributions are compatible with realizations. Three algorithms are proposed in this paper. One is a fast approximation algorithm to split conformal predictive system. Building on that, the other two algorithms are proposed based on the out-of-bag samples and the weighted approach of QRF. Experiments with 20 public data sets were conducted and the results demonstrated that our algorithms are empirically valid and compared favourably with the original QRF.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552200037X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Conformal map",
      "Econometrics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Probabilistic logic",
      "Quantile",
      "Quantile regression",
      "Random forest",
      "Regression",
      "Regression analysis",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Di"
      },
      {
        "surname": "Wang",
        "given_name": "Ping"
      },
      {
        "surname": "Wang",
        "given_name": "Cong"
      },
      {
        "surname": "Wang",
        "given_name": "Pingping"
      }
    ]
  },
  {
    "title": "A Tri-Attention fusion guided multi-modal segmentation network",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108417",
    "abstract": "In the field of multimodal segmentation, the correlation between different modalities can be considered for improving the segmentation results. Considering the correlation between different MR modalities, in this paper, we propose a multi-modality segmentation network guided by a novel tri-attention fusion. Our network includes N model-independent encoding paths with N image sources, a tri-attention fusion block, a dual-attention fusion block, and a decoding path. The model independent encoding paths can capture modality-specific features from the N modalities. Considering that not all the features extracted from the encoders are useful for segmentation, we propose to use dual attention based fusion to re-weight the features along the modality and space paths, which can suppress less informative features and emphasize the useful ones for each modality at different positions. Since there exists a strong correlation between different modalities, based on the dual attention fusion block, we propose a correlation attention module to form the tri-attention fusion block. In the correlation attention module, a correlation description block is first used to learn the correlation between modalities and then a constraint based on the correlation is used to guide the network to learn the latent correlated features which are more relevant for segmentation. Finally, the obtained fused feature representation is projected by the decoder to obtain the segmentation results. Our experiment results tested on BraTS 2018 dataset for brain tumor segmentation demonstrate the effectiveness of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321005938",
    "keywords": [],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Tongxue"
      },
      {
        "surname": "Ruan",
        "given_name": "Su"
      },
      {
        "surname": "Vera",
        "given_name": "Pierre"
      },
      {
        "surname": "Canu",
        "given_name": "Stéphane"
      }
    ]
  },
  {
    "title": "Guided neighborhood affine subspace embedding for feature matching",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108489",
    "abstract": "Feature matching, which refers to determining reliable correspondences between two sets of feature points, is a fundamental component of numerous visual tasks. This paper proposes a novel method, termed as guided neighborhood affine subspace embedding (NASE), to eliminate false matches from the given tentative feature matches. Its essential philosophy is to preserve the underlying intrinsic manifold of potential true matches. Specifically, we aim to approximate the manifold of an inlier with an affine subspace fitted on its neighbors by imposing a motion-consistency constraint. Considering that the “corresponding manifold” of inliers may be biased by gross outliers, we introduce a density-based seed point selection strategy for neighborhood refinement. Based on the above two strategies, we further formulate the general feature matching problem into a mathematical optimization model and deduce a closed-form solution with linearithmic time complexity (i.e., O ( N log N ) ) for mismatch removal. Additionally, we devise a multi-scale strategy for neighborhood construction, making our method more robust to various degradations. Extensive experiments on general feature matching, fundamental matrix estimation, and loop closure detection demonstrate the clear superiority of NASE over the state-of-the-arts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006658",
    "keywords": [
      "Affine hull",
      "Affine space",
      "Affine transformation",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Constraint (computer-aided design)",
      "Dimensionality reduction",
      "Embedding",
      "Engineering",
      "Feature (linguistics)",
      "Geometry",
      "Linguistics",
      "Manifold (fluid mechanics)",
      "Manifold alignment",
      "Matching (statistics)",
      "Mathematics",
      "Mechanical engineering",
      "Nonlinear dimensionality reduction",
      "Outlier",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pure mathematics",
      "Statistics",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Zizhuo"
      },
      {
        "surname": "Ma",
        "given_name": "Yong"
      },
      {
        "surname": "Mei",
        "given_name": "Xiaoguang"
      },
      {
        "surname": "Huang",
        "given_name": "Jun"
      },
      {
        "surname": "Ma",
        "given_name": "Jiayi"
      }
    ]
  },
  {
    "title": "Relevance attack on detectors",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108491",
    "abstract": "This paper focuses on high-transferable adversarial attacks on detectors, which are hard to attack in a black-box manner, because of their multiple-output characteristics and the diversity across architectures. To pursue a high attack transferability, one plausible way is to find a common property across detectors, which facilitates the discovery of common weaknesses. We are the first to suggest that the relevance map from interpreters for detectors is such a property. Based on it, we design a Relevance Attack on Detectors (RAD), which achieves a state-of-the-art transferability, exceeding existing results by above 20%. On MS COCO, the detection mAPs for all 8 black-box architectures are more than halved and the segmentation mAPs are also significantly influenced. Given the great transferability of RAD, we generate the first adversarial dataset for object detection and instance segmentation, i.e., Adversarial Objects in COntext (AOCO), which helps to quickly evaluate and improve the robustness of detectors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006671",
    "keywords": [
      "Adversarial system",
      "Archaeology",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Detector",
      "Epistemology",
      "Gene",
      "Geography",
      "Law",
      "Logit",
      "Machine learning",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Property (philosophy)",
      "Relevance (law)",
      "Robustness (evolution)",
      "Segmentation",
      "Telecommunications",
      "Transferability"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Sizhe"
      },
      {
        "surname": "He",
        "given_name": "Fan"
      },
      {
        "surname": "Huang",
        "given_name": "Xiaolin"
      },
      {
        "surname": "Zhang",
        "given_name": "Kun"
      }
    ]
  },
  {
    "title": "Developing a generic framework for anomaly detection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108500",
    "abstract": "The fusion of one-class classifiers (OCCs) has been shown to exhibit promising performance in a variety of machine learning applications. The ability to assess the similarity or correlation between the output of various OCCs is an important prerequisite for building of a meaningful OCCs ensemble. However, this aspect of the OCC fusion problem has been mostly ignored so far. In this paper, we propose a new method of constructing a fusion of OCCs with three contributions: (a) As a key contribution, enabling an OCC ensemble design using exclusively non anomalous samples, we propose a novel fitness function to evaluate the competency of OCCs without requiring samples from the anomalous class; (b) As a minor, but impactful contribution, we investigate alternative forms of score normalisation of OCCs, and identify a novel two-sided normalisation method as the best in coping with long tail non anomalous data distributions; (c) In the context of building our proposed OCC fusion system based on the weighted averaging approach, we find that the weights optimised using a particle swarm optimisation algorithm produce the most effective solution. We evaluate the merits of the proposed method on 15 benchmarking datasets from different application domains including medical, anti-spam and face spoofing detection. The comparison of the proposed approach with state-of-the-art methods alongside the statistical analysis confirm the effectiveness of the proposed model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006762",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Benchmarking",
      "Business",
      "Computer science",
      "Data mining",
      "Machine learning",
      "Marketing",
      "Particle swarm optimization",
      "Pattern recognition (psychology)",
      "Probabilistic logic"
    ],
    "authors": [
      {
        "surname": "Fatemifar",
        "given_name": "Soroush"
      },
      {
        "surname": "Awais",
        "given_name": "Muhammad"
      },
      {
        "surname": "Akbari",
        "given_name": "Ali"
      },
      {
        "surname": "Kittler",
        "given_name": "Josef"
      }
    ]
  },
  {
    "title": "Photoplethysmographic biometrics: A comprehensive survey",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.03.006",
    "abstract": "The wide diffusion of wearable sensors and mobile devices encouraged the study of biometric recognition techniques that require a low level of cooperation from users. Among them, the analysis of cardiac information extracted from plethysmographic (PPG) signals is attracting the research community due to the possibility of performing continuous authentications using low-cost devices that can acquire signals without any action required from the users. Although PPG-based biometric systems are relatively recent technologies, machine learning techniques and deep learning strategies have shown accuracy in heterogeneous application scenarios. This paper presents the first literature review of PPG-based biometric recognition approaches. First, we describe the application contexts suitable for PPG-based biometrics. Second, we analyze the systems in the literature, describe the acquisition sensors, and present a classification of the processing methods. Third, we summarize the available public datasets and the results achieved by recent state-of-the-art approaches. Finally, we analyze the open problems in this research field.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000721",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Embedded system",
      "Field (mathematics)",
      "Human–computer interaction",
      "Machine learning",
      "Mathematics",
      "Mobile device",
      "Open research",
      "Pure mathematics",
      "Wearable computer",
      "Wearable technology",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Donida Labati",
        "given_name": "Ruggero"
      },
      {
        "surname": "Piuri",
        "given_name": "Vincenzo"
      },
      {
        "surname": "Rundo",
        "given_name": "Francesco"
      },
      {
        "surname": "Scotti",
        "given_name": "Fabio"
      }
    ]
  },
  {
    "title": "Hierarchical domain adaptation with local feature patterns",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108445",
    "abstract": "Domain adaptation is proposed to generalize learning machines and address performance degradation of models that are trained from one specific source domain but applied to novel target domains. Existing domain adaptation methods focus on transferring holistic features whose discriminability is generally tailored to be source-specific and inferiorly generic to be transferable. As a result, standard domain adaptation on holistic features usually damages feature structures, especially local feature statistics, and deteriorates the learned discriminability. To alleviate this issue, we propose to transfer primitive local feature patterns, whose discriminability are shown to be inherently more sharable, and perform hierarchical feature adaptation. Concretely, we first learn a cluster of domain-shared local feature patterns and partition the feature space into cells. Local features are adaptively aggregated inside each cell to obtain cell features, which are further integrated into holistic features. To achieve fine-grained adaptations, we simultaneously perform alignment on local features, cell features and holistic features, within which process the local and cell features are aligned independently inside each cell to maintain the learned local structures and prevent negative transfer. Experimenting on typical one-to-one unsupervised domain adaptation for both image classification and action recognition tasks, partial domain adaptation, and domain-agnostic adaptation, we show that the proposed method achieves more reliable feature transfer by consistently outperforming state-of-the-art models and the learned domain-invariant features generalize well to novel domains.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100621X",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Classifier (UML)",
      "Combinatorics",
      "Computer science",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Feature (linguistics)",
      "Feature vector",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Partition (number theory)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics"
    ],
    "authors": [
      {
        "surname": "Wen",
        "given_name": "Jun"
      },
      {
        "surname": "Yuan",
        "given_name": "Junsong"
      },
      {
        "surname": "Zheng",
        "given_name": "Qian"
      },
      {
        "surname": "Liu",
        "given_name": "Risheng"
      },
      {
        "surname": "Gong",
        "given_name": "Zhefeng"
      },
      {
        "surname": "Zheng",
        "given_name": "Nenggan"
      }
    ]
  },
  {
    "title": "Loss function search for person re-identification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108432",
    "abstract": "In recent years, person re-identification, which learns discriminative features for the specific person retrieval problem across non-overlapping cameras, has attracted extensive attention. One of the main challenges in person re-identification with deep neural networks is the design of the loss function, which plays a vital role in improving the discrimination of the learned features. However, most existing models utilize the hand-designed loss functions, which are usually sub-optimal and time-consuming. The search spaces of the two existing AutoML-based methods are either too complicated or too simple to include various forms of loss functions. In order to solve the irrationality of the above search spaces, in this paper, we propose a method of AutoML for loss function search named LFS-ReID for person ReID in the framework of the margin-based softmax loss function. Specifically, we first analyze the margin-based softmax loss function and conclude four key properties. Then we carefully design a sampling distribution based on the non-independent truncated Gaussian distributions to sample the loss function, which conforms to the above four properties. Finally, a method based on reinforcement learning is adopted to optimize the sampling distribution dynamically. Experimental results demonstrate that the proposed method achieves state-of-the-art performance on four commonly used datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006087",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Botany",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Evolutionary biology",
      "Filter (signal processing)",
      "Function (biology)",
      "Gaussian",
      "Hinge loss",
      "Identification (biology)",
      "Machine learning",
      "Margin (machine learning)",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Sampling (signal processing)",
      "Softmax function",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Gu",
        "given_name": "Hongyang"
      },
      {
        "surname": "Li",
        "given_name": "Jianmin"
      },
      {
        "surname": "Fu",
        "given_name": "Guangyuan"
      },
      {
        "surname": "Yue",
        "given_name": "Min"
      },
      {
        "surname": "Zhu",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Deep reinforcement learning with credit assignment for combinatorial optimization",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108466",
    "abstract": "Recent advances in Deep Reinforcement Learning (DRL) demonstrates the potential for solving Combinatorial Optimization (CO) problems. DRL shows advantages over traditional methods both on scalability and computation efficiency. However, the DRL problems transformed from CO problems usually have a huge state space, and the main challenge of solving them has changed from high computation complexity to high sample complexity. Credit assignment determines the contribution of each internal decision to the final success or failure, and it has been shown to be effective in reducing the sample complexity of the training process. In this paper, we resort to a model-based reinforcement learning method to assign credits for model-free DRL methods. Since heuristic methods plays an important role on state-of-the-art solutions for CO problems, we propose using a model to represent those heuristic knowledge and derive the credit assignment from the model. This model-based credit assignment can facilitate the model-free DRL to perform a more effective exploration, and the data collected by the model-free DRL refines the model continuously as the training progresses. Extensive experiments on various CO problems with different settings show that our framework outperforms previous state-of-the-art methods on performance and training efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006427",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computation",
      "Computer science",
      "Database",
      "Heuristic",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Reinforcement learning",
      "Sample complexity",
      "Scalability",
      "State space",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Dong"
      },
      {
        "surname": "Weng",
        "given_name": "Jiayi"
      },
      {
        "surname": "Huang",
        "given_name": "Shiyu"
      },
      {
        "surname": "Li",
        "given_name": "Chongxuan"
      },
      {
        "surname": "Zhou",
        "given_name": "Yichi"
      },
      {
        "surname": "Su",
        "given_name": "Hang"
      },
      {
        "surname": "Zhu",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Adaptive loss function based least squares one-class support vector machine",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.03.009",
    "abstract": "Least squares one-class support vector machine (LS-OCSVM) can accurately describe the similarity between new sample and training set. However, LS-OCSVM is very sensitive to the outliers among training samples, which means that the separating hyperplane of LS-OCSVM may deviate from the normal data even with a few outliers. To enhance the anti-outlier performance of LS-OCSVM, a novel adaptive loss function based LS-OCSVM is proposed. In the proposed method, an adaptive loss function is utilized to substitute the square loss function in the objective function of LS-OCSVM. The property of Fisher consistency for the adaptive loss function is validated from the theoretical viewpoint. The optimization problem of the proposed method is solved by the iteratively reweighted least squares (IRLS) method. In comparison with its nine related methods, the proposed method demonstrates better anti-outlier and generalization abilities on synthetic and benchmark data sets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000745",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Class (philosophy)",
      "Computer science",
      "Estimator",
      "Evolutionary biology",
      "Function (biology)",
      "Least squares support vector machine",
      "Least-squares function approximation",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Statistics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Xing",
        "given_name": "Hong-Jie"
      },
      {
        "surname": "He",
        "given_name": "Zi-Chuan"
      }
    ]
  },
  {
    "title": "Introduction to conformal predictors",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108507",
    "abstract": "This paper aims to provide a compact but accessible introduction to Conformal Predictors (CP), a Machine Learning method with the distinguishing property of producing predictions that exhibit a chosen error rate. This property, referred to as validity, is backed by not only asymptotic, but also finite-sample probabilistic guarantees. CPs differ from the conventional approach to prediction in that they introduce hedging in the form of set-valued predictions. The CP validity guarantees do not require assumptions such as priors, but are of broad applicability as they rely solely on exchangeability. The CP framework is universal in the sense that it operates on top of virtually any Machine Learning method. In addition to the formal definition, this introduction discusses CP variants that can be computed efficiently (Inductive or “split” CP) or that are suitable for imbalanced data sets (class-conditional CP). Finally, a short survey of the field provides references for relevant research and highlights the variety of domains in which CPs have found valuable application.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032100683X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Conformal map",
      "Geology",
      "Geometry",
      "Mathematics"
    ],
    "authors": [
      {
        "surname": "Toccaceli",
        "given_name": "Paolo"
      }
    ]
  },
  {
    "title": "Hyperspherical class prototypes for adversarial robustness",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108527",
    "abstract": "This work addresses the problem of adversarial robustness in deep neural network classification from an optimal class boundary estimation perspective. It is argued that increased model robustness to adversarial attacks can be achieved when the feature learning process is monitored by geometrically-inspired optimization criteria. To this end, we propose to learn hyperspherical class prototypes in the neural feature embedding space, along with training the network parameters. Three concurrent optimization functions for the intermediate hidden layer training data activations are devised, requiring items of the same class to be enclosed by the corresponding class prototype boundaries, to have minimum distance from their class prototype vector (i.e., hypersphere center) and to have maximum distance from the remainder hypersphere centers. Our experiments show that training standard classification model architectures with the proposed objectives, significantly increases their robustness to white-box adversarial attacks, without adverse (if not beneficial) effects to their classification accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000085",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Ellipse",
      "Embedding",
      "Feature vector",
      "Gene",
      "Geometry",
      "Hypersphere",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Mygdalis",
        "given_name": "Vasileios"
      },
      {
        "surname": "Pitas",
        "given_name": "Ioannis"
      }
    ]
  },
  {
    "title": "Symmetry-Driven hyper feature GCN for skeleton-based gait recognition",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108520",
    "abstract": "Gait recognition, as an attractive task in biometrics, remains challenging due to significant intra-class changes of clothing and pose variations across different cameras. Recent approaches mainly focus on silhouette-based gait mode, which is easy to model in Convolutional Neural Networks (CNNs). Compared with silhouettes, the dynamics of skeletons essentially convey more robust information, which is invariant to view and clothing changes. Conventional approaches for modeling skeletons usually rely on hand-crafted features or traversal rules, thus resulting in limited expressive power and difficulties of generalization. In this work, we address the skeleton-based gait recognition task with a novel Symmetry-Driven Hyper Feature Graph Convolutional Network (SDHF-GCN), which goes beyond the limitations of previous approaches by automatically learning multiple dynamic patterns and hierarchical semantic features in a unified Graph Convolutional Network (GCN). This model involves three dynamic patterns: natural connection, temporal correlation and symmetric interaction, which enriches the description of dynamic patterns by exploiting symmetry perceptual principles. Furthermore, a hyper feature network is proposed to aggregate the hierarchical semantic features, including dynamic features at the high level, structured features at the intermediate level, and static features at the low level, which complement each other to enhance the discriminative ability. By integrating different patterns in the hierarchical structure, the model is able to generate versatile and discriminative representations, thus improving the recognition rate. On the CASIA-B and OUMVLP-Pose datasets, the proposed SDHF-GCN renders substantial improvements over mainstream methods, especially in the coat-wearing scenario, with superior robustness to covariate factors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000012",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Convolutional neural network",
      "Discriminative model",
      "Feature (linguistics)",
      "Gait",
      "Gene",
      "Graph",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physiology",
      "Robustness (evolution)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Xiaokai"
      },
      {
        "surname": "You",
        "given_name": "Zhaoyang"
      },
      {
        "surname": "He",
        "given_name": "Yuxiang"
      },
      {
        "surname": "Bi",
        "given_name": "Sheng"
      },
      {
        "surname": "Wang",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "Generative adversarial U-Net for domain-free few-shot medical diagnosis",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.03.022",
    "abstract": "In interactive remote medical diagnosis, medical diagnosis support system plays an important role in the pre-screening stage. Deep learning methods are widely used in such system especially medical imaging area to provide an initial diagnosis to support medical professional. However, the shortage of annotated medical images is one of the biggest challenges in the field of medical image computing. Without a sufficient number of training samples, deep learning based models are very likely to suffer from over-fitting problem. The common solution is image manipulation such as image rotation, cropping, or resizing. Those methods can help relieve the over-fitting problem as more training samples are introduced. However, they do not really introduce new images with additional information and may lead to data leakage as the test set may contain similar samples which appear in the training set. To address this challenge, we propose to generate diverse images with generative adversarial network. In this paper, we develop a novel generative method named generative adversarial U-Net, which utilizes both generative adversarial network and U-Net. Different from existing approaches, our newly designed model is domain-free and generalizable to various medical images. Extensive experiments are conducted over eight diverse datasets including computed tomography (CT) scan, pathology, X-ray, etc. The visualization and quantitative results demonstrate the efficacy and good generalization of the proposed method on generating a wide array of high-quality medical images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000873",
    "keywords": [
      "Adversarial system",
      "Algorithm",
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Domain (mathematical analysis)",
      "Engineering",
      "Generative grammar",
      "Geometry",
      "Mathematical analysis",
      "Mathematics",
      "Mechanical engineering",
      "Net (polyhedron)",
      "One shot",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Shot (pellet)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Xiaocong"
      },
      {
        "surname": "Li",
        "given_name": "Yun"
      },
      {
        "surname": "Yao",
        "given_name": "Lina"
      },
      {
        "surname": "Adeli",
        "given_name": "Ehsan"
      },
      {
        "surname": "Zhang",
        "given_name": "Yu"
      },
      {
        "surname": "Wang",
        "given_name": "Xianzhi"
      }
    ]
  },
  {
    "title": "Contour-enhanced attention CNN for CT-based COVID-19 segmentation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108538",
    "abstract": "Accurate detection of COVID-19 is one of the challenging research topics in today's healthcare sector to control the coronavirus pandemic. Automatic data-powered insights for COVID-19 localization from medical imaging modality like chest CT scan tremendously augment clinical care assistance. In this research, a Contour-aware Attention Decoder CNN has been proposed to precisely segment COVID-19 infected tissues in a very effective way. It introduces a novel attention scheme to extract boundary, shape cues from CT contours and leverage these features in refining the infected areas. For every decoded pixel, the attention module harvests contextual information in its spatial neighborhood from the contour feature maps. As a result of incorporating such rich structural details into decoding via dense attention, the CNN is able to capture even intricate morphological details. The decoder is also augmented with a Cross Context Attention Fusion Upsampling to robustly reconstruct deep semantic features back to high-resolution segmentation map. It employs a novel pixel-precise attention model that draws relevant encoder features to aid in effective upsampling. The proposed CNN was evaluated on 3D scans from MosMedData and Jun Ma benchmarked datasets. It achieved state-of-the-art performance with a high dice similarity coefficient of 85.43% and a recall of 88.10%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200019X",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Deep learning",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Leverage (statistics)",
      "Linguistics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Segmentation",
      "Upsampling"
    ],
    "authors": [
      {
        "surname": "Karthik",
        "given_name": "R."
      },
      {
        "surname": "Menaka",
        "given_name": "R."
      },
      {
        "surname": "M",
        "given_name": "Hariharan"
      },
      {
        "surname": "Won",
        "given_name": "Daehan"
      }
    ]
  },
  {
    "title": "Collaborative boundary-aware context encoding networks for error map prediction",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108515",
    "abstract": "Accurately assessing the medical image segmentation quality of the automatically generated predictions is essential for guaranteeing the reliability of the results of computer-assisted diagnosis (CAD). Many researchers have studied segmentation quality estimation without labeled ground truths. Recently, a novel idea is proposed, which transforms segmentation quality assessment (SQA) into the pixel-wise or voxel-wise error map segmentation task. However, the simple application of vanilla segmentation structures in medical domain fails to achieve satisfactory error segmentation results. In this paper, we propose collaborative boundary-aware context encoding networks called EP-Net for error segmentation task. Specifically, we propose a collaborative feature transformation branch for better feature fusion between images and masks, and precise localization of error regions. Further, we propose a context encoding module to utilize the global predictor from the error map to enhance the feature representation and regularize the networks. Extensive experiments on IBSR V2.0 dataset, ACDC dataset and M&Ms dataset demonstrate that EP-Net achieves better error segmentation results compared with the traditional segmentation patterns. Based on error prediction results, we obtain a proxy metric of segmentation quality, which has high Pearson correlation coefficient with the real segmentation accuracy on all datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006919",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Boundary (topology)",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Data mining",
      "Encoding (memory)",
      "Geography",
      "Mathematical analysis",
      "Mathematics"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zhenxi"
      },
      {
        "surname": "Tian",
        "given_name": "Chunna"
      },
      {
        "surname": "Gao",
        "given_name": "Xinbo"
      },
      {
        "surname": "Li",
        "given_name": "Jie"
      },
      {
        "surname": "Jiao",
        "given_name": "Zhicheng"
      },
      {
        "surname": "Wang",
        "given_name": "Cui"
      },
      {
        "surname": "Zhong",
        "given_name": "Zhusi"
      }
    ]
  },
  {
    "title": "A unified perspective of classification-based loss and distance-based loss for cross-view gait recognition",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108519",
    "abstract": "Gait can be used to recognize people in an uncooperative and noninvasive manner and it is hard to imitate or counterfeit, which makes it suitable for video surveillance. The current solutions for gait recognition are still not robust to handle the conditions when the view angles of the gallery and query are different. We improve the performance of cross-view gait recognition from the perspective of metric learning. Specifically, we propose to use angular softmax loss to impose an angular margin for extracting separable features. At the same time, we use triplet loss to make the extracted features more discriminative. Additionally, we add a batch-normalization layer after extracting gait features to effectively optimize two different losses. We evaluate our approach on two widely-used gait dataset: CASIA-B dataset and TUM GAID dataset. The experiment results show that our approach outperforms the prior state-of-the-art approaches, which shows the effectiveness of our approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006956",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Deep learning",
      "Discriminative model",
      "Engineering",
      "Gait",
      "Machine learning",
      "Margin (machine learning)",
      "Metric (unit)",
      "Normalization (sociology)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Physiology",
      "Sociology",
      "Softmax function"
    ],
    "authors": [
      {
        "surname": "Han",
        "given_name": "Feng"
      },
      {
        "surname": "Li",
        "given_name": "Xuejian"
      },
      {
        "surname": "Zhao",
        "given_name": "Jian"
      },
      {
        "surname": "Shen",
        "given_name": "Furao"
      }
    ]
  },
  {
    "title": "Attentive occlusion-adaptive deep network for facial landmark detection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108510",
    "abstract": "To be very specific in this paper, an Attentive Occlusion-adaptive Deep Network, hereafter referred as AODN, is proposed for facial landmark detection, consisting of the geometry-aware module, attention module, and low-rank learning module. Facial Landmark Detection (FLD) is a fundamental pre-processing step of facial related tasks. Occlusion, extreme pose, different expressions and illumination are the main challenges in facial landmark detection related tasks. Convolutional Neural Network (CNN) based FLD methods have attained significant improvement regarding accurate FLD but, to deal with occlusion is still very challenging even for CNN. It is because; probably occlusion misleads CNN on feature representation learning. If faces are partially occluded, the localization accuracy will drop significantly. The role of attention in the human visual system is vital, and researchers proved its significance for the computer vision problem. Taking advantage of geometric relationships among different facial components and attention, we extended our already established Occlusion-adaptive Deep Network (ODN). We introduced the attention module consisting of Channel-wise Attention (CA) and Spatial Attention (SA) to improve its ability to deal with the occlusion and enhance feature representation ability simultaneously. The occlusion probability assists as adaptive weights of high-level features and minimizes the effect of the occlusion and assist in modelling the occlusion. Ablation studies prove the synergistic effect of each module. The summary of our trifold contribution is as follows: i) we introduced attention mechanism in our already established ODN model, to deal with occlusion more precisely, and get the rich feature representation to achieve better performance. ii) As per our best of knowledge, we are the pioneers to introduce CA and SA for FLD to model occlusion. iii) Our proposed methodology reduces the number of entire network parameters, which effectually decreases training time and cost. So, the proposed model is more suitable for scalable data processing. Experimental results prove the better performance of proposed AODN on challenging benchmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006865",
    "keywords": [
      "Artificial intelligence",
      "Cardiology",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Landmark",
      "Law",
      "Linguistics",
      "Medicine",
      "Occlusion",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Sadiq",
        "given_name": "Muhammad"
      },
      {
        "surname": "Shi",
        "given_name": "Daming"
      }
    ]
  },
  {
    "title": "CORE: A knowledge graph entity type prediction method via complex space regression and embedding",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.03.024",
    "abstract": "Entity type prediction is an important problem in knowledge graph (KG) research. A new KG entity type prediction method, named CORE (COmplex space Regression and Embedding), is proposed in this work. The proposed CORE method leverages the expressive power of two complex space embedding models; namely, RotatE and ComplEx models. It embeds entities and types in two different complex spaces using either RotatE or ComplEx. Then, we derive a complex regression model to link these two spaces. Finally, a mechanism to optimize embedding and regression parameters jointly is introduced. Experiments show that CORE outperforms benchmarking methods on representative KG entity type inference datasets. Strengths and weaknesses of various entity type prediction methods are analyzed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000897",
    "keywords": [
      "Artificial intelligence",
      "Benchmarking",
      "Business",
      "Computer science",
      "Core (optical fiber)",
      "Data mining",
      "Embedding",
      "Graph",
      "Inference",
      "Machine learning",
      "Marketing",
      "Mathematics",
      "Regression",
      "Regression analysis",
      "Statistics",
      "Telecommunications",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Ge",
        "given_name": "Xiou"
      },
      {
        "surname": "Wang",
        "given_name": "Yun-Cheng"
      },
      {
        "surname": "Wang",
        "given_name": "Bin"
      },
      {
        "surname": "Kuo",
        "given_name": "C.C. Jay"
      }
    ]
  },
  {
    "title": "Safe incomplete label distribution learning",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108518",
    "abstract": "Label Distribution Learning (LDL) is a popular scenario for solving label ambiguity problems by learning the relative importance of each label to a particular instance. Nevertheless, the label is often incomplete due to the difficulty in annotating label distribution. In this mixing label case with complete and incomplete labels, it is often expected that the learning method can achieve better performance than the baseline method merely utilizing complete labeled data. However, the usage of incomplete labeled data may degrade the performance in real applications. Therefore, it is vital to design a safe incomplete LDL method, which will not deteriorate the performance when exploiting incomplete labeled data. To tackle this important but rarely studied problem, we propose a Safe Incomplete LDL method (SILDL), which learns a classifier that can prevent incomplete labeled instances from worsening the performance. Concretely, we learn predictions from multiple incomplete supervised learners and design an efficient solving algorithm by formulating it as a convex quadratic program. Theoretically, we prove that SILDL can obtain the maximal performance gain against the best one of the multiple baseline methods with mild conditions. Extensive experimental results validate the safeness of the proposed approach and show improvements in performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006944",
    "keywords": [
      "Ambiguity",
      "Artificial intelligence",
      "Baseline (sea)",
      "Classifier (UML)",
      "Complete information",
      "Computer science",
      "Geology",
      "Labeled data",
      "Machine learning",
      "Mathematical economics",
      "Mathematics",
      "Oceanography",
      "Programming language",
      "Semi-supervised learning"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Jing"
      },
      {
        "surname": "Tao",
        "given_name": "Hong"
      },
      {
        "surname": "Luo",
        "given_name": "Tingjin"
      },
      {
        "surname": "Hou",
        "given_name": "Chenping"
      }
    ]
  },
  {
    "title": "Exploring multi-tasking learning in document attribute classification",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.02.015",
    "abstract": "In this work, we adhere to explore a Multi-Tasking learning (MTL) based network to perform document attribute classification such as the font type, font size, font emphasis and scanning resolution classification of a document image. To accomplish these tasks, we operate on either segmented word level or on uniformed size patches randomly cropped out of the document. Furthermore, a hybrid convolution neural network (CNN) architecture ”MTL+MI”, which is based on the combination of MTL and Multi-Instance (MI) of patch and word is used to accomplish joint learning for the classification of the same document attributes. The contribution of this paper are three fold: firstly, based on segmented word images and patches, we present a MTL based network for the classification of a full document image. Secondly, we propose a MTL and MI (using segmented words and patches) based combined CNN architecture (“MTL+MI”) for the classification of same document attributes. Thirdly, based on the multi-tasking classifications of the words and/or patches, we propose an intelligent voting system which is based on the posterior probabilities of each words and/or patches to perform the classification of document’s attributes of complete document image.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000599",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Contextual image classification",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Document classification",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Mondal",
        "given_name": "Tanmoy"
      },
      {
        "surname": "Das",
        "given_name": "Abhijit"
      },
      {
        "surname": "Ming",
        "given_name": "Zuheng"
      }
    ]
  },
  {
    "title": "Improved fuzzy c -means clustering by varying the fuzziness parameter",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.03.017",
    "abstract": "Fuzzy c -means (FCM) is one of the most frequently used methods for clustering, where the fuzziness weighting exponent m is a key hyper-parameter that directly affects the clustering performance. However, FCM requires careful tuning the fuzziness parameter which results in significant time costs. In this research, an improved FCM clustering by varying the fuzziness parameter, called vFCM, is proposed to overcome this issue, based on the facts that the FCM objective is easy to optimize when m is large, while more local valleys appear as m decreases, hence the optimization problem presents a search process from simple to complex when m varies from a large value to a small value approaching 1. Here, the nature of m is similar to the temperature parameter in the deterministic annealing, and moving along a sequence of the FCM objectives by a linear method that proposes to update m automatically provides a form of annealing. Extensive experiments on simulated and real-world data sets show that vFCM is not only more robust to initialization but also improves the clustering performance in high dimensions. Furthermore, the clustering results of vFCM have a low fluctuation according to different m , so it does not require careful tuning the fuzziness parameter. The time that vFCM takes is greatly reduced.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000824",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Fuzzy clustering",
      "Fuzzy logic",
      "Initialization",
      "Mathematics",
      "Medicine",
      "Programming language",
      "Radiology",
      "Simulated annealing",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yuxue"
      },
      {
        "surname": "Zhou",
        "given_name": "Shuisheng"
      },
      {
        "surname": "Zhang",
        "given_name": "Ximin"
      },
      {
        "surname": "Li",
        "given_name": "Dong"
      },
      {
        "surname": "Fu",
        "given_name": "Cui"
      }
    ]
  },
  {
    "title": "Distilled light GaitSet: Towards scalable gait recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.03.019",
    "abstract": "Gait recognition has made significant progress recently. However, most of existing methods utilize complicated neural networks, which lead to high computation cost. In this paper, a lightweight model named Distilled Light GaitSet (DLGS) is proposed for efficient gait recognition. More specifically, a lightweight CNN is designed for efficient computation, and a Joint Knowledge Distillation algorithm is proposed to boost the accuracy of the simplified model. Extensive experiments on the CASIA-B dataset and the OU-MVLP dataset show that the proposed DLGS can reduce the number of parameters and computation cost significantly while achieving the state-of-the-art performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000848",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Chemistry",
      "Computation",
      "Computer science",
      "Database",
      "Distillation",
      "Gait",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Physiology",
      "Scalability"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Xu"
      },
      {
        "surname": "Huang",
        "given_name": "Yan"
      },
      {
        "surname": "Huang",
        "given_name": "Yan"
      },
      {
        "surname": "Shan",
        "given_name": "Caifeng"
      },
      {
        "surname": "Wang",
        "given_name": "Jilong"
      },
      {
        "surname": "Chen",
        "given_name": "Yu"
      }
    ]
  },
  {
    "title": "Scarcity-aware spam detection technique for big data ecosystem",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.03.021",
    "abstract": "To expand their business, companies in the industry use the big data ecosystem for handling enormous amounts of information. For this purpose, text data must be analyzed while ensuring data security and organizing authenticated and valuable data using spam filters. Several methods are available such as Word2Vec, bag-of-words, BERT, and term frequency-inverse document frequency (TF-IDF). However, none of these resolve the data scarcity issue that may result in the presence of incomplete information in collected documents. A technique that groups each document by subject and applies approximation using statistical methods is required to effectively solve this problem. This study proposes a natural language processing-based technique for spam detection that alters topics using a least-squares model and uses gradient-descent and altering-least-squares (AMALS) models to estimate missing data through TF-IDF and uniform-distribution. A performance evaluation demonstrates that the proposed technique outperforms 98% than the existing industrial TF-IDF model in predicting spam in big data ecosystems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000861",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Big data",
      "Computer science",
      "Data mining",
      "Economics",
      "Embedding",
      "Gradient descent",
      "Microeconomics",
      "Physics",
      "Quantum mechanics",
      "Scarcity",
      "Stochastic gradient descent",
      "Term (time)",
      "Word2vec",
      "tf–idf"
    ],
    "authors": [
      {
        "surname": "Park",
        "given_name": "Woo Hyun"
      },
      {
        "surname": "Siddiqui",
        "given_name": "Isma Farah"
      },
      {
        "surname": "Chakraborty",
        "given_name": "Chinmay"
      },
      {
        "surname": "Qureshi",
        "given_name": "Nawab Muhammad Faseeh"
      },
      {
        "surname": "Shin",
        "given_name": "Dong Ryeol"
      }
    ]
  },
  {
    "title": "Hypergraph matching via game-theoretic hypergraph clustering",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108526",
    "abstract": "Feature matching is used to build correspondences between features in the model and test images. As the extension of graph matching, hypergraph matching is able to encode rich invariance between feature tuples and improve matching accuracy. Different from many existing algorithms based on maximizing the matching score between correspondences, our approach formulates hypergraph matching as a non-cooperative multi-player game and obtains matches by extracting the evolutionary stable strategies (ESS). While this approach generates a high matching accuracy, the number of matches is usually small and it involves a large computation load to obtain more matches. To solve this problem, we extract multiple ESS clusters instead of one single ESS group, thereby transforming hypergraph matching of features to hypergraph clustering of candidate matches. By extracting an appropriate number of clusters, we increase the number of matches efficiently, and improve the matching accuracy by imposing the one-to-one constraint. In experiments with three real datasets, our algorithm is shown to generate a large number of matches efficiently. It also shows significant advantage in matching accuracy in comparison with some other hypergraph matching algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000073",
    "keywords": [
      "3-dimensional matching",
      "Artificial intelligence",
      "Blossom algorithm",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Constraint (computer-aided design)",
      "Discrete mathematics",
      "Geometry",
      "Hypergraph",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Statistics",
      "Tuple"
    ],
    "authors": [
      {
        "surname": "Hou",
        "given_name": "Jian"
      },
      {
        "surname": "Pelillo",
        "given_name": "Marcello"
      },
      {
        "surname": "Yuan",
        "given_name": "Huaqiang"
      }
    ]
  },
  {
    "title": "Text-to-image synthesis with self-supervised learning",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.010",
    "abstract": "Text-to-image synthesis extracts the meaning from the text description and converts it into an image correspondingly. Text-to-image synthesis is widely leveraged in many applications, such as graphic design, image editing, etc. Text-to-image synthesis approaches are mainly built on the basis of generative adversarial networks. One of the main challenges in text-to-image synthesis is to generate images that are visually realistic. Not only that, the text-to-image synthesis model is inherently susceptible to overconfidence and training instability issues. To address these challenges, this paper proposes a self-supervised text-to-image synthesis with some enhancements, including self-supervised learning, feature matching, L1 distance loss, and one-sided label smoothing. The self-supervised learning offers more image variations thus improving the classification power of the discriminator. The feature matching and L1 distance functions motivate the generator to synthesize images that are visually more similar to the real images based on the given text description. The one-sided label smoothing adds a penalty value when the discriminator makes a correct classification to alleviate the overconfidence problem and to improve the training stability. The performance of the proposed self-supervised text-to-image synthesis is evaluated on the Oxford-102 and CUB datasets. The empirical results demonstrate that the proposed self-supervised text-to-image synthesis generates images with richer image content diversity, more visually realistic, and more semantically consistent with the given text description. The proposed self-supervised text-to-image synthesis also outshines the methods in comparison in terms of the inception score and Structural Similarity Index.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001064",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Detector",
      "Discriminator",
      "Feature (linguistics)",
      "Generator (circuit theory)",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Smoothing",
      "Stability (learning theory)",
      "Supervised learning",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Tan",
        "given_name": "Yong Xuan"
      },
      {
        "surname": "Lee",
        "given_name": "Chin Poo"
      },
      {
        "surname": "Neo",
        "given_name": "Mai"
      },
      {
        "surname": "Lim",
        "given_name": "Kian Ming"
      }
    ]
  },
  {
    "title": "Entropy regularization for unsupervised clustering with adaptive neighbors",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108517",
    "abstract": "Graph-based clustering has been considered as an effective kind of method in unsupervised manner to partition various items into several groups, such as Spectral Clustering (SC). However, there are three species of drawbacks in SC: (1) The effects of clustering is sensitive to the affinity matrix that is fixed by original data. (2) The input affinity matrix is simply based on distance measurement, which lacks of clear physical meaning under probabilistic prediction. (3) Additional discretization procedures still need to be operated. To cope with these issues, we propose a new clustering model, which refers to Entropy Regularization for unsupervised Clustering with Adaptive Neighbors (ERCAN), to dynamically and simultaneously update affinity matrix and clustering results. Firstly, the maximized entropy regularization term is introduced in probability model to avoid trivial similarity distributions. Additionally, we newly introduce the Laplacian rank constraint with ℓ 0 -norm to construct adaptive neighbors for sparsity and strength segmentation ability without extra discretization process. Finally, we present a novel monotonic function optimization method, which reveals the consistence between graph sparsity and neighbor assignment, to address the ℓ 0 -norm constraint in alternative optimization process. Comprehensive experiments show the superiority of our method with promising results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006932",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Constrained clustering",
      "Entropy (arrow of time)",
      "Fuzzy clustering",
      "Graph",
      "Laplacian matrix",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Probabilistic logic",
      "Quantum mechanics",
      "Spectral clustering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jingyu"
      },
      {
        "surname": "Ma",
        "given_name": "Zhenyu"
      },
      {
        "surname": "Nie",
        "given_name": "Feiping"
      },
      {
        "surname": "Li",
        "given_name": "Xuelong"
      }
    ]
  },
  {
    "title": "Multiple change point clustering of count processes with application to California COVID data",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.03.025",
    "abstract": "In this paper, a model-based clustering algorithm relying on a finite mixture of negative binomial Lévy processes is proposed. The algorithm models heterogeneous stochastic count process data and automatically estimates multiple change points upon fitting the mixture model. Such change point estimation identifies time points when deviation from the standard process has occurred and serves as an important diagnostic tool for analyzing temporal data. The proposed model is applied to the COVID-positive ICU cases in the state of California with very interesting results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000903",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Change detection",
      "Cluster analysis",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Count data",
      "Counting process",
      "Data mining",
      "Disease",
      "Geometry",
      "Infectious disease (medical specialty)",
      "Mathematics",
      "Medicine",
      "Negative binomial distribution",
      "Operating system",
      "Pathology",
      "Point (geometry)",
      "Point process",
      "Poisson distribution",
      "Process (computing)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Sarkar",
        "given_name": "Shuchismita"
      },
      {
        "surname": "Zhu",
        "given_name": "Xuwen"
      }
    ]
  },
  {
    "title": "A bibliometric analysis of off-line handwritten document analysis literature (1990–2020)",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108513",
    "abstract": "Providing computers with the ability to process handwriting is both important and challenging, since many difficulties (e.g., different writing styles, alphabets, languages, etc.) need to be overcome for addressing a variety of problems (text recognition, signature verification, writer identification, word spotting, etc.). This paper reviews the growing literature on off-line handwritten document analysis over the last thirty years. A sample of 5389 articles is examined using bibliometric techniques. Using bibliometric techniques, this paper identifies (i) the most influential articles in the area, (ii) the most productive authors and their collaboration networks, (iii) the countries and institutions that have led research on the topic, (iv) the journals and conferences that have published most papers, and (v) the most relevant research topics (and their related tasks and methodologies) and their evolution over the years.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006890",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Data science",
      "Feature extraction",
      "Handwriting",
      "Handwriting recognition",
      "Identification (biology)",
      "Information retrieval",
      "Natural language processing",
      "Operating system",
      "Process (computing)",
      "Sample (material)",
      "Spotting",
      "Variety (cybernetics)"
    ],
    "authors": [
      {
        "surname": "Ruiz-Parrado",
        "given_name": "Victoria"
      },
      {
        "surname": "Heradio",
        "given_name": "Ruben"
      },
      {
        "surname": "Aranda-Escolastico",
        "given_name": "Ernesto"
      },
      {
        "surname": "Sánchez",
        "given_name": "Ángel"
      },
      {
        "surname": "Vélez",
        "given_name": "José F."
      }
    ]
  },
  {
    "title": "Mobile behavioral biometrics for passive authentication",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.03.014",
    "abstract": "Current mobile user authentication systems based on PIN codes, fingerprint, and face recognition have several shortcomings. Such limitations have been addressed in the literature by exploring the feasibility of passive authentication on mobile devices through behavioral biometrics. In this line of research, this work carries out a comparative analysis of unimodal and multimodal behavioral biometric traits acquired while the subjects perform different activities on the phone such as typing, scrolling, drawing a number, and tapping on the screen, considering the touchscreen and the simultaneous background sensor data (accelerometer, gravity sensor, gyroscope, linear accelerometer, and magnetometer). Our experiments are performed over HuMIdb, 1 1 https://github.com/BiDAlab/HuMIdb one of the largest and most comprehensive freely available mobile user interaction databases to date. A separate Recurrent Neural Network (RNN) with triplet loss is implemented for each single modality. Then, the weighted fusion of the different modalities is carried out at score level. In our experiments, the most discriminative background sensor is the magnetometer, whereas among touch tasks the best results are achieved with keystroke in a fixed-text scenario. In all cases, the fusion of modalities is very beneficial, leading to Equal Error Rates (EER) ranging from 4% to 9% depending on the modality combination in a 3-second interval.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552200071X",
    "keywords": [
      "Accelerometer",
      "Aerospace engineering",
      "Artificial intelligence",
      "Authentication (law)",
      "Biometrics",
      "Computer science",
      "Computer security",
      "Engineering",
      "Fingerprint (computing)",
      "Gyroscope",
      "Human–computer interaction",
      "Keystroke logging",
      "Mobile device",
      "Mobile phone",
      "Modalities",
      "Modality (human–computer interaction)",
      "Operating system",
      "Scrolling",
      "Sensor fusion",
      "Social science",
      "Sociology",
      "Speech recognition",
      "SwIPe",
      "Telecommunications",
      "Touchscreen",
      "Word error rate"
    ],
    "authors": [
      {
        "surname": "Stragapede",
        "given_name": "Giuseppe"
      },
      {
        "surname": "Vera-Rodriguez",
        "given_name": "Ruben"
      },
      {
        "surname": "Tolosana",
        "given_name": "Ruben"
      },
      {
        "surname": "Morales",
        "given_name": "Aythami"
      },
      {
        "surname": "Acien",
        "given_name": "Alejandro"
      },
      {
        "surname": "Le Lan",
        "given_name": "Gaël"
      }
    ]
  },
  {
    "title": "ADR-MVSNet: A cascade network for 3D point cloud reconstruction with pixel occlusion",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108516",
    "abstract": "3D point cloud reconstruction is an urgent task in computer vision for environment perception. Nevertheless, the reconstructed scene is inaccurate and incomplete, because the visibility of pixels is not taken into account by existing methods. In this paper, a cascaded network with a multiple cost volume aggregation module named ADR-MVSNet is proposed. Three improvements are presented in ADR-MVSNet. First, to improve the reconstruction accuracy and reduce the time complexity, an adaptive depth reduction module, which adaptively adjusts the depth range of the pixel through the confidence interval, is proposed. Second, to more accurately estimate the depth of occluded pixels in multiview images, a multiple cost volume aggregation module, in which Gini impurity is introduced to measure the confidence of pixel depth prediction, is proposed. Third, a multiscale photometric consistency filter module is proposed, which considers the information in multiple confidence maps at the same time and filters out outliers accurately to remove pixels with low confidence. Therefore, the accuracy of point cloud reconstruction is improved. The experimental results on the DTU and Tanks and Temple datasets demonstrate that ADR-MVSNet achieves highly accurate and highly complete reconstruction compared with state-of-the-art benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006920",
    "keywords": [
      "3D reconstruction",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Filter (signal processing)",
      "Iterative reconstruction",
      "Optics",
      "Outlier",
      "Physics",
      "Pixel",
      "Point cloud",
      "Quantum mechanics",
      "Visibility",
      "Volume (thermodynamics)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Ying"
      },
      {
        "surname": "Zhao",
        "given_name": "Zhijie"
      },
      {
        "surname": "Fan",
        "given_name": "Jiahao"
      },
      {
        "surname": "Li",
        "given_name": "Wenyue"
      }
    ]
  },
  {
    "title": "Greedy-layer pruning: Speeding up transformer models for natural language processing",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.03.023",
    "abstract": "Fine-tuning transformer models after unsupervised pre-training reaches a very high performance on many different natural language processing tasks. Unfortunately, transformers suffer from long inference times which greatly increases costs in production. One possible solution is to use knowledge distillation, which solves this problem by transferring information from large teacher models to smaller student models. Knowledge distillation maintains high performance and reaches high compression rates, nevertheless, the size of the student model is fixed after pre-training and can not be changed individually for a given downstream task and use-case to reach a desired performance/speedup ratio. Another solution to reduce the size of models in a much more fine-grained and computationally cheaper fashion is to prune layers after the pre-training. The price to pay is that the performance of layer-wise pruning algorithms is not on par with state-of-the-art knowledge distillation methods. In this paper, Greedy-layer pruning is introduced to (1) outperform current state-of-the-art for layer-wise pruning, (2) close the performance gap when compared to knowledge distillation, while (3) providing a method to adapt the model size dynamically to reach a desired performance/speedup tradeoff without the need of additional pre-training phases. Our source code is available on https://github.com/deepopinion/greedy-layer-pruning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000885",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Biology",
      "Chemistry",
      "Computer science",
      "Distillation",
      "Inference",
      "Language model",
      "Machine learning",
      "Organic chemistry",
      "Parallel computing",
      "Physics",
      "Pruning",
      "Quantum mechanics",
      "Speedup",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Peer",
        "given_name": "David"
      },
      {
        "surname": "Stabinger",
        "given_name": "Sebastian"
      },
      {
        "surname": "Engl",
        "given_name": "Stefan"
      },
      {
        "surname": "Rodríguez-Sánchez",
        "given_name": "Antonio"
      }
    ]
  },
  {
    "title": "KGBoost: A classification-based knowledge base completion method with negative sampling",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.001",
    "abstract": "Knowledge base completion is formulated as a binary classification problem in this work, where an XGBoost binary classifier is trained for each relation using relevant links in knowledge graphs (KGs). The new method, named KGBoost, adopts a modularized design and attempts to find hard negative samples so as to train a powerful classifier for missing link prediction. We conduct experiments on multiple benchmark datasets and demonstrate that KGBoost outperforms state-of-the-art methods across most datasets. Furthermore, as compared with models trained by end-to-end optimization, KGBoost works well under the low-dimensional setting so as to allow a smaller model size.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000939",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Base (topology)",
      "Benchmark (surveying)",
      "Binary classification",
      "Binary number",
      "Binary relation",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Discrete mathematics",
      "Geodesy",
      "Geography",
      "Knowledge base",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yun-Cheng"
      },
      {
        "surname": "Ge",
        "given_name": "Xiou"
      },
      {
        "surname": "Wang",
        "given_name": "Bin"
      },
      {
        "surname": "Kuo",
        "given_name": "C.-C. Jay"
      }
    ]
  },
  {
    "title": "VD-PCR: Improving visual dialog with pronoun coreference resolution",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108540",
    "abstract": "The visual dialog task requires an AI agent to interact with humans in multi-round dialogs based on a visual environment. As a common linguistic phenomenon, pronouns are often used in dialogs to improve the communication efficiency. As a result, resolving pronouns (i.e., grounding pronouns to the noun phrases they refer to) is an essential step towards understanding dialogs. In this paper, we propose VD-PCR, a novel framework to improve Visual Dialog understanding with Pronoun Coreference Resolution in both implicit and explicit ways. First, to implicitly help models understand pronouns, we design novel methods to perform the joint training of the pronoun coreference resolution and visual dialog tasks. Second, after observing that the coreference relationship of pronouns and their referents indicates the relevance between dialog rounds, we propose to explicitly prune the irrelevant history rounds in visual dialog models’ input. With pruned input, the models can focus on relevant dialog history and ignore the distraction in the irrelevant one. With the proposed implicit and explicit methods, VD-PCR achieves state-of-the-art experimental results on the VisDial dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000218",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Coreference",
      "Dialog box",
      "Linguistics",
      "Natural language processing",
      "Philosophy",
      "Pronoun",
      "Resolution (logic)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Xintong"
      },
      {
        "surname": "Zhang",
        "given_name": "Hongming"
      },
      {
        "surname": "Hong",
        "given_name": "Ruixin"
      },
      {
        "surname": "Song",
        "given_name": "Yangqiu"
      },
      {
        "surname": "Zhang",
        "given_name": "Changshui"
      }
    ]
  },
  {
    "title": "Graph label prediction based on local structure characteristics representation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108525",
    "abstract": "A recent study has shown that the real-time anti-noise challenges faced by molecular activity prediction algorithms can be solved by using the part structure features of the molecular graph. However, the sub-structures selected by this method are distributed in a scattered manner such that although they include as many block features as possible, they do not fully consider the connections between these blocks. Therefore, this study was conducted to fully consider the physical interpretation of the betweenness centrality node in the graph, and a sub-structure was obtained by depth-first search (DFS) from this node. This sub-structure not only contains the characteristics of each region but also retains the connections between each region. Then, a cascading multi-layer perception (MLP) model was designed to learn the characteristic representation of the graph from its local structure features. Experiments demonstrated that the performance of our algorithm is superior to that of other algorithms when evaluated on different datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000061",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Betweenness centrality",
      "Centrality",
      "Chemical physics",
      "Combinatorics",
      "Computer science",
      "Engineering",
      "Graph",
      "Law",
      "Local structure",
      "Mathematics",
      "Node (physics)",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Structural engineering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Ding",
        "given_name": "Jingyi"
      },
      {
        "surname": "Cheng",
        "given_name": "Ruohui"
      },
      {
        "surname": "Song",
        "given_name": "Jian"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiangrong"
      },
      {
        "surname": "Jiao",
        "given_name": "Licheng"
      },
      {
        "surname": "Wu",
        "given_name": "Jianshe"
      }
    ]
  },
  {
    "title": "ResT-ReID: Transformer block-based residual learning for person re-identification",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.03.020",
    "abstract": "The Transformer has been applied into computer vision to explore long-range dependencies with multi-head self-attention strategy, therefore numerous Transformer-based methods for person re-identification (ReID) are designed for extracting effective as well as robust representation. However, the memory and computational complexity of scaled dot-product attention in Transformer cost vast overheads. To overcome these limitations, this paper presents ResT-ReID method, which designs a hybrid backbone Res-Transformer based on ResNet-50 and Transformer block for effective identify information. Specifically, we use global self-attention in place of depth-wise convolution in the fourth layer’s residual bottleneck of ResNet-50. For fully exploiting the entire knowledge of the person, we devise attention-guided Graph Convolution Networks (GCNs) with side information embedding (SIE-AGCN), which has an attention layer located into two GCN layers. The quantified experiments on two large-scale ReID benchmarks demonstrate that the proposed ResT-ReID achieves competitive results compared with several state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552200085X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bottleneck",
      "Computer engineering",
      "Computer science",
      "Deep learning",
      "Dot product",
      "Electrical engineering",
      "Embedded system",
      "Embedding",
      "Engineering",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Residual",
      "Residual neural network",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Ying"
      },
      {
        "surname": "Xia",
        "given_name": "Shixiong"
      },
      {
        "surname": "Zhao",
        "given_name": "Jiaqi"
      },
      {
        "surname": "Zhou",
        "given_name": "Yong"
      },
      {
        "surname": "Niu",
        "given_name": "Qiang"
      },
      {
        "surname": "Yao",
        "given_name": "Rui"
      },
      {
        "surname": "Zhu",
        "given_name": "Dongjun"
      },
      {
        "surname": "Liu",
        "given_name": "Dongjingdian"
      }
    ]
  },
  {
    "title": "User-based network embedding for opinion spammer detection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108512",
    "abstract": "Due to the huge commercial interests behind online reviews, a tremendous amount of spammers manufacture spam reviews for product reputation manipulation. To further enhance the influence of spam reviews, spammers often collaboratively post spam reviews within a short period of time, the activities of whom are called collective opinion spam campaign. The goals and members of the spam campaign activities change frequently, and some spammers also imitate normal purchases to conceal the identity, which makes the spammer detection challenging. In this paper, we propose an unsupervised network embedding-based approach to jointly exploiting different types of relations, e.g., direct common behavior relation, and indirect co-reviewed relation to effectively represent the relevances of users for detecting the collective opinion spammers. The average improvements of our method over the state-of-the-art solutions on dataset AmazonCn and YelpHotel are [14.09%,12.04%] and [16.25%,12.78%] in terms of AP and AUC, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006889",
    "keywords": [
      "Acoustics",
      "Computer science",
      "Computer security",
      "Data mining",
      "Geometry",
      "Identity (music)",
      "Mathematics",
      "Physics",
      "Product (mathematics)",
      "Relation (database)",
      "Reputation",
      "Social science",
      "Sociology",
      "Spambot",
      "Spamming",
      "The Internet",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Ziyang"
      },
      {
        "surname": "Wei",
        "given_name": "Wei"
      },
      {
        "surname": "Mao",
        "given_name": "Xian-Ling"
      },
      {
        "surname": "Guo",
        "given_name": "Guibing"
      },
      {
        "surname": "Zhou",
        "given_name": "Pan"
      },
      {
        "surname": "Jiang",
        "given_name": "Sheng"
      }
    ]
  },
  {
    "title": "Phase retrieval from incomplete data via weighted nuclear norm minimization",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108537",
    "abstract": "Recovering an unknown object from the magnitude of its Fourier transform is a phase retrieval problem. Here, we consider a much difficult case, where those observed intensity values are incomplete and contaminated by both salt-and-pepper and random-valued impulse noise. To take advantage of the low-rank property within the image of the object, we use a regularization term which penalizes high weighted nuclear norm values of image patch groups. For outliers (impulse noise) in the observation, the ℓ 1 − 2 metric is adopted as the data fidelity term. Then we break down the resulting optimization problem into smaller ones, for example, weighted nuclear norm proximal mapping and ℓ 1 − 2 minimization, because the nonconvex and nonsmooth subproblems have available closed-form solutions. The convergence results are also presented, and numerical experiments are provided to demonstrate the superior reconstruction quality of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000188",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Eigenvalues and eigenvectors",
      "Fourier transform",
      "Impulse noise",
      "Law",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Matrix norm",
      "Minification",
      "Norm (philosophy)",
      "Optimization problem",
      "Outlier",
      "Phase retrieval",
      "Physics",
      "Pixel",
      "Political science",
      "Quantum mechanics",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Zhi"
      },
      {
        "surname": "Yan",
        "given_name": "Ming"
      },
      {
        "surname": "Zeng",
        "given_name": "Tieyong"
      },
      {
        "surname": "Zhang",
        "given_name": "Guixu"
      }
    ]
  },
  {
    "title": "BADet: Boundary-Aware 3D Object Detection from Point Clouds",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108524",
    "abstract": "Currently, existing state-of-the-art 3D object detectors are in two-stage paradigm. These methods typically comprise two steps: 1) Utilize a region proposal network to propose a handful of high-quality proposals in a bottom-up fashion. 2) Resize and pool the semantic features from the proposed regions to summarize RoI-wise representations for further refinement. Note that these RoI-wise representations in step 2) are considered individually as uncorrelated entries when fed to following detection headers. Nevertheless, we observe these proposals generated by step 1) offset from ground truth somehow, emerging in local neighborhood densely with an underlying probability. Challenges arise in the case where a proposal largely forsakes its boundary information due to coordinate offset while existing networks lack corresponding information compensation mechanism. In this paper, we propose BADet for 3D object detection from point clouds. Specifically, instead of refining each proposal independently as previous works do, we represent each proposal as a node for graph construction within a given cut-off threshold, associating proposals in the form of local neighborhood graph, with boundary correlations of an object being explicitly exploited. Besides, we devise a lightweight Region Feature Aggregation Module to fully exploit voxel-wise, pixel-wise, and point-wise features with expanding receptive fields for more informative RoI-wise representations. We validate BADet both on widely used KITTI Dataset and highly challenging nuScenes Dataset. As of Apr. 17th, 2021, our BADet achieves on par performance on KITTI 3D detection leaderboard and ranks 1 st on M o d e r a t e difficulty of C a r category on KITTI BEV detection leaderboard. The source code is available at https://github.com/rui-qian/BADet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200005X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Exploit",
      "Graph",
      "Ground truth",
      "Object (grammar)",
      "Object detection",
      "Offset (computer science)",
      "Pattern recognition (psychology)",
      "Pixel",
      "Point cloud",
      "Programming language",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Qian",
        "given_name": "Rui"
      },
      {
        "surname": "Lai",
        "given_name": "Xin"
      },
      {
        "surname": "Li",
        "given_name": "Xirong"
      }
    ]
  },
  {
    "title": "Unsupervised person re-identification with multi-label learning guided self-paced clustering",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108521",
    "abstract": "Although unsupervised person re-identification (Re-ID) has drawn increasing research attention recently, it remains challenging to learn discriminative features without annotations across disjoint camera views. In this paper, we address the unsupervised person Re-ID with a conceptually novel yet simple framework, termed as Multi-label Learning guided self-paced Clustering (MLC). MLC mainly learns discriminative features with three crucial modules, namely a multi-scale network, a multi-label learning module, and a self-paced clustering module. Specifically, the multi-scale network generates multi-granularity person features in both global and local views. The multi-label learning module leverages a memory feature bank and assigns each image with a multi-label vector based on the similarities between the image and feature bank. After multi-label training for several epochs, the self-paced clustering joins in training and assigns a pseudo label for each image. The benefits of our MLC come from three aspects: i) the multi-scale person features for better similarity measurement, ii) the multi-label assignment based on the whole dataset ensures that every image can be trained, and iii) the self-paced clustering removes some noisy samples for better feature learning. Extensive experiments on three popular large-scale Re-ID benchmarks demonstrate that our MLC outperforms previous state-of-the-art methods and significantly improves the performance of unsupervised person Re-ID.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000024",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Cluster analysis",
      "Computer science",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature learning",
      "Identification (biology)",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Similarity (geometry)",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Qing"
      },
      {
        "surname": "Peng",
        "given_name": "Xiaojiang"
      },
      {
        "surname": "Qiao",
        "given_name": "Yu"
      },
      {
        "surname": "Hao",
        "given_name": "Qi"
      }
    ]
  },
  {
    "title": "IDeAuth: A novel behavioral biometric-based implicit deauthentication scheme for smartphones",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.03.011",
    "abstract": "Many studies have shown that single entry-point authentication schemes for smartphones can easily be circumvented. IDeAuth is an implicit deauthentication scheme that aims to minimize unauthorized access to security-sensitive applications and services running on users’ smartphones when unauthorized access or intrusions are detected. IDeAuth verifies legitimate owners of their smartphones by exploiting their micro hand-movements and decides to sign off the default user account revoking security-sensitive applications and services linked with it. We design and develop an Android-based prototype application as a proof-of-concept and collect a new dataset consisting of 21263 observations from 41 users in a real scenario. The user verification process employs four different one-class classifiers (OCCs), which is evaluated on the collected dataset using the holdout test method. IDeAuth achieves a Half Total Error Rate (HTER) of ≈ 4 % after applying a decision-level-fusion enhancing the best individual classifier’s performance by ≈ 1 % .",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000770",
    "keywords": [
      "Android (operating system)",
      "Artificial intelligence",
      "Biometrics",
      "Classifier (UML)",
      "Computer science",
      "Computer security",
      "Data mining",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Scheme (mathematics)"
    ],
    "authors": [
      {
        "surname": "Gupta",
        "given_name": "Sandeep"
      },
      {
        "surname": "Kumar",
        "given_name": "Rajesh"
      },
      {
        "surname": "Kacimi",
        "given_name": "Mouna"
      },
      {
        "surname": "Crispo",
        "given_name": "Bruno"
      }
    ]
  },
  {
    "title": "Hyper-graph-based attention curriculum learning using a lexical algorithm for mental health",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.03.018",
    "abstract": "In this paper, we propose a structure hypergraph and an emotional lexicon for word representation. Our method can solve problems related to vocabulary size, grammatical representation of words, and the lack of an emotional lexicon. Natural Language Processing (NLP) and attention-based curriculum learning are then used in the developed model. The goal is to achieve semantic word representations using a graph model. Later, embedding is used to label the text using clinical procedures. The experimental results show the emotional word representation with the structure hypergraph. The bidirectional Long Short Term Memory (LSTM) architecture with an attention mechanism achieved a Receiver Operating Characteristic (ROC) value of 0.96. The learning method can help psychiatrists in note taking and contributes to the detection rate of depression symptoms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000836",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Discrete mathematics",
      "Embedding",
      "Graph",
      "Hypergraph",
      "Law",
      "Lexicon",
      "Linguistics",
      "Mathematics",
      "Mental lexicon",
      "Natural language processing",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Theoretical computer science",
      "Vocabulary",
      "Word embedding"
    ],
    "authors": [
      {
        "surname": "Ahmed",
        "given_name": "Usman"
      },
      {
        "surname": "Lin",
        "given_name": "Jerry Chun-Wei"
      },
      {
        "surname": "Srivastava",
        "given_name": "Gautam"
      }
    ]
  },
  {
    "title": "SynCoLFinGer: Synthetic contactless fingerprint generator",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.003",
    "abstract": "We present the first method for synthetic generation of contactless fingerprint images, referred to as SynCoLFinGer. To this end, the constituent components of contactless fingerprint images regarding capturing, subject characteristics, and environmental influences are modeled and applied to a synthetically generated ridge pattern using the SFinGe algorithm. The proposed method is able to generate different synthetic samples corresponding to a single finger and it can be parameterized to generate contactless fingerprint images of various quality levels. The resemblance of the synthetically generated contactless fingerprints to real fingerprints is confirmed by evaluating biometric sample quality using an adapted NFIQ 2.0 algorithm and biometric utility using a state-of-the-art contactless fingerprint recognition system.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000915",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biometrics",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Computer vision",
      "Fingerprint (computing)",
      "Fingerprint Verification Competition",
      "Fingerprint recognition",
      "Generator (circuit theory)",
      "Parameterized complexity",
      "Pattern recognition (psychology)",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Sample (material)"
    ],
    "authors": [
      {
        "surname": "Priesnitz",
        "given_name": "Jannis"
      },
      {
        "surname": "Rathgeb",
        "given_name": "Christian"
      },
      {
        "surname": "Buchmann",
        "given_name": "Nicolas"
      },
      {
        "surname": "Busch",
        "given_name": "Christoph"
      }
    ]
  },
  {
    "title": "Reduced annotation based on deep active learning for arabic text detection in natural scene images",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.03.016",
    "abstract": "Providing labeled Arabic text images dataset for scene text detection is inherently difficult and costly at the same time. Consequently, only few small datasets are available for this task. Previous work has only focused on the data augmentation technique of small datasets; however, the images generated with these techniques cannot reproduce the complexity and variability of natural images. In this paper, we propose a new Arabic text images dataset using the Google Street View service named Tunisia Street View Dataset (TSVD). The dataset contains 7k images collected from different Tunisian cities. It is much more diverse and complex than current image datasets. Taking advantage of this dataset to train Convolutional Neural Network (CNN) models, annotation is required for building high performance models. The annotation task consumes a lot of time and effort for researchers due to its repetitiveness. The development time of text detection systems in natural images is valuable with an effective use. We believe that we have developed a Deep Active Learning algorithm for the annotation phase. A Deep Active Learning algorithm for the annotation phase has been developed by approaching the annotation suggestion task using a deep learning text detector. CNN are used to perform the text detection in natural scene images. Our deep active learning framework combines CNN and active learning approach. This reduces annotation effort by making pertinent suggestions on the most effective annotation areas. We utilize uncertainty provided by CNN models to determine the maximum uncertain areas for annotation. Deep active learning is shown in order to reduce significantly the number of training samples required and also to minimize the annotation work of our dataset up to 1/5. Our dataset is publicly available in IEEE DataPort https://dx.doi.org/10.21227/extw-0k60.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000800",
    "keywords": [
      "Active learning (machine learning)",
      "Annotation",
      "Artificial intelligence",
      "Automatic image annotation",
      "Comprehension approach",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Economics",
      "Image (mathematics)",
      "Image retrieval",
      "Language technology",
      "Machine learning",
      "Management",
      "Natural language",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Task (project management)",
      "Temporal annotation"
    ],
    "authors": [
      {
        "surname": "Boukthir",
        "given_name": "Khalil"
      },
      {
        "surname": "Qahtani",
        "given_name": "Abdulrahman M."
      },
      {
        "surname": "Almutiry",
        "given_name": "Omar"
      },
      {
        "surname": "Dhahri",
        "given_name": "Habib"
      },
      {
        "surname": "Alimi",
        "given_name": "Adel M."
      }
    ]
  },
  {
    "title": "Auto uning of price prediction models for high-frequency trading via reinforcement learning",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108543",
    "abstract": "In this paper, we propose an online model optimization algorithm based on reinforcement learning for quantitative trading. The combination of prediction model and trading policy is the most commonly used framework in practical quantitative trading. Integrated with machine learning methods, this framework brings huge profits to quantified companies. In the framework, the prediction model is used to predict future trading price trend, and the trading policy is used to determine the price and number of orders. Even though, the shortcomings of machine learning models are obvious, mainly are, (1) Slow prediction speed. Huge human-craft features and model computing cost much time, which is ten times of pure trading policy without model. (2) Poor generalization. This kind of models can hardly adapt to market data in each period, because market traders will change time to time at micro level, thus the distribution of market data will change. But current model is trained on a long period dataset, it achieves best effect at average, but can not adapt to different market at each period. To address this problem, we propose a novel online model optimization algorithm. A light model library will be constructed. Each light model in this library corresponds to a different market distribution. By devising the appropriate reward function via inverse reinforcement learning algorithm, the algorithm can accurately estimate the profits of each model. Then the model can be selected automatically in real-time trading, so that the trading policies can automatically adapt to changes in trading market, overcoming previous shortcoming of manually updating model and slow prediction speed. Experimental results show that the proposed algorithm achieves state-of-the-art performance on China Commodity Futures Market Data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000243",
    "keywords": [
      "Algorithmic trading",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Econometrics",
      "Economics",
      "Evolutionary biology",
      "Finance",
      "Function (biology)",
      "Generalization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Reinforcement learning",
      "Trading strategy"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Weipeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Ning"
      },
      {
        "surname": "Yan",
        "given_name": "Junchi"
      },
      {
        "surname": "Li",
        "given_name": "Guofu"
      },
      {
        "surname": "Yang",
        "given_name": "Xiaokang"
      }
    ]
  },
  {
    "title": "Cloud security based attack detection using transductive learning integrated with Hidden Markov Model",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.02.012",
    "abstract": "In recent years, organizations and enterprises put huge attention on their network security. The attackers were able to influence vulnerabilities for the configuration of the network through the network. Zero-day (0-day) is defined as vulnerable software or application that is either defined by the vendor or not patched by any vendor of organization. When zero-day attack is identified within the network there is no proper mechanism when observed. To mitigate challenges related to the zero-day attack, this paper presented HMM_TDL, a deep learning model for detection and prevention of attack in the cloud platform. The presented model is carried out in three phases like at first, Hidden Markov Model (HMM) is incorporated for the detection of attacks. With the derived HMM model, hyper alerts are transmitted to the database for attack prevention. In the second stage, a transductive deep learning model with k-medoids clustering is adopted for attack identification. With k-medoids clustering, soft labels are assigned for attack and data and update to the database. In the last phase, with computed HMM_TDL database is updated with computed trust value for attack prevention within the cloud.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000563",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Business",
      "Cloud computing",
      "Cluster analysis",
      "Computer science",
      "Computer security",
      "Data mining",
      "Hidden Markov model",
      "Identification (biology)",
      "Machine learning",
      "Marketing",
      "Operating system",
      "Vendor"
    ],
    "authors": [
      {
        "surname": "Aoudni",
        "given_name": "Yassine"
      },
      {
        "surname": "Donald",
        "given_name": "Cecil"
      },
      {
        "surname": "Farouk",
        "given_name": "Ahmed"
      },
      {
        "surname": "Sahay",
        "given_name": "Kishan Bhushan"
      },
      {
        "surname": "Babu",
        "given_name": "D. Vijendra"
      },
      {
        "surname": "Tripathi",
        "given_name": "Vikas"
      },
      {
        "surname": "Dhabliya",
        "given_name": "Dharmesh"
      }
    ]
  },
  {
    "title": "Joint image denoising with gradient direction and edge-preserving regularization",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108506",
    "abstract": "Joint image denoising algorithms use the structures of the guidance image as a prior to restore the noisy target image. While the provided guidance images are helpful to improve the denoising performance, the denoised edges are most likely to be blurred especially when the edges of the guidance image are weak or inexistent. To address this weakness, this paper proposes a new gradient-direction-based joint image denoising method in which the absolute cosine value of the angle between two gradient vectors of the guidance image and those of the image to recover is employed as the parallel measurement to ensure that the gradient directions of the denoised image are approximately the same as or opposite to those of the guidance image. Besides, a new edge-preserving regularization term is developed to alleviate the effects of the unreliable prior information from guidance image. To simplify the resultant complex nonconvex and nonlinear fractional model, the logarithm function is employed to convert the multiplication operation into addition operation. Then, we construct the surrogate function for the logarithmic term of l 2 -norm, and separate the variables to transform the objective function into convex one with high numerical stability while retaining high efficiency. Finally, the optimal solutions can be obtained by directly minimizing the convex functions. Experimental results on public datasets and from nine benchmark methods consistently demonstrate the effectiveness of the proposed method both visually and quantitatively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006828",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Deblurring",
      "Directional derivative",
      "Edge detection",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Image gradient",
      "Image processing",
      "Image restoration",
      "Law",
      "Logarithm",
      "Mathematical analysis",
      "Mathematics",
      "Noise reduction",
      "Norm (philosophy)",
      "Political science",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Pengliang"
      },
      {
        "surname": "Liang",
        "given_name": "Junli"
      },
      {
        "surname": "Zhang",
        "given_name": "Miaohua"
      },
      {
        "surname": "Fan",
        "given_name": "Wen"
      },
      {
        "surname": "Yu",
        "given_name": "Guoyang"
      }
    ]
  },
  {
    "title": "Utilizing differential characteristics of high dimensional data as a mechanism for dimensionality reduction",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.03.015",
    "abstract": "Dimensionality reduction and visualization of high dimensional data are fundamental tasks across scientific disciplines. Current dimensionality reduction methods rely on the use of some restrictive mathematical rules in projecting the data onto lower dimensions, which often leads to results of limited accuracy. Here, we present a generally applicable strategy of analyzing high-dimensional data by leveraging the differential characteristics of the data with respect to a reference data set. Depending on the problem, the reference data can be obtained either through an independent measurement or from the data themselves by finding a set of representative data points. The differentiating characteristics between the two sets of data are optimized by correlation transformation and scaling of the covariance matrices. We show the superior performance of the proposed method over existing techniques for a variety of data sets from different biomedical applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000812",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Computer science",
      "Covariance",
      "Curse of dimensionality",
      "Data mining",
      "Data point",
      "Data reduction",
      "Data set",
      "Data transformation",
      "Data warehouse",
      "Dimensionality reduction",
      "Gene",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Multidimensional scaling",
      "Programming language",
      "Reduction (mathematics)",
      "Scaling",
      "Set (abstract data type)",
      "Statistics",
      "Transformation (genetics)",
      "Variety (cybernetics)",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Xing",
        "given_name": "Samuel S."
      },
      {
        "surname": "Islam",
        "given_name": "Md Tauhidul"
      }
    ]
  },
  {
    "title": "Learning upper patch attention using dual-branch training strategy for masked face recognition",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108522",
    "abstract": "In the context of pandemic, COVID-19, recognition of masked face images is a challenging problem, as most of the facial components become invisible. By utilizing prior information that mask-occlusion is located in the lower half of the face, we propose a dual-branch training strategy to guide the model to focus on the upper half of the face to extract robust features for Masked face recognition (MFR). During training, the features learned at the intermediate layers of the global branch are fed to our proposed attention module, named Upper Patch Attention (UPA), which acts as a local branch. Both branches are jointly optimized to enhance the feature extraction from non-occluded regions. We also propose a self-attention module, which integrates into the backbone network to enhance the interaction between the channels and spatial locations in the learning process. Extensive experiments on synthetic and real-masked face datasets demonstrate the effectiveness of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000036",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Dual (grammatical number)",
      "Face (sociological concept)",
      "Facial recognition system",
      "Feature (linguistics)",
      "Feature extraction",
      "Focus (optics)",
      "Linguistics",
      "Literature",
      "Operating system",
      "Optics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Process (computing)",
      "Social science",
      "Sociology",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yuxuan"
      },
      {
        "surname": "Wang",
        "given_name": "Xin"
      },
      {
        "surname": "Shakeel",
        "given_name": "M. Saad"
      },
      {
        "surname": "Wan",
        "given_name": "Hao"
      },
      {
        "surname": "Kang",
        "given_name": "Wenxiong"
      }
    ]
  },
  {
    "title": "Improving query expansion using pseudo-relevant web knowledge for information retrieval",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.013",
    "abstract": "In the field of information retrieval, query expansion (QE) has long been used as a technique to deal with the fundamental issue of word mismatch between a user’s query and the target information. In the context of the relationship between the query and expanded terms, existing weighting techniques often fail to appropriately capture the term-term relationship and term to the whole query relationship, resulting in low retrieval effectiveness. Our proposed QE approach addresses this by proposing three weighting models based on (1) tf-idf, (2) k-nearest neighbor (kNN) based cosine similarity, and (3) correlation score. Further, to extract the initial set of expanded terms, we use pseudo-relevant web knowledge consisting of the top N web pages returned by the three popular search engines namely, Google, Bing, and DuckDuckGo, in response to the original query. Among the three weighting models, tf-idf scores each of the individual terms obtained from the web content, kNN-based cosine similarity scores the expansion terms to obtain the term-term relationship, and correlation score weighs the selected expansion terms with respect to the whole query. The proposed model, called web knowledge based query expansion (WKQE), achieves an improvement of 25.89% on the Mean Average Precision (MAP) score and 30.83% on the Geometric Mean Average precision (GMAP) score over the unexpanded queries on the FIRE dataset. A comparative analysis of the WKQE techniques with other related approaches clearly shows significant improvement in the retrieval performance. We have also analyzed the effect of varying the number of pseudo-relevant documents and expansion terms on the retrieval effectiveness of the proposed model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001088",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Cosine similarity",
      "Data mining",
      "Field (mathematics)",
      "Image (mathematics)",
      "Information retrieval",
      "Mathematics",
      "Medicine",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Pure mathematics",
      "Quantum mechanics",
      "Query expansion",
      "Query optimization",
      "Radiology",
      "Sargable",
      "Search engine",
      "Set (abstract data type)",
      "Similarity (geometry)",
      "Term (time)",
      "Web query classification",
      "Web search query",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Azad",
        "given_name": "Hiteshwar Kumar"
      },
      {
        "surname": "Deepak",
        "given_name": "Akshay"
      },
      {
        "surname": "Chakraborty",
        "given_name": "Chinmay"
      },
      {
        "surname": "Abhishek",
        "given_name": "Kumar"
      }
    ]
  },
  {
    "title": "Touch keystroke dynamics for demographic classification",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.023",
    "abstract": "Soft biometric traits are not fully distinctive in recognition tasks, but they can effectively added to biometric recognition systems to improve the overall performance. In this work, the focus is on the analysis of touch keystroke dynamics of smartphone’s users for demographic classification in age, gender and user experience. Starting from the data collected in three publicly available datasets and using traditional lightweight machine learning classification algorithms, the results reported in this work shows that an effective demographic analysis can be achieved as well as continuous authentication could be improved. Moreover, the study emphasize a critical issue affecting the experimental protocols in soft biometric analysis, discussing how sensibly the performance of a system can increase on a not wise splitting of the samples in the datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001222",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Authentication (law)",
      "Biometrics",
      "Computer science",
      "Computer security",
      "Dynamics (music)",
      "Focus (optics)",
      "Human–computer interaction",
      "Keystroke dynamics",
      "Keystroke logging",
      "Machine learning",
      "Optics",
      "Password",
      "Physics",
      "S/KEY"
    ],
    "authors": [
      {
        "surname": "Cascone",
        "given_name": "Lucia"
      },
      {
        "surname": "Nappi",
        "given_name": "Michele"
      },
      {
        "surname": "Narducci",
        "given_name": "Fabio"
      },
      {
        "surname": "Pero",
        "given_name": "Chiara"
      }
    ]
  },
  {
    "title": "Deep attention aware feature learning for person re-Identification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108567",
    "abstract": "Visual attention has proven to be effective in improving the performance of person re-identification. Most existing methods apply visual attention heuristically by learning an additional attention map to re-weight the feature maps for person re-identification, however, this kind of methods inevitably increase the model complexity and inference time. In this paper, we propose to incorporate the ability of predicting attention maps as additional objectives in a person ReID network without changing the original structure, thus maintain the same inference time and model size. Two kinds of attention maps have been considered to make the learned feature maps being aware of the person and related body parts respectively. Globally, a holistic attention branch (HAB) is proposed to make the feature maps obtained by backbone could focus on persons so as to alleviate the influence of background. Locally, a partial attention branch (PAB) is proposed to make the extracted features can be decoupled into several groups that are separately responsible for different body parts, thus increasing the robustness to pose variation and partial occlusion. These two kinds of attentions are universal and can be incorporated into existing ReID networks. We have tested its performance on two typical networks (TriNet [1] and Bag of Tricks [2]) and observed significant performance improvement on five widely used datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000486",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Botany",
      "Chemistry",
      "Computer science",
      "Feature (linguistics)",
      "Focus (optics)",
      "Gene",
      "Identification (biology)",
      "Inference",
      "Linguistics",
      "Machine learning",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yifan"
      },
      {
        "surname": "Wang",
        "given_name": "Han"
      },
      {
        "surname": "Sun",
        "given_name": "Xiaolu"
      },
      {
        "surname": "Fan",
        "given_name": "Bin"
      },
      {
        "surname": "Tang",
        "given_name": "Chu"
      },
      {
        "surname": "Zeng",
        "given_name": "Hui"
      }
    ]
  },
  {
    "title": "Hierarchical electricity time series prediction with cluster analysis and sparse penalty",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108555",
    "abstract": "In big data applications, hierarchical time series prediction is an important element of decision-making and concerns the inherent aggregation consistency, which is maintained by reconciliation methods. The paper proposes a novel multiple alternative clustering time series analysis based hierarchical electricity time series prediction method. Instead of adhering the aggregation consistency passively, we first exploit time series mining to construct a hierarchy, and then apply an optimal reconciliation method to improve the prediction accuracy. In particular, k -means clustering method is employed to cluster time series for many times with different k so as to make a large number of time series clusters (patterns), and then the clusters (patterns) based hierarchies are constructed respectively. With the large number of clusters hierarchies and the original geographical hierarchy, an optimal aggregation consistency reconciliation based prediction approach is proposed. Furthermore, the sparse penalty is adapted in our method for “ideal” clusters selection to improve the prediction performance. Compared with the state-of-the-art methods on real-life datasets, our method achieves the improvement of 11.13 % and 24.07 % accurate one-step ahead forecasts on electricity load and solar power data respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200036X",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Cluster (spacecraft)",
      "Computer science",
      "Data mining",
      "Machine learning",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Programming language",
      "Series (stratigraphy)",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Pang",
        "given_name": "Yue"
      },
      {
        "surname": "Zhou",
        "given_name": "Xiangdong"
      },
      {
        "surname": "Zhang",
        "given_name": "Junqi"
      },
      {
        "surname": "Sun",
        "given_name": "Quan"
      },
      {
        "surname": "Zheng",
        "given_name": "Jianbin"
      }
    ]
  },
  {
    "title": "Deep face recognition for dim images",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108580",
    "abstract": "The performance of many state-of-the-art deep face recognition models deteriorates significantly for images captured under low illumination, mainly because the features of dim probe face images cannot match well with those of normal-illumination gallery images. The issue cannot be satisfactorily addressed by enhancing the illumination of face images and performing face recognition on the resulted images alone. We propose a novel deep face recognition framework that consists of a feature restoration network, a feature extraction network, and an embedding matching module. The feature restoration network adopts a two-branch structure based on the convolutional neural network to generate a feature image from the raw image and the illumination-enhanced image. The feature extraction network encodes the feature image into an embedding, which is then used by the embedding matching module for face verification and identification. The overall verification accuracy is improved from 1.1% to 6.7% when tested on the Specs on Faces (SoF) dataset. For face identification, the rank-1 identification accuracy is improved by 2.8%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000619",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Embedding",
      "Face (sociological concept)",
      "Face detection",
      "Facial recognition system",
      "Feature (linguistics)",
      "Feature extraction",
      "Identification (biology)",
      "Image (mathematics)",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Social science",
      "Sociology",
      "Statistics",
      "Three-dimensional face recognition"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Yu-Hsuan"
      },
      {
        "surname": "Chen",
        "given_name": "Homer H."
      }
    ]
  },
  {
    "title": "Energy minimization for image focus volume in shape from focus",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108559",
    "abstract": "In shape from focus (SFF) methods, the quality of depth map is mainly dependent on the accuracy level of image focus volume. Most of the SFF techniques optimize focus volume without incorporating any prior or additional structural information about the scene and thus resultant depth maps are deteriorated. We mitigate this deficiency by proposing to optimize focus volume through energy minimization. The proposed energy function contains smoothness and structural similarity along with data term. Smoothness constraint enforces spatial coherence while structural similarity constraint tries to preserve structures which are consistent with image sequence. This results in an optimized focus volume that imitates the underlying scene accurately. For the implementation of our 3D objective function, we employ an efficient technique that decomposes the problem into a sequence of 1D simple sub-problems. Experiments conducted on synthetic and real image sequences from a variety of datasets demonstrate that the proposed method optimizes the focus volume effectively and thus provides improved depth maps.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000401",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Chemistry",
      "Coherence (philosophical gambling strategy)",
      "Computational chemistry",
      "Computer science",
      "Computer vision",
      "Constraint (computer-aided design)",
      "Energy (signal processing)",
      "Energy minimization",
      "Evolutionary biology",
      "Focus (optics)",
      "Function (biology)",
      "Geometry",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Minification",
      "Optics",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Similarity (geometry)",
      "Smoothness",
      "Statistics",
      "Volume (thermodynamics)"
    ],
    "authors": [
      {
        "surname": "Ali",
        "given_name": "Usman"
      },
      {
        "surname": "Mahmood",
        "given_name": "Muhammad Tariq"
      }
    ]
  },
  {
    "title": "GAN for vision, KG for relation: A two-stage network for zero-shot action recognition",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108563",
    "abstract": "Zero-shot action recognition can recognize samples of unseen classes that are unavailable in training by exploring common latent semantic representation in samples. However, most methods neglected the connotative relation and extensional relation between the action classes, which leads to the poor generalization ability of the zero-shot learning. Furthermore, the learned classifier inclines to predict the samples of seen class, which leads to poor classification performance. To solve the above problems, we propose a two-stage deep neural network for zero-shot action recognition, which consists of a feature generation sub-network serving as the sampling stage and a graph attention sub-network serving as the classification stage. In the sampling stage, we utilize generative adversarial networks (GAN) trained by action features and word vectors of seen classes to synthesize the action features of unseen classes, which can balance the training sample data of seen classes and unseen classes. In the classification stage, we construct a knowledge graph (KG) based on the relationship between word vectors of action classes and related objects, and propose a graph convolution network (GCN) based on attention mechanism, which dynamically updates the relationship between action classes and objects, and enhances the generalization ability of zero-shot learning. In both stages, we all use word vectors as bridges for feature generation and classifier generalization from seen classes to unseen classes. We compare our method with state-of-the-art methods on UCF101 and HMDB51 datasets. Experimental results show that our proposed method improves the classification performance of the trained classifier and achieves higher accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000449",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Feature vector",
      "Generalization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Bin"
      },
      {
        "surname": "Kong",
        "given_name": "Dehui"
      },
      {
        "surname": "Wang",
        "given_name": "Shaofan"
      },
      {
        "surname": "Li",
        "given_name": "Jinghua"
      },
      {
        "surname": "Yin",
        "given_name": "Baocai"
      },
      {
        "surname": "Luo",
        "given_name": "Xiaonan"
      }
    ]
  },
  {
    "title": "Prediction with expert advice for a finite number of experts: A practical introduction",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108557",
    "abstract": "In this paper, prediction with expert advice is surveyed focusing on Vovk’s Aggregating Algorithm. The established theory as well as extensions developed in the recent decade are considered. The paper is aimed at practitioners and covers important application scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000383",
    "keywords": [
      "Advice (programming)",
      "Artificial intelligence",
      "Computer science",
      "Data science",
      "Engineering",
      "Machine learning",
      "Management science",
      "Operations research",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Kalnishkan",
        "given_name": "Yuri"
      }
    ]
  },
  {
    "title": "Missing Data Imputation via Conditional Generator and Correlation Learning for Multimodal Brain Tumor Segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.019",
    "abstract": "Brain tumor is one of the most high-risk cancers which causes the 5-year survival rate of only about 36%. Accurate diagnosis of brain tumor is critical for the treatment planning. However, it’s common to missing one modality in clinical scenarios. In this paper, we propose a novel brain tumor segmentation network to impute the missing data. The proposed network consists of a conditional generator, a multi-source correlation network and a segmentation network. To impute the missing data, we propose to use a conditional generator to generate the missing modality under the condition of the available modalities. As the multi MR modalities have a strong relationship in tumor regions, we design a multi-source correlation network to learn the multi-source correlation. On the one hand, the multi-source correlation network can help the conditional generator to generate the missing modality which should keep the consistent correlation with the available modalities. On the other hand, it can guide the segmentation network to learn the correlated feature representations to improve the segmentation performance. The experiments evaluated on BraTS 2018 dataset demonstrate the superior performance of the proposed method when compared with the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001155",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Correlation",
      "Generator (circuit theory)",
      "Geometry",
      "Imputation (statistics)",
      "Machine learning",
      "Mathematics",
      "Missing data",
      "Pattern recognition (psychology)",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Tongxue"
      },
      {
        "surname": "Vera",
        "given_name": "Pierre"
      },
      {
        "surname": "Canu",
        "given_name": "Stéphane"
      },
      {
        "surname": "Ruan",
        "given_name": "Su"
      }
    ]
  },
  {
    "title": "Super-encoder with cooperative autoencoder networks",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108562",
    "abstract": "Dimensionality reduction plays a crucial role in classification, object detection, and pattern recognition tasks. Its main objective is to decrease the dimension of the original data while retaining the most distinctive information. With the emergence of deep learning, an autoencoder has become a state-of-the-art non-linear dimensionality-reduction method. Nonetheless, as the existing autoencoder models are devised to follow the data distribution and employ similarity techniques, preserving distinctive information can be problematic. To tackle this issue, we propose super-encoder (SE) networks trained in a supervised and cooperative manner. The SE consists of an encoder, separator, and decoder networks. The encoder combined with separator networks are dedicated to generating separable latent representation based on the label, and the decoder network should be able to reconstruct it to the original data simultaneously. Herein, we introduce a novel cooperative learning mechanism with a new loss function; therefore, the encoder, separator, and decoder networks can cooperate to achieve these objectives. Extensive experiments using benchmark datasets were conducted. The results indicated that the SE is more effective in extracting separable latent code than the existing supervised and unsupervised dimensionality-reduction models. Furthermore, as a generator, it can obtain highly competitive realistic images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000437",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Benchmark (surveying)",
      "Binary code",
      "Binary number",
      "Computer science",
      "Decoding methods",
      "Deep learning",
      "Dimensionality reduction",
      "Encoder",
      "Feature learning",
      "Geodesy",
      "Geography",
      "MNIST database",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Kamal",
        "given_name": "Imam Mustafa"
      },
      {
        "surname": "Bae",
        "given_name": "Hyerim"
      }
    ]
  },
  {
    "title": "Learning multi-scale synergic discriminative features for prostate image segmentation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108556",
    "abstract": "Although deep convolutional neural networks (DCNNs) have been proposed for prostate MR image segmentation, the effectiveness of these methods is often limited by inadequate semantic discrimination and spatial context modeling. To address these issues, we propose a Multi-scale Synergic Discriminative Network (MSD-Net), which includes a shared encoder, a segmentation decoder, and a boundary detection decoder. We further design the cascaded pyramid convolutional block and residual refinement block, and incorporate them and the channel attention block into MSD-Net to exploit the multi-scale spatial contextual information and semantically consistent features of the gland. We also fuse the features from two decoders to boost the segmentation performance, and introduce the synergic multi-task loss to impose the consistence constraint on the joint segmentation and boundary detection. We evaluated MSD-Net against several prostate segmentation methods on three public datasets and achieved an improved accuracy. Our results indicate that the proposed MSD-Net outperforms existing methods with setting the new state-of-the-art for prostate segmentation in magnetic resonance images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000371",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Block (permutation group theory)",
      "Computer science",
      "Context (archaeology)",
      "Convolutional neural network",
      "Discriminative model",
      "Encoder",
      "Geometry",
      "Image segmentation",
      "Mathematics",
      "Operating system",
      "Optics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Physics",
      "Pooling",
      "Pyramid (geometry)",
      "Scale-space segmentation",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Jia",
        "given_name": "Haozhe"
      },
      {
        "surname": "Cai",
        "given_name": "Weidong"
      },
      {
        "surname": "Huang",
        "given_name": "Heng"
      },
      {
        "surname": "Xia",
        "given_name": "Yong"
      }
    ]
  },
  {
    "title": "Bag dissimilarity regularized multi-instance learning",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108583",
    "abstract": "Multi-instance learning (MIL) is able to cope with the weakly supervised problems where the training data is represented by labeled bags consisting of multiple unlabeled instances. Due to its practical significance, MIL has recently drawn increasing attention. Introducing bag representations is an attractive way to learn MIL data. However, it is difficult for the existing MIL methods to utilize both implicit and explicit bag representations simultaneously. In this paper, we propose a bag dissimilarity regularized (BDR) framework that incorporates multiple bag representations regardless of explicitness or implicitness. Here, the implicit bag representations are incorporated into a regularization term that contains the intrinsic geometric information provided by the bag dissimilarities. The regularization term can be added to the objective function of supervised classifiers. An effective method for explicit bag embedding is also proposed, which exploits the Fisher score derived from factor analysis. Finally, we propose two specific BDR methods based on support vector machine and broad learning system. The proposed BDR methods are evaluated on 14 datasets, and have achieved competitive results with limited computation consumption. We also discuss the effectiveness and the characteristics of BDR framework.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000644",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computation",
      "Computer science",
      "Computer security",
      "Embedding",
      "Exploit",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Shiluo"
      },
      {
        "surname": "Liu",
        "given_name": "Zheng"
      },
      {
        "surname": "Jin",
        "given_name": "Wei"
      },
      {
        "surname": "Mu",
        "given_name": "Ying"
      }
    ]
  },
  {
    "title": "Repurposing existing deep networks for caption and aesthetic-guided image cropping",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108485",
    "abstract": "We propose a novel optimization framework that crops a given image based on user description and aesthetics. Unlike existing image cropping methods, where one typically trains a deep network to regress to crop parameters or cropping actions, we propose to directly optimize for the cropping parameters by repurposing pre-trained networks on image captioning and aesthetic tasks, without any fine-tuning, thereby avoiding training a separate network. Specifically, we search for the best crop parameters that minimize a combined loss of the initial objectives of these networks. To make the optimization stable, we propose three strategies: (i) multi-scale bilinear sampling, (ii) annealing the scale of the crop region, therefore effectively reducing the parameter space, (iii) aggregation of multiple optimization results. Through various quantitative and qualitative evaluations, we show that our framework can produce crops that are well-aligned to intended user descriptions and aesthetically pleasing.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006610",
    "keywords": [
      "Agricultural engineering",
      "Agriculture",
      "Artificial intelligence",
      "Bilinear interpolation",
      "Biology",
      "Closed captioning",
      "Computer science",
      "Computer vision",
      "Cropping",
      "Ecology",
      "Engineering",
      "Image (mathematics)",
      "RGB color model",
      "Repurposing"
    ],
    "authors": [
      {
        "surname": "Horanyi",
        "given_name": "Nora"
      },
      {
        "surname": "Xia",
        "given_name": "Kedi"
      },
      {
        "surname": "Yi",
        "given_name": "Kwang Moo"
      },
      {
        "surname": "Bojja",
        "given_name": "Abhishake Kumar"
      },
      {
        "surname": "Leonardis",
        "given_name": "Aleš"
      },
      {
        "surname": "Chang",
        "given_name": "Hyung Jin"
      }
    ]
  },
  {
    "title": "Face spoofing detection ensemble via multistage optimisation and pruning",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.006",
    "abstract": "Despite the recent improvements in facial recognition, face spoofing attacks can still pose a serious security threat to biometric systems. As fraudsters are coming up with novel spoofing attacks, anomaly-based detectors, compared to the binary spoofing attack counterparts, have certain generalisation performance advantages. In this work, we investigate the merits of fusing multiple anomaly classifiers using weighted averaging (WA) fusion. The design of the entire system is based on genuine-access data only. To optimise the parameters of WA, we propose a novel three-stage optimisation method with the following contributions: (a) A new hybrid optimisation method using Genetic Algorithm (GA) and Pattern Search (PS) to explore the weight space more effectively (b) a novel two-sided score normalisation method to improve the anomaly detection performance (c) a new ensemble pruning method to improve the generalisation performance. To further boost the performance of the proposed anomaly detection ensemble, we incorporate client-specific information to train the proposed model. We evaluate the capability of the proposed model on publicly available face spoofing databases including Replay-Attack, Replay-Mobile and Rose-Youtu. The experimental results demonstrate that the proposed WA fusion outperforms the state-of-the-art anomaly-based and multiclass approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001027",
    "keywords": [
      "Agronomy",
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Authentication (law)",
      "Biology",
      "Biometrics",
      "Computer science",
      "Computer security",
      "Condensed matter physics",
      "Data mining",
      "Face (sociological concept)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Pruning",
      "Replay attack",
      "Social science",
      "Sociology",
      "Spoofing attack"
    ],
    "authors": [
      {
        "surname": "Fatemifar",
        "given_name": "Soroush"
      },
      {
        "surname": "Asadi",
        "given_name": "Shahrokh"
      },
      {
        "surname": "Awais",
        "given_name": "Muhammad"
      },
      {
        "surname": "Akbari",
        "given_name": "Ali"
      },
      {
        "surname": "Kittler",
        "given_name": "Josef"
      }
    ]
  },
  {
    "title": "Laplacian encoder-decoder network for raindrop removal",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.016",
    "abstract": "Digital raindrop removal is a branch of image restoration that aims at identifying adherent droplets on a glass surface and replacing them with plausible content. When successfully performed, raindrop removal was proven in the past to positively affect both the perceived appearance of the scene, and the performance of computer vision tasks such as semantic segmentation and object detection. In this paper, we design and implement a new encoder-decoder neural network for supervised raindrop removal. Our network, given a rainy input image, produces as output the Laplacian pyramid of a rain-free version of the input, making it possible to handle the variety of appearances of rain droplets by processing different frequency bands independently. To this end, we define and experimentally prove the effectiveness of a custom loss function that combines the errors of the different Laplacian frequency bands. We test our model for raindrop removal on a standard dataset, using multiple objective metrics to provide a detailed analysis of its performance. We confirm the superiority of our proposal in a comparison with other methods from the state of the art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001143",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Encoder",
      "Geometry",
      "Image (mathematics)",
      "Laplace operator",
      "Mathematical analysis",
      "Mathematics",
      "Object (grammar)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Pyramid (geometry)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Zini",
        "given_name": "Simone"
      },
      {
        "surname": "Buzzelli",
        "given_name": "Marco"
      }
    ]
  },
  {
    "title": "ASMFS: Adaptive-similarity-based multi-modality feature selection for classification of Alzheimer's disease",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108566",
    "abstract": "Multimodal classification methods using different modalities have great advantages over traditional single-modality-based ones for the diagnosis of Alzheimer's disease (AD) and its prodromal stage mild cognitive impairment (MCI). With the increasing amount of high-dimensional heterogeneous data to be processed, multi-modality feature selection has become a crucial research direction for AD classification. However, traditional methods usually depict the data structure using pre-defined similarity matrix as a priori, which is difficult to precisely measure the intrinsic relationship across different modalities in high-dimensional space. In this paper, we propose a novel multimodal feature selection method called Adaptive-Similarity-based Multi-modality Feature Selection (ASMFS) which performs adaptive similarity learning and feature selection simultaneously. Specifically, a similarity matrix is learned by jointly considering different modalities and at the same time, an efficient feature selection is conducted by imposing group sparsity-inducing l 2 , 1 -norm constraint. Evaluated on the Alzheimer's Disease Neuroimaging Initiative (ADNI) database with baseline MRI and FDG-PET imaging data collected from 51 AD, 43 MCI converters (MCI-C), 56 MCI non-converters (MCI-NC) and 52 normal controls (NC), we demonstrate the effectiveness and superiority of our proposed method against other state-of-the-art approaches for multi-modality classification of AD/MCI.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000474",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Feature (linguistics)",
      "Feature selection",
      "Image (mathematics)",
      "Linguistics",
      "Modality (human–computer interaction)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Selection (genetic algorithm)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Yuang"
      },
      {
        "surname": "Zu",
        "given_name": "Chen"
      },
      {
        "surname": "Hong",
        "given_name": "Mei"
      },
      {
        "surname": "Zhou",
        "given_name": "Luping"
      },
      {
        "surname": "Wang",
        "given_name": "Lei"
      },
      {
        "surname": "Wu",
        "given_name": "Xi"
      },
      {
        "surname": "Zhou",
        "given_name": "Jiliu"
      },
      {
        "surname": "Zhang",
        "given_name": "Daoqiang"
      },
      {
        "surname": "Wang",
        "given_name": "Yan"
      }
    ]
  },
  {
    "title": "Twitter sentiment analysis using ensemble based deep learning model towards COVID-19 in India and European countries",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.027",
    "abstract": "As of November 2021, more than 24.80 crore people are diagnosed with the coronavirus in that around 50.20 lakhs people lost their lives, because of this infectious disease. By understanding the people's sentiment's expressed in their social media (Facebook, Twitter, Instagram etc.) helps their governments in controlling, monitoring, and eradicating the coronavirus. Compared to other social media's, the twitter data are indispensable in the extraction of useful awareness information related to any crisis. In this article, a sentiment analysis model is proposed to analyze the real time tweets, which are related to coronavirus. Initially, around 3100 Indian and European people's tweets are collected between the time period of 23.03.2020 to 01.11.2021. Next, the data pre-processing and exploratory investigation are accomplished for better understanding of the collected data. Further, the feature extraction is performed using Term Frequency-Inverse Document Frequency (TF-IDF), GloVe, pre-trained Word2Vec, and fast text embedding's. The obtained feature vectors are fed to the ensemble classifier (Gated Recurrent Unit (GRU) and Capsule Neural Network (CapsNet)) for classifying the user's sentiment's as anger, sad, joy, and fear. The obtained experimental outcomes showed that the proposed model achieved 97.28% and 95.20% of prediction accuracy in classifying the both Indian and European people's sentiments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001246",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Disease",
      "Embedding",
      "Feature extraction",
      "Infectious disease (medical specialty)",
      "Machine learning",
      "Medicine",
      "Pathology",
      "Physics",
      "Quantum mechanics",
      "Sentiment analysis",
      "Social media",
      "Term (time)",
      "Word2vec",
      "World Wide Web",
      "tf–idf"
    ],
    "authors": [
      {
        "surname": "Sunitha",
        "given_name": "D."
      },
      {
        "surname": "Patra",
        "given_name": "Raj Kumar"
      },
      {
        "surname": "Babu",
        "given_name": "N.V."
      },
      {
        "surname": "Suresh",
        "given_name": "A."
      },
      {
        "surname": "Gupta",
        "given_name": "Suresh Chand"
      }
    ]
  },
  {
    "title": "A hybrid active contour model based on pre-fitting energy and adaptive functions for fast image segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.025",
    "abstract": "In this study, a hybrid active contour model driven by pre-fitting energy with an adaptive edge indicator function and an adaptive sign function is proposed. The key idea of employing the pre-fitting energy is to define two pre-fitting functions to calculate mean intensities of two sub-regions separated from the selected local region based on pre-calculated median intensity of the selected local region before the curve evolves, which saves a huge amount of computation cost. In addition, a brand-new single-well potential function and its associated evolution speed function are put forward to enable evolution process to converge faster as well as more robust. Experimental outcomes indicate that this model is competent to obtain motion boundaries of different targets effectively and efficiently. Compared with traditional models and recently developed models, this model not only reduces the CPU running time and iteration number significantly as well as improves segmentation accuracies (Dice similarity coefficient (DSC) and Jaccard similarity coefficient (JSC)), but also exhibits insensitivity to initialization.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001234",
    "keywords": [
      "Active contour model",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Energy (signal processing)",
      "Image (mathematics)",
      "Image segmentation",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Scale-space segmentation",
      "Segmentation",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Ge",
        "given_name": "Pengqiang"
      },
      {
        "surname": "Chen",
        "given_name": "Yiyang"
      },
      {
        "surname": "Wang",
        "given_name": "Guina"
      },
      {
        "surname": "Weng",
        "given_name": "Guirong"
      }
    ]
  },
  {
    "title": "CSCNet: Contextual semantic consistency network for trajectory prediction in crowded spaces",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108552",
    "abstract": "Trajectory prediction aims to predict the movement trend of the agents like pedestrians, bikers, vehicles. It is helpful to analyze and understand human activities in crowded spaces and widely applied in many areas such as surveillance video analysis and autonomous driving systems. Thanks to the success of deep learning, trajectory prediction has made significant progress. The current methods are dedicated to studying the agents’ future trajectories under the social interaction and the sceneries’ physical constraints. Moreover, how to deal with these factors still catches researchers’ attention. However, they ignore the Semantic Shift Phenomenon when modeling these interactions in various prediction sceneries. There exist several kinds of semantic deviations inner or between social and physical interactions, which we call the “Gap”. In this paper, we propose a Contextual Semantic Consistency Network (CSCNet) to predict agents’ future activities with powerful and efficient context constraints. We utilize a well-designed context-aware transfer to obtain the intermediate representations from the scene images and trajectories. Then we eliminate the differences between social and physical interactions by aligning activity semantics and scene semantics to cross the Gap. Experiments demonstrate that CSCNet performs better than most of the current methods quantitatively and qualitatively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000334",
    "keywords": [
      "Artificial intelligence",
      "Astronomy",
      "Computer science",
      "Consistency (knowledge bases)",
      "Physics",
      "Trajectory"
    ],
    "authors": [
      {
        "surname": "Xia",
        "given_name": "Beihao"
      },
      {
        "surname": "Wong",
        "given_name": "Conghao"
      },
      {
        "surname": "Peng",
        "given_name": "Qinmu"
      },
      {
        "surname": "Yuan",
        "given_name": "Wei"
      },
      {
        "surname": "You",
        "given_name": "Xinge"
      }
    ]
  },
  {
    "title": "Learning time-aware features for action quality assessment",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.015",
    "abstract": "Action quality assessment (AQA) is a task to assess the performance of a human action, which can be widely used in many real-world scenarios such as sport events. Current AQA methods generally extract features from the video and perform regression analysis to obtain the action quality score. In this process, aggregated video features may not reflect different stages of an action, which are important to evaluate an action is good or not. To address this issue, we propose to divide the video into different clips and learn the relationship between them, which may capture the action changes for accurate assessment. Time-aware (TA) attention mechanism is used to evaluate this relationship. In the experiment, our proposed method achieves promising results on the MTL-AQA dataset compared with existing AQA methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001131",
    "keywords": [
      "Action (physics)",
      "Artificial intelligence",
      "Computer science",
      "Engineering",
      "Epistemology",
      "Evaluation methods",
      "Machine learning",
      "Natural language processing",
      "Philosophy",
      "Physics",
      "Quality (philosophy)",
      "Quality assessment",
      "Quantum mechanics",
      "Reliability engineering"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yu"
      },
      {
        "surname": "Xiong",
        "given_name": "Wei"
      },
      {
        "surname": "Mi",
        "given_name": "Siya"
      }
    ]
  },
  {
    "title": "Adaptive-order proximity learning for graph-based clustering",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108550",
    "abstract": "Recently, structured proximity matrix learning, which aims to learn a structured proximity matrix with explicit clustering structures from the first-order proximity matrix, has become the mainstream of graph-based clustering. However, the first-order proximity matrix always lacks several must-links compared to the groundtruth in real-world data, which results in a mismatched problem and affects the clustering performance. To alleviate this problem, this work introduces the high-order proximity to structured proximity matrix learning, and explores a novel framework named Adaptive-Order Proximity Learning (AOPL) to learn a consensus structured proximity matrix from the proximities of multiple orders. To be specific, AOPL selects the appropriate orders first, then assigns weights to these selected orders adaptively. In this way, a consensus structured proximity matrix is learned from the proximity matrices of appropriate orders. Based on AOPL framework, two practical models with different properties are derived, namely AOPL-Root and AOPL-Log. Besides, AOPL and the derived models are regarded as the same optimization problem subjected to some slightly different constraints. An efficient algorithm is proposed to solve them and the corresponding theoretical analyses are provided. Extensive experiments on several real-world datasets demonstrate superb performance of our model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000310",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Composite material",
      "Computer science",
      "Graph",
      "Machine learning",
      "Materials science",
      "Matrix (chemical analysis)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Danyang"
      },
      {
        "surname": "Chang",
        "given_name": "Wei"
      },
      {
        "surname": "Lu",
        "given_name": "Jitao"
      },
      {
        "surname": "Nie",
        "given_name": "Feiping"
      },
      {
        "surname": "Wang",
        "given_name": "Rong"
      },
      {
        "surname": "Li",
        "given_name": "Xuelong"
      }
    ]
  },
  {
    "title": "Meta-seg: A survey of meta-learning for image segmentation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108586",
    "abstract": "A well-performed deep learning model in image segmentation relies on a large number of labeled data. However, it is hard to obtain sufficient high-quality raw data in industrial applications. Meta-learning, one of the most promising research areas, is recognized as a powerful tool for approaching image segmentation. To this end, this paper reviews the state-of-the-art image segmentation methods based on meta-learning. We firstly introduce the background of the image segmentation, including the methods and metrics of image segmentation. Second, we review the timeline of meta-learning and give a more comprehensive definition of meta-learning. The differences between meta-learning and other similar methods are compared comprehensively. Then, we categorize the existing meta-learning methods into model-based, optimization-based, and metric-based. For each categorization, the popular used meta-learning models are discussed in image segmentation. Next, we conduct comprehensive computational experiments to compare these models on two pubic datasets: ISIC-2018 and Covid-19. Finally, the future trends of meta-learning in image segmentation are highlighted.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200067X",
    "keywords": [
      "Artificial intelligence",
      "Categorization",
      "Computer science",
      "Economics",
      "Image (mathematics)",
      "Image segmentation",
      "Machine learning",
      "Management",
      "Meta learning (computer science)",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Scale-space segmentation",
      "Segmentation",
      "Segmentation-based object categorization",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Shuai"
      },
      {
        "surname": "Li",
        "given_name": "Yujie"
      },
      {
        "surname": "Gao",
        "given_name": "Pengxiang"
      },
      {
        "surname": "Wang",
        "given_name": "Yichuan"
      },
      {
        "surname": "Serikawa",
        "given_name": "Seiichi"
      }
    ]
  },
  {
    "title": "A novel unsupervised ensemble framework using concept-based linguistic methods and machine learning for twitter sentiment analysis",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.004",
    "abstract": "Concept-based sentiment analysis (CBSA) methods have gained prominence in natural language processing in recent years. These methods consider the underlying semantic meanings of text to perform different tasks such as Twitter sentiment analysis (assigning positive, negative, or neutral sentiment to Tweets). CBSA is superior to traditional statistical methods for accurately discovering sentiment labels. Due to a limited knowledge base, these methods are unable to identify the sentiment polarity of all kinds of text. Therefore, supervised learning techniques are mostly ensembled with CBSA methods to classify the whole text. These techniques require labeled data. It is a tedious and time-consuming task due to the manually labeling of large datasets (Such as Twitter datasets). Therefore, an unsupervised learning mechanism can be a better alternative to solve this problem. In this paper, a novel unsupervised learning framework based on Concept-based and hierarchical clustering is proposed for Twitter sentiment analysis. Popular hierarchical clustering methods including single linkage, complete linkage, and average linkage algorithms are ensembled serially. Two different feature representation methods including Boolean and Term frequency-inverse document frequency (TF-IDF) are investigated. We have also experimented with Well-known classifiers (Naive Bayes, Neural Network) for a fair comparison. Accuracy measure (proportion of correct predictions) is used to evaluate the performance of understudied techniques. It is empirically shown that the performance of unsupervised learning techniques is comparable with supervised learning techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000927",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Cluster analysis",
      "Computer science",
      "Feature (linguistics)",
      "Linguistics",
      "Machine learning",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Sentiment analysis",
      "Supervised learning",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Bibi",
        "given_name": "Maryum"
      },
      {
        "surname": "Abbasi",
        "given_name": "Wajid Arshad"
      },
      {
        "surname": "Aziz",
        "given_name": "Wajid"
      },
      {
        "surname": "Khalil",
        "given_name": "Sundus"
      },
      {
        "surname": "Uddin",
        "given_name": "Mueen"
      },
      {
        "surname": "Iwendi",
        "given_name": "Celestine"
      },
      {
        "surname": "Gadekallu",
        "given_name": "Thippa Reddy"
      }
    ]
  },
  {
    "title": "Structure-aware conditional variational auto-encoder for constrained molecule optimization",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108581",
    "abstract": "The goal of molecule optimization is to optimize molecular properties by modifying molecule structures. Conditional generative models provide a promising way to transfer the input molecules to the ones with better property. However, molecular properties are highly sensitive to small changes in molecular structures. This leads to an interesting thought that we can improve the property of molecules with limited modification in structure. In this paper, we propose a structure-aware conditional Variational Auto-Encoder, namely SCVAE, which exploits the topology of molecules as structure condition and optimizes the molecular properties with constrained structural modification. SCVAE leverages graph alignment of two-level molecule structures in an unsupervised manner to bind the structure conditions between two molecules. Then, this structure condition facilitates the molecule optimization with limited structural modification, namely, constrained molecule optimization, under a novel variational auto-encoder framework. Extensive experimental evaluations demonstrate that structure-aware CVAE generates new molecules with high similarity to the original ones and better molecular properties.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000620",
    "keywords": [
      "Algorithm",
      "Biological system",
      "Biology",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Encoder",
      "Graph",
      "Mathematics",
      "Molecular graph",
      "Molecule",
      "Operating system",
      "Organic chemistry",
      "Theoretical computer science",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Junchi"
      },
      {
        "surname": "Xu",
        "given_name": "Tingyang"
      },
      {
        "surname": "Rong",
        "given_name": "Yu"
      },
      {
        "surname": "Huang",
        "given_name": "Junzhou"
      },
      {
        "surname": "He",
        "given_name": "Ran"
      }
    ]
  },
  {
    "title": "Rule extraction with guarantees from regression models",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108554",
    "abstract": "Tools for understanding and explaining complex predictive models are critical for user acceptance and trust. One such tool is rule extraction, i.e., approximating opaque models with less powerful but interpretable models. Pedagogical (or black-box) rule extraction, where the interpretable model is induced using the original training instances, but with the predictions from the opaque model as targets, has many advantages compared to the decompositional (white-box) approach. Most importantly, pedagogical methods are agnostic to the kind of opaque model used, and any learning algorithm producing interpretable models can be employed for the learning step. The pedagogical approach has, however, one main problem, clearly limiting its utility. Specifically, while the extracted models are trained to mimic the opaque, there are absolutely no guarantees that this will transfer to novel data. This potentially low test set fidelity must be considered a severe drawback, in particular when the extracted models are used for explanation and analysis. In this paper, a novel approach, solving the problem with test set fidelity by utilizing the conformal prediction framework, is suggested for extracting interpretable regression models from opaque models. The extracted models are standard regression trees, but augmented with valid prediction intervals in the leaves. Depending on the exact setup, the use of conformal prediction guarantees that either the test set fidelity or the test set accuracy will be equal to a preset confidence level, in the long run. In the extensive empirical investigation, using 20 publicly available data sets, the validity of the extracted models is demonstrated. In addition, it is shown how normalization can be used to provide individualized prediction intervals, thus providing highly informative extracted models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000358",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Fidelity",
      "Machine learning",
      "Mathematics",
      "Programming language",
      "Regression",
      "Regression analysis",
      "Set (abstract data type)",
      "Statistics",
      "Telecommunications",
      "Test set"
    ],
    "authors": [
      {
        "surname": "Johansson",
        "given_name": "Ulf"
      },
      {
        "surname": "Sönströd",
        "given_name": "Cecilia"
      },
      {
        "surname": "Löfström",
        "given_name": "Tuwe"
      },
      {
        "surname": "Boström",
        "given_name": "Henrik"
      }
    ]
  },
  {
    "title": "A globally convergent approximate Newton method for non-convex sparse learning",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108560",
    "abstract": "Newton-type greedy pursuit methods have been shown to work favorably for cardinality-constrained sparse learning problems. The appealing sparsity recovery performance of the existing Newton-type greedy pursuit methods, however, is typically guaranteed within a local neighborhood around the target solution. To address this limitation, we present in this paper a novel approximate Newton pursuit method for sparse learning with linear models. The computation procedure of our method iterates between constructing an inexact Newton-type quadratic majorization to the global empirical risk and solving the quadratic approximation via iterative hard thresholding. Provable global guarantees on mean squared prediction error, which is less understood for prior methods, are provided for our method. Numerical evidence is provided to show the advantages of our approach over the prior methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000413",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Geometry",
      "Image (mathematics)",
      "Iterated function",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Newton's method",
      "Nonlinear system",
      "Physics",
      "Quadratic equation",
      "Quantum mechanics",
      "Thresholding"
    ],
    "authors": [
      {
        "surname": "Ji",
        "given_name": "Fanfan"
      },
      {
        "surname": "Shuai",
        "given_name": "Hui"
      },
      {
        "surname": "Yuan",
        "given_name": "Xiao-Tong"
      }
    ]
  },
  {
    "title": "Decoupling music notation to improve end-to-end Optical Music Recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.032",
    "abstract": "Inspired by the Text Recognition field, end-to-end schemes based on Convolutional Recurrent Neural Networks (CRNN) trained with the Connectionist Temporal Classification (CTC) loss function are considered one of the current state-of-the-art techniques for staff-level Optical Music Recognition (OMR). Unlike text symbols, music-notation elements may be defined as a combination of (i) a shape primitive located in (ii) a certain position in a staff. However, this double nature is generally neglected in the learning process, as each combination is treated as a single token. In this work, we study whether exploiting such particularity of music notation actually benefits the recognition performance and, if so, which approach is the most appropriate. For that, we thoroughly review existing specific approaches that explore this premise and propose different combinations of them. Furthermore, considering the limitations observed in such approaches, a novel decoding strategy specifically designed for OMR is proposed. The results obtained with four different corpora of historical manuscripts show the relevance of leveraging this double nature of music notation since it outperforms the standard approaches where it is ignored. In addition, the proposed decoding leads to significant reductions in the error rates with respect to the other cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001428",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Art",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer security",
      "Connectionism",
      "Control engineering",
      "Convolutional neural network",
      "Decoding methods",
      "Decoupling (probability)",
      "End-to-end principle",
      "Engineering",
      "Law",
      "Linguistics",
      "Mathematics",
      "Musical",
      "Musical notation",
      "Natural language processing",
      "Notation",
      "Philosophy",
      "Political science",
      "Premise",
      "Relevance (law)",
      "Security token",
      "Speech recognition",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Alfaro-Contreras",
        "given_name": "María"
      },
      {
        "surname": "Ríos-Vila",
        "given_name": "Antonio"
      },
      {
        "surname": "Valero-Mas",
        "given_name": "Jose J."
      },
      {
        "surname": "Iñesta",
        "given_name": "José M."
      },
      {
        "surname": "Calvo-Zaragoza",
        "given_name": "Jorge"
      }
    ]
  },
  {
    "title": "Low-resolution human pose estimation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108579",
    "abstract": "Human pose estimation has achieved significant progress on images with high imaging resolution. However, low-resolution imagery data bring nontrivial challenges which are still under-studied. To fill this gap, we start with investigating existing methods and reveal that the most dominant heatmap-based methods would suffer more severe model performance degradation from low-resolution, and offset learning is an effective strategy. Established on this observation, in this work we propose a novel Confidence-Aware Learning (CAL) method which further addresses two fundamental limitations of existing offset learning methods: inconsistent training and testing, decoupled heatmap and offset learning. Specifically, CAL selectively weighs the learning of heatmap and offset with respect to ground-truth and most confident prediction, whilst capturing the statistical importance of model output in mini-batch learning manner. Extensive experiments conducted on the COCO benchmark show that our method outperforms significantly the state-of-the-art methods for low-resolution human pose estimation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000607",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Geodesy",
      "Geography",
      "Ground truth",
      "High resolution",
      "Image (mathematics)",
      "Low resolution",
      "Machine learning",
      "Offset (computer science)",
      "Pattern recognition (psychology)",
      "Pose",
      "Programming language",
      "Remote sensing",
      "Superresolution"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Chen"
      },
      {
        "surname": "Zhang",
        "given_name": "Feng"
      },
      {
        "surname": "Zhu",
        "given_name": "Xiatian"
      },
      {
        "surname": "Ge",
        "given_name": "Shuzhi Sam"
      }
    ]
  },
  {
    "title": "MoRE: Multi-output residual embedding for multi-label classification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108584",
    "abstract": "Multi-label classification (MLC) is one of the challenging tasks in computer vision, where it confronts high dimensional problem both in output label and input feature spaces. This paper proposed solving MLC through multi-output residual embedding (MoRE), which learns appropriate distance metric by analyzing the residuals between input and output spaces. Unlike traditional MLC paradigms that learn relationships between label space and feature space, our proposed approach further learns a low-rank structure in residuals between input and output spaces. And it encodes such residual projection to achieve dimension reduction in label space, enhancing the performance of the proposed algorithm in processing high dimensional MLC task. Furthermore, considering the label correlations between instances and its neighbors, multiple residuals of instances neighbors are also incorporated into the proposed model to further learn more appropriate distance metric in the same way. Overall, with residual embedding learning from instances and their neighbors, the obtained metric can learn a more appropriate low-rank structure in label space to handle high dimensional problem in MLC. Experimental results on several data sets, such as Cal500, Corel5k, Bibtex, Delicious, Tmc2007, 20ng, Mirflickr and Rcv1s1, demonstrate the excellent predictive performance of MoRE among STOA methods, such as LMMO-kNN, M3MDC, KRAM, SEEM, CPLST, CSSP, FaIE.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000656",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Dimension (graph theory)",
      "Dimensionality reduction",
      "Economics",
      "Embedding",
      "Feature (linguistics)",
      "Feature vector",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Metric (unit)",
      "Metric space",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Projection (relational algebra)",
      "Pure mathematics",
      "Rank (graph theory)",
      "Residual"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Siyu"
      },
      {
        "surname": "Song",
        "given_name": "Xuehua"
      },
      {
        "surname": "Ma",
        "given_name": "Zhongchen"
      },
      {
        "surname": "Ganaa",
        "given_name": "Ernest Domanaanmwi"
      },
      {
        "surname": "Shen",
        "given_name": "XiangJun"
      }
    ]
  },
  {
    "title": "HarrisZ+: Harris corner selection for next-gen image matching pipelines",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.022",
    "abstract": "Due to its role in many computer vision tasks, image matching has been subjected to an active investigation by researchers, which has lead to better and more discriminant feature descriptors and to more robust matching strategies, also thanks to the advent of the deep learning and the increased computational power of the modern hardware. Despite of these achievements, the keypoint extraction process at the base of the image matching pipeline has not seen equivalent progresses. This paper presents HarrisZ+, an upgrade to the HarrisZ corner detector, optimized to synergically take advance of the recent improvements of the other steps of the image matching pipeline. HarrisZ+ does not only consists of a tuning of the setup parameters, but introduces further refinements to the selection criteria delineated by HarrisZ, so providing more, yet discriminative, keypoints, which are better distributed on the image and with higher localization accuracy. The image matching pipeline including HarrisZ+, together with the other modern components, obtained in different recent matching benchmarks state-of-the-art results among the classic image matching pipelines. These results are quite close to those obtained by the more recent fully deep end-to-end trainable approaches and show that there is still a proper margin of improvement that can be granted by the research in classic image matching methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001179",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Engineering",
      "Environmental engineering",
      "Feature extraction",
      "Image (mathematics)",
      "Machine learning",
      "Margin (machine learning)",
      "Matching (statistics)",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Pipeline (software)",
      "Pipeline transport",
      "Process (computing)",
      "Programming language",
      "Scale-invariant feature transform",
      "Selection (genetic algorithm)",
      "Statistics",
      "Template matching"
    ],
    "authors": [
      {
        "surname": "Bellavia",
        "given_name": "Fabio"
      },
      {
        "surname": "Mishkin",
        "given_name": "Dmytro"
      }
    ]
  },
  {
    "title": "Special Issue on Conformal and Probabilistic Prediction with Applications: Preface",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108561",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000425",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Conformal map",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Probabilistic logic"
    ],
    "authors": [
      {
        "surname": "Gammerman",
        "given_name": "Alexander"
      },
      {
        "surname": "Vovk",
        "given_name": "Vladimir"
      },
      {
        "surname": "Cristani",
        "given_name": "Marco"
      }
    ]
  },
  {
    "title": "Class-attribute inconsistency learning for novelty detection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108582",
    "abstract": "In this paper, we address the problem of novelty detection whose goal is to recognize instances from unseen classes during testing. Our key idea is to leverage the inconsistency between class similarity and (latent) attribute similarity. We are motivated by the observation that a novel class may holistically appear like a certain known class (class-level reference) but often exhibits unique properties similar to others (attribute-level references). That is, the related class- and attribute-level references are often inconsistent for a novel class. A new two-stage Class-Attribute Inconsistency Learning network (CAILNet) is proposed to explore class-attribute inconsistency for novelty detection. Stage one aims to learn both class and attribute features based on the class labels and fake attribute labels, and stage two aims to search for the corresponding references and make fine-grained comparisons for final novelty decision. Empirically we conduct comprehensive experiments on three benchmark datasets, and demonstrate state-of-the-art performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000632",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Class (philosophy)",
      "Computer science",
      "Computer security",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Key (lock)",
      "Leverage (statistics)",
      "Machine learning",
      "Novelty",
      "Novelty detection",
      "Psychology",
      "Similarity (geometry)",
      "Social psychology"
    ],
    "authors": [
      {
        "surname": "Du",
        "given_name": "Shuaiyuan"
      },
      {
        "surname": "Hong",
        "given_name": "Chaoyi"
      },
      {
        "surname": "Chen",
        "given_name": "Yinpeng"
      },
      {
        "surname": "Cao",
        "given_name": "Zhiguo"
      },
      {
        "surname": "Zhang",
        "given_name": "Ziming"
      }
    ]
  },
  {
    "title": "An enhanced N-point interpolation method to eliminate average precision distortion",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.028",
    "abstract": "Existing N -point interpolation methods generate large errors in the average precision calculation for object detection. These errors lead to average precision distortion, which makes it impossible to accurately evaluate the performance of the model. We investigate the reason for the average precision distortion and propose an enhanced N -point interpolation method. These improvements are based on the N -point interpolation method and can be summarized in two parts: (1) The interpolation point position is changed to the middle interpolation. (2) Dynamic selection of parameters for calculating the area of the interpolation interval. Experiments verify the existence of severe average precision distortion in the N -point interpolation method. Furthermore, the proposed enhanced N -point interpolation method reduces the average precision distortion by more than 90% to only 0.04%. In this way, the enhanced N -point interpolation method is able to replace the all-point interpolation method for fast and accurate evaluation of object detection model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552200126X",
    "keywords": [
      "Algorithm",
      "Amplifier",
      "Bandwidth (computing)",
      "Bilinear interpolation",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Distortion (music)",
      "Economics",
      "Finance",
      "Geometry",
      "Interpolation (computer graphics)",
      "Mathematics",
      "Motion (physics)",
      "Multivariate interpolation",
      "Nearest-neighbor interpolation",
      "Point (geometry)",
      "Position (finance)",
      "Stairstep interpolation",
      "Trilinear interpolation"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Haodi"
      },
      {
        "surname": "Rogozan",
        "given_name": "Alexandrina"
      },
      {
        "surname": "Bensrhair",
        "given_name": "Abdelaziz"
      }
    ]
  },
  {
    "title": "Split, Embed and Merge: An accurate table structure recognizer",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108565",
    "abstract": "Table structure recognition is an essential part for making machines understand tables. Its main task is to recognize the internal structure of a table. However, due to the complexity and diversity in their structure and style, it is very difficult to parse the tabular data into the structured format which machines can understand, especially for complex tables. In this paper, we introduce Split, Embed and Merge (SEM), an accurate table structure recognizer. SEM is mainly composed of three parts, splitter, embedder and merger. In the first stage, we apply the splitter to predict the potential regions of the table row/column separators, and obtain the fine grid structure of the table. In the second stage, by taking a full consideration of the textual information in the table, we fuse the output features for each table grid from both vision and text modalities. Moreover, we achieve a higher precision in our experiments through providing additional textual features. Finally, we process the merging of these basic table grids in a self-regression manner. The corresponding merging results are learned through the attention mechanism. In our experiments, SEM achieves an average F1-Measure of 97.11 % on the SciTSR dataset which outperforms other methods by a large margin. We also won the first place of complex tables and third place of all tables in Task-B of ICDAR 2021 Competition on Scientific Literature Parsing. Extensive experiments on other publicly available datasets further demonstrate the effectiveness of our proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000462",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Geometry",
      "Grid",
      "Information retrieval",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematics",
      "Merge (version control)",
      "Natural language processing",
      "Parsing",
      "Pattern recognition (psychology)",
      "Splitter",
      "Table (database)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zhenrong"
      },
      {
        "surname": "Zhang",
        "given_name": "Jianshu"
      },
      {
        "surname": "Du",
        "given_name": "Jun"
      },
      {
        "surname": "Wang",
        "given_name": "Fengren"
      }
    ]
  },
  {
    "title": "QuadNet: Quadruplet loss for multi-view learning in baggage re-identification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108546",
    "abstract": "Recently, baggage re-identification (ReID) has become an attractive topic in computer vision because it plays an important role in intelligent surveillance. However, the wide variations in different views of baggage items degrade baggage ReID performance. In this paper, a novel QuadNet is proposed to solve the multi-view problem in baggage ReID at three levels. At the sample level, we propose a multi-view sampling strategy which samples hard examples from multiple identities in multiple views. The sampled baggage items are used to construct quadruplets. At the feature level, view-aware attentional local features are extracted from discriminative regions in each view. These local features are fused with global features to obtain better representations of the quadruplets. At the loss level, a multi-view quadruplet loss operating on the representations of quadruplets is proposed to reduce the intra-class distances caused by view variations and increase the inter-class distances of baggage images captured in the same view. A random local blur data augmentation is proposed to handle the motion blur which is often found in baggage images. The multi-task learning of materials is introduced to obtain discriminative features based on the materials of baggage surfaces. Extensive experiments on three ReID datasets, MVB, Market-1501 and VeRi-776, indicate the remarkable effectiveness and good generalization of the QuadNet model. It has achieved the state-of-the-art performance on the three datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000279",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Chemistry",
      "Chromatography",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Construct (python library)",
      "Discriminative model",
      "Feature (linguistics)",
      "Generalization",
      "Genetics",
      "Gestation",
      "Identification (biology)",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pregnancy",
      "Programming language",
      "Quadruplets",
      "Sample (material)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Hao"
      },
      {
        "surname": "Chu",
        "given_name": "Xiuxiu"
      },
      {
        "surname": "Zhang",
        "given_name": "Li"
      },
      {
        "surname": "Sun",
        "given_name": "Yunda"
      },
      {
        "surname": "Li",
        "given_name": "Dong"
      },
      {
        "surname": "Maybank",
        "given_name": "Stephen J."
      }
    ]
  },
  {
    "title": "Time-series estimation from randomly time-warped observations",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.020",
    "abstract": "We consider the problem of estimating a signal from its warped observations. Such estimation is commonly performed by altering the observations through some inverse-warping, or solving a computationally demanding optimization formulation. While these may be unavoidable if observations are few, when large amounts of warped observations are available, the cost of running such algorithms can be prohibitive. We consider the scenario where we have many observations, and propose a computationally simple algorithm for estimating the function of interest. We demonstrate the utility of the algorithm on streaming biomedical signals.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001192",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Computer science",
      "Economics",
      "Estimation",
      "Geology",
      "Management",
      "Mathematics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Series (stratigraphy)",
      "Statistics",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Bayram",
        "given_name": "İlker"
      }
    ]
  },
  {
    "title": "Class-specific discriminative metric learning for scene recognition",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108589",
    "abstract": "Metric learning aims to learn an appropriate distance metric for a given machine learning task. Despite its impressive performance in the field of image recognition, it may still not be discriminative enough for scene recognition because of the high within-class diversity and high between-class similarity of scene images. In this paper, we propose a novel class-specific discriminative metric learning method (CSDML) to alleviate these problems. More specifically, we learn a distinctive linear transformation for each class (or, equivalently, a Mahalanobis distance metric for each class), which allows to project the samples of that class into a corresponding low-dimensional discriminative space. The overall aim is to simultaneously minimize the Euclidean distances between the projections of samples of the same class (or, equivalently, the Mahalanobis distances between these samples) and maximize the Euclidean distances between the projections of samples of different classes. Additionally, we incorporate least squares regression into the optimization problem, rendering class-specific metric learning more flexible and better suited to tackle scene recognition. Experimental results on four benchmark scene datasets demonstrate that the proposed method outperforms most of the state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200070X",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Discriminative model",
      "Economics",
      "Euclidean distance",
      "Machine learning",
      "Mahalanobis distance",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Chen"
      },
      {
        "surname": "Peng",
        "given_name": "Guohua"
      },
      {
        "surname": "De Baets",
        "given_name": "Bernard"
      }
    ]
  },
  {
    "title": "Spoken language identification in unseen channel conditions using modified within-sample similarity loss",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.018",
    "abstract": "State-of-the-art spoken language identification (LID) systems use sophisticated training strategies to improve the robustness to unseen channel conditions in the real-world test samples. However, all these approaches require training samples from multiple channels with corresponding channel-labels, which is not available in many cases. Recent research in this regard has shown the possibility of learning a channel-invariant representation of the speech using an auxiliary loss function called within-sample similarity loss (WSSL), which does not require samples from multiple channels. Specifically, the WSSL encourages the LID network to ignore channel-specific contents in the speech by minimizing the similarities between two utterance-level embeddings of same sample. However, as WSSL approach operates at sample-level, it ignores the channel variations that may be present across different training samples within same dataset. In this work, we propose a modification to the WSSL approach to address this limitation. Specifically, along with the WSSL, the proposed modified WSSL (mWSSL) approach additionally considers the similarities with two global-level embeddings which represent the average channel-specific contents in a given mini-batch of training samples. The proposed modification allows the network to have a better view of the channel-specific contents in the training dataset, leading to improved performance in unseen channel conditions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001167",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Channel (broadcasting)",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Identification (biology)",
      "Image (mathematics)",
      "Language identification",
      "Linguistics",
      "Natural language",
      "Natural language processing",
      "Philosophy",
      "Sample (material)",
      "Similarity (geometry)",
      "Speech recognition",
      "Spoken language",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "H．",
        "given_name": "Muralikrishna"
      },
      {
        "surname": "Aroor Dinesh",
        "given_name": "Dileep"
      }
    ]
  },
  {
    "title": "mSODANet: A network for multi-scale object detection in aerial images using hierarchical dilated convolutions",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108548",
    "abstract": "The object detection in aerial images is one of the most commonly used tasks in the wide-range of computer vision applications. However, the object detection is more challenging due to the following issues: (a) the pixel occupancy vary among the different scales of objects, (b) the distribution of objects is not uniform in aerial images, (c) the appearance of an object varies with different view-points and illumination conditions, and (d) the number of objects, even though they belong to same type, vary across the images. To address these issues, we propose a novel network for multi-scale object detection in aerial images using hierarchical dilated convolutions, called as mSODANet. In particular, we probe hierarchical dilated network using parallel dilated convolutions to learn the contextual information of different types of objects at multiple scales and multiple field-of-views. The introduced hierarchical dilated network captures the visual information of aerial image more effectively and enhances the detection capability of the model. Further, the extensive experiments conducted on three challenging publicly available datasets, i.e., Visdrone2019, DOTA (OBB & HBB), NWPU VHR-10, demonstrate the effectiveness of the proposed mSODANet and achieve the state-of-the-art performance on all three datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000292",
    "keywords": [
      "Aerial image",
      "Artificial intelligence",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Geography",
      "Image (mathematics)",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Pixel",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Chalavadi",
        "given_name": "Vishnu"
      },
      {
        "surname": "Jeripothula",
        "given_name": "Prudviraj"
      },
      {
        "surname": "Datla",
        "given_name": "Rajeshreddy"
      },
      {
        "surname": "Ch",
        "given_name": "Sobhan Babu"
      },
      {
        "surname": "C",
        "given_name": "Krishna Mohan"
      }
    ]
  },
  {
    "title": "Robust image matching via local graph structure consensus",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108588",
    "abstract": "Image matching plays a vital role in many computer vision tasks, and this paper focuses on the mismatch removal problem of feature-based matching. We formulate the problem into a general yet effective optimization framework based on graph matching by combining integer quadratic programming with a compensation term for discouraging matches, termed as Local Graph Structure Consensus (LGSC). Considering the local area similarity of those potential true matches, we design a local graph structure for preserving geometric topology, which contains a local indicator vector and a local affinity vector for each correspondence. The local indicator vector is utilized for edge construction, while the local affinity vector represents the match correctness of the nodes and edges between two graphs. In particular, the ranking shift with scale and rotation invariance is exploited to represent the node affinity. Ultimately, we derive a closed-form solution with linearithmic time and linear space complexity. Moreover, a multi-scale and iterative graph construction strategy is proposed to promote the performance of our method in terms of robustness and effectiveness. Extensive experiments on various real image datasets demonstrate that our LGSC can achieve superior performance over current state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000693",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Correctness",
      "Feature vector",
      "Gene",
      "Graph",
      "Linear programming",
      "Matching (statistics)",
      "Mathematical optimization",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Xingyu"
      },
      {
        "surname": "Xia",
        "given_name": "Yifan"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiao-Ping"
      },
      {
        "surname": "Ma",
        "given_name": "Jiayi"
      }
    ]
  },
  {
    "title": "An attention-enhanced cross-task network to analyse lung nodule attributes in CT images",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108576",
    "abstract": "Accurate characterization of visual attributes such as spiculation, lobulation, and calcification of lung nodules in computed tomography (CT) images is critical in cancer management. The characterization of these attributes is often subjective, which may lead to high inter- and intra-observer variability. Furthermore, lung nodules are often heterogeneous in the cross-sectional image slices of a 3D volume. Current state-of-the-art methods that score multiple attributes rely on deep learning-based multi-task learning (MTL) schemes. These methods, however, extract shared visual features across attributes and then examine each attribute without explicitly leveraging their inherent intercorrelations. Furthermore, current methods treat each slice with equal importance without considering their relevance or heterogeneity, which limits performance. In this study, we address these challenges with a new convolutional neural network (CNN)-based MTL model that incorporates multiple attention-based learning modules to simultaneously score 9 visual attributes of lung nodules in CT image volumes. Our model processes entire nodule volumes of arbitrary depth and uses a slice attention module to filter out irrelevant slices. We also introduce cross-attribute and attribute specialization attention modules that learn an optimal amalgamation of meaningful representations to leverage relationships between attributes. We demonstrate that our model outperforms previous state-of-the-art methods at scoring attributes using the well-known public LIDC-IDRI dataset of pulmonary nodules from over 1,000 patients. Our model also performs competitively when repurposed for benign-malignant classification. Our attention modules provide easy-to-interpret weights that offer insights into the predictions of the model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000577",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Filter (signal processing)",
      "Law",
      "Leverage (statistics)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Political science",
      "Relevance (law)"
    ],
    "authors": [
      {
        "surname": "Fu",
        "given_name": "Xiaohang"
      },
      {
        "surname": "Bi",
        "given_name": "Lei"
      },
      {
        "surname": "Kumar",
        "given_name": "Ashnil"
      },
      {
        "surname": "Fulham",
        "given_name": "Michael"
      },
      {
        "surname": "Kim",
        "given_name": "Jinman"
      }
    ]
  },
  {
    "title": "Recognizing micro actions in videos by learning multi-layer local features",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.002",
    "abstract": "Recognizing micro actions such as slight head shaking or hand clapping from videos can be challenging since they only involve small movements of local body parts. In this paper, we propose to fuse features from both higher-level and lower-level layers of convolutional neural networks for improving the accuracy of micro-action recognition. Deep features in higher-level layers have been shown to be effective in recognizing general actions, such as biking and jumping, that involve relatively large movements. Different from features in higher-level layers, features in lower-level layers are usually of higher resolution and can help capture small motions in micro actions. In this paper, we employ class-discriminative information as a guidance in lower-level layers to learn local features that are highly associated with micro-action regions. In the experiments, we evaluate the proposed method on two micro-action video datasets and achieve new state-of-the-art performance. We also test the proposed method on two general-action video datasets with promising performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000940",
    "keywords": [
      "Action (physics)",
      "Action recognition",
      "Artificial intelligence",
      "Chemistry",
      "Class (philosophy)",
      "Computer science",
      "Convolutional neural network",
      "Discriminative model",
      "Electrical engineering",
      "Engineering",
      "Fuse (electrical)",
      "Layer (electronics)",
      "Machine learning",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Mi",
        "given_name": "Yang"
      },
      {
        "surname": "Liu",
        "given_name": "Zhihao"
      },
      {
        "surname": "Zhao",
        "given_name": "Kai"
      },
      {
        "surname": "Wang",
        "given_name": "Song"
      }
    ]
  },
  {
    "title": "Fast data reduction by space partitioning via convex hull and MBR computation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108553",
    "abstract": "Large volumes of training data introduce high computational cost in instance-based classification. Data reduction algorithms select or generate a small (condensing) set of representative training prototypes from the available training data. The Reduction by Space Partitioning algorithm is one of the most well-known prototype generation algorithms that repetitively divides the original training data into subsets. This partitioning process needs to identify the diameter of each subset, i.e., its two farthest instances. This is a costly process since it requires the calculation of all distances between the instances in each subset. The paper introduces two new very fast variations that, instead of computing the actual diameter of a subset, choose a pair of distant-enough instances. The first variation uses instances belonging to an exact 3d convex hull of the subset, while the second one uses instances belonging to the minimum bounding rectangle of the subset. Our experimental study shows that the new variations vastly outperform the original algorithm without a penalty in classification accuracy and reduction rate.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000346",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bounding overwatch",
      "Computation",
      "Computer science",
      "Convex hull",
      "Data mining",
      "Data reduction",
      "Data set",
      "Geometry",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Programming language",
      "Rectangle",
      "Reduction (mathematics)",
      "Regular polygon",
      "Set (abstract data type)",
      "Space (punctuation)"
    ],
    "authors": [
      {
        "surname": "Giorginis",
        "given_name": "Thomas"
      },
      {
        "surname": "Ougiaroglou",
        "given_name": "Stefanos"
      },
      {
        "surname": "Evangelidis",
        "given_name": "Georgios"
      },
      {
        "surname": "Dervos",
        "given_name": "Dimitris A."
      }
    ]
  },
  {
    "title": "Multi-level augmented inpainting network using spatial similarity",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108547",
    "abstract": "Recently, multi-scale neural networks have shown promising improvements in image inpainting. However, most of them adopt the progressive way, in which the errors on lower scales may be propagated on higher scales. Addressing this issue, we propose a multi-level augmented inpainting network (MLA-Net) to rationally harmonize the inter- and intra-level contexts. Here, a pyramid reconstruction structure (PRS) with three parallel levels is designed to establish the inter-level relationship, which can boost the representation of the features by integrating the texture details into semantics. Then, we propose a novel spatial similarity based attention mechanism (SSA) to ensure the intra-level local continuity between the holes and related available patches. In SSA, in order to focus on the important textures and structures rather than calculating each pixel of the feature equally, a spatial map is utilized to highlight the corresponding spatial locations during the similarity computation. The experiments are evaluated on multiple challenging datasets, which demonstrate that MLA-Net can generate accurate results with better visual quality compared with the state-of-the-art methods. For the 256 × 256 Places2 dataset, PSNR increases 1.02 dB, while FID decreases 0.075. For the 256 × 256 CelebA-HQ dataset, there are 0.22 dB and 0.613 improvements in PSNR and FID.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000280",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Attention network",
      "Cartography",
      "Computation",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Focus (optics)",
      "Geography",
      "Geometry",
      "Image (mathematics)",
      "Inpainting",
      "Law",
      "Linguistics",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pixel",
      "Political science",
      "Politics",
      "Programming language",
      "Pyramid (geometry)",
      "Representation (politics)",
      "Scale (ratio)",
      "Semantics (computer science)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Qin",
        "given_name": "Jia"
      },
      {
        "surname": "Bai",
        "given_name": "Huihui"
      },
      {
        "surname": "Zhao",
        "given_name": "Yao"
      }
    ]
  },
  {
    "title": "SDUNet: Road extraction via spatial enhanced and densely connected UNet",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108549",
    "abstract": "Extracting road maps from high-resolution optical remote sensing images has received much attention recently, especially with the rapid development of deep learning methods. However, most of these CNN based approaches simply focused on multi-scale encoder architectures or multiple branches in neural networks, and ignored some inherent characteristics of the road surface. In this paper, we design a novel network for road extraction based on spatial enhanced and densely connected UNet, called SDUNet. SDUNet aggregates both the multi-level features and global prior information of road networks by combining the strengths of spatial CNN-based segmentation and densely connected blocks. To enhance the feature learning about prior information of road surface, a structure preserving model is designed to explore the continuous clues in the spatial level. Experimental results on two benchmark datasets show that the proposed method achieves the state-of-the-art performance, compared with previous approaches for road extraction. Code will be made available on https://github.com/MrStrangerYang/SDUNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000309",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cartography",
      "Code (set theory)",
      "Composite material",
      "Computer science",
      "Deep learning",
      "Encoder",
      "Feature (linguistics)",
      "Feature extraction",
      "Geography",
      "Linguistics",
      "Materials science",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Remote sensing",
      "Road surface",
      "Segmentation",
      "Set (abstract data type)",
      "Spatial analysis"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Mengxing"
      },
      {
        "surname": "Yuan",
        "given_name": "Yuan"
      },
      {
        "surname": "Liu",
        "given_name": "Ganchao"
      }
    ]
  },
  {
    "title": "AccLoc: Anchor-Free and two-stage detector for accurate object localization",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108523",
    "abstract": "Current anchor-free object detectors have obtained detection performances comparable to those of anchor-based object detectors while avoiding the weaknesses of anchor designs. However, two challenges limit the localization performance. First, such anchor-free detectors have one stage that predicts the classification and localization results directly. A large regression space reduces the localization performance of such methods. Second, most of the existing detectors extract features which are ineffective for accurate localization. In this paper, for the first challenge, we propose two-stage networks to predict regression results stage by stage, thereby reducing the scope of the prediction space. For the second challenge, we design two novel modules with the aim of extracting effective features for accurate localization. Experimental results validate that each module in our approach is effective and validate that our approach has better object localization performance than previous related and advanced methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000048",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Detector",
      "Limit (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Operating system",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Programming language",
      "Regression",
      "Scope (computer science)",
      "Space (punctuation)",
      "Stage (stratigraphy)",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Piao",
        "given_name": "Zhengquan"
      },
      {
        "surname": "Wang",
        "given_name": "Junbo"
      },
      {
        "surname": "Tang",
        "given_name": "Linbo"
      },
      {
        "surname": "Zhao",
        "given_name": "Baojun"
      },
      {
        "surname": "Wang",
        "given_name": "Wenzheng"
      }
    ]
  },
  {
    "title": "Universal predictive systems",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108536",
    "abstract": "This paper describes probability forecasting systems that are universal, or universally consistent, in the sense of being consistent under any data-generating distribution, assuming that the observations are produced independently in the IID fashion. The notion of universal consistency is asymptotic and does not imply any small-sample guarantees of validity. On the other hand, the method of conformal prediction has been recently adapted to producing predictive distributions that satisfy a natural property of small-sample validity, namely they are automatically probabilistically calibrated. The main result of the paper is the existence of universal conformal predictive systems, which output predictive distributions that are both probabilistically calibrated and universally consistent.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000176",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Conformal map",
      "Consistency (knowledge bases)",
      "Epistemology",
      "Mathematical analysis",
      "Mathematics",
      "Philosophy",
      "Probability distribution",
      "Property (philosophy)",
      "Sample (material)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Vovk",
        "given_name": "Vladimir"
      }
    ]
  },
  {
    "title": "DP- k -modes: A self-tuning k -modes clustering algorithm",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.026",
    "abstract": "The k -modes clustering algorithm was proposed by Huang for handling datasets with categorical attributes, however, the dissimilarity measure used limits its applicability. Ng et al. improved on Huang’s k -modes algorithm by proposing a new dissimilarity measure between objects. Moreover, both k -modes algorithms require the initial seeds to be randomly chosen and the number of clusters be specified manually. To overcome the limitations of Huang’s and Ng’s k -modes clustering algorithms, we first extend the clustering algorithm published in Science in 2014 (“clustering by fast search and find of density peaks”). The optimal initial seeds and the number of clusters of a dataset are determined simultaneously by taking the standard deviation as the self-tuning cutoff distance and the simple match dissimilarity as the distance measurement in the definition of the density of a point. A new dissimilarity measure is proposed to calculate the dissimilarities between objects to improve on that of Ng’s k -modes algorithm. The performance of our resulting self-tuning k -modes clustering algorithm was tested on nine datasets (three being relatively large) from the UCI (University of California in Irvine) machine learning repository. The clustering results were compared to those produced by Huang’s and Ng’s algorithms. Statistical tests of three k -modes algorithms were undertaken to determine whether or not there is significant difference between our self-tuning k -modes algorithm and Huang’s and Ng’s k -modes algorithms. All these experimental results demonstrate that our proposed k -modes clustering algorithm is superior to Hang’s and Ng’s k -modes algorithms in terms of clustering accuracy (ACC) and the well-known Adjusted Rand Index (ARI) metric. Our self-tuning k -modes algorithm is significantly different from both Huang’s and Ng’s k -modes algorithms, and there is no statistically significant difference between Ng’s and Huang’s k -modes algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001258",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Categorical variable",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Machine learning",
      "Mathematics",
      "Measure (data warehouse)"
    ],
    "authors": [
      {
        "surname": "Xie",
        "given_name": "Juanying"
      },
      {
        "surname": "Wang",
        "given_name": "Mingzhao"
      },
      {
        "surname": "Lu",
        "given_name": "Xiaoxiao"
      },
      {
        "surname": "Liu",
        "given_name": "Xinglin"
      },
      {
        "surname": "Grant",
        "given_name": "Philip W."
      }
    ]
  },
  {
    "title": "An efficient framework for zero-shot sketch-based image retrieval",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108528",
    "abstract": "Zero-shot sketch-based image retrieval (ZS-SBIR) has recently attracted the attention of the computer vision community due to its real-world applications, and the more realistic and challenging setting that it presents over SBIR. ZS-SBIR inherits the main challenges of multiple computer vision problems including content-based Image Retrieval (CBIR), zero-shot learning and domain adaptation. The majority of previous studies using deep neural networks have achieved improved results by either projecting sketch and images into a common low-dimensional space, or transferring knowledge from seen to unseen classes. However, those approaches are trained with complex frameworks composed of multiple deep convolutional neural networks (CNNs) and are dependent on category-level word labels. This increases the requirements for training resources and datasets. In comparison, we propose a simple and efficient framework that does not require high computational training resources, and learns the semantic embedding space from a vision model rather than a language model, as is done by related studies. Furthermore, at training and inference stages our method only uses a single CNN. In this work, a pre-trained ImageNet CNN (i.e., ResNet50) is fine-tuned with three proposed learning objects: domain-balanced quadruplet loss, semantic classification loss, and semantic knowledge preservation loss. The domain-balanced quadruplet and semantic classification losses are introduced to learn discriminative, semantic and domain invariant features by considering ZS-SBIR as an object detection and verification problem. To preserve semantic knowledge learned with ImageNet and exploit it for unseen categories, the semantic knowledge preservation loss is proposed. To reduce computational cost and increase the accuracy of the semantic knowledge distillation process, ground-truth semantic knowledge is prepared in a class-oriented fashion prior to training. Extensive experiments are conducted on three challenging ZS-SBIR datasets: Sketchy Extended, TU-Berlin Extended and QuickDraw Extended. The proposed method achieves state-of-the-art results, and outperforms the majority of related works by a substantial margin.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000097",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Contextual image classification",
      "Convolutional neural network",
      "Deep learning",
      "Discriminative model",
      "Domain (mathematical analysis)",
      "Domain knowledge",
      "Exploit",
      "Image (mathematics)",
      "Image retrieval",
      "Inference",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Sketch"
    ],
    "authors": [
      {
        "surname": "Tursun",
        "given_name": "Osman"
      },
      {
        "surname": "Denman",
        "given_name": "Simon"
      },
      {
        "surname": "Sridharan",
        "given_name": "Sridha"
      },
      {
        "surname": "Goan",
        "given_name": "Ethan"
      },
      {
        "surname": "Fookes",
        "given_name": "Clinton"
      }
    ]
  },
  {
    "title": "CSDA-Net: Seeking reliable correspondences by channel-Spatial difference augment network",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108539",
    "abstract": "Establishing reliable correspondences is a fundamental task in computer vision, and it requires rich contextual information. In this paper, we propose a Channel-Spatial Difference Augment Network (CSDA-Net), by selectively aggregating information from spatial and channel aspects, to seek reliable correspondences for feature matching. Specifically, we firstly introduce the spatial and channel attention mechanism to construct a simple yet effective block for discriminately extracting the global context. After that, we design a Overlay Attention block by further exploiting the spatial and channel attention mechanism with different squeeze operations, to gather more comprehensive contextual information. Finally, the proposed CSDA-Net is able to achieve feature maps with a strong representative ability for feature matching due to the integration of the two novel blocks. Extensive experiments on outlier rejection and relative pose estimation have shown better performance improvements of our CSDA-Net over current state-of-the-art methods on both outdoor and indoor datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000206",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Construct (python library)",
      "Context (archaeology)",
      "Engineering",
      "Feature (linguistics)",
      "Geography",
      "Geometry",
      "Linguistics",
      "Machine learning",
      "Matching (statistics)",
      "Mathematics",
      "Net (polyhedron)",
      "Outlier",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Spatial contextual awareness",
      "Statistics",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Shunxing"
      },
      {
        "surname": "Zheng",
        "given_name": "Linxin"
      },
      {
        "surname": "Xiao",
        "given_name": "Guobao"
      },
      {
        "surname": "Zhong",
        "given_name": "Zhen"
      },
      {
        "surname": "Ma",
        "given_name": "Jiayi"
      }
    ]
  },
  {
    "title": "Confidence intervals for the random forest generalization error",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.031",
    "abstract": "We show that the byproducts of the standard training process of a random forest yield not only the well known and almost computationally free out-of-bag point estimate of the model generalization error, but also open a direct path to compute confidence intervals for the generalization error which avoids processes of data splitting and model retraining. Besides the low computational cost involved in their construction, these confidence intervals are shown through simulations to have good coverage and appropriate shrinking rate of their width in terms of the training sample size.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001416",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Confidence interval",
      "Generalization",
      "Generalization error",
      "Geometry",
      "Mathematical analysis",
      "Mathematics",
      "Path (computing)",
      "Point (geometry)",
      "Point process",
      "Programming language",
      "Random forest",
      "Sample size determination",
      "Standard error",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Marques F．",
        "given_name": "Paulo C."
      }
    ]
  },
  {
    "title": "CDANet: Common-and-Differential Attention Network for Object Detection and Instance Segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.005",
    "abstract": "In this paper, we propose a simple and effective Common-and-Differential Attention Network (CDANet) for object detection and instance segmentation. For an input intermediate feature map, CDANet infers parallelly attention modules along channel and spatial dimensions respectively, then both attention modules are multiplied with the input feature map for the refined features. Specially, since redundant and confusing background may misdirect the localization at object boundary, our attention network applies Common-and-Differential operations to weaken useless background interference and focus on meaningful object features. The proposed CDANet is verified performance through comprehensive experiments on PASCAL VOC2007 and MS COCO2017 datasets for object detection and instance segmentation tasks. CDANet obtains consistently improved results on various detectors with different backbones, indicating the significant effectiveness and applicability of CDANet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522000952",
    "keywords": [
      "Aerospace engineering",
      "Artificial intelligence",
      "Attention network",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Differential (mechanical device)",
      "Engineering",
      "Feature (linguistics)",
      "Focus (optics)",
      "Linguistics",
      "Object (grammar)",
      "Object detection",
      "Optics",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Programming language",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yan"
      },
      {
        "surname": "Li",
        "given_name": "Yang"
      },
      {
        "surname": "Guo",
        "given_name": "Xiaohui"
      },
      {
        "surname": "Jiao",
        "given_name": "Licheng"
      },
      {
        "surname": "Liu",
        "given_name": "Xu"
      }
    ]
  },
  {
    "title": "An infodemiological framework for tracking the spread of SARS-CoV-2 using integrated public data",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.030",
    "abstract": "The outbreak of the SARS-CoV-2 novel coronavirus has caused a health crisis of immeasurable magnitude. Signals from heterogeneous public data sources could serve as early predictors for infection waves of the pandemic, particularly in its early phases, when infection data was scarce. In this article, we characterize temporal pandemic indicators by leveraging an integrated set of public data and apply them to a Prophet model to predict COVID-19 trends. An effective natural language processing pipeline was first built to extract time-series signals of specific articles from a news corpus. Bursts of these temporal signals were further identified with Kleinberg's burst detection algorithm. Across different US states, correlations for Google Trends of COVID-19 related terms, COVID-19 news volume, and publicly available wastewater SARS-CoV-2 measurements with weekly COVID-19 case numbers were generally high with lags ranging from 0 to 3 weeks, indicating them as strong predictors of viral spread. Incorporating time-series signals of these effective predictors significantly improved the performance of the Prophet model, which was able to predict the COVID-19 case numbers between one and two weeks with average mean absolute error rates of 0.38 and 0.46 respectively across different states",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001404",
    "keywords": [
      "2019-20 coronavirus outbreak",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Data mining",
      "Data set",
      "Disease",
      "Econometrics",
      "Infectious disease (medical specialty)",
      "Machine learning",
      "Mathematics",
      "Medicine",
      "Outbreak",
      "Paleontology",
      "Pandemic",
      "Pathology",
      "Pedagogy",
      "Pipeline (software)",
      "Programming language",
      "Psychology",
      "Ranging",
      "Series (stratigraphy)",
      "Set (abstract data type)",
      "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)",
      "Statistics",
      "Telecommunications",
      "Time series",
      "Tracking (education)",
      "Virology"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Zhimin"
      },
      {
        "surname": "Jiang",
        "given_name": "Zuodong"
      },
      {
        "surname": "Kip",
        "given_name": "Geoffrey"
      },
      {
        "surname": "Snigdha",
        "given_name": "Kirti"
      },
      {
        "surname": "Xu",
        "given_name": "Jennings"
      },
      {
        "surname": "Wu",
        "given_name": "Xiaoying"
      },
      {
        "surname": "Khan",
        "given_name": "Najat"
      },
      {
        "surname": "Schultz",
        "given_name": "Timothy"
      }
    ]
  },
  {
    "title": "Discriminative feature selection with directional outliers correcting for data classification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108541",
    "abstract": "With the rapid development of multimedia technologies (e.g. deep learning), Feature Selection (FS) is now playing a critical role in acquiring discriminative features from massive data. Traditional FS methods score feature importance and select the top best features by treating all instances equally; Hence, valuable instances like directional outliers (DOs), which are specific outliers closer to other class centres than to their owns, seldom receive particular attention during feature selection. Based on our observation, DOs derive from “misclassified instances” which lead to misclassification. In this paper, we present a novel supervised feature selection method entitled Feature Selection via Directional Outliers Correcting (FSDOC), for accurate data classification. The proposed FSDOC includes an optimization algorithm to capture DOs, and two correcting algorithms to reasonably capture redundant features by correcting DOs with intraclass deviation minimization and interclass relative distance maximization. We give theoretical guarantees and adequate analysis on all algorithms to show the effectiveness of FSDOC. Extensive experiments on fifteen public datasets, and two case studies of deep features and very-high dimensional Fisher Vector selection, demonstrate the superior performance of FSDOC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200022X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Dimensionality reduction",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature selection",
      "Linear discriminant analysis",
      "Linguistics",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Outlier",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Yuan",
        "given_name": "Lixin"
      },
      {
        "surname": "Yang",
        "given_name": "Guoqiang"
      },
      {
        "surname": "Xu",
        "given_name": "Qian"
      },
      {
        "surname": "Lu",
        "given_name": "Tong"
      }
    ]
  },
  {
    "title": "Geometric imbalanced deep learning with feature scaling and boundary sample mining",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108564",
    "abstract": "Data imbalance is a significant factor affecting classification performance in computer vision. In particular, data imbalance is harmful to classification learning and representation learning. To address this issue, this paper proposes a geometric deep learning framework combined with Feature Scaling Module (FSM) and Boundary Samples Mining Module (BSMM). Considering the geometric information in sample distributions of training samples, FSM is proposed to scale the features by hypersphere radius of each class, which improves the representation ability of minority classes. Meanwhile, it is noteworthy that the relationships and information between samples are essential for classification. Therefore, BSMM is proposed to mine the boundary samples by Gabriel Graph that takes the relationships into account. Finally, a loss scheduler is designed to adjust the training process of these two modules. With the scheduler, the model first learns representation and then focuses more on minority classes gradually. Extensive experiments on three benchmark datasets demonstrate the advantages of the proposed learning framework over the state-of-the-art models for solving the imbalance problem.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000450",
    "keywords": [
      "Artificial intelligence",
      "Boundary (topology)",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Feature (linguistics)",
      "Geometry",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Sample (material)",
      "Scaling"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zhe"
      },
      {
        "surname": "Dong",
        "given_name": "Qida"
      },
      {
        "surname": "Guo",
        "given_name": "Wei"
      },
      {
        "surname": "Li",
        "given_name": "Dongdong"
      },
      {
        "surname": "Zhang",
        "given_name": "Jing"
      },
      {
        "surname": "Du",
        "given_name": "Wenli"
      }
    ]
  },
  {
    "title": "Transferring discriminative knowledge via connective momentum clustering on person re-identification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108569",
    "abstract": "Unsupervised domain adaptation in person re-identification remains a challenge to learning discriminative representations due to the absence of labels in target domain. Clustering could provide pseudo-labels, but the limitation mainly comes from imperfect clustering and noisy pseudo-labels. To address this drawback, we propose Connective Momentum Clustering (CMC) framework to build a connection estimator via graph convolutional networks to transfer rich connection knowledge from the annotation space of source data to target domain. It estimates connections from context to reveal relationship between unlabeled data and helps to discover more reliable clusters. With momentum mechanism, stable pseudo-labels are updated iteratively with confidence and refined consistently to encourage more discriminative networks. Meanwhile, we notice that the huge domain gap between source and target domains results in severe pollution in BatchNorm layers. To tackle this problem, we normalize the data stream separately to decouple different distribution and further boost the performance in target domain. We adopt our CMC framework on mainstream tasks and achieves 80.2% mAP / 91.3% Rank-1 on Duke → Market task and 70.4% mAP / 82.4% Rank-1 on Market → Duke task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000504",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Discriminative model",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Yichen"
      },
      {
        "surname": "Deng",
        "given_name": "Weihong"
      }
    ]
  },
  {
    "title": "Video super-resolution via mixed spatial-temporal convolution and selective fusion",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108577",
    "abstract": "Video super-resolution aims to recover the high-resolution (HR) contents from the low-resolution (LR) observations relying on compositing the spatial-temporal information in the LR frames. It is crucial to model the spatial-temporal information jointly since the video sequences are three-dimensional spatial-temporal signals. Compared with explicitly estimating motions between the 2D frames, 3D convolutional neural networks (CNNs) have been shown its efficiency and effectiveness for video super-resolution (SR), as a natural way of spatial-temporal data modelling. Though promising, the performance of 3D CNNs is still far from satisfactory. The high computational and memory requirements limit the development of more advanced designs to extract and fuse the information from a larger spatial and temporal scale. We thus propose a Mixed Spatial-Temporal Convolution (MSTC) block that simultaneously extracts the spatial information and the supplemented temporal dependency among frames by jointly applying 2D and 3D convolution. To further fuse the learned features corresponding to different frames, we propose a novel similarity-based selective features strategy, unlike precious methods directly stacking the learned features. Additionally, an attention-based motion compensation module is applied to alleviate the influence of misalignment between frames. Experiments on three widely used benchmark datasets and real-world dataset show that, relying on superior feature extraction and fusion ability, the proposed network can outperform previous state-of-the-art methods, especially for recovering the confusing details.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000589",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Fusion",
      "Image resolution",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Resolution (logic)"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Wei"
      },
      {
        "surname": "Gong",
        "given_name": "Dong"
      },
      {
        "surname": "Shi",
        "given_name": "Javen Qinfeng"
      },
      {
        "surname": "van den Hengel",
        "given_name": "Anton"
      },
      {
        "surname": "Zhang",
        "given_name": "Yanning"
      }
    ]
  },
  {
    "title": "Zero-shot semantic segmentation via spatial and multi-scale aware visual class embedding",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.011",
    "abstract": "Fully supervised semantic segmentation technologies bring a paradigm shift in scene understanding. However, the burden of expensive labeling cost remains as a challenge. To solve the cost problem, recent studies proposed language model based zero-shot semantic segmentation (L-ZSSS) approaches. In this paper, we address L-ZSSS has a limitation in generalization which is a virtue of zero-shot learning. Tackling the limitation, we propose a language-model-free zero-shot semantic segmentation framework, Spatial and Multi-scale aware Visual Class Embedding Network (SM-VCENet). Furthermore, leveraging vision-oriented class embedding SM-VCENet enriches visual information of the class embedding by multi-scale attention and spatial attention. We also propose a novel benchmark (PASCAL2COCO) for zero-shot semantic segmentation, which provides generalization evaluation by domain adaptation and contains visually challenging samples. In experiments, our SM-VCENet outperforms zero-shot semantic segmentation state-of-the-art by a significant margin in both PASCAL- 5 i and PASCAL2COCO benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001076",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Embedding",
      "Generalization",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematical analysis",
      "Mathematics",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Cha",
        "given_name": "Sungguk"
      },
      {
        "surname": "Wang",
        "given_name": "Yooseung"
      }
    ]
  },
  {
    "title": "Differentiable neural architecture learning for efficient neural networks",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108448",
    "abstract": "Efficient neural networks has received ever-increasing attention with the evolution of convolutional neural networks (CNNs), especially involving their deployment on embedded and mobile platforms. One of the biggest problems to obtaining such efficient neural networks is efficiency, even recent differentiable neural architecture search (DNAS) requires to sample a small number of candidate neural architectures for the selection of the optimal neural architecture. To address this computational efficiency issue, we introduce a novel architecture parameterization based on scaled sigmoid function, and propose a general Differentiable Neural Architecture Learning (DNAL) method to obtain efficient neural networks without the need to evaluate candidate neural networks. Specifically, for stochastic supernets as well as conventional CNNs, we build a new channel-wise module layer with the architecture components controlled by a scaled sigmoid function. We train these neural network models from scratch. The network optimization is decoupled into the weight optimization and the architecture optimization, which avoids the interaction between the two types of parameters and alleviates the vanishing gradient problem. We address the non-convex optimization problem of efficient neural networks by the continuous scaled sigmoid method instead of the common softmax method. Extensive experiments demonstrate our DNAL method delivers superior performance in terms of efficiency, and adapts to conventional CNNs (e.g., VGG16 and ResNet50), lightweight CNNs (e.g., MobileNetV2) and stochastic supernets (e.g., ProxylessNAS). The optimal neural networks learned by DNAL surpass those produced by the state-of-the-art methods on the benchmark CIFAR-10 and ImageNet-1K dataset in accuracy, model size and computational complexity. Our source code is available at https://github.com/QingbeiGuo/DNAL.git.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006245",
    "keywords": [
      "Archaeology",
      "Architecture",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Differentiable function",
      "Geography",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Time delay neural network"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Qingbei"
      },
      {
        "surname": "Wu",
        "given_name": "Xiao-Jun"
      },
      {
        "surname": "Kittler",
        "given_name": "Josef"
      },
      {
        "surname": "Feng",
        "given_name": "Zhiquan"
      }
    ]
  },
  {
    "title": "A hierarchical receptive network oriented to target recognition in SAR images",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108558",
    "abstract": "The recent years have witnessed a resurgence on neural network. Many functional layers are stacked hierarchically to learn the high-level representations. Yet the large album of radar image with label information are scarce. The fitting power of deep architectures are therefore limited. Additionally, the coherent imaging mechanism inevitably produce many speckles. They are with the statistical specificity of multiplicative noise, and hence make the image interpretation difficult. To solve the problems, this paper presents a new hierarchical receptive neural network. A signal-wise receptive module is first built by a family of delicate convolutional filters, with which the empirical features and knowledge are encoded. The receptive features are further refined in a patch-wise receptive unit, where some convolutional blocks are configured sequentially. The refined representations are finally used to make the inference. Multiple comparative studies are performed to demonstrate the advantage of proposed strategy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000395",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Inference",
      "Pattern recognition (psychology)",
      "Receptive field"
    ],
    "authors": [
      {
        "surname": "Dong",
        "given_name": "Ganggang"
      },
      {
        "surname": "Liu",
        "given_name": "Hongwei"
      }
    ]
  },
  {
    "title": "Scene captioning with deep fusion of images and point clouds",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.017",
    "abstract": "Recently, the fusion of images and point clouds has received appreciable attentions in various fields, for example, autonomous driving, whose advantage over single-modal vision has been verified. However, it has not been extensively exploited in the scene captioning task. In this paper, a novel scene captioning framework with deep fusion of images and point clouds based on region correlation and attention is proposed to improve performances of captioning models. In our model, a symmetrical processing pipeline is designed for point clouds and images. First, 3D and 2D region features are generated respectively through region proposal generation, proposal fusion, and region pooling modules. Then, a feature fusion module is designed to integrate features according to the region correlation rule and the attention mechanism, which increases the interpretability of the fusion process and results in a sequence of fused visual features. Finally, the fused features are transformed into captions by an attention-based caption generation module. Comprehensive experiments indicate that the performance of our model reaches the state of the art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001180",
    "keywords": [
      "Artificial intelligence",
      "Closed captioning",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Fusion",
      "Fusion mechanism",
      "Geometry",
      "Image (mathematics)",
      "Interpretability",
      "Linguistics",
      "Lipid bilayer fusion",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pipeline (software)",
      "Point (geometry)",
      "Point cloud",
      "Process (computing)",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Qiang"
      },
      {
        "surname": "Zhang",
        "given_name": "Chunxia"
      },
      {
        "surname": "Weng",
        "given_name": "Lubin"
      },
      {
        "surname": "Xiang",
        "given_name": "Shiming"
      },
      {
        "surname": "Pan",
        "given_name": "Chunhong"
      }
    ]
  },
  {
    "title": "Enhanced nuclear norm based matrix regression for occluded face recognition",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108585",
    "abstract": "An effective approach for the task of face recognition is proposed in this paper, which formulates the problem as an enhanced nuclear norm based matrix regression model and explores the low-rank property of the reconstructed image. Previous works have already leveraged the nuclear norm to obtain a low-rank representation of the error image and get a promising recognition rate. Motivated by the low-rank property of the reconstructed image through theoretical observation, our model imposes the nuclear norm constraints not only on the representation residual but also on the reconstructed image. The proposed method preserves the 2D structural information of the error images and reconstructs images, which is significant for the face recognition tasks. To further improve the performance of the proposed model, we explore the impact of different regularization terms under various scenarios. Extensive experiments on several benchmark datasets show the efficacy of the proposed model especially in terms of robustness against contiguous occlusion and illumination changes, which achieves superior performance over the most competitive methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000668",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Eigenvalues and eigenvectors",
      "Facial recognition system",
      "Gene",
      "Law",
      "Mathematics",
      "Matrix norm",
      "Norm (philosophy)",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Quantum mechanics",
      "Regression",
      "Residual",
      "Robustness (evolution)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Qin"
      },
      {
        "surname": "He",
        "given_name": "Huihui"
      },
      {
        "surname": "Lai",
        "given_name": "Hong"
      },
      {
        "surname": "Cai",
        "given_name": "Tie"
      },
      {
        "surname": "Wang",
        "given_name": "Qianqian"
      },
      {
        "surname": "Gao",
        "given_name": "QuanXue"
      }
    ]
  },
  {
    "title": "D2T: A Framework For transferring detection to tracking",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108544",
    "abstract": "Object detection methods draw increasing attention in deep learning based visual tracking algorithms due to their robust discrimination and powerful regression ability. To further explore the potential of object detection methods in the visual tracking task, there are two gaps that need to be bridged. The first is the difference in object definition. Object detection is class-specific while visual tracking is class-agnostic. Moreover, visual tracking needs to differentiate the target from intra-class distractors. The second is the difference in temporal dimension. Different from object detection which processes still-image, visual tracking concentrates on objects which vary continuously with time. In this paper, we propose a Detection to Tracking (D2T) framework to address the above issues and effectively transfer existing advanced detection methods to visual tracking task. Specifically, to bridge the gap of object definition, we propose a general-to-specific network that separates learning general object features and instance-level features. To make full use of the contextual information while adapting to the appearance variation of targets, we propose a temporal strategy combining short-term constraint and long-term updating. To the best of our knowledge, our D2T framework is the first universal framework which directly transfers deep learning based object detectors to visual tracking task. It provides a novel solution to visual object tracking, and it achieves superior performance in several public datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000255",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Cognitive neuroscience of visual object recognition",
      "Computer science",
      "Computer vision",
      "Constraint (computer-aided design)",
      "Economics",
      "Eye tracking",
      "Geometry",
      "Management",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Psychology",
      "Task (project management)",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Qin",
        "given_name": "Huai"
      },
      {
        "surname": "Yu",
        "given_name": "Changqian"
      },
      {
        "surname": "Gao",
        "given_name": "Changxin"
      },
      {
        "surname": "Sang",
        "given_name": "Nong"
      }
    ]
  },
  {
    "title": "Decomposing generation networks with structure prediction for recipe generation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108578",
    "abstract": "Recipe generation from food images and ingredients is a challenging task, which requires the interpretation of the information from another modality. Different from the image captioning task, where the captions usually have one sentence, cooking instructions contain multiple sentences and have obvious structures. To help the model capture the recipe structure and avoid missing some cooking details, we propose a novel framework: Decomposing Generation Networks (DGN) with structure prediction, to get more structured and complete recipe generation outputs. Specifically, we split each cooking instruction into several phases, and assign different sub-generators to each phase. Our approach includes two novel ideas: (i) learning the recipe structures with the global structure prediction component and (ii) producing recipe phases in the sub-generator output component based on the predicted structure. Extensive experiments on the challenging large-scale Recipe1M dataset validate the effectiveness of our proposed model, which improves the performance over the state-of-the-art results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000590",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Food science",
      "Mathematics",
      "Recipe"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Hao"
      },
      {
        "surname": "Lin",
        "given_name": "Guosheng"
      },
      {
        "surname": "Hoi",
        "given_name": "Steven C.H."
      },
      {
        "surname": "Miao",
        "given_name": "Chunyan"
      }
    ]
  },
  {
    "title": "Iterative ridge regression using the aggregating algorithm",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.021",
    "abstract": "In this paper, regularised regression for sequential data is investigated and a new ridge regression algorithm is proposed. It uses the Aggregating Algorithm (AA) to devise an iterative version of ridge regression (IRR). This algorithm is called AAIRR. A competitive analysis is conducted to show that the guarantee on the performance of AAIRR is better than that of the known online ridge regression algorithms. Moreover, an empirical study is carried out on real-world datasets to demonstrate the superior performance over those state-of-the-art algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001209",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Elastic net regularization",
      "Local regression",
      "Machine learning",
      "Mathematics",
      "Paleontology",
      "Polynomial regression",
      "Regression",
      "Regression analysis",
      "Ridge",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Jamil",
        "given_name": "Waqas"
      },
      {
        "surname": "Bouchachia",
        "given_name": "Abdelhamid"
      }
    ]
  },
  {
    "title": "Table detection in business document images by message passing networks",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108641",
    "abstract": "Tabular structures in business documents offer a complementary dimension to the raw textual data. For instance, there is information about the relationships among pieces of information. Nowadays, digital mailroom applications have become a key service for workflow automation. Therefore, the detection and interpretation of tables is crucial. With the recent advances in information extraction, table detection and recognition has gained interest in document image analysis, in particular, with the absence of rule lines and unknown information about rows and columns. However, business documents usually contain sensitive contents limiting the amount of public benchmarking datasets. In this paper, we propose a graph-based approach for detecting tables in document images which do not require the raw content of the document. Hence, the sensitive content can be previously removed and, instead of using the raw image or textual content, we propose a purely structural approach to keep sensitive data anonymous. Our framework uses graph neural networks (GNNs) to describe the local repetitive structures that constitute a table. In particular, our main application domain are business documents. We have carefully validated our approach in two invoice datasets and a modern document benchmark. Our experiments demonstrate that tables can be detected by purely structural approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001224",
    "keywords": [
      "Artificial intelligence",
      "Benchmarking",
      "Business",
      "Computer science",
      "Data mining",
      "Database",
      "Information extraction",
      "Information retrieval",
      "Marketing",
      "Row",
      "Table (database)",
      "Workflow"
    ],
    "authors": [
      {
        "surname": "Riba",
        "given_name": "Pau"
      },
      {
        "surname": "Goldmann",
        "given_name": "Lutz"
      },
      {
        "surname": "Terrades",
        "given_name": "Oriol Ramos"
      },
      {
        "surname": "Rusticus",
        "given_name": "Diede"
      },
      {
        "surname": "Fornés",
        "given_name": "Alicia"
      },
      {
        "surname": "Lladós",
        "given_name": "Josep"
      }
    ]
  },
  {
    "title": "Making person search enjoy the merits of person re-identification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108654",
    "abstract": "Person search is an extended task of person re-identification (Re-ID). However, most existing one-step person search works do not study how to employ existing Re-ID models to improve the one-step person search. To address this issue, we propose a Teacher-guided Disentangling Network (TDN) to make the one-step person search enjoy the merits of existing Re-ID research. The proposed TDN can significantly boost person search performance by transferring the advanced person Re-ID knowledge to the person search model. In the proposed TDN, for better knowledge transfer from the Re-ID teacher model to the one-step person search model, we design a new one-step person search base framework by partially disentangling the two subtasks. Besides, we propose a Knowledge Transfer Bridge module to bridge the scale gap caused by different input formats between the Re-ID model and the one-step person search model. Moreover, we also propose a Ranking with Context Persons strategy to exploit the context information in panoramic images for better ranking. Experiments on two public person search datasets demonstrate the favorable performance of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001352",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Bridge (graph theory)",
      "Computer science",
      "Computer security",
      "Context (archaeology)",
      "Data mining",
      "Engineering",
      "Exploit",
      "Identification (biology)",
      "Information retrieval",
      "Internal medicine",
      "Machine learning",
      "Medicine",
      "Paleontology",
      "Ranking (information retrieval)",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Chuang"
      },
      {
        "surname": "Yang",
        "given_name": "Hua"
      },
      {
        "surname": "Zhou",
        "given_name": "Qin"
      },
      {
        "surname": "Zheng",
        "given_name": "Shibao"
      }
    ]
  },
  {
    "title": "Cross lingual handwritten character recognition using long short term memory network with aid of elephant herding optimization algorithm",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.038",
    "abstract": "In the recent decades, the handwritten character recognition is still a challenging process in the pattern recognition field. The handwritten digits and characters are not always of the similar width, orientation and size, due to different writing instruments and writing style of the individuals. This makes the handwritten recognition a tricky and tough task. In this manuscript, a new deep learning based model is developed for an automatic character recognition. Initially, the handwritten images are acquired from Chars74K and MADbase digits datasets, and then data pre-processing is carried out using Gaussian filtering and skew detection techniques. Additionally, the individual lines and characters are segmented from the denoised images by utilizing projection profile and thresholding techniques. In addition, the Inverse Difference Moment Normalized (IDMN) and Enhanced Local Binary Pattern (ELBP) descriptors are applied to extract features from the segmented images, and then the discriminative features are selected by employing Elephant Herding Optimization (EHO) algorithm. Lastly, the Long Short Term Memory (LSTM) network utilizes the selected features to classify 64 classes in English language, 10 classes in Arabic language, and 657 classes in Kannada language. Simulation outcomes confirmed that the proposed EHO-LSTM model obtained better performance in handwritten character recognition related to the comparative models. The proposed EHO-LSTM model achieved 96.66%, 96.67%, and 99.93% of accuracy in English, Kannada and Arabic character recognition on chars74K and MADbase digits datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001490",
    "keywords": [
      "Artificial intelligence",
      "Character (mathematics)",
      "Character recognition",
      "Computer science",
      "Discriminative model",
      "Geometry",
      "Image (mathematics)",
      "Intelligent character recognition",
      "Intelligent word recognition",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Skew",
      "Speech recognition",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Guptha",
        "given_name": "Nirmala S"
      },
      {
        "surname": "Balamurugan",
        "given_name": "V."
      },
      {
        "surname": "Megharaj",
        "given_name": "Geetha"
      },
      {
        "surname": "Sattar",
        "given_name": "Khalid Nazim Abdul"
      },
      {
        "surname": "Rose",
        "given_name": "J. Dhiviya"
      }
    ]
  },
  {
    "title": "2K-Fold-Net and feature enhanced 4-Fold-Net for medical image segmentation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108625",
    "abstract": "For segmenting medical images, U-Net has become a popular and effective tool. However, it also has some shortcomings in segmenting fuzzy boundaries and eliminating interferences. Improvements of the original U-Net have been proposed by many authors, resulting in many variants such as MultiResUNet, DoubleU-Net and W-Net. Based on the common characteristics of these structures, we propose in this work a generalized structure by multiplying the folds of a fully convolutional network (FCN) for even more times, and thus name it as “2K-Fold-Net”. The more folds in this structure provide more freedoms to create cross links between the neighboring folds. The influence of the fold-pair number K on its performance is also studied. The realizations with K up to 6 are compared to three other variants of cascaded U-Nets using the CVC-ClinicDB dataset. Then the special case “4-Fold-Net” is further empowered with the feature enhancing functionalities recently seen in the attention-aware feature enhancement method. This new net is hence named as “Enhanced-Feature-4-Fold-Net”, abbreviated as “EF 3 -Net”. Finally, 2K-Fold-Net and EF 3 -Net have been compared with U-Net, SegNet, DoubleU-Net, MultiResUNet and its variants using four challenging medical image datasets. The results have demonstrated that the proposed nets outperform the other variants of U-Net, even with slightly lower amount of parameters. The code is available on: https://github.com/raik7/EF3-Net.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001066",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Elastic net regularization",
      "Feature (linguistics)",
      "Feature selection",
      "Fold (higher-order function)",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Net (polyhedron)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Segmentation",
      "The Internet",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yunchu"
      },
      {
        "surname": "Dong",
        "given_name": "Jianfei"
      }
    ]
  },
  {
    "title": "Learning scene-specific object detectors based on a generative-discriminative model with minimal supervision",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.05.007",
    "abstract": "One object class may show large variations due to diverse illuminations, backgrounds, and camera viewpoints in the multi-scene object detection task. Traditional object detection methods generally perform poorly under unconstrained video environments. To address this problem, many modern approaches provide deep hierarchical appearance representations for object detection. Most of these methods require time-consuming training procedures on large manually annotated sample sets. In this paper, we propose a self-learning object detection framework to resolve the multi-scene detection problem in a bottom-up manner. A scene-specific objector is obtained from an autonomous learning process triggered by marking several bounding boxes around an object in the first video frame via a mouse. Here, artificially labeled training data or generic detectors are not needed. This learning process is conveniently replicated many times in different surveillance scenarios and produces scene-specific detectors from various camera viewpoints. Obviously, the initial scene-specific detector, initialized by several bounding boxes, exhibits poor detection performance and is difficult to be improved by traditional online learning algorithms. Consequently, we propose the Generative-Discriminative model (GDM) based detection method to partition detection response space and assign each partition an individual descriptor that progressively achieves high classification accuracy. Online gradual optimization process is proposed to optimize the Generative-Discriminative model and focus on those hard samples lying near the decision boundary. Experimental results on nine video datasets show that our approach achieves comparable performance to that of robust supervised methods, and outperforms state-of-the-art scene-specific object detection methods under varying imaging conditions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552200160X",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Bounding overwatch",
      "Computer science",
      "Computer vision",
      "Detector",
      "Discriminative model",
      "Generative grammar",
      "Generative model",
      "Machine learning",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Telecommunications",
      "Viewpoints",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Dapeng"
      },
      {
        "surname": "Lei",
        "given_name": "Siyuan"
      },
      {
        "surname": "Guo",
        "given_name": "Peng"
      },
      {
        "surname": "Gao",
        "given_name": "Changxin"
      },
      {
        "surname": "Chen",
        "given_name": "Ying"
      },
      {
        "surname": "Li",
        "given_name": "Jinsheng"
      },
      {
        "surname": "Wei",
        "given_name": "Longsheng"
      }
    ]
  },
  {
    "title": "An optimized recommendation framework exploiting textual review based opinion mining for generating pleasantly surprising, novel yet relevant recommendations",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.05.003",
    "abstract": "Serendipity is a critical factor in the Recommender Systems (RS) in delivering pleasantly surprising, novel, yet contextually relevant recommendations. Most existing methods improve serendipity in RS by learning user preferences based on item popularity or similarity. However, the effectiveness of these methods in mitigating popularity bias and generating novel and unexpected item recommendations remains poorly understood. Recent studies suggest improvement in user preference by incorporating textual opinion provided by the user on an item. Additionally, the trade-off relationship between serendipity's conflicting components, including relevance, novelty, and unexpectedness, warrants further investigation to improve the quality of top-n recommendations. Hence, this research proposes an opinion mining-based approach to learn the users' personalized preferences from the textual reviews and incorporate both rating and reviews preferences to improve the quality of the recommendation list. Next, we design a new Two-Fold Algorithmic (TFA) approach-based objective function for serendipity to mitigate the popularity bias by aggregating uncertainty based on item popularity and item similarity to user preferences. Lastly, a multi-objective evolutionary algorithm-based Serendipity Objective Optimization-based Recommendation Framework(SOORF) is designed to optimize the serendipity's conflicting components. Extensive simulations are conducted over four benchmark datasets. The Mean Absolute Precision(MAP)@n and Serendipity@n based evaluation findings of SOORF and TFA demonstrate an improvement of at least 8.10% and 58.48%, respectively. The Precision@n and Recall@n based evaluations on different dataset sparsity conditions are observed with an improvement of at least 13.59% and 27.73% over baseline models. The Pareto front shows the models' ability to generate surprising, novel, yet relevant recommendations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001568",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Epistemology",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Information retrieval",
      "Law",
      "Machine learning",
      "Mathematics",
      "Novelty",
      "Philosophy",
      "Political science",
      "Popularity",
      "Preference",
      "Psychology",
      "Quality (philosophy)",
      "Recommender system",
      "Relevance (law)",
      "Serendipity",
      "Similarity (geometry)",
      "Social psychology",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Shrivastava",
        "given_name": "Rahul"
      },
      {
        "surname": "Sisodia",
        "given_name": "Dilip Singh"
      },
      {
        "surname": "Nagwani",
        "given_name": "Naresh Kumar"
      },
      {
        "surname": "BP",
        "given_name": "Upendra Roy"
      }
    ]
  },
  {
    "title": "Nested conformal prediction and quantile out-of-bag ensemble methods",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2021.108496",
    "abstract": "Conformal prediction is a popular tool for providing valid prediction sets for classification and regression problems, without relying on any distributional assumptions on the data. While the traditional description of conformal prediction starts with a nonconformity score, we provide an alternate (but equivalent) view that starts with a sequence of nested sets and calibrates them to find a valid prediction set. The nested framework subsumes all nonconformity scores, including recent proposals based on quantile regression and density estimation. While these ideas were originally derived based on sample splitting, our framework seamlessly extends them to other aggregation schemes like cross-conformal, jackknife+ and out-of-bag methods. We use the framework to derive a new algorithm (QOOB, pronounced cube) that combines four ideas: quantile regression, cross-conformalization, ensemble methods and out-of-bag predictions. We develop a computationally efficient implementation of cross-conformal, that is also used by QOOB. In a detailed numerical investigation, QOOB performs either the best or close to the best on all simulated and real datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320321006725",
    "keywords": [
      "Code (set theory)",
      "Computer science",
      "Conformal map",
      "Data mining",
      "Engineering",
      "Estimator",
      "Jackknife resampling",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Nonconformity",
      "Operations management",
      "Programming language",
      "Quantile",
      "Quantile regression",
      "Regression",
      "Set (abstract data type)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Gupta",
        "given_name": "Chirag"
      },
      {
        "surname": "Kuchibhotla",
        "given_name": "Arun K."
      },
      {
        "surname": "Ramdas",
        "given_name": "Aaditya"
      }
    ]
  },
  {
    "title": "LiTMNet: A deep CNN for efficient HDR image reconstruction from a single LDR image",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108620",
    "abstract": "Existing methods can generate a high dynamic range (HDR) image from a single low dynamic range (LDR) image using convolutional neural networks (CNNs). However, they are too cumbersome to run on mobile devices with limited computational resources. In this work, we design a lightweight CNN, namely LiTMNet which takes a single LDR image as input and recovers the lost information in its saturated regions to reconstruct an HDR image. To avoid trading off the reconstruction quality for efficiency, LiTMNet does not only adapt a lightweight encoder for efficient feature extraction, but also contains newly designed upsampling blocks in the decoder to alleviate artifacts and further accelerate the reconstruction. The final HDR image is produced by nonlinearly blending the network prediction and the original LDR image. Qualitative and quantitative comparisons demonstrate that LiTMNet produces HDR images of high quality comparable with the current state of the art and is 38 × faster as tested on a mobile device. Please refer to the supplementary video for additional visual results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001017",
    "keywords": [
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Image (mathematics)"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Guotao"
      },
      {
        "surname": "Song",
        "given_name": "Ran"
      },
      {
        "surname": "Zhang",
        "given_name": "Mingxin"
      },
      {
        "surname": "Li",
        "given_name": "Xiaolei"
      },
      {
        "surname": "Rosin",
        "given_name": "Paul L."
      }
    ]
  },
  {
    "title": "Impact of metrics on biclustering solution and quality: A review",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108612",
    "abstract": "To understand how subspace clustering algorithms discover distinct bicluster types and how their effectiveness has been validated, we offer a systematic literature review on available merit functions and how they affect the biclustering task. The covered principles are structured within a methodology to show how evaluation and validation measures/metrics determine the bicluster coherence, ensuring the algorithm effectiveness, and the limitations reported in some selected works. The review did not find any metrics that can be used in a generic way to guarantee the effectiveness of a biclustering algorithm when compared to all others. Therefore, the choice of evaluation metrics must meet to specific objectives of the application. So in this work, we present the measures and metrics in 7 major classes, including metrics based on residues, score thresholding, plaid, and order-preserving constraints, space transforms, correlations, theoretical and probabilistic frames, and set operations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000930",
    "keywords": [
      "Artificial intelligence",
      "Biclustering",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Economics",
      "Epistemology",
      "Image (mathematics)",
      "Machine learning",
      "Management",
      "Philosophy",
      "Probabilistic logic",
      "Programming language",
      "Quality (philosophy)",
      "Set (abstract data type)",
      "Subspace topology",
      "Task (project management)",
      "Thresholding"
    ],
    "authors": [
      {
        "surname": "Noronha",
        "given_name": "Marta D.M."
      },
      {
        "surname": "Henriques",
        "given_name": "Rui"
      },
      {
        "surname": "Madeira",
        "given_name": "Sara C."
      },
      {
        "surname": "Zárate",
        "given_name": "Luis E."
      }
    ]
  },
  {
    "title": "SA-DPNet: Structure-aware dual pyramid network for salient object detection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108624",
    "abstract": "Salient object detection aims at highlighting the most visually distinctive objects in the scene. Previous deep learning based works mainly focus on designing different integration strategies of multi-level features to improve the quality of prediction. However, due to the negligence of spatial structure coherence in predicted saliency maps, they fail to produce satisfactory results in complex scenarios. In this work, we present a structure-aware dual pyramid network (SA-DPNet) for salient object detection. By explicitly formulating spatial location information and spatial covariance features into the self-attention mechanism, a structure-aware spatial non-local block is proposed in SA-DPNet to learn the spatial-sensitive global context. With the proposed edge loss and adversarial loss, the edge structure context and patch-based global structure context are introduced to refine the structural coherence of the predicted results. Comprehensive experimental results on six RGB saliency benchmark datasets and three RGB-D saliency benchmark datasets demonstrate the superiority of proposed SA-DPNet over other state-of-the-art methods, both quantitatively and visually.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001054",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Coherence (philosophical gambling strategy)",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Encoder",
      "Enhanced Data Rates for GSM Evolution",
      "Focus (optics)",
      "Geodesy",
      "Geography",
      "Geometry",
      "Mathematics",
      "Object detection",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Pyramid (geometry)",
      "RGB color model",
      "Salient",
      "Spatial contextual awareness",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Xuemiao"
      },
      {
        "surname": "Chen",
        "given_name": "Jiaxing"
      },
      {
        "surname": "Zhang",
        "given_name": "Huaidong"
      },
      {
        "surname": "Han",
        "given_name": "Guoqiang"
      }
    ]
  },
  {
    "title": "Iterative brain tumor retrieval for MR images based on user’s intention model",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108650",
    "abstract": "Generally, medical content-based image retrieval (CBIR) systems select low-level visual features as image descriptors. However, these descriptors fail to provide clues for understanding the content of medical images in a similar way as a human expert, which makes the retrieval results inconsistent with the user’s intention. To solve this problem, we propose a closed-loop brain tumor retrieval system for MR images with an eye-tracking based relevance feedback mechanism. In our method, we first model the intention of the user by training a convolutional neural network based on the temporal and spatial features extracted from his/her eye-tracking data collected when inspecting the relevance between different images. Upon using visual features as a bridge, the relevancy degree to the query image of any of the database images is computed with our user’s intention model by transferring to it the eye movement data from the most visually similar image amongst images iteratively accumulated in the canvas. Our proposed retrieval system is implemented in an iterative manner. In each round of iteration, user’s eye movement data when inspecting the system returns are collected and the canvas collection of images is also updated by appending to it the user inspected system returns. With the updated canvas collections, the relevancy degree of database images can be recomputed and the system can begin a new round search of the most relevant images. Extensive experiments have been performed on a publicly available T1-weighted contrast-enhanced magnetic resonance image (CE-MRI) dataset that consists of three types of brain tumors (glioma, meningioma, and pituitary tumor) collected from 233 patients with a total of 3064 images across the axial, coronal, and sagittal views. Experimental results of 22 volunteers (11 males and 11 females, with an average age of 24.4 years) from our medical school show that upon implicit involvement of users in the brain tumor retrieving process, our proposed system significantly outperforms state-of-the-art methods and achieves P r e c @ 10 to 99.94 % , m A P to 97.95 % after the third round of iteration.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001315",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Content-based image retrieval",
      "Convolutional neural network",
      "Eye tracking",
      "Image (mathematics)",
      "Image retrieval",
      "Information retrieval",
      "Law",
      "Pattern recognition (psychology)",
      "Political science",
      "Relevance (law)",
      "Relevance feedback"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Mengli"
      },
      {
        "surname": "Zou",
        "given_name": "Wei"
      },
      {
        "surname": "Hu",
        "given_name": "Nan"
      },
      {
        "surname": "Wang",
        "given_name": "Jiajun"
      },
      {
        "surname": "Chi",
        "given_name": "Zheru"
      }
    ]
  },
  {
    "title": "A Hybrid Tucker-VQ Tensor Sketch decomposition model for coding and streaming real world light fields using stack of differently focused images",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.034",
    "abstract": "Computational multi-view displays involving light fields are a fast emerging choice for 3D presentation of real-world scenes. Tensor autostereoscopic glasses-free displays use just few light attenuating layers in front of a backlight to output high quality light field. We propose three novel schemes, Focal Stack - Hybrid Tucker-TensorSketch Vector Quantization (FS-HTTSVQ), Focal Stack - Tucker-TensorSketch (FS-TTS), and Focal Stack - Tucker Alternating Least-Squares (FS-TALS), for efficient representation, streaming and coding of light fields using a stack of differently focused images. Working with a focal stack instead of the entire light field majorly reduces the data acquisition cost as well as the computation and processing cost. Extensive experiments with real world light field focal stacks demonstrate that proposed novel one-pass Tucker decomposition using TensorSketch with hybrid vector quantization in FS-HTTSVQ, compactly represents the approximated focal stack in codebook form for better transmission and streaming. Encoding with High Efficiency Video Coding (HEVC) eliminates all intrinsic redundancies present in the approximated focal stack. Resultant low-rank approximated and coded focal stack is then employed to analytically optimize layer patterns for the tensor display. The complete end-to-end light field processing pipelines flexibly work for multiple bitrates and are adaptable for a variety of multi-view autostereoscopic platforms. Our schemes exhibit note-worthy performances on focal stacks compared to direct encoding of an entire light field using a standard codec like HEVC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001465",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Backlight",
      "Codebook",
      "Coding (social sciences)",
      "Computer science",
      "Computer vision",
      "Encoder",
      "Geometry",
      "Light field",
      "Liquid-crystal display",
      "Mathematics",
      "Operating system",
      "Optics",
      "Physics",
      "Programming language",
      "Quantization (signal processing)",
      "Stack (abstract data type)",
      "Statistics",
      "Tensor (intrinsic definition)",
      "Tensor decomposition",
      "Tucker decomposition",
      "Vector quantization"
    ],
    "authors": [
      {
        "surname": "Ravishankar",
        "given_name": "Joshitha"
      },
      {
        "surname": "Sharma",
        "given_name": "Mansi"
      },
      {
        "surname": "Khaidem",
        "given_name": "Sally"
      }
    ]
  },
  {
    "title": "Learning residue-aware correlation filters and refining scale for real-time UAV tracking",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108614",
    "abstract": "Unmanned aerial vehicle (UAV)-based tracking finds its applications in agriculture, aviation, navigation, transportation and public security, etc and develops rapidly recently. However, due to limitations of computing resources, battery capacity, requirement of low power and maximum load of UAV, the deployment of deep learning-based tracking algorithms in UAV is currently not feasible and therefore discriminative correlation filters (DCF)-based trackers have stood out in UAV tracking community for their high efficiency and appealing robustness on a single CPU. But confronted with difficult challenges the efficiency and accuracy of existing DCF-based approaches is still not satisfying. Inspired by the good optimization properties associated with residue representation, in this paper we exploit the residue nature inherent to videos and propose residue-aware correlation filters which demonstrate better convergence properties in filter learning. In addition, we propose a scale refinement strategy to improve the wildly adopted discriminative scale estimation in DCF-based trackers, which, in fact, greatly impacts the precision and accuracy of the trackers since accumulated scale error degrades the appearance model as online updating goes on. Extensive experiments are conducted on four UAV benchmarks, namely, UAV123@10fps, DTB70, UAVDT and Vistrone2018 (VisDrone2018-test-dev). The results show that our method achieves state-of-the-art performance in UAV tracking.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000954",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "BitTorrent tracker",
      "Chemistry",
      "Computer science",
      "Discriminative model",
      "Eye tracking",
      "Gene",
      "Machine learning",
      "Reinforcement learning",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Shuiwang"
      },
      {
        "surname": "Liu",
        "given_name": "Yuting"
      },
      {
        "surname": "Zhao",
        "given_name": "Qijun"
      },
      {
        "surname": "Feng",
        "given_name": "Ziliang"
      }
    ]
  },
  {
    "title": "Reversible image data hiding based on scalable difference expansion",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.05.014",
    "abstract": "Reversible image data hiding technology can not only ensure that the receiver can extract the hidden data like the traditional data hiding technology, but also restore all the pixel values of the original image. Among the existing reversible image data hiding technologies, the method based on differential expansion is the most popular one. In the classical differential expansion method, a bit of data is hidden in the expanded difference of a pair of pixel values. However, expanding difference of different pixel pairs may have different effects on the fidelity of the host image, and some may cause pixel value overflow. In order to solve this problem, this paper proposes a scalable difference expansion based data hiding technology under the condition of meeting data hiding capacity. We use super pixel, more precisely pixel block, to replace a single pixel used in the traditional difference expansion as the basic carrier. Data is hidden in the expanded difference of a pair of super pixels. Adjusting the size of super pixels and/or the differential magnification can scale data hiding capacity and fidelity of stego image. Experimental results verify the feasibility of the method and our expected performance. Because the size and shape of super pixels can be adjusted, it can provide security for data hiding to some extent. In addition, the proposed algorithm does not involve complex operations such as frequency domain transform and inverse transform, so it can be applied to real-time reversible data hiding.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001726",
    "keywords": [
      "Aerospace engineering",
      "Algorithm",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Computer vision",
      "Differential (mechanical device)",
      "Engineering",
      "Fidelity",
      "Geometry",
      "Image (mathematics)",
      "Information hiding",
      "Mathematics",
      "Pixel",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Ding",
        "given_name": "Wenjia"
      },
      {
        "surname": "Zhang",
        "given_name": "Huyin"
      },
      {
        "surname": "Reulke",
        "given_name": "Ralf"
      },
      {
        "surname": "Wang",
        "given_name": "Yulin"
      }
    ]
  },
  {
    "title": "Incorporating global and local social networks for group recommendations",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108601",
    "abstract": "Due to the social nature of human beings, group activities have become an integral part of daily life. This creates the need for an in-depth study of the group-recommendation task: recommending items to a group of users. Unlike individual decision-making, which relies primarily on personal preferences, group decision-making is a process of negotiation and agreement among group members, in which social characteristics are a critical factor in achieving positive recommendation results. Therefore, in this paper, we propose a new model to solve the group recommendation problem from both global and local social networks. In a global network, a user’s social influence spreads through social connections and affects the preferences of others. In a local network, group members may contribute differently to the final decision, forming a dynamic negotiation and consensus process. We propose to model global and local networks with two components: 1) an attentive graph convolutional network based global network diffusion (GND) module to simulate the spread of social influence and capture the social gate of each user, and 2) a multi-channel attention-based local network fusion (LNF) module to learn the complex decision-making process among group members and integrate them into a final representation of the group. Finally, two separate neural collaborative filtering (NCF) modules are presented to model group-item and user-item interactions, respectively, to enhance each other. Extensive experimental results from two real-world datasets show the effectiveness of our proposed model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000826",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Group (periodic table)",
      "Organic chemistry"
    ],
    "authors": [
      {
        "surname": "Leng",
        "given_name": "Youfang"
      },
      {
        "surname": "Yu",
        "given_name": "Li"
      }
    ]
  },
  {
    "title": "Arbitrarily shaped scene text detection with dynamic convolution",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108608",
    "abstract": "Arbitrarily shaped scene text detection has witnessed great development in recent years, and text detection using segmentation has been proven to an effective approach. However, problems caused by the diverse attributes of text instances, such as shapes, scales, and presentation styles (dense or sparse), persist. In this paper, we propose a novel text detector, termed DText, which can effectively formulate an arbitrarily shaped scene text detection task based on dynamic convolution. Our method can dynamically generate independent text-instance-aware convolutional parameters for each text instance from multi-features thus overcoming some intractable limitations of arbitrary text detection, such as the splitting of similar adjacent text, which poses challenges to fixed instance-shared convolutional parameters-based methods. Unlike standard segmentation methods relying on regions-of-interest bounding boxes, DText focuses on enhancing the flexibility of the network to retain details of instances from diverse resolutions while effectively improving prediction accuracy. Moreover, we propose encoding the shape and position information according to the characteristics of the text instance, termed text-shape sensitive position embedding. Thus, it can provide explicit shape and position information to the generator of the dynamic convolution parameters. Experiments on five benchmarks (Total-Text, SCUT-CTW1500, MSRA-TD500, ICDAR2015, and MLT) showed that our method achieves superior detection performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000899",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Bounding overwatch",
      "Computer science",
      "Convolution (computer science)",
      "Detector",
      "Economics",
      "Embedding",
      "Finance",
      "Generator (circuit theory)",
      "Image (mathematics)",
      "Pattern recognition (psychology)",
      "Physics",
      "Position (finance)",
      "Power (physics)",
      "Quantum mechanics",
      "Segmentation",
      "Telecommunications",
      "Text detection"
    ],
    "authors": [
      {
        "surname": "Cai",
        "given_name": "Ying"
      },
      {
        "surname": "Liu",
        "given_name": "Yuliang"
      },
      {
        "surname": "Shen",
        "given_name": "Chunhua"
      },
      {
        "surname": "Jin",
        "given_name": "Lianwen"
      },
      {
        "surname": "Li",
        "given_name": "Yidong"
      },
      {
        "surname": "Ergu",
        "given_name": "Daji"
      }
    ]
  },
  {
    "title": "Contour deformation network for instance segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.05.025",
    "abstract": "To improve the precision of the contour in instance segmentation, this study proposes an iterative contour deformation network (CD-Net) based on a graph convolutional network (GCN). The proposed method treats the segmentation results of the Mask R-CNN model as the initial contours and refines the instances contour iteratively. Specifically, a contour point set is first sampled from the initial contour. Considering the various sizes of the instances, and according to the size of corresponding bounding boxes determined by the Mask R-CNN, a local neighborhood graph is constructed for each selected contour point. Subsequently, multi-scales features are automatically selected and combined with features learned in Mask R-CNN for each point in the local neighborhood graph. The local neighborhood graphs with features are then fed into the GCN to learn the deformation vectors, and the instance contours are refined accordingly. Finally, the refined contour is treated as the initial contour, and the above process is repeated to obtain the final instance contours. The experimental results on the COCO and Cityscapes datasets demonstrate that the proposed method achieves state-of-the-art performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001854",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deformation (meteorology)",
      "Geology",
      "Oceanography",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Lv",
        "given_name": "Kefeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Yongsheng"
      },
      {
        "surname": "Yu",
        "given_name": "Ying"
      },
      {
        "surname": "Wang",
        "given_name": "Hanyun"
      },
      {
        "surname": "Li",
        "given_name": "Lei"
      },
      {
        "surname": "Jiang",
        "given_name": "Huaigang"
      },
      {
        "surname": "Dai",
        "given_name": "Chenguang"
      }
    ]
  },
  {
    "title": "Semantic-refined spatial pyramid network for crowd counting",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.029",
    "abstract": "In this paper, we propose a novel encoder-decoder model called Semantic-refined Spatial Pyramid Network (SSPNet) for generating high-quality density maps, which aims to build a scale-aware counting network to estimate the number of crowds accurately. The SSPNet consists of the front-end based on VGG-16, spatial pyramid multi-scale module (SPMM), and semantic enhancement module (SEM). First, a series of convolutional neural layers are utilized as the front-end to get deeper features without the extra computational cost. Moreover, the SPMM, which has a spatial pyramid structure with multiple receptive fields, is employed to capture multi-scale features. Furthermore, the SEM is designed to refine the features captured by SPMM, which uses deep semantic information to better integrate multi-scale features. Finally, the shallow texture information is adopted to compensate for the detail of the feature map to enhance the quality of the density map. Extensive experiments and comparisons on three challenge datasets, including ShanghaiTech Part_A & Part_B, UCF_CC_50, and UCF-QNRF, illustrate the superiority of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001398",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Encoder",
      "Geometry",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Pyramid (geometry)",
      "Quantum mechanics",
      "Scale (ratio)",
      "Spatial analysis",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Lifang"
      },
      {
        "surname": "Wang",
        "given_name": "Peiwen"
      },
      {
        "surname": "Li",
        "given_name": "Weisheng"
      },
      {
        "surname": "Leng",
        "given_name": "Jiaxu"
      },
      {
        "surname": "Lei",
        "given_name": "Bangjun"
      }
    ]
  },
  {
    "title": "Occlusion-aware spatial attention transformer for occluded object recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.05.006",
    "abstract": "Object classification under partial occlusion has been challenging for deep convolutional neural networks due to their innate locality in extracting features. We propose an Occlusion-aware Spatial Attention Transformer (OSAT) architecture based on Vision Transformer (ViT), CutMix augmentation, and Occlusion Mask Predictor (OMP) to solve the occlusion problem. ViT mainly utilizes the self-attention mechanism, which enables the model to capture spatially distant information. In addition, for occluded image augmentation, we combine CutMix augmentation with ViT. OMP is used as a multi-task learning method and for spatial attention on non-occluded region. Our proposed OSAT achieves state-of-the-art performance on occluded vehicle classification datasets from PASCAL3D+ and MS-COCO. Moreover, additional experiments show that OMP outperforms previous approach in occluder localization both quantitatively and qualitatively. According to our ablation studies, ViT is effective at analyzing occluded objects, and our approach of CutMix augmentation and OMP led to further improvements.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001581",
    "keywords": [
      "Artificial intelligence",
      "Cardiology",
      "Cognitive neuroscience of visual object recognition",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Feature extraction",
      "Linguistics",
      "Locality",
      "Medicine",
      "Occlusion",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Heo",
        "given_name": "Jiseong"
      },
      {
        "surname": "Wang",
        "given_name": "Yooseung"
      },
      {
        "surname": "Park",
        "given_name": "Jihun"
      }
    ]
  },
  {
    "title": "Camera domain adaptation based on cross-patch transformers for person re-identification",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.05.005",
    "abstract": "As an essential task applied to video surveillance, person re-identification (Re-ID) suffers from variations across different cameras. In this paper we propose an effective transformer-based Re-ID framework for learning the identity-discriminative and camera-invariant feature representations. In contrast to the recent direction of using generative models to augment training data and enhance the invariance to input variations, we show that explicitly designing a novel adversarial loss from the perspective of feature representation learning helps to penalize the distribution discrepancy across multiple camera domains effectively. Recently, the pure transformer model has gained much attention due to its strong representation capabilities. We employ a pure transformer encoder to extract a global feature vector for the patch tokens of each person image. Notably, a novel cross-patch encoder is introduced to obtain structural information between image patches. Extensive experiments on three challenging datasets demonstrate the effectiveness and superiority of the proposed learning framework.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552200157X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Electrical engineering",
      "Encoder",
      "Engineering",
      "Feature learning",
      "Feature vector",
      "Generative grammar",
      "Operating system",
      "Pattern recognition (psychology)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Ran",
        "given_name": "Zhidan"
      },
      {
        "surname": "Lu",
        "given_name": "Xiaobo"
      }
    ]
  },
  {
    "title": "Statistical independence of ECG for biometric authentication",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108640",
    "abstract": "The biometric authentication system using electrocardiogram (ECG) may protect individuals’ privacy and prevent identity frauds. Researchers have demonstrated that ECG is suitable for biometrics use due to its pervasiveness, immutability, measurability, acceptance, and individuality. However, ECG’s statistical independence for biometric authentication has yet to be substantiated. Thereby, this paper proposes a novel model to evaluate the statistical independence of ECG among individuals using heartbeat morphological features. The signal is qualitatively improved and heartbeat features are extracted using signal processing techniques. Three classes of features such as interval, amplitude, and angle are extracted from each heartbeat. The hypothesis estimating the probability of resemblance of interval, amplitude, and angle classes of features is derived. The accumulated effect of these classes of features measure the statistical independence of ECG. Further, the proposed model of statistical independence of ECG biometrics is validated by comparing the statistical performance with the empirical performance of the ECG verification system. The empirical performance is estimated using three different ECG biometric methods, i.e., traditional intraclass-interclass features, artificial neural network, and convolutional neural network. The false resemblance probabilities of heartbeats among individuals computed for four interval class features, five amplitude class features, and five angle class features are found to be 3.4 × 10 − 6 , 1.0 × 10 − 7 , and 3.9 × 10 − 8 , respectively. The cumulative probability of resemblance computed using fourteen heartbeat features of interval, amplitude, and angle classes is found as 1.3 × 10 − 20 .",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001212",
    "keywords": [
      "Artificial intelligence",
      "Authentication (law)",
      "Biometrics",
      "Combinatorics",
      "Computer science",
      "Computer security",
      "Heartbeat",
      "Independence (probability theory)",
      "Interval (graph theory)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Speech recognition",
      "Statistical model",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Srivastva",
        "given_name": "Ranjeet"
      },
      {
        "surname": "Singh",
        "given_name": "Yogendra Narain"
      },
      {
        "surname": "Singh",
        "given_name": "Ashutosh"
      }
    ]
  },
  {
    "title": "Rotation invariant point cloud analysis: Where local geometry meets global topology",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108626",
    "abstract": "Point cloud analysis is a fundamental task in 3D computer vision. Most previous works have conducted experiments on synthetic datasets with well-aligned data; while real-world point clouds are often not pre-aligned. How to achieve rotation invariance remains an open problem in point cloud analysis. To meet this challenge, we propose an approach toward achieving rotation-invariant (RI) representations by combining local geometry with global topology. In our local-global-representation (LGR)-Net, we have designed a two-branch network where one stream encodes local geometric RI features and the other encodes global topology-preserving RI features. Motivated by the observation that local geometry and global topology have different yet complementary RI responses in varying regions, two-branch RI features are fused by an innovative multi-layer perceptron (MLP) based attention module. To the best of our knowledge, this work is the first principled approach toward adaptively combining global and local information under the context of RI point cloud analysis. Extensive experiments have demonstrated that our LGR-Net achieves the state-of-the-art performance on various rotation-augmented versions of ModelNet40, ShapeNet, ScanObjectNN, and S3DIS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001078",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Geometry",
      "Invariant (physics)",
      "Mathematical physics",
      "Mathematics",
      "Point cloud",
      "Rotation (mathematics)",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Chen"
      },
      {
        "surname": "Yang",
        "given_name": "Jiaqi"
      },
      {
        "surname": "Xiong",
        "given_name": "Xin"
      },
      {
        "surname": "Zhu",
        "given_name": "Angfan"
      },
      {
        "surname": "Cao",
        "given_name": "Zhiguo"
      },
      {
        "surname": "Li",
        "given_name": "Xin"
      }
    ]
  },
  {
    "title": "Fast computation of mutual information in the frequency domain with applications to global multimodal image alignment",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.05.022",
    "abstract": "Multimodal image alignment is the process of finding spatial correspondences between images formed by different imaging techniques or under different conditions, to facilitate heterogeneous data fusion and correlative analysis. The information-theoretic concept of mutual information (MI) is widely used as a similarity measure to guide multimodal alignment processes, where most works have focused on local maximization of MI, which typically works well only for small displacements. This points to a need for global maximization of MI, which has previously been computationally infeasible due to the high run-time complexity of existing algorithms. We propose an efficient algorithm for computing MI for all discrete displacements (formalized as the cross-mutual information function (CMIF)), which is based on cross-correlation computed in the frequency domain. We show that the algorithm is equivalent to a direct method while superior in terms of run-time. Furthermore, we propose a method for multimodal image alignment for transformation models with few degrees of freedom (e.g., rigid) based on the proposed CMIF-algorithm. We evaluate the efficacy of the proposed method on three distinct benchmark datasets, containing remote sensing images, cytological images, and histological images, and we observe excellent success-rates (in recovering known rigid transformations), overall outperforming alternative methods, including local optimization of MI, as well as several recent deep learning-based approaches. We also evaluate the run-times of a GPU implementation of the proposed algorithm and observe speed-ups from 100 to more than 10,000 times for realistic image sizes compared to a GPU implementation of a direct method. Code is shared as open-source at github.com/MIDA-group/globalign.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001817",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computation",
      "Computer science",
      "Computer vision",
      "Domain (mathematical analysis)",
      "Frequency domain",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Mutual information",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Öfverstedt",
        "given_name": "Johan"
      },
      {
        "surname": "Lindblad",
        "given_name": "Joakim"
      },
      {
        "surname": "Sladoje",
        "given_name": "Nataša"
      }
    ]
  },
  {
    "title": "Task adaptive siamese neural networks for open-set recognition of encrypted network traffic with bidirectional dropout",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.05.011",
    "abstract": "Existing deep learning approaches have achieved high performance in encrypted network traffic analysis tasks. However, practical requirements such as open-set recognition on dynamically changing tasks (e.g., changes in the target website list), challenge existing methods. While few-shot learning and open-set recognition methods have been proposed for domains such as computer vision, few-shot open-set recognition for encrypted network traffic remains an unexplored area. This paper proposes a task adaptive siamese neural network for open-set recognition of encrypted network traffic with bidirectional dropout data augmentation. Our contributions are three-fold: First, we introduce generated positive and negative pairs into the siamese neural network training process to shape a more precise similarity boundary through bidirectional dropout data augmentation. Second, we utilize Dirichlet Process Gaussian Mixture Model (DPGMM) distribution to fit the similarity scores of the negative pairs constructed by the support set of each query task, and create a new open-set recognition metric. Third, by leveraging the extracted features at coarse and fine granular levels, we construct a hierarchical cross entropy loss to improve the confidence of the similarity score. Extensive experiments on a network traffic dataset and the Omniglot dataset demonstrate the superiority and generalizability of our proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001702",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer network",
      "Computer science",
      "Dropout (neural networks)",
      "Encryption",
      "Engineering",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Yi"
      },
      {
        "surname": "Li",
        "given_name": "Ying"
      },
      {
        "surname": "Heyes",
        "given_name": "Timothy"
      },
      {
        "surname": "Jourjon",
        "given_name": "Guillaume"
      },
      {
        "surname": "Cheng",
        "given_name": "Adriel"
      },
      {
        "surname": "Seneviratne",
        "given_name": "Suranga"
      },
      {
        "surname": "Thilakarathna",
        "given_name": "Kanchana"
      },
      {
        "surname": "Webb",
        "given_name": "Darren"
      },
      {
        "surname": "Xu",
        "given_name": "Richard Yi Da"
      }
    ]
  },
  {
    "title": "The impact of domain randomization on cross-device monocular deep 6DoF detection",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.008",
    "abstract": "This work evaluates the use of synthetic data to train deep 6DoF pose estimation models that use a monocular RGB camera as input. We have compared different training strategies combining real and synthetic data (with domain randomization) to investigate how to better handle real-world challenges. We show that it is possible to obtain accurate models using less real data and suggest how to utilize this strategy. In this work, we have captured and made available two datasets: one real and one synthetic, totaling over 110,000 annotated frames. These datasets are organized according to the different cameras used and the challenges present in the sequences, all featuring textureless 3D printed objects. We also show that synthetic data can help models generalize, handling challenges such as fast motion, occlusion, illumination changes, color variation, scale changes, and unexpected geometry. Finally, we evaluated 70 different models to understand how a model trained for one camera sensor performs when used with a different sensor. To this end, we also suggest how to handle this challenge better by using synthetic simulations to supplement training.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001040",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Domain (mathematical analysis)",
      "Mathematical analysis",
      "Mathematics",
      "Monocular",
      "Pattern recognition (psychology)",
      "RGB color model",
      "Synthetic data"
    ],
    "authors": [
      {
        "surname": "da Cunha",
        "given_name": "Kelvin B."
      },
      {
        "surname": "Brito",
        "given_name": "Caio"
      },
      {
        "surname": "Valença",
        "given_name": "Lucas"
      },
      {
        "surname": "Figueiredo",
        "given_name": "Lucas"
      },
      {
        "surname": "Simões",
        "given_name": "Francisco"
      },
      {
        "surname": "Teichrieb",
        "given_name": "Veronica"
      }
    ]
  },
  {
    "title": "Automatic fine-grained glomerular lesion recognition in kidney pathology",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108648",
    "abstract": "Recognition of glomeruli lesions is the key for diagnosis and treatment planning in kidney pathology; however, the coexisting glomerular structures such as mesangial regions exacerbate the difficulties of this task. In this paper, we introduce a scheme to recognize fine-grained glomeruli lesions from whole slide images. First, a focal instance structural similarity loss is proposed to drive the model to locate all types of glomeruli precisely. Then an Uncertainty Aided Apportionment Network is designed to carry out the fine-grained visual classification without bounding-box annotations. This double branch-shaped structure extracts common features of the child class from the parent class and produces the uncertainty factor for reconstituting the training dataset. Results of slide-wise evaluation illustrate the effectiveness of the entire scheme, with an 8–22% improvement of the mean Average Precision compared with remarkable detection methods. The comprehensive results clearly demonstrate the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001297",
    "keywords": [
      "Artificial intelligence",
      "Bounding overwatch",
      "Class (philosophy)",
      "Computer science",
      "Engineering",
      "Image (mathematics)",
      "Lesion",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)",
      "Scheme (mathematics)",
      "Similarity (geometry)",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Nan",
        "given_name": "Yang"
      },
      {
        "surname": "Li",
        "given_name": "Fengyi"
      },
      {
        "surname": "Tang",
        "given_name": "Peng"
      },
      {
        "surname": "Zhang",
        "given_name": "Guyue"
      },
      {
        "surname": "Zeng",
        "given_name": "Caihong"
      },
      {
        "surname": "Xie",
        "given_name": "Guotong"
      },
      {
        "surname": "Liu",
        "given_name": "Zhihong"
      },
      {
        "surname": "Yang",
        "given_name": "Guang"
      }
    ]
  },
  {
    "title": "EANet: Iterative edge attention network for medical image segmentation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108636",
    "abstract": "Accurate and automatic segmentation of medical images can greatly assist the clinical diagnosis and analysis. However, it remains a challenging task due to (1) the diversity of scale in the medical image targets and (2) the complex context environments of medical images, including ambiguity of structural boundaries, complexity of shapes, and the heterogeneity of textures. To comprehensively tackle these challenges, we propose a novel and effective iterative edge attention network (EANet) for medical image segmentation with steps as follows. First, we propose a dynamic scale-aware context (DSC) module, which dynamically adjusts the receptive fields to extract multi-scale contextual information efficiently. Second, an edge-attention preservation (EAP) module is employed to effectively remove noise and help the edge stream focus on processing only the boundary-related information. Finally, a multi-level pairwise regression (MPR) module is designed to combine the complementary edge and region information for refining the ambiguous structure. This iterative optimization helps to learn better representations and more accurate saliency maps. Extensive experimental results demonstrate that the proposed network achieves superior segmentation performance to state-of-the-art methods in four different challenging medical segmentation tasks, including lung nodule segmentation, COVID-19 infection segmentation, lung segmentation, and thyroid nodule segmentation. The source code of our method is available at https://github.com/DLWK/EANet",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001170",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Enhanced Data Rates for GSM Evolution",
      "Image segmentation",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Scale-space segmentation",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Kun"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaohong"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiangbo"
      },
      {
        "surname": "Lu",
        "given_name": "Yuting"
      },
      {
        "surname": "Huang",
        "given_name": "Sheng"
      },
      {
        "surname": "Yang",
        "given_name": "Dan"
      }
    ]
  },
  {
    "title": "From general to specific: Online updating for blind super-resolution",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108613",
    "abstract": "Most deep learning-based super-resolution (SR) methods are not image-specific: 1) They are trained on samples synthesized by predefined degradations (e.g.bicubic downsampling), regardless of the domain gap between training and testing data. 2) During testing, they super-resolve all images by the same set of model weights, ignoring the degradation variety. As a result, most previous methods may suffer a performance drop when the degradations of test images are unknown and various (i.e.the case of blind SR). To address these issues, we propose an online SR (ONSR) method. It does not rely on predefined degradations and allows the model weights to be updated according to the degradation of the test image. Specifically, ONSR consists of two branches, namely internal branch (IB) and external branch (EB). IB could learn the specific degradation of the given test LR image, and EB could learn to super resolve images degraded by the learned degradation. In this way, ONSR could customize a specific model for each test image, and thus get more robust to various degradations. Extensive experiments on both synthesized and real-world images show that ONSR can generate more visually favorable SR results and achieve state-of-the-art performance in blind SR.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000942",
    "keywords": [
      "Artificial intelligence",
      "Bicubic interpolation",
      "Computer science",
      "Computer vision",
      "Degradation (telecommunications)",
      "Domain (mathematical analysis)",
      "Image (mathematics)",
      "Image processing",
      "Linear interpolation",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Standard test image",
      "Telecommunications",
      "Test set",
      "Upsampling"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Shang"
      },
      {
        "surname": "Zhang",
        "given_name": "Guixuan"
      },
      {
        "surname": "Luo",
        "given_name": "Zhengxiong"
      },
      {
        "surname": "Liu",
        "given_name": "Jie"
      },
      {
        "surname": "Zeng",
        "given_name": "Zhi"
      },
      {
        "surname": "Zhang",
        "given_name": "Shuwu"
      }
    ]
  },
  {
    "title": "Detail preserving conditional random field as 2-D RNN for gland segmentation in histology images",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.05.001",
    "abstract": "Grading of cancer offers crucial insights for treatment planning. Morphology of glands in histology images is of prime importance for grading several types of cancers. Therefore, accurate segmentation of glands plays a pivotal role in planning the treatment in case of such cancers. We introduce a first-of-its-kind detail preserving conditional random field for gland segmentation from histology images. Our design involves a novel formulation of Gibbs energy that captures the spatial interaction between neighboring pixels through the hidden state of a 2-D recurrent neural network (2-D RNN). We show that the iterative training of the 2-D RNN results in the minimization of the Gibbs energy leading to accurate gland segmentation. Experiments on publicly available histology image datasets show the efficacy of the proposed method in accurate gland segmentation. Our model achieves at least 7% improvement in terms of Hausdorff distance for gland segmentation compared to a number of state-of-the-art techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001532",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Conditional random field",
      "Ecology",
      "Grading (engineering)",
      "Histology",
      "Image segmentation",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Chattopadhyay",
        "given_name": "Aratrik"
      },
      {
        "surname": "Paul",
        "given_name": "Angshuman"
      },
      {
        "surname": "Mukherjee",
        "given_name": "Dipti Prasad"
      }
    ]
  },
  {
    "title": "Few shot learning-based fast adaptation for human activity recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.014",
    "abstract": "Sensor-based human activity recognition (HAR) is a common application in the fields of mobile computing and pattern recognition. Existing approaches and models of HAR can present ideal recognition performance only in well-designed, specific, and deterministic scenarios. However, in real scenes, new types of activities, new human bodies that performs activities, and other new situations are encountered. When new situations arise, it is difficult to collect sufficient and high-quality data in time. Thus, the existing approaches and models suffer from a lack of interoperability and scalability. To address these challenges, this study proposes a Model-agnostic Meta Learnings(MAML)-Coarse-Fine Convolutional Neural Networks(CFCNN) mixed task strategy to achieve fast adaptation to the human activity recognition under new situations. This is a novel method that incorporates shot learning to recognize tasks in situations where several kinds of new scenarios exist. First, some modifications were carried out on traditional MAML for multi-scale feature extraction, and a mixed task strategy was adopted during training. The proposed method improves the generalization ability of the model, and is capable of learning quickly when dealing with diffirent new tasks. Finally, the model was compared with other models when facing two new scenes of a new activity category and a new human body. The results showed that the accuracy of the proposed MAML-CFCNN based on the mixed task strategy is higher than that of the state-of-the-art methods, including MAML, First-Order MAML (FOMAML), REPTILE, and so on.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001106",
    "keywords": [
      "Activity recognition",
      "Adaptation (eye)",
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Database",
      "Engineering",
      "Feature (linguistics)",
      "Generalization",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Philosophy",
      "Physics",
      "Scalability",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Nie",
        "given_name": "Lanshun"
      },
      {
        "surname": "Li",
        "given_name": "Xue"
      },
      {
        "surname": "Gong",
        "given_name": "Tianying"
      },
      {
        "surname": "Zhan",
        "given_name": "Dechen"
      }
    ]
  },
  {
    "title": "Virtual special issue on advances in digital security: Biometrics and forensics",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.05.018",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001787",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Domain (mathematical analysis)",
      "Mathematical analysis",
      "Mathematics",
      "Monocular",
      "Motion (physics)",
      "Motion capture",
      "RGB color model",
      "Synthetic data"
    ],
    "authors": [
      {
        "surname": "Gragnaniello",
        "given_name": "Diego"
      },
      {
        "surname": "Li",
        "given_name": "Chang-Tsu"
      },
      {
        "surname": "Marra",
        "given_name": "Francesco"
      },
      {
        "surname": "Riccio",
        "given_name": "Daniel"
      }
    ]
  },
  {
    "title": "Head pose estimation: An extensive survey on recent techniques and applications",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108591",
    "abstract": "Biometric based systems are involved in many areas, from surveillance to user authentication, from autonomous systems to human-robot interactions. Head pose estimation (HPE) is the task to support biometric systems in which any of the biometric traits of the head is involved, as face, ear or iris. This particular biometric branch finds its application in driver attention detection, surveillance for recognition, face frontalization, best frame selection and so on. The goal of HPE is to determine the head pose orientation (yaw, pitch, roll). The implemented methods use different techniques depending on the kind of input. In this survey we present an overview of involved datasets, recent techniques and applications. We evaluate and compare the different approaches with respect to their advantages and practical usage. In addition, we propose a technical comparison between training and training-free techniques for the most popular HPE methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000723",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Face (sociological concept)",
      "Facial recognition system",
      "Frame (networking)",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Orientation (vector space)",
      "Pattern recognition (psychology)",
      "Pose",
      "Robot",
      "Selection (genetic algorithm)",
      "Social science",
      "Sociology",
      "Systems engineering",
      "Task (project management)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Abate",
        "given_name": "Andrea F."
      },
      {
        "surname": "Bisogni",
        "given_name": "Carmen"
      },
      {
        "surname": "Castiglione",
        "given_name": "Aniello"
      },
      {
        "surname": "Nappi",
        "given_name": "Michele"
      }
    ]
  },
  {
    "title": "Dilated residual grooming kernel model for breast cancer detection",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.037",
    "abstract": "Breast cancer is the most significant cause of mortality among women. When detected and treated early, it saves lives. Breast cancer detection is becoming more accessible and accurate thanks to machine learning and deep learning models. This research aims to enhance medical science and technology by employing a deep learning model to detect small cancer cells with pinpoint accuracy. The proposed model uses datasets from the Breast Cancer Histopathological Image Classification (BreakHis) and Breast Cancer Histopathological Annotation and Diagnosis (BreCaHAD). Next, the image processing procedure employs strain normalization to rectify color divergence caused by using different slide scanners, staining processes, and biopsy materials. The data augmentation with nineteen parameters such as scaling, rotation, flip, resize, and gamma value tackles the overfitting problems. The augmented images are then processed using the dilated residual (DR) model; the DR model combines the proposed dilated spatial convolution unit, fully connected dilation unit, and dilated channel convolution. The first unit is the dilated spatial convolution, which handles all channels equally and amplifies the valuable aspects. The second unit is a fully connected dilation unit; it displays low-level properties such as edges, contours, and color. The third unit is dilated channel convolution, which detects tiny objects and thin boundaries without adding complexity. The proposed dilated residual grooming kernel (DRGK) model is a 14-layer deep learning model that stretches the receptive field while retaining feature information, using the proposed DR unit and ghost model, as well as convolution, pooling, downsampling, and dilated convolution. Dilated convolutions are extensively used in the proposed model to extract features. Accuracy, the area under the curve, average precision score, precision, sensitivity, and f1 score all improve with a learning rate of 0.001. With 96.33% and 93.35% marks, the proposed approach surpasses several state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001507",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Deep learning",
      "Dilation (metric space)",
      "Image (mathematics)",
      "Mathematics",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Residual",
      "Upsampling"
    ],
    "authors": [
      {
        "surname": "Kashyap",
        "given_name": "Ramgopal"
      }
    ]
  },
  {
    "title": "Improved deep convolutional embedded clustering with re-selectable sample training",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108611",
    "abstract": "The deep clustering algorithm can learn the latent features of the embedded subspace, and further realize the clustering of samples in the feature space. The existing deep clustering algorithms mostly integrate neural networks and traditional clustering algorithms. However, for sample sets with many noise points, the effect of the clustering remains unsatisfactory. To address this issue, we propose an improved deep convolutional embedded clustering algorithm using reliable samples (IDCEC) in this paper. The algorithm first uses the convolutional autoencoder to extract features and cluster the samples. Then we select reliable samples with pseudo-labels and pass them to the convolutional neural network for training to get a better clustering model. We construct a new loss function for backpropagation training and implement an unsupervised deep clustering method. To verify the performance of the method proposed in this paper, we conducted experimental tests on standard data sets such as MNIST and USPS. Experimental results show that our method has better performance compared to traditional clustering algorithms and the state-of-the-art deep clustering algorithm under four clustering metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000929",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Cluster analysis",
      "Computer science",
      "Geography",
      "Meteorology",
      "Pattern recognition (psychology)",
      "Sample (material)",
      "Training (meteorology)"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Hu"
      },
      {
        "surname": "Chen",
        "given_name": "Chao"
      },
      {
        "surname": "Wei",
        "given_name": "Hui"
      },
      {
        "surname": "Ma",
        "given_name": "Zhongchen"
      },
      {
        "surname": "Jiang",
        "given_name": "Ke"
      },
      {
        "surname": "Wang",
        "given_name": "Yingquan"
      }
    ]
  },
  {
    "title": "Multilabel learning based adaptive graph convolutional network for human parsing",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108593",
    "abstract": "In human parsing, graph convolutional networks (GCNs), which naturally model the skeleton of the human body as a fixed graph, have been witnessed to obtain remarkable performance. However, the existing methods perform the fixed graph modeling over all the training samples. This may not be an optimal graph for the diversity of the samples that contain various shapes of human parts, complex body postures, severe occlusions and dense crowd, etc. Focusing on this, we propose a new Multilabel Learning based Adaptive Graph Convolutional Network (ML-AGCN) for human parsing. The ML-AGCN includes three modules: adaptive graph generation module, semantic parts based attention module and label consistency loss. Concretely, to effectively deal with the different sizes and connectivities of the optimal graph for different samples, we first propose an adaptive graph generation module based on multilabel learning that contains graph node adaptation (GNA) and graph connection adaptation (GCA). Then, for a more comprehensive node embedding, we design a semantic parts based attention module to optimally fuse fixed graph embeddings and adaptive graph embeddings. Besides, to further explicitly constraint the consistency between the predicted multilabel and the predicted human parsing results, we propose a label consistency loss that can simultaneously refine the human parsing results and optimize the accuracy of the adaptive graph. Extensive experiments on four challenging datasets, including PASCAL-Person-Part, ATR, LIP and CIHP, well demonstrate the effectiveness of our model, and it outperforms other state-of-the-art methods in human parsing.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000747",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Graph",
      "Parsing",
      "Pascal (unit)",
      "Programming language",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Hao",
        "given_name": "Huaqing"
      },
      {
        "surname": "Liu",
        "given_name": "Weibin"
      },
      {
        "surname": "Xing",
        "given_name": "Weiwei"
      },
      {
        "surname": "Zhang",
        "given_name": "Shunli"
      }
    ]
  },
  {
    "title": "A novel hybrid model for short-term prediction of wind speed",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108623",
    "abstract": "Due to the randomness and contingency of wind speed size and direction, it is difficult to predict the wind speed accurately, which seriously affects the stable operation of the power system. To improve the operation stability of power system, the accurate prediction of wind speed is very important. In this paper, a new hybrid model based on gray wolf algorithm (GWO) and support vector machine (SVM) for wind speed prediction is proposed. Firstly, Neo4j(NE) is utilized to identify the data and preprocess the data. Secondly, k-means clustering(KC) is utilized to analyze data and eliminate invalid data. Thirdly, GWO is utilized to optimize the kernel function parameters and penalty factors of SVM to improve the prediction results. Fourthly, The four modules are combined into NE-KC-GWO-SVM model to predict the wind speed accurately. Finally, to verify the effectiveness of the proposed model, the prediction accuracy of the model is experimentally analyzed from two parts. One is to analyze the superiority of the model itself by using the method of single model removed. The results show that the proposed model is the best, and has high accuracy, and can reflect the characteristics of wind speed well and truly. The other one is that models similar to those proposed in the literature are selected for comparative analysis. The experimental results show that compared with the other two models, the proposed model has the best accuracy. At the same time, the proposed model has good prediction stability and acceptable time complexity. Based on all the experimental results, it can be obtained that the proposed model has better prediction effect, which can provide a scientific basis for the macro-control of power system and improve the operation security and stability of power system.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001042",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Electrical engineering",
      "Engineering",
      "Machine learning",
      "Mathematics",
      "Meteorology",
      "Physics",
      "Quantum mechanics",
      "Randomness",
      "Stability (learning theory)",
      "Statistics",
      "Support vector machine",
      "Term (time)",
      "Wind power",
      "Wind speed"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Haize"
      },
      {
        "surname": "Li",
        "given_name": "Yunyi"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiangping"
      },
      {
        "surname": "Fang",
        "given_name": "Mengge"
      }
    ]
  },
  {
    "title": "Incremental and compressible kernel null discriminant analysis",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108642",
    "abstract": "Kernel discriminant analysis (KDA), the nonlinear extension of linear Discriminant Analysis (LDA), is a popular tool for learning one or multiple categories in nonlinear data sets. However, in most modern pattern recognition applications such as video surveillance, data are collected in flow and require sequential processing. In this context, KDA is faced two critical issues: an original formulation unsuited to the dynamic nature of the data and an increasing memory requirement for the kernel matrix storage. Motivated by the state-of-the-art performance reported by the null KDA, we propose in this paper a new solution to solve the null KDA (NKDA) in the context of data streams. Compared to previous works, our contribution is based on three points: first, we develop an exact incremental scheme which guarantees accurate solutions. Secondly, we develop a compression mechanism based on the following observation: rger the size of the training data set more the distances in the null space contract This property of the null space leads to formulate an indicator of redundancy in the training data set. This criterion is the cornerstone of our incremental KNDA because it authorizes incremental learning on large-scale data sets. Third, the problem of novelty detection in multi-class and one-class scenarios is addressed. More precisely, the fact that distances in the null space change over the training period leads us to define adjustable novelty thresholds. Lastly, numerous experiments based on various publicly available data sets and state-of-the-art classifiers show that the proposed method is effective both for multi-class and one-class real applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001236",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Combinatorics",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Kernel (algebra)",
      "Kernel method",
      "Linear discriminant analysis",
      "Machine learning",
      "Mathematics",
      "Novelty",
      "Novelty detection",
      "Null (SQL)",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Support vector machine",
      "Theology"
    ],
    "authors": [
      {
        "surname": "Dufrenois",
        "given_name": "F."
      }
    ]
  },
  {
    "title": "CubeNet: X-shape connection for camouflaged object detection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108644",
    "abstract": "Camouflaged object detection (COD) aims to detect out-of-attention regions in an image. Current binary segmentation solutions fail to tackle COD easily, since COD is more challenging due to object often accompany with weak boundaries, low contrast, or similar patterns to the background. That is, we need a more efficient scheme to address this problem. In this work, we propose a new COD framework called CubeNet by introducing X connection to the standard encoder-decoder architecture. Specifically, CubeNet consists of two square fusion decoder (SFD) and a sub edge decoder (SED). The special designed SFD takes full advantage of low-level and high-level features extracted from encoder-decoder blocks, providing more powerful representations at each stage. To explicitly modeling the weak boundaries of the objects, we introduced a SED between the two SFD. With such kind of holistic designs, these three decoder modules resolve the challenging ambiguity of camouflaged object detection. CubeNet significantly advance the cutting-edge model on three challenging COD datasets (i.e., COD10K, CAMO, and CHAMELEON), and achieves the real-time (50fps) inference.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200125X",
    "keywords": [
      "Ambiguity",
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Cognitive neuroscience of visual object recognition",
      "Computer science",
      "Computer vision",
      "Connection (principal bundle)",
      "Edge detection",
      "Encoder",
      "Enhanced Data Rates for GSM Evolution",
      "Geometry",
      "Image (mathematics)",
      "Image processing",
      "Inference",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Operating system",
      "Pattern recognition (psychology)",
      "Programming language",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Zhuge",
        "given_name": "Mingchen"
      },
      {
        "surname": "Lu",
        "given_name": "Xiankai"
      },
      {
        "surname": "Guo",
        "given_name": "Yiyou"
      },
      {
        "surname": "Cai",
        "given_name": "Zhihua"
      },
      {
        "surname": "Chen",
        "given_name": "Shuhan"
      }
    ]
  },
  {
    "title": "Visual explanation of black-box model: Similarity Difference and Uniqueness (SIDU) method",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108604",
    "abstract": "Explainable Artificial Intelligence (XAI) has in recent years become a well-suited framework to generate human understandable explanations of ‘black- box’ models. In this paper, a novel XAI visual explanation algorithm known as the Similarity Difference and Uniqueness (SIDU) method that can effectively localize entire object regions responsible for prediction is presented in full detail. The SIDU algorithm robustness and effectiveness is analyzed through various computational and human subject experiments. In particular, the SIDU algorithm is assessed using three different types of evaluations (Application, Human and Functionally-Grounded) to demonstrate its superior performance. The robustness of SIDU is further studied in the presence of adversarial attack on ’black-box’ models to better understand its performance. Our code is available at: https://github.com/satyamahesh84/SIDU_XAI_CODE.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000851",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Black box",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Similarity (geometry)",
      "Uniqueness"
    ],
    "authors": [
      {
        "surname": "Muddamsetty",
        "given_name": "Satya M."
      },
      {
        "surname": "Jahromi",
        "given_name": "Mohammad N.S."
      },
      {
        "surname": "Ciontos",
        "given_name": "Andreea E."
      },
      {
        "surname": "Fenoy",
        "given_name": "Laura M."
      },
      {
        "surname": "Moeslund",
        "given_name": "Thomas B."
      }
    ]
  },
  {
    "title": "Sparse matrix factorization with L 2 , 1 norm for matrix completion",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108655",
    "abstract": "Matrix factorization is a popular matrix completion method, however, it is difficult to determine the ranks of the factor matrices. We propose two new sparse matrix factorization methods with l 2 , 1 norm to explicitly force the row sparseness of the factor matrices, where the rank of the factor matrices is adaptively controlled by the regularization coefficient. We further theoretically prove the convergence property of our algorithms. The experimental results on the simulation and the benchmark datasets show that our methods achieve superior performance than its counterparts. Moreover our proposed methods can attain comparable performance with the deep learning-based matrix completion methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001364",
    "keywords": [
      "Algorithm",
      "Combinatorics",
      "Composite material",
      "Computer science",
      "Eigenvalues and eigenvectors",
      "Factorization",
      "Materials science",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Matrix decomposition",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Jin",
        "given_name": "Xiaobo"
      },
      {
        "surname": "Miao",
        "given_name": "Jianyu"
      },
      {
        "surname": "Wang",
        "given_name": "Qiufeng"
      },
      {
        "surname": "Geng",
        "given_name": "Guanggang"
      },
      {
        "surname": "Huang",
        "given_name": "Kaizhu"
      }
    ]
  },
  {
    "title": "Unabridged adjacent modulation for clothing parsing",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108594",
    "abstract": "Clothing parsing has made tremendous progress in the domain of computer vision recently. Most state-of-the-art methods are based on the encoder-decoder architecture. However, the existing methods mainly neglect problems of feature uncalibration within blocks and semantics dilution between blocks. In this work, we propose an unabridged adjacent modulation network (UAM-Net) to aggregate multi-level features for clothing parsing. We first build an unabridged channel attention (UCA) mechanism on feature maps within each block for feature recalibration. We further design a top-down adjacent modulation (TAM) for decoder blocks. By deploying TAM, high-level semantic information and visual contexts can be gradually transferred into lower-level layers without loss. The joint implementation of UCA and TAM ensures that the encoder has an enhanced feature representation ability, and the low-level features of the decoders contain abundant semantic contexts. Quantitative and qualitative experimental results on two challenging benchmarks (i.e., colorful fashion parsing and the modified fashion clothing) declare that our proposed UAM-Net can achieve competitive high-accurate performance with the state-of-the-art methods. The source codes are available at: https://github.com/ctzuo/UAM-Net.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000759",
    "keywords": [
      "Acoustics",
      "Archaeology",
      "Artificial intelligence",
      "Clothing",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "History",
      "Modulation (music)",
      "Natural language processing",
      "Parsing",
      "Physics",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Dong"
      },
      {
        "surname": "Zuo",
        "given_name": "Chengting"
      },
      {
        "surname": "Wu",
        "given_name": "Qianhao"
      },
      {
        "surname": "Fu",
        "given_name": "Liyong"
      },
      {
        "surname": "Xiang",
        "given_name": "Xinguang"
      }
    ]
  },
  {
    "title": "Conference on graphics, patterns and images",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.05.002",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001544",
    "keywords": [
      "Artificial intelligence",
      "Computer graphics",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Graphics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Cesar",
        "given_name": "Roberto"
      },
      {
        "surname": "Musse",
        "given_name": "Soraia Raupp"
      },
      {
        "surname": "Pelechano",
        "given_name": "Nuria"
      },
      {
        "surname": "(Atlas) Wang",
        "given_name": "Zhangyang"
      }
    ]
  },
  {
    "title": "MOJ-DB: A new database of Arabic historical handwriting and a novel approach for subwords extraction",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.040",
    "abstract": "The digitalization of historical documents is vital to preserving their content and the historical memory of nations. Although, the results of historical Arabic handwritten text recognition and word spotting are still unsatisfactory. The increasing research efforts during the last few years are still not sufficient since handwriting recognition systems rely heavily on robust databases. In this paper, we present a new contour-based method of subword extraction from Arabic historical documents and a novel database of Arabic historical subwords MOJ-DB. The proposed method of subword extraction includes a process of touching components resolving. It proved high performance and consistency while tested on different databases and compared with other methods from the literature. The proposed database contains 560000 subwords distributed on 5600 different classes. It was built using 64 pages extracted from 10 books written in the 17th and 16th centuries. MOJ-DB database is divided into three sets; 70%,20%, and 10% for training, testing, and validation, respectively. Ground truth is established iteratively to guarantee minimal error. It includes information about the subword as of the sourcebook and page. We conducted several experiments to verify the robustness of the proposed database as well as the validity of the segmentation process. The database is freely available for the public research community. It can be used for word and subword recognition, word spotting, subword extraction, and database construction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001520",
    "keywords": [
      "Arabic",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Consistency (knowledge bases)",
      "Database",
      "Gene",
      "Geometry",
      "Ground truth",
      "Handwriting",
      "Information retrieval",
      "Linguistics",
      "Mathematics",
      "Natural language processing",
      "Philosophy",
      "Robustness (evolution)",
      "Segmentation",
      "Spotting",
      "Text segmentation",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Zoizou",
        "given_name": "Abdelhay"
      },
      {
        "surname": "Zarghili",
        "given_name": "Arsalane"
      },
      {
        "surname": "Chaker",
        "given_name": "Ilham"
      }
    ]
  },
  {
    "title": "Unsupervised feature selection via adaptive graph and dependency score",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108622",
    "abstract": "Unsupervised feature selection is an important topic in the fields of machine learning, pattern recognition and data mining. The representation methods include adaptive-graph-based methods and self-representation-based methods. The former methods have a longstanding and undiscovered problem about imbalanced neighbors, and the latter ones do not perform well when features are not linearly dependent. To deal with these problems, a novel unsupervised feature selection method is proposed to ensure k connectivity and eliminate more redundant features based on adaptive graph and dependency score (AGDS). Extensive experiments conducted on 13 benchmark datasets show the effectiveness of AGDS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001030",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Data mining",
      "Dependency (UML)",
      "Feature (linguistics)",
      "Feature learning",
      "Feature selection",
      "Geodesy",
      "Geography",
      "Graph",
      "Law",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Theoretical computer science",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Pei"
      },
      {
        "surname": "Yang",
        "given_name": "Xiaowei"
      }
    ]
  },
  {
    "title": "Task-specific dependency-based word embedding methods",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.05.016",
    "abstract": "While most traditional word embedding methods target generic tasks, two task-specific dependency-based word embedding methods are proposed for better performance in text classification tasks in this work. First, we exploit the dependency parsing tree structure to capture the structural information of a sentence, and develop a method called dependency-based word embedding (DWE). It finds keywords and neighbor words of a target word as contexts via dependency parsing. Next, we leverage the word-class co-occurrence statistics to model the class distributional information and incorporate it into the embedding learning process. This leads to the class-enhanced dependency-based word embedding (CEDWE) method. Task-specific corpora and the matrix-factorization-based framework are used to train DWE and CEDWE. Seven text classification datasets are used to evaluate the performance of DWE and CEDWE, and experimental results show that they outperform several state-of-the-art word embedding methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552200174X",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Dependency (UML)",
      "Dependency grammar",
      "Embedding",
      "Geometry",
      "Leverage (statistics)",
      "Mathematics",
      "Natural language processing",
      "Parsing",
      "Sentence",
      "Word (group theory)",
      "Word embedding"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Chengwei"
      },
      {
        "surname": "Wang",
        "given_name": "Bin"
      },
      {
        "surname": "Kuo",
        "given_name": "C.-C. Jay"
      }
    ]
  },
  {
    "title": "CE-text: A context-Aware and embedded text detector in natural scene images",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.05.004",
    "abstract": "With the significant power of deep learning architectures, researchers have made much progress on effectiveness and efficiency of text detection in the past few years. However, due to the lack of consideration of unique characteristics of text components, directly applying deep learning models to perform text detection task is prone to result in low accuracy, especially producing false positive detection results. To ease this problem, we propose a lightweight and context-aware deep convolutional neural network (CNN) named as CE-Text, which appropriately encodes multi-level channel attention information to construct discriminative feature map for accurate and efficient text detection. To fit with low computation resource of embedded systems, we further transform CE-Text into a lighter version with a frequency based deep CNN compression method, which expands applicable scenarios of CE-Text into variant embedded systems. Experiments on several popular datasets show that CE-Text not only has achieved accurate text detection results in scene images, but also could run with fast performance in embedded systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001556",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Detector",
      "Geography",
      "Image (mathematics)",
      "Information retrieval",
      "Natural (archaeology)",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Telecommunications",
      "Text detection",
      "Text recognition"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Yirui"
      },
      {
        "surname": "Zhang",
        "given_name": "Wen"
      },
      {
        "surname": "Wan",
        "given_name": "Shaohua"
      }
    ]
  },
  {
    "title": "Convergence analysis of connection center evolution and faster clustering",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108639",
    "abstract": "Clustering is a subjective task, that is, several different results can be obtained from a single clustering hierarchy, depending on the observation scale. A local view of the data may necessitate more clusters, whereas a global view requires fewer clusters. It is, therefore, important to provide users with an appropriate clustering hierarchy and let them select the final clustering result based on their own observation scale. Thus, a new clustering method, named connection center evolution (CCE), was recently developed by Geng and Tang (2020). CCE provides gradual clustering results by iteratively merging cluster centers. However, theoretical evidence for its convergence is missing, and the center evolution requires a connectivity matrix of a significantly higher order as iterations proceed, resulting in higher computational costs. Accordingly, we present a convergence analysis of CCE using the properties of ergodic Markov chains and propose a faster algorithm using the enhanced connectivity graph derived from the convergence analysis. Empirical evidence from numerical experiments and theoretical proofs demonstrate the advantages of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001200",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Computer science",
      "Connection (principal bundle)",
      "Convergence (economics)",
      "Correlation clustering",
      "Data mining",
      "Economic growth",
      "Economics",
      "Ergodic theory",
      "Fuzzy clustering",
      "Geometry",
      "Hierarchy",
      "Machine learning",
      "Market economy",
      "Markov chain",
      "Mathematical analysis",
      "Mathematics",
      "Programming language",
      "Single-linkage clustering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Jaemin"
      },
      {
        "surname": "Han",
        "given_name": "Minseok"
      },
      {
        "surname": "Lee",
        "given_name": "Jong-Seok"
      }
    ]
  },
  {
    "title": "SVC-onGoing: Signature verification competition",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108609",
    "abstract": "This article presents SVC-onGoing 1 1 https://competitions.codalab.org/competitions/27295. , an on-going competition for on-line signature verification where researchers can easily benchmark their systems against the state of the art in an open common platform using large-scale public databases, such as DeepSignDB 2 2 https://github.com/BiDAlab/DeepSignDB. and SVC2021_EvalDB 3 3 https://github.com/BiDAlab/SVC2021_EvalDB. , and standard experimental protocols. SVC-onGoing is based on the ICDAR 2021 Competition on On-Line Signature Verification (SVC 2021), which has been extended to allow participants anytime. The goal of SVC-onGoing is to evaluate the limits of on-line signature verification systems on popular scenarios (office/mobile) and writing inputs (stylus/finger) through large-scale public databases. Three different tasks are considered in the competition, simulating realistic scenarios as both random and skilled forgeries are simultaneously considered on each task. The results obtained in SVC-onGoing prove the high potential of deep learning methods in comparison with traditional methods. In particular, the best signature verification system has obtained Equal Error Rate (EER) values of 3.33% (Task 1), 7.41% (Task 2), and 6.04% (Task 3). Future studies in the field should be oriented to improve the performance of signature verification systems on the challenging mobile scenarios of SVC-onGoing in which several mobile devices and the finger are used during the signature acquisition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000905",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Competition (biology)",
      "Computer engineering",
      "Computer science",
      "Computer vision",
      "Ecology",
      "Engineering",
      "Geodesy",
      "Geography",
      "Geometry",
      "Mathematics",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)",
      "Signature (topology)",
      "Stylus",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Tolosana",
        "given_name": "Ruben"
      },
      {
        "surname": "Vera-Rodriguez",
        "given_name": "Ruben"
      },
      {
        "surname": "Gonzalez-Garcia",
        "given_name": "Carlos"
      },
      {
        "surname": "Fierrez",
        "given_name": "Julian"
      },
      {
        "surname": "Morales",
        "given_name": "Aythami"
      },
      {
        "surname": "Ortega-Garcia",
        "given_name": "Javier"
      },
      {
        "surname": "Carlos Ruiz-Garcia",
        "given_name": "Juan"
      },
      {
        "surname": "Romero-Tapiador",
        "given_name": "Sergio"
      },
      {
        "surname": "Rengifo",
        "given_name": "Santiago"
      },
      {
        "surname": "Caruana",
        "given_name": "Miguel"
      },
      {
        "surname": "Jiang",
        "given_name": "Jiajia"
      },
      {
        "surname": "Lai",
        "given_name": "Songxuan"
      },
      {
        "surname": "Jin",
        "given_name": "Lianwen"
      },
      {
        "surname": "Zhu",
        "given_name": "Yecheng"
      },
      {
        "surname": "Galbally",
        "given_name": "Javier"
      },
      {
        "surname": "Diaz",
        "given_name": "Moises"
      },
      {
        "surname": "Angel Ferrer",
        "given_name": "Miguel"
      },
      {
        "surname": "Gomez-Barrero",
        "given_name": "Marta"
      },
      {
        "surname": "Hodashinsky",
        "given_name": "Ilya"
      },
      {
        "surname": "Sarin",
        "given_name": "Konstantin"
      },
      {
        "surname": "Slezkin",
        "given_name": "Artem"
      },
      {
        "surname": "Bardamova",
        "given_name": "Marina"
      },
      {
        "surname": "Svetlakov",
        "given_name": "Mikhail"
      },
      {
        "surname": "Saleem",
        "given_name": "Mohammad"
      },
      {
        "surname": "Lia Szcs",
        "given_name": "Cintia"
      },
      {
        "surname": "Kovari",
        "given_name": "Bence"
      },
      {
        "surname": "Pulsmeyer",
        "given_name": "Falk"
      },
      {
        "surname": "Wehbi",
        "given_name": "Mohamad"
      },
      {
        "surname": "Zanca",
        "given_name": "Dario"
      },
      {
        "surname": "Ahmad",
        "given_name": "Sumaiya"
      },
      {
        "surname": "Mishra",
        "given_name": "Sarthak"
      },
      {
        "surname": "Jabin",
        "given_name": "Suraiya"
      }
    ]
  },
  {
    "title": "Soft pseudo-Label shrinkage for unsupervised domain adaptive person re-identification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108615",
    "abstract": "One effective way to tackle unsupervised domain adaptation (UDA) on person re-identification (Re-ID) is to use clustering-based self-training approach, where a model is trained with hard pseudo-labels obtained from a clustering method. Using a hard pseudo-label, a sample is assigned to the cluster with the highest probability, which is sensitive to the incorrect clustering result due to imperfect clustering algorithms. Soft pseudo-labels can mitigate this issue by representing the sample with the full range of class probabilities from all clusters. Specifically, soft pseudo-labels comprise probabilities of full range classes, because they consider both the hard samples and easy samples. This will distract the model from learning more discriminative features in the hard examples. To solve this issue, we propose a coarse-to-fine refinement mechanism to produce robust refined soft pseudo-labels by progressively focusing more on the hard samples while less on the easy samples. The proposed refined soft pseudo-labels can be readily integrated into cross-entropy loss as a strong supervision to guide the model to learn more discriminative features. Extensive experiments demonstrate that our proposed method outperforms the state-of-the-art unsupervised domain adaptation approaches on person Re-ID with a considerable margin. Code will be available at: http://github.com/Dingyuan-Zheng/ctf-UDA.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000966",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Classifier (UML)",
      "Cluster analysis",
      "Code (set theory)",
      "Composite material",
      "Computer science",
      "Data mining",
      "Discriminative model",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Entropy (arrow of time)",
      "Identification (biology)",
      "Machine learning",
      "Margin (machine learning)",
      "Materials science",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Range (aeronautics)",
      "Set (abstract data type)",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Dingyuan"
      },
      {
        "surname": "Xiao",
        "given_name": "Jimin"
      },
      {
        "surname": "Chen",
        "given_name": "Ke"
      },
      {
        "surname": "Huang",
        "given_name": "Xiaowei"
      },
      {
        "surname": "Chen",
        "given_name": "Lin"
      },
      {
        "surname": "Zhao",
        "given_name": "Yao"
      }
    ]
  },
  {
    "title": "Bayesian mixture of gaussian processes for data association problem",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108592",
    "abstract": "We address the data association problem and propose a Bayesian approach based on a mixture of Gaussian Processes (GPs) having two key components, the assignment probabilities and the GPs. In the proposed approach, the two key components are simultaneously updated according to observations through an efficient Expectation-Maximization (EM) algorithm that we develop. The proposed approach is thus more adaptive to the observations than the existing approaches for data association. To validate the performance of the proposed approach, we provide experimental results with real data sets as well as two synthetic data sets. We also provide a theoretical analysis to show the effectiveness of the Bayesian update.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000735",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Association (psychology)",
      "Bayesian probability",
      "Computer science",
      "Computer security",
      "Data mining",
      "Epistemology",
      "Expectation–maximization algorithm",
      "Gaussian",
      "Gaussian process",
      "Global Positioning System",
      "Key (lock)",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Maximum likelihood",
      "Mixture model",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Jeon",
        "given_name": "Younghwan"
      },
      {
        "surname": "Hwang",
        "given_name": "Ganguk"
      }
    ]
  },
  {
    "title": "Graph-based modelling of superpixels for automatic identification of empty shelves in supermarkets",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108627",
    "abstract": "Automatic detection of empty spaces (gaps) between the displayed products as seen in the images of shelves of a supermarket is an interesting image segmentation problem. This paper presents the first known attempt to solve this commercially relevant challenge. The shelf image is first over-segmented into a number of superpixels to construct a graph of superpixels (SG). Subsequently, a graph convolutional network and a Siamese network are built to process the SG. Finally, a structural support vector machine based inference model is formulated based on SG for segmenting the gap and non-gap regions. In order to validate our method, we manually annotate the images of shelves of three benchmark datasets of retail products. We have achieved ∼ 70 to ∼ 85 % segmentation accuracy (in terms of mean intersection-over-union) on the annotated datasets. A part of the annotated data is released at https://github.com/gapDetection/gapDetectionDatasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200108X",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Botany",
      "Cartography",
      "Computer science",
      "Construct (python library)",
      "Geography",
      "Graph",
      "Identification (biology)",
      "Inference",
      "Intersection (aeronautics)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Segmentation",
      "Support vector machine",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Santra",
        "given_name": "Bikash"
      },
      {
        "surname": "Ghosh",
        "given_name": "Udita"
      },
      {
        "surname": "Mukherjee",
        "given_name": "Dipti Prasad"
      }
    ]
  },
  {
    "title": "GA3N: Generative adversarial AutoAugment network",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108637",
    "abstract": "Data augmentation is beneficial for improving robustness of deep meta-learning. However, data augmentation methods for the recent deep meta-learning are still based on photometric or geometric manipulations or combinations of images. This paper proposes a generative adversarial autoaugment network (GA3N) for enlarging the augmentation search space and improving classification accuracy. To achieve, we first extend the search space of image augmentation by using GANs. However, the main challenge is to generate images suitable for the task. For solution, we find the best policy by optimizing a target and GAN losses alternatively. We then use the manipulated and generated samples determined by the policy network as augmented samples for improving the target tasks. To show the effects of our method, we implement classification networks by combining our GA3N and evaluate them on CIFAR-100 and Tiny-ImageNet datasets. As a result, we achieve better accuracy than the recent AutoAugment methods on each dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001182",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Deep learning",
      "Economics",
      "Gene",
      "Generative adversarial network",
      "Generative grammar",
      "Image (mathematics)",
      "Machine learning",
      "Management",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Chinbat",
        "given_name": "Vanchinbal"
      },
      {
        "surname": "Bae",
        "given_name": "Seung-Hwan"
      }
    ]
  },
  {
    "title": "Cross-modal propagation network for generalized zero-shot learning",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.05.009",
    "abstract": "Zero-shot learning (ZSL) aims to recognize unseen classes by transferring semantic knowledge from seen classes to unseen ones. Since only seen classes are available during training, the domain bias issue, i.e., the trained model is biased toward seen classes, is the key issue for ZSL. To alleviate the bias problem, generation-based approaches are proposed to build generative models that can generate fake visual features of unseen classes by utilizing semantic vectors. However, most of the existing generative methods still suffer some degree of domain bias caused by the ambiguous generation of fake features. In this paper, we propose a cross-modal propagation network (CMPN), which adopts an episode-based meta-learning strategy. CMPN incorporates the adaptive graph construction and label propagation into the generative ZSL framework for guaranteeing an unambiguous and discriminative fake feature generating. By further leveraging the manifold structure of different modalities in the latent space, CMPN can implicitly ensure intra-class compactness and inter-class separation through label propagation classification in latent space. Extensive experiments on four datasets validate the effectiveness of CMPN under both ZSL and generalized ZSL (GZSL) settings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001611",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Class (philosophy)",
      "Computer science",
      "Computer security",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature vector",
      "Generative grammar",
      "Generative model",
      "Graph",
      "Key (lock)",
      "Linguistics",
      "Machine learning",
      "Modal",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Polymer chemistry",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Ting"
      },
      {
        "surname": "Liang",
        "given_name": "Jianqing"
      },
      {
        "surname": "Liang",
        "given_name": "Jiye"
      },
      {
        "surname": "Xie",
        "given_name": "Guo-Sen"
      }
    ]
  },
  {
    "title": "Discriminative information restoration and extraction for weakly supervised low-resolution fine-grained image recognition",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108629",
    "abstract": "The existing methods of fine-grained image recognition mainly devote to learning subtle yet discriminative features from the high-resolution input. However, their performance deteriorates significantly when they are used for low quality images because a lot of discriminative details of images are missing. We propose a discriminative information restoration and extraction network, termed as DRE-Net, to address the problem of low-resolution fine-grained image recognition, which has widespread application potential, such as shelf auditing and surveillance scenarios. DRE-Net is the first framework for weakly supervised low-resolution fine-grained image recognition and consists of two sub-networks: (1) fine-grained discriminative information restoration sub-network (FDR) and (2) recognition sub-network with the semantic relation distillation loss (SRD-loss). The first module utilizes the structural characteristic of minimum spanning tree (MST) to establish context information for each pixel by employing the spatial structures between each pixel and other pixels, which can help FDR focus on and restore the critical texture details. The second module employs the SRD-loss to calibrate recognition sub-network by transferring the correct relationships between every two pixels on the feature map. Meanwhile the SRD-loss can further prompt the FDR to recover reliable and accurate fine-grained details and guide the recognition sub-network to perceive the discriminative features from the correct relationships. Extensive experiments on three benchmark datasets and one retail product dataset demonstrate the effectiveness of our proposed framework.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001108",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Discriminative model",
      "Feature extraction",
      "Machine learning",
      "Paleontology",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Tiantian"
      },
      {
        "surname": "Shi",
        "given_name": "Jian"
      },
      {
        "surname": "Li",
        "given_name": "Haojie"
      },
      {
        "surname": "Luo",
        "given_name": "Zhongxuan"
      },
      {
        "surname": "Wang",
        "given_name": "Zhihui"
      }
    ]
  },
  {
    "title": "Efficient deep neural network for photo-realistic image super-resolution",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108649",
    "abstract": "Recent progress in deep learning-based models has improved photo-realistic (or perceptual) single-image super-resolution significantly. However, despite their powerful performance, many methods are difficult to apply to real-world applications because of the heavy computational requirements. To facilitate the use of a deep model under such demands, we focus on keeping the network efficient while maintaining its performance. In detail, we design an architecture that implements a cascading mechanism on a residual network to boost the performance with limited resources via multi-level feature fusion. In addition, our proposed model adopts group convolution and recursive schemes in order to achieve extreme efficiency. We further improve the perceptual quality of the output by employing the adversarial learning paradigm and a multi-scale discriminator approach. The performance of our method is investigated through extensive internal experiments and benchmarks using various datasets. Our results show that our models outperform the recent methods with similar complexity, for both traditional pixel-based and perception-based tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001303",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computational complexity theory",
      "Computer engineering",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Deep learning",
      "Detector",
      "Discriminator",
      "Feature (linguistics)",
      "Focus (optics)",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Residual",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Ahn",
        "given_name": "Namhyuk"
      },
      {
        "surname": "Kang",
        "given_name": "Byungkon"
      },
      {
        "surname": "Sohn",
        "given_name": "Kyung-Ah"
      }
    ]
  },
  {
    "title": "Enhanced iris presentation attack detection via contraction-expansion CNN",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.007",
    "abstract": "The vulnerability of iris recognition algorithms against presentation attacks including textured contact lens is a serious concern. Several presentation attack detection (PAD) algorithms are developed; however, most of them suffer from low generalization capability. In this research, we propose a two head ‘contraction expansion’ convolutional neural network (CNN) architecture for robust presentation attack detection. The architecture’s input consists of raw image and edge enhanced image to learn the discriminating features for binary classification. The experiments using multiple iris presentation attack databases, including the LivDet-2017 and IIITD contact lens database (CLI), showcase the efficacy of the proposed algorithm. For example, the proposed iris presentation attack detection (IPAD) network yields 11.1% lower average classification error rate than recent state-of-the-art algorithm namely MVANet on the UnMIPA database when the system is trained on an unseen IIITD CLI database.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001039",
    "keywords": [
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Convolutional neural network",
      "Generalization",
      "Iris recognition",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Pattern recognition (psychology)",
      "Presentation (obstetrics)",
      "Radiology",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Agarwal",
        "given_name": "Akshay"
      },
      {
        "surname": "Noore",
        "given_name": "Afzel"
      },
      {
        "surname": "Vatsa",
        "given_name": "Mayank"
      },
      {
        "surname": "Singh",
        "given_name": "Richa"
      }
    ]
  },
  {
    "title": "One-net: Convolutional color constancy simplified",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.035",
    "abstract": "Images have an ever-increasing presence in our daily lives. This increases the need for accurate and efficient image processing. One of the first processing steps in modern cameras is image white-balancing, the process of making the image invariant to the illumination of the scene. This can be achieved by estimating the illumination of the scene, which is used to chromatically adapt the image. Many existing state-of-the-art approaches use pre-trained models as feature extractors. These models are pre-trained on ImageNet and usually have several million parameters. In this paper, we introduce a simple convolutional neural network without pre-trained layers, that achieves state-of-the-art results. The model contains five convolutional layers, and all of them have a small kernel of size (1,1). Experiments with different model complexities and different kernel sizes have shown that high-level semantic information obtained using larger kernels is not required to achieve state-of-the-art results. Cross camera experiments were also performed and they showed that simple image pre-processing can significantly decrease the effect of camera-sensor on the method. The proposed method has less than 22 000 parameters and achieves state-of-the-art results. The model was tested on three different datasets: the Cube+ dataset, the NUS-8 dataset, and the Intel-TAU dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001477",
    "keywords": [
      "Artificial intelligence",
      "Color constancy",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Image processing",
      "Invariant (physics)",
      "Kernel (algebra)",
      "Linguistics",
      "Mathematical physics",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)"
    ],
    "authors": [
      {
        "surname": "Domislović",
        "given_name": "Ilija"
      },
      {
        "surname": "Vršnak",
        "given_name": "Donik"
      },
      {
        "surname": "Subašić",
        "given_name": "Marko"
      },
      {
        "surname": "Lončarić",
        "given_name": "Sven"
      }
    ]
  },
  {
    "title": "AUCO ResNet: an end-to-end network for Covid-19 pre-screening from cough and breath",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108656",
    "abstract": "This study presents the Auditory Cortex ResNet (AUCO ResNet), it is a biologically inspired deep neural network especially designed for sound classification and more specifically for Covid-19 recognition from audio tracks of coughs and breaths. Differently from other approaches, it can be trained end-to-end thus optimizing (with gradient descent) all the modules of the learning algorithm: mel-like filter design, feature extraction, feature selection, dimensionality reduction and prediction. This neural network includes three attention mechanisms namely the squeeze and excitation mechanism, the convolutional block attention module, and the novel sinusoidal learnable attention. The attention mechanism is able to merge relevant information from activation maps at various levels of the network. The net takes as input raw audio files and it is able to fine tune also the features extraction phase. In fact, a Mel-like filter is designed during the training, thus adapting filter banks on important frequencies. AUCO ResNet has proved to provide state of art results on many datasets. Firstly, it has been tested on many datasets containing Covid-19 cough and breath. This choice is related to the fact that that cough and breath are language independent, allowing for cross dataset tests with generalization aims. These tests demonstrate that the approach can be adopted as a low cost, fast and remote Covid-19 pre-screening tool. The net has also been tested on the famous UrbanSound 8K dataset, achieving state of the art accuracy without any data preprocessing or data augmentation technique.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001376",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "End-to-end principle",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Preprocessor",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Dentamaro",
        "given_name": "Vincenzo"
      },
      {
        "surname": "Giglio",
        "given_name": "Paolo"
      },
      {
        "surname": "Impedovo",
        "given_name": "Donato"
      },
      {
        "surname": "Moretti",
        "given_name": "Luigi"
      },
      {
        "surname": "Pirlo",
        "given_name": "Giuseppe"
      }
    ]
  },
  {
    "title": "Bi-discriminator GAN for tabular data synthesis",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.05.023",
    "abstract": "This paper introduces a bi-discriminator GAN for synthesizing tabular datasets containing continuous, binary, and discrete columns. Our proposed approach employs an adapted preprocessing scheme and a novel conditional term using the χ β 2 distribution for the generator network to more effectively capture the input sample distributions. Additionally, we implement straightforward yet effective architectures for discriminator networks aiming at providing more discriminative gradient information to the generator. Our experimental results on four benchmarking public datasets corroborates the superior performance of our GAN both in terms of likelihood fitness metric and machine learning efficacy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001830",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Benchmarking",
      "Binary number",
      "Business",
      "Computer science",
      "Data mining",
      "Detector",
      "Discriminative model",
      "Discriminator",
      "Engineering",
      "Generator (circuit theory)",
      "Machine learning",
      "Marketing",
      "Mathematical analysis",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Physics",
      "Power (physics)",
      "Preprocessor",
      "Quantum mechanics",
      "Scheme (mathematics)",
      "Telecommunications",
      "Term (time)"
    ],
    "authors": [
      {
        "surname": "Esmaeilpour",
        "given_name": "Mohammad"
      },
      {
        "surname": "Chaalia",
        "given_name": "Nourhene"
      },
      {
        "surname": "Abusitta",
        "given_name": "Adel"
      },
      {
        "surname": "Devailly",
        "given_name": "Franşois-Xavier"
      },
      {
        "surname": "Maazoun",
        "given_name": "Wissem"
      },
      {
        "surname": "Cardinal",
        "given_name": "Patrick"
      }
    ]
  },
  {
    "title": "Exploring interactive attribute reduction via fuzzy complementary entropy for unlabeled mixed data",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108651",
    "abstract": "Attribute reduction is one of the important applications in fuzzy rough set theory. However, most attribute reduction methods in fuzzy rough theory mainly focus on removing irrelevant or redundant attributes. There are few reports about the method of considering attribute interaction. For this reason, this paper proposes an interactive attribute reduction method for unlabeled mixed data. First, some uncertainty measures based on fuzzy complementary entropy are further defined. Then, based on the proposed uncertainty measure, the attribute evaluation criteria of maximal information, minimal redundancy, and maximal interactivity are developed respectively. As a result, the evaluation index of the attribute importance is established by using the idea of unsupervised maximal information-minimal redundancy-maximal interactivity. Finally, a corresponding algorithm is designed to select attributes. The experimental results show that the proposed algorithm has better performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001327",
    "keywords": [
      "Artificial intelligence",
      "Attribute domain",
      "Computer science",
      "Data mining",
      "Entropy (arrow of time)",
      "Fuzzy logic",
      "Fuzzy set",
      "Interactivity",
      "Mathematics",
      "Multimedia",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Redundancy (engineering)",
      "Rough set"
    ],
    "authors": [
      {
        "surname": "Yuan",
        "given_name": "Zhong"
      },
      {
        "surname": "Chen",
        "given_name": "Hongmei"
      },
      {
        "surname": "Li",
        "given_name": "Tianrui"
      }
    ]
  },
  {
    "title": "On computational aspects of high-order dual Hahn moments",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108596",
    "abstract": "In this paper, we present two new algorithms for the fast and stable computation of high-order discrete orthogonal dual Hahn polynomials (DHPs). These algorithms are essentially based on the proposed computation method of the initial values of DHPs following the order n and the variable s. For both algorithms, a single stable value is computed, fully independent of the gamma function that is the source of the numerical overflow, and then the rest of DHPs values are computed recursively via the proposed recurrence scheme. By analyzing the DHPs matrix, we propose a new method, which allows ensuring the numerical stability of high-order DHPs and dual Hahn moments (DHMs) until the last order. This method is based on the use of appropriate stability conditions. The results of simulations and comparisons carried out show on one hand that the second algorithm with the stability condition allows to compute DHPs up to the order n = 17,603 without propagation of numerical error. On the other hand, the performance of analyzing large-size signals and images by high-order DHMs computed by the proposed method significantly exceeds the existing methods in terms of numerical stability, accuracy of reconstruction and in terms of maximum size of the analyzed signals and images. After the acceptance of this paper, the proposed algorithms for high-order DHPs computation will be made publically available at https://github.com/AchrafDaoui/On-Computational-Aspects-of-High-Order-Dual-Hahn-Moments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000772",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Art",
      "Biology",
      "Computation",
      "Computer science",
      "DHPS",
      "Dual (grammatical number)",
      "Economics",
      "Estimator",
      "Finance",
      "Immunology",
      "Literature",
      "Machine learning",
      "Malaria",
      "Mathematical analysis",
      "Mathematics",
      "Method of moments (probability theory)",
      "Numerical analysis",
      "Numerical stability",
      "Order (exchange)",
      "Plasmodium falciparum",
      "Stability (learning theory)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Daoui",
        "given_name": "Achraf"
      },
      {
        "surname": "Karmouni",
        "given_name": "Hicham"
      },
      {
        "surname": "Yamni",
        "given_name": "Mohamed"
      },
      {
        "surname": "Sayyouri",
        "given_name": "Mhamed"
      },
      {
        "surname": "Qjidaa",
        "given_name": "Hassan"
      }
    ]
  },
  {
    "title": "Retraction notice to “Deep learning for real-time semantic segmentation: Application in ultrasound imaging” [Pattern Recognition Letters 144 (2021) 27–34]",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.033",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001453",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Law",
      "Medicine",
      "Notice",
      "Pattern recognition (psychology)",
      "Political science",
      "Radiology",
      "Segmentation",
      "Ultrasound",
      "Ultrasound imaging"
    ],
    "authors": [
      {
        "surname": "Ouahabi",
        "given_name": "Abdeldjalil"
      },
      {
        "surname": "Taleb-Ahmed",
        "given_name": "Abdelmalik"
      }
    ]
  },
  {
    "title": "EEPNet: An efficient and effective convolutional neural network for palmprint recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.05.015",
    "abstract": "Palmprint recognition is an important biometrics technology. In recent years, a lot of palmprint recognition methods based on convolution neural networks (CNNs) have been proposed. However, existing CNNs specially designed for palmprint recognition have high computational complexity. In order to make palmprint recognition method based on deep learning work well on mobile devices, lightweight neural networks must be used. However, up to now, there is very little research on this topic. In this paper, we propose an efficient and effective palmprint recognition network (EEPNet), which is a lightweight neural network. EEPNet is designed based on MobileNet-V3, and further compresses the number of layers and enlarges the convolution kernel. In addition, we design two new loss functions including Balanced Loss and Contrast Loss. Balanced Loss is suitable for various specific data sets, while Contrast Loss can achieve the purpose of training difficult samples without manual parameter adjustment. According to the characteristics of palmprint recognition, we add five strategies to improve the recognition performance including image splicing, image dimension reduction, data augmentation, cascade channel attention mechanism, and hard case mining mechanism. We conduct thorough experiments on seven palmprint databases. The experimental results show that the overall recognition performance of our method outperforms classic and state-of-the-art palmprint recognition methods on the palmprint databases with normal quality. We compare our method with other CNNs in four aspects: precision, speed, parameter quantity and FLOPs. The experimental results show that our method is more efficient and has high recognition accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001738",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biometrics",
      "Combinatorics",
      "Computer science",
      "Contrast (vision)",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Deep learning",
      "Facial recognition system",
      "Kernel (algebra)",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Jia",
        "given_name": "Wei"
      },
      {
        "surname": "Ren",
        "given_name": "Qiang"
      },
      {
        "surname": "Zhao",
        "given_name": "Yang"
      },
      {
        "surname": "Li",
        "given_name": "Shujie"
      },
      {
        "surname": "Min",
        "given_name": "Hai"
      },
      {
        "surname": "Chen",
        "given_name": "Yanxiang"
      }
    ]
  },
  {
    "title": "Temporal-adaptive sparse feature aggregation for video object detection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108587",
    "abstract": "Video object detection is a challenging task due to the appearance deterioration in video frames. To enhance feature representation of the deteriorated frames, previous methods usually aggregate features from fixed-density and fixed-length adjacent frames. However, due to the redundancy of videos and irregular object movements over time, temporal information may not be efficiently exploited using the traditional inflexible strategy. Alternatively, we present a temporal-adaptive sparse feature aggregation framework, an accurate and efficient method for video object detection. Instead of adopting a fixed-density and fixed-length window fusion strategy, a temporal-adaptive sparse sampling strategy is proposed using a stride predictor to encode informative frames more efficiently. A collaborative feature aggregation framework, which consists of a pixel-adaptive aggregation module and an object-relational aggregation module, is proposed for feature enhancement. The pixel-adaptive aggregation module enhances pixel-level features on the current frame using corresponding pixel-level features from other frames. Similarly, the object-relational aggregation module further enhances feature representation at proposal level. A graph is constructed to model the relations between different proposals so that the relation features and proposal features are adaptively fused for feature enhancement. Experiments demonstrate that our proposed framework significantly surpasses traditional dense aggregation methods, and comprehensive ablation studies verify the effectiveness of each proposed module in our framework.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000681",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Data structure",
      "Feature (linguistics)",
      "Law",
      "Linguistics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Political science",
      "Politics",
      "Programming language",
      "Redundancy (engineering)",
      "Representation (politics)",
      "Tree structure"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Fei"
      },
      {
        "surname": "Li",
        "given_name": "Qiaozhe"
      },
      {
        "surname": "Zhao",
        "given_name": "Xin"
      },
      {
        "surname": "Huang",
        "given_name": "Kaiqi"
      }
    ]
  },
  {
    "title": "Unsupervised domain adaptation via distilled discriminative clustering",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108638",
    "abstract": "Unsupervised domain adaptation addresses the problem of classifying data in an unlabeled target domain, given labeled source domain data that share a common label space but follow a different distribution. Most of the recent methods take the approach of explicitly aligning feature distributions between the two domains. Differently, motivated by the fundamental assumption for domain adaptability, we re-cast the domain adaptation problem as discriminative clustering of target data, given strong privileged information provided by the closely related, labeled source data. Technically, we use clustering objectives based on a robust variant of entropy minimization that adaptively filters target data, a soft Fisher-like criterion, and additionally the cluster ordering via centroid classification. To distill discriminative source information for target clustering, we propose to jointly train the network using parallel, supervised learning objectives over labeled source data. We term our method of distilled discriminative clustering for domain adaptation as DisClusterDA. We also give geometric intuition that illustrates how constituent objectives of DisClusterDA help learn class-wisely pure, compact feature distributions. We conduct careful ablation studies and extensive experiments on five popular benchmark datasets, including a multi-source domain adaptation one. Based on commonly used backbone networks, DisClusterDA outperforms existing methods on these benchmarks. It is also interesting to observe that in our DisClusterDA framework, adding an additional loss term that explicitly learns to align class-level feature distributions across domains does harm to the adaptation performance, though more careful studies in different algorithmic frameworks are to be conducted.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001194",
    "keywords": [
      "Artificial intelligence",
      "Centroid",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Discriminative model",
      "Machine learning",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Hui"
      },
      {
        "surname": "Wang",
        "given_name": "Yaowei"
      },
      {
        "surname": "Jia",
        "given_name": "Kui"
      }
    ]
  },
  {
    "title": "Editorial for the special issue on implicit biometric authentication and monitoring through Internet of Biometric Things (I-BIO)",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.05.020",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001805",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Encoder",
      "Feature (linguistics)",
      "Feature learning",
      "Feature vector",
      "Linguistics",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Ricciardi",
        "given_name": "Stefano"
      },
      {
        "surname": "Castrillòn Santana",
        "given_name": "Modesto"
      }
    ]
  },
  {
    "title": "Online product sentiment analysis using random evolutionary whale optimization algorithm and deep belief network",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.024",
    "abstract": "In the recent decades, the online product sentiment analysis is an emerging research topic that assists the customers to take better decisions on purchasing the products and to achieve better sales of the products. Recently, several machine learning techniques are experimented on many datasets for analyzing the customer's sentiments through online portals. Still, the customers are struggling to obtain the aspect sentiments expressed by other customers, particularly in the amazon websites. Therefore, a novel automated model is proposed in this manuscript for an effective online product sentiment analysis. After collecting the multimodal data from the amazon websites, the image and data normalization techniques are employed for better understanding of the collected data. Further, the feature extraction is performed by utilizing Latent Semantic Analysis (LSA), Term Frequency- Inverse Document Frequency (TF-IDF), Modified Local Binary Pattern (MLBP), and Speeded Up Robust Features (SURF) descriptors for extracting the textual and visual feature vectors from the preprocessed data. Finally, the Random Evolutionary Whale Optimization Algorithm (REWOA) and Deep Belief Network (DBN) classifier are integrated for feature vector optimization and sentiment classification. By using feature optimization, the system complexity and running time of the classifier is improved. The experimental investigation states that the developed REWOA-DBN model achieved 96.86% of classification accuracy, which is better compared to other optimizers and classifiers",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001210",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Deep belief network",
      "Fishery",
      "Geometry",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Optimization algorithm",
      "Product (mathematics)",
      "Sentiment analysis",
      "Whale"
    ],
    "authors": [
      {
        "surname": "Mehbodniya",
        "given_name": "Abolfazl"
      },
      {
        "surname": "Rao",
        "given_name": "M. Varaprasad"
      },
      {
        "surname": "David",
        "given_name": "Leo Gertrude"
      },
      {
        "surname": "Joe Nigel",
        "given_name": "K. Gerard"
      },
      {
        "surname": "Vennam",
        "given_name": "Preethi"
      }
    ]
  },
  {
    "title": "Cross-lingual transfer learning: A PARAFAC2 approach",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.05.008",
    "abstract": "The proposed framework addresses the problem of cross-lingual transfer learning resorting to Parallel Factor Analysis 2 (PARAFAC2). To avoid the need for multilingual parallel corpora, a pairwise setting is adopted where a PARAFAC2 model is fitted to documents written in English (source language) and a different target language. Firstly, an unsupervised PARAFAC2 model is fitted to parallel unlabelled corpora pairs to learn the latent relationship between the source and target language. The fitted model is used to create embeddings for a text classification task (document classification or authorship attribution). Subsequently, a logistic regression classifier is fitted to the training source language embeddings and tested on the training target language embeddings. Following the zero-shot setting, no labels are exploited for the target language documents. The proposed framework incorporates a self-learning process by utilizing the predicted labels as pseudo-labels to train a new, pseudo-supervised PARAFAC2 model, which aims to extract latent class-specific information while fusing language-specific information. Thorough evaluation is conducted on cross-lingual document classification and cross-lingual authorship attribution. Remarkably, the proposed framework achieves competitive results when compared to deep learning methods in cross-lingual transfer learning tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001593",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Machine learning",
      "Natural language processing",
      "Pairwise comparison",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Pantraki",
        "given_name": "Evangelia"
      },
      {
        "surname": "Tsingalis",
        "given_name": "Ioannis"
      },
      {
        "surname": "Kotropoulos",
        "given_name": "Constantine"
      }
    ]
  },
  {
    "title": "A robust handwritten recognition system for learning on different data restriction scenarios",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.009",
    "abstract": "Handwritten Text Recognition (HTR) systems have gained interest in fields of academic research and commercial applications. Deep learning techniques, and more precisely Convolutional Neural Networks (CNNs), have enabled many recent successes in the computer vision community. However, due to high computational costs, applying CNNs to many real applications is challenging since the specific training data is restricted in many cases. Therefore, in this paper, we present a Gated-CNN-BGRU optical model capable of dealing with this complex challenge. The proposed model was evaluated on five well-known datasets in HTR (Bentham, IAM, RIMES, Saint Gall, and Washington). Additionally, we redefine the training and validation partitions for each dataset, progressively varying the percentage of data between both partitions to create a total of 50 scenarios with different data volumes. The experiment validates and shows that the proposed model presents statistically significant results, surpassing the current models by an average of 2.96 and 8.91 percentage points in character and word recognition accuracy. In the most complex scenario of using 49 images for training, we achieved character and word precision of 87.25% and 71.54% respectively. That means an improvement of 78.32 and 53.54 percentage points, respectively, of the state-of-the-art optical models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001052",
    "keywords": [
      "Artificial intelligence",
      "Character (mathematics)",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Geometry",
      "Image (mathematics)",
      "Machine learning",
      "Mathematics",
      "Optical character recognition",
      "Pattern recognition (psychology)",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Sousa Neto",
        "given_name": "Arthur Flor de"
      },
      {
        "surname": "Leite Dantas Bezerra",
        "given_name": "Byron"
      },
      {
        "surname": "Hector Toselli",
        "given_name": "Alejandro"
      },
      {
        "surname": "Baptista Lima",
        "given_name": "Estanislau"
      }
    ]
  },
  {
    "title": "Texture analysis using two-dimensional permutation entropy and amplitude-aware permutation entropy",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.05.017",
    "abstract": "Entropy algorithms have been applied extensively for time series analysis. The entropy value given by the algorithm quantifies the irregularity of the data structure. For higher irregular data structures, the entropy is higher. Both permutation entropy (PE) and amplitude-aware permutation entropy (AAPE) have been previously used to analyze time series. These two metrics have the advantage, over others, of being computationally fast and simple. However, fewer entropy measures have been proposed to process images. Two-dimensional entropy algorithms can be used to study texture and analyze the irregular structure of images. Herein, we propose the extension of AAPE for two-dimensional analysis (AAPE 2 D ). To the best of our knowledge, AAPE 2 D has never been proposed to analyze texture of images. For comparison purposes, we also study the two-dimensional permutation entropy (PE 2 D ) to analyze the effect of the amplitude consideration in texture analysis. In this study, we compare AAPE 2 D method with PE 2 D in terms of irregularity discrimination, parameters sensitivity, and artificial texture differentiation. Both AAPE 2 D and PE 2 D appear to be interesting entropy-based approaches for image texture analysis. When applied to a biomedical dataset of chest X-rays with healthy subjects and pneumonia patients, both methods showed to statistically differentiate both groups for P < 0.01 . Finally, using a SVM model and multiscale entropy values as features, AAPE 2 D achieves an average of 75.7% accuracy which is slightly better than the results of PE 2 D . Overall, both entropy algorithms are promising and achieve similar conclusions. This work is a new step towards the development of other entropy-based texture measures.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001775",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Entropy (arrow of time)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Gaudêncio",
        "given_name": "Andreia S."
      },
      {
        "surname": "Hilal",
        "given_name": "Mirvana"
      },
      {
        "surname": "Cardoso",
        "given_name": "João M."
      },
      {
        "surname": "Humeau-Heurtier",
        "given_name": "Anne"
      },
      {
        "surname": "Vaz",
        "given_name": "Pedro G."
      }
    ]
  },
  {
    "title": "A transductive learning method to leverage graph structure for few-shot learning",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.05.013",
    "abstract": "Few-shot learning has attracted extensive research attention given its capability to classify unseen data from limited samples, potentially addressing the key issue of data scarcity commonly existing in many machine learning tasks. This paper proposes a new transductive learning method that integrates information propagation and prototype rectification in few-shot learning, which achieves state-of-the-art classification performance on four popular datasets. We use first-order information propagation instead of infinite order method to avoid the over-smoothing caused by iterations of information aggregation and node updating in graph neural networks. We further reveal that current transductive few-shot learning models often assume the datasets have balanced classes, which cannot be guaranteed in practice. We thus propose to estimate the distribution of task samples to optimize the number of iterations so as to enhance the robustness of the model. Extensive experiments validate the proposed model and reveal the confirmation bias that could be effectively addressed by the optimization strategy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001696",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Graph",
      "Interpretability",
      "Leverage (statistics)",
      "Machine learning",
      "Smoothing",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yaning"
      },
      {
        "surname": "Liu",
        "given_name": "Zijian"
      },
      {
        "surname": "Luo",
        "given_name": "Yang"
      },
      {
        "surname": "Luo",
        "given_name": "Chunbo"
      }
    ]
  },
  {
    "title": "Referring expression grounding by multi-context reasoning",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.05.024",
    "abstract": "Referring expression grounding plays a fundamental role in vision-language understanding, which aims at locating a certain target region in an image described by a natural language expression. It needs to understand high-level semantic correlations between objects in the image according to the referred expression for the task. Thus, it inherently requires reasoning the context information, i.e., appearance context and relationship context. While most existing approaches either ignore to explore the appearance details of the target region or rely on a manually designed reasoning structure and treat the context information of each neighboring object equivalently, inflexible to the scenario where referring expressions are complicated. In this paper, we put forward Multi-context Reasoning Network (MCRN) for referring expression grounding task, which can apply appearance context reasoning and relationship context reasoning simultaneously. Methodologically, for appearance context reasoning, we propose a local node attention to obtain local representation of the target object, which gives a more focus on its appearance details. For relationship context reasoning, we approach it as a language-guided multi-step reasoning problem and design a multi-step graph reasoning module to capture intra-context and inter-context between the target region of its intra-class and inter-class neighboring objects in an iterative way, which makes the reasoning process more reliable and interpretable. Our method demonstrates superiority based on extensive experimental outputs on three popular benchmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001842",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Causal reasoning",
      "Class (philosophy)",
      "Cognition",
      "Computer science",
      "Context (archaeology)",
      "Context model",
      "Economics",
      "Expression (computer science)",
      "Focus (optics)",
      "Geodesy",
      "Geography",
      "Management",
      "Natural language processing",
      "Neuroscience",
      "Object (grammar)",
      "Optics",
      "Paleontology",
      "Physics",
      "Process (computing)",
      "Programming language",
      "Reasoning system",
      "Spatial contextual awareness",
      "Task (project management)",
      "Visual reasoning"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xing"
      },
      {
        "surname": "Xie",
        "given_name": "De"
      },
      {
        "surname": "Zheng",
        "given_name": "Yuanshi"
      }
    ]
  },
  {
    "title": "Towards generalizable detection of face forgery via self-guided model-agnostic learning",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.06.007",
    "abstract": "Face forgery detection is an important yet challenging task that aims to distinguish whether a face video has been modified. As various types of face forgery are constantly produced and made available, existing methods usually overfit to the manipulation methods they are trained for, and cannot generalize well to detect the unseen or unknown forgery types. To address this issue, we present a systematic study on a more generalizable solution of face forgery detection, which endows the model an ability to recognize fake videos with unpredictable forgery types. Specifically, we develop a model-agnostic learning approach with a gradient-based meta-train and meta-test procedure to simulate the domain shift from known to unknown forgery types. To further emphasize the relative importance of different available forgery types during training, we propose a self-guided importance sampling strategy, which is integrated with a general video-level classification network. We also build a dataset with a wide range of 10 different forgery types to benchmark the generalization ability of face forgery detection. Extensive experiments on multiple testing protocols of evaluating generalization ability show that our method generalizes significantly better on unknown forgery manipulations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552200201X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Domain (mathematical analysis)",
      "Economics",
      "Face (sociological concept)",
      "Face detection",
      "Facial recognition system",
      "Generalization",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Social science",
      "Sociology",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Xiao"
      },
      {
        "surname": "Liu",
        "given_name": "Shilong"
      },
      {
        "surname": "Dong",
        "given_name": "Yinpeng"
      },
      {
        "surname": "Su",
        "given_name": "Hang"
      },
      {
        "surname": "Zhang",
        "given_name": "Lei"
      },
      {
        "surname": "Zhu",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "End-to-end weakly supervised semantic segmentation with reliable region mining",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108663",
    "abstract": "Weakly supervised semantic segmentation is a challenging task that only takes image-level labels as supervision but produces pixel-level predictions for testing. To address such a challenging task, most current approaches generate pseudo pixel masks first that are then fed into a separate semantic segmentation network. However, these two-step approaches suffer from high complexity and being hard to train as a whole. In this work, we harness the image-level labels to produce reliable pixel-level annotations and design a fully end-to-end network to learn to predict segmentation maps. Concretely, we firstly leverage an image classification branch to generate class activation maps for the annotated categories, which are further pruned into tiny reliable object/background regions. Such reliable regions are then directly served as ground-truth labels for the segmentation branch, where both global information and local information sub-branches are used to generate accurate pixel-level predictions. Furthermore, a new joint loss is proposed that considers both shallow and high-level features. Despite its apparent simplicity, our end-to-end solution achieves competitive mIoU scores (val: 65.4%, test: 65.3%) on Pascal VOC compared with the two-step counterparts. By extending our one-step method to two-step, we get a new state-of-the-art performance on the Pascal VOC 2012 dataset(val: 69.3%, test: 69.2%). Code is available at: https://github.com/zbf1991/RRM.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001443",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "End-to-end principle",
      "Ground truth",
      "Image segmentation",
      "Leverage (statistics)",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Pixel",
      "Programming language",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Bingfeng"
      },
      {
        "surname": "Xiao",
        "given_name": "Jimin"
      },
      {
        "surname": "Wei",
        "given_name": "Yunchao"
      },
      {
        "surname": "Huang",
        "given_name": "Kaizhu"
      },
      {
        "surname": "Luo",
        "given_name": "Shan"
      },
      {
        "surname": "Zhao",
        "given_name": "Yao"
      }
    ]
  },
  {
    "title": "Nakagami-Fuzzy imaging framework for precise lesion segmentation in MRI",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108675",
    "abstract": "Nakagami distribution and related imaging methods are very efficient in diagnostic ultrasonography for visualization and characterization of tissues for years. Abnormalities in tissues are distinguished from surrounding cells by application of the distribution ruled by the Nakagami m-parameter. The potential of discrimination in ultrasonography enables intelligent segmentation of lesions by other diagnostic tools and the imaging technique is very promising in other areas of medicine, like magnetic resonance imaging (MRI) for brain lesion identification, as presented in this paper. Therefore, we propose a novel Nakagami-Fuzzy imaging framework for intelligent and fully automated suspicious region segmentation from axial FLAIR MRI images exhibiting brain tumor characteristics to satisfy ground truth images with different precision levels. The images from MRI data set are processed by applying Nakagami distribution from pre-Rayleigh to post-Rayleigh for adjusting m-parameter. Amorphous and non-homogenous suspicious regions revealed by Nakagami imaging are segmented using customized Fuzzy 2-means to compare with two types of binary ground truths. The framework we propose is an outstanding example of fuzzy-based expert systems providing an average of 92.61% dice score for the main clinical experiment we conducted using the images and two types of ground truths provided by University of Hospital, Hradec Kralove. We also tested our framework by the BraTS 2012 and BraTS 2020 datasets and achieved an average of 91.88% and 89.25% dice scores respectively, which are competitive among the relevant researches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200156X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Decoding methods",
      "Fading",
      "Fuzzy logic",
      "Ground truth",
      "Image segmentation",
      "Magnetic resonance imaging",
      "Medical imaging",
      "Medicine",
      "Nakagami distribution",
      "Pattern recognition (psychology)",
      "Radiology",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Alpar",
        "given_name": "Orcan"
      },
      {
        "surname": "Dolezal",
        "given_name": "Rafael"
      },
      {
        "surname": "Ryska",
        "given_name": "Pavel"
      },
      {
        "surname": "Krejcar",
        "given_name": "Ondrej"
      }
    ]
  },
  {
    "title": "GSTA: Pedestrian trajectory prediction based on global spatio-temporal association of graph attention network",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.06.011",
    "abstract": "Most encoder-decoder structure based predictions models usually predict trajectory according to the position and historical movement of nearby pedestrians. Their input range (receptive field) is small. They often ignore some specific information such as the speed and direction of pedestrians’ movement or the temporal attention. This leads to detailed pedestrian interaction that cannot be obtained. Therefore, we propose a novel spatio-temporal graph attention network (GAT) called GSTA. In the spatial domain, GSTA obtains complex interaction by spatial attention (SA) based on multi-feature fusion, and expands the receptive field through feature updating mechanism (FUM). In the temporal domain, we design temporal attention module (TAM) and feature selection module (FSM). TAM is used to discover the internal relationship of historical trajectory and solve the problem that temporal attention is averaged. FSM overcomes the adverse effect of small temporal perceptual range and reasonably controls the flow of feature information. Experimental results on 5 commonly used pedestrian trajectory datasets show that the prediction accuracy of our proposed model is further improved.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002057",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Astronomy",
      "Computer science",
      "Encoder",
      "Feature (linguistics)",
      "Geography",
      "Graph",
      "Linguistics",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Pedestrian",
      "Philosophy",
      "Physics",
      "Theoretical computer science",
      "Trajectory"
    ],
    "authors": [
      {
        "surname": "Kong",
        "given_name": "Wei"
      },
      {
        "surname": "Liu",
        "given_name": "Yun"
      },
      {
        "surname": "Li",
        "given_name": "Hui"
      },
      {
        "surname": "Wang",
        "given_name": "Chuanxu"
      },
      {
        "surname": "Tao",
        "given_name": "Ye"
      },
      {
        "surname": "Kong",
        "given_name": "Xiangzhen"
      }
    ]
  },
  {
    "title": "Possibility results for graph clustering: A novel consistency axiom",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108687",
    "abstract": "Kleinberg introduced three natural clustering properties, or axioms, and showed they cannot be simultaneously satisfied by any clustering algorithm. We present a new clustering property, Monotonic Consistency, which avoids the well-known problematic behaviour of Kleinberg’s Consistency axiom, and the impossibility result. Namely, we describe a clustering algorithm, Morse Clustering, inspired by Morse Theory in Differential Topology, which satisfies Kleinberg’s original axioms with Consistency replaced by Monotonic Consistency. Morse clustering uncovers the underlying flow structure on a set or graph and returns a partition into trees representing basins of attraction of critical vertices. We also generalise Kleinberg’s axiomatic approach to sparse graphs, showing an impossibility result for Consistency, and a possibility result for Monotonic Consistency and Morse clustering.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001686",
    "keywords": [
      "Axiom",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Consistency (knowledge bases)",
      "Discrete mathematics",
      "Geometry",
      "Impossibility",
      "Law",
      "Mathematical analysis",
      "Mathematics",
      "Monotonic function",
      "Morse code",
      "Political science",
      "Statistics",
      "Telecommunications",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Strazzeri",
        "given_name": "Fabio"
      },
      {
        "surname": "Sánchez-García",
        "given_name": "Rubén J."
      }
    ]
  },
  {
    "title": "Preserving similarity order for unsupervised clustering",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108670",
    "abstract": "Unsupervised clustering categorizes a sample set into several groups, where the samples in the same group share high-level concepts. As the clustering performances are heavily determined by the metric to assess the similarity between sample pairs, we propose to learn a deep similarity score function and use it to capture the correlations between sample pairs for improved clustering. We formulate the learning procedure in a ranking framework and introduce two new supervisory signals to train our model. Specifically, we train the similarity score function to guarantee 1) a sample should have a higher level of similarity with its nearest neighbors than others in order to achieve correct clustering, and 2) the ordering of the similarity between neighboring sample pairs should be preserved in order to achieve robust clustering. To this end, we not only study the relevance between neighboring sample pairs for local structure learning, but also study the relevance between each sample and the boundary samples for global structure learning. Extensive experiments on seven public available datasets validate the effectiveness of our proposed framework, including face image clustering, object image clustering, and real-world image clustering.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001510",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Economics",
      "Fuzzy clustering",
      "Image (mathematics)",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Ranking (information retrieval)",
      "Sample (material)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jinghua"
      },
      {
        "surname": "Wang",
        "given_name": "Li"
      },
      {
        "surname": "Jiang",
        "given_name": "Jianmin"
      }
    ]
  },
  {
    "title": "Total Bregman divergence-driven possibilistic fuzzy clustering with kernel metric and local information for grayscale image segmentation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108686",
    "abstract": "Kernel possibilistic fuzzy C-means with local information (KWPFLICM) has important research significance of image segmentation, but it is very sensitive to high noise or outliers. To enhance the segmentation performance of the algorithm, this paper proposes a kernelized total Bregman divergence-driven possibilistic fuzzy clustering with local information (TKWPFLICM). Firstly, a polynomial kernel function is introduced to kernelize total Bregman divergence (TBD), and local neighborhood information of the pixel is used to modify it, which overcomes the shortcomings of Bregman divergence (BD) with rotation variability; Secondly, the modified kernelized TBD and possibilistic typicality are combined to further enhance the anti-noise ability of the algorithm; Finally, the modified kernelized TBD is introduced into the objective function of KWPFLICM algorithm, then a novel robust fuzzy clustering algorithm is derived by optimization theory. Experimental results show that compared with existing fuzzy clustering-related algorithms, the average SA improvement on TKWPFLICM algorithm is in the range of 0.791% to 33.237%. Therefore, TKWPFLICM algorithm has better anti-noise robustness and segmentation accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001674",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Bregman divergence",
      "Chemistry",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Divergence (linguistics)",
      "Fuzzy clustering",
      "Fuzzy logic",
      "Gene",
      "Image segmentation",
      "Kernel (algebra)",
      "Linguistics",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Robustness (evolution)",
      "Segmentation",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Chengmao"
      },
      {
        "surname": "Zhang",
        "given_name": "Xue"
      }
    ]
  },
  {
    "title": "Human interaction recognition framework based on interacting body part attention",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108645",
    "abstract": "Human activity recognition in videos has been widely studied and has recently gained significant advances with deep learning approaches; however, it remains a challenging task. In this paper, we propose a novel framework that simultaneously considers both implicit and explicit representations of human interactions by fusing information of local image where the interaction actively occurred, primitive motion with the posture of individual subject’s body parts, and the co-occurrence of overall appearance change. Human interactions change, depending on how the body parts of each human interact with the other. The proposed method captures the subtle difference between different interactions using interacting body part attention. Semantically important body parts that interact with other objects are given more weight during feature representation. The combined feature of interacting body part attention-based individual representation and the co-occurrence descriptor of the full-body appearance change is fed into long short-term memory to model the temporal dynamics over time in a single framework. The experimental results on five widely used public datasets demonstrate the effectiveness of the proposed method to recognize human interactions from videos.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001261",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Economics",
      "Feature (linguistics)",
      "Human body",
      "Law",
      "Linguistics",
      "Management",
      "Motion (physics)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Dong-Gyu"
      },
      {
        "surname": "Lee",
        "given_name": "Seong-Whan"
      }
    ]
  },
  {
    "title": "Baby steps towards few-shot learning with multiple semantics",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.06.012",
    "abstract": "Learning from one or few visual examples is one of the key capabilities of humans since early infancy, but is still a significant challenge for modern AI systems. While considerable progress has been achieved in few-shot learning from a few image examples, much less attention has been given to the verbal descriptions that are usually provided to infants when they are presented with a new object. In this paper, we focus on the role of additional semantics that can significantly facilitate few-shot visual learning. Building upon recent advances in few-shot learning with additional semantic information, we demonstrate that further improvements are possible by combining multiple and richer semantics (category labels, attributes, and natural language descriptions). Using these ideas, we offer the community new results on the popular miniImageNet and CUB few-shot benchmarks, comparing favorably to the previous state-of-the-art results for both visual only and visual plus semantics-based approaches. We also performed an ablation study investigating the components and design choices of our approach. Code available on github.com/EliSchwartz/mutiple-semantics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002069",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Focus (optics)",
      "Key (lock)",
      "Natural language processing",
      "Object (grammar)",
      "Optics",
      "Organic chemistry",
      "Physics",
      "Programming language",
      "Semantics (computer science)",
      "Shot (pellet)"
    ],
    "authors": [
      {
        "surname": "Schwartz",
        "given_name": "Eli"
      },
      {
        "surname": "Karlinsky",
        "given_name": "Leonid"
      },
      {
        "surname": "Feris",
        "given_name": "Rogerio"
      },
      {
        "surname": "Giryes",
        "given_name": "Raja"
      },
      {
        "surname": "Bronstein",
        "given_name": "Alex"
      }
    ]
  },
  {
    "title": "Directionally Weighted Distance for Minutiae-Triplets Preservation on Elastic Deformation of Fingerprint Recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.05.027",
    "abstract": "Most fingerprint matching methods suffer from elastic deformation of fingerprints, resulting in an increment of the false-rejection rate. We propose a new distance model for the minutiae-triplets formation that can remedy the elastic deformation of fingerprints. The new distance model, called a directionally weighted distance model, provides higher priority to neighboring minutiae within the same direction or the same ridge flow of the observed minutia. We introduce two methods that apply the proposed distance model to the minutiae-triplets formation. While the first method directly applies the model, the second method combines the model with ridge flow to handle highly curved areas such as singular-point areas or highly distorted fingerprint areas. We evaluate the proposed methods using two minutiae-triplets matching algorithms on sixteen public domain fingerprint databases. The experimental results show that the proposed distance model can significantly improve the accuracy of both matching algorithms on most databases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001878",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deformation (meteorology)",
      "Fingerprint (computing)",
      "Fingerprint recognition",
      "Geology",
      "Mathematics",
      "Minutiae",
      "Oceanography",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Rungchokanun",
        "given_name": "Arucha"
      },
      {
        "surname": "Areekul",
        "given_name": "Vutipong"
      }
    ]
  },
  {
    "title": "Causal GraphSAGE: A robust graph method for classification based on causal sampling",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108696",
    "abstract": "GraphSAGE is a widely-used graph neural network for classification, which generates node embeddings in two steps: sampling and aggregation. In this paper, we introduce causal inference into the GraphSAGE sampling stage, and propose Causal GraphSAGE (C-GraphSAGE) to improve the robustness of the classifier. In C-GraphSAGE, we use causal bootstrapping to obtain a weighting between the target node's neighbors and their label. Then, these weights are used to resample the node's neighbors to enforce the robustness of the sampling stage. Finally, an aggregation function is trained to integrate the features of the selected neighbors to obtain the embedding of the target node. Experimental results on the Cora, Pubmed, and Citeseer citation datasets show that the classification performance of C-GraphSAGE is equivalent to that of GraphSAGE, GCN, GAT, and RL-GraphSAGE in the case of no perturbation, and outperforms these as the perturbation ratio increases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001777",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Graph",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Tao"
      },
      {
        "surname": "Shan",
        "given_name": "Hao-Ran"
      },
      {
        "surname": "Little",
        "given_name": "Max A."
      }
    ]
  },
  {
    "title": "Interpolation-based nonrigid deformation estimation under manifold regularization constraint",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108695",
    "abstract": "This paper addresses the image/surface deformation problem by estimating interpolation functions pixel by pixel(or voxel by voxel) between control point pairs using labeled control points and unlabeled feature points as input. The labeled control points are usually selected by users and labeled through user operations; the unlabeled feature points are extracted from the source image. We formulate the interpolation function estimation at each pixel as a weighted semi-supervised learning problem. Specially, we employ moving least squares to estimate the nonrigid deformation function according to the weights between each pixel and the labeled control points and exploit manifold regularization to preserve the intrinsic geometric information of the unlabeled feature points contained in the object. Moreover, we define the nonrigid deformation function in a reproducing kernel Hilbert space to derive a closed-form solution. To reduce the computational complexity, we also adopt a sparse approximation to realize a fast implementation. It is worth mentioning that our proposed method is a unified framework with two different basis functions. Both basis-function-based methods are applied to 2D image deformation, 3D surface deformation, and medical image registration. Extensive experiments on the data and the resulting mean opinion score (MOS) on the 2D deformation demonstrate that our methods are superior to state-of-the-art ones.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001765",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Basis function",
      "Computer science",
      "Computer vision",
      "Dimensionality reduction",
      "Image (mathematics)",
      "Interpolation (computer graphics)",
      "Manifold alignment",
      "Mathematical analysis",
      "Mathematics",
      "Moving least squares",
      "Nonlinear dimensionality reduction",
      "Pattern recognition (psychology)",
      "Pixel",
      "Regularization (linguistics)",
      "Voxel"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Huabing"
      },
      {
        "surname": "Xu",
        "given_name": "Zhichao"
      },
      {
        "surname": "Tian",
        "given_name": "Yulu"
      },
      {
        "surname": "Yu",
        "given_name": "Zhenghong"
      },
      {
        "surname": "Zhang",
        "given_name": "Yanduo"
      },
      {
        "surname": "Ma",
        "given_name": "Jiayi"
      }
    ]
  },
  {
    "title": "Learning multi-level weight-centric features for few-shot learning",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108662",
    "abstract": "Few-shot learning is currently enjoying a considerable resurgence of interest, aided by the recent advance of deep learning. Contemporary approaches based on weight-generation scheme delivers a straightforward and flexible solution to the problem. However, they did not fully consider both the representation power for unseen categories and weight generation capacity in feature learning, making it a significant performance bottleneck. This paper proposes a multi-level weight-centric feature learning to give full play to feature extractor’s dual roles in few-shot learning. Our proposed method consists of two essential techniques: a weight-centric training strategy to improve the features’ prototype-ability and a multi-level feature incorporating a mid- and relation-level information. The former increases the feasibility of constructing a discriminative decision boundary based on a few samples. Simultaneously, the latter helps improve the transferability for characterizing novel classes and preserve classification capability for base classes. We extensively evaluate our approach to low-shot classification benchmarks. Experiments demonstrate our proposed method significantly outperforms its counterparts in both standard and generalized settings and using different network backbones.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001431",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Bottleneck",
      "Computer science",
      "Data mining",
      "Decision boundary",
      "Discriminative model",
      "Embedded system",
      "Feature (linguistics)",
      "Feature learning",
      "Geodesy",
      "Geography",
      "Law",
      "Linguistics",
      "Machine learning",
      "Margin (machine learning)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Relation (database)",
      "Representation (politics)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Liang",
        "given_name": "Mingjiang"
      },
      {
        "surname": "Huang",
        "given_name": "Shaoli"
      },
      {
        "surname": "Pan",
        "given_name": "Shirui"
      },
      {
        "surname": "Gong",
        "given_name": "Mingming"
      },
      {
        "surname": "Liu",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "A categorical data clustering framework on graph representation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108694",
    "abstract": "Clustering categorical data is an important task of machine learning, since the type of data widely exists in real world. However, the lack of an inherent order on the domains of categorical features prevents most of classical clustering algorithms from being directly applied for the type of data. Therefore, it is very key issue to learn an appropriate representation of categorical data for the clustering task. In order to address this issue, we develop a categorical data clustering framework based on graph representation. In this framework, a graph-based representation method for categorical data is proposed, which learns the representation of categorical values from their similar graph to provide similar representations for similar categorical values. We compared the proposed framework with other representation methods for categorical data clustering on benchmark data sets. The experiment results illustrate the proposed framework is very effective, compared to other methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001753",
    "keywords": [
      "Artificial intelligence",
      "Categorical variable",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "External Data Representation",
      "Graph",
      "Law",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Bai",
        "given_name": "Liang"
      },
      {
        "surname": "Liang",
        "given_name": "Jiye"
      }
    ]
  },
  {
    "title": "Cross-project defect prediction based on G-LSTM model",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.04.039",
    "abstract": "Cross-project defect prediction (CPDP) is currently a hot research direction in the field of software reliability. Traditional CPDP methods cannot capture the semantic and contextual information of programs by handcrafted features, which affects the prediction performance. In this paper, we apply technology in the NLP domain to solve it. We first extract token vectors from the abstract syntax tree (AST) of source and target code files, and then convert them into numerical vectors by the word embedding algorithm of continuous bag-of-word model (CBOW) as the input of the proposed deep learning model named Generative Adversarial Long-Short Term Memory Neural Networks (G-LSTM). The model integrates generative adversarial network (GAN) and bidirectional long-short term memory networks (BiLSTM) with attention mechanism to automatically learn semantic and contextual features of programs. Specifically, GAN is used to eliminate the differences in data distribution between source and target projects, and BiLSTM is the feature extraction encoder. We compose five projects of the PROMISE dataset into 20 source-target project pairs and conduct comparison experiments on them. The experimental results demonstrate that our method outperforms some traditional and state-of-the-art CPDP methods in terms of the evaluation metrics of AUC and Acc.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001519",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Computer security",
      "Deep learning",
      "Embedding",
      "Encoder",
      "Field (mathematics)",
      "Generative grammar",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Natural language processing",
      "Operating system",
      "Philosophy",
      "Pure mathematics",
      "Rank (graph theory)",
      "Security token",
      "Source code",
      "Word (group theory)",
      "Word embedding"
    ],
    "authors": [
      {
        "surname": "Xing",
        "given_name": "Ying"
      },
      {
        "surname": "Qian",
        "given_name": "Xiaomeng"
      },
      {
        "surname": "Guan",
        "given_name": "Yu"
      },
      {
        "surname": "Yang",
        "given_name": "Bin"
      },
      {
        "surname": "Zhang",
        "given_name": "Yuwei"
      }
    ]
  },
  {
    "title": "Molecular substructure graph attention network for molecular property identification in drug discovery",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108659",
    "abstract": "Molecular machine learning based on graph neural network has a broad prospect in molecular property identification in drug discovery. Molecules contain many types of substructures that may affect their properties. However, conventional methods based on graph neural networks only consider the interaction information between nodes, which may lead to the oversmoothing problem in the multi-hop operations. These methods may not efficiently express the interacting information between molecular substructures. Hence, We develop a Molecular SubStructure Graph ATtention (MSSGAT) network to capture the interacting substructural information, which constructs a composite molecular representation with multi-substructural feature extraction and processes such features effectively with a nested convolution plus readout scheme. We evaluate the performance of our model on 13 benchmark data sets, in which 9 data sets are from the ChEMBL data base and 4 are the SIDER, BBBP, BACE, and HIV data sets. Extensive experimental results show that MSSGAT achieves the best results on most of the data sets compared with other state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001406",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Drug discovery",
      "Engineering",
      "Graph",
      "Molecular graph",
      "Structural engineering",
      "Substructure",
      "Theoretical computer science",
      "chEMBL"
    ],
    "authors": [
      {
        "surname": "Ye",
        "given_name": "Xian-bin"
      },
      {
        "surname": "Guan",
        "given_name": "Quanlong"
      },
      {
        "surname": "Luo",
        "given_name": "Weiqi"
      },
      {
        "surname": "Fang",
        "given_name": "Liangda"
      },
      {
        "surname": "Lai",
        "given_name": "Zhao-Rong"
      },
      {
        "surname": "Wang",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Non-volume preserving-based fusion to group-level emotion recognition on crowd videos",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108646",
    "abstract": "Group-level emotion recognition (ER) is a growing research area as the demands for assessing crowds of all sizes are becoming an interest in both the security arena as well as social media. This work extends the earlier ER investigations, which focused on either group-level ER on single images or within a video, by fully investigating group-level expression recognition on crowd videos. In this paper, we propose an effective deep feature level fusion mechanism to model the spatial-temporal information in the crowd videos. In our approach, the fusing process is performed on the deep feature domain by a generative probabilistic model, Non-Volume Preserving Fusion (NVPF), that models spatial information relationships. Furthermore, we extend our proposed spatial NVPF approach to the spatial-temporal NVPF approach to learn the temporal information between frames. To demonstrate the robustness and effectiveness of each component in the proposed approach, three experiments were conducted: (i) evaluation on AffectNet database to benchmark the proposed EmoNet for recognizing facial expression; (ii) evaluation on EmotiW2018 to benchmark the proposed deep feature level fusion mechanism NVPF; and, (iii) examine the proposed TNVPF on an innovative Group-level Emotion on Crowd Videos (GECV) dataset composed of 627 videos collected from publicly available sources. GECV dataset is a collection of videos containing crowds of people. Each video is labeled with emotion categories at three levels: individual faces, group of people, and the entire video frame.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001273",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Crowd psychology",
      "Crowds",
      "Feature extraction",
      "Gene",
      "Generative grammar",
      "Generative model",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Probabilistic logic",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Quach",
        "given_name": "Kha Gia"
      },
      {
        "surname": "Le",
        "given_name": "Ngan"
      },
      {
        "surname": "Duong",
        "given_name": "Chi Nhan"
      },
      {
        "surname": "Jalata",
        "given_name": "Ibsa"
      },
      {
        "surname": "Roy",
        "given_name": "Kaushik"
      },
      {
        "surname": "Luu",
        "given_name": "Khoa"
      }
    ]
  },
  {
    "title": "Collision avoidance approaches for autonomous mobile robots to tackle the problem of pedestrians roaming on campus road",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.06.005",
    "abstract": "A novel algorithm for collision-free navigation of a mobile robot on the campus road with pedestrian roaming is described in this paper. Proposed method uses the RGB-D depth sensor, utilizing optical flow estimation and object detection, to predict pedestrian locations. Given these environmental uncertainties, we present a Velocity Obstacle (VO) algorithm based on mobility rules to calculate the velocity has been presented. It is proposed to use the Markov Decision Process (MDP) for decision-making (to maneuver the robot whenever it approaches the target). The proposed algorithm is a hybrid combination of deep learning and model-based techniques and provides better results in terms of navigation time and collision avoidance success rate than conventional algorithms. The real-time performance of the proposed algorithm is highlighted using a real-world dynamic scenario for an up-to-date kid ride car that has been redesigned to work as an autonomous mobile robot.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001994",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Collision",
      "Collision avoidance",
      "Collision avoidance system",
      "Computer network",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Engineering",
      "Geography",
      "Mobile robot",
      "Mobile robot navigation",
      "Obstacle",
      "Obstacle avoidance",
      "Operating system",
      "Pedestrian",
      "Process (computing)",
      "Real-time computing",
      "Roaming",
      "Robot",
      "Robot control",
      "Simulation",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Manikandan",
        "given_name": "N.S."
      },
      {
        "surname": "Kaliyaperumal",
        "given_name": "Ganesan"
      }
    ]
  },
  {
    "title": "Node-Feature Convolution for Graph Convolutional Networks",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108661",
    "abstract": "Graph convolutional network (GCN) is an effective neural network model for graph representation learning. However, standard GCN suffers from three main limitations: (1) most real-world graphs have no regular connectivity and node degrees can range from one to hundreds or thousands, (2) neighboring nodes are aggregated with fixed weights, and (3) node features within a node feature vector are considered equally important. Several extensions have been proposed to tackle the limitations respectively. This paper focuses on tackling all the proposed limitations. Specifically, we propose a new node-feature convolutional (NFC) layer for GCN. The NFC layer first constructs a feature map using features selected and ordered from a fixed number of neighbors. It then performs a convolution operation on this feature map to learn the node representation. In this way, we can learn the usefulness of both individual nodes and individual features from a fixed-size neighborhood. Experiments on three benchmark datasets show that NFC-GCN consistently outperforms state-of-the-art methods in node classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200142X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Engineering",
      "Feature (linguistics)",
      "Feature learning",
      "Geodesy",
      "Geography",
      "Graph",
      "Law",
      "Linguistics",
      "Node (physics)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Structural engineering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Li"
      },
      {
        "surname": "Song",
        "given_name": "Heda"
      },
      {
        "surname": "Aletras",
        "given_name": "Nikolaos"
      },
      {
        "surname": "Lu",
        "given_name": "Haiping"
      }
    ]
  },
  {
    "title": "Super U-Net: A modularized generalizable architecture",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108669",
    "abstract": "Objective To develop and validate a novel convolutional neural network (CNN) termed “Super U-Net” for medical image segmentation. Methods Super U-Net integrates a dynamic receptive field module and a fusion upsampling module into the classical U-Net architecture. The model was developed and tested to segment retinal vessels, gastrointestinal (GI) polyps, skin lesions on several image types (i.e., fundus images, endoscopic images, dermoscopic images). We also trained and tested the traditional U-Net architecture, seven U-Net variants, and two non-U-Net segmentation architectures. K-fold cross-validation was used to evaluate performance. The performance metrics included Dice similarity coefficient (DSC), accuracy, positive predictive value (PPV), and sensitivity. Results Super U-Net achieved average DSCs of 0.808±0.0210, 0.752±0.019, 0.804±0.239, and 0.877±0.135 for segmenting retinal vessels, pediatric retinal vessels, GI polyps, and skin lesions, respectively. The Super U-net consistently outperformed U-Net, seven U-Net variants, and two non-U-Net segmentation architectures (p < 0.05). Conclusion Dynamic receptive fields and fusion upsampling can significantly improve image segmentation performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001509",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Net (polyhedron)",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Upsampling"
    ],
    "authors": [
      {
        "surname": "Beeche",
        "given_name": "Cameron"
      },
      {
        "surname": "Singh",
        "given_name": "Jatin P"
      },
      {
        "surname": "Leader",
        "given_name": "Joseph K"
      },
      {
        "surname": "Gezer",
        "given_name": "Naciye S"
      },
      {
        "surname": "Oruwari",
        "given_name": "Amechi P"
      },
      {
        "surname": "Dansingani",
        "given_name": "Kunal K"
      },
      {
        "surname": "Chhablani",
        "given_name": "Jay"
      },
      {
        "surname": "Pu",
        "given_name": "Jiantao"
      }
    ]
  },
  {
    "title": "Compositional coding capsule network with k-means routing for text classification",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.05.028",
    "abstract": "Text classification is a challenging problem which aims to identify the category of texts. In the process of training, word embeddings occupy a large part of parameters. Under the constraint of limited computing resources, it indirectly restricts the ability of subsequent network design. In order to reduce the number of parameters for constructing word embeddings, this paper explores compositional coding mechanism and proposes a compositional weighted coding method to replace the conventional embedding layer. Furthermore, inspired by the excellent performance of capsule network in image classification, we design a capsule network combined with our compositional weighted coding method for text classification. We also offer a new routing algorithm based on k-means clustering theory to thoroughly mine the relationship between capsules. Experiments conducted on eight challenging text classification datasets show that the proposed method achieves competitive accuracy compared to the state-of-the-art approach with significantly fewer parameters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552200188X",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Coding (social sciences)",
      "Computer science",
      "Data mining",
      "Embedding",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Ren",
        "given_name": "Hao"
      },
      {
        "surname": "Lu",
        "given_name": "Hong"
      }
    ]
  },
  {
    "title": "Time series classifier recommendation by a meta-learning approach",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108671",
    "abstract": "This work addresses time series classifier recommendation for the first time in the literature by considering several recommendation forms or meta-targets: classifier accuracies, complete ranking, top-M ranking, best set and best classifier. For this, an ad-hoc set of quick estimators of the accuracies of the candidate classifiers (landmarkers) are designed, which are used as predictors for the recommendation system. The performance of our recommender is compared with the performance of a standard method for non-sequential data and a set of baseline methods, which our method outperforms in 7 of the 9 considered scenarios. Since some meta-targets can be inferred from the predictions of other more fine-grained meta-targets, the last part of the work addresses the hierarchical inference of meta-targets. The experimentation suggests that, in many cases, a single model is sufficient to output many types of meta-targets with competitive results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001522",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Classifier (UML)",
      "Computer science",
      "Machine learning",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Series (stratigraphy)"
    ],
    "authors": [
      {
        "surname": "Abanda",
        "given_name": "A."
      },
      {
        "surname": "Mori",
        "given_name": "U."
      },
      {
        "surname": "Lozano",
        "given_name": "Jose A."
      }
    ]
  },
  {
    "title": "HandyPose: Multi-level framework for hand pose estimation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108674",
    "abstract": "Hand pose estimation is a challenging task due to the large number of degrees of freedom and the frequent occlusions of joints. To address these challenges, we propose HandyPose, a single-pass, end-to-end trainable architecture for 2D hand pose estimation using a single RGB image as input. Adopting an encoder-decoder framework with multi-level features, along with a novel multi-level waterfall atrous spatial pooling module for multi-scale representations, our method achieves high accuracy in hand pose while maintaining manageable size complexity and modularity of the network. HandyPose takes a multi-scale approach to representing context by incorporating spatial information at various levels of the network to mitigate the loss of resolution due to pooling. Our advanced multi-level waterfall module leverages the efficiency of progressive cascade filtering while maintaining larger fields-of-view through the concatenation of multi-level features from different levels of the network in the waterfall module. The decoder incorporates both the waterfall and multi-scale features for the generation of accurate joint heatmaps in a single stage. Our results demonstrate state-of-the-art performance on popular datasets and show that HandyPose is a robust and efficient architecture for 2D hand pose estimation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001558",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Biology",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Concatenation (mathematics)",
      "Context (archaeology)",
      "Encoder",
      "Geometry",
      "History",
      "Mathematics",
      "Operating system",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Pooling",
      "Pose",
      "Pyramid (geometry)",
      "Waterfall"
    ],
    "authors": [
      {
        "surname": "Gupta",
        "given_name": "Divyansh"
      },
      {
        "surname": "Artacho",
        "given_name": "Bruno"
      },
      {
        "surname": "Savakis",
        "given_name": "Andreas"
      }
    ]
  },
  {
    "title": "Spatial information enhancement network for 3D object detection from point cloud",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108684",
    "abstract": "LiDAR-based 3D object detection pushes forward an immense influence on autonomous vehicles. Due to the limitation of the intrinsic properties of LiDAR, fewer points are collected at the objects farther away from the sensor. This imbalanced density of point clouds degrades the detection accuracy but is generally neglected by previous works. To address the challenge, we propose a novel two-stage 3D object detection framework, named SIENet. Specifically, we design the Spatial Information Enhancement (SIE) module to predict the spatial shapes of the foreground points within proposals, and extract the structure information to learn the representative features for further box refinement. The predicted spatial shapes are complete and dense point sets, thus the extracted structure information contains more semantic representation. Besides, we design the Hybrid-Paradigm Region Proposal Network (HP-RPN) which includes multiple branches to learn discriminate features and generate accurate proposals for the SIE module. Extensive experiments on the KITTI 3D object detection benchmark show that our elaborately designed SIENet outperforms the state-of-the-art methods by a large margin. Codes will be publicly available at https://github.com/Liz66666/SIENet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001650",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Geodesy",
      "Geography",
      "Geometry",
      "Law",
      "Lidar",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Point (geometry)",
      "Point cloud",
      "Political science",
      "Politics",
      "Remote sensing",
      "Representation (politics)",
      "Spatial analysis"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Ziyu"
      },
      {
        "surname": "Yao",
        "given_name": "Yuncong"
      },
      {
        "surname": "Quan",
        "given_name": "Zhibin"
      },
      {
        "surname": "Xie",
        "given_name": "Jin"
      },
      {
        "surname": "Yang",
        "given_name": "Wankou"
      }
    ]
  },
  {
    "title": "High quality proposal feature generation for crowded pedestrian detection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108605",
    "abstract": "Occlusion is a severe problem for pedestrian detection in crowded scenes. Due to the diversity of pedestrian postures and occlusion forms, leading to false detection and missed detection. In this paper, we propose a high quality proposal feature generation pedestrian detection algorithm to improve detection performance. Firstly, Dual-Region Feature Generation (DRFG) is proposed to generate high quality proposal features. Specifically, visible regions with less occlusion are introduced and low-precision proposals are generated for both the full-body and visible regions respectively. Then, proposals are respectively selected from the two kinds of proposals mentioned above to match in pairs, so as to guarantee a strong correspondence in information between the two proposals. Afterwards, the successfully matched proposal features are fused by Selective Kernel Feature Fusion (SKFF) to generate high quality proposal features. Secondly, Paired Multiple Instance Prediction(PMIP) is performed on the fused features to generate multiple prediction branches, and each prediction branch generates full-body and visible prediction box. Finally, Paired Non-Maximum Suppression(PNMS) is applied to the prediction boxes to reduce the false positives. Experiments have been conducted on CrowdHuman [1] and CityPersons [2] datasets. Comparing with baseline, our methods have achieved 5.9% A P and 1.5% M R − 2 improvement on the above two datasets, sufficiently verifying the effectiveness of our methods in crowded pedestrian detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000863",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Engineering",
      "False positive paradox",
      "Feature (linguistics)",
      "Kernel (algebra)",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pedestrian",
      "Pedestrian detection",
      "Philosophy",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jing"
      },
      {
        "surname": "Zhao",
        "given_name": "Cailing"
      },
      {
        "surname": "Huo",
        "given_name": "Zhanqiang"
      },
      {
        "surname": "Qiao",
        "given_name": "Yingxu"
      },
      {
        "surname": "Sima",
        "given_name": "Haifeng"
      }
    ]
  },
  {
    "title": "Transformer-based Cross Reference Network for video salient object detection",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.06.006",
    "abstract": "Video salient object detection is a fundamental computer vision task aimed at highlighting the most conspicuous objects in a video sequence. There are two key challenges presented in video salient object detection: (1) how to extract effective feature representations from appearance and motion cues, and (2) how to combine both of them into robust saliency representation. To handle these challenges, in this paper, we propose a novel Transformer-based Cross Reference Network (TCRN), which fully exploits long-range context dependencies in both feature representation extraction and cross-modal (i.e., appearance and motion) integration. In contrast to existing CNN-based methods, our approach formulates video salient object detection as a sequence-to-sequence prediction task. In the proposed approach, the deep feature extraction is achieved by a pure vision transformer with multi-resolution token representations. Specifically, we design a Gated Cross Reference (GCR) module to effectively integrate appearance and motion into saliency representation. The GCR first propagates global context information between different modalities, and then perform cross-modal fusion by a gate mechanism. Extensive evaluations on five widely-used benchmarks show that the proposed Transformer-based method performs favorably against the existing state-of-the-art methods",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002008",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Feature extraction",
      "Object detection",
      "Pattern recognition (psychology)",
      "Salient",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Kan"
      },
      {
        "surname": "Tian",
        "given_name": "Chunwei"
      },
      {
        "surname": "Su",
        "given_name": "Jingyong"
      },
      {
        "surname": "Lin",
        "given_name": "Jerry Chun-Wei"
      }
    ]
  },
  {
    "title": "Searching part-specific neural fabrics for human pose estimation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108652",
    "abstract": "Neural architecture search (NAS) has emerged in many domains to jointly learn the architectures and weights of neural networks. The core spirit behind NAS is to automatically search neural architectures for target tasks with better performance-efficiency trade-offs. However, existing approaches emphasize on only searching a single architecture with less human intervention to replace a human-designed neural network, yet making the search process almost independent of the domain knowledge. In this paper, we aim to apply NAS for human pose estimation and we ask: when NAS meets this localization task, can the articulated human body structure help to search better task-specific architectures? To this end, we first design a new neural architecture search space, Cell-based Neural Fabric (CNF), to learn micro as well as macro neural architecture using a differentiable search strategy. Then, by viewing locating human parts as multiple disentangled prediction sub-tasks, we exploit the compositionality of human body structure as guidance to search multiple part-specific CNFs specialized for different human parts. After the search, all these part-specific neural fabrics have been tailored with distinct micro and macro architecture parameters. The results show that such knowledge-guided NAS-based model outperforms a hand-crafted part-based baseline model, and the resulting multiple part-specific architectures gain significant performance improvement against a single NAS-based architecture for the whole body. The experiments on MPII and COCO datasets show that our models 1 1 Code is available at https://github.com/yangsenius/PoseNFS. achieve comparable performance against the state-of-the-art methods while being relatively lightweight.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001339",
    "keywords": [
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer security",
      "Domain (mathematical analysis)",
      "Domain knowledge",
      "Engineering",
      "Exploit",
      "Machine learning",
      "Macro",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Process (computing)",
      "Programming language",
      "Systems engineering",
      "Task (project management)",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Sen"
      },
      {
        "surname": "Yang",
        "given_name": "Wankou"
      },
      {
        "surname": "Cui",
        "given_name": "Zhen"
      }
    ]
  },
  {
    "title": "Adversarial momentum-contrastive pre-training",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.07.005",
    "abstract": "Recently proposed adversarial self-supervised learning methods usually require big batches and long training epochs to extract robust features, which will bring heavy computational overhead on platforms with limited resources. In order to help the network learn more powerful feature representations in smaller batches and fewer epochs, this paper proposes a novel adversarial momentum contrastive learning method, which introduces two memory banks corresponding to clean samples and adversarial samples, respectively. These memory banks can be dynamically incorporated into the training process to track invariant features among historical mini-batches. Compared with the previous adversarial pre-training model, our method achieves superior performance with smaller batch size and less training epochs. In addition, the model outperforms some state-of-the-art supervised defensive methods on multiple benchmark datasets after being fine-tuned on downstream classification tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002161",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Meteorology",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Process (computing)",
      "Training (meteorology)"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Cong"
      },
      {
        "surname": "Li",
        "given_name": "Dan"
      },
      {
        "surname": "Yang",
        "given_name": "Min"
      }
    ]
  },
  {
    "title": "Personalized knowledge-aware recommendation with collaborative and attentive graph convolutional networks",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108628",
    "abstract": "Knowledge graphs (KGs) are increasingly used to solve the data sparsity and cold start problems of collaborative filtering. Recently, graph neural networks (GNNs) have been applied to build KG-based recommender systems and achieved competitive performance. However, existing GNN-based methods are either limited in their ability to capture fine-grained semantics in a KG, or insufficient in effectively modeling user-item interactions. To address these issues, we propose a novel framework with collaborative and attentive graph convolutional networks for personalized knowledge-aware recommendation. Particularly, we model the user-item graph and the KG separately and simultaneously with an efficient graph convolutional network and a personalized knowledge graph attention network, where the former aims to extract informative collaborative signals, while the latter is designed to capture fine-grained semantics. Collectively, they are able to learn meaningful node representations for predicting user-item interactions. Extensive experiments on benchmark datasets demonstrate the effectiveness of the proposed method compared with state-of-the-arts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001091",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Collaborative filtering",
      "Computer science",
      "Convolutional neural network",
      "Geodesy",
      "Geography",
      "Graph",
      "Information retrieval",
      "Knowledge graph",
      "Machine learning",
      "Programming language",
      "Recommender system",
      "Semantics (computer science)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Dai",
        "given_name": "Quanyu"
      },
      {
        "surname": "Wu",
        "given_name": "Xiao-Ming"
      },
      {
        "surname": "Fan",
        "given_name": "Lu"
      },
      {
        "surname": "Li",
        "given_name": "Qimai"
      },
      {
        "surname": "Liu",
        "given_name": "Han"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaotong"
      },
      {
        "surname": "Wang",
        "given_name": "Dan"
      },
      {
        "surname": "Lin",
        "given_name": "Guli"
      },
      {
        "surname": "Yang",
        "given_name": "Keping"
      }
    ]
  },
  {
    "title": "Adaptive open domain recognition by coarse-to-fine prototype-based network",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108657",
    "abstract": "Open domain recognition has attracted great attention in recent two years, which aims to assign a specific identification for each target sample in the presence of large domain discrepancy both in label space and data distributions. Most existing approaches rely on abundant prior information about the relationship of the label sets between the source and the target domain, which is a great limitation for their applications in practical wild. In this paper, a new Adaptive Open Domain Recognition (AODR) task is introduced, which can generalize to various openness and requires no prior information on the label set. To achieve this adaptive transfer task, a two-stage Progressive Adaptation Network is designed, whose learning process consists of multiple episodes. Each episode is performed to simulate an AODR task. Through training and refining multiple episodes, the basic model has progressively accumulated wealthy experience on predicting unseen categories in the presence of large domain discrepancy, which will well generalize to various openness. More specifically, Fusion Information Guided Feature Prototype Generation module is proposed to synthesize visual feature prototype conditioned on category semantic prototype in training stage. Further, Class-Aware Feature Prototype Alignment module is designed in refining stage to align the global feature prototype for each class between two domains. Experimental results verify that the proposed model not only has superiority on classifying the image instances of known and unknown classes, but also well adapts to various openness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001388",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Domain (mathematical analysis)",
      "Engineering",
      "Feature (linguistics)",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Programming language",
      "Set (abstract data type)",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Yuan",
        "given_name": "Yuan"
      },
      {
        "surname": "He",
        "given_name": "Xinxing"
      },
      {
        "surname": "Jiang",
        "given_name": "Zhiyu"
      }
    ]
  },
  {
    "title": "A hybrid method based on estimation of distribution algorithms to train convolutional neural networks for text categorization",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.06.008",
    "abstract": "Convolutional Neural Networks for text categorization allows the extraction of features from the text represented through word embedding. The high dimensionality of the texts themselves implies a larger number of network parameters and a more complex optimization surface. Artificial neural network training is an NP-Hard optimization problem, which has been addressed by methods based on partial derivatives of the objective function and presents several theoretical and practical limitations, such as the probability of convergence to local minimums. In this work, we propose a hybrid method based on the Estimation of Distribution Algorithms for training a Convolutional Neural Network. For this, we train together gradient-based methods with the Estimation of Multivariate Normal Algorithm and Univariate Marginal Distribution Algorithm by dividing the training process into two stages. The different variants obtained with the proposed method are compared with gradient-based methods on public benchmark datasets and statistical differences are analyzed by nonparametric tests. The proposed method increases the accuracy of the convolutional network applied to the text categorization task and overcome in about 0.22%–24% the state-of-the-art algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002021",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Categorization",
      "Computer science",
      "Convolutional neural network",
      "Curse of dimensionality",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Multivariate statistics",
      "Pattern recognition (psychology)",
      "Univariate"
    ],
    "authors": [
      {
        "surname": "Toledano-López",
        "given_name": "Orlando Grabiel"
      },
      {
        "surname": "Madera",
        "given_name": "Julio"
      },
      {
        "surname": "González",
        "given_name": "Hector"
      },
      {
        "surname": "Simón-Cuevas",
        "given_name": "Alfredo"
      }
    ]
  },
  {
    "title": "Few shots are all you need: A progressive learning approach for low resource handwritten text recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.06.003",
    "abstract": "Handwritten text recognition in low resource scenarios, such as manuscripts with rare alphabets, is a challenging problem. In this paper, we propose a few-shot learning-based handwriting recognition approach that significantly reduces the human annotation process, by requiring only a few images of each alphabet symbols. The method consists of detecting all the symbols of a given alphabet in a textline image and decoding the obtained similarity scores to the final sequence of transcribed symbols. Our model is first pretrained on synthetic line images generated from an alphabet, which could differ from the alphabet of the target domain. A second training step is then applied to reduce the gap between the source and the target data. Since this retraining would require annotation of thousands of handwritten symbols together with their bounding boxes, we propose to avoid such human effort through an unsupervised progressive learning approach that automatically assigns pseudo-labels to the unlabeled data. The evaluation on different datasets shows that our model can lead to competitive results with a significant reduction in human effort. The code will be publicly available in the following repository: https://github.com/dali92002/HTRbyMatching",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552200191X",
    "keywords": [
      "Algorithm",
      "Alphabet",
      "Annotation",
      "Artificial intelligence",
      "Biology",
      "Bounding overwatch",
      "Code (set theory)",
      "Computer science",
      "Decoding methods",
      "Domain (mathematical analysis)",
      "Feature extraction",
      "Genetics",
      "Handwriting",
      "Handwriting recognition",
      "Image (mathematics)",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Programming language",
      "Sequence (biology)",
      "Set (abstract data type)",
      "Similarity (geometry)",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Souibgui",
        "given_name": "Mohamed Ali"
      },
      {
        "surname": "Fornés",
        "given_name": "Alicia"
      },
      {
        "surname": "Kessentini",
        "given_name": "Yousri"
      },
      {
        "surname": "Megyesi",
        "given_name": "Beáta"
      }
    ]
  },
  {
    "title": "Expecting individuals’ body reaction to Covid-19 based on statistical Naïve Bayes technique",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108693",
    "abstract": "Covid-19, what a strange, unpredictable mutated virus. It has baffled many scientists, as no firm rule has yet been reached to predict the effect that the virus can inflict on people if they are infected with it. Recently, many researches have been introduced for diagnosing Covid-19; however, none of them pay attention to predict the effect of the virus on the person's body if the infection occurs but before the infection really takes place. Predicting the extent to which people will be affected if they are infected with the virus allows for some drastic precautions to be taken for those who will suffer from serious complications, while allowing some freedom for those who expect not to be affected badly. This paper introduces Covid-19 Prudential Expectation Strategy (CPES) as a new strategy for predicting the behavior of the person's body if he has been infected with Covid-19. The CPES composes of three phases called Outlier Rejection Phase (ORP), Feature Selection Phase (FSP), and Classification Phase (CP). For enhancing the classification accuracy in CP, CPES employs two proposed techniques for outlier rejection in ORP and feature selection in FSP, which are called Hybrid Outlier Rejection (HOR) method and Improved Binary Genetic Algorithm (IBGA) method respectively. In ORP, HOR rejects outliers in the training data using a hybrid method that combines standard division and Binary Gray Wolf Optimization (BGWO) method. On the other hand, in FSP, IBGA as a hybrid method selects the most useful features for the prediction process. IBGA includes Fisher Score (FScore) as a filter method to quickly select the features and BGA as a wrapper method to accurately select the features based on the average accuracy value from several classification models as a fitness function to guarantee the efficiency of the selected subset of features with any classifier. In CP, CPES has the ability to classify people based on their bodies’ reaction to Covid-19 infection, which is built upon a proposed Statistical Naïve Bayes (SNB) classifier after performing the previous two phases. CPES has been compared against recent related strategies in terms of accuracy, error, recall, precision, and run-time using Covid-19 dataset [1]. This dataset contains routine blood tests collected from people before and after their infection with covid-19 through a Web-based form created by us. CPES outperforms the competing methods in experimental results because it provides the best results with values of 0.87, 0.13, 0.84, and 0.79 for accuracy, error, precision, and recall.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001741",
    "keywords": [
      "Algorithm",
      "Anomaly detection",
      "Arithmetic",
      "Artificial intelligence",
      "Bayes' theorem",
      "Bayesian probability",
      "Binary classification",
      "Binary number",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Data mining",
      "Disease",
      "Feature selection",
      "Infectious disease (medical specialty)",
      "Machine learning",
      "Mathematics",
      "Medicine",
      "Outlier",
      "Pathology",
      "Pattern recognition (psychology)",
      "Selection (genetic algorithm)",
      "Statistics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Rabie",
        "given_name": "Asmaa H."
      },
      {
        "surname": "Mansour",
        "given_name": "Nehal A."
      },
      {
        "surname": "Saleh",
        "given_name": "Ahmed I."
      },
      {
        "surname": "Takieldeen",
        "given_name": "Ali E."
      }
    ]
  },
  {
    "title": "A spatially constrained skew Student’s-t mixture model for brain MR image segmentation and bias field correction",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108658",
    "abstract": "Accurate segmentation of brain magnetic resonance images is a key step in quantitative analysis of brain images. Finite mixture model is one of the most widely used methods in brain magnetic resonance image segmentation. However, due to the presence of intensity inhomogeneity artifact and noise, the image histogram distribution of brain MR images may follow a heavy tailed distribution or asymmetric distribution, which makes traditional finite mixture model, such as Gaussian mixture model, hard to achieve accurate segmentation results. To alleviate these problems, a novel spatially constrained finite skew student’s-t mixture model is proposed in this paper. Firstly, we propose anisotropic two-level spatial information, which combines the prior and posterior probabilities, to reduce the impact of noise. The proposed spatial information can preserve rich details, such as edges and corners. Secondly, we couple the anisotropic spatial information into the skew student’s-t distribution to fit the intensity distribution of observation data with heavy tail distribution or asymmetric distribution. Thirdly, we use a linear combination of a set of orthogonal basis functions to model the intensity inhomogeneities. Finally, the objective function integrates both tissue segmentation and the bias field estimation. In the implementation, we used an improved expectation maximization (EM) algorithm to estimate the model parameters. The experimental results of our model on synthetic data and brain magnetic resonance images are better than other state-of-the-art segmentation methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200139X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Expectation–maximization algorithm",
      "Histogram",
      "Image (mathematics)",
      "Image segmentation",
      "Mathematics",
      "Maximum likelihood",
      "Mixture model",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Scale-space segmentation",
      "Segmentation",
      "Skew",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Cheng",
        "given_name": "Ning"
      },
      {
        "surname": "Cao",
        "given_name": "Chunzheng"
      },
      {
        "surname": "Yang",
        "given_name": "Jianwei"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhichao"
      },
      {
        "surname": "Chen",
        "given_name": "Yunjie"
      }
    ]
  },
  {
    "title": "MFSNet: A multi focus segmentation network for skin lesion segmentation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108673",
    "abstract": "Segmentation is essential for medical image analysis to identify and localize diseases, monitor morphological changes, and extract discriminative features for further diagnosis. Skin cancer is one of the most common types of cancer globally, and its early diagnosis is pivotal for the complete elimination of malignant tumors from the body. This research develops an Artificial Intelligence (AI) framework for supervised skin lesion segmentation employing the deep learning approach. The proposed framework, called MFSNet (Multi-Focus Segmentation Network), uses differently scaled feature maps for computing the final segmentation mask using raw input RGB images of skin lesions. In doing so, initially, the images are preprocessed to remove unwanted artifacts and noises. The MFSNet employs the Res2Net backbone, a recently proposed convolutional neural network (CNN), for obtaining deep features used in a Parallel Partial Decoder (PPD) module to get a global map of the segmentation mask. In different stages of the network, convolution features and multi-scale maps are used in two boundary attention (BA) modules and two reverse attention (RA) modules to generate the final segmentation output. MFSNet, when evaluated on three publicly available datasets: P H 2 , ISIC 2017, and HAM10000, outperforms state-of-the-art methods, justifying the reliability of the framework. The relevant codes for the proposed approach are accessible at https://github.com/Rohit-Kundu/MFSNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001546",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Deep learning",
      "Discriminative model",
      "Feature (linguistics)",
      "Focus (optics)",
      "Image segmentation",
      "Linguistics",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Basak",
        "given_name": "Hritam"
      },
      {
        "surname": "Kundu",
        "given_name": "Rohit"
      },
      {
        "surname": "Sarkar",
        "given_name": "Ram"
      }
    ]
  },
  {
    "title": "The Cobb-Douglas Learning Machine",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108701",
    "abstract": "In this paper, we propose a novel machine learning approach based on robust optimization. Our proposal defines the task of maximizing the two class accuracies of a binary classification problem as a Cobb-Douglas function. This function is well known in production economics and is used to model the relationship between two or more inputs as well as the quantity produced by those inputs. A robust optimization problem is defined to construct the decision function. The goal of the model is to classify each training pattern correctly, up to a given class accuracy, even for the worst possible data distribution. We demonstrate the theoretical advantages of the Cobb-Douglas function in terms of the properties of the resulting second-order cone programming problem. Important extensions are proposed and discussed, including the use of kernel functions and regularization. Experiments performed on several classification datasets confirm these advantages, leading to the best average performance in comparison to various alternative classifiers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001820",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "CobB",
      "Computer science",
      "Genetics",
      "Machine learning"
    ],
    "authors": [
      {
        "surname": "Maldonado",
        "given_name": "Sebastián"
      },
      {
        "surname": "López",
        "given_name": "Julio"
      },
      {
        "surname": "Carrasco",
        "given_name": "Miguel"
      }
    ]
  },
  {
    "title": "Learning invariant representation for unsupervised domain adaptive thorax disease classification",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.06.015",
    "abstract": "Unsupervised Domain Adaptation (UDA) based thorax disease classification is a challenging task due to the data distribution discrepancy between source and target domains, and the lack of labeling information in target domain. In this paper, we present an innovative UDA framework that learns invariant and discriminative feature representations from Chest X-rays (CXR) images for UDA-based thorax disease classification across domains. Specifically, a Convolutional Neural Network (CNN) is adopted that explicitly learns discriminative feature representation from CXR images under the supervision of a labeled source domain. A domain-invariance constraint is designed to further align feature distributions between the labeled source domain and the unlabeled target domain. In addition, an instance-invariance constraint and a perturbation-invariance constraint are designed that guide the learning to capture robust and discriminative features from target CXR images. The proposed method has been evaluated on the ChestX-ray14 and SYSU datasets and the experimental results demonstrate its superior robustness and effectiveness relative to state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002094",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Classifier (UML)",
      "Computer science",
      "Constraint (computer-aided design)",
      "Convolutional neural network",
      "Discriminative model",
      "Domain adaptation",
      "Feature (linguistics)",
      "Feature learning",
      "Gene",
      "Geometry",
      "Invariant (physics)",
      "Law",
      "Linguistics",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Ruihua"
      },
      {
        "surname": "Yang",
        "given_name": "Fan"
      },
      {
        "surname": "Luo",
        "given_name": "Yan"
      },
      {
        "surname": "Liu",
        "given_name": "Jianyi"
      },
      {
        "surname": "Wang",
        "given_name": "Cong"
      }
    ]
  },
  {
    "title": "Cross-modality person re-identification via multi-task learning",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108653",
    "abstract": "Despite its promising preliminary results, existing cross-modality Visible-Infrared Person Re-IDentification (VI-PReID) models incorporating semantic (person) masks simply use these person masks as selection maps to separate person features from background regions. Such models do not dedicate to extracting more modality-invariant person body features in the VI-PReID network itself, thus leading to suboptimal results in VI-PReID. Differently, we aim to better capture person body information in the VI-PReID network itself for VI-PReID by exploiting the inner relations between person mask prediction and VI-PReID. To this end, a novel multi-task learning model is presented in this paper, where person body features obtained by person mask prediction potentially facilitate the extraction of discriminative modality-shared person body information for VI-PReID. On top of that, considering the task difference between person mask prediction and VI-PReID, we propose a novel task translation sub-network to transfer discriminative person body information, extracted by person mask prediction, into VI-PReID. Doing so enables our model to better exploit discriminative and modality-invariant person body information. Thanks to more discriminative modality-shared features, our method outperforms previous state-of-the-arts by a significant margin on several benchmark datasets. Our intriguing findings validate the effectiveness of extracting discriminative person body features for the VI-PReID task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001340",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Deep learning",
      "Discriminative model",
      "Economics",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Management",
      "Margin (machine learning)",
      "Modalities",
      "Modality (human–computer interaction)",
      "Pattern recognition (psychology)",
      "Social science",
      "Sociology",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Nianchang"
      },
      {
        "surname": "Liu",
        "given_name": "Kunlong"
      },
      {
        "surname": "Liu",
        "given_name": "Yang"
      },
      {
        "surname": "Zhang",
        "given_name": "Qiang"
      },
      {
        "surname": "Han",
        "given_name": "Jungong"
      }
    ]
  },
  {
    "title": "Complex shearlets and rotary phase congruence tensor for corner detection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108606",
    "abstract": "Corner detection algorithms based on multi-scale analysis attract more attention due to their promising performance. However, they only consider amplitude information, neglect phase information and partially utilize multi-scale decomposition coefficients to detect corners. This limits their detection accuracy, repeatability and localization ability. This paper describes a new multi-scale analysis based corner detector. To overcome the problems of bilateral margin responses, edge extension and lack of phase information in traditional shearlets, a novel complex shearlet transform is proposed to better localize distributed discontinuities and especially to extract phase information from geometrical features. Moreover, a new rotary phase congruence tensor is proposed to utilize all amplitude and phase information for corner detection. Its tolerances to noise and ability for corner localization are improved further by screening and normalizing the amplitude information. Experimental results demonstrate that the localization ability and detection accuracy of the proposed method are superior to current detectors, and its repeatability is generally higher than current detectors and recent machine learning based interest point detectors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000875",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Classification of discontinuities",
      "Computer science",
      "Computer vision",
      "Corner detection",
      "Detector",
      "Edge detection",
      "Image (mathematics)",
      "Image processing",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Shearlet",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Mingzhe"
      },
      {
        "surname": "Sun",
        "given_name": "Changming"
      },
      {
        "surname": "Sowmya",
        "given_name": "Arcot"
      }
    ]
  },
  {
    "title": "Synthetic document generator for annotation-free layout recognition",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108660",
    "abstract": "Analyzing the layout of a document to identify headers, sections, tables, figures etc. is critical to understanding its content. Deep learning based approaches for detecting the layout structure of document images have been promising. However, these methods require a large number of annotated examples during training, which are both expensive and time consuming to obtain. We describe here a synthetic document generator that automatically produces realistic documents with labels for spatial positions, extents and categories of the layout elements. The proposed generative process treats every physical component of a document as a random variable and models their intrinsic dependencies using a Bayesian Network graph. Our hierarchical formulation using stochastic templates allow parameter sharing between documents for retaining broad themes and yet the distributional characteristics produces visually unique samples, thereby capturing complex and diverse layouts. We empirically illustrate that a deep layout detection model trained purely on the synthetic documents can match the performance of a model that uses real documents.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001418",
    "keywords": [
      "Annotation",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Document layout analysis",
      "Generative grammar",
      "Generative model",
      "Generator (circuit theory)",
      "Graph",
      "Image (mathematics)",
      "Information retrieval",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Power (physics)",
      "Process (computing)",
      "Programming language",
      "Quantum mechanics",
      "Template",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Raman",
        "given_name": "Natraj"
      },
      {
        "surname": "Shah",
        "given_name": "Sameena"
      },
      {
        "surname": "Veloso",
        "given_name": "Manuela"
      }
    ]
  },
  {
    "title": "MS 2 GAH: Multi-label semantic supervised graph attention hashing for robust cross-modal retrieval",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108676",
    "abstract": "Due to the strong nonlinear representation capabilities of deep neural networks and the low storage and high efficiency characteristics of hash learning, deep cross-modal hashing has been propelled to the forefront of academics. How to preferably bridge semantic relevance to further bridge the semantic modality gap is the vital bottleneck to improve model performance. Confronting samples with rich semantics, how to comprehensively explore the hidden correlations and establish more precise modality relationships is the primary issue to be solved. In this work, we propose a novel deep hashing method called M ulti-Label S emantic S upervised G raph A ttention H ashing (MS 2 GAH), which is an end-to-end framework that integrates graph attention networks (GATs). It constructs graph features through the adjacency of nodes and assigns different weights to adjacent edges to enhance the robustness of the model. Simultaneously, multi-label annotations are utilized to bridge the semantic relevance between modalities in a more fine-grained manner. To make preferable use of rich semantic information, an end-to-end label encoder is designed to mine high-level semantics from multi-label annotations to guide the feature extraction process of specific-modality networks, thereby further narrowing the modality gap. Finally, extensive experiments have been conducted on four datasets, and the results show that MS 2 GAH is superior to other baselines and one step forward.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001571",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bottleneck",
      "Computer science",
      "Computer security",
      "Data mining",
      "Deep learning",
      "Embedded system",
      "Graph",
      "Hash function",
      "Image (mathematics)",
      "Image retrieval",
      "Leverage (statistics)",
      "Machine learning",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Programming language",
      "Semantic gap",
      "Semantic similarity",
      "Semantics (computer science)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Duan",
        "given_name": "Youxiang"
      },
      {
        "surname": "Chen",
        "given_name": "Ning"
      },
      {
        "surname": "Zhang",
        "given_name": "Peiying"
      },
      {
        "surname": "Kumar",
        "given_name": "Neeraj"
      },
      {
        "surname": "Chang",
        "given_name": "Lunjie"
      },
      {
        "surname": "Wen",
        "given_name": "Wu"
      }
    ]
  },
  {
    "title": "A robust intrinsic feature of images derived from the tensor manifold",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.06.002",
    "abstract": "As an important feature of images, the structure tensor (ST) provides properties of the local image intensities. However, it is a challenging task to analyse images directly using STs since STs are 2nd-order symmetric positive semi-definite matrices. In this paper, we define a robust intrinsic feature of images using ST. In addition, using our feature, we propose an improved image similarity measure. By converting the ST into a symmetric positive definite (called tensor) matrix, we define our feature using the extended geodesic distance of tensors calculated in a Riemannian manifold. Our feature shows the essential natural properties of ST and images as tensors are analysed on the tensor manifold. Moreover, defined by the global embedded geometry of the structure tensor, our feature provides a stable intrinsic property of images. The experiments show that our feature performs well in representing the essential attributes of images, especially the edges and important structures. It also shows that our image similarity measure can accurately detect similar images or patches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001908",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Engineering",
      "Feature (linguistics)",
      "Geodesic",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Manifold (fluid mechanics)",
      "Mathematics",
      "Measure (data warehouse)",
      "Mechanical engineering",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pure mathematics",
      "Similarity (geometry)",
      "Structure tensor",
      "Tensor (intrinsic definition)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Xiangyuan"
      },
      {
        "surname": "Wu",
        "given_name": "Zhongke"
      },
      {
        "surname": "Wang",
        "given_name": "Xingce"
      }
    ]
  },
  {
    "title": "Kernel-based convolution expansion for facial expression recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.06.013",
    "abstract": "The ever-growing depth and width of Convolutional Neural Networks (CNNs) drastically increases the number of their parameters and requires more powerful devices to train and deploy. In this paper, we propose a new architecture that outperforms the classical linear convolution function by expanding the latter to a higher degree kernel function without additional weights. We opt for Taylor Series Kernel which maps input data to a higher-dimensional Reproducing Kernel Hilbert Space (RKHS). Mapping features to a higher-order RKHS is performed in both implicit and explicit ways. For the former way, we compute several polynomial kernels of different degrees leveraging the kernel trick. Whereas, the latter way is achieved by concatenating the result of these polynomial kernels. The proposed Taylor Series Kernelized Convolution (TSKC) is able to learn more complex patterns than the linear convolution kernel and thus be more discriminative. The experiments conducted on Facial Expression Recognition (FER) datasets demonstrate that TSKC outperforms the ordinary convolution layer without additional parameters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002070",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Discrete mathematics",
      "Discriminative model",
      "Hilbert space",
      "Kernel (algebra)",
      "Kernel embedding of distributions",
      "Kernel method",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Polynomial",
      "Polynomial kernel",
      "Pure mathematics",
      "Radial basis function kernel",
      "Reproducing kernel Hilbert space",
      "String kernel",
      "Support vector machine",
      "Taylor series",
      "Tree kernel",
      "Variable kernel density estimation"
    ],
    "authors": [
      {
        "surname": "Mahmoudi",
        "given_name": "M. Amine"
      },
      {
        "surname": "Chetouani",
        "given_name": "Aladine"
      },
      {
        "surname": "Boufera",
        "given_name": "Fatma"
      },
      {
        "surname": "Tabia",
        "given_name": "Hedi"
      }
    ]
  },
  {
    "title": "Learning to resolve uncertainties for large-scale face recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.06.004",
    "abstract": "Facial recognition is a category of biometric security, used widely in various industries where we identify and authenticate an individuals identity using their face. In the modern deep learning era, face recognition datasets are playing a significant role in achieving state-of-the-art accuracy by acquiring and training millions of face images. Annotating such a large-scale face recognition dataset is challenging due to low-quality face images, and incorrect annotations unknowingly made by annotators. Training a deep learning model with such uncertainties leads to deep model overfitting on noisy uncertain samples and degradation of the discriminative ability of the model. To address these issues, we propose a simple yet effective uncertainty learning network that efficiently reduces over-fitting caused by uncertain face images. More specifically, our FC module weights each sample in the mini-batch at the decision layer, and relabeling mechanism carefully modify the labels of incorrect samples in the mini- batch. Results on IJB-B, IJB-C, LFW, AgeDB30, CFP-FP, CALFW and CPLFW public datasets demonstrate that our approach achieves state-of-the-art performance",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001921",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Biometrics",
      "Chemistry",
      "Computer science",
      "Deep learning",
      "Discriminative model",
      "Face (sociological concept)",
      "Face detection",
      "Facial recognition system",
      "Gene",
      "Machine learning",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Robustness (evolution)",
      "Scale (ratio)",
      "Social science",
      "Sociology",
      "Three-dimensional face recognition"
    ],
    "authors": [
      {
        "surname": "Boragule",
        "given_name": "Abhijeet"
      },
      {
        "surname": "Akram",
        "given_name": "Hamna"
      },
      {
        "surname": "Kim",
        "given_name": "Jeongbae"
      },
      {
        "surname": "Jeon",
        "given_name": "Moongu"
      }
    ]
  },
  {
    "title": "Nonconvex clustering via ℓ 0 fusion penalized regression",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108689",
    "abstract": "Cluster analysis has attracted widespread attention in the past several decades. Generally speaking, clustering is considered as an important unsupervised learning method because its goal is to discover unknown subgroups in data without category label information. In this paper, we propose the ℓ 0 fusion penalized clustering model ( ℓ 0 -PClust), which is a novel clustering framework founded on the penalized regression method. Theoretically, we first analyze the existence of the optimal solutions of our model and deduce an upper bound of the tuning parameter. Then we define the Karush-Kuhn-Tucker point and P-stationary point of the ℓ 0 -PClust model, and establish the relationship between them and local optimal solutions. Moreover, based on the P-stationary point of the ℓ 0 -PClust model, we prove that the distances among different cluster centers are greater than a positive threshold. Computationally, we solve the ℓ 0 -PClust model via the famous alternating direction method of multipliers, whose limit point is a P-stationary point and local optimal solution of the model. Finally, we conduct extensive experiments on both synthetic and real data sets. Experimental results show outstanding performance of our method in comparison with several state-of-the-art clustering methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001704",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Geometry",
      "Mathematics",
      "Point (geometry)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Huangyue"
      },
      {
        "surname": "Kong",
        "given_name": "Lingchen"
      },
      {
        "surname": "Li",
        "given_name": "Yan"
      }
    ]
  },
  {
    "title": "Gradient-based refined class activation map for weakly supervised object localization",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108664",
    "abstract": "Weakly supervised object localization locates objects based on the localization map generated from the classification network. However, most existing methods utilize the information of the target class to locate objects based on the feature map of a single image, which ignores both the relationships of inter-class and intra-class. In this work, we propose a Gradient-based Refined Class Activation Map (GRCAM) approach to achieve more accurate localization. Two kinds of gradients are applied to reveal the relationships of inter-class and intra-class during the testing stage. First, we exploit the gradients of the classification loss function concerning the feature map to enhance class-specific information. The gradients of classification loss reveal the connection among the predicted probabilities of all classes. Second, we design a regression function that refers to the loss between the pseudo-bounding box coordinates containing category consistency and the predicted coordinates generated from the localization map. The predicted coordinates are revised by the gradients of the regression function. The gradients of the regression function reveal the consistency within a class. Despite the apparent simplicity, we demonstrate the advantages of GRCAM on ILSVRC and CUB-200-2011 in extensive experiments. Especially, on ILSVRC dataset, the proposed GRCAM achieves a new state-of-the-art Top-1 localization error of 42.94%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001455",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Bounding overwatch",
      "Class (philosophy)",
      "Computer science",
      "Consistency (knowledge bases)",
      "Evolutionary biology",
      "Feature (linguistics)",
      "Function (biology)",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Minimum bounding box",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Regression",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Hui",
        "given_name": "Wenjun"
      },
      {
        "surname": "Tan",
        "given_name": "Chuangchuang"
      },
      {
        "surname": "Gu",
        "given_name": "Guanghua"
      },
      {
        "surname": "Zhao",
        "given_name": "Yao"
      }
    ]
  },
  {
    "title": "An attention-based framework for multi-view clustering on Grassmann manifold",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108610",
    "abstract": "The key problem of multi-view clustering is to handle the inconsistency among multiple views. This article proposes an attention-based framework for multi-view clustering on Grassmann manifold (AMCGM). To be specific, the proposed AMCGM framework aims to learn a representative element on Grassmann manifold with the following four highlights: 1) AMCGM framework performs an attention-based weighted-learning scheme to capture the difference of views; 2) The clustering results can be directly generated by the structured graph learned via AMCGM, avoiding the randomness caused by traditional label-generation procedures, such as K -means clustering; 3) AMCGM has high extensibility since it can generate many multi-view clustering models on Grassmann manifold; 4) On Grassmann manifold, the relationship between the projection metric (PM)-based multi-view clustering model and squared projection metric (SPM)-based model is studied. Based on AMCGM framework, we propose some generated models and provide some useful conclusions. Moreover, to solve the optimization problems involved in the proposed AMCGM framework and generated models, we propose an efficiently iterative algorithm and provide rigorous convergence analysis. Extensive experimental results demonstrate the superb performance of our framework.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000917",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Economics",
      "Engineering",
      "Graph",
      "Grassmannian",
      "Manifold (fluid mechanics)",
      "Mathematics",
      "Mechanical engineering",
      "Metric (unit)",
      "Operations management",
      "Projection (relational algebra)",
      "Randomness",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Danyang"
      },
      {
        "surname": "Dong",
        "given_name": "Xia"
      },
      {
        "surname": "Nie",
        "given_name": "Feiping"
      },
      {
        "surname": "Wang",
        "given_name": "Rong"
      },
      {
        "surname": "Li",
        "given_name": "Xuelong"
      }
    ]
  },
  {
    "title": "Diverse video captioning through latent variable expansion",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.05.021",
    "abstract": "Automatically describing video content with text description is challenging but important task, which has been attracting a lot of attention in computer vision community. Previous works mainly strive for the accuracy of the generated sentences, while ignoring the sentences diversity, which is inconsistent with human behavior. In this paper, we aim to caption each video with multiple descriptions and propose a novel framework. Concretely, for a given video, the intermediate latent variables of conventional encode-decode process are utilized as input to the conditional generative adversarial network (CGAN) with the purpose of generating diverse sentences. We adopt different Convolutional Neural Networks (CNNs) as our generator that produces descriptions conditioned on latent variables and discriminator that assesses the quality of generated sentences. Simultaneously, a novel DCE metric is designed to assess the diverse captions. We evaluate our method on the benchmark datasets, where it demonstrates its ability to generate diverse descriptions and achieves superior results against other state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001829",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Closed captioning",
      "Computer science",
      "Convolutional neural network",
      "Detector",
      "Discriminator",
      "ENCODE",
      "Economics",
      "Gene",
      "Generative grammar",
      "Generator (circuit theory)",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Latent variable",
      "Latent variable model",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Metric (unit)",
      "Natural language processing",
      "Operating system",
      "Operations management",
      "Physics",
      "Power (physics)",
      "Process (computing)",
      "Programming language",
      "Quantum mechanics",
      "Semantics (computer science)",
      "Task (project management)",
      "Telecommunications",
      "Variable (mathematics)"
    ],
    "authors": [
      {
        "surname": "Xiao",
        "given_name": "Huanhou"
      },
      {
        "surname": "Shi",
        "given_name": "Jinglun"
      }
    ]
  },
  {
    "title": "Reflection symmetry detection of shapes based on shape signatures",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108667",
    "abstract": "We present two novel shape signature-based reflection symmetry detection methods with their theoretical underpinning and empirical evaluation. LIP-signature and R-signature share similar beneficial properties allowing to detect reflection symmetry directions in a high-performing manner. For the shape signature of a given shape, its merit profile is constructed to detect candidates of symmetry direction. A verification process is utilized to eliminate the false candidates by addressing Radon projections. The proposed methods can effectively deal with compound shapes which are challenging for traditional contour-based methods. To quantify the symmetric efficiency, a new symmetry measure is proposed over the range [0, 1]. Furthermore, we introduce two symmetry shape datasets with a new evaluation protocol and a lost measure for evaluating symmetry detectors. Experimental results using standard and new datasets suggest that the proposed methods prominently perform compared to state of the art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001480",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Composite material",
      "Computer science",
      "Data mining",
      "Detector",
      "Geometry",
      "Materials science",
      "Mathematics",
      "Measure (data warehouse)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Range (aeronautics)",
      "Reflection (computer programming)",
      "Reflection symmetry",
      "Rotational symmetry",
      "Signature (topology)",
      "Symmetry (geometry)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Nguyen",
        "given_name": "Thanh Phuong"
      },
      {
        "surname": "Truong",
        "given_name": "Hung Phuoc"
      },
      {
        "surname": "Nguyen",
        "given_name": "Thanh Tuan"
      },
      {
        "surname": "Kim",
        "given_name": "Yong-Guk"
      }
    ]
  },
  {
    "title": "SPARE: Self-supervised part erasing for ultra-fine-grained visual categorization",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108691",
    "abstract": "This paper presents SPARE, a self-supervised part erasing framework for ultra-fine-grained visual categorization. The key insight of our model is to learn discriminative representations by encoding a self-supervised module that performs random part erasing and prediction on the contextual position of the erased parts. This drives the network to exploit intrinsic structure of data, i.e., understanding and recognizing the contextual information of the objects, thus facilitating more discriminative part-level representation. This also enhances the learning capability of the model by introducing more diversified training part segments with semantic meaning. We demonstrate that our approach is able to achieve strong performance on seven publicly available datasets covering ultra-fine-grained visual categorization and fine-grained visual categorization tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001728",
    "keywords": [
      "Artificial intelligence",
      "Categorization",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Information retrieval",
      "Machine learning",
      "Natural language processing",
      "Operations management",
      "Pattern recognition (psychology)",
      "Spare part"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Xiaohan"
      },
      {
        "surname": "Zhao",
        "given_name": "Yang"
      },
      {
        "surname": "Gao",
        "given_name": "Yongsheng"
      }
    ]
  },
  {
    "title": "Memory-based Transformer with shorter window and longer horizon for multivariate time series forecasting",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.05.010",
    "abstract": "Multivariate time series forecasting is an important problem that spans many fields. One challenge of this problem is the complex and non-linear interdependence between time steps and different variables. Recent studies have shown that Transformer has potential in capturing long-term dependencies. However, in the field of time series forecasting, Transformer still has some problems to solve, such as prediction fragmentation and insensitivity to data scale. In addition, traditional forecasting models often require a large amount of input data to support the training of the model when predicting long-term data. However, it is hard to provide sufficient time series input data due to equipment damage or weather situation. To solve these limitations, a memory-based Transformer with shorter window and longer horizon is proposed, called SWLHT. It uses the memory mechanism to make the model no longer only rely on a single input, but can combine the previous forecast results to assist in capturing long-term dependencies, thereby avoiding the requirement of excessively long input sequence. Furthermore, the memory mechanism can alleviate the prediction fragmentation to some extent. The experimental results and comparison of baselines on several real-world multivariate time series datasets have verified the effectiveness of the proposed model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522001623",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Data mining",
      "Electrical engineering",
      "Engineering",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Multivariate statistics",
      "Paleontology",
      "Series (stratigraphy)",
      "Time horizon",
      "Time series",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yang"
      },
      {
        "surname": "Wang",
        "given_name": "Zheng"
      },
      {
        "surname": "Yu",
        "given_name": "Xinyang"
      },
      {
        "surname": "Chen",
        "given_name": "Xin"
      },
      {
        "surname": "Sun",
        "given_name": "Meijun"
      }
    ]
  },
  {
    "title": "Erlang planning network: An iterative model-based reinforcement learning with multi-perspective",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108668",
    "abstract": "For model-based reinforcement learning (MBRL), one of the key challenges is modeling error, which cripples the effectiveness of model planning and causes poor robustness during training. In this paper, we propose a bi-level Erlang Planning Network (EPN) architecture, which is composed of an upper-level agent and several multi-scale parallel sub-agents, trained in an iterative way. The proposed method focuses upon the expansion of representation by environment: a multi-perspective over the world model, which presents a varied way to represent an agent’s knowledge about the world that alleviates the problem of falling into local optimal points and enhances robustness during the progress of model planning. Moreover, our experiments evaluate EPN on a range of continuous-control tasks in MuJoCo, the evaluation results show that the proposed framework finds exemplar solutions faster and consistently reaches the state-of-the-art performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001492",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Distributed computing",
      "Erlang (programming language)",
      "Functional programming",
      "Gene",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Reinforcement learning",
      "Robustness (evolution)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jiao"
      },
      {
        "surname": "Zhang",
        "given_name": "Lemin"
      },
      {
        "surname": "He",
        "given_name": "Zhiqiang"
      },
      {
        "surname": "Zhu",
        "given_name": "Can"
      },
      {
        "surname": "Zhao",
        "given_name": "Zihui"
      }
    ]
  },
  {
    "title": "Enhanced task attention with adversarial learning for dynamic multi-task CNN",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108672",
    "abstract": "Multi-task deep learning is promising to solve multi-label multi-instance visual recognition tasks. However, flexible information sharing in the task group might bring performance bottlenecks to an individual task. To tackle this problem, we propose a novel learning framework of multi-task Convolutional Neural Network (CNN) to enhance task attention through conditionally tuning the Task Transfer Connections (TTC) with adversarial learning. For the dynamic multi-task CNN, we set up a shared subnet to extract shared features across multiple tasks and a task discriminator shared by all layers to distinguish features of all subnets. The adversarial training is introduced between the shared subnet and the task discriminator to guide each task subnet to focus on its specific task. To apply adversarial learning to the complex labeling system of multiple tasks, we design an even-label strategy for the multi-task model with a shared subnet to make adversarial learning feasible for the complex labeling system of multiple tasks. As a result, the proposed model can constrain the shared subnet’s learning unbiased to any single task and achieve task attention for all task subnets. Experimental results of the ablation study and the TTC analysis validate the effectiveness of the proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001534",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Convolutional neural network",
      "Detector",
      "Discriminator",
      "Engineering",
      "Machine learning",
      "Multi-task learning",
      "Programming language",
      "Set (abstract data type)",
      "Subnet",
      "Systems engineering",
      "Task (project management)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Fang",
        "given_name": "Yuchun"
      },
      {
        "surname": "Xiao",
        "given_name": "Shiwei"
      },
      {
        "surname": "Zhou",
        "given_name": "Menglu"
      },
      {
        "surname": "Cai",
        "given_name": "Sirui"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhaoxiang"
      }
    ]
  },
  {
    "title": "A feature consistency driven attention erasing network for fine-grained image retrieval",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108618",
    "abstract": "Large-scale fine-grained image retrieval based hashing learning method has two main problems. First, low dimension feature embedding can fasten the retrieval process but bring accuracy decrease due to much information loss. Second, fine-grained images lead to the same category query hash codes mapping into the different cluster in database hash latent space. To handle these issues, we propose a feature consistency driven attention erasing network (FCAENet) for fine-grained image retrieval. For the first issue, we propose an adaptive augmentation module in FCAENet, which is the selective region erasing module (SREM). SREM makes the network more robust on subtle differences of fine-grained task by adaptively covering some regions of raw images. The feature extractor and hash layer can learn more representative hash codes for fine-grained images by SREM. With regard to the second issue, we fully exploit the pair-wise similarity information and add the enhancing space relation loss (ESRL) in FCAENet to make the vulnerable relation stabler between the query hash code and database hash code. We conduct extensive experiments on five fine-grained benchmark datasets (CUB2011, Aircraft, NABirds, VegFru, Food101) for 12bits, 24bits, 32bits, 48bits hash codes. The results show that FCAENet achieves the state-of-the-art (SOTA) fine-grained image retrieval performance based on the hashing learning method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322000991",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer security",
      "Consistency (knowledge bases)",
      "Double hashing",
      "Feature (linguistics)",
      "Feature hashing",
      "Geodesy",
      "Geography",
      "Hash function",
      "Hash table",
      "Image (mathematics)",
      "Image retrieval",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Qi"
      },
      {
        "surname": "Wang",
        "given_name": "Xu"
      },
      {
        "surname": "Lyu",
        "given_name": "Shuchang"
      },
      {
        "surname": "Liu",
        "given_name": "Binghao"
      },
      {
        "surname": "Yang",
        "given_name": "Yifan"
      }
    ]
  },
  {
    "title": "Encoder deep interleaved network with multi-scale aggregation for RGB-D salient object detection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108666",
    "abstract": "Recently, RGB-D salient object detection (SOD) has aroused widespread research interest. Existing RGB-D SOD approaches mainly consider the cross-modal information fusion in the decoder. And their multi-modal interaction mainly concentrates on the same level of features between RGB stream and depth stream. They do not deeply explore the coherence of multi-model features at different levels. In this paper, we design a two-stream deep interleaved encoder network to extract RGB and depth information and realize their mixing simultaneously. This network allows us to gradually learn multi-modal representation at different levels from shallow to deep. Moreover, to further fuse multi-modal features in the decoding stage, we propose a cross-modal mutual guidance module and a residual multi-scale aggregation module to implement the global guidance and local refinement of the salient region. Extensive experiments on six benchmark datasets demonstrate that the proposed approach performs favorably against most state-of-the-art methods under different evaluation metrics. During the testing stage, this model can run at a real-time speed of 93 FPS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001479",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Database",
      "Decoding methods",
      "Electrical engineering",
      "Encoder",
      "Encoding (memory)",
      "Engineering",
      "Fuse (electrical)",
      "Geodesy",
      "Geography",
      "Law",
      "Modal",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Polymer chemistry",
      "Quantum mechanics",
      "RGB color model",
      "Representation (politics)",
      "Salient",
      "Scalability",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Feng",
        "given_name": "Guang"
      },
      {
        "surname": "Meng",
        "given_name": "Jinyu"
      },
      {
        "surname": "Zhang",
        "given_name": "Lihe"
      },
      {
        "surname": "Lu",
        "given_name": "Huchuan"
      }
    ]
  },
  {
    "title": "Topic-word-constrained sentence generation with variational autoencoder",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.06.016",
    "abstract": "We propose a topic-word-constrained sentence-generation model with a variational autoencoder and convolutional neural network. It can generate sentences conditioned on a given topic distribution and a certain word. Unlike the vanilla variational autoencoder that assumes a standard Gaussian prior for the latent code, our model specifies the prior for the topic latent code as multiple Gaussian distributions, where each Gaussian distribution corresponds to a topic vector parameterized by a convolutional neural topic model. For word constraints, the decoder in the variational autoencoder generates sentences backward and forward starting from a given word. The topic latent space is arranged by the similarity of topic vectors, and the topic latent code restricts the sentence latent code through a loss term, through which expanded semantically meaningful latent spaces can be learned and provide topic guidance while generating sentences. Experimental results show that our model can generate coherent and diverse sentences related to given topics and words, while also avoiding the Kullback–Leibler divergence collapse problem. Moreover, it outperforms alternative approaches in terms of sentence reconstruction, latent space property and the quality, diversity, and topic controllability of generated sentences.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002100",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Autoencoder",
      "Code (set theory)",
      "Computer science",
      "Deep learning",
      "Divergence (linguistics)",
      "Gaussian",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Natural language processing",
      "Philosophy",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Sentence",
      "Set (abstract data type)",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Tianbao"
      },
      {
        "surname": "Sun",
        "given_name": "Jingbo"
      },
      {
        "surname": "Liu",
        "given_name": "Xin"
      },
      {
        "surname": "Song",
        "given_name": "Jihua"
      },
      {
        "surname": "Peng",
        "given_name": "Weiming"
      }
    ]
  },
  {
    "title": "ERANNs: Efficient residual audio neural networks for audio pattern recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.07.012",
    "abstract": "Audio pattern recognition (APR) is an important research topic and can be applied to several fields related to our lives. Therefore, accurate and efficient APR systems need to be developed as they are useful in real applications. In this paper, we propose a new convolutional neural network (CNN) architecture and a method for improving the inference speed of CNN-based systems for APR tasks. Moreover, using the proposed method, we can improve the performance of our systems, as confirmed in experiments conducted on four audio datasets. In addition, we investigate the impact of data augmentation techniques and transfer learning on the performance of our systems. Our best system achieves a mean average precision (mAP) of 0.450 on the AudioSet dataset. Although this value is less than that of the state-of-the-art system, the proposed system is 7.1x faster and 9.7x smaller. On the ESC-50, UrbanSound8K, and RAVDESS datasets, we obtain state-of-the-art results with accuracies of 0.961, 0.908, and 0.748, respectively. Our system for the ESC-50 dataset is 1.7x faster and 2.3x smaller than the previous best system. For the RAVDESS dataset, our system is 3.3x smaller than the previous best system. We name our systems “Efficient Residual Audio Neural Networks”.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002239",
    "keywords": [],
    "authors": [
      {
        "surname": "Verbitskiy",
        "given_name": "Sergey"
      },
      {
        "surname": "Berikov",
        "given_name": "Vladimir"
      },
      {
        "surname": "Vyshegorodtsev",
        "given_name": "Viacheslav"
      }
    ]
  },
  {
    "title": "LandmarkGAN: Synthesizing faces from landmarks",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.07.004",
    "abstract": "Face synthesis is an important problem in computer vision with many applications. In this work, we describe a new method, namely LandmarkGAN, to synthesize faces based on facial landmarks as input. Facial landmarks are a natural, intuitive, and effective representation for facial expressions and orientations, which are independent from the target’s texture or color and background scene. Our method is able to transform a set of facial landmarks into new faces of different subjects, while retains the same facial expression and orientation. Experimental results on face synthesis and reenactments demonstrate the effectiveness of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552200215X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Face (sociological concept)",
      "Facial expression",
      "Geometry",
      "Image (mathematics)",
      "Law",
      "Mathematics",
      "Orientation (vector space)",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Set (abstract data type)",
      "Social science",
      "Sociology",
      "Texture (cosmology)"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Pu"
      },
      {
        "surname": "Li",
        "given_name": "Yuezun"
      },
      {
        "surname": "Qi",
        "given_name": "Honggang"
      },
      {
        "surname": "Lyu",
        "given_name": "Siwei"
      }
    ]
  },
  {
    "title": "Unsupervised ensemble learning for genome sequencing",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108721",
    "abstract": "Unsupervised ensemble learning refers to methods devised for a particular task that combine data provided by decision learners taking into account their reliability, which is usually inferred from the data. Here, the variant calling step of the next generation sequencing technologies is formulated as an unsupervised ensemble classification problem. A variant calling algorithm based on the expectation-maximization algorithm is further proposed that estimates the maximum-a-posteriori decision among a number of classes larger than the number of different labels provided by the learners. Experimental results with real human DNA sequencing data show that the proposed algorithm is competitive compared to state-of-the-art variant callers as GATK, HTSLIB, and Platypus.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002023",
    "keywords": [],
    "authors": [
      {
        "surname": "Pagès-Zamora",
        "given_name": "Alba"
      },
      {
        "surname": "Ochoa",
        "given_name": "Idoia"
      },
      {
        "surname": "Cavero",
        "given_name": "Gonzalo Ruiz"
      },
      {
        "surname": "Villalvilla-Ornat",
        "given_name": "Pol"
      }
    ]
  },
  {
    "title": "Variational cycle-consistent imputation adversarial networks for general missing patterns",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108720",
    "abstract": "Imputation of missing data is an important but challenging issue because we do not know the underlying distribution of the missing data. Previous imputation models have addressed this problem by assuming specific kinds of missing distributions. However, in practice, the mechanism of the missing data is unknown, so the most general case of missing pattern needs to be considered for successful imputation. In this paper, we present cycle-consistent imputation adversarial networks to discover the underlying distribution of missing patterns closely under some relaxations. Using adversarial training, our model successfully learns the most general case of missing patterns. Therefore our method can be applied to a wide variety of imputation problems. We empirically evaluated the proposed method with numerical and image data. The result shows that our method yields the state-of-the-art performance quantitatively and qualitatively on standard datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002011",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Imputation (statistics)",
      "Machine learning",
      "Missing data"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Woojin"
      },
      {
        "surname": "Lee",
        "given_name": "Sungyoon"
      },
      {
        "surname": "Byun",
        "given_name": "Junyoung"
      },
      {
        "surname": "Kim",
        "given_name": "Hoki"
      },
      {
        "surname": "Lee",
        "given_name": "Jaewook"
      }
    ]
  },
  {
    "title": "Unsupervised moving object segmentation using background subtraction and optimal adversarial noise sample search",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108719",
    "abstract": "Moving Objects Segmentation (MOS) is a fundamental task in many computer vision applications such as human activity analysis, visual object tracking, content based video search, traffic monitoring, surveillance, and security. MOS becomes challenging due to abrupt illumination variations, dynamic backgrounds, camouflage and scenes with bootstrapping. To address these challenges we propose a MOS algorithm exploiting multiple adversarial regularizations including conventional as well as least squares losses. More specifically, our model is trained on scene background images with the help of cross-entropy loss, least squares adversarial loss and ℓ 1 loss in image space working jointly to learn the dynamic background changes. During testing, our proposed method aims to generate test image background scenes by searching optimal noise samples using joint minimization of ℓ 1 loss in image space, ℓ 1 loss in feature space, and discriminator least squares loss. These loss functions force the generator to synthesize dynamic backgrounds similar to the test sequences which upon subtraction results in moving objects segmentation. Experimental evaluations on five benchmark datasets have shown excellent performance of the proposed algorithm compared to the twenty one existing state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200200X",
    "keywords": [
      "Artificial intelligence",
      "Background subtraction",
      "Computer science",
      "Computer vision",
      "Detector",
      "Discriminator",
      "Image (mathematics)",
      "Inpainting",
      "Pattern recognition (psychology)",
      "Pixel",
      "Segmentation",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Sultana",
        "given_name": "Maryam"
      },
      {
        "surname": "Mahmood",
        "given_name": "Arif"
      },
      {
        "surname": "Jung",
        "given_name": "Soon Ki"
      }
    ]
  },
  {
    "title": "ChildGAN: Face aging and rejuvenation to find missing children",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108761",
    "abstract": "Child-face aging and rejuvenation have amassed considerable active research interest, owing to their immense impact on a broad range of social and security applications, e.g., digital entertainment, fashion and wellness, and searching for long-lost children using childhood photos. All current face aging approaches based on generative adversarial networks (GANs) focus on adult images or long-term aging. We present a new large-scale longitudinal Indian child (ICD) benchmark dataset to facilitate face age progression and regression, cross-age face recognition, age estimation, gender prediction, and kinship face recognition to alleviate these issues. Furthermore, we propose an automatic child-face age progression and regression model, namely, ChildGAN, that generates visually realistic images for enhanced face-identification accuracy while preserving the identity. Consequently, we have trained state-of-the-art (SOTA) face aging models on ICD for comprehensive qualitative and quantitative evaluations. We also present a multi-racial experiments dataset named Multi-Racial Child Dataset (MRCD) containing 64,965 child face images. The images are selected from publicly available datasets and web crawling. Finally, we investigate the generalization of ChildGAN by experimenting with White, Black, Asian, and Indian races. The experimental results suggest that the proposed ChildGAN and SOTA models can aid in reconnecting young children, who were lost at a young age as victims of child trafficking or abduction, with their families. The model and the MRCD web crawled images are available at https://github.com/praveenkumarchandaliya/ChildGAN_Tamp1/.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002424",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Face (sociological concept)",
      "Facial recognition system",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Chandaliya",
        "given_name": "Praveen Kumar"
      },
      {
        "surname": "Nain",
        "given_name": "Neeta"
      }
    ]
  },
  {
    "title": "Self-weighted learning framework for adaptive locality discriminant analysis",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108778",
    "abstract": "Linear discriminant analysis (LDA) is one of the most important dimensionality reduction techniques and applied in many areas. However, traditional LDA algorithms aim to capture the global structure from data and ignore the local information. That may lead to the failure of LDA in some real-world datasets which have a complex geometry distribution. Although there are many previous works that focus on preserving the local information, they are all stuck in the same problem that the neighbor relationships of pairwise data points obtained from the original space may not be reliable, especially in the case of heavy noise. Therefore, we proposed a novel self-weighted learning framework, named Self-Weighted Adaptive Locality Discriminant Analysis (SALDA), for locality-aware based dimensionality reduction. The proposed framework can adaptively learn an intrinsic low-dimensional subspace, so that we can explore the better neighbor relationships for samples under the ideal subspace. In addition, our model can automatically learn to assign the weights to data pairwise points within the same class and takes no extra parameters compared to other classical locality-aware methods. At last, the experimental results on both synthetic and real-world benchmark datasets demonstrate the effectiveness and superiority of the proposed algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200259X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Curse of dimensionality",
      "Dimensionality reduction",
      "Discriminant",
      "Linear discriminant analysis",
      "Linguistics",
      "Locality",
      "Machine learning",
      "Mathematics",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Chang",
        "given_name": "Wei"
      },
      {
        "surname": "Nie",
        "given_name": "Feiping"
      },
      {
        "surname": "Wang",
        "given_name": "Zheng"
      },
      {
        "surname": "Wang",
        "given_name": "Rong"
      },
      {
        "surname": "Li",
        "given_name": "Xuelong"
      }
    ]
  },
  {
    "title": "Versatile, full‐spectrum, and swift network sampling for model generation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108729",
    "abstract": "Given one task, it is difficult to generate CNN models for many different hardware platforms with extremely diverse computing power for this task. Repeating network pruning or architecture search for each platform is very time-consuming. In this paper, we propose properties that are required for this model generation problem: versatile (fits diverse applications and network structures), full-spectrum (generates models for devices with tiny to gigantic computing power), and swift (total training time for all platforms is short, and generated models have low latency). We show that existing methods do not satisfy these requirements and propose a VFS method (the V/F/S represents Versatile/Full-spectrum/Swift, respectively). VFS uses importance sampling to sample many submodels with versatile structures and with different input image resolutions. We propose new fine-tuning strategies that only need to fine-tune a best candidate submodel for few epochs for each platform. VFS satisfies all three requirements. It generates versatile models with low latency for diverse applications, is suitable for devices with a wide range of computing power differences, and the models which are generated by VFS achieve state-of-the-art accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002102",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Biology",
      "Composite material",
      "Computer engineering",
      "Computer network",
      "Computer science",
      "Distributed computing",
      "Economics",
      "Latency (audio)",
      "Low latency (capital markets)",
      "Management",
      "Materials science",
      "Programming language",
      "Pruning",
      "Range (aeronautics)",
      "Real-time computing",
      "Swift",
      "Task (project management)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Huanyu"
      },
      {
        "surname": "Zhang",
        "given_name": "Yongshun"
      },
      {
        "surname": "Wu",
        "given_name": "Jianxin"
      }
    ]
  },
  {
    "title": "Atlanta scaled layouts from non-central panoramas",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108740",
    "abstract": "In this work we present a novel approach for 3D layout recovery of indoor environments using a non-central acquisition system. From a single non-central panorama, full and scaled 3D lines can be independently recovered by geometry reasoning without additional nor scale assumptions. However, their sensitivity to noise and complex geometric modeling has led these panoramas and required algorithms being little investigated. Our new pipeline aims to extract the boundaries of the structural lines of an indoor environment with a neural network and exploit the properties of non-central projection systems in a new geometrical processing to recover scaled 3D layouts. The results of our experiments show that we improve state-of-the-art methods for layout recovery and line extraction in non-central projection systems. We completely solve the problem both in Manhattan and Atlanta environments, handling occlusions and retrieving the metric scale of the room without extra measurements. As far as the authors’ knowledge goes, our approach is the first work using deep learning on non-central panoramas and recovering scaled layouts from single panoramas.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002217",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Image (mathematics)",
      "Metric (unit)",
      "Noise (video)",
      "Operations management",
      "Panorama",
      "Pipeline (software)",
      "Programming language",
      "Projection (relational algebra)"
    ],
    "authors": [
      {
        "surname": "Berenguel-Baeta",
        "given_name": "Bruno"
      },
      {
        "surname": "Bermudez-Cameo",
        "given_name": "Jesus"
      },
      {
        "surname": "Guerrero",
        "given_name": "Jose J."
      }
    ]
  },
  {
    "title": "A new distance between multivariate clusters of varying locations, elliptical shapes, and directions",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108780",
    "abstract": "Clustering methods are based on the computations of both the distances between every pair of the n observations in a multivariate dataset as well as the distances between every pair of clusters in the dataset. The clusters can have different locations and varying elliptical shapes and directions. Numerous methods have been proposed in the literature for computing both of these two types of distances. The contributions of this paper are two folds. First, we propose a new elliptical distance between pairs of clusters in a dataset with different cluster centers and elliptical shapes and directions, Second, we proved analytically that the Ward distance and the Euclidean distance are equivalent. We propose a new classical method for computing the distance between a pair of clusters in the dataset. It is the only distance that does not assume spherical clusters. The proposed classical distances could also be made robust by replacing estimates of location and scale by their respective robust estimators. The proposed distance has a number of advantages including simplicity, interpretability, computational efficiency as well as the ability to accurately capture both the variability of the cluster centers as well as the variability of shapes and directions of their respective covariance matrices. The method is also illustrated by several motivating examples that demonstrate the need of the new proposed distance. The superiority of the proposed method is also demonstrated by application to real-life as well as challenging synthetic data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002618",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Combinatorics",
      "Computation",
      "Computer science",
      "Covariance",
      "Distance matrices in phylogeny",
      "Distance matrix",
      "Elliptical distribution",
      "Estimator",
      "Euclidean distance",
      "Interpretability",
      "Mathematics",
      "Multivariate normal distribution",
      "Multivariate statistics",
      "Probability distribution",
      "Programming language",
      "Statistical distance",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Hadi",
        "given_name": "Ali S."
      }
    ]
  },
  {
    "title": "DuETNet: Dual Encoder based Transfer Network for thoracic disease classification",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.08.007",
    "abstract": "In thoracic disease classification, the original chest X-ray images are high resolution images. Nevertheless, in existing convolution neural network (CNN) models, the original images are resized to 224 × 224 before use. Diseases in local areas may not be sufficiently represented because the chest X-ray images have been resized, which compresses information excessively. Therefore, a higher resolution is required to focus on the local representations. Although the large input resolution reduces memory efficiency, previous studies have investigated using CNNs with the large input for classification performance improvement. Moreover, optimization for imbalanced classes is required because chest X-ray images have highly imbalanced pathology labels. Hence, this study proposes the Dual Encoder based Transfer Network (DuETNet) to counter the inefficiency caused by large input resolution and improve classification performance by adjusting the input size based on the RandomResizedCrop method. This image transformation method crops a random area of a given image and resizes it to a given size. Thus, a resolution calibration guideline is a practical way to achieve memory efficiency and performance gains under restricted resources by adjusting the scale factor σ on the training and test images. To treat high class imbalance, we propose entropy based label smoothing method. The method enhances generalization performance for the imbalanced minor classes by penalizing the major classes. The dual encoder comprises channel and spatial encoders, which apply channel- and spatial-wise attention to enhance the relatively significant features from the adjusted images. To evaluate the performance of DuETNet, we used the ChestX-ray14 and MIMIC-CXR-JPG datasets, and DuETNet achieved a new state-of-the-art method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002446",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Dual (grammatical number)",
      "Encoder",
      "Literature",
      "Operating system",
      "Parallel computing",
      "Pattern recognition (psychology)",
      "Transfer (computing)"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Min Seok"
      },
      {
        "surname": "Han",
        "given_name": "Sung Won"
      }
    ]
  },
  {
    "title": "On the robustness of self-supervised representations for multi-view object classification",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.07.016",
    "abstract": "It is known that representations from self-supervised pre-training can perform on par, and often better, on various downstream tasks than representations from fully-supervised pre-training. This has been shown in a host of settings such as generic object classification and detection, semantic segmentation, and image retrieval. However, some issues have recently come to the fore that demonstrate some of the failure modes of self-supervised representations, such as performance on non-ImageNet-like data, or complex scenes. In this paper, we show that self-supervised representations based on the instance discrimination objective lead to better representations of objects that are more robust to changes in the viewpoint and perspective of the object. We perform experiments of modern self-supervised methods against multiple supervised baselines to demonstrate this, including approximating object viewpoint variation through homographies, and real-world tests based on several multi-view datasets. We find that self-supervised representations are more robust to object viewpoint and appear to encode more pertinent information about objects that facilitate the recognition of objects from novel views.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002276",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Cognitive neuroscience of visual object recognition",
      "Computer science",
      "ENCODE",
      "Gene",
      "Machine learning",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Robustness (evolution)",
      "Segmentation",
      "Supervised learning"
    ],
    "authors": [
      {
        "surname": "Torpey",
        "given_name": "David"
      },
      {
        "surname": "Klein",
        "given_name": "Richard"
      }
    ]
  },
  {
    "title": "Cycle-reconstructive subspace learning with class discriminability for unsupervised domain adaptation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108700",
    "abstract": "Unsupervised domain adaptation is used to effectively learn a classifier for data of the unlabeled target domain by utilizing the data of the source domain with sufficient labels but different distributions. In general, a transformation matrix is employed to acquire a common subspace where the distributions of the two domains are aligned, which is easy to lose lots of unique information of the two domains. To better preserve useful information during the transformation process, we propose a novel Cycle-Reconstructive Subspace Learning with Class Discriminability (CRSL) approach that uses two reconstructive matrixes through an iterative strategy to cycle-reconstruct data matrixes and update the common subspace. In this way, we learn the invariant features in the common subspace while better preserving global and local structures of the two original domains. Finally, we implement additional discriminative constraints such as intra-class aggregation and inter-class diffusion on the transformed features to ensure the class discriminability of data of the two domains. Extensive experiment results show that our conventional method outperforms state-of-the-art conventional methods and is comparable with advanced deep methods on four current domain adaptation datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001819",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Discriminative model",
      "Domain adaptation",
      "Gene",
      "Pattern recognition (psychology)",
      "Subspace topology",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Yayun"
      },
      {
        "surname": "Yan",
        "given_name": "Hua"
      }
    ]
  },
  {
    "title": "Unsupervised discriminative feature learning via finding a clustering-friendly embedding space",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108768",
    "abstract": "In this paper, we propose an enhanced deep clustering network (EDCN), which is composed of a Feature Extractor, a Conditional Generator, a Discriminator and a Siamese Network. Specifically, we will utilize two kinds of generated data based on adversarial training, as well as the original data, to train the Feature Extractor for learning effective latent representations. In addition, we adopt the Siamese network to find an embedding space, where a better affinity similarity matrix is obtained as the key to success of spectral clustering in providing reliable pseudo-labels. Particularly, the obtained pseudo-labels will be used to generate realistic data by the Generator. Finally, the discriminator is used to model the real joint distribution of data and corresponding latent representations for Feature Extractor enhancement. To evaluate our proposed EDCN, we conduct extensive experiments on multiple data sets including MNIST, USPS, FRGC, CIFAR-10, STL-10, and Fashion-MNIST by comparing our method with a number of state-of-the-art deep clustering methods, and experimental results demonstrate its effectiveness and superiority.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002497",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Deep learning",
      "Detector",
      "Discriminative model",
      "Discriminator",
      "Embedding",
      "Engineering",
      "Extractor",
      "Feature (linguistics)",
      "Feature learning",
      "Feature vector",
      "Generator (circuit theory)",
      "Image (mathematics)",
      "Linguistics",
      "MNIST database",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Power (physics)",
      "Process engineering",
      "Quantum mechanics",
      "Similarity (geometry)",
      "Spectral clustering",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Cao",
        "given_name": "Wenming"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhongfan"
      },
      {
        "surname": "Liu",
        "given_name": "Cheng"
      },
      {
        "surname": "Li",
        "given_name": "Rui"
      },
      {
        "surname": "Jiao",
        "given_name": "Qianfen"
      },
      {
        "surname": "Yu",
        "given_name": "Zhiwen"
      },
      {
        "surname": "Wong",
        "given_name": "Hau-San"
      }
    ]
  },
  {
    "title": "A sparse graph wavelet convolution neural network for video-based person re-identification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108708",
    "abstract": "Video-based person re-identification (Re-ID) aims to match identical person sequences captured across non-overlapping surveillance areas. It is an essential yet challenging task to effectively embed spatial and temporal information into the video feature representation. For one thing, we observe that different frames in the video can provide complementary information for each other. Also, local features which is lost due to target occlusion or visual ambiguity in one frame can be supplemented by the same pedestrian part in other frames. For another thing, graph neural network enables the contextual interactions between relevant regional features. Therefore, we propose a novel sparse graph wavelet convolution neural network (SGWCNN) for video-based person Re-ID. Distinct from previous graph-based Re-ID methods, we exploit the weighted sparse graph to model the semantic relation among the local patches of pedestrians in the video. Each local patch in one frame can extract supplementary information from highly related patches in other frames. Moreover, to effectively solve the problems of short time occlusion and pedestrian misalignment, the graph wavelet convolution neural network is adopted for feature propagation to refine regional features iteratively. Experiments and evaluation on three challenging benchmarks, that is, MARS, DukeMTMC-VideoReID, and iLIDS-VID, show that the proposed SGWCNN effectively improves the performance of video-based person re-identification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001893",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Frame (networking)",
      "Graph",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Telecommunications",
      "Theoretical computer science",
      "Wavelet"
    ],
    "authors": [
      {
        "surname": "Yao",
        "given_name": "Yingmao"
      },
      {
        "surname": "Jiang",
        "given_name": "Xiaoyan"
      },
      {
        "surname": "Fujita",
        "given_name": "Hamido"
      },
      {
        "surname": "Fang",
        "given_name": "Zhijun"
      }
    ]
  },
  {
    "title": "Dual relation network for temporal action localization",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108725",
    "abstract": "Temporal action localization is a challenging task for video understanding. Most previous methods process each proposal independently and neglect the reasoning of proposal-proposal and proposal-context relations. We argue that the supplementary information obtained by exploiting these relations can enhance the proposal representation and further boost the action localization. To this end, we propose a dual relation network to model both proposal-proposal and proposal-context relations. Concretely, a proposal-proposal relation module is leveraged to learn discriminative supplementary information from relevant proposals, which allows the network to model their interaction based on appearance and geometric similarities. Meanwhile, a proposal-context relation module is employed to mine contextual clues by adaptively learning from the global context outside of region-based proposals. They effectively leverage the inherent correlation between actions and the long-term dependency with videos for high-quality proposal refinement. As a result, the proposed framework enables the model to distinguish similar action instances and locate temporal boundaries more precisely. Extensive experiments on the THUMOS14 dataset and ActivityNet v1.3 dataset demonstrate that the proposed method significantly outperforms recent state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002060",
    "keywords": [
      "Action (physics)",
      "Art",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Dependency (UML)",
      "Discriminative model",
      "Dual (grammatical number)",
      "Economics",
      "Law",
      "Leverage (statistics)",
      "Literature",
      "Machine learning",
      "Management",
      "Operating system",
      "Paleontology",
      "Physics",
      "Political science",
      "Politics",
      "Process (computing)",
      "Quantum mechanics",
      "Relation (database)",
      "Representation (politics)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Xia",
        "given_name": "Kun"
      },
      {
        "surname": "Wang",
        "given_name": "Le"
      },
      {
        "surname": "Zhou",
        "given_name": "Sanping"
      },
      {
        "surname": "Hua",
        "given_name": "Gang"
      },
      {
        "surname": "Tang",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "MPCCL: Multiview predictive coding with contrastive learning for person re-identification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108710",
    "abstract": "In this paper, we investigate a new representation learning approach, termed as Multiview Predictive Coding with Contrastive Learning (MPCCL), for person re-identification (re-ID). Different from the conventional re-ID approaches that focus on learning representations from semantic label, our approach learns the identification of invariant information via representation reconstruction, which explores more fine-grained semantic information in representation space. Specifically, given a chosen identity, the learned representation of its single view can be reconstructed by those of other views. Therefore, kernel density estimation (KDE) is firstly introduced for the adaptive reconstruction of the representation. Then, contrastive learning is adopted to increase the distance between the representations of the same views with different identities. Finally, representation reconstruction and contrastive learning jointly supervise the representation learning process, thus obtaining fine-grained semantic information and appearance-free representations. Extensive experiments on several re-ID datasets demonstrate that the proposed approach yields state-of-the-art results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001911",
    "keywords": [
      "Artificial intelligence",
      "Coding (social sciences)",
      "Computer science",
      "Feature learning",
      "Focus (optics)",
      "Law",
      "Machine learning",
      "Mathematics",
      "Natural language processing",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Predictive coding",
      "Representation (politics)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Yin",
        "given_name": "Junhui"
      },
      {
        "surname": "Xie",
        "given_name": "Jiyang"
      },
      {
        "surname": "Ma",
        "given_name": "Zhanyu"
      },
      {
        "surname": "Guo",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Representation learning with deep sparse auto-encoder for multi-task learning",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108742",
    "abstract": "We demonstrate an effective framework to achieve a better performance based on Deep Sparse auto-encoder for Multi-task Learning, called DSML for short. To learn the reconstructed and higher-level features on cross-domain instances for multiple tasks, we combine the labeled and unlabeled data from all tasks to reconstruct the feature representations. Furthermore, we propose the model of Stacked Reconstruction Independence Component Analysis (SRICA for short) for the optimization of feature representations with a large amount of unlabeled data, which can effectively address the redundancy of image data. Our proposed SRICA model is developed from RICA and is based on deep sparse auto-encoder. In addition, we adopt a Semi-Supervised Learning method (SSL for short) based on model parameter regularization to build a unified model for multi-task learning. There are several advantages in our proposed framework as follows: 1) The proposed SRICA makes full use of a large amount of unlabeled data from all tasks. It is used to pursue an optimal sparsity feature representation, which can overcome the over-fitting problem effectively. 2) The deep architecture used in our SRICA model is applied for higher-level and better representation learning, which is designed to train on patches for sphering the input data. 3) Training parameters in our proposed framework has lower computational cost compared to other common deep learning methods such as stacked denoising auto-encoders. Extensive experiments on several real image datasets demonstrate our proposed framework outperforms the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002230",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Computer science",
      "Deep learning",
      "Economics",
      "Encoder",
      "Feature (linguistics)",
      "Feature learning",
      "Law",
      "Linguistics",
      "Machine learning",
      "Management",
      "Multi-task learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Redundancy (engineering)",
      "Regularization (linguistics)",
      "Representation (politics)",
      "Semi-supervised learning",
      "Sparse approximation",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Yi"
      },
      {
        "surname": "Wu",
        "given_name": "Xindong"
      },
      {
        "surname": "Qiang",
        "given_name": "Jipeng"
      },
      {
        "surname": "Hu",
        "given_name": "Xuegang"
      },
      {
        "surname": "Zhang",
        "given_name": "Yuhong"
      },
      {
        "surname": "Li",
        "given_name": "Peipei"
      }
    ]
  },
  {
    "title": "Fovea localization by blood vessel vector in abnormal fundus images",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108711",
    "abstract": "In human eyes, macula is responsible for sharp central vision with fovea in the center. The location of fovea becomes an important landmark in diagnosing the retinal diseases. As macula doesn’t have the clear boundary and obvious shape, deep learning techniques to locate the fovea often fail in complicated lesions and insufficient training samples, and the unsupervised method is incapable for illumination variations. In this paper, a new unsupervised fovea localization method using the retinal raphe and region searching is proposed, and the blood vessel vector (BVV) model is developed. After detecting blood vessels and OD by U-net and probability bubbles, the BVVs are conceived and the retinal raphe is obtained by summating all the BVVs, then the fovea is estimated through the local region searching. Compared with the parabola model, the BVV model does not involve the coordinate transformation and reduces the complexity to the linear time cost O ( N ) . Two other unsupervised techniques the parabola model and intensity searching and five supervised techniques cGAN, U-net, DRNet, MedTnet and EANet are included and compared. The global feature of retinal vessels is utilized which makes the proposed method more robust to the lesions than the other localization methods. The experiments on public datasets Kaggle, MESSIDOR and IDRiD validate the effectiveness of the proposed method by the student’s t-test, and our method obtains the least average Euclidean distance to the groundtruth on Kaggle and almost least on Base 33 of MESSIDOR.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001923",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Euclidean distance",
      "Fundus (uterus)",
      "Medicine",
      "Ophthalmology",
      "Pattern recognition (psychology)",
      "Retinal"
    ],
    "authors": [
      {
        "surname": "Fu",
        "given_name": "Yinghua"
      },
      {
        "surname": "Zhang",
        "given_name": "Ge"
      },
      {
        "surname": "Li",
        "given_name": "Jiang"
      },
      {
        "surname": "Pan",
        "given_name": "Dongyan"
      },
      {
        "surname": "Wang",
        "given_name": "Yongxiong"
      },
      {
        "surname": "Zhang",
        "given_name": "Dawei"
      }
    ]
  },
  {
    "title": "EEG-ConvTransformer for single-trial EEG-based visual stimulus classification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108757",
    "abstract": "Different categories of visual stimuli evoke distinct activation patterns in the human brain. These patterns can be captured with EEG for utilization in application such as Brain-Computer Interface (BCI). However, accurate classification of these patterns acquired using single-trial data is challenging due to the low signal-to-noise ratio of EEG. Recently, deep learning-based transformer models with multi-head self-attention have shown great potential for analyzing variety of data. This work introduces an EEG-ConvTranformer network that is based on both multi-headed self-attention and temporal convolution. The novel architecture incorporates self-attention modules to capture inter-region interaction patterns and convolutional filters to learn temporal patterns in a single module. Experimental results demonstrate that EEG-ConvTransformer achieves improved classification accuracy over state-of-the-art techniques across five different visual stimulus classification tasks. Finally, quantitative analysis of inter-head diversity also shows low similarity in representational space, emphasizing the implicit diversity of multi-head attention.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002382",
    "keywords": [
      "Artificial intelligence",
      "Brain–computer interface",
      "Cognitive psychology",
      "Computer science",
      "Electroencephalography",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Psychology",
      "Speech recognition",
      "Stimulus (psychology)"
    ],
    "authors": [
      {
        "surname": "Bagchi",
        "given_name": "Subhranil"
      },
      {
        "surname": "Bathula",
        "given_name": "Deepti R."
      }
    ]
  },
  {
    "title": "Transductive polyhedral conic classifiers for machine learning applications",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.07.001",
    "abstract": "In this paper, we introduce novel methods called Transductive Polyhedral Conic Classifiers that use both labeled and unlabeled data for classification. The proposed methodology utilizes the concave-convex procedure to solve the resulting optimization problems as in the Robust Transductive Support Vector Machines (RTSVMs). However, unlike RTSVM that uses SVM formulation, our proposed methods use the polyhedral conic classifier formulation that returns tight and closed decision boundaries compared to SVM. We tested the proposed methods on various datasets and experimental results show that our proposed methods yield better results than the existing transductive learning classifiers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002124",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Conic section",
      "Geometry",
      "Labeled data",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Regular polygon",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Cevikalp",
        "given_name": "Hakan"
      },
      {
        "surname": "Saglamlar",
        "given_name": "Halil"
      }
    ]
  },
  {
    "title": "Entropy guided attention network for weakly-supervised action localization",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108718",
    "abstract": "One major challenge of Weakly-supervised Temporal Action Localization (WTAL) is to handle diverse backgrounds in videos. To model background frames, most existing methods treat them as an additional action class. However, because background frames usually do not share common semantics, squeezing all the different background frames into a single class hinders network optimization. Moreover, the network would be confused and tends to fail when tested on videos with unseen background frames. To address this problem, we propose an Entropy Guided Attention Network (EGA-Net) to treat background frames as out-of-domain samples. Specifically, we design a two-branch module, where a domain branch detects whether a frame is an action by learning a class-agnostic attention map, and an action branch recognizes the action category of the frame by learning a class-specific attention map. By aggregating the two attention maps to model the joint domain-class distribution of frames, our EGA-Net can handle varying backgrounds. To train the class-agnostic attention map with only the video-level class labels, we propose an Entropy Guided Loss (EGL), which employs entropy as the supervision signal to distinguish action and background. Moreover, we propose a Global Similarity Loss (GSL) to enhance the action-specific attention map via action class center. Extensive experiments on THUMOS14, ActivityNet1.2 and ActivityNet1.3 datasets demonstrate the effectiveness of our EGA-Net.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001996",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Entropy (arrow of time)",
      "Frame (networking)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Principle of maximum entropy",
      "Quantum mechanics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Cheng",
        "given_name": "Yi"
      },
      {
        "surname": "Sun",
        "given_name": "Ying"
      },
      {
        "surname": "Fan",
        "given_name": "Hehe"
      },
      {
        "surname": "Zhuo",
        "given_name": "Tao"
      },
      {
        "surname": "Lim",
        "given_name": "Joo-Hwee"
      },
      {
        "surname": "Kankanhalli",
        "given_name": "Mohan"
      }
    ]
  },
  {
    "title": "MetaAP: A meta-tree-based ranking algorithm optimizing the average precision from imbalanced data",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.07.019",
    "abstract": "In this paper, we address the challenging problem of learning to rank from highly imbalanced data. This scenario requires to resort to specific metrics able to account the scarcity of the so-called positive examples. We present MetaAP, a tree-based ranking algorithm, which induces meta-trees by optimizing directly during the learning process the Average Precision (AP). This latter has been shown to be more relevant than the area under the ROC curve (AUC–ROC) when the objective is to push the examples of interest at the very top of the list. This effect of the AP in tree-based ranking is particularly wished to address fraud detection tasks where (i) the budget is often constrained (in terms of possible controls) and (ii) the interpretability of the induced models is required to support decision making. After an extensive comparative study on 28 public datasets showing that MetaAP is significantly better than other tree-based ranking methods, we tackle a tax fraud detection task coming from a partnership with the French Ministry of Economy and Finance. The results show that MetaAP is able to make the tax audit process much more efficient.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002379",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Audit",
      "CHAID",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Decision tree",
      "Economics",
      "Interpretability",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Process (computing)",
      "Rank (graph theory)",
      "Ranking (information retrieval)",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Viola",
        "given_name": "Rémi"
      },
      {
        "surname": "Gautheron",
        "given_name": "Léo"
      },
      {
        "surname": "Habrard",
        "given_name": "Amaury"
      },
      {
        "surname": "Sebban",
        "given_name": "Marc"
      }
    ]
  },
  {
    "title": "Corrigendum to “Keyword weight optimization using gradient strategies in event focused web crawling” Pattern recognition letters volume 142 (2021) pages 3-10",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.07.002",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002136",
    "keywords": [
      "Anatomy",
      "Artificial intelligence",
      "Astrophysics",
      "Biology",
      "Computer science",
      "Crawling",
      "Event (particle physics)",
      "Information retrieval",
      "Physics",
      "Quantum mechanics",
      "Volume (thermodynamics)"
    ],
    "authors": [
      {
        "surname": "Rajiv",
        "given_name": "S"
      },
      {
        "surname": "Navaneethan",
        "given_name": "C"
      }
    ]
  },
  {
    "title": "A bi-level formulation for multiple kernel learning via self-paced training",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108770",
    "abstract": "Multiple kernel learning (MKL) is a crucial issue which has been widely researched over the last two decades. Although existing MKL algorithms have achieved satisfactory performance in a broad range of applications, these methods do not adequately consider the adverse effects of unreliable or less reliable instances. To handle this shortcoming, we formulate multiple kernel learning in a bi-level learning paradigm consisting of the kernel combination weight learning (KWL) stage and the self-paced learning (SPL) stage, which alternatively negotiate with each other. The KWL stage dynamically absorbs reliable instances into model learning to accurately capture neighborhood relationships and obtains kernel coefficients via maximizing both global and local kernel alignment in a common schema. The SPL stage automatically evaluates the reliability of training samples via self-paced training. The extensive experiments indicate the robustness and superiority of the presented approach in comparison with existing MKL methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002515",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Kernel (algebra)",
      "Kernel method",
      "Machine learning",
      "Mathematics",
      "Meteorology",
      "Multiple kernel learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Support vector machine",
      "Training (meteorology)"
    ],
    "authors": [
      {
        "surname": "Alavi",
        "given_name": "Fatemeh"
      },
      {
        "surname": "Hashemi",
        "given_name": "Sattar"
      }
    ]
  },
  {
    "title": "Graph regularization multidimensional projection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108690",
    "abstract": "This paper introduces a novel multidimensional projection method of datasets. Our method called Graph Regularization Multidimensional Projection (GRMP) is based on a technique from the graph signal processing theory, the graph regularization. Initially, a similarity graph is built on the high-dimensional space where the dataset lies. A two-dimensional distribution of points is then created in the visual space using a phyllotactic distribution. The similarity graph is copied properly over the phyllotactic distribution and the graph regularization is applied to their coordinates, which are interpreted as graph signals. The graph regularization reorganizes the phyllotactic distribution by bringing together points that represent similar data in the high-dimensional space. We employ synthetic and real datasets to demonstrate the effectiveness of our method. Furthermore, since the solution of the graph regularization can still be approximated using a fast approximation mechanism based on the Chebyshev polynomials, our method is computationally efficient even for large graphs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001716",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Graph",
      "Graph bandwidth",
      "Lattice graph",
      "Line graph",
      "Mathematics",
      "Regularization (linguistics)",
      "Theoretical computer science",
      "Voltage graph"
    ],
    "authors": [
      {
        "surname": "Dal Col",
        "given_name": "Alcebiades"
      },
      {
        "surname": "Petronetto",
        "given_name": "Fabiano"
      }
    ]
  },
  {
    "title": "Cross-View kernel transfer",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108759",
    "abstract": "We consider the kernel completion problem with the presence of multiple views in the data. In this context the data samples can be fully missing in some views, creating missing columns and rows to the kernel matrices that are calculated individually for each view. We propose to solve the problem of completing the kernel matrices with Cross-View Kernel Transfer (CVKT) procedure, in which the features of the other views are transformed to represent the view under consideration. The transformations are learned with kernel alignment to the known part of the kernel matrix, allowing for finding generalizable structures in the kernel matrix under completion. Its missing values can then be predicted with the data available in other views. We illustrate the benefits of our approach with simulated data, multivariate digits dataset and multi-view dataset on gesture classification, as well as with real biological datasets from studies of pattern formation in early Drosophila melanogaster embryogenesis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002400",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Combinatorics",
      "Computer science",
      "Context (archaeology)",
      "Graph kernel",
      "Kernel (algebra)",
      "Kernel embedding of distributions",
      "Kernel method",
      "Kernel principal component analysis",
      "Machine learning",
      "Mathematics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Polynomial kernel",
      "Radial basis function kernel",
      "String kernel",
      "Support vector machine",
      "Tree kernel"
    ],
    "authors": [
      {
        "surname": "Huusari",
        "given_name": "Riikka"
      },
      {
        "surname": "Capponi",
        "given_name": "Cécile"
      },
      {
        "surname": "Villoutreix",
        "given_name": "Paul"
      },
      {
        "surname": "Kadri",
        "given_name": "Hachem"
      }
    ]
  },
  {
    "title": "Incremental multi-target domain adaptation for object detection with efficient domain transfer",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108771",
    "abstract": "Recent advances in unsupervised domain adaptation have significantly improved the recognition accuracy of CNNs by alleviating the domain shift between (labeled) source and (unlabeled) target data distributions. While the problem of single-target domain adaptation (STDA) for object detection has recently received much attention, multi-target domain adaptation (MTDA) remains largely unexplored, despite its practical relevance in several real-world applications, such as multi-camera video surveillance. Compared to the STDA problem that may involve large domain shifts between complex source and target distributions, MTDA faces additional challenges, most notably the computational requirements and catastrophic forgetting of previously-learned targets, which can depend on the order of target adaptations. STDA for detection can be applied to MTDA by adapting one model per target, or one common model with a mixture of data from target domains. However, these approaches are either costly or inaccurate. The only state-of-art MTDA method specialized for detection learns targets incrementally, one target at a time, and mitigates the loss of knowledge by using a duplicated detection model for knowledge distillation, which is computationally expensive and does not scale well to many domains. In this paper, we introduce an efficient approach for incremental learning that generalizes well to multiple target domains. Our MTDA approach is more suitable for real-world applications since it allows updating the detection model incrementally, without storing data from previous-learned target domains, nor retraining when a new target domain becomes available. Our approach leverages domain discriminators to train a novel Domain Transfer Module (DTM), which only incurs a modest overhead. The DTM transforms source images according to diverse target domains, allowing the model to access a joint representation of previously-learned target domains, and to effectively limit catastrophic forgetting. Our proposed method – called MTDA with DTM (MTDA-DTM) – is compared against state-of-the-art approaches on several MTDA detection benchmarks and Wildtrack, a benchmark for multi-camera pedestrian detection. Results indicate that MTDA-DTM achieves the highest level of detection accuracy across multiple target domains, yet requires significantly fewer computational resources. Our code is available. 1 1 https://github.com/Natlem/M-HTCN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002527",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Law",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Relevance (law)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Nguyen-Meidine",
        "given_name": "Le Thanh"
      },
      {
        "surname": "Kiran",
        "given_name": "Madhu"
      },
      {
        "surname": "Pedersoli",
        "given_name": "Marco"
      },
      {
        "surname": "Dolz",
        "given_name": "Jose"
      },
      {
        "surname": "Blais-Morin",
        "given_name": "Louis-Antoine"
      },
      {
        "surname": "Granger",
        "given_name": "Eric"
      }
    ]
  },
  {
    "title": "Example-based color transfer with Gaussian mixture modeling",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108716",
    "abstract": "Color transfer, which plays a key role in image editing, has attracted noticeable attention recently. It has remained a challenge to date due to various issues such as time-consuming manual adjustments and prior segmentation issues. In this paper, we propose to model color transfer under a probability framework and cast it as a parameter estimation problem. In particular, we relate the transferred image with the example image under the Gaussian Mixture Model (GMM) and regard the transferred image color as the GMM centroids. We employ the Expectation-Maximization (EM) algorithm (E-step and M-step) for optimization. To better preserve gradient information, we introduce a Laplacian based regularization term to the objective function at the M-step which is solved by deriving a gradient descent algorithm. Given the input of a source image and an example image, our method is able to generate multiple color transfer results with increasing EM iterations. Extensive experiments show that our approach generally outperforms other competitive color transfer methods, both visually and quantitatively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001972",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Centroid",
      "Color image",
      "Computer science",
      "Computer vision",
      "Gaussian",
      "Gradient descent",
      "Image (mathematics)",
      "Image gradient",
      "Image processing",
      "Image segmentation",
      "Laplace operator",
      "Mathematical analysis",
      "Mathematics",
      "Mixture model",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "Gu",
        "given_name": "Chunzhi"
      },
      {
        "surname": "Lu",
        "given_name": "Xuequan"
      },
      {
        "surname": "Zhang",
        "given_name": "Chao"
      }
    ]
  },
  {
    "title": "CSF: Closed-mask-guided semantic fusion method for semantic perception of unknown scenes",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.07.020",
    "abstract": "Though many high-precision semantic segmentation models have been proposed, how to improve the generalization ability of these models is still an urgent problem. Recently, a great number of unsupervised domain adaptation (UDA) algorithms around domain adaptation problems have been studied in semantic segmentation. These methods require labeled source domain data and unlabeled target domain data. In this paper, we propose a closed-mask-guided semantic fusion method (CSF) to improve the semantic segmentation results of unknown scenes, where the target domain data is not obtained in advance. First, a Closed Mask Generation (CMG) module is designed to convert the edge detection result into a mask that can segment the image into several image blocks. Then, a Semantic Confidence Fusion (SCF) module based on information entropy and voting method is introduced, which can select reliable semantic segmentation results for each image block by comparing the confidence of several semantic segmentation networks. In addition, the experimental results on both KITTI and COCO Stuff datasets validate the effectiveness of this method. The code is publicly available at https://github.com/tryhere/CSF.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002380",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Code (set theory)",
      "Computer science",
      "Domain (mathematical analysis)",
      "Geometry",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Segmentation",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Minghong"
      },
      {
        "surname": "Shu",
        "given_name": "Ruijun"
      },
      {
        "surname": "Zhu",
        "given_name": "Dongchen"
      },
      {
        "surname": "Li",
        "given_name": "Jiamao"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaolin"
      }
    ]
  },
  {
    "title": "High-resolution rectified gradient-based visual explanations for weakly supervised segmentation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108724",
    "abstract": "Visual explanations for convolutional neural networks (CNNs) act as the backbone for weakly supervised segmentation with image-level labels. This paper proposes a high-resolution rectified gradient-based class activation mapping with bounding box annotations (bbox) to improve the initial seed for weakly supervised segmentation (WSS) tasks. HRCAM extends Grad-CAM by separating the gradient maps from the class activation maps from the shallow layer for higher resolution. Gradient rectified methods are proposed to improve the visualization and WSS score. Experiments and evaluations are conducted to verify the performance of HRCAM-BB on Pascal VOC 2012 and COCO datasets. On Pascal VOC 2012 set, our method achieves outstanding performance with a mean intersection over union (mIOU) of 71.6 with image-level labels and 78.2 with bbox on WSSS, and increases the WSIS mIOU (AP 50 ) to 52.1 with image-level labels, and 61.9 with bbox. our method surpasses the previous SOTA approach in the same condition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002059",
    "keywords": [
      "Artificial intelligence",
      "Bounding overwatch",
      "Cartography",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Geography",
      "Image (mathematics)",
      "Intersection (aeronautics)",
      "Minimum bounding box",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Segmentation",
      "Set (abstract data type)",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Tianyou"
      },
      {
        "surname": "Wang",
        "given_name": "Qiang"
      },
      {
        "surname": "Shen",
        "given_name": "Yue"
      },
      {
        "surname": "Ma",
        "given_name": "Xiang"
      },
      {
        "surname": "Lin",
        "given_name": "Xiaotian"
      }
    ]
  },
  {
    "title": "Affective word embedding in affective explanation generation for fine art paintings",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.07.009",
    "abstract": "Fine art paintings take an important place in human history and are one of the fundamental components of human culture. Even though deep learning in fine art paintings attracts increasing attention from researchers, few works focus on understanding the interplay between the visual content, its triggered affect, and language aiming to explain that affect. Most visual captioning models can only deal with tasks of generating captions describing objective affairs but lack the capabilities of generating affective explanations. In this paper, we introduce the use of the VAD (Valence, Arousal, and Dominance) dictionary in our model and propose a gated concatenation mechanism to construct word affective embedding. Corporating with the use of the affective loss function, our model outperforms the state-of-the-art in automatic evaluation metrics and subjective evaluations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002227",
    "keywords": [
      "Affect (linguistics)",
      "Arithmetic",
      "Arousal",
      "Art",
      "Artificial intelligence",
      "Closed captioning",
      "Cognitive psychology",
      "Cognitive science",
      "Communication",
      "Computer science",
      "Concatenation (mathematics)",
      "Embedding",
      "Focus (optics)",
      "Image (mathematics)",
      "Mathematics",
      "Natural language processing",
      "Optics",
      "Painting",
      "Physics",
      "Psychology",
      "Quantum mechanics",
      "Rendering (computer graphics)",
      "Social psychology",
      "Valence (chemistry)",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Jianhao"
      },
      {
        "surname": "Wang",
        "given_name": "Wenmin"
      },
      {
        "surname": "Yu",
        "given_name": "Cheng"
      }
    ]
  },
  {
    "title": "Comparison of color imaging vs. hyperspectral imaging for texture classification",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.08.001",
    "abstract": "Many approaches of texture analysis by color or hyperspectral imaging are based on the assumption that the image of a texture can be viewed as a multi-component image, where spatial interactions within and between components are jointly considered (opponent component approach) or not (marginal approach). When color images are coded in multiple color spaces, texture descriptors are based on Multi Color Channel (MCC) representations. By extension, a Multi Spectral Band (MSB) representation can be used to characterize the texture of material surfaces in hyperspectral images. MSB and MCC representations are compared in this paper for texture classification issues. The contribution of each representation is investigated with marginal and/or opponent component strategies. For this purpose, several relevant texture descriptors are considered. Since MSB and MCC representations generate high-dimensional feature spaces, a dimensionality reduction is applied to avoid the curse of dimensionality. Experimental results carried out on three hyperspectral texture databases (HyTexiLa, SpecTex and an original dataset extracted from the Timbers database) show that considering between component interactions in addition to the within ones significantly improves the classification accuracies. The proposed approaches allow also to outperform state of the art hand-designed descriptors and color texture descriptors based on deep learning networks. This study highlights the contribution of hyperspectral imaging compared to color imaging for texture classification purposes but also the advantages of color imaging depending on the considered texture representation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002392",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Curse of dimensionality",
      "Dimensionality reduction",
      "Feature (linguistics)",
      "Hyperspectral imaging",
      "Image (mathematics)",
      "Image processing",
      "Image texture",
      "Law",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Principal component analysis",
      "Representation (politics)",
      "Texture (cosmology)"
    ],
    "authors": [
      {
        "surname": "Porebski",
        "given_name": "Alice"
      },
      {
        "surname": "Alimoussa",
        "given_name": "Mohamed"
      },
      {
        "surname": "Vandenbroucke",
        "given_name": "Nicolas"
      }
    ]
  },
  {
    "title": "Harmonic convolutional networks based on discrete cosine transform",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108707",
    "abstract": "Convolutional neural networks (CNNs) learn filters in order to capture local correlation patterns in feature space. We propose to learn these filters as combinations of preset spectral filters defined by the Discrete Cosine Transform (DCT). Our proposed DCT-based harmonic blocks replace conventional convolutional layers to produce partially or fully harmonic versions of new or existing CNN architectures. Using DCT energy compaction properties, we demonstrate how the harmonic networks can be efficiently compressed by truncating high-frequency information in harmonic blocks thanks to the redundancies in the spectral domain. We report extensive experimental validation demonstrating benefits of the introduction of harmonic blocks into state-of-the-art CNN models in image classification, object detection and semantic segmentation applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001881",
    "keywords": [
      "Acoustics",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Discrete cosine transform",
      "Feature (linguistics)",
      "Frequency domain",
      "Harmonic",
      "Image (mathematics)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Transform coding"
    ],
    "authors": [
      {
        "surname": "Ulicny",
        "given_name": "Matej"
      },
      {
        "surname": "Krylov",
        "given_name": "Vladimir A."
      },
      {
        "surname": "Dahyot",
        "given_name": "Rozenn"
      }
    ]
  },
  {
    "title": "The devil in the tail: Cluster consolidation plus cluster adaptive balancing loss for unsupervised person re-identification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108763",
    "abstract": "Unsupervised person re-identification (Re-ID) is to retrieve pedestrians from different camera views without supervision information. State-of-the-art methods are usually built upon training a convolution neural network with pseudo labels generated by clustering. Unfortunately, the pseudo labels are highly unbalanced and heavily noisy, carrying ineffective or even erroneous supervision information. To address these deficiencies, we present an effective clustering and reorganization approach, called Cluster Consolidation, which aims to separate a small proportion of unreliable data points from each cluster. This approach benefits to improve the quality of the pseudo labels, but also yields more tiny clusters. Thus, we further propose a Cluster Adaptive Balancing (CAB) loss to effectively train the network with the imbalance pseudo labels, where our CAB loss is able to automatically balance the importance of each cluster. We conduct extensive experiments on widely used person Re-ID benchmark datasets and demonstrate the effectiveness of our proposals.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002448",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Botany",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Geodesy",
      "Geography",
      "Identification (biology)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Mingkun"
      },
      {
        "surname": "Sun",
        "given_name": "He"
      },
      {
        "surname": "Lin",
        "given_name": "Chaoqun"
      },
      {
        "surname": "Li",
        "given_name": "Chun-Guang"
      },
      {
        "surname": "Guo",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Multi-level graph learning network for hyperspectral image classification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108705",
    "abstract": "Graph Convolutional Network (GCN) has emerged as a new technique for hyperspectral image (HSI) classification. However, in current GCN-based methods, the graphs are usually constructed with manual effort and thus is separate from the classification task, which could limit the representation power of GCN. Moreover, the employed graphs often fail to encode the global contextual information in HSI. Hence, we propose a Multi-level Graph Learning Network (MGLN) for HSI classification, where the graph structural information at both local and global levels can be learned in an end-to-end fashion. First, MGLN employs attention mechanism to adaptively characterize the spatial relevance among image regions. Then localized feature representations can be produced and further used to encode the global contextual information. Finally, prediction can be acquired with the help of both local and global contextual information. Experiments on three real-world hyperspectral datasets reveal the superiority of our MGLN when compared with the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001868",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Graph",
      "Hyperspectral imaging",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wan",
        "given_name": "Sheng"
      },
      {
        "surname": "Pan",
        "given_name": "Shirui"
      },
      {
        "surname": "Zhong",
        "given_name": "Shengwei"
      },
      {
        "surname": "Yang",
        "given_name": "Jie"
      },
      {
        "surname": "Yang",
        "given_name": "Jian"
      },
      {
        "surname": "Zhan",
        "given_name": "Yibing"
      },
      {
        "surname": "Gong",
        "given_name": "Chen"
      }
    ]
  },
  {
    "title": "HFA-Net: High frequency attention siamese network for building change detection in VHR remote sensing images",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108717",
    "abstract": "Building change detection (BCD) recently can be handled well under the booming of deep-learning based computer vision techniques. However, segmentation and recognition for objects with sharper boundaries still suffer from the poorly acquired high frequency information, which can result in the deteriorated annotation of building boundaries in BCD. To better obtain the high frequency pattern under the deep learning pipeline, we propose a high frequency attention-guided Siamese network (HFA-Net) in which a novel built-in high frequency attention block (HFAB) is applied. HFA-Net is designed to enhance high frequency information of buildings via HFAB which is composed of two main stages, i.e., the spatial-wise attention (SA) and the high frequency enhancement (HF). The SA firstly guides the model to search and focus on buildings, and HF is employed afterwards to highlight the high frequency information of the input feature maps. With high frequency information of buildings enhanced by HFAB, HFA-Net is able to better detect the edges of changed buildings, so as to improve the performance of BCD. Our proposed method is evaluated on three widely-used public datasets, i.e., WHU-CD, LEVIR-CD, and Google dataset. Remarkable experimental results on these datasets indicate that our proposed method can better detect edges of changed buildings and shows a better performance. The source code will be released at: https://github.com/HaiXing-1998/HFANet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001984",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Code (set theory)",
      "Computer science",
      "Deep learning",
      "Feature (linguistics)",
      "Focus (optics)",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pipeline (software)",
      "Programming language",
      "Segmentation",
      "Set (abstract data type)",
      "Source code"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Hanhong"
      },
      {
        "surname": "Gong",
        "given_name": "Maoguo"
      },
      {
        "surname": "Liu",
        "given_name": "Tongfei"
      },
      {
        "surname": "Jiang",
        "given_name": "Fenlong"
      },
      {
        "surname": "Zhan",
        "given_name": "Tao"
      },
      {
        "surname": "Lu",
        "given_name": "Di"
      },
      {
        "surname": "Zhang",
        "given_name": "Mingyang"
      }
    ]
  },
  {
    "title": "SELF-LLP: Self-supervised learning from label proportions with self-ensemble",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108767",
    "abstract": "In this paper, we tackle the problem called learning from label proportions (LLP), where the training data is arranged into various bags, with only the proportions of different categories in each bag available. Existing efforts mainly focus on training a model with only the limited proportion information in a weakly supervised manner, thus result in apparent performance gap to supervised learning, as well as computational inefficiency. In this work, we propose a multi-task pipeline called SELF-LLP to make full use of the information contained in the data and model themselves. Specifically, to intensively learn representation from the data, we leverage the self-supervised learning as a plug-in auxiliary task to learn better transferable visual representation. The main insight is to benefit from the self-supervised representation learning with deep model, as well as improving classification performance by a large margin. Meanwhile, in order to better leverage the implicit benefits from the model itself, we incorporate the self-ensemble strategy to guide the training process with an auxiliary supervision information, which is constructed by aggregating multiple previous network predictions. Furthermore, a ramp-up mechanism is further employed to stabilize the training process. In the extensive experiments, our method demonstrates compelling advantages in both accuracy and efficiency over several state-of-the-art LLP approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002485",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Data mining",
      "Economics",
      "Ensemble learning",
      "Inefficiency",
      "Law",
      "Leverage (statistics)",
      "Machine learning",
      "Management",
      "Margin (machine learning)",
      "Microeconomics",
      "Operating system",
      "Pipeline (software)",
      "Political science",
      "Politics",
      "Process (computing)",
      "Programming language",
      "Representation (politics)",
      "Semi-supervised learning",
      "Supervised learning",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Jiabin"
      },
      {
        "surname": "Qi",
        "given_name": "Zhiquan"
      },
      {
        "surname": "Wang",
        "given_name": "Bo"
      },
      {
        "surname": "Tian",
        "given_name": "YingJie"
      },
      {
        "surname": "Shi",
        "given_name": "Yong"
      }
    ]
  },
  {
    "title": "Query-efficient decision-based attack via sampling distribution reshaping",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108728",
    "abstract": "With a limited query budget and only the final decision of a target model, how to find adversarial examples with low-magnitude distortion has attracted great attention among researchers. Recent solutions to this issue made use of the estimated normal vector at a boundary data point to search for adversarial examples. However, since the sampling independence between two sampling epochs, they still suffer from a prohibitively high query budget, which will get worse when the dimensionality of the attacked samples get increased. To push for further development, in this paper, we pay attention to a query-efficient method to estimate the normal vector for decision-based attack in high-dimensional space. Specifically, we propose a simple yet effective normal vector estimation framework for high-dimension decision-based attack via Sampling Distribution Reshaping, dubbed SDR. Next, SDR is incorporated into general geometric attack framework. Briefly, SDR leverages all the historically sampled noise to build a guiding vector, which will be used to reshape the next sampling distribution. Besides, we also extend SDR to different ℓ p norms for p = { 2 , ∞ } and deploy low-frequency constraint to enhance the performance of SDR. Compared to peer decision-based attacks, SDR can reach the competitive ℓ p norms for p = { 2 , ∞ } , according to extensive experimental evaluations against both defended and undefended classifiers. Since the simplicity and effectiveness of SDR, we think that reshaping the sampling distribution deserves further research in future works.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002096",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Curse of dimensionality",
      "Data mining",
      "Decision boundary",
      "Dimension (graph theory)",
      "Filter (signal processing)",
      "Geometry",
      "Image (mathematics)",
      "Independence (probability theory)",
      "Mathematical optimization",
      "Mathematics",
      "Noise (video)",
      "Point (geometry)",
      "Pure mathematics",
      "Sampling (signal processing)",
      "Statistics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Xuxiang"
      },
      {
        "surname": "Cheng",
        "given_name": "Gong"
      },
      {
        "surname": "Pei",
        "given_name": "Lei"
      },
      {
        "surname": "Han",
        "given_name": "Junwei"
      }
    ]
  },
  {
    "title": "Representation learning using deep random vector functional link networks for clustering",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108744",
    "abstract": "Random Vector Functional Link (RVFL) Networks have received a lot of attention due to the fast training speed as the non-iterative solution characteristic. Currently, the main research direction of RVFLs has supervised learning, including semi-supervised and multi-label. There are hardly any unsupervised research results for RVFLs. In this paper, we propose the unsupervised RVFL (usRVFL), and the unsupervised framework is generic that can be used with other RVFL variants, thus we extend it to an ensemble deep variant, unsupervised deep RVFL (usdRVFL). The unsupervised method is based on the manifold regularization while the deep variant is related to the consensus clustering method, which can increase the capability and diversity of RVFLs. Our unsupervised approaches also benefit from fast training speed, even the deep variant offers a very competitive computation efficiency. Empirical experiments on several benchmark datasets demonstrated the effectiveness of the proposed algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002254",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cluster analysis",
      "Computer science",
      "Deep learning",
      "Feature learning",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Minghui"
      },
      {
        "surname": "Suganthan",
        "given_name": "P.N."
      }
    ]
  },
  {
    "title": "A novel forget-update module for few-shot domain generalization",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108704",
    "abstract": "Existing Few-Shot Learning (FSL) methods learn and recognize new classes with the help of prior knowledge. However, they cannot handle this task well in a cross-domain scenario when training and testing sets are from different domains, since the fact that prior knowledge in different domains often varies greatly. To solve this problem, in this paper, we propose a few-shot domain generalization method, which is designed to extract relationship embeddings using Forget-Update Modules named FUM. The relationship embedding considers valuable relational information between samples in a specific task, and the forget-update module takes into account differences between domains and adjusts the distribution of relational embeddings through forgetting and updating mechanisms based on specific tasks. To evaluate the few-shot domain generalization ability of FUM, extensive experiments on eight cross-domain scenarios and six same-domain scenarios are conducted, and the results show that FUM achieves superior performances compared to recent few-shot learning methods. Visualization results also show that the distribution of the relationship embeddings extracted by FUM has stronger few-shot domain generalization ability than the feature embeddings used in the existing FSL methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001856",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Discrete mathematics",
      "Domain (mathematical analysis)",
      "Domain theory",
      "Economics",
      "Embedding",
      "Feature (linguistics)",
      "Generalization",
      "Linguistics",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Shot (pellet)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Yuan",
        "given_name": "Minglei"
      },
      {
        "surname": "Cai",
        "given_name": "Chunhao"
      },
      {
        "surname": "Lu",
        "given_name": "Tong"
      },
      {
        "surname": "Wu",
        "given_name": "Yirui"
      },
      {
        "surname": "Xu",
        "given_name": "Qian"
      },
      {
        "surname": "Zhou",
        "given_name": "Shijie"
      }
    ]
  },
  {
    "title": "Improving the Facial Expression Recognition and Its Interpretability via Generating Expression Pattern-map",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108737",
    "abstract": "Facial expression recognition focuses on extracting expression-related features on a face. In this paper, a novel method is proposed for facial expression modeling based on the following two aspects: seeking expression-related regions more accurately, and enhancing expression features more discriminating. To this end, we design a model containing three submodules: the Expression Feature Extractor (EFE), the Expression Mask Refiner (EMR), and the Expression Pattern-Map Generator (EPMG). The EFE module is the backbone that extracts expression features and generates a coarse attention mask which roughly indicates expression-related regions. The EMR module refines the mask to be more precise by modeling the relationship among expression-related regions, and generates the masked features. The EPMG module utilizes the masked features to further model the fusion and extraction process which obtains a compact and discriminating expression-salient embedding for recognition, and generates an expression pattern-map. We propose the concept of the expression pattern-map, which provides a unified visualization of expression features and improves the interpretability of facial expression recognition. Our model is evaluated on four public datasets (CK+, Oulu-CASIA, RAF-DB, AffectNet), and achieves the competitive performance compared with the state-of-the-art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002187",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Expression (computer science)",
      "Face (sociological concept)",
      "Facial expression",
      "Facial expression recognition",
      "Facial recognition system",
      "Feature (linguistics)",
      "Generator (circuit theory)",
      "Interpretability",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Power (physics)",
      "Programming language",
      "Quantum mechanics",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Jing"
      },
      {
        "surname": "Yu",
        "given_name": "Huimin"
      }
    ]
  },
  {
    "title": "Generalized multi-output Gaussian process censored regression",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108751",
    "abstract": "When modelling censored observations (i.e. data in which the value of a measurement or observation is un-observable beyond a given threshold), a typical approach in current regression methods is to use a censored-Gaussian (i.e. Tobit) model to describe the conditional output distribution. In this paper, as in the case of missing data, we argue that exploiting correlations between multiple outputs can enable models to better address the bias introduced by censored data. To do so, we introduce a heteroscedastic multi-output Gaussian process model which combines the non-parametric flexibility of GPs with the ability to leverage information from correlated outputs under input-dependent noise conditions. To address the resulting inference intractability, we further devise a variational bound to the marginal log-likelihood suitable for stochastic optimization. We empirically evaluate our model against other generative models for censored data on both synthetic and real world tasks and further show how it can be generalized to deal with arbitrary likelihood functions. Results show how the added flexibility allows our model to better estimate the underlying non-censored (i.e. true) process under potentially complex censoring dynamics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002321",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Censored regression model",
      "Censoring (clinical trials)",
      "Computer science",
      "Econometrics",
      "Gaussian",
      "Gaussian process",
      "Heteroscedasticity",
      "Inference",
      "Leverage (statistics)",
      "Machine learning",
      "Mathematics",
      "Physics",
      "Quantum mechanics",
      "Regression analysis"
    ],
    "authors": [
      {
        "surname": "Gammelli",
        "given_name": "Daniele"
      },
      {
        "surname": "Rolsted",
        "given_name": "Kasper Pryds"
      },
      {
        "surname": "Pacino",
        "given_name": "Dario"
      },
      {
        "surname": "Rodrigues",
        "given_name": "Filipe"
      }
    ]
  },
  {
    "title": "Stochastic batch size for adaptive regularization in deep network optimization",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108776",
    "abstract": "We propose a first-order stochastic optimization algorithm incorporating adaptive regularization for pattern recognition problems in deep learning framework. The adaptive regularization is imposed by stochastic process in determining batch size for each model parameter at each optimization iteration. The stochastic batch size is determined by the update probability of each parameter following a distribution of gradient norms in consideration of their local and global properties in the neural network architecture where the range of gradient norms may vary within and across layers. We empirically demonstrate the effectiveness of our algorithm using an image classification task based on conventional network models applied to commonly used benchmark datasets. The quantitative evaluation indicates that our algorithm outperforms the state-of-the-art optimization algorithms in generalization while providing less sensitivity to the selection of batch size which often plays a critical role in optimization, thus achieving more robustness to the selection of regularity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002576",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Batch processing",
      "Computer science",
      "Mathematical optimization",
      "Mathematics",
      "Programming language",
      "Regularization (linguistics)",
      "Stochastic optimization"
    ],
    "authors": [
      {
        "surname": "Nakamura",
        "given_name": "Kensuke"
      },
      {
        "surname": "Soatto",
        "given_name": "Stefano"
      },
      {
        "surname": "Hong",
        "given_name": "Byung-Woo"
      }
    ]
  },
  {
    "title": "Realistic frontal face reconstruction using coupled complementarity of far-near-sighted face images",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108754",
    "abstract": "There is still a huge gap in the accuracy of face recognition in public video surveillance scenarios. The far-sighted low-resolution (LR) frontal faces have holistic facial profiles but lack sufficient clearness, while the near-sighted high-resolution (HR) tilted faces show rich facial details yet incomplete facial structure suffering from the overhead self-occlusion of the head blocking the face. Following this observation, this paper proposes a dual-branch HR frontal face reconstruction network to explicitly exploit such coupled complementarity hidden in the far-near face images of the same subject, where one branch performs super-resolution (SR) of the LR frontal face and the other branch performs detail fusion and holistic compensation between multiple HR tilted faces as well as the super-resolved frontal result. In particular, we propose a secondary relevance attention mechanism to enhance the embedding of key features, which sequentially performs rough and precise feature matching and embedding, thus enabling coarse-to-fine progressive compensation. Further, scale-entangled densely connected blocks (SEDCB) are used to gradually integrate the relevance information at different scales (due to the different sighting distances) to promote the information interaction between the features of tilted faces. Besides, we also propose a ternary coupled sample pair (LR far-sighted frontal face, HR near-sighted tilted face, normal ground truth clear face) training scheme to supervise the network optimization. Extensive experimental results on two real-world tilt-view face datasets show that our method can not only reconstruct more realistic HR frontal faces but also facilitate the down-stream face identification task compared with the competing counterparts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002357",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Embedding",
      "Face (sociological concept)",
      "Face detection",
      "Face hallucination",
      "Facial recognition system",
      "Normalization (sociology)",
      "Pattern recognition (psychology)",
      "Social science",
      "Sociology",
      "Three-dimensional face recognition"
    ],
    "authors": [
      {
        "surname": "Zeng",
        "given_name": "Kangli"
      },
      {
        "surname": "Wang",
        "given_name": "Zhongyuan"
      },
      {
        "surname": "Lu",
        "given_name": "Tao"
      },
      {
        "surname": "Chen",
        "given_name": "Jianyu"
      },
      {
        "surname": "Huang",
        "given_name": "Baojin"
      },
      {
        "surname": "Han",
        "given_name": "Zhen"
      },
      {
        "surname": "Tian",
        "given_name": "Xin"
      }
    ]
  },
  {
    "title": "Salient object detection with image-level binary supervision",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108782",
    "abstract": "Recent deep learning based salient object detection (SOD) methods have achieved impressive performance. However, while fully-supervised methods require a large amount of labeled data, weakly-supervised methods still require a considerable human effort. To address this problem, we propose a novel weakly-supervised method for salient object detection based on only binary image tags, which are much cheaper to collect. Our basic idea is to construct a dataset of images that are labeled as either salient (with salient objects) or non-salient (without salient objects), and leverage such binary labels as supervision to learn a salient object detector based on existing unsupervised methods. In particular, we propose a target saliency map hallucinator, which can synthesize pseudo ground truth saliency maps for the salient images in the training data solely from binary labels. We can then use the pseudo ground truth labels to train a salient object detector. Experimental results show that our method performs comparably to the state-of-the-art weakly-supervised methods, but requires considerably less human supervision.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002631",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Computer science",
      "Computer vision",
      "Construct (python library)",
      "Detector",
      "Ground truth",
      "Image (mathematics)",
      "Leverage (statistics)",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Salient",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Pengjie"
      },
      {
        "surname": "Liu",
        "given_name": "Yuxuan"
      },
      {
        "surname": "Cao",
        "given_name": "Ying"
      },
      {
        "surname": "Yang",
        "given_name": "Xin"
      },
      {
        "surname": "Luo",
        "given_name": "Yu"
      },
      {
        "surname": "Lu",
        "given_name": "Huchuan"
      },
      {
        "surname": "Liang",
        "given_name": "Zijian"
      },
      {
        "surname": "Lau",
        "given_name": "Rynson W.H."
      }
    ]
  },
  {
    "title": "Security and privacy enhanced smartphone-based gait authentication with random representation learning and digital lockers",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108765",
    "abstract": "Gait data captured by inertial sensors of smartphone have demonstrated promising results on user authentication. However, most existing models stored the enrolled gait pattern in plaintext for matching with the pattern being validated, thus, posed critical security and privacy issues. In this study, we present a gait cryptosystem that generates from gait data captured by smartphone sensors the random keys for user authentication, meanwhile, secures the gait pattern. First, we propose a revocable and random binary string extraction method using deep neural network followed by feature-wise binarization. A novel loss function for network optimization is also designed, to tackle not only the intra-user stability but also the inter-user randomness. Second, we propose a new biometric key generation scheme, namely Irreversible Error Correct and Obfuscate (IECO), improved from the Error Correct and Obfuscate (ECO) scheme, to securely generate from the binary string a random and irreversible key. The model was evaluated with two benchmark datasets as OU-ISIR and whuGAIT. The evaluation showed that our model could generate the key of 139 bits from 5-second data sequence with zero False Acceptance Rate (FAR) and False Rejection Rate (FRR) smaller than 5.441 % . In addition, the security and user privacy analyses showed that our model was secure against existing attacks on biometric template protection, and fulfilled the irreversibility and unlinkability requirements.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002461",
    "keywords": [
      "Artificial intelligence",
      "Authentication (law)",
      "Biology",
      "Biometrics",
      "Computer science",
      "Computer security",
      "Cryptosystem",
      "Data mining",
      "Encryption",
      "Gait",
      "Key (lock)",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physiology",
      "Plaintext",
      "String (physics)"
    ],
    "authors": [
      {
        "surname": "Tran",
        "given_name": "Lam"
      },
      {
        "surname": "Nguyen",
        "given_name": "Thuc"
      },
      {
        "surname": "Kim",
        "given_name": "Hyunil"
      },
      {
        "surname": "Choi",
        "given_name": "Deokjai"
      }
    ]
  },
  {
    "title": "Believe the HiPe: Hierarchical perturbation for fast, robust, and model-agnostic saliency mapping",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108743",
    "abstract": "Understanding the predictions made by Artificial Intelligence (AI) systems is becoming more and more important as deep learning models are used for increasingly complex and high-stakes tasks. Saliency mapping – a popular visual attribution method – is one important tool for this, but existing formulations are limited by either computational cost or architectural constraints. We therefore propose Hierarchical Perturbation, a very fast and completely model-agnostic method for interpreting model predictions with robust saliency maps. Using standard benchmarks and datasets, we show that our saliency maps are of competitive or superior quality to those generated by existing model-agnostic methods – and are over 20 × faster to compute.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002242",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Gene",
      "Image (mathematics)",
      "Machine learning",
      "Perturbation (astronomy)",
      "Physics",
      "Quantum mechanics",
      "Robustness (evolution)",
      "Saliency map"
    ],
    "authors": [
      {
        "surname": "Cooper",
        "given_name": "Jessica"
      },
      {
        "surname": "Arandjelović",
        "given_name": "Ognjen"
      },
      {
        "surname": "Harrison",
        "given_name": "David J"
      }
    ]
  },
  {
    "title": "FocusNet: Classifying better by focusing on confusing classes",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108709",
    "abstract": "Nowadays, most classification networks use one-hot encoding to represent categorical data because of its simplicity. However, one-hot encoding may affect the generalization ability as it neglects inter-class correlations. We observe that, even when a neural network trained with one-hot labels produces incorrect predictions, it still pays attention to the target image region and reveals which classes confuse the network. Inspired by this observation, we propose a confusion-focusing mechanism to address the class-confusion issue. Our confusion-focusing mechanism is implemented by a two-branch network architecture. Its baseline branch generates confusing classes, and its FocusNet branch, whose architecture is flexible, discriminates correct labels from these confusing classes. We also introduce a novel focus-picking loss function to improve classification accuracy by encouraging FocusNet to focus on the most confusing classes. The experimental results validate that our FocusNet is effective for image classification on common datasets, and that our focus-picking loss function can also benefit the current neural networks in improving their classification accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200190X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Categorical variable",
      "Class (philosophy)",
      "Computer science",
      "Computer security",
      "Confusion",
      "Confusion matrix",
      "Contextual image classification",
      "Encoding (memory)",
      "Epistemology",
      "Evolutionary biology",
      "Focus (optics)",
      "Function (biology)",
      "Generalization",
      "Image (mathematics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Network architecture",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Psychoanalysis",
      "Psychology",
      "Simplicity"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Xue"
      },
      {
        "surname": "Sheng",
        "given_name": "Zehua"
      },
      {
        "surname": "Shen",
        "given_name": "Hui-Liang"
      }
    ]
  },
  {
    "title": "Pay attention to what you read: Non-recurrent handwritten text-Line recognition",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108766",
    "abstract": "The advent of recurrent neural networks for handwriting recognition marked an important milestone reaching impressive recognition accuracies despite the great variability that we observe across different writing styles. Sequential architectures are a perfect fit to model text lines, not only because of the inherent temporal aspect of text, but also to learn probability distributions over sequences of characters and words. However, using such recurrent paradigms comes at a cost at training stage, since their sequential pipelines prevent parallelization. In this work, we introduce a novel method that bypasses any recurrence during the training process with the use of transformer models. By using multi-head self-attention layers both at the visual and textual stages, we are able to tackle character recognition as well as to learn language-related dependencies of the character sequences to be decoded. Our model is unconstrained to any predefined vocabulary, being able to recognize out-of-vocabulary words, i.e. words that do not appear in the training vocabulary. We significantly advance over prior art and demonstrate that satisfactory recognition accuracies are yielded even in few-shot learning scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002473",
    "keywords": [
      "Artificial intelligence",
      "Character (mathematics)",
      "Character recognition",
      "Computer science",
      "Feature extraction",
      "Geometry",
      "Handwriting",
      "Handwriting recognition",
      "Image (mathematics)",
      "Intelligent character recognition",
      "Language model",
      "Linguistics",
      "Mathematics",
      "Natural language processing",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Speech recognition",
      "Transformer",
      "Vocabulary",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Kang",
        "given_name": "Lei"
      },
      {
        "surname": "Riba",
        "given_name": "Pau"
      },
      {
        "surname": "Rusiñol",
        "given_name": "Marçal"
      },
      {
        "surname": "Fornés",
        "given_name": "Alicia"
      },
      {
        "surname": "Villegas",
        "given_name": "Mauricio"
      }
    ]
  },
  {
    "title": "Three-dimensional affinity learning based multi-branch ensemble network for breast tumor segmentation in MRI",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108723",
    "abstract": "Accurate and automatic breast tumor segmentation based on dynamic contrast-enhancement magnetic resonance imaging (DCE-MRI) plays an important role in breast cancer analysis. However, the background parenchymal enhancement and large variations in tumor size, shape or appearance make the task very challenging, and also the segmentation performance is still not satisfactory, especially for non-mass enhancement (NME) and small size tumors ( ≤ 2 cm). To address these challenges, we propose a novel 3D affinity learning based multi-branch ensemble network for accurate breast tumor segmentation. Specifically, two different types of subnetworks are built to form a multi-branch network. The two subnetworks are equipped with effective operation components, i.e., residual connection and channel-wise attention or making use of dense connectivity patterns, which can process the input images in parallel. Second, we propose an end-to-end trainable 3D affinity learning based refinement module by calculating the similarities between features of voxels, which is useful in discovering more pixels belonging to breast tumors. Third, two local affinity matrices are constructed by 3D affinity learning, which are used to refine the segmentation outputs of two subnetworks, respectively. Finally, a novel ensemble module is proposed to combine the information derived from the subnetworks, which can hierarchically merge the local and global affinity matrices derived from subnetworks. A large-scale breast DCE-MR images dataset with 420 subjects are built for evaluation, and comprehensive experiments have been conducted to demonstrate that our proposed method achieves superior performance over state-of-the-art medical image segmentation methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002047",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Information retrieval",
      "Merge (version control)",
      "Pattern recognition (psychology)",
      "Pixel",
      "Residual",
      "Segmentation",
      "Voxel"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Lei"
      },
      {
        "surname": "Wang",
        "given_name": "Shuai"
      },
      {
        "surname": "Sun",
        "given_name": "Kun"
      },
      {
        "surname": "Zhou",
        "given_name": "Tao"
      },
      {
        "surname": "Yan",
        "given_name": "Fuhua"
      },
      {
        "surname": "Shen",
        "given_name": "Dinggang"
      }
    ]
  },
  {
    "title": "Paying attention for adjacent areas: Learning discriminative features for large-scale 3D scene segmentation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108722",
    "abstract": "Despite recent improvements in analyzing large-scale 3D point clouds, several problems still exist: (a) segmentation models suffer from intra-class inconsistency and inter-class indistinction; (b) the existing methods ignore the inherent long-tailed class distribution of real-world 3D data. These problems result in unsatisfactory semantic segmentation predictions, especially in object adjacent areas. To handle these problems, this paper proposes a novel Adjacent areas Refinement Network (ARNet). Specifically, an Adjacent areas Refinement (AR) module is designed, which consists of two parallel attention blocks. Besides, our proposed attention blocks can process a large number of points ( N ∼ 10 5 ) with a slight increase in the computational complexity and time cost. Additionally, to deal with the inherent long-tailed class distribution in real-world 3D data, imbalance adjustment loss and occupancy regression loss are introduced. Based on this, the proposed network can handle the classification of both majority and minority classes, which is essential in distinguishing the ambiguous parts in large-scale 3D scenes. The proposed AR module and the loss functions can be easily integrated into the cutting-edge backbone networks, contributing to better performance in modeling semantic inter-dependencies and significantly improving the accuracy of the state-of-the-art semantic segmentation methods on indoor and outdoor scenes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002035",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Discriminative model",
      "Enhanced Data Rates for GSM Evolution",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Object (grammar)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Point (geometry)",
      "Point cloud",
      "Process (computing)",
      "Quantum mechanics",
      "Scale (ratio)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Mengtian"
      },
      {
        "surname": "Xie",
        "given_name": "Yuan"
      },
      {
        "surname": "Ma",
        "given_name": "Lizhuang"
      }
    ]
  },
  {
    "title": "Attention regularized semi-supervised learning with class-ambiguous data for image classification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108727",
    "abstract": "Data augmentation via randomly combining training instances and interpolating the corresponding labels has shown impressive gains in image classification. However, model attention regions are not necessarily meaningful in class semantics, especially for the case of limited supervision. In this paper, we present a semi-supervised classification model based on Class-Ambiguous Data with Attention Regularization, which is referred to as CADAR. Specifically, we adopt a Random Regional Interpolation (RRI) module to construct complex and effective class-ambiguous data, such that the model behavior can be regularized around decision boundaries. By aggregating the parameters of a classification network over training epochs to produce more reliable predictions on unlabeled data, RRI can also be applied to them as well as labeled data. Further, the classifier is enforced to apply consistent attention on the original and constructed data. This is important for inducing the model to learn discriminative features from the class-related regions. The experiment results demonstrate that CADAR significantly benefits from the constructed data and attention regularization, and thus achieves superior performance across multiple standard benchmarks and different amounts of labeled data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002084",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Contextual image classification",
      "Discriminative model",
      "Image (mathematics)",
      "Labeled data",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "Huo",
        "given_name": "Xiaoyang"
      },
      {
        "surname": "Zeng",
        "given_name": "Xiangping"
      },
      {
        "surname": "Wu",
        "given_name": "Si"
      },
      {
        "surname": "Wong",
        "given_name": "Hau-San"
      }
    ]
  },
  {
    "title": "Negational symmetry of quantum neural networks for binary pattern classification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108750",
    "abstract": "Although quantum neural networks (QNNs) have shown promising results in solving simple machine learning tasks recently, the behavior of QNNs in binary pattern classification is still underexplored. In this work, we find that QNNs have an Achilles’ heel in binary pattern classification. To illustrate this point, we provide a theoretical insight into the properties of QNNs by presenting and analyzing a new form of symmetry embedded in a family of QNNs with full entanglement, which we term negational symmetry. Due to negational symmetry, QNNs can not differentiate between a quantum binary signal and its negational counterpart. We empirically evaluate the negational symmetry of QNNs in binary pattern classification tasks using Google’s quantum computing framework. Both theoretical and experimental results suggest that negational symmetry is a fundamental property of QNNs, which is not shared by classical models. Our findings also imply that negational symmetry is a double-edged sword in practical quantum applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200231X",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Computer science",
      "Geometry",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Point (geometry)",
      "Quantum",
      "Quantum entanglement",
      "Quantum mechanics",
      "Statistical physics",
      "Symmetry (geometry)",
      "Theoretical computer science",
      "Theoretical physics"
    ],
    "authors": [
      {
        "surname": "Dong",
        "given_name": "Nanqing"
      },
      {
        "surname": "Kampffmeyer",
        "given_name": "Michael"
      },
      {
        "surname": "Voiculescu",
        "given_name": "Irina"
      },
      {
        "surname": "Xing",
        "given_name": "Eric"
      }
    ]
  },
  {
    "title": "Emotion Recognition Based on Brain Connectivity Reservoir and Valence Lateralization for Cyber-Physical-Social Systems",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.08.009",
    "abstract": "As an important application of pattern recognition, emotion recognition can make Cyber-Physical-Social Systems (CPSS) provide more efficient services for humans. In order to improve the recognition accuracy, this paper proposes an electroencephalogram (EEG) emotion recognition method based on brain connectivity reservoir (BCR) and valence lateralization (VL) for CPSS. First, for the purpose of comprehensively considering the temporality, nonlinearity, and correlation of EEG signals, an emotion recognition model based on BCR is established. Specifically, according to the connectivity index, the correlation between EEG channels is calculated to determine the brain connectivity structure of BCR, and the features of EEG signals are represented through BCR, then the classification result is obtained by the fully connected neural network according to the feature representation. Second, for the purpose of enhancing the feature representation capability of BCR, a training algorithm of BCR based on VL is proposed. Specifically, BCR is divided into two parts, i.e., the left hemi-BCR and the right hemi-BCR. These two parts are trained separately, so that the lateralization characteristic of the brain is better reflected. Finally, the experimental results on DEAP demonstrate that the proposed method achieves a recognition accuracy of 85.55% which is higher than the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002471",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Correlation",
      "Electroencephalography",
      "Feature extraction",
      "Functional connectivity",
      "Geometry",
      "Lateralization of brain function",
      "Mathematics",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Physics",
      "Psychology",
      "Quantum mechanics",
      "Speech recognition",
      "Valence (chemistry)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Jian"
      },
      {
        "surname": "Zhao",
        "given_name": "Tiantian"
      },
      {
        "surname": "Xie",
        "given_name": "Yong"
      },
      {
        "surname": "Xiao",
        "given_name": "Fu"
      },
      {
        "surname": "Sun",
        "given_name": "Lijuan"
      }
    ]
  },
  {
    "title": "AVPL: Augmented visual perception learning for person Re-identification and beyond",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108736",
    "abstract": "In this work, we propose an Augmented Visual Perception Learning (AVPL) method for Person Re-identification (ReID) which is inspired by the two-stream hypothesis theory of Human Visual System (HVS). Deep learning methods dominate ReID and many state-of-the-art performances are achieved from the perspective of optimizing the model of ’what’ visual pathway. It does not blend ’what’ and ’where’ well. Our AVPL method uses the essential mechanism of the ventro-dorsal stream of the ’where’ visual pathway to expand the perception field of the model, and integrates with the ’what’ to complete the information of the visually salient regions. A novel Batch Attention (BA), the key component of our Augmented Visual Perception (AVP) module, is proposed to apply fusion and augmentation into all input feature maps within each batch. Through AVP module, the improved attention-based model attaches more importance to enhancement of salient features, therefore acquiring better perceptual ability of salient regions which provide the most distinguishably distinctions for ReID. Extensive experiments have been carried out on four main stream ReID datasets and two recognition datasets. In terms of ReID, our method achieves rank-1 accuracy of 95.2% on CUHK03-NP, 98.7% on Market-1501, 96.0% on DukeMTMC-reID and 88.8% on MSMT17-V1, outperforming the state-of-the-art methods by a large margin. Besides, it has been experimentally proven to be applicable and effective in other recognition tasks including facial expression recognition and action recognition with an improved accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002175",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Feature (linguistics)",
      "Field (mathematics)",
      "Identification (biology)",
      "Linguistics",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematics",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Perception",
      "Perspective (graphical)",
      "Philosophy",
      "Psychology",
      "Pure mathematics",
      "Salient",
      "Visual perception"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Yewen"
      },
      {
        "surname": "Lian",
        "given_name": "Sicheng"
      },
      {
        "surname": "Hu",
        "given_name": "Haifeng"
      }
    ]
  },
  {
    "title": "Unsupervised video anomaly detection via normalizing flows with implicit latent features",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108703",
    "abstract": "In contemporary society, surveillance anomaly detection, i.e., spotting anomalous events such as crimes or accidents in surveillance videos, is a critical task. As anomalies occur rarely, most training data consists of unlabeled videos without anomalous events, which makes the task challenging. Most existing methods use an autoencoder (AE) to learn to reconstruct normal videos; they then detect anomalies based on their failure to reconstruct the appearance of abnormal scenes. However, because anomalies are distinguished by appearance as well as motion, many previous approaches have explicitly separated appearance and motion informationfor example, using a pre-trained optical flow model. This explicit separation restricts reciprocal representation capabilities between two types of information. In contrast, we propose an implicit two-path AE (ITAE), a structure in which two encoders implicitly model appearance and motion features, along with a single decoder that combines them to learn normal video patterns. For the complex distribution of normal scenes, we suggest normal density estimation of ITAE features through normalizing flow (NF)-based generative models to learn the tractable likelihoods and identify anomalies using out-of-distribution detection. NF models intensify ITAE performance by learning normality through implicitly learned features. Finally, we demonstrate the effectiveness of ITAE and its feature distribution modeling on six benchmarks, including databases that contain various anomalies in real-world scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001844",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Autoencoder",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Feature (linguistics)",
      "Generative grammar",
      "Generative model",
      "Image (mathematics)",
      "Law",
      "Linguistics",
      "Mathematics",
      "Normality",
      "Optical flow",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Cho",
        "given_name": "MyeongAh"
      },
      {
        "surname": "Kim",
        "given_name": "Taeoh"
      },
      {
        "surname": "Kim",
        "given_name": "Woo Jin"
      },
      {
        "surname": "Cho",
        "given_name": "Suhwan"
      },
      {
        "surname": "Lee",
        "given_name": "Sangyoun"
      }
    ]
  },
  {
    "title": "Acceleration-based gait analysis for frailty assessment in older adults",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.07.006",
    "abstract": "Frailty in older individuals has been a hot research topic in the past two decades. This syndrome of physiological decline is characterized by damage to physical function. In the literature, walking speed and gait variability have been discussed and identified as major indicators of frailty. Short walking tests under supervised conditions are frequently considered when assessing the frailty status, where the subject is instructed to walk a certain distance, and the speed and temporospatial features are compared to thresholds. In this paper, we propose a more generalized and fully automated approach using wearable sensors, by suggesting a set of parameters extracted from acceleration signals. The interconnections between these variables, which are related to the gait quality and the frailty trajectory of the subject, are investigated. This study was done on older adults of diverse profiles, in free-living conditions during their daily routine, without any predefined protocol.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002197",
    "keywords": [
      "Acceleration",
      "Classical mechanics",
      "Computer science",
      "Embedded system",
      "Gait",
      "Gait analysis",
      "Gerontology",
      "Medicine",
      "Physical medicine and rehabilitation",
      "Physics",
      "Preferred walking speed",
      "Programming language",
      "Set (abstract data type)",
      "Wearable computer"
    ],
    "authors": [
      {
        "surname": "Abbas",
        "given_name": "Manuel"
      },
      {
        "surname": "Le Bouquin Jeannès",
        "given_name": "Régine"
      }
    ]
  },
  {
    "title": "Pose error analysis method based on a single circular feature",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108726",
    "abstract": "The measurement accuracy of pose parameters based on a single circular feature depends not only on the accuracy of camera calibration and feature extraction but also on the relative pose of the feature and camera—different poses correspond to different error transmission coefficients. To obtain the relationship between measurement errors and pose parameters, we propose an error analysis method based on geometric interpretation. The method characterises measurement error by the sensitivity the imaging feature has to the variation of pose parameters. In addition, the method can be extended to the error analysis work of other coplanar features' pose measurement algorithms. We conducted simulations on measurement errors of pose parameters under different poses, and the results show that the error distribution of pose parameters is in good agreement with the theoretical analysis. Moreover, we propose a method for judging and optimising outliers, and experimental results show the feasibility of this method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002072",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Calibration",
      "Computer science",
      "Computer vision",
      "Electronic engineering",
      "Engineering",
      "Feature (linguistics)",
      "Feature extraction",
      "Linguistics",
      "Mathematics",
      "Observational error",
      "Outlier",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pose",
      "Sensitivity (control systems)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zepeng"
      },
      {
        "surname": "Chen",
        "given_name": "Derong"
      },
      {
        "surname": "Gong",
        "given_name": "Jiulu"
      }
    ]
  },
  {
    "title": "Robust distance metric optimization driven GEPSVM classifier for pattern classification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108779",
    "abstract": "Proximal support vector machine via generalized eigenvalues (GEPSVM) is one of the most successful methods for classification problems. However, GEPSVM is vulnerable to outliers since it learns classifiers based on the squared L2-norm distance without a specific strategy to deal with the outliers. Motivated by existing studies that improve the robustness of GEPSVM via the L1-norm distance or not-squared L2-norm distance formulation, a novel GEPSVM formulation that minimizes the p-order of L2-norm distance is proposed, namely, L2,p-GEPSVM. This formulation weakens the negative effects of both light and heavy outliers in the data. An iterative algorithm is designed to solve the general L2,p-norm distance minimization problems and rigorously prove its convergence. In addition, we adjust the parameters of L2,p-GEPSVM to balance the accuracy and training time. This is especially useful for larger datasets. Extensive results indicate that the L2,p-GEPSVM improves the classification performance and robustness in various experimental settings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002606",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Classifier (UML)",
      "Computer science",
      "Eigenvalues and eigenvectors",
      "Gene",
      "Law",
      "Mahalanobis distance",
      "Mathematical optimization",
      "Mathematics",
      "Mean squared error",
      "Minification",
      "Norm (philosophy)",
      "Outlier",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Quantum mechanics",
      "Robustness (evolution)",
      "Statistics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "He"
      },
      {
        "surname": "Fu",
        "given_name": "Liyong"
      },
      {
        "surname": "Zhang",
        "given_name": "Tian'an"
      },
      {
        "surname": "Hu",
        "given_name": "Jun"
      },
      {
        "surname": "Ye",
        "given_name": "Qiaolin"
      },
      {
        "surname": "Qi",
        "given_name": "Yong"
      },
      {
        "surname": "Yu",
        "given_name": "Dong-Jun"
      }
    ]
  },
  {
    "title": "Multi-manifold discriminant local spline embedding",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108714",
    "abstract": "Manifold learning reveals the intrinsic low-dimensional manifold structure of high-dimensional data and has achieved great success in a wide spectrum of applications. However, traditional manifold learning methods assume that all the data lie on a common manifold, hence fail to capture the complicated geometry structure of the real-world data lying on multiple manifolds. This paper proposes a novel Multi-manifold Discriminant Local Spline Embedding (MDLSE) algorithm for high-dimensional classification, which considers a more realistic scenario where data of the same class lies on the same manifold. On the basis of this assumption, MDLSE seeks to reconstruct multiple manifolds for different classes of data in the embedding and separate them as apart as possible. In order to preserve the geometry structure of all the manifolds, MDLSE employs thin plate splines to align the local patches within each manifold compatibly in the global embedding. Meanwhile, to separate the different manifolds, MDLSE utilizes discriminative information to ensure the neighboring data from different manifolds to be mapped far from each other. Extensive experiments on both synthetic and real-world datasets demonstrate the superiority of MDLSE over the other representative manifold learning algorithms. The advantage of MDLSE is often more obvious on smaller size of training data and in lower embedding dimensions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001959",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Dimensionality reduction",
      "Discriminant",
      "Discriminative model",
      "Embedding",
      "Engineering",
      "Invariant manifold",
      "Manifold (fluid mechanics)",
      "Manifold alignment",
      "Mathematics",
      "Mechanical engineering",
      "Nonlinear dimensionality reduction",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Ping"
      },
      {
        "surname": "Xu",
        "given_name": "Xiaohua"
      },
      {
        "surname": "Chang",
        "given_name": "Xincheng"
      },
      {
        "surname": "Ding",
        "given_name": "Jie"
      },
      {
        "surname": "Chen",
        "given_name": "Suquan"
      }
    ]
  },
  {
    "title": "Orthogonal channel attention-based multi-task learning for multi-view facial expression recognition",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108753",
    "abstract": "Multi-view facial expression recognition (FER) is a challenging computer vision task due to the large intra-class difference caused by viewpoint variations. This paper presents a novel orthogonal channel attention-based multi-task learning (OCA-MTL) approach for FER. The proposed OCA-MTL approach adopts a Siamese convolutional neural network (CNN) to force the multi-view expression recognition model to learn the same features as the frontal expression recognition model. To further enhance the recognition accuracy of non-frontal expression, the multi-view expression model adopts a multi-task learning framework that regards head pose estimation (HPE) as an auxiliary task. A separated channel attention (SCA) module is embedded in the multi-task learning framework to generate individual attention for FER and HPE. Furthermore, orthogonal channel attention loss is presented to force the model to employ different feature channels to represent the facial expression and head pose, thereby decoupling them. The proposed approach is performed on two public facial expression datasets to evaluate its effectiveness and achieves an average recognition accuracy rate of 88.41 % under 13 viewpoints on Multi-PIE and 89.04% under 5 viewpoints on KDEF, outperforming state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002345",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Convolutional neural network",
      "Engineering",
      "Expression (computer science)",
      "Facial expression",
      "Facial expression recognition",
      "Facial recognition system",
      "Feature (linguistics)",
      "Linguistics",
      "Machine learning",
      "Multi-task learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Speech recognition",
      "Systems engineering",
      "Task (project management)",
      "Viewpoints",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Jingying"
      },
      {
        "surname": "Yang",
        "given_name": "Lei"
      },
      {
        "surname": "Tan",
        "given_name": "Lei"
      },
      {
        "surname": "Xu",
        "given_name": "Ruyi"
      }
    ]
  },
  {
    "title": "Rethinking class orders and transferability in class incremental learning",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.07.014",
    "abstract": "Class Incremental Learning (CIL), an indispensable ability for open-world applications such as service robots, has received increasing attention in recent years. Although many CIL methods sprouted out, researchers usually adopt default class orders, leaving the characteristics of different class orders less visited. In this paper, we rethink class orders in CIL from the following aspects: first, we show from preliminary studies that class orders do have an impact on the performance, and mainstream episodic memory-based CIL methods generally favor an interleaved way of arranging class orders; then, we interpret the phenomena above with transferability and propose transferability measures of class orders, which are in line with the method performance under different class orders; based on that, we propose a Class Order Search Algorithm (COSA) to obtain an optimal class order by finding which one has almost the highest transferability. Experiments on Group ImageNet and iNaturalist verify the importance of class orders in CIL methods, and demonstrate the effectiveness of our proposed transferability measures and COSA. These findings may help raise more attention to the hardly visited class orders in CIL.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002252",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Logit",
      "Machine learning",
      "Transferability"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Chen"
      },
      {
        "surname": "Wang",
        "given_name": "Ruiping"
      },
      {
        "surname": "Chen",
        "given_name": "Xilin"
      }
    ]
  },
  {
    "title": "Adaptive Modulation and Rectangular Convolutional Network for Stereo Image Super-Resolution",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.07.018",
    "abstract": "Deep learning based stereo image super-resolution algorithm can make use of the additional information in the stereo image pairs to improve the quality of the reconstructed images. However, extracting the similarity information of one image to another precisely is challenging when partially occluded regions exist in stereo images. In this paper, we propose an adaptive modulation alignment mechanism to modulate the aligned features calculated from the parallax attention mechanism and effectively deal with the inaccuracy caused by occlusion. Furthermore, because disparities of stereo image exist only along the epipolar line, we use rectangular convolution kernel in some convolution layers, to extend the receptive field horizontally. Finally, experimental results demonstrate that our method achieves state-of-the-art performance on the Middlebury, KITTI 2012 and KITTI 2015 stereo benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002367",
    "keywords": [
      "Aesthetics",
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Epipolar geometry",
      "Image (mathematics)",
      "Kernel (algebra)",
      "Mathematics",
      "Modulation (music)",
      "Parallax",
      "Philosophy",
      "Similarity (geometry)",
      "Stereo image"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xiumei"
      },
      {
        "surname": "Li",
        "given_name": "Tianmeng"
      },
      {
        "surname": "Hui",
        "given_name": "Zheng"
      },
      {
        "surname": "Cheng",
        "given_name": "Peitao"
      }
    ]
  },
  {
    "title": "SiameseFuse: A computationally efficient and a not-so-deep network to fuse visible and infrared images",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108712",
    "abstract": "Recent developments in pattern analysis have motivated many researchers to focus on developing deep learning based solutions in various image processing applications. Fusing multi-modal images has been one such application area where the interest is combining different information coming from different modalities in a more visually meaningful and informative way. For that purpose, it is important to first extract salient features from each modality and then fuse them as efficiently and informatively as possible. Recent literature on fusing multi-modal images reports multiple deep solutions that combine both visible (RGB) and infra-red (IR) images. In this paper, we study the performance of various deep solutions available in the literature while seeking an answer to the question: “Do we really need deeper networks to fuse multi-modal images?” To have an answer for that question, we introduce a novel architecture based on Siamese networks to fuse RGB (visible) images with infrared (IR) images and report the state-of-the-art results. We present an extensive analysis on increasing the layer numbers in the architecture with the above-mentioned question in mind to see if using deeper networks (or adding additional layers) adds significant performance in our proposed solution. We report the state-of-the-art results on visually fusing given visible and IR image pairs in multiple performance metrics, while requiring the least number of trainable parameters. Our experimental results suggest that shallow networks (as in our proposed solutions in this paper) can fuse both visible and IR images as well as the deep networks that were previously proposed in the literature (we were able to reduce the total number of trainable parameters up to 96.5%, compare 2,625 trainable parameters to the 74,193 trainable parameters).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001935",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Electrical engineering",
      "Engineering",
      "Focus (optics)",
      "Fuse (electrical)",
      "Image (mathematics)",
      "Image fusion",
      "Modal",
      "Modality (human–computer interaction)",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Polymer chemistry",
      "RGB color model",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Özer",
        "given_name": "Sedat"
      },
      {
        "surname": "Ege",
        "given_name": "Mert"
      },
      {
        "surname": "Özkanoglu",
        "given_name": "Mehmet Akif"
      }
    ]
  },
  {
    "title": "Dual Gaussian Modeling for Deep Face Embeddings",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.07.010",
    "abstract": "Most currently existing face recognition methods model face images as deterministic points in the latent space. However, it could encounter performance drop inevitably in a fully unconstrained scenario for the intrinsic noise of the images. In order to mitigate the detrimental impact of noisy data on model training, distribution estimation is introduced to face recognition, which models each face image as a Gaussian distribution and improves the robustness against noise effectively. But the uncertainty (variance) learned only relates to one attribute: the quality of the image. We propose dual Gaussian modeling (DGM) for deep face embeddings. For an input image, the network learns two Gaussian distributions simultaneously. The main Gaussian branch focuses on learning easy samples in the training dataset, while the other one mainly deals with faces with large pose. The uncertainty is not only correlated with image quality, but also with facial pose. During training, a sample-specific adaptive weight is learned to integrate the two sub-Gaussian features into a more compact discriminant embedding for classification. Besides, we introduce weighted Euclidean distance and minimize the entropy of the adaptive weight to regulate the relationship between the two distributions. Comprehensive experiments and analysis demonstrate that our method can boost the performance of face recognition under common or more unconstrained benchmarks, such as IJB-C.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002215",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Embedding",
      "Entropy (arrow of time)",
      "Facial recognition system",
      "Gaussian",
      "Gaussian network model",
      "Gaussian noise",
      "Gene",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Yuying"
      },
      {
        "surname": "Deng",
        "given_name": "Weihong"
      }
    ]
  },
  {
    "title": "Methods for segmenting cracks in 3d images of concrete: A comparison based on semi-synthetic images",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108747",
    "abstract": "Concrete is the standard construction material for buildings, bridges, and roads. As safety plays a central role in the design, monitoring, and maintenance of such constructions, it is important to understand the cracking behavior of concrete. Computed tomography captures the microstructure of building materials and allows to study crack initiation and propagation. Manual segmentation of crack surfaces in large 3d images is not feasible. In this paper, automatic crack segmentation methods for 3d images are reviewed and compared. Classical image processing methods (edge detection filters, template matching, minimal path and region growing algorithms) and learning methods (convolutional neural networks, random forests) are considered and tested on semi-synthetic 3d images. Their performance strongly depends on parameter selection which should be adapted to the grayvalue distribution of the images and the geometric properties of the concrete. In general, the learning methods perform best, in particular for thin cracks and low grayvalue contrast.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200228X",
    "keywords": [
      "Abstraction",
      "Artificial intelligence",
      "Artificial neural network",
      "Composite material",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Cracking",
      "Edge detection",
      "Engineering",
      "Enhanced Data Rates for GSM Evolution",
      "Epistemology",
      "Image (mathematics)",
      "Image processing",
      "Image segmentation",
      "Matching (statistics)",
      "Materials science",
      "Mathematics",
      "Path (computing)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Segmentation",
      "Statistics",
      "Structural engineering"
    ],
    "authors": [
      {
        "surname": "Barisin",
        "given_name": "Tin"
      },
      {
        "surname": "Jung",
        "given_name": "Christian"
      },
      {
        "surname": "Müsebeck",
        "given_name": "Franziska"
      },
      {
        "surname": "Redenbach",
        "given_name": "Claudia"
      },
      {
        "surname": "Schladitz",
        "given_name": "Katja"
      }
    ]
  },
  {
    "title": "An end-to-end identity association network based on geometry refinement for multi-object tracking",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108738",
    "abstract": "In multi-target tracking, object interactions and occlusions are two significant factors that affect tracking performance. To settle this, we propose an identity association network (IANet) that integrates the geometry refinement network (GRNet) and the identity verification (IV) module to perform data association and reason the mapping between the detections and tracklets. In our data association process, the object drifts caused by object interactions are suppressed effectively by encoding the direction and velocity of objects to refine the geometric position of tracklets. The tracklets with refined geometric information are further utilized in the IV module to achieve a sufficient encoding of multivariate spatial cues including both appearance and geometry information, which defeats the misleading impacts of interactions and occlusions dramatically in multi-object tracking. The extensive experiments and comparative evaluations have demonstrated that our proposed method can significantly outperform many state-of-the-art methods on benchmarks of 2D MOT2015, MOT16, MOT17, MOT20, and KITTI by using public detection and online settings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002199",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Association (psychology)",
      "Computer science",
      "Computer vision",
      "Data association",
      "Economics",
      "Encoding (memory)",
      "Epistemology",
      "Filter (signal processing)",
      "Finance",
      "Geometry",
      "Identity (music)",
      "Mathematics",
      "Object (grammar)",
      "Operating system",
      "Orientation (vector space)",
      "Pedagogy",
      "Philosophy",
      "Physics",
      "Position (finance)",
      "Process (computing)",
      "Psychology",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Rui"
      },
      {
        "surname": "Zhang",
        "given_name": "Baopeng"
      },
      {
        "surname": "Teng",
        "given_name": "Zhu"
      },
      {
        "surname": "Fan",
        "given_name": "Jianping"
      }
    ]
  },
  {
    "title": "Editorial paper for pattern recognition letters VSI on advances in graph-based recognition for pattern recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.07.017",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002288",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Epipolar geometry",
      "Graph",
      "Image (mathematics)",
      "Kernel (algebra)",
      "Mathematics",
      "Parallax",
      "Pattern recognition (psychology)",
      "Similarity (geometry)",
      "Stereo image",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Conte",
        "given_name": "Donatello"
      },
      {
        "surname": "Ramel",
        "given_name": "Jean-Yves"
      },
      {
        "surname": "Foggia",
        "given_name": "Pasquale"
      }
    ]
  },
  {
    "title": "Maximization and restoration: Action segmentation through dilation passing and temporal reconstruction",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108764",
    "abstract": "Action segmentation aims to split videos into segments of different actions. Recent work focuses on dealing with long-range dependencies of long, untrimmed videos, but still suffers from over-segmentation and performance saturation due to increased model complexity. This paper addresses the aforementioned issues through a divide-and-conquer strategy that first maximizes the frame-wise classification accuracy of the model and then reduces the over-segmentation errors. This strategy is implemented with the Dilation Passing and Reconstruction Network, composed of the Dilation Passing Network, which primarily aims to increase accuracy by propagating information of different dilations, and the Temporal Reconstruction Network, which reduces over-segmentation errors by temporally encoding and decoding the output features from the Dilation Passing Network. We also propose a weighted temporal mean squared error loss that further reduces over-segmentation. Through evaluations on the 50Salads, GTEA, and Breakfast datasets, we show that our model achieves significant results compared to existing state-of-the-art models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200245X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Decoding methods",
      "Dilation (metric space)",
      "Divide and conquer algorithms",
      "Encoding (memory)",
      "Image segmentation",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Message passing",
      "Pattern recognition (psychology)",
      "Programming language",
      "Scale-space segmentation",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Park",
        "given_name": "Junyong"
      },
      {
        "surname": "Kim",
        "given_name": "Daekyum"
      },
      {
        "surname": "Huh",
        "given_name": "Sejoon"
      },
      {
        "surname": "Jo",
        "given_name": "Sungho"
      }
    ]
  },
  {
    "title": "Federating recommendations using differentially private prototypes",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108746",
    "abstract": "Machine learning methods exploit similarities in users’ activity patterns to provide recommendations in applications across a wide range of fields including entertainment, dating, and commerce. However, in domains that demand protection of personally sensitive data, such as medicine or banking, how can we learn recommendation models without accessing the sensitive data and without inadvertently leaking private information? Many situations in the medical field prohibit centralizing the data from different hospitals and thus require learning from information kept in separate databases. We propose a new federated approach to learning global and local private models for recommendation without collecting raw data, user statistics, or information about personal preferences. Our method produces a set of locally learned prototypes that allow us to infer global behavioral patterns while providing differential privacy guarantees for users in any database of the system. By requiring only two rounds of communication, we both reduce the communication costs and avoid excessive privacy loss associated with typical federated learning iterative procedures. We test our framework on synthetic data, real federated medical data, and a federated version of Movielens ratings. We show that local adaptation of the global model allows the proposed method to outperform centralized matrix-factorization-based recommender system models, both in terms of the accuracy of matrix reconstruction and in terms of the relevance of recommendations, while maintaining provable privacy guarantees. We also show that our method is more robust and has smaller variance than individual models learned by independent entities.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002278",
    "keywords": [
      "Accounting",
      "Adaptation (eye)",
      "Artificial intelligence",
      "Business",
      "Collaborative filtering",
      "Computer science",
      "Computer security",
      "Data mining",
      "Differential privacy",
      "Eigenvalues and eigenvectors",
      "Exploit",
      "Field (mathematics)",
      "Information privacy",
      "Information sensitivity",
      "Law",
      "Machine learning",
      "Mathematics",
      "Matrix decomposition",
      "MovieLens",
      "Optics",
      "Physics",
      "Political science",
      "Private information retrieval",
      "Programming language",
      "Pure mathematics",
      "Quantum mechanics",
      "Raw data",
      "Recommender system",
      "Relevance (law)",
      "Set (abstract data type)",
      "Synthetic data",
      "Variance (accounting)"
    ],
    "authors": [
      {
        "surname": "Ribero",
        "given_name": "Mónica"
      },
      {
        "surname": "Henderson",
        "given_name": "Jette"
      },
      {
        "surname": "Williamson",
        "given_name": "Sinead"
      },
      {
        "surname": "Vikalo",
        "given_name": "Haris"
      }
    ]
  },
  {
    "title": "Uncertainty-aware twin support vector machines",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108706",
    "abstract": "There exist uncertain data in the real world due to some factors such as imprecise measurements and noise. Unlike deterministic data, the features of samples in uncertain data are often described by interval numbers or random vectors with probability density functions. In this paper we propose novel twin support vector machines (TSVMs) to handle uncertain data. In the proposed models which are referred to as uncertainty-aware TSVMs, each uncertain sample is modeled as a random vector with Gaussian distributions. To deal with the multi-dimensional integrals in the original models, we derive an interesting and important theorem which helps us transform the original models into the model involving one-dimensional integrals. The simplification of models makes the optimization problem tractable and the simplified models are solved by using the quasi-Newton optimization algorithm. The proposed decision rule allows us to classify uncertain samples with means and covariance matrices. In addition, we extend the proposed models to their kernel versions to capture the nonlinear structure of uncertain data. Experiments on a series of data sets have been performed to demonstrate that the proposed models gain better classification performance than some existing algorithms, especially for representing uncertain cross-plane problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200187X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Covariance",
      "Kernel (algebra)",
      "Mathematical optimization",
      "Mathematics",
      "Nonlinear system",
      "Physics",
      "Quantum mechanics",
      "Statistics",
      "Support vector machine",
      "Uncertain data"
    ],
    "authors": [
      {
        "surname": "Liang",
        "given_name": "Zhizheng"
      },
      {
        "surname": "Zhang",
        "given_name": "Lei"
      }
    ]
  },
  {
    "title": "GaitPrivacyON: Privacy-preserving mobile gait biometrics using unsupervised learning",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.07.015",
    "abstract": "Numerous studies in the literature have already shown the potential of biometrics on mobile devices for authentication purposes. However, it has been shown that, the learning processes associated to biometric systems might expose sensitive personal information about the subjects. This study proposes GaitPrivacyON, a novel mobile gait biometrics verification approach that provides accurate authentication results while preserving the sensitive information of the subject. It comprises two modules: i) two convolutional Autoencoders with shared weights that transform attributes of the biometric raw data, such as the gender or the activity being performed, into a new privacy-preserving representation; and ii) a mobile gait verification system based on the combination of Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) with a Siamese architecture. The main advantage of GaitPrivacyON is that the first module (convolutional Autoencoders) is trained in an unsupervised way, without specifying the sensitive attributes of the subject to protect. Two experimental studies have been examinated: i) MotionSense and MobiAct databases; and ii) OU-ISIR database. The experimental results achieved suggest the potential of GaitPrivacyON to significantly improve the privacy of the subject while keeping user authentication results higher than 96.6% Area Under the Curve (AUC). To the best of our knowledge, this is the first mobile gait verification approach that considers privacy-preserving methods trained in an unsupervised way.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002264",
    "keywords": [
      "Artificial intelligence",
      "Authentication (law)",
      "Biology",
      "Biometrics",
      "Computer science",
      "Computer security",
      "Convolutional neural network",
      "Deep learning",
      "Gait",
      "Law",
      "Machine learning",
      "Mobile device",
      "Pattern recognition (psychology)",
      "Physiology",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Unsupervised learning",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Delgado-Santos",
        "given_name": "Paula"
      },
      {
        "surname": "Tolosana",
        "given_name": "Ruben"
      },
      {
        "surname": "Guest",
        "given_name": "Richard"
      },
      {
        "surname": "Vera-Rodriguez",
        "given_name": "Ruben"
      },
      {
        "surname": "Deravi",
        "given_name": "Farzin"
      },
      {
        "surname": "Morales",
        "given_name": "Aythami"
      }
    ]
  },
  {
    "title": "3D hand pose and shape estimation from RGB images for keypoint-based hand gesture recognition",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108762",
    "abstract": "Estimating the 3D pose of a hand from a 2D image is a well-studied problem and a requirement for several real-life applications such as virtual reality, augmented reality, and hand gesture recognition. Currently, reasonable estimations can be computed from single RGB images, especially when a multi-task learning approach is used to force the system to consider the shape of the hand when its pose is determined. However, depending on the method used to represent the hand, the performance can drop considerably in real-life tasks, suggesting that stable descriptions are required to achieve satisfactory results. In this paper, we present a keypoint-based end-to-end framework for 3D hand and pose estimation and successfully apply it to the task of hand gesture recognition as a study case. Specifically, after a pre-processing step in which the images are normalized, the proposed pipeline uses a multi-task semantic feature extractor generating 2D heatmaps and hand silhouettes from RGB images, a viewpoint encoder to predict the hand and camera view parameters, a stable hand estimator to produce the 3D hand pose and shape, and a loss function to guide all of the components jointly during the learning phase. Tests were performed on a 3D pose and shape estimation benchmark dataset to assess the proposed framework, which obtained state-of-the-art performance. Our system was also evaluated on two hand-gesture recognition benchmark datasets and significantly outperformed other keypoint-based approaches, indicating that it is an effective solution that is able to generate stable 3D estimates for hand pose and shape.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002436",
    "keywords": [
      "3D pose estimation",
      "Artificial intelligence",
      "Augmented reality",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Economics",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Gesture",
      "Gesture recognition",
      "Linguistics",
      "Management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pipeline (software)",
      "Pose",
      "Programming language",
      "RGB color model",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Avola",
        "given_name": "Danilo"
      },
      {
        "surname": "Cinque",
        "given_name": "Luigi"
      },
      {
        "surname": "Fagioli",
        "given_name": "Alessio"
      },
      {
        "surname": "Foresti",
        "given_name": "Gian Luca"
      },
      {
        "surname": "Fragomeni",
        "given_name": "Adriano"
      },
      {
        "surname": "Pannone",
        "given_name": "Daniele"
      }
    ]
  },
  {
    "title": "Instance exploitation for learning temporary concepts from sparsely labeled drifting data streams",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108749",
    "abstract": "Continual learning from streaming data sources becomes more and more popular due to the increasing number of online tools and systems. Dealing with dynamic and everlasting problems poses new challenges for which traditional batch-based offline algorithms turn out to be insufficient in terms of computational time and predictive performance. One of the most crucial limitations is that we cannot assume having an access to a finite and complete data set – we always have to be ready for new data that may complement our model. This poses a critical problem of providing labels for potentially unbounded streams. In real world, we are forced to deal with very strict budget limitations, therefore, we will most likely face the scarcity of annotated instances, which are essential in supervised learning. In our work, we emphasize this problem and propose a novel instance exploitation technique. We show that when: (i) data is characterized by temporary non-stationary concepts, and (ii) there are very few labels spanned across a long time horizon, it is actually better to risk overfitting and adapt models more aggressively by exploiting the only labeled instances we have, instead of sticking to a standard learning mode and suffering from severe underfitting. We present different strategies and configurations for our methods, as well as an ensemble algorithm that attempts to maintain a sweet spot between risky and normal adaptation. Finally, we conduct a complex in-depth comparative analysis of our methods, using state-of-the-art streaming algorithms relevant for the given problem.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002308",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Complement (music)",
      "Complementation",
      "Computer science",
      "Concept drift",
      "Data stream mining",
      "Face (sociological concept)",
      "Gene",
      "Machine learning",
      "Optics",
      "Overfitting",
      "Phenotype",
      "Physics",
      "Programming language",
      "Set (abstract data type)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Korycki",
        "given_name": "Łukasz"
      },
      {
        "surname": "Krawczyk",
        "given_name": "Bartosz"
      }
    ]
  },
  {
    "title": "HAM: Hybrid attention module in deep convolutional neural networks for image classification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108785",
    "abstract": "Recently, many researches have demonstrated that the attention mechanism has great potential in improving the performance of deep convolutional neural networks (CNNs). However, the existing methods either ignore the importance of using channel attention and spatial attention mechanisms simultaneously or bring much additional model complexity. In order to achieve a balance between performance and model complexity, we propose the Hybrid Attention Module (HAM), a really lightweight yet efficient attention module. Given an intermediate feature map as the input feature, HAM firstly produces one channel attention map and one channel refined feature through the channel submodule, and then based on the channel attention map, the spatial submodule divides the channel refined feature into two groups along the channel axis to generate a pair of spatial attention descriptors. By applying saptial attention descriptors, the spatial submodule generates the final refined feature which can adaptively emphasize the important regions. Besides, HAM is a simple and general module, it can be embedded into various mainstream deep CNN architectures seamlessly and can be trained with base CNNs in the end-to-end way. We evaluate HAM through abundant of experiments on CIFAR-10, CIFAR-100 and STL-10 datasets. The experimental results show that HAM-integrated networks achieve accuracy improvements and further reduce the negative impact of less training data on deeper networks performance than its counterparts, which proves the effectiveness of HAM.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002667",
    "keywords": [
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Feature (linguistics)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Guoqiang"
      },
      {
        "surname": "Fang",
        "given_name": "Qi"
      },
      {
        "surname": "Zha",
        "given_name": "Linlin"
      },
      {
        "surname": "Gao",
        "given_name": "Xin"
      },
      {
        "surname": "Zheng",
        "given_name": "Nenggan"
      }
    ]
  },
  {
    "title": "VFMVAC: View-filtering-based multi-view aggregating convolution for 3D shape recognition and retrieval",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108774",
    "abstract": "Multi-view based 3D shape recognition methods have achieved state-of-the-art performance in 3D shape recognition and retrieval. The main focus of multi-view based approaches is determining how to fuse multi-view features into a compact, descriptive, and robust 3D shape descriptor that can then be utilized for 3D shape recognition and retrieval. This paper proposes a novel multi-view aggregating framework, view-filtering-based multi-view aggregating convolution (VFMVAC) to learn global shape descriptors for 3D shape recognition. The proposed VFMVAC applies a voting-based view filtering strategy to select representative views, also introduces a novel multi-view aggregating module to integrate multi-view features; this substantially improves the descriptiveness of the descriptors, and therefore improves the performance of 3D shape recognition and retrieval. Specifically, all views are fed into a voting-based view filtering module to select the top-k representative views. Subsequently, the features of the top-k views are fed into the multi-view aggregating module, which first conducts cross-view channel shuffle for achieving cross-view information flowing, and the resulted reshaped features are then fed into the aggregating convolution module for feature fusion. Experiments on benchmark datasets demonstrate that the proposed VFMVAC is effective and outperforms several recent techniques with respect to the classification and retrieval performance, robustness and efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002552",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Convolution (computer science)",
      "Electrical engineering",
      "Engineering",
      "Fuse (electrical)",
      "Gene",
      "Geodesy",
      "Geography",
      "Law",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Robustness (evolution)",
      "Voting"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Zehua"
      },
      {
        "surname": "Zhang",
        "given_name": "Yuhe"
      },
      {
        "surname": "Gao",
        "given_name": "Jian"
      },
      {
        "surname": "Wang",
        "given_name": "Shurui"
      }
    ]
  },
  {
    "title": "Recurrent flow networks: A recurrent latent variable model for density estimation of urban mobility",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108752",
    "abstract": "Mobility-on-demand (MoD) systems represent a rapidly developing mode of transportation wherein travel requests are dynamically handled by a coordinated fleet of vehicles. Crucially, the efficiency of an MoD system highly depends on how well supply and demand distributions are aligned in spatio-temporal space (i.e., to satisfy user demand, cars have to be available in the correct place and at the desired time). To do so, we argue that predictive models should aim to explicitly disentangle between temporal and spatial variability in the evolution of urban mobility demand. However, current approaches typically ignore this distinction by either treating both sources of variability jointly, or completely ignoring their presence in the first place. In this paper, we propose recurrent flow networks 1 1 Code available at https://www.github.com/DanieleGammelli/recurrent-flow-nets (RFN), where we explore the inclusion of (i) latent random variables in the hidden state of recurrent neural networks to model temporal variability, and (ii) normalizing flows to model the spatial distribution of mobility demand. We demonstrate how predictive models explicitly disentangling between spatial and temporal variability exhibit several desirable properties, and empirically show how this enables the generation of distributions matching potentially complex urban topologies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002333",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Econometrics",
      "Flow (mathematics)",
      "Geometry",
      "Latent variable",
      "Matching (statistics)",
      "Mathematical analysis",
      "Mathematics",
      "Recurrent neural network",
      "Statistics",
      "Variable (mathematics)"
    ],
    "authors": [
      {
        "surname": "Gammelli",
        "given_name": "Daniele"
      },
      {
        "surname": "Rodrigues",
        "given_name": "Filipe"
      }
    ]
  },
  {
    "title": "Multilevel wavelet-based hierarchical networks for image compressed sensing",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108758",
    "abstract": "Recently, deep learning-based compressed sensing (CS) algorithms have been reported, which remarkably achieve pleasing reconstruction quality with low computational complexity. However, the sampling process of the common deep learning-based CS methods and the conventional ones cannot sufficiently exploit the structured sparsity within image sequences, especially in preserving finer texture details. In this paper, we propose a novel multilevel wavelet-based hierarchical networks for image compressed sensing (dubbed MWHCS-Net). In particular, MWHCS-Net consists of three modules: a sampling module based on a multilevel wavelet transform, a hierarchical initial reconstruction module and a lightweight deep reconstruction module. Motivated by the fact that a sparser signal is easier to reconstruct accurately, we present the sampling module based on multilevel wavelet transform with hierarchical subspace learning for progressive acquisition of measurements to further optimize sampling efficiency and stability. To enhance the finer texture details, the hierarchical initial reconstruction module is designed as a basic initial reconstruction network plus an enhanced initial reconstruction network, which corresponding to the dominant structure component and the texture detail component of the reconstructed image, respectively. At the same time, we also further explore the impact of the hierarchical initial reconstruction module and prove that the texture detail component branch plays an important role in improving the reconstruction quality. Experimental results demonstrate that the proposed MWHCS-Net achieves the state-of-the-art performance while maintaining an efficient running speed. Furthermore, MWHCS-Net outperforms the existing image CS methods based on deep learning in terms of anti-noise performance in most cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002394",
    "keywords": [
      "Artificial intelligence",
      "Compressed sensing",
      "Computer science",
      "Computer vision",
      "Discrete wavelet transform",
      "Image (mathematics)",
      "Pattern recognition (psychology)",
      "Set partitioning in hierarchical trees",
      "Wavelet",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Yin",
        "given_name": "Zhu"
      },
      {
        "surname": "Shi",
        "given_name": "WuZhen"
      },
      {
        "surname": "Wu",
        "given_name": "Zhongcheng"
      },
      {
        "surname": "Zhang",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Mask-guided cycle-GAN for specular highlight removal",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.06.014",
    "abstract": "Specular highlight removal is an important yet challenging problem in image enhancement. Recent methods based on deep learning and trained by massive paired or unpaired data have demonstrated promising performance for this task. Methods based on unpaired data have recently gained more attention for easier training data collection. In this paper, we present a Mask-Guided Cycle-GAN for specular highlight removal on unpaired data. Incorporating the idea that specular highlight mainly has characteristics in lightness, we attempt to train a module only on luminance channel before considering all channels, and then adopt the training results to guide the subsequent highlight removal module. We further convert the highlight removal problem to image-to-image translation by using cycle-consistent adversarial network (Cycle-GAN). In the proposed network, a non-negative matrix factorization (NMF) based method is incorporated to obtain accurate highlight masks. The proposed method is evaluated using the specular highlight image quadruples (SHIQ) and the LIME datasets, and the advantages are demonstrated via comparative experimental results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002082",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Channel (broadcasting)",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Eigenvalues and eigenvectors",
      "Engineering",
      "Gene",
      "Image (mathematics)",
      "Matrix decomposition",
      "Messenger RNA",
      "Non-negative matrix factorization",
      "Optics",
      "Physics",
      "Quantum mechanics",
      "Specular reflection",
      "Systems engineering",
      "Task (project management)",
      "Telecommunications",
      "Translation (biology)"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Guangwei"
      },
      {
        "surname": "Zheng",
        "given_name": "Yuanfeng"
      },
      {
        "surname": "Yan",
        "given_name": "Haoran"
      },
      {
        "surname": "Hua",
        "given_name": "Guang"
      },
      {
        "surname": "Yan",
        "given_name": "Yuchen"
      }
    ]
  },
  {
    "title": "Adaptive preference transfer for personalized IoT entity recommendation",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.08.011",
    "abstract": "Internet of Things (IoT) recommendation can effectively improve the convenience and intelligence of users in obtaining information about interested physical entities in a huge entity search space. However, existing transfer learning-based recommendation methods mainly focus on Internet resources and services, lack personalized transfer weight consideration for different user preferences, and fail to address the entity rating matrix sparsity problem, resulting in limited IoT entity recommendation performance. Thus, we design an adaptive preference transfer IoT entity (APTE) recommendation method in this paper. Firstly, considering various user preference characteristics, an adaptive dual-domain transfer model for item domain and social domain is designed to meet users’ personalized requirements. Then, a lightweight user feedback embedding method is proposed to mine the explicit and implicit features and to embed the entity rating matrix to solve the problem of sparse user feedback information. Simulation results demonstrate that APTE can effectively improve the recommendation performance compared with state-of-the-art baselines.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002495",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Information retrieval",
      "Mathematics",
      "Parallel computing",
      "Preference",
      "Recommender system",
      "Statistics",
      "Transfer (computing)"
    ],
    "authors": [
      {
        "surname": "Zhen",
        "given_name": "Yan"
      },
      {
        "surname": "Liu",
        "given_name": "Huan"
      },
      {
        "surname": "Sun",
        "given_name": "Meiyu"
      },
      {
        "surname": "Yang",
        "given_name": "Boran"
      },
      {
        "surname": "Zhang",
        "given_name": "Puning"
      }
    ]
  },
  {
    "title": "Real-time identification of eye fixations and saccades using radial basis function networks and Markov chains",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.08.013",
    "abstract": "Analysis of eye-movements is crucial to many applications, from medical diagnosis to gaming. A critical step in this process lies in segmenting raw gaze coordinates provided by the eye-tracker into eye saccade and fixation events. This detection is generally executed offline, as most methods require a complete dataset or large temporal windows. Many of these algorithms also rely on heuristics such as fixed velocity thresholds, yielding variations in the results depending on the user’s choice of parameters. To overcome such limitations, we designed a new approach, named RBFNMC, which combines Radial Basis Function Network (RBFN) and Markov Chains (MC) in a Concept Drift framework. Our approach is capable of automatically categorizing saccades and fixations in an online scenario. Comparisons with previous detection techniques revealed accurate predictions, while not requiring fixed threshold parameters. Our results were estimated from real eye-movement datasets collected in experiments with: (i) monkeys in a free-viewing paradigm; (ii) human subjects looking at different types of stimuli. Comparing RBFNMC with several other methods widely cited in the literature, our contribution constitutes a new computational approach to process spatial data streams in an online and unsupervised fashion. As a consequence, we provide an efficient mechanism to detect saccade and fixations which support eye-tracking research in neuroscience and other areas. Finally, it is worth emphasizing that our work additionally provides the possibility of interpreting the decision process by inspecting graph-based visualizations of RBFN and of the transition probabilities in MC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002513",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Demography",
      "Eye movement",
      "Eye tracking",
      "Fixation (population genetics)",
      "Gaze",
      "Heuristics",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Population",
      "Process (computing)",
      "Saccade",
      "Sliding window protocol",
      "Sociology",
      "Window (computing)"
    ],
    "authors": [
      {
        "surname": "Lobão-Neto",
        "given_name": "Ruivaldo"
      },
      {
        "surname": "Brilhault",
        "given_name": "Adrien"
      },
      {
        "surname": "Neuenschwander",
        "given_name": "Sergio"
      },
      {
        "surname": "Rios",
        "given_name": "Ricardo"
      }
    ]
  },
  {
    "title": "Self-supervised rigid transformation equivariance for accurate 3D point cloud registration",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108784",
    "abstract": "Transformation equivariance has been widely investigated in 3D point cloud representation learning for more informative descriptors, which formulates the change of the representation with respect to the transformation of the input point clouds explicitly. In this paper, we extend this property to the task of 3D point cloud registration and propose a rigid transformation equivariance (RTE) for accurate 3D point cloud registration. Specifically, RTE formulates the change of the relative pose explicitly with respect to the rigid transformation of the input point clouds. To exploit RTE, we adopt a Siamese structure network with two shared registration branches. One focuses on the input pair of point clouds, and the other one focuses on the new pair achieved by applying two random rigid transformations to the input point clouds respectively. Since the change of the two output relative poses has been predicted according to RTE, a new additional self-supervised loss is obtained to supervise the training. This general network structure can be integrated with most learning-based point cloud registration frameworks easily to improve the performance. Our method adopts the state-of-the-art virtual point-based pipelines as our shared branches, in which we propose a data-driven matching based on learned cost volume (LCV) rather than traditional hand-crafted matching strategies. Experimental evaluations on both synthetic datasets and real datasets validate the effectiveness of our proposed framework. The source code will be made public.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002655",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Gene",
      "Geometry",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Point (geometry)",
      "Point cloud",
      "Point set registration",
      "Rigid transformation",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zhiyuan"
      },
      {
        "surname": "Sun",
        "given_name": "Jiadai"
      },
      {
        "surname": "Dai",
        "given_name": "Yuchao"
      },
      {
        "surname": "Zhou",
        "given_name": "Dingfu"
      },
      {
        "surname": "Song",
        "given_name": "Xibin"
      },
      {
        "surname": "He",
        "given_name": "Mingyi"
      }
    ]
  },
  {
    "title": "Auto-weighted sample-level fusion with anchors for incomplete multi-view clustering",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108772",
    "abstract": "Aiming at solving the problem of clustering in the multi-view datasets which include samples with information missing in one or more views, incomplete multi-view clustering has received considerable attention. However, most studies can not get satisfying accuracy and efficiency when dealing with datasets in which a considerable number of instances are missing in partial views. To address this problem, a method named Auto-weighted Sample-level Fusion with Anchors for Incomplete Multi-view Clustering (ASA-IC) is proposed in this paper. It designs an auto-weighted sample-level fusion strategy, which realizes the optimized conversion from the individual instance-to-anchor similarity learning to the concensus instance-to-anchor similarity matrix construction. ASA-IC can not only handle incomplete samples and effectively explore the relationship between each instance and anchors, but also deal with various incomplete clustering situations and be applied in large-scale datasets as well. Besides, experiments on 5 complete datasets and 27 incomplete ones illustrate its effectiveness quantitatively and qualitatively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002539",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Fusion",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Missing data",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Sample (material)",
      "Scale (ratio)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Xiao"
      },
      {
        "surname": "Liu",
        "given_name": "Hui"
      },
      {
        "surname": "Lin",
        "given_name": "Yuxiu"
      },
      {
        "surname": "Wu",
        "given_name": "Yan"
      },
      {
        "surname": "Zhang",
        "given_name": "Caiming"
      }
    ]
  },
  {
    "title": "Joint prediction of monocular depth and structure using planar and parallax geometry",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108806",
    "abstract": "Supervised learning depth estimation methods can achieve good performance when trained on high-quality ground-truth, like LiDAR data. However, LiDAR can only generate sparse 3D maps which causes losing information. Obtaining high-quality ground-truth depth data per pixel is difficult to acquire. In order to overcome this limitation, we propose a novel approach combining structure information from a promising Plane and Parallax geometry pipeline with depth information into a U-Net supervised learning network, which results in quantitative and qualitative improvement compared to existing popular learning-based methods. In particular, the model is evaluated on two large-scale and challenging datasets: KITTI Vision Benchmark and Cityscapes dataset and achieve the best performance in terms of relative error. Compared with pure depth supervision models, our model has impressive performance on depth prediction of thin objects and edges, and compared to structure prediction baseline, our model performs more robustly.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002874",
    "keywords": [
      "A priori and a posteriori",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Epistemology",
      "Geodesy",
      "Geology",
      "Ground truth",
      "Lidar",
      "Monocular",
      "Parallax",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pipeline (software)",
      "Programming language",
      "Quantum mechanics",
      "Remote sensing",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Xing",
        "given_name": "Hao"
      },
      {
        "surname": "Cao",
        "given_name": "Yifan"
      },
      {
        "surname": "Biber",
        "given_name": "Maximilian"
      },
      {
        "surname": "Zhou",
        "given_name": "Mingchuan"
      },
      {
        "surname": "Burschka",
        "given_name": "Darius"
      }
    ]
  },
  {
    "title": "Continual semi-supervised learning through contrastive interpolation consistency",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.08.006",
    "abstract": "Continual Learning (CL) investigates how to train Deep Networks on a stream of tasks without incurring forgetting. CL settings proposed in literature assume that every incoming example is paired with ground-truth annotations. However, this clashes with many real-world applications: gathering labeled data, which is in itself tedious and expensive, becomes infeasible when data flow as a stream. This work explores Continual Semi-Supervised Learning (CSSL): here, only a small fraction of labeled input examples are shown to the learner. We assess how current CL methods (e.g.: EWC, LwF, iCaRL, ER, GDumb, DER) perform in this novel and challenging scenario, where overfitting entangles forgetting. Subsequently, we design a novel CSSL method that exploits metric learning and consistency regularization to leverage unlabeled examples while learning. We show that our proposal exhibits higher resilience to diminishing supervision and, even more surprisingly, relying only on 25 % supervision suffices to outperform SOTA methods trained under full supervision.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002458",
    "keywords": [],
    "authors": [
      {
        "surname": "Boschini",
        "given_name": "Matteo"
      },
      {
        "surname": "Buzzega",
        "given_name": "Pietro"
      },
      {
        "surname": "Bonicelli",
        "given_name": "Lorenzo"
      },
      {
        "surname": "Porrello",
        "given_name": "Angelo"
      },
      {
        "surname": "Calderara",
        "given_name": "Simone"
      }
    ]
  },
  {
    "title": "Few-shot Website Fingerprinting attack with Meta-Bias Learning",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108739",
    "abstract": "Website fingerprinting (WF) attack aims to identify which website a user is visiting from the traffic data patterns. Whilst existing methods assume many training samples, we investigate a more realistic and scalable few-shot WF attack with only a few labeled training samples per website. To solve this problem, we introduce a novel Meta-Bias Learning (MBL) method for few-shot WF learning. Taking the meta-learning strategy, MBL simulates and optimizes the target tasks. Moreover, a new model parameter factorization idea is introduced for facilitating meta-training with superior task adaptation. Expensive experiments show that our MBL outperforms significantly existing hand-crafted feature and deep learning based alternatives in both closed-world and open-world attack scenarios, at the absence and presence of defense.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002205",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Database",
      "Economics",
      "Feature (linguistics)",
      "Linguistics",
      "Machine learning",
      "Management",
      "Meta learning (computer science)",
      "Organic chemistry",
      "Philosophy",
      "Scalability",
      "Shot (pellet)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Mantun"
      },
      {
        "surname": "Wang",
        "given_name": "Yongjun"
      },
      {
        "surname": "Zhu",
        "given_name": "Xiatian"
      }
    ]
  },
  {
    "title": "Classification for high-dimension low-sample size data",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108828",
    "abstract": "High-dimension and low-sample-size (HDLSS) data sets have posed great challenges to many machine learning methods. To deal with practical HDLSS problems, development of new classification techniques is highly desired. After the cause of the over-fitting phenomenon is identified, a new classification criterion for HDLSS data sets, termed tolerance similarity, is proposed to emphasize maximization of within-class variance on the premise of class separability. Leveraging on this criterion, a novel linear binary classifier, termed No-separated Data Maximum Dispersion classifier (NPDMD), is designed. The main idea of the NPDMD is to spread samples of two classes in a large interval in the respective positive or negative space along the projecting direction when the distance between the projection means for two classes is large enough. The salient features of the proposed NPDMD are: (1) The NPDMD operates well on HDLSS data sets; (2) The NPDMD solves the objective function in the entire feature space to avoid the data-piling phenomenon. (3) The NPDMD leverages on the low-rank property of the covariance matrix for HDLSS data sets to accelerate the computation speed. (4) The NPDMD is suitable for different real-word applications. (5) The NPDMD can be implemented readily using Quadratic Programming. Not only theoretical properties of the NPDMD have been derived, but also a series of evaluations have been conducted on one simulated and six real-world benchmark data sets, including face classification and mRNA classification. Experimental results and comprehensive studies demonstrate the superiority of the NPDMD in terms of correct classification rate, mean within-group correct classification rate and the area under the ROC curve.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003090",
    "keywords": [
      "Artificial intelligence",
      "Binary classification",
      "Classifier (UML)",
      "Computer science",
      "Feature vector",
      "Linear classifier",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Pattern recognition (psychology)",
      "Sample size determination",
      "Sample space",
      "Statistics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Shen",
        "given_name": "Liran"
      },
      {
        "surname": "Er",
        "given_name": "Meng Joo"
      },
      {
        "surname": "Yin",
        "given_name": "Qingbo"
      }
    ]
  },
  {
    "title": "Residual objectness for imbalance reduction",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108781",
    "abstract": "As most object detectors rely on dense candidate samples to cover objects, they have always suffered from the extreme imbalance between very few foreground samples and numerous background samples during training, i.e., the foreground-background imbalance. Although several resampling and reweighting schemes (e.g., OHEM, Focal Loss, GHM) have been proposed to alleviate the imbalance, they are usually heuristic with multiple hyper-parameters, which is difficult to generalize on different object detectors and datasets. In this paper, we propose a novel Residual Objectness (ResObj) mechanism that adaptively learns how to address the foreground-background imbalance problem in object detection. Specifically, we first formulate the imbalance problems on all object classes as an imbalance problem on an “objectness” class. Then, we design multiple cascaded objectness estimators with residual connections for that objectness class to progressively distinguish the foreground samples from background samples. With our residual objectness mechanism, object detectors can learn how to address the foreground-background problem in an end-to-end way, rather than rely on hand-crafted resampling or reweighting schemes. Extensive experiments on the COCO benchmark demonstrate the effectiveness and compatibility of our method for various object detectors: the RetinaNet-ResObj, YOLOv3-ResObj and FasterRCNN-ResObj achieve relative 3 % ∼ 4 % Average Precision (AP) improvements compared with their vanilla models, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200262X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Detector",
      "Estimator",
      "Geodesy",
      "Geography",
      "Geometry",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Reduction (mathematics)",
      "Resampling",
      "Residual",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Joya"
      },
      {
        "surname": "Liu",
        "given_name": "Dong"
      },
      {
        "surname": "Luo",
        "given_name": "Bin"
      },
      {
        "surname": "Peng",
        "given_name": "Xuezheng"
      },
      {
        "surname": "Xu",
        "given_name": "Tong"
      },
      {
        "surname": "Chen",
        "given_name": "Enhong"
      }
    ]
  },
  {
    "title": "Rotation invariant Gabor convolutional neural network for image classification",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.08.010",
    "abstract": "Gabor filters have been recently integrated with deep convolutional neural networks to learn better features with fewer model parameters. However, during feature learning the rotation invariance is not well addressed. In this paper, we propose a rotation invariant Gabor convolutional neural network (RIGCN) for image classification. First, we transform each input image to generate multiple rotated image instances and feed them into a weight-sharing Siamese network architecture to learn Gabor-guided deep convolutional features. Then, we compute the maximum and average feature responses from all the rotated instances of the same input image and send them into a convolutional fusion module to obtain a rotation invariant image representation. Finally, we use the cross-entropy loss for image classification. In our method, the use of Siamese architecture enables us to obtain rotation invariant features from rotated image instances. The use of convolutional fusion operator enables us to obtain richer statistical features with high efficiency. Experimental results on several benchmark datasets demonstrate the effectiveness of RIGCN for rotation invariant image classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002483",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Contextual image classification",
      "Convolutional neural network",
      "Feature extraction",
      "Gabor filter",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Invariant (physics)",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Rotation (mathematics)"
    ],
    "authors": [
      {
        "surname": "Yao",
        "given_name": "Xiaoqin"
      },
      {
        "surname": "Song",
        "given_name": "Tiecheng"
      }
    ]
  },
  {
    "title": "Distributional barycenter problem through data-driven flows",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108795",
    "abstract": "A new method is proposed for the solution of the data-driven optimal transport barycenter problem and of the more general distributional barycenter problem that the article introduces. The distributional barycenter problem provides a conceptual and computational toolbox for central problems in pattern recognition, such as the simulation of conditional distributions, the construction of a representative for a family of distributions indexed by a covariate and a new class of data-based generative models. The method proposed improves on previous approaches based on adversarial games, by slaving the discriminator to the generator and minimizing the need for parameterizations. It applies not only to a discrete family of distributions, but to more general distributions conditioned to factors z of any cardinality and type. The methodology is applied to numerical examples, including an analysis of the MNIST data set with a new cost function that penalizes non-isometric maps.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200276X",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Cardinality (data modeling)",
      "Class (philosophy)",
      "Computer science",
      "Data mining",
      "Detector",
      "Discriminator",
      "Generator (circuit theory)",
      "MNIST database",
      "Mathematical optimization",
      "Mathematics",
      "Physics",
      "Power (physics)",
      "Programming language",
      "Quantum mechanics",
      "Set (abstract data type)",
      "Telecommunications",
      "Toolbox"
    ],
    "authors": [
      {
        "surname": "Tabak",
        "given_name": "Esteban G."
      },
      {
        "surname": "Trigila",
        "given_name": "Giulio"
      },
      {
        "surname": "Zhao",
        "given_name": "Wenjun"
      }
    ]
  },
  {
    "title": "Local to Global Feature Learning for Salient Object Detection",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.09.004",
    "abstract": "Existing works mainly focus on how to aggregate multi-level features for salient object detection, which may generate sub-optimal results due to interference with redundant details. To handle this problem, we aim to learn a local to global feature representation, so as to segment the detailed structures in a local perspective and locate the salient objects in a global perspective. In particular, we design a novel L2GF network which mainly consists of three modules, i.e., L-Net, G-Net, and F-Net. L-Net employs our enhanced auto-encoder structure to extract local contexts that provide rich boundary information of objects, which is able to learn rich local features in a certain receptive field. G-Net feeds the tokenized feature patches as input sequence, and leverages the well-known Transformer structure to extract global contexts which are helpful to derive the relationship between multiple salient regions and produce more complete salient results. F-Net is a coarse-to-fine process, which takes the features and maps of both local and global branches as inputs and calculate the final high-quality salient map. Extensive experiments on five benchmark datasets demonstrate that our L2GF network performs favorably against the state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002677",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cartography",
      "Computer science",
      "Encoder",
      "Feature (linguistics)",
      "Feature learning",
      "Geography",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Net (polyhedron)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Feng",
        "given_name": "Xuelu"
      },
      {
        "surname": "Zhou",
        "given_name": "Sanping"
      },
      {
        "surname": "Zhu",
        "given_name": "Zixin"
      },
      {
        "surname": "Wang",
        "given_name": "Le"
      },
      {
        "surname": "Hua",
        "given_name": "Gang"
      }
    ]
  },
  {
    "title": "Self-Attention based fine-grained cross-media hybrid network",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108748",
    "abstract": "Due to the heterogeneity gap, the data representations of different types of media are inconsistent. It is challenging to measure the fine-grained gap between different media. To this end, we propose a self-attention-based hybrid network to learn the common representations of different media data. Specifically, we first utilize a local self-attention layer to learn the common attention space between different media data. Then we propose a similarity concatenation method to understand the content relationship between features. To further improve the robustness of the model, we also learn a local position encoding to capture the spatial relationships between features. Therefore, our proposed approach can effectively reduce the gap between different feature distributions on cross-media retrieval tasks. Extensive experiments and ablation studies demonstrate that our proposed method achieves state-of-the-art performance. The source code and models are publicly available at: https://github.com/NUST-Machine-Intelligence-Laboratory/SAFGCMHN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002291",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Concatenation (mathematics)",
      "Data mining",
      "Encoding (memory)",
      "Gene",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Robustness (evolution)",
      "Source code"
    ],
    "authors": [
      {
        "surname": "Shan",
        "given_name": "Wei"
      },
      {
        "surname": "Huang",
        "given_name": "Dan"
      },
      {
        "surname": "Wang",
        "given_name": "Jiangtao"
      },
      {
        "surname": "Zou",
        "given_name": "Feng"
      },
      {
        "surname": "Li",
        "given_name": "Suwen"
      }
    ]
  },
  {
    "title": "FastOPM—A practical method for partial match of time series",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108808",
    "abstract": "In applications like stock markets, engineering, medicine, etc., a large amount of time series data has been collected. Interrogating the data for patterns is important for analysis like event prediction and event investigation. A fundamental operation to support such analysis is query processing. In this paper, we aim to efficiently find the optimal match of a query in a timeseries when the match is calculated based on the trend and allows points to be skipped from the middle and ends of the sequences. This problem requires global optimization. The solutions in the literature have prohibitively high time complexities and are not practical for long timeseries. Our method consists of three parts. The first part is an efficiency improvement algorithm called FastOPM which applies the Dijkstra algorithm to get the optimal solution in an efficient manner. The second part derives bounds for optimal solutions. The third part is an algorithm for efficiently searching the target timeseries for the best optimal match of a query. Our experiments show that our method is faster than the baseline method, the bounds are effective, and the search algorithm can identify the best optimal match efficiently. Overall, our algorithm effectively outperforms the state-of-the-art algorithms DTW and MASS in retrieving target segments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002898",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Data mining",
      "Dijkstra's algorithm",
      "Dynamic time warping",
      "Efficient algorithm",
      "Event (particle physics)",
      "Graph",
      "Machine learning",
      "Paleontology",
      "Physics",
      "Quantum mechanics",
      "Series (stratigraphy)",
      "Shortest path problem",
      "Theoretical computer science",
      "Time complexity",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Jixue"
      },
      {
        "surname": "Li",
        "given_name": "Jiuyong"
      },
      {
        "surname": "Liu",
        "given_name": "Lin"
      }
    ]
  },
  {
    "title": "Fully convolutional Deep Stacked Denoising Sparse Auto encoder network for partial face reconstruction",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108783",
    "abstract": "Face recognition is one of the most successful applications of image analysis. Since 1960s, automatic face recognition research has been carried out, but the problem is still unresolved. Therefore, in this manuscript, a novel Partial face reconstruction (PFR) algorithm called Self- motivated feature mapping (SMFM) combining a Fully Convolutional Network (FCN) and Deep Stacked Denoising Sparse Autoencoders (DS-DSA) algorithm is proposed to overcome the challenges. The proposed approach focuses on the generation of feature maps from the Fully Convolutional Network and it is used Deep Stacked Denoising Sparse Autoencoders to perform the partial face reconstruction. The spatial maps are generated by extracting the features from Fully Convolutional Network and it is supplied as the input for partial reconstruction and re-identification to the Deep Stacked Denoising Sparse Autoencoders network. The main aim of the proposed work is “to enhance the accuracy during facial reconstruction”. The proposed approach is implemented in MATLAB platform. The performance of the proposed approach attains 23.45% and 20.41% accuracy,25.93`% and 19.43% sensitivity, 22.21% and 24.41% precision and20.21% and 23.41% Specificity greater than the existing approaches, like Partial Face Reconstruction using generative adversarial networks (GANs), Partial Face Reconstruction using Deep Recurrent neural network (DRNN).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002643",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Face (sociological concept)",
      "Facial recognition system",
      "Feature (linguistics)",
      "Generative adversarial network",
      "Iterative reconstruction",
      "Linguistics",
      "Noise reduction",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Dinesh",
        "given_name": "P.S."
      },
      {
        "surname": "Manikandan",
        "given_name": "M."
      }
    ]
  },
  {
    "title": "Two-stage generative adversarial networks for binarization of color document images",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108810",
    "abstract": "Document image enhancement and binarization methods are often used to improve the accuracy and efficiency of document image analysis tasks such as text recognition. Traditional non-machine-learning methods are constructed on low-level features in an unsupervised manner but have difficulty with binarization on documents with severely degraded backgrounds. Convolutional neural network (CNN)based methods focus only on grayscale images and on local textual features. In this paper, we propose a two-stage color document image enhancement and binarization method using generative adversarial neural networks. In the first stage, four color-independent adversarial networks are trained to extract color foreground information from an input image for document image enhancement. In the second stage, two independent adversarial networks with global and local features are trained for image binarization of documents of variable size. For the adversarial neural networks, we formulate loss functions between a discriminator and generators having an encoder–decoder structure. Experimental results show that the proposed method achieves better performance than many classical and state-of-the-art algorithms over the Document Image Binarization Contest (DIBCO) datasets, the LRDE Document Binarization Dataset (LRDE DBD), and our shipping label image dataset. We plan to release the shipping label dataset as well as our implementation code at github.com/opensuh/DocumentBinarization/.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002916",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Generative adversarial network",
      "Generative grammar",
      "Geology",
      "Image (mathematics)",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Stage (stratigraphy)"
    ],
    "authors": [
      {
        "surname": "Suh",
        "given_name": "Sungho"
      },
      {
        "surname": "Kim",
        "given_name": "Jihun"
      },
      {
        "surname": "Lukowicz",
        "given_name": "Paul"
      },
      {
        "surname": "Lee",
        "given_name": "Yong Oh"
      }
    ]
  },
  {
    "title": "Multi-attribute balanced sampling for disentangled GAN controls",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.08.012",
    "abstract": "Various controls over the generated data can be extracted from the latent space of a pre-trained GAN, as it implicitly encodes the semantics of the training data. The discovered controls allow to vary semantic attributes in the generated images but usually lead to entangled edits that affect multiple attributes at the same time. Supervised approaches typically sample and annotate a collection of latent codes, then train classifiers in the latent space to identify the controls. Since the data generated by GANs reflects the biases of the original dataset, so do the resulting semantic controls. We propose to address disentanglement by balancing the semantics of the dataset before training the classifiers. We demonstrate the effectiveness of this approach by extracting disentangled linear directions for face manipulation on state-of-the-art GAN architectures (including StyleGAN2 and StyleGAN3) and two datasets, CelebAHQ and FFHQ. We show that this simple and general approach outperforms state-of-the-art classifier-based methods while avoiding the need for disentanglement-enforcing post-processing.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002501",
    "keywords": [
      "Computer science",
      "Detector",
      "Sampling (signal processing)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Doubinsky",
        "given_name": "Perla"
      },
      {
        "surname": "Audebert",
        "given_name": "Nicolas"
      },
      {
        "surname": "Crucianu",
        "given_name": "Michel"
      },
      {
        "surname": "Le Borgne",
        "given_name": "Hervé"
      }
    ]
  },
  {
    "title": "Characterizing pandemic waves: A latent class analysis of COVID-19 spread across US counties",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.08.017",
    "abstract": "The spread of the COVID-19 pandemic is observed to follow the shape of “waves” (i.e., the rise and fall of population-adjusted daily new infection cases with time). Different geographic regions of the world have experienced different position and span of these waves over time. The presence and strength of these waves broadly characterize the dynamics of the pandemic spread in a given area, so their characterization is important to draw meaningful intervention and mitigation plans tailored for that area. In this paper, we propose a novel technique to represent the trend of COVID-19 spread as a sequence of a fixed-length text string defined on three symbols: R (rise), S (Steady), and F (fall). These strings, termed as trend strings, enabled us searching for specific patterns in them (such as for waves). After analyzing county-level infection data, we observe that, US counties—despite their wide variation in trend strings—can be grouped into a number of heterogeneous classes each of which might have a representative COVID spread pattern over time (in terms of presence and propensity of waves). To this end, we conduct a latent class analysis to cluster 3142 US counties into four distinct classes based on their wave characteristics for one year pandemic data (January 2020 to January 2021). We observe that counties in each class have distinct socio-demographics, location, and human mobility characteristics. In short summary, counties have differing number of waves (class 1 counties have only one wave and class 3 counties have three) and their positions also vary (class 1 had the wave later in the year whereas class 3 had waves throughout the year). We believe that this way of characterizing pandemic waves would provide better insights in understanding the complex dynamics of COVID-19 spread and its future evolution, and would, therefore, help in taking class-specific policy interventions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002628",
    "keywords": [
      "2019-20 coronavirus outbreak",
      "Biology",
      "Coronavirus disease 2019 (COVID-19)",
      "Demography",
      "Disease",
      "Economics",
      "Finance",
      "Genetics",
      "Geography",
      "Infectious disease (medical specialty)",
      "Mathematical physics",
      "Mathematics",
      "Medicine",
      "Outbreak",
      "Pandemic",
      "Pathology",
      "Population",
      "Position (finance)",
      "Sequence (biology)",
      "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)",
      "Sociology",
      "String (physics)",
      "Virology"
    ],
    "authors": [
      {
        "surname": "Sarwar Uddin",
        "given_name": "Md Yusuf"
      },
      {
        "surname": "Rafiq",
        "given_name": "Rezwana"
      }
    ]
  },
  {
    "title": "LSRML: A latent space regularization based meta-learning framework for MR image segmentation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108821",
    "abstract": "Data sources for medical image segmentation can be quite extensive, and models trained with data from a source domain may perform poorly on data from the target domain owing to domain shift issues. To overcome the impact of domain shift, we propose a novel meta-learning-based multi-source domain adaptation framework for medical image segmentation. Specifically, we designed a domain discriminator module to produce category prediction over the latent features, and an image reconstruction module to reconstruct the foreground and background of the target domain image separately. Furthermore, we constructed a large-scale multi-modal prostate dataset, which contained 495,902 magnetic resonance images of 419 cases, with prostate and lesion masks, as well as diagnostic descriptions for each patient. We evaluated our proposed method through extensive experiments using the proposed and the benchmark datasets. Experimental results show that our model achieves better segmentation and generalization performance compared to state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003028",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Detector",
      "Discriminator",
      "Domain (mathematical analysis)",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Image segmentation",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)",
      "Segmentation",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Bo"
      },
      {
        "surname": "Tan",
        "given_name": "Yunpeng"
      },
      {
        "surname": "Wang",
        "given_name": "Hui"
      },
      {
        "surname": "Zhang",
        "given_name": "Zheng"
      },
      {
        "surname": "Zhou",
        "given_name": "Xiuzhuang"
      },
      {
        "surname": "Wu",
        "given_name": "Jingyun"
      },
      {
        "surname": "Mi",
        "given_name": "Yue"
      },
      {
        "surname": "Huang",
        "given_name": "Haiwen"
      },
      {
        "surname": "Wang",
        "given_name": "Wendong"
      }
    ]
  },
  {
    "title": "Discrete curve model for non-elastic shape analysis on shape manifold",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108760",
    "abstract": "In this paper, we construct a novel finite dimensional shape manifold for shape analyses. Elements of the shape manifold are a set of discrete, planar, and closed curves, which stand for object boundaries and are represented by direction function. On this manifold, we use a set of N-dimensional Fourier basis to construct the tangent space of the shape manifold as a finite dimensional space. Furthermore, we construct the shape manifold as a Riemannian manifold, in which the Riemannian metric is interpreted as an l 2 metric. Our method improves the performance of bending-only models in the issues of shape analysis including the shape synthesis, comparison, and statistic analysis. We evaluate the performance of the manifold via the following applications: 1) shape interpolation and extrapolation between curves, 2) shape retrieval on the Flavia leaf database, 3) shape synthesis using an estimated probability distribution on the manifold, and 4) a novel application named shape arithmetic. All the above experiments clearly demonstrate our approach achieves superior performance to state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002412",
    "keywords": [
      "Active shape model",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Engineering",
      "Evolutionary biology",
      "Geometry",
      "Manifold (fluid mechanics)",
      "Mathematical analysis",
      "Mathematics",
      "Mechanical engineering",
      "Programming language",
      "Segmentation",
      "Shape analysis (program analysis)",
      "Shape change",
      "Shape parameter",
      "Static analysis",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Peng"
      },
      {
        "surname": "Li",
        "given_name": "Xutao"
      },
      {
        "surname": "Ding",
        "given_name": "Changxing"
      },
      {
        "surname": "Liu",
        "given_name": "Jianxing"
      },
      {
        "surname": "Wu",
        "given_name": "Ligang"
      }
    ]
  },
  {
    "title": "Asymmetric cross–modal hashing with high–level semantic similarity",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108823",
    "abstract": "Cross-modal hashing aims at using modality content to retrieve semantically relevant objects of different modalities, so cross-modal retrieval has attracted much attention. To effectively exploit the discriminative label information and retain more semantic information in the process of hash learning, we propose a novel cross-modal hashing method, named high-level semantic similarity analysis hashing (HSSAH) for cross-modal retrieval. To reduce time complexity and enhance discriminant ability in hash codes, HSSAH constructs an asymmetric high-level semantic similarity learning framework to replace the binary semantic similarity matrix. Moreover, the developed HSSAH is a two-stage approach, and a semantic-enhanced scheme is proposed in the second stage, which fully leverages the label information to gain more powerful hash functions. We conducted comprehensive experiments on three benchmark datasets to evaluate the performance of HSSAH. Experimental results show that HSSAH can achieve significantly better retrieval precision and outperforms several state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003041",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Hash function",
      "Image (mathematics)",
      "Modal",
      "Pattern recognition (psychology)",
      "Polymer chemistry",
      "Programming language",
      "Semantic similarity",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Fan"
      },
      {
        "surname": "Liu",
        "given_name": "Yufeng"
      },
      {
        "surname": "Ding",
        "given_name": "Xiaojian"
      },
      {
        "surname": "Ma",
        "given_name": "Fumin"
      },
      {
        "surname": "Cao",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "Dual-frame spatio-temporal feature modulation for video enhancement",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108822",
    "abstract": "Current video enhancement approaches have achieved good performance in specific rainy, hazy, foggy, and snowy weather conditions. However, they currently suffer from two important limitations. First, they can only handle degradation caused by single weather. Second, they use large, complex models with 10–50 millions of parameters needing high computing resources. As video enhancement is a pre-processing step for applications like video surveillance, traffic monitoring, autonomous driving, etc., it is necessary to have a lightweight enhancement module. Therefore, we propose a dual-frame spatio-temporal feature modulation architecture to handle the degradation caused by diverse weather conditions. The proposed architecture combines the concept of spatio-temporal multi-resolution feature modulation with a multi-receptive parallel encoders and domain-based feature filtering modules to learn domain-specific features. Further, the architecture provides temporal consistency with recurrent feature merging, achieved by providing feedback of the previous frame output. The indoor (REVIDE, NYUDepth), synthetically generated outdoor weather degraded video de-hazing, and de-raining with veiling effect databases are used for experimentation. Also, the performance of the proposed method is analyzed for night-time de-hazing and de-raining with veiling effect weather conditions. Experimental results show the superior performance of our framework compared to existing state-of-the-art methods used for video de-hazing (indoor/outdoor) and de-raining with veiling effect weather conditions. The code is available at https://github.com/pwp1208/PR2022",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200303X",
    "keywords": [
      "Aesthetics",
      "Artificial intelligence",
      "Code (set theory)",
      "Computer science",
      "Computer vision",
      "Domain (mathematical analysis)",
      "Encoder",
      "Feature (linguistics)",
      "Frame (networking)",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Modulation (music)",
      "Operating system",
      "Philosophy",
      "Programming language",
      "Real-time computing",
      "Set (abstract data type)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Patil",
        "given_name": "Prashant W."
      },
      {
        "surname": "Gupta",
        "given_name": "Sunil"
      },
      {
        "surname": "Rana",
        "given_name": "Santu"
      },
      {
        "surname": "Venkatesh",
        "given_name": "Svetha"
      }
    ]
  },
  {
    "title": "Editorial for the special issue on deep learning for precise and efficient object detection",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.08.004",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002422",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Machine learning",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Pang",
        "given_name": "Yanwei"
      },
      {
        "surname": "Han",
        "given_name": "Jungong"
      },
      {
        "surname": "Lu",
        "given_name": "Xin"
      },
      {
        "surname": "Conci",
        "given_name": "Nicola"
      }
    ]
  },
  {
    "title": "Automatic dottization of Arabic text (Rasms) using deep recurrent neural networks",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.09.001",
    "abstract": "Arabic letters in their early stages were only shapes (Rasm) without dots. Dots were added later to ease reading and reduce ambiguity. Thereafter, diacritics were introduced for phonetic guidance, mainly for nonnative speakers. Many studies have been conducted to automatically diacritize Arabic texts using machine learning techniques. However, to the best of our knowledge, automatically adding dots to Arabic Rasms has not been reported in the literature. In this work, we present the automatic addition of dots to Arabic Rasms using deep recurrent neural networks. Different design choices were explored, including the use of character sequences and word sequences as tokens. The presented techniques were evaluated on four diverse publicly available datasets. Character-level models with stacked BiGRU architecture outperformed all the other architectures with character error rates ranging from 2.0% to 5.5% and dottization error rates ranging from 4.2% to 11.0% on independent test sets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002641",
    "keywords": [
      "Ambiguity",
      "Arabic",
      "Artificial intelligence",
      "Artificial neural network",
      "Character (mathematics)",
      "Computer science",
      "Deep learning",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Natural language processing",
      "Philosophy",
      "Programming language",
      "Ranging",
      "Reading (process)",
      "Speech recognition",
      "Telecommunications",
      "Word error rate"
    ],
    "authors": [
      {
        "surname": "Alhathloul",
        "given_name": "Zainab"
      },
      {
        "surname": "Ahmad",
        "given_name": "Irfan"
      }
    ]
  },
  {
    "title": "Sample complexity of rank regression using pairwise comparisons",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108688",
    "abstract": "We consider a rank regression setting, in which a dataset of N samples with features in R d is ranked by an oracle via M pairwise comparisons. Specifically, there exists a latent total ordering of the samples; when presented with a pair of samples, a noisy oracle identifies the one ranked higher with respect to the underlying total ordering. A learner observes a dataset of such comparisons and wishes to regress sample ranks from their features. We show that to learn the model parameters with ϵ > 0 accuracy, it suffices to conduct M ∈ Ω ( d N log 3 N / ϵ 2 ) comparisons uniformly at random when N is Ω ( d / ϵ 2 ) .",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322001698",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Combinatorics",
      "Computer science",
      "Mathematics",
      "Oracle",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Rank (graph theory)",
      "Regression",
      "Sample (material)",
      "Software engineering",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Kadıoğlu",
        "given_name": "Berkan"
      },
      {
        "surname": "Tian",
        "given_name": "Peng"
      },
      {
        "surname": "Dy",
        "given_name": "Jennifer"
      },
      {
        "surname": "Erdoğmuş",
        "given_name": "Deniz"
      },
      {
        "surname": "Ioannidis",
        "given_name": "Stratis"
      }
    ]
  },
  {
    "title": "Contrastive author-aware text clustering",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108787",
    "abstract": "In the era of User Generated Content (UGC), authors (IDs) of texts widely exist and play a key role in determining the topic categories of texts. Existing text clustering efforts are mainly attributed to utilizing textual information, but the effect of authors on text clustering remains largely underexplored. To mitigate this issue, we propose a novel Contrastive Author-aware Text clustering approach, dubbed as CAT. CAT injects author information not only in characterizing texts through representations but also in pushing or pulling text representations of different authors through contrastive learning, which is rarely adopted by text clustering. Specifically, the developed contrastive learning method conducts both cluster-instance contrast by the text representation augmentation and instance-instance contrast by the multi-view representations. We perform comprehensive experiments on three public datasets, demonstrating that CAT largely outperforms strong competitive text clustering baselines and validating the effectiveness of the CAT’s main components.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002680",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Computer security",
      "Contrast (vision)",
      "Document clustering",
      "Information retrieval",
      "Key (lock)",
      "Law",
      "Natural language processing",
      "Political science",
      "Politics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Xudong"
      },
      {
        "surname": "Dong",
        "given_name": "Chao"
      },
      {
        "surname": "Zhang",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "CVM-Cervix: A hybrid cervical Pap-smear image classification framework using CNN, visual transformer and multilayer perceptron",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108829",
    "abstract": "Cervical cancer is the seventh most common cancer among all the cancers worldwide and the fourth most common cancer among women. Cervical cytopathology image classification is an important method to diagnose cervical cancer. However, manual inspection is very troublesome, and experts are prone to make mistakes. The emergence of the automatic computer-aided diagnosis system solves this problem. This paper proposes a framework called CVM-Cervix based on deep learning to perform cervical cell classification tasks. It can analyze pap slides quickly and accurately. CVM-Cervix first proposes a Convolutional Neural Network module and a Visual Transformer module for local and global feature extraction respectively, then a Multilayer Perceptron module is designed to fuse the local and global features for the final classification. Experimental results show the effectiveness and potential of the proposed CVM-Cervix in the field of cervical Pap smear image classification. In addition, according to the practical needs of clinical work, we perform a lightweight post-processing to compress the model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003107",
    "keywords": [
      "Artificial intelligence",
      "Cancer",
      "Cervical cancer",
      "Cervix",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Cytology",
      "Cytopathology",
      "Feature extraction",
      "Internal medicine",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Wanli"
      },
      {
        "surname": "Li",
        "given_name": "Chen"
      },
      {
        "surname": "Xu",
        "given_name": "Ning"
      },
      {
        "surname": "Jiang",
        "given_name": "Tao"
      },
      {
        "surname": "Rahaman",
        "given_name": "Md Mamunur"
      },
      {
        "surname": "Sun",
        "given_name": "Hongzan"
      },
      {
        "surname": "Wu",
        "given_name": "Xiangchen"
      },
      {
        "surname": "Hu",
        "given_name": "Weiming"
      },
      {
        "surname": "Chen",
        "given_name": "Haoyuan"
      },
      {
        "surname": "Sun",
        "given_name": "Changhao"
      },
      {
        "surname": "Yao",
        "given_name": "Yudong"
      },
      {
        "surname": "Grzegorzek",
        "given_name": "Marcin"
      }
    ]
  },
  {
    "title": "Pseudo loss active learning for deep visual tracking",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108773",
    "abstract": "In visual tracking tasks, the training data are commonly composed of a large number of video sequences and each frame in the sequences needs to be labeled manually, which is labor-intensive and time-consuming. In addition, considering the similarity among the consecutive frames in the same sequence, there is significant redundancy in the training data. To address these problems, a novel pseudo loss active learning (PLAL) method is developed in this paper. PLAL aims to select the most informative and least redundant data for training to reduce the cost of labeling and maintain competitive tracking results simultaneously. Firstly, the Gaussian distribution based pseudo label is generated for the unlabeled candidates based on the tracking model which is initially trained on a small amount of training data. Then, the pseudo loss based on cross entropy is designed to compute the difference between the pseudo label and the target response map. The pseudo loss measures the uncertainty of the target spatial context which is used as the informativeness criterion of the image frame for selection. Meanwhile, a sampling interval threshold and a temporal penalty are employed for frame selection to avoid drastic variation in target appearance and reduce the redundancy within the consecutive candidate frames. Only the selected frames are labeled by the oracle (human expert) and then added to the training data. Extensive experiments on public benchmarks (OTB2013, OTB2015, VOT2018, UAV123, GOT-10K, TrackingNet, LaSOT, OxUvA and TLP) demonstrate that PLAL method outperforms the baseline and other recent active learning approaches. With only 3% of labeled data from the training dataset, PLAL reaches competitive performance (98-100%) compared to the model trained on the entire training dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002540",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Eye tracking",
      "Machine learning",
      "Operating system",
      "Oracle",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Redundancy (engineering)",
      "Software engineering"
    ],
    "authors": [
      {
        "surname": "Cui",
        "given_name": "Zhiyan"
      },
      {
        "surname": "Lu",
        "given_name": "Na"
      },
      {
        "surname": "Wang",
        "given_name": "Weifeng"
      }
    ]
  },
  {
    "title": "Hippocampus-heuristic character recognition network for zero-shot learning in Chinese character recognition",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108818",
    "abstract": "The recognition of Chinese characters has always been a challenging task due to their huge variety and complex structures. The current radical-based methods fail to recognize Chinese characters without learning all of their radicals in the training stage. To this end, we propose a novel Hippocampus-heuristic Character Recognition Network (HCRN), which can recognize unseen Chinese characters only by training part of radicals. More specifically, the network architecture of HCRN is a new pseudo-siamese network designed by us, which can learn features from pairs of input samples and use them to predict unseen characters. The experimental results on the recognition of printed and handwritten characters show that HCRN is robust and effective on zero/few-shot learning tasks. For the printed characters, the mean accuracy of HCRN outperforms the state-of-the-art approach by 23.93% on recognizing unseen characters. For the handwritten characters, HCRN improves the mean accuracy by 11.25% on recognizing unseen characters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002990",
    "keywords": [
      "Artificial intelligence",
      "Character (mathematics)",
      "Character recognition",
      "Chinese characters",
      "Computer science",
      "Engineering",
      "Geometry",
      "Heuristic",
      "Image (mathematics)",
      "Intelligent character recognition",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Speech recognition",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Guanjie"
      },
      {
        "surname": "Luo",
        "given_name": "Xiangyu"
      },
      {
        "surname": "Wang",
        "given_name": "Shaowei"
      },
      {
        "surname": "Gu",
        "given_name": "Tianlong"
      },
      {
        "surname": "Su",
        "given_name": "Kaile"
      }
    ]
  },
  {
    "title": "Towards lifelong object recognition: A dataset and benchmark",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108819",
    "abstract": "Lifelong learning algorithms aim to enable robots to handle open-set and detrimental conditions, and yet there is a lack of adequate datasets with diverse factors for benchmarking. In this work, we constructed and released a lifelong learning robotic vision dataset, OpenLORIS-Object. This dataset was collected by RGB-D camera capturing dynamic environment in daily life scenarios with diverse factors, including illumination, occlusion, object pixel size and clutter, of quantified difficulty levels. To the best of our knowledge, this is an unique real-world dataset for robotic vision with independent and quantifiable environmental factors, which are currently unaccounted for in other lifelong learning datasets such as CORe50 and NICO. We tested 9 state-of-the-art algorithms with 4 evaluation metrics over the dataset in Domain Incremental Learning, Task Incremental Learning, and Class Incremental Learning scenarios. The results demonstrate that these existing algorithms are insufficient to handle lifelong learning task in dynamic environments. Our dataset and benchmarks are now publicly available at this website. 2 2 https://lifelong-robotic-vision.github.io/dataset/object",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003004",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Benchmarking",
      "Business",
      "Class (philosophy)",
      "Clutter",
      "Computer science",
      "Domain (mathematical analysis)",
      "Engineering",
      "Geodesy",
      "Geography",
      "Lifelong learning",
      "Machine learning",
      "Marketing",
      "Mathematical analysis",
      "Mathematics",
      "Object (grammar)",
      "Pedagogy",
      "Programming language",
      "Psychology",
      "Radar",
      "Set (abstract data type)",
      "Systems engineering",
      "Task (project management)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Lan",
        "given_name": "Chuanlin"
      },
      {
        "surname": "Feng",
        "given_name": "Fan"
      },
      {
        "surname": "Liu",
        "given_name": "Qi"
      },
      {
        "surname": "She",
        "given_name": "Qi"
      },
      {
        "surname": "Yang",
        "given_name": "Qihan"
      },
      {
        "surname": "Hao",
        "given_name": "Xinyue"
      },
      {
        "surname": "Mashkin",
        "given_name": "Ivan"
      },
      {
        "surname": "Kei",
        "given_name": "Ka Shun"
      },
      {
        "surname": "Qiang",
        "given_name": "Dong"
      },
      {
        "surname": "Lomonaco",
        "given_name": "Vincenzo"
      },
      {
        "surname": "Shi",
        "given_name": "Xuesong"
      },
      {
        "surname": "Wang",
        "given_name": "Zhengwei"
      },
      {
        "surname": "Guo",
        "given_name": "Yao"
      },
      {
        "surname": "Zhang",
        "given_name": "Yimin"
      },
      {
        "surname": "Qiao",
        "given_name": "Fei"
      },
      {
        "surname": "Chan",
        "given_name": "Rosa H.M."
      }
    ]
  },
  {
    "title": "Classification of emotions using EEG activity associated with different areas of the brain",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.08.018",
    "abstract": "It is important that interactive Affective Brain-Computer Interface (aBCI) applications support some degree of emotional intelligence. Many of the previous works that have explored the use of the Electroencephalogram (EEG) for the purpose of emotion recognition have focused on using data coming from the entire brain. However, the emotional activity in humans is not constant across the brain but varies from one region to another. Therefore, this paper aims to classify human emotions into high/low arousal and high/low valence using EEG and other physiological data from three datasets - DEAP, DREAMER and DASPS, showing the differences in the classification accuracies for different regions of the brain such as frontal lobe, parietal lobe, temporal lobe, occipital lobe, left frontal region, right frontal region, left parietal-temporal-occipital region and the right parietal-temporal-occipital region. The classification experiments are performed using the 1D Convolutional LSTM network and its performance is then compared with two baseline machine learning (ML) algorithms K-Nearest Neighbor and Random Forest. The experimental results show the variation of classification accuracies from one brain region to another. Moreover, they also indicate that the fusion of EEG and peripheral data produces higher emotion classification accuracy as compared to using the EEG and peripheral data separately.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552200263X",
    "keywords": [
      "Arousal",
      "Artificial intelligence",
      "Brain activity and meditation",
      "Computer science",
      "Electroencephalography",
      "Epilepsy",
      "Frontal lobe",
      "Neuroscience",
      "Occipital lobe",
      "Parietal lobe",
      "Pattern recognition (psychology)",
      "Psychology",
      "Temporal lobe"
    ],
    "authors": [
      {
        "surname": "Agarwal",
        "given_name": "Rupal"
      },
      {
        "surname": "Andujar",
        "given_name": "Marvin"
      },
      {
        "surname": "Canavan",
        "given_name": "Shaun"
      }
    ]
  },
  {
    "title": "Discriminative and regularized echo state network for time series classification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108811",
    "abstract": "An echo State Network (ESN) is a special structure of a recurrent neural network (RNN) in which the recurrent neurons are randomly connected. ESN models which have achieved a high accuracy on time series prediction tasks can be used as time series prediction models in many domains. Nevertheless, in most ESN models, the input weights are randomly generated and the output weights calculated by the least square method are susceptible to outliers, which cannot guarantee that the ESN models will always be optimal for a given task. In this paper, a novel discriminative and regularized ESN (DR-ESN) combines discriminative feature aggregation (DFA) and outlier-robust weights (ORW) algorithms are proposed for time series classification. DFA is firstly proposed to replace the random input weights of ESN with the constrained weights generated from sample information. In DFA, weight vectors are selected from the vector space spanned by initial input sequence vectors, then the new generated input weights can adequately represent the data features. Secondly, ORW is employed to enhance the robustness of output weights by constraining the weights assigned to samples with large training errors. The weights evaluation and experiments on a massive set of the synthetic time series data, real-world bearing fault data and UCR benchmarks indicate that the proposed DR-ESN can not only considerably improve the original ESN classifier but also effectively suppress the effect of outliers on classification performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002928",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Discriminative model",
      "Echo state network",
      "Feature vector",
      "Gene",
      "Machine learning",
      "Outlier",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Recurrent neural network",
      "Robustness (evolution)",
      "Series (stratigraphy)",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Heshan"
      },
      {
        "surname": "Liu",
        "given_name": "Yuxi"
      },
      {
        "surname": "Wang",
        "given_name": "Dongshu"
      },
      {
        "surname": "Luo",
        "given_name": "Yong"
      },
      {
        "surname": "Tong",
        "given_name": "Chudong"
      },
      {
        "surname": "Lv",
        "given_name": "Zhaomin"
      }
    ]
  },
  {
    "title": "3D Object Detection for Autonomous Driving: A Survey",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108796",
    "abstract": "Autonomous driving is regarded as one of the most promising remedies to shield human beings from severe crashes. To this end, 3D object detection serves as the core basis of perception stack especially for the sake of path planning, motion prediction, and collision avoidance etc.. Taking a quick glance at the progress we have made, we attribute challenges to visual appearance recovery in the absence of depth information from images, representation learning from partially occluded unstructured point clouds, and semantic alignments over heterogeneous features from cross modalities. Despite existing efforts, 3D object detection for autonomous driving is still in its infancy. Recently, a large body of literature have been investigated to address this 3D vision task. Nevertheless, few investigations have looked into collecting and structuring this growing knowledge. We therefore aim to fill this gap in a comprehensive survey, encompassing all the main concerns including sensors, datasets, performance metrics and the recent state-of-the-art detection methods, together with their pros and cons. Furthermore, we provide quantitative comparisons with the state of the art. A case study on fifteen selected representative methods is presented, involved with runtime analysis, error analysis, and robustness analysis. Finally, we provide concluding remarks after an in-depth analysis of the surveyed works and identify promising directions for future work.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002771",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Data science",
      "Economics",
      "Finance",
      "Gene",
      "Machine learning",
      "Modalities",
      "Object detection",
      "Point cloud",
      "Robustness (evolution)",
      "Segmentation",
      "Social science",
      "Sociology",
      "Structuring"
    ],
    "authors": [
      {
        "surname": "Qian",
        "given_name": "Rui"
      },
      {
        "surname": "Lai",
        "given_name": "Xin"
      },
      {
        "surname": "Li",
        "given_name": "Xirong"
      }
    ]
  },
  {
    "title": "General nonconvex total variation and low-rank regularizations: Model, algorithm and applications",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108692",
    "abstract": "Total Variation and Low-Rank regularizations have shown significant successes in machine learning, data mining, and image processing in past decades. This paper develops the general nonconvex composite regularized model, which contains previous regularizers and motivates novel ones. Although the classical Alternating Direction Methods of Multiplier (ADMM) algorithm is applicable for this model, the nonconvexity of the problem and the complicacy of choosing the parameters increase the difficulty in the use of ADMM. Thus, by the penalty method, we propose the Alternating Minimization (AM) algorithm, whose convergence results are proved under mild assumptions. The proposed model and algorithm are applied to the image restoration problem. Numerical results demonstrate the efficiency of our model and algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200173X",
    "keywords": [
      "Algorithm",
      "Astrophysics",
      "Combinatorics",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Macroeconomics",
      "Mathematical optimization",
      "Mathematics",
      "Minification",
      "Multiplier (economics)",
      "Physics",
      "Rank (graph theory)",
      "Variation (astronomy)"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Tao"
      },
      {
        "surname": "Li",
        "given_name": "Dongsheng"
      }
    ]
  },
  {
    "title": "The CP‐ABM approach for modelling COVID‐19 infection dynamics and quantifying the effects of non‐pharmaceutical interventions",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108790",
    "abstract": "The motivation for this research is to develop an approach that reliably captures the disease dynamics of COVID-19 for an entire population in order to identify the key events driving change in the epidemic through accurate estimation of daily COVID-19 cases. This has been achieved through the new CP-ABM approach which uniquely incorporates Change Point detection into an Agent Based Model taking advantage of genetic algorithms for calibration and an efficient infection centric procedure for computational efficiency. The CP-ABM is applied to the Northern Ireland population where it successfully captures patterns in COVID-19 infection dynamics over both waves of the pandemic and quantifies the significant effects of non-pharmaceutical interventions (NPI) on a national level for lockdowns and mask wearing. To our knowledge, there is no other approach to date that has captured NPI effectiveness and infection spreading dynamics for both waves of the COVID-19 pandemic for an entire country population.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002710",
    "keywords": [
      "Artificial intelligence",
      "Calibration",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Disease",
      "Dynamics (music)",
      "Econometrics",
      "Environmental health",
      "Infectious disease (medical specialty)",
      "Machine learning",
      "Mathematics",
      "Medicine",
      "Pandemic",
      "Pathology",
      "Pedagogy",
      "Population",
      "Psychiatry",
      "Psychological intervention",
      "Psychology",
      "Risk analysis (engineering)",
      "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Novakovic",
        "given_name": "Aleksandar"
      },
      {
        "surname": "Marshall",
        "given_name": "Adele H."
      }
    ]
  },
  {
    "title": "Time-varying Group Lasso Granger Causality Graph for High Dimensional Dynamic system",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108789",
    "abstract": "Feature selection is a crucial preprocessing step in data analysis and machine learning. Since causal relationships imply the underlying mechanism of a system, causality-based feature selection methods have gradually attracted great attentions. For a high dimensional system undergoing dynamic transformation, because of the non-stationarity and sample scarcity, modeling the causal structure among these features is difficult. In this paper, we propose a time-varying Granger causal networks to capture the causal relations underlying high dimensional time-varying vector autoregressive models with high order lagged dependence. A kernel reweighted group lasso method is proposed, which overcomes the limitations of sample scarcity and transforms the problem of Granger causal structural learning into a group variable selection problem. The asymptotic consistency of the proposed algorithm is proved. We apply the time-varying Granger causal networks to simulation experiments and real data in the financial market. The study demonstrates that the method provides an efficient tool to detect changes and analysis characters of causal dependency structure in network evolution.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002709",
    "keywords": [
      "Artificial intelligence",
      "Autoregressive model",
      "Computer science",
      "Econometrics",
      "Feature selection",
      "Granger causality",
      "Lasso (programming language)",
      "Machine learning",
      "Mathematics",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Wei"
      },
      {
        "surname": "Yang",
        "given_name": "Haizhong"
      }
    ]
  },
  {
    "title": "Visual-to-EEG cross-modal knowledge distillation for continuous emotion recognition",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108833",
    "abstract": "Visual modality is one of the most dominant modalities for current continuous emotion recognition methods. Compared to which the EEG modality is relatively less sound due to its intrinsic limitation such as subject bias and low spatial resolution. This work attempts to improve the continuous prediction of the EEG modality by using the dark knowledge from the visual modality. The teacher model is built by a cascade convolutional neural network - temporal convolutional network (CNN-TCN) architecture, and the student model is built by TCNs. They are fed by video frames and EEG average band power features, respectively. Two data partitioning schemes are employed, i.e., the trial-level random shuffling (TRS) and the leave-one-subject-out (LOSO). The standalone teacher and student can produce continuous prediction superior to the baseline method, and the employment of the visual-to-EEG cross-modal KD further improves the prediction with statistical significance, i.e., p -value < 0.01 for TRS and p -value < 0.05 for LOSO partitioning. The saliency maps of the trained student model show that the brain areas associated with the active valence state are not located in precise brain areas. Instead, it results from synchronized activity among various brain areas. And the fast beta and gamma waves, with the frequency of 18 − 30 H z and 30 − 45 H z , contribute the most to the human emotion process compared to other bands. The code is available at https://github.com/sucv/Visual_to_EEG_Cross_Modal_KD_for_CER.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003144",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Convolutional neural network",
      "Electroencephalography",
      "Modal",
      "Modality (human–computer interaction)",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Polymer chemistry",
      "Psychology",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Su"
      },
      {
        "surname": "Tang",
        "given_name": "Chuangao"
      },
      {
        "surname": "Guan",
        "given_name": "Cuntai"
      }
    ]
  },
  {
    "title": "Enforced block diagonal subspace clustering with closed form solution",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108791",
    "abstract": "Subspace clustering aims to fit each category of data points by learning an underlying subspace and then conduct clustering according to the learned subspace. Ideally, the learned subspace is expected to be block diagonal such that the similarities between clusters are zeros. In this paper, we provide the explicit theoretical connection between spectral clustering and the subspace clustering based on block diagonal representation. We propose Enforced Block Diagonal Subspace Clustering (EBDSC) and show that the spectral clustering with the Radial Basis Function kernel can be regarded as EBDSC. Compared with the exiting subspace clustering methods, an analytical, nonnegative and symmetrical solution can be obtained by EBDSC. An important difference with respect to the existing ones is that our model is a more general case. EBDSC directly uses the obtained solution as the similarity matrix, which can avoid the complex computation of the optimization program. Then the solution obtained by the proposed method can be used for the final clustering. Finally, we provide the experimental analysis to show the efficiency and effectiveness of our method on the synthetic data and several benchmark data sets in terms of different metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002722",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Block matrix",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Correlation clustering",
      "Diagonal",
      "Eigenvalues and eigenvectors",
      "Geometry",
      "Image (mathematics)",
      "Kernel (algebra)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Similarity (geometry)",
      "Spectral clustering",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Qin",
        "given_name": "Yalan"
      },
      {
        "surname": "Wu",
        "given_name": "Hanzhou"
      },
      {
        "surname": "Zhao",
        "given_name": "Jian"
      },
      {
        "surname": "Feng",
        "given_name": "Guorui"
      }
    ]
  },
  {
    "title": "Cross-modality attentive feature fusion for object detection in multispectral remote sensing imagery",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108786",
    "abstract": "Cross-modality fusing complementary information of multispectral remote sensing image pairs can improve the perception ability of detection algorithms, making them more robust and reliable for a wider range of applications, such as nighttime detection. Compared with prior methods, we think different features should be processed specifically, the modality-specific features should be retained and enhanced, while the modality-shared features should be cherry-picked from the RGB and thermal IR modalities. Following this idea, a novel and lightweight multispectral feature fusion approach with joint common-modality and differential-modality attentions are proposed, named Cross-Modality Attentive Feature Fusion (CMAFF). Given the intermediate feature maps of RGB and thermal images, our module parallel infers attention maps from two separate modalities, common- and differential-modality, then the attention maps are multiplied to the input feature map respectively for adaptive feature enhancement or selection. Extensive experiments demonstrate that our proposed approach can achieve the state-of-the-art performance at a low computation cost.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002679",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Feature selection",
      "Image (mathematics)",
      "Image fusion",
      "Linguistics",
      "Modalities",
      "Modality (human–computer interaction)",
      "Multispectral image",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "RGB color model",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Qingyun",
        "given_name": "Fang"
      },
      {
        "surname": "Zhaokui",
        "given_name": "Wang"
      }
    ]
  },
  {
    "title": "The iterative convolution–thresholding method (ICTM) for image segmentation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108794",
    "abstract": "Variational methods, which have been tremendously successful in image segmentation, work by minimizing a given objective functional. The objective functional usually consists of a fidelity term and a regularization term. Because objective functionals may vary from different types of images, developing an efficient, simple, and general numerical method to minimize them has become increasingly vital. However, many existing methods are model-based, converge relatively slowly, or involve complicated techniques. In this paper, we develop a novel iterative convolution–thresholding method (ICTM) that is simple, efficient, and applicable to a wide range of variational models for image segmentation. In ICTM, the interface between two different segment domains is implicitly represented by the characteristic functions of domains. The fidelity term is usually written into a linear functional of the characteristic functions, and the regularization term is approximated by a functional of characteristic functions in terms of heat kernel convolution. This allows us to design an iterative convolution–thresholding method to minimize the approximate energy. The method has the energy-decaying property, and thus the unconditional stability is theoretically guaranteed. Numerical experiments show that the method is simple, easy to implement, robust, and applicable to various image segmentation models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002758",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Convolution (computer science)",
      "Energy functional",
      "Epistemology",
      "Fidelity",
      "Image (mathematics)",
      "Image segmentation",
      "Iterative method",
      "Kernel (algebra)",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Philosophy",
      "Regularization (linguistics)",
      "Segmentation",
      "Simple (philosophy)",
      "Telecommunications",
      "Thresholding"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Dong"
      },
      {
        "surname": "Wang",
        "given_name": "Xiao-Ping"
      }
    ]
  },
  {
    "title": "GasHis-Transformer: A multi-scale visual transformer approach for gastric histopathological image detection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108827",
    "abstract": "In this paper, a multi-scale visual transformer model, referred as GasHis-Transformer, is proposed for Gastric Histopathological Image Detection (GHID), which enables the automatic global detection of gastric cancer images. GasHis-Transformer model consists of two key modules designed to extract global and local information using a position-encoded transformer model and a convolutional neural network with local convolution, respectively. A publicly available hematoxylin and eosin (H&E) stained gastric histopathological image dataset is used in the experiment. Furthermore, a Dropconnect based lightweight network is proposed to reduce the model size and training time of GasHis-Transformer for clinical applications with improved confidence. Moreover, a series of contrast and extended experiments verify the robustness, extensibility and stability of GasHis-Transformer. In conclusion, GasHis-Transformer demonstrates high global detection performance and shows its significant potential in GHID task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003089",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Gene",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Haoyuan"
      },
      {
        "surname": "Li",
        "given_name": "Chen"
      },
      {
        "surname": "Wang",
        "given_name": "Ge"
      },
      {
        "surname": "Li",
        "given_name": "Xiaoyan"
      },
      {
        "surname": "Mamunur Rahaman",
        "given_name": "Md"
      },
      {
        "surname": "Sun",
        "given_name": "Hongzan"
      },
      {
        "surname": "Hu",
        "given_name": "Weiming"
      },
      {
        "surname": "Li",
        "given_name": "Yixin"
      },
      {
        "surname": "Liu",
        "given_name": "Wanli"
      },
      {
        "surname": "Sun",
        "given_name": "Changhao"
      },
      {
        "surname": "Ai",
        "given_name": "Shiliang"
      },
      {
        "surname": "Grzegorzek",
        "given_name": "Marcin"
      }
    ]
  },
  {
    "title": "Directly solving normalized cut for multi-view data",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108809",
    "abstract": "Graph-based multi-view clustering, which aims to uncover clusters from multi-view data with graph clustering technique, is one of the most important multi-view clustering methods. Such methods usually perform eigen-decomposition first to solve the relaxed problem and then obtain the final cluster indicator matrix from eigenvectors by k -means or spectral rotation. However, such a two-step process may result in undesired clustering result since the two steps aim to solve different problems. In this paper, we propose a k -way normalized cut method for multi-view data, named as the Multi-view Discrete Normalized Cut (MDNC). The new method learns a set of implicit weights for each view to identify its quality, and a novel iterative algorithm is proposed to directly solve the new model without relaxation and post-processing. Moreover, we propose a new method to adjust the distribution of the implicit view weights to obtain better clustering result. Extensive experimental results show that the performance of our approach is superior to the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002904",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Eigenvalues and eigenvectors",
      "Graph",
      "Physics",
      "Programming language",
      "Psychology",
      "Quantum mechanics",
      "Relaxation (psychology)",
      "Set (abstract data type)",
      "Social psychology",
      "Spectral clustering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Chen"
      },
      {
        "surname": "Chen",
        "given_name": "Xiaojun"
      },
      {
        "surname": "Nie",
        "given_name": "Feiping"
      },
      {
        "surname": "Huang",
        "given_name": "Joshua Zhexue"
      }
    ]
  },
  {
    "title": "Learning attention-guided pyramidal features for few-shot fine-grained recognition",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108792",
    "abstract": "Few-shot fine-grained recognition (FS-FGR) aims to distinguish several highly similar objects from different sub-categories with limited supervision. However, traditional few-shot learning solutions typically exploit image-level features and are committed to capturing global silhouettes while accidentally ignore to exploring local details, resulting in an inevitable problem of inconspicuous but distinguishable information loss. Thus, how to effectively address the fine-grained recognition issue given limited samples still remains a major challenging. In this article, we tend to propose an effective bidirectional pyramid architecture to enhance internal representations of features to cater to fine-grained image recognition task in the few-shot learning scenario. Specifically, we deploy a multi-scale feature pyramid and a multi-level attention pyramid on the backbone network, and progressively aggregated features from different granular spaces via both of them. We then further present an attention-guided refinement strategy in collaboration with a multi-level attention pyramid to reduce the uncertainty brought by backgrounds conditioned by limited samples. In addition, the proposed method is trained with the meta-learning framework in an end-to-end fashion without any extra supervision. Extensive experimental results on four challenging and widely-used fine-grained benchmarks show that the proposed method performs favorably against state-of-the-arts, especially in the one-shot scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002734",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Deep learning",
      "Engineering",
      "Exploit",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Mechanical engineering",
      "One shot",
      "Optics",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pyramid (geometry)",
      "Shot (pellet)",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Hao"
      },
      {
        "surname": "Yuan",
        "given_name": "Chengcheng"
      },
      {
        "surname": "Li",
        "given_name": "Zechao"
      },
      {
        "surname": "Tang",
        "given_name": "Jinhui"
      }
    ]
  },
  {
    "title": "Multimodal channel-wise attention transformer inspired by multisensory integration mechanisms of the brain",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108837",
    "abstract": "Multisensory integration has attracted intense studies for decades. How to combine visual and auditory information to optimize perception and decision-making is a key question in neuroscience as well as machine learning. Inspired by the mechanisms of multisensory integration in the brain, we propose a multimodal channel-wise attention transformer (MCAT) that performs reliability-weighted integration and revises the weights allocation according to a top-down attention-like mechanism. We apply MCAT on EF-LSTM neural networks for a fine-grained video bird recognition task, and on MulT neural networks for an emotion recognition task. The performance of both models is improved remarkably. Ablation study shows that the attention mechanism is indispensable for effective multisensory integration. Moreover, we found that cross-modal integration models are in accordance with the law of inverse effectiveness of multisensory integration in the brain, which reveals that our model may have mechanisms similar to those in the brain. Taken together, the results demonstrate that the brain-inspired MCAT block is effective for improving multisensory integration, providing useful clues for designing new algorithms and understanding multisensory integration in the brain.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003181",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Economics",
      "Epistemology",
      "Human–computer interaction",
      "Information integration",
      "Machine learning",
      "Management",
      "Mechanism (biology)",
      "Multisensory integration",
      "Neuroscience",
      "Perception",
      "Philosophy",
      "Psychology",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Qianqian"
      },
      {
        "surname": "Fan",
        "given_name": "Junsong"
      },
      {
        "surname": "Wang",
        "given_name": "Zuoren"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhaoxiang"
      }
    ]
  },
  {
    "title": "Learning a deep dual-level network for robust DeepFake detection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108832",
    "abstract": "Face manipulation techniques, especially DeepFake techniques, are causing severe social concerns and security problems. When faced with skewed data distributions such as those found in the real world, existing DeepFake detection methods exhibit significantly degraded performance, especially the AUC score. In this paper, we focus on DeepFake detection in real-world situations. We propose a dual-level collaborative framework to detect frame-level and video-level forgeries simultaneously with a joint loss function to optimize both the AUC score and error rate at the same time. Our experiments indicate that the AUC loss boosts imbalanced learning performance and outperforms focal loss, a state-of-the-art loss function to address imbalanced data. In addition, our multitask structure enables mutual reinforcement of frame-level and video-level detection and achieves outstanding performance in imbalanced learning. Our proposed method is also more robust to video quality variations and shows better generalization ability in cross-dataset evaluations than existing DeepFake detection methods. Our implementation is available online at https://github.com/PWB97/Deepfake-detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003132",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Dual (grammatical number)",
      "Face (sociological concept)",
      "Frame (networking)",
      "Generalization",
      "Literature",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Reinforcement learning",
      "Social science",
      "Sociology",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Pu",
        "given_name": "Wenbo"
      },
      {
        "surname": "Hu",
        "given_name": "Jing"
      },
      {
        "surname": "Wang",
        "given_name": "Xin"
      },
      {
        "surname": "Li",
        "given_name": "Yuezun"
      },
      {
        "surname": "Hu",
        "given_name": "Shu"
      },
      {
        "surname": "Zhu",
        "given_name": "Bin"
      },
      {
        "surname": "Song",
        "given_name": "Rui"
      },
      {
        "surname": "Song",
        "given_name": "Qi"
      },
      {
        "surname": "Wu",
        "given_name": "Xi"
      },
      {
        "surname": "Lyu",
        "given_name": "Siwei"
      }
    ]
  },
  {
    "title": "A multi-embedding neural model for incident video retrieval",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108807",
    "abstract": "Many internet search engines have been developed, however, the retrieval of video clips remains a challenge. This paper considers the retrieval of incident videos, which may contain more spatial and temporal semantics. We propose an encoder-decoder ConvLSTM model that explores multiple embeddings of a video to facilitate comparison of similarity between a pair of videos. The model is able to encode a video into an embedding that integrates both its spatial information and temporal semantics. Multiple video embeddings are then generated from coarse- and fine-grained features of a video to capture high- and low-level meanings. Subsequently, a learning-based comparative model is proposed to compare the similarity of two videos based on their embeddings. Extensive evaluations are presented and show that our model outperforms state-of-the-art methods for several video retrieval tasks on the FIVR-200K, CC_WEB_VIDEO, and EVVE datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002886",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "ENCODE",
      "Embedding",
      "Encoder",
      "Gene",
      "Image (mathematics)",
      "Information retrieval",
      "Operating system",
      "Pattern recognition (psychology)",
      "Programming language",
      "Semantics (computer science)",
      "Similarity (geometry)",
      "Video processing",
      "Video retrieval",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Chiang",
        "given_name": "Ting-Hui"
      },
      {
        "surname": "Tseng",
        "given_name": "Yi-Chun"
      },
      {
        "surname": "Tseng",
        "given_name": "Yu-Chee"
      }
    ]
  },
  {
    "title": "Application of DNN for radar micro-doppler signature-based human suspicious activity recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.08.005",
    "abstract": "The non-availability of open-source datasets covering various human suspicious activities and well-trained deep learning (DL) architecture limits the effective utilization of DL network-supported radar systems for real-time autonomous human activity recognition (HAR). The development of a dataset and validation of its micro-Doppler-signature-distinguishable-features by a suitable DL network becomes significant, and that is the key contribution given in this research. In this work, an indigenously developed X-band CW radar is employed to create a diverse DIAT- μ RadHAR dataset, which includes (a) army marching, (b) Stone pelting/Grenades throwing, (c) jumping with holding a gun, (d) army Jogging, (e) army crawling and (f) boxing activities. Six Pre-trained CNNs models supported DL architectures, trained with DIAT- μ RadHAR dataset containing 3780 m-D images, are proposed, which can be used for open-field HAR. The characteristics of our dataset and performance analysis of the proposed two DL architectures are statistically computed, and their results are compared in terms of receiver operating characteristic (ROC), precision, F1-score, recall, and confusion matrix. The pre-trained VGG19 with transfer learning outperforms other CNNs with an overall classification accuracy of 98%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002434",
    "keywords": [
      "Activity recognition",
      "Anatomy",
      "Artificial intelligence",
      "Computer science",
      "Confusion matrix",
      "Crawling",
      "Deep learning",
      "Machine learning",
      "Medicine",
      "Pattern recognition (psychology)",
      "Radar",
      "Receiver operating characteristic",
      "Telecommunications",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Chakraborty",
        "given_name": "Mainak"
      },
      {
        "surname": "Kumawat",
        "given_name": "Harish C."
      },
      {
        "surname": "Dhavale",
        "given_name": "Sunita Vikrant"
      },
      {
        "surname": "Raj A",
        "given_name": "Arockia Bazil"
      }
    ]
  },
  {
    "title": "Detection of Intrusion behavior in cloud applications using Pearson's chi-squared distribution and decision tree classifiers",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.08.008",
    "abstract": "The information transmission in machine learning is a challengable task which must be performed productively in cloud based fog applications. There is a few methodologies has been pronounced for the issue of information transmission in cloudbased machines, which utilizes a few techniques and components in the choice of portable sensor hubs which has information put away. However, not just the decision could work on the presentation of information transmission, and there are such countless elements which influence information transmission in a roundabout way. The proposed technique keeps up with the rundown of versatile fog nodes and the follow about their information accessibility, unwavering quality, and the last time window information assortment performed number of supporting hub accessible in wakeup mode. In light of all the above factors the strategy registers the proficient information transmission for every one of the area considered, and at every locale, the technique computes the information accessibility measure for various information hubs to choose them for information assortment. At first utilized Pearson's Chi-Squared Distribution for choosing the ideal highlights from dataset. Then, at that point, human-in the loop experiment is proposed for arranging the hubs in an organization as typical or strange which brings about further developed intrusion discovery rate and time. The recreation of human-in-the loop experiment is directed on parameters, such as, intrusion detection rate, Data transmission proportion, intrusion identification time and prediction rate. The evaluated result shows that the proposed work performs better compared to different strategies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552200246X",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Cloud computing",
      "Computer science",
      "Data mining",
      "Decision tree",
      "Identification (biology)",
      "Intrusion detection system",
      "Machine learning",
      "Operating system",
      "Telecommunications",
      "Transmission (telecommunications)"
    ],
    "authors": [
      {
        "surname": "N",
        "given_name": "Sathish"
      },
      {
        "surname": "K",
        "given_name": "Valarmathi"
      }
    ]
  },
  {
    "title": "Spatio-temporal convolutional emotional attention network for spotting macro- and micro-expression intervals in long video sequences",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.09.008",
    "abstract": "Emotional detection based on facial micro-expressions is essential in high-risk tasks such as criminal investigation or lie detection. However, micro-expressions often occur in high-risk tasks when people often use facial expressions to conceal their actual emotional states. Therefore, spotting macro- and micro-expression intervals in long video sequences has become hot research. Considering the difference in duration and facial muscle movement intensity between macro- and micro-expression, we propose a novel Spatio-temporal Convolutional Emotional Attention Network (STCEAN) for spotting macro- and micro-expression intervals in long video sequences. The spatial features of each frame in the video sequence are extracted through the convolution neural network. Then the emotional self-attention model is used to analyze the temporal weights of spatial features in different emotional dimensions. The emotional weights in the temporal dimension are filtered for spotting macro- and micro-expressions intervals. Finally, the STCEAN model is jointly optimized by the dual emotional focal loss of macro- and micro-expression to solve the problem of sample unbalance. The experimental results on the CAS(ME)2 and SAMM-LV datasets show that the STCEAN model achieves competitive results in the Facial Micro-Expression Challenge 2021.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002707",
    "keywords": [
      "Artificial intelligence",
      "Cognitive psychology",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Emotional expression",
      "Expression (computer science)",
      "Facial expression",
      "Frame (networking)",
      "Keyword spotting",
      "Macro",
      "Pattern recognition (psychology)",
      "Programming language",
      "Psychology",
      "Speech recognition",
      "Spotting",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Pan",
        "given_name": "Hang"
      },
      {
        "surname": "Xie",
        "given_name": "Lun"
      },
      {
        "surname": "Wang",
        "given_name": "Zhiliang"
      }
    ]
  },
  {
    "title": "Online multiple object tracking using joint detection and embedding network",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108793",
    "abstract": "Multiple object tracking (MOT) generally employs the paradigm of tracking-by-detection, where object detection and object tracking are executed conventionally using separate systems. Current progress in MOT has focused on detecting and tracking objects by harnessing the representational power of deep learning. Since existing methods always combine two submodules in the same network, it is particularly important that they must be trained effectively together. Therefore, the development of a suitable network architecture for the end-to-end joint training of detection and tracking submodules remains a challenging issue. The present work addresses this issue by proposing a novel architecture denoted as YOLOTracker that performs online MOT by exploiting a joint detection and embedding network. First, an efficient and powerful joint detection and tracking model is constructed to accomplish instance-level embedded training, which can ensure that the proposed tracker achieves highly accurate MOT results with high efficiency. Then, the Path Aggregation Network is employed to combine low-resolution and high-resolution features for integrating textural features and semantic information and mitigating the misalignment of the re-identification features. Experiments are conducted on three challenging and publicly available benchmark datasets and results demonstrate the proposed tracker outperforms other state-of-the-art MOT trackers in terms of accuracy and efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002746",
    "keywords": [
      "Architectural engineering",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "BitTorrent tracker",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Embedding",
      "Engineering",
      "Eye tracking",
      "Geodesy",
      "Geography",
      "Joint (building)",
      "Network architecture",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Psychology",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Chan",
        "given_name": "Sixian"
      },
      {
        "surname": "Jia",
        "given_name": "Yangwei"
      },
      {
        "surname": "Zhou",
        "given_name": "Xiaolong"
      },
      {
        "surname": "Bai",
        "given_name": "Cong"
      },
      {
        "surname": "Chen",
        "given_name": "Shengyong"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaoqin"
      }
    ]
  },
  {
    "title": "Model scheduling and sample selection for ensemble adversarial example attacks",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108824",
    "abstract": "Adversarial examples refer to the malicious inputs that can mislead deep neural networks (DNNs) to falsely classify them. In practice, some adversarial examples are transferable and hence can deceive different target models. In multi-stage ensemble adversarial example attacks, adversaries can generate strongly transferable adversarial examples through iteratively perturbing legitimate examples to attack well-trained source models in a white-box manner. Limited by computational and memory resources (e.g., GPU memory), however, adversaries cannot handle all models and all legitimate examples at a time. This brings an important but never studied research issue: how to optimally schedule source models and appropriately select samples to improve adversarial example transferability and reduce unnecessary computational overheads? To shed light on this problem, we develop a novel multi-stage ensemble adversarial example attack method based on our proposed strategies of model scheduling and sample selection. The first strategy schedules source models to be attacked in every stage, based on the criteria of decision boundary similarity and model diversity. The second selects input samples to be handled by ensemble attacks, according to their sensitivity level for adversarial perturbations. To our knowledge, we are the first to study model scheduling and sample selection for multi-stage ensemble attacks. We conduct extensive experiments on three datasets with a variety of source and target models. Experiments show that our model scheduling based ensemble attack outperforms the all-model ensemble attack and the state-of-the-art ensemble attacks SCES, SMBEA and EnsembleFool in transferability. Moreover, our sample selection strategy improves attack success rate by about 138 % .",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003053",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Scheduling (production processes)",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Zichao"
      },
      {
        "surname": "Li",
        "given_name": "Heng"
      },
      {
        "surname": "Yuan",
        "given_name": "Liheng"
      },
      {
        "surname": "Cheng",
        "given_name": "Zhang"
      },
      {
        "surname": "Yuan",
        "given_name": "Wei"
      },
      {
        "surname": "Zhu",
        "given_name": "Ming"
      }
    ]
  },
  {
    "title": "DMT: Dynamic mutual training for semi-supervised learning",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108777",
    "abstract": "Recent semi-supervised learning methods use pseudo supervision as core idea, especially self-training methods that generate pseudo labels. However, pseudo labels are unreliable. Self-training methods usually rely on single model prediction confidence to filter low-confidence pseudo labels, thus remaining high-confidence errors and wasting many low-confidence correct labels. In this paper, we point out it is difficult for a model to counter its own errors. Instead, leveraging inter-model disagreement between different models is a key to locate pseudo label errors. With this new viewpoint, we propose mutual training between two different models by a dynamically re-weighted loss function, called Dynamic Mutual Training (DMT). We quantify inter-model disagreement by comparing predictions from two different models to dynamically re-weight loss in training, where a larger disagreement indicates a possible error and corresponds to a lower loss value. Extensive experiments show that DMT achieves state-of-the-art performance in both image classification and semantic segmentation. Our codes are released at https://github.com/voldemortX/DST-CBC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002588",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Code (set theory)",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Core (optical fiber)",
      "Evolutionary biology",
      "Filter (signal processing)",
      "Function (biology)",
      "Geometry",
      "Key (lock)",
      "Machine learning",
      "Mathematics",
      "Meteorology",
      "Pattern recognition (psychology)",
      "Physics",
      "Point (geometry)",
      "Programming language",
      "Segmentation",
      "Set (abstract data type)",
      "Telecommunications",
      "Training (meteorology)"
    ],
    "authors": [
      {
        "surname": "Feng",
        "given_name": "Zhengyang"
      },
      {
        "surname": "Zhou",
        "given_name": "Qianyu"
      },
      {
        "surname": "Gu",
        "given_name": "Qiqi"
      },
      {
        "surname": "Tan",
        "given_name": "Xin"
      },
      {
        "surname": "Cheng",
        "given_name": "Guangliang"
      },
      {
        "surname": "Lu",
        "given_name": "Xuequan"
      },
      {
        "surname": "Shi",
        "given_name": "Jianping"
      },
      {
        "surname": "Ma",
        "given_name": "Lizhuang"
      }
    ]
  },
  {
    "title": "Novel hyperbolic clustering-based band hierarchy (HCBH) for effective unsupervised band selection of hyperspectral images",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108788",
    "abstract": "For dimensionality reduction of HSI, many clustering-based unsupervised band selection (UBS) methods have been proposed due to their superiority of reducing the high redundancy between selected bands. However, most of these methods fail to reflect the data structure of HSI, leading to inconsistent results of band selection. To tackle this particular issue, we have proposed a novel hyperbolic clustering-based band hierarchy (HCBH) to fully represent the underlying spectral structure and obtain a more consistent band selection. With the proposed adaptive hyperbolic clustering, the performance can be effectively improved with the aid of geometrical information. By introducing a cluster-centre based ranking metric, the desired band subset can be naturally obtained during the clustering process. Experimental results on three popularly used datasets have validated the superior performance of the proposed approach, which outperforms a few state-of-the-art (SOTA) UBS approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002692",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Curse of dimensionality",
      "Data mining",
      "Dimensionality reduction",
      "Hyperspectral imaging",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Ranking (information retrieval)",
      "Redundancy (engineering)",
      "Selection (genetic algorithm)",
      "Spectral clustering"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "He"
      },
      {
        "surname": "Zhang",
        "given_name": "Lei"
      },
      {
        "surname": "Ren",
        "given_name": "Jinchang"
      },
      {
        "surname": "Huang",
        "given_name": "Hua"
      }
    ]
  },
  {
    "title": "Identifying the key frames: An attention-aware sampling method for action recognition",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108797",
    "abstract": "Deep learning based methods have achieved remarkable progress in action recognition. Existing works mainly focus on designing novel deep architectures to learn video representations for action recognition. Most existing methods treat sampled frames equally and average all the frame-level predictions to generate video-level predictions at the testing stage. However, within a video, discriminative actions may occur sparsely in a few frames whereas most other frames are irrelevant to the ground truth which may even lead to wrong results. As a result, we think that the strategy of selecting relevant frames would be a further important key to enhance the existing deep learning based action recognition. In this paper, we propose an attention-aware sampling method for action recognition, which aims to discard the irrelevant and misleading frames and preserve the most discriminative frames. We formulate the process of mining key frames from videos as a Markov decision process and train the attention agent through deep reinforcement learning without extra labels. The agent takes features and predictions from the baseline model as inputs and generates importance scores for all frames. Moreover, our approach is extensible, which can be applied to different existing deep learning based action recognition models. We achieve very competitive action recognition performance on two widely used action recognition datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002783",
    "keywords": [
      "Action (physics)",
      "Action recognition",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Computer security",
      "Deep learning",
      "Discriminative model",
      "Frame (networking)",
      "Key (lock)",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Process (computing)",
      "Quantum mechanics",
      "Reinforcement learning",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Dong",
        "given_name": "Wenkai"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhaoxiang"
      },
      {
        "surname": "Song",
        "given_name": "Chunfeng"
      },
      {
        "surname": "Tan",
        "given_name": "Tieniu"
      }
    ]
  },
  {
    "title": "An entity-weights-based convolutional neural network for large-sale complex knowledge embedding",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108841",
    "abstract": "Knowledge graph (KG) has increasingly been seen as a significant resource in financial applications (e.g., risk control, auditing and anti-fraud). However, there are few prior studies that focus on multi-relational circles, extracting additional information under the completed KG and selecting similarity measures for knowledge representation. In this paper, we introduce multi-relational circles and propose a novel embedding model, which considers entity weights calculated by PageRank algorithm to improve TransE method. In order to extract additional information, we use entity weights to convert embeddings into an on-map mining problem, and propose a model called CNNe based on entity weights and a convolutional neural network with three hidden layers, which converts vectors of entities, entity weights and relationships into matrices to perform link prediction in the same way as image processing. With the help of ten different similarity measures, it is demonstrated that the choice of distance measure greatly effect the results of the translation embedding models. Moreover, we propose two embedding methods, sMFE and tMFE, to enhance the results using matrix factorization. The complete incidence matrix is first applied to knowledge embedding, which contains the most comprehensive topological properties of the graph. Experimental results on standard benchmark datasets demonstrate that the proposed models are effective. In particular, CNNe achieves a mean rank of 166 less than the baseline method and an improvement of 2.1% on the proportion of correct entities ranked in the top ten on YAGO3-10 dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003223",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Eigenvalues and eigenvectors",
      "Embedding",
      "Graph",
      "Matrix decomposition",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zhengdi"
      },
      {
        "surname": "Yang",
        "given_name": "Lvqing"
      },
      {
        "surname": "Lei",
        "given_name": "Zhenfeng"
      },
      {
        "surname": "Ul Haq",
        "given_name": "Anwar"
      },
      {
        "surname": "Zhang",
        "given_name": "Defu"
      },
      {
        "surname": "Yang",
        "given_name": "Shuangyuan"
      },
      {
        "surname": "Francis",
        "given_name": "Akindipe Olusegun"
      }
    ]
  },
  {
    "title": "A novel part-level feature extraction method for fine-grained vehicle recognition",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108869",
    "abstract": "In this paper, we propose a novel part-level feature extraction method to enhance the discriminative ability of deep convolutional features for the task of fine-grained vehicle recognition. Generally, the challenges for fine-grained vehicle recognition are mainly caused by the subtle visual differences between part regions of vehicles. Therefore, it is essential to extract discriminative features from part regions. Many existing methods, especially deep convolutional neural networks (D-CNNs), tend to detect the discriminative part regions explicitly or learn the part information implicitly through network restructuring and neglect the abundant part-level information contained in the high-level features generated by CNNs. In light of this, we propose a simple and effective part-level feature extraction method to enhance the representation of part-level features within the global features of target object generated by the backbone networks. The proposed method is built on the deep convolutional layers from which the discriminative part features could be integrated and extracted accordingly. More specifically, a basic feature grouping module is adopted to integrate the feature maps of deep convolutional layers into groups in each of which the related discriminative parts are assembled. The feature grouping process is performed in a multi-stage manner to ensure the integration process. Then a fusion module follows to model the coarse-to-fine relationship of the part features and further ensure the integrity and effectiveness of the part features. We conduct comparison experiments on public datasets, and the results show that the proposed method achieves comparable performance with state-of-the-art algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003508",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature extraction",
      "Law",
      "Linguistics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Process (computing)",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Lei"
      },
      {
        "surname": "Wang",
        "given_name": "Ping"
      },
      {
        "surname": "Cao",
        "given_name": "Yijie"
      }
    ]
  },
  {
    "title": "Robust structural similarity index measure for images with non-Gaussian distortions",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.09.011",
    "abstract": "Structural Similarity Index Measure (SSIM) has been a very successful tool in image processing and computer vision. After almost two decades, it is still the most popular method for quantifying the similarity between original and distorted images with applications in image quality assessment, image retrieval, video coding, computer vision, image encryption and data-hiding. Despite its general success, there are some types of distortions, such as when the distortion is of non-Gaussian character, where SSIM does not sustain its success. We provide a generalized version of SSIM utilizing l p -norm ( 1 ≤ p ≤ 2 ) based sample moments as opposed to the classical SSIM which uses l 2 -norm based sample moments. In particular, we study the l 1 -norm special case and our simulations on well-known image-databases show superior performance compared to SSIM and related measures.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002744",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Gaussian",
      "Image (mathematics)",
      "Index (typography)",
      "Mathematics",
      "Measure (data warehouse)",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Similarity (geometry)",
      "Similarity measure",
      "Structural similarity",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Lili"
      },
      {
        "surname": "Chen",
        "given_name": "Hong"
      },
      {
        "surname": "Kuruoglu",
        "given_name": "Ercan Engin"
      },
      {
        "surname": "Zhou",
        "given_name": "Wenhui"
      }
    ]
  },
  {
    "title": "Self-guided information for few-shot classification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108880",
    "abstract": "Few-shot classification aims to identify novel categories using only a few labeled samples. Generally, the metric-based few-shot classification methods compare the feature embedding of Query samples (unlabeled samples) with Support samples (labeled samples) in a metric algorithm to predict which category the Query sample belongs to. Obtaining a good feature embedding for each sample in the feature extraction stage can improve the classification accuracy in the metric stage. Based on this, we design the Self-Guided Information Convolution (SGI-Conv), an improved convolution structure, which utilizes the high-level features to guide the network to extract the required discriminative features. To effectively utilize the feature embeddings of samples, we divide the metric network into multiple blocks and build a multi-layer graph convolutional network by sharing adjacent matrices. The multi-layer structure enhances the aggregation ability of graph convolution. Extensive experiments on multiple benchmark datasets demonstrate that our method has achieved competitive results on the few-shot classification tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003612",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Convolution (computer science)",
      "Discriminative model",
      "Economics",
      "Embedding",
      "Feature (linguistics)",
      "Feature extraction",
      "Geodesy",
      "Geography",
      "Graph",
      "Linguistics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Zhineng"
      },
      {
        "surname": "Liu",
        "given_name": "Qifan"
      },
      {
        "surname": "Cao",
        "given_name": "Wenming"
      },
      {
        "surname": "Lian",
        "given_name": "Deliang"
      },
      {
        "surname": "He",
        "given_name": "Zhihai"
      }
    ]
  },
  {
    "title": "Symbolic sequence representation with Markovian state optimization",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108849",
    "abstract": "Sequence representation, which is aimed at embedding sequentially symbolic data in a real space, is a foundational task in sequence pattern recognition. It is a difficult problem due to the challenges entailed in learning the intrinsic structural features within sequences in small sample size cases, in an unsupervised way. In this paper, we propose to represent each symbolic sequence by its transition probability distribution over discriminating topics, formalized by a set of optimized Hidden Markov Model (HMM) states shared by all sequences. An efficient method, called Markovian state clustering with hierarchical model selection, is proposed to optimize the Markovian states and to adaptively determine the number of topics. The proposed method is experimentally evaluated on human activity recognition and protein recognition, and results obtained demonstrate its effectiveness and efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003302",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Cluster analysis",
      "Computer science",
      "Economics",
      "Embedding",
      "Genetics",
      "Hidden Markov model",
      "Law",
      "Machine learning",
      "Management",
      "Markov chain",
      "Markov process",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Sequence (biology)",
      "Sequence labeling",
      "Set (abstract data type)",
      "Statistics",
      "Task (project management)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Lifei"
      },
      {
        "surname": "Wu",
        "given_name": "Haiyan"
      },
      {
        "surname": "Kang",
        "given_name": "Wenxuan"
      },
      {
        "surname": "Wang",
        "given_name": "Shengrui"
      }
    ]
  },
  {
    "title": "Imbalanced regression for intensity series of pain expression from videos by regularizing spatio-temporal face nets",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.09.022",
    "abstract": "Obtaining accurate patient-reported pain intensity is essential for effective pain management. An automatic pain recognition system can simplify the pain reporting process and reduce the strain on manual efforts. Limited and imbalanced labeled data are available for the research of estimating the intensity of pain based on facial expressions. However, the ability to train deep networks for automated pain assessment is limited by small datasets with imbalanced labels of patient-reported pain levels. Fortunately, fine-tuning from a data-extensive pre-trained domain, such as face verification or recognition, can alleviate this problem to some extent. In this paper, we propose a network which fine-tunes a face verification or recognition network using a regularized regression loss and additional data with pain-intensity labels. The expression intensity regression task can benefit from the rich feature representations trained on a large number of data for face analysis tasks. In order to explore the temporal information between frames, we combine CNN with LSTM to obtain a better prediction result of each frame in videos. A weighted evaluation metric and re-sampling technique are also proposed to address the imbalance issue of different pain levels. The proposed regularized deep regressor is applied to estimate the pain expression intensity and verified on the widely-used UNBC-McMaster Shoulder-Pain dataset and BioVid Heat Pain dataset, achieving the state-of-the-art performance. As pain detection is a form of micro facial expression recognition, we also apply the transferred deep regressor to estimate the intensity of facial action units, obtaining high quality performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002926",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Economics",
      "Expression (computer science)",
      "Face (sociological concept)",
      "Facial expression",
      "Facial recognition system",
      "Feature (linguistics)",
      "Frame (networking)",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Regression",
      "Regression analysis",
      "Social science",
      "Sociology",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Xiang",
        "given_name": "Xiang"
      },
      {
        "surname": "Wang",
        "given_name": "Feng"
      },
      {
        "surname": "Tan",
        "given_name": "Yuwen"
      },
      {
        "surname": "Yuille",
        "given_name": "Alan L."
      }
    ]
  },
  {
    "title": "A novel way to formalize stable graph cores by using matching-graphs",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108846",
    "abstract": "The increasing amount of data available and the rate at which it is collected leads to rapid developments of systems for intelligent information processing and pattern recognition. Often the underlying data is inherently complex, making it difficult to represent it by linear, vectorial data structures. This is where graphs offer a versatile alternative for formal data representation. Actually, quite an amount of graph-based methods for pattern recognition has been proposed. A considerable part of these methods rely on graph matching. In the present paper, we propose a novel encoding of specific graph matching information. The basic idea is to formalize the stable cores of individual classes of graphs – discovered during intra-class matchings – by means of so called matching-graphs. We evaluate the benefit of these matching-graphs by researching two classification approaches that rely on this novel data structure. The first approach is a distance based classifier focusing on the matching-graphs during dissimilarity computation. For the second approach, we propose to use sets of matching-graphs to embed input graphs into a vector space. The basic idea is to produce hundreds of matching-graphs first, and then represent each graph g as a vector that shows the occurrence of, or the distance to, each matching-graph. In a thorough experimental evaluation on seven real world data sets we empirically confirm that our novel approaches are able to improve the classification accuracy of systems that rely on comparable information as well as state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003272",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Graph",
      "Line graph",
      "Matching (statistics)",
      "Mathematics",
      "Modular decomposition",
      "Pathwidth",
      "Pattern recognition (psychology)",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Fuchs",
        "given_name": "Mathias"
      },
      {
        "surname": "Riesen",
        "given_name": "Kaspar"
      }
    ]
  },
  {
    "title": "Using global information to refine local patterns for texture representation and classification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108843",
    "abstract": "Local binary pattern (LBP) and its variants have been successfully applied in texture feature extraction. However, it is hard for most LBP-based methods to effectively describe and distinguish the local neighborhoods with similar structures (that is, the calculated feature patterns are identical) but different contrasts or grayscales. To alleviate such problems, we propose a novel global refined local binary pattern (GRLBP) by analyzing the nature of pixel intensity distribution in local neighborhoods. GRLBP consists of two descriptors called magnitude refined local sign binary pattern (MRLBP_S) and center refined local magnitude binary pattern (CRLBP_M). MRLBP_S distinguishes local neighborhoods with contrast differences by using global magnitude anchors to refine local sign patterns. And CRLBP_M identifies local neighborhoods with grayscale differences by employing global central grayscale anchors to refine local magnitude patterns. Finally, frequency histograms of MRLBP_S and CRLBP_M from each image are cascaded to generate the GRLBP. Extensive experimental results on seven benchmark texture databases: Outex, CUReT, KTH-TIPS, UMD, UIUC, KTH-T2b, and DTD demonstrate that the proposed GRLBP can represent the detailed information of texture images. Furthermore, compared with state-of-the-art LBP variants, GRLBP has competitive advantages in classification accuracy, feature dimension, and computational complexity, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003247",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Astronomy",
      "Benchmark (surveying)",
      "Binary number",
      "Computer science",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature extraction",
      "Geodesy",
      "Geography",
      "Grayscale",
      "Histogram",
      "Image (mathematics)",
      "Linguistics",
      "Local binary patterns",
      "Magnitude (astronomy)",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pixel",
      "Sign (mathematics)",
      "Texture (cosmology)"
    ],
    "authors": [
      {
        "surname": "Shu",
        "given_name": "Xin"
      },
      {
        "surname": "Pan",
        "given_name": "Hui"
      },
      {
        "surname": "Shi",
        "given_name": "Jinlong"
      },
      {
        "surname": "Song",
        "given_name": "Xiaoning"
      },
      {
        "surname": "Wu",
        "given_name": "Xiao-Jun"
      }
    ]
  },
  {
    "title": "On the effect of selfie beautification filters on face detection and recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.09.018",
    "abstract": "Beautification and augmented reality filters are very popular in applications that use selfie images. However, they can distort or modify biometric features, severely affecting the ability to recognise the individuals’ identity or even detect the face. Accordingly, we address the effect of such filters on the accuracy of automated face detection and recognition. The social media image filters studied modify the image contrast, illumination, or occlude parts of the face. We observe that the effect of some of these filters is harmful to face detection and identity recognition, especially if they obfuscate the eye or (to a lesser extent) the nose. To counteract such effect, we develop a method to reverse the applied manipulation with a modified version of the U-NET segmentation network. This method is observed to contribute to better face detection and recognition accuracy. From a recognition perspective, we employ distance measures and trained machine learning algorithms applied to features extracted using several CNN backbones. We also evaluate if incorporating filtered images into the training set of machine learning approaches is beneficial. Our results show good recognition when filters do not occlude important landmarks, especially the eyes. The combined effect of the proposed approaches also allows mitigating the impact produced by filters that occlude parts of the face.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002884",
    "keywords": [
      "Artificial intelligence",
      "Beautification",
      "Civil engineering",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Face (sociological concept)",
      "Face detection",
      "Facial recognition system",
      "Pattern recognition (psychology)",
      "Selfie",
      "Social science",
      "Sociology",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Hedman",
        "given_name": "Pontus"
      },
      {
        "surname": "Skepetzis",
        "given_name": "Vasilios"
      },
      {
        "surname": "Hernandez-Diaz",
        "given_name": "Kevin"
      },
      {
        "surname": "Bigun",
        "given_name": "Josef"
      },
      {
        "surname": "Alonso-Fernandez",
        "given_name": "Fernando"
      }
    ]
  },
  {
    "title": "Locality preserving projection with symmetric graph embedding for unsupervised dimensionality reduction",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108844",
    "abstract": "Preserving the intrinsic structure of data is very important for unsupervised dimensionality reduction. For structure preserving, graph embedding technique is widely considered. However, most of the existing unsupervised graph embedding based methods cannot effectively preserve the intrinsic structure of data since these methods either use the constant graph or only explore the geometric structure based on the distance information or representation information. To solve this problem, a novel method, called locality preserving projection with symmetric graph embedding (LPP_SGE), is proposed. LPP_SGE introduces a novel adaptive graph learning model and can obtain the intrinsic graph and projection in a unified framework by fully exploring the representation information and distance information of the original data. Different from the existing works which generally introduce no less than two constraints to capture the representation information and distance information, LPP_SGE can simultaneously capture the above two kinds of structure information in one term. Moreover, LPP_SGE introduces an ‘ l 2 , 1 ’ norm based projection constraint to select the most discriminative features from the complex data for dimensionality reduction, such that the robustness is enhanced. Experimental results on four databases and two kinds of noisy databases show that LPP_SGE performs better than many well-known methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003259",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Dimensionality reduction",
      "Discriminative model",
      "Embedding",
      "Graph",
      "Graph embedding",
      "Linguistics",
      "Locality",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Xiaohuan"
      },
      {
        "surname": "Long",
        "given_name": "Jiang"
      },
      {
        "surname": "Wen",
        "given_name": "Jie"
      },
      {
        "surname": "Fei",
        "given_name": "Lunke"
      },
      {
        "surname": "Zhang",
        "given_name": "Bob"
      },
      {
        "surname": "Xu",
        "given_name": "Yong"
      }
    ]
  },
  {
    "title": "A modified interval type-2 Takagi-Sugeno fuzzy neural network and its convergence analysis",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108861",
    "abstract": "In this paper, to compute the firing strength values of type-2 fuzzy models, a soft version of minimum is presented, which endows the fuzzy model with the ability to solve large dimensional problems. In addition, a conjugate gradient method is borrowed to train the designed interval type-2 Takagi-Sugeno fuzzy model. Compared with the existing gradient-based learning strategy, this scheme can efficiently enhance the fuzzy model performance. Last but not least, convergence analysis for this modified interval type-2 Takagi-Sugeno fuzzy neural network (MIT2TSFNN) is conducted in detail, which proves that the gradient of the error function tends to zero with the iteration increasing (weak convergence) and the sequence of model parameters (weights) convergences to a fixed point (strong convergence). To validate the effectiveness of the proposed MIT2TSFNN and its theoretical results, simulation results of six regression and six classification problems are presented.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003429",
    "keywords": [
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Combinatorics",
      "Computer science",
      "Conjugate gradient method",
      "Control (management)",
      "Control theory (sociology)",
      "Convergence (economics)",
      "Ecology",
      "Economic growth",
      "Economics",
      "Fuzzy logic",
      "Genetics",
      "Gradient descent",
      "Interval (graph theory)",
      "Mathematical optimization",
      "Mathematics",
      "Sequence (biology)",
      "Type (biology)"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Tao"
      },
      {
        "surname": "Bai",
        "given_name": "Xiao"
      },
      {
        "surname": "Wang",
        "given_name": "Chen"
      },
      {
        "surname": "Zhang",
        "given_name": "Liang"
      },
      {
        "surname": "Zheng",
        "given_name": "Jin"
      },
      {
        "surname": "Wang",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "A comprehensive scheme for tattoo text detection",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.10.007",
    "abstract": "Tattoo text detection provides a vital clue for person and crime identification. Due to the freestyle and unconstrained nature of handwritten tattoo text over skin regions, accurate tattoo text detection is very challenging. This paper proposes a comprehensive scheme for tattoo text detection which comprises (a) adaptive Deformable Convolutional Neural Network (DCNN) for skin region detection to reduce text detection complexity (b) a Decoupled Gradient Text Detector (DGTD) for tattoo text detection from skin region (c) a Deep Q-Network (DQN) to refine the bounding boxes detected by DGTD, and (d) a Term-Frequency-Inverse-Document-Frequency (TF-IDF) model to group the words into text lines based on semantic information to fix the bounding box for the line. To test the effectiveness, the proposed method is evaluated on different datasets, namely, (i) a newly developed tattoo text dataset, (ii) benchmark bib number dataset of the marathon, and (iii) person re-identification dataset. The proposed method achieves 91.2, 87.5, and 88.8 F-scores from these three respective datasets. To demonstrate its superior performance, the text detection module (without skin detection) is also compared with state-of-the-art scene text detection methods on benchmark datasets, namely, ICDAR 2019 ArT, Total-Text, and DAST1500 and the proposed method achieves 90.3, 88.5 and 89.8 F-score from these respective datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003002",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Botany",
      "Bounding overwatch",
      "Computer science",
      "Convolutional neural network",
      "Geodesy",
      "Geography",
      "Identification (biology)",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Minimum bounding box",
      "Pattern recognition (psychology)",
      "Scheme (mathematics)",
      "Text detection"
    ],
    "authors": [
      {
        "surname": "Banerjee",
        "given_name": "Ayan"
      },
      {
        "surname": "Shivakumara",
        "given_name": "Palaiahnakote"
      },
      {
        "surname": "Pal",
        "given_name": "Umapada"
      },
      {
        "surname": "Raghavendra",
        "given_name": "Ramachandra"
      },
      {
        "surname": "Liu",
        "given_name": "Cheng-Lin"
      }
    ]
  },
  {
    "title": "Cross-modal prototype learning for zero-shot handwritten character recognition",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108859",
    "abstract": "Traditional methods of handwritten character recognition rely on extensive labeled data. However, humans can generalize to unseen handwritten characters by watching a few printed examples in textbooks. To simulate this ability, we propose a cross-modal prototype learning method (CMPL) to realize zero-shot recognition. For each character class, a prototype is generated by mapping the printed character into a deep neural network feature space. For unseen character class, its prototype can be directly produced from a printed character sample, therefore, not requiring any handwritten samples to realize class-incremental learning. Specifically, CMPL considers different modalities simultaneously - online handwritten trajectories, offline handwritten images, and auxiliary printed character images. The joint learning of the above modalities is achieved through sharing printed prototypes between online and offline data. In zero-shot inference, we feed CMPL the printed samples to obtain corresponding class prototypes, and then the unseen handwritten character can be recognized by the nearest prototype. Our experimental results demonstrate that CMPL outperforms the state-of-the-art methods in both online and offline zero-shot handwritten Chinese character recognition. Moreover, we also show the cross-domain generalization of CMPL from two perspectives: cross-language and modern-to-ancient handwritten character recognition, focusing on the transferability between different languages and different styles (i.e., modern and historical handwritings).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003405",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Character (mathematics)",
      "Character recognition",
      "Chemistry",
      "Class (philosophy)",
      "Computer science",
      "Feature (linguistics)",
      "Geometry",
      "Image (mathematics)",
      "Intelligent character recognition",
      "Linguistics",
      "Mathematics",
      "Modal",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Polymer chemistry",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Ao",
        "given_name": "Xiang"
      },
      {
        "surname": "Zhang",
        "given_name": "Xu-Yao"
      },
      {
        "surname": "Liu",
        "given_name": "Cheng-Lin"
      }
    ]
  },
  {
    "title": "Cross-view panorama image synthesis with progressive attention GANs",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108884",
    "abstract": "Despite the significant progress of conditional image generation, it remains difficult to synthesize a ground-view panorama image from a top-view aerial image. Among the core challenges are the vast differences in image appearance and resolution between aerial images and panorama images, and the limited aside information available for top-to-ground viewpoint transformation. To address these challenges, we propose a new Progressive Attention Generative Adversarial Network (PAGAN) with two novel components: a multistage progressive generation framework and a cross-stage attention module. In the first stage, an aerial image is fed into a U-Net-like network to generate one local region of the panorama image and its corresponding segmentation map. Then, the synthetic panorama image region is extended and refined through the following generation stages with our proposed cross-stage attention module that passes semantic information forward stage-by-stage. In each of the successive generation stages, the synthetic panorama image and segmentation map are separately fed into an image discriminator and a segmentation discriminator to compute both later real and fake, as well as feature alignment score maps for discrimination. The model is trained with a novel orientation-aware data augmentation strategy based on the geometric relation between aerial and panorama images. Extensive experimental results on two cross-view datasets show that PAGAN generates high-quality panorama images with more convincing details than state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200365X",
    "keywords": [
      "Aerial image",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Detector",
      "Discriminator",
      "Feature (linguistics)",
      "Geometry",
      "Image (mathematics)",
      "Image segmentation",
      "Linguistics",
      "Mathematics",
      "Orientation (vector space)",
      "Panorama",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Segmentation",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Songsong"
      },
      {
        "surname": "Tang",
        "given_name": "Hao"
      },
      {
        "surname": "Jing",
        "given_name": "Xiao-Yuan"
      },
      {
        "surname": "Qian",
        "given_name": "Jianjun"
      },
      {
        "surname": "Sebe",
        "given_name": "Nicu"
      },
      {
        "surname": "Yan",
        "given_name": "Yan"
      },
      {
        "surname": "Zhang",
        "given_name": "Qinghua"
      }
    ]
  },
  {
    "title": "Micro-expression recognition with supervised contrastive learning",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.09.006",
    "abstract": "Facial micro-expressions are involuntary movements of facial muscles that expose individual underlying emotions. Because of the subtle and diverse facial muscles change, extracting effective features to recognize micro-expressions is challenging. In this paper, a framework for micro-expression recognition with supervised contrastive learning (MER-Supcon) is proposed, and the primary purpose is to extract crucial features of micro-expressions and overcome the noise caused by irrelevant facial movements. First, a novel dual-terminal micro-expression acquisition strategy is proposed and applied to obtain optical flow maps, which aims to expand the datasets and reduce the adverse impact of micro-expression spotting. Then, supervised contrastive learning is introduced to learn the key representation of micro-expressions for classification. The results on CASME II and SAMM datasets show that the approach is effective and competitive compared with the state-of-the-art methods both on three-classes and five-classes evaluations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002690",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Dual (grammatical number)",
      "Expression (computer science)",
      "Facial expression",
      "Facial expression recognition",
      "Facial recognition system",
      "Image (mathematics)",
      "Law",
      "Literature",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Speech recognition",
      "Spotting"
    ],
    "authors": [
      {
        "surname": "Zhi",
        "given_name": "Ruicong"
      },
      {
        "surname": "Hu",
        "given_name": "Jing"
      },
      {
        "surname": "Wan",
        "given_name": "Fei"
      }
    ]
  },
  {
    "title": "Comprehensive-perception dynamic reasoning for visual question answering",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108878",
    "abstract": "The goal of Visual Question Answering (VQA) is to answer questions based on an image. In the VQA task, reasoning plays an important role in dealing with relations because this task has a high requirement for modeling complex features. In most existing models, the features are only extracted and integrated between adjacent layers. This pattern arguably affects the integrity of information interaction during reasoning. In this paper, we propose a comprehensive-perception dynamic reasoning (CPDR) model to utilize the cross-layer object features for multi-step compound reasoning. It calculates the interactions among the object features from all previous layers and integrates these interactions to generate new object features, iteratively. Finally, the object features of all layers will be used for the final prediction. Empirical results show that our model achieves superior performance among VQA models which are not VLP-based, and incorporating the CPDR module into the VLP models brings considerable performance improvements.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003594",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Economics",
      "Machine learning",
      "Management",
      "Neuroscience",
      "Object (grammar)",
      "Perception",
      "Question answering",
      "Task (project management)",
      "Visual reasoning"
    ],
    "authors": [
      {
        "surname": "Shuang",
        "given_name": "Kai"
      },
      {
        "surname": "Guo",
        "given_name": "Jinyu"
      },
      {
        "surname": "Wang",
        "given_name": "Zihan"
      }
    ]
  },
  {
    "title": "Semi-supervised partial multi-label classification via consistency learning",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108839",
    "abstract": "Partial multi-label learning refers to the problem that each instance is associated with a candidate label set involving both relevant and noisy labels. Existing solutions mainly focus on label disambiguation, while ignoring the negative effect of the inconsistency between feature information and label information. Specifically, the existence of completely unlabeled instances makes the estimation of label co-occurrence difficult. To tackle these problems, we propose a novel framework for partial multi-label learning in semi-supervised scenarios by solving the inconsistency between features and labels. In the first stage, the label-level correlation matrix on both labeled and unlabeled instances is derived via Hilbert-Schmidt Independence Criterion (HSIC). The correlation matrix can characterize the label correlation of labeled instances and can propagate the label correlation of unlabeled instances. In the second stage, the proposed framework achieves the training of feature mapping, the recovery of ground-truth labels, and the alleviation of noisy labels in a mutually beneficial manner, and develops an alternative optimization procedure to optimize them. In addition, a nonlinear version is extended by using kernel trick. Experimental studies demonstrate that the proposed methods can achieve competitive superiority against existing well-established methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200320X",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Consistency (knowledge bases)",
      "Feature (linguistics)",
      "Focus (optics)",
      "Independence (probability theory)",
      "Kernel (algebra)",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Multi-label classification",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Tan",
        "given_name": "Anhui"
      },
      {
        "surname": "Liang",
        "given_name": "Jiye"
      },
      {
        "surname": "Wu",
        "given_name": "Wei-Zhi"
      },
      {
        "surname": "Zhang",
        "given_name": "Jia"
      }
    ]
  },
  {
    "title": "Computational linguistics processing in indigenous language",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.10.006",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002999",
    "keywords": [
      "Applied linguistics",
      "Artificial intelligence",
      "Biology",
      "Computational linguistics",
      "Computer science",
      "Ecology",
      "Indigenous",
      "Linguistics",
      "Natural language processing",
      "Philosophy",
      "Quantitative linguistics"
    ],
    "authors": [
      {
        "surname": "B．D．",
        "given_name": "Parameshachari"
      },
      {
        "surname": "Rak",
        "given_name": "Tomasz"
      },
      {
        "surname": "De Silva",
        "given_name": "Liyanage Chandratilak"
      }
    ]
  },
  {
    "title": "Single image super-resolution using Wasserstein generative adversarial network with gradient penalty",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.09.012",
    "abstract": "Due to its strong sample generating ability, Generative Adversarial Network (GAN) has been used to solve single image super-resolution (SISR) problem and obtains high perceptual quality super-resolution (SR) images. However, GAN suffers from the disadvantage of training instability, even fails to converge. In this paper, a new SISR method is proposed based on Wasserstein GAN, which is a training more stable GAN with Wasserstein metric. To further increase the SR performance and make the training process more easier and stable, two modifications are made on the original WGAN. First, a gradient penalty (GP) is adopted to replace weight clipping. Second, a new residual block with “pre-activation” of the weight layer is constructed in the generators of WGAN. Extensive experiments show that the proposed method yields superior SR performance than original GAN based SR methods and many other methods in accuracy and perceptual quality of × 4 magnification factor on four diverse testing datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002756",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Clipping (morphology)",
      "Computer science",
      "Economics",
      "Generative adversarial network",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Residual",
      "Resolution (logic)",
      "Superresolution"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Yinggan"
      },
      {
        "surname": "Liu",
        "given_name": "Chenglu"
      },
      {
        "surname": "Zhang",
        "given_name": "Xuguang"
      }
    ]
  },
  {
    "title": "Shedding light on images: Multi-level image brightness enhancement guided by arbitrary references",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108867",
    "abstract": "The non-linearity between human perception and image brightness levels results in different definitions of NORMAL-light. Thus, most existing low-light image enhancement methods which produce one-to-one mapping can not meet the aesthetic demand. Other pioneers enhance low-light images guided by a given value. However, the inherent problem of non-linearity will cause poor usability. To this end, we propose a user-friendly neural network for multi-level low-light image enhancement. Inspired by style transfer, our method decomposes an image into content component feature and luminance component feature in the latent space. Then we enhance the image brightness to different levels by concatenating the content components from low-light images and the luminance components from reference images. The network meets various user requirements by selecting different brightness references. Moreover, information except for brightness is preserved to alleviate color distortion. Extensive experiments demonstrate the superiority of our network against existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200348X",
    "keywords": [
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Brightness",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Distortion (music)",
      "Feature (linguistics)",
      "Human–computer interaction",
      "Image (mathematics)",
      "Linearity",
      "Linguistics",
      "Luminance",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Usability"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Ya’nan"
      },
      {
        "surname": "Jiang",
        "given_name": "Zhuqing"
      },
      {
        "surname": "Liu",
        "given_name": "Chang"
      },
      {
        "surname": "Li",
        "given_name": "Kai"
      },
      {
        "surname": "Men",
        "given_name": "Aidong"
      },
      {
        "surname": "Wang",
        "given_name": "Haiying"
      },
      {
        "surname": "Chen",
        "given_name": "Xiaobo"
      }
    ]
  },
  {
    "title": "H-ProMed: Ultrasound image segmentation based on the evolutionary neural network and an improved principal curve",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108890",
    "abstract": "The purpose of this work is to develop a method for accurate and robust prostate segmentation in transrectal ultrasound (TRUS) images. These images are difficult to segment due to missing/ambiguous boundary between the prostate and neighboring structures, the presence of shadow artifacts, as well as the large variability in prostate shapes. This paper develops a novel hybrid method for TRUS prostate segmentation by combining an improved principal curve-based method with an evolutionary neural network; the former for achieving the data sequences while and the latter for improving the smoothness of the prostate contour. Both qualitative and quantitative experimental results showed that our proposed method achieved superior segmentation accuracy and robustness as compared to state-of-the-art methods. The average Dice similarity coefficient (DSC), Jaccard similarity coefficient (Ω), and accuracy (ACC) of prostate contours against ground-truths were 96.8%, 95.7%, and 96.4%, and the DSC of around 92% and 95% for other deep learning and hybrid methods, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003715",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Image segmentation",
      "Operating system",
      "Pattern recognition (psychology)",
      "Principal (computer security)",
      "Principal component analysis",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Tao"
      },
      {
        "surname": "Zhao",
        "given_name": "Jing"
      },
      {
        "surname": "Gu",
        "given_name": "Yidong"
      },
      {
        "surname": "Wang",
        "given_name": "Caishan"
      },
      {
        "surname": "Wu",
        "given_name": "Yiyun"
      },
      {
        "surname": "Cheng",
        "given_name": "Xiuxiu"
      },
      {
        "surname": "Cai",
        "given_name": "Jing"
      }
    ]
  },
  {
    "title": "Deep rank hashing network for cancellable face identification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108886",
    "abstract": "Cancellable biometrics (CB) is one of the major approaches for biometric template protection. However, almost all the prior arts are designed to work under verification (one-to-one matching). This paper proposes a deep learning-based cancellable biometric scheme for face identification (one-to-many matching). Our scheme comprises two key ingredients: a deep rank hashing (DRH) network and a cancellable identification scheme. The DRH network transforms a raw face image into discriminative yet compact face hash codes based upon the nonlinear subspace ranking notion. The network is designed to be trained for both identification and hashing goals with their respective rich identity-related and rank hashing relevant loss functions. A modified softmax function is utilized to alleviate the hashing quantization error, and a regularization term is designed to encourage hash code balance. The hash code is binarized, compressed, and secured with the randomized lookup table function. Unlike prior CB schemes that require two input factors for verification, the proposed scheme demands no additional input except face images during identification, yet the face template is replaceable whenever needed based upon a one-time XOR cipher notion. The proposed scheme is evaluated on five public unconstrained face datasets in terms of verification, closed-set and open-set identification performance accuracy, computation cost, template protection criteria, and security.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003673",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Computer security",
      "Discriminative model",
      "Double hashing",
      "Fingerprint (computing)",
      "Fingerprint Verification Competition",
      "Fingerprint recognition",
      "Hash function",
      "Hash table",
      "Linear hashing",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Dong",
        "given_name": "Xingbo"
      },
      {
        "surname": "Cho",
        "given_name": "Sangrae"
      },
      {
        "surname": "Kim",
        "given_name": "Youngsam"
      },
      {
        "surname": "Kim",
        "given_name": "Soohyung"
      },
      {
        "surname": "Teoh",
        "given_name": "Andrew Beng Jin"
      }
    ]
  },
  {
    "title": "Alleviating the estimation bias of deep deterministic policy gradient via co-regularization",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108872",
    "abstract": "The overestimation in Deep Deterministic Policy Gradients (DDPG) caused by value approximation error may result in unstable policy training. Twin Delayed Deep Deterministic Policy Gradient (TD3) addresses the overestimation but suffers from the underestimation. In this paper, we propose a Co-Regularization based Deep Deterministic (CoD2) policy gradient method to mitigate the estimation bias. Two learners characterized by overestimated and underestimated biases are trained with Co-regularization to achieve this goal. The overestimated and underestimated values are updated conservatively in CoD2 for policy evaluation. Experimental results show that our method achieves comparable performance compared with other methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003533",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Computer science",
      "Econometrics",
      "Estimator",
      "Gradient method",
      "Mathematical optimization",
      "Mathematics",
      "Regularization (linguistics)",
      "Statistics",
      "Unbiased Estimation"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yao"
      },
      {
        "surname": "Wang",
        "given_name": "YuHui"
      },
      {
        "surname": "Gan",
        "given_name": "YaoZhong"
      },
      {
        "surname": "Tan",
        "given_name": "XiaoYang"
      }
    ]
  },
  {
    "title": "Wasserstein approximate bayesian computation for visual tracking",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108905",
    "abstract": "In this study, we present novel visual tracking methods based on the Wasserstein approximate Bayesian computation (ABC). For visual tracking, the proposed Wasserstein ABC (WABC) method approximates the likelihood within the Wasserstein space more accurately than the conventional ABC methods by directly measuring the discrepancy between the likelihood distributions. To encode the temporal dependency among time-series likelihood distributions, we extend the WABC method to the time-series WABC (TWABC) method. Subsequently, the proposed Hilbert TWABC (HTWABC) method reduces the computational costs caused by the TWABC method while substituting the original Wasserstein distance with the Hilbert distance. Experimental results demonstrate that the proposed visual trackers outperform other state-of-the-art visual tracking methods quantitatively. Moreover, ablation studies verify the effectiveness of individual components consisting of the proposed method (e.g., the Wasserstein distance, curve matching, and Hilbert metric).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003867",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Bayesian probability",
      "Computation",
      "Computer science",
      "Computer vision",
      "Economics",
      "Eye tracking",
      "Hilbert space",
      "Matching (statistics)",
      "Mathematical analysis",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Psychology",
      "Statistics",
      "Tracking (education)",
      "Wasserstein metric"
    ],
    "authors": [
      {
        "surname": "Park",
        "given_name": "Jinhee"
      },
      {
        "surname": "Kwon",
        "given_name": "Junseok"
      }
    ]
  },
  {
    "title": "Complementarity-aware cross-modal feature fusion network for RGB-T semantic segmentation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108881",
    "abstract": "RGB-T semantic segmentation has attracted growing attention because it makes a model robust towards challenging illumination. Most existing methods fuse RGB and thermal information in an equal manner along spatial dimensions, which results in feature redundancy and affects the discriminability of cross-modal features. In this paper, we propose a Complementarity-aware Cross-modal Feature Fusion Network (CCFFNet) including a Complementarity-Aware Encoder (CAE) and a Three-Path Fusion and Supervision (TPFS). The CAE, which consists of cascaded cross-modal fusion modules, can select complementary information from RGB and thermal features via a novel gate and fuse them by a channel-wise weighting mechanism. TPFS not only iteratively performs Three-Path Fusion (TPF) to further enhance cross-modal features, but also supervise the training of CCFFNet along three branches by Three-Supervision (TS). Extensive experiments are carried out and the results demonstrate that our model outperforms the state-of-the-art models by at least 1.6% mIoU on MFNet dataset and 2.9% mIoU on PST900 dataset, respectively. And a single-modality-based model can be easily applied to multi-modal semantic segmentation when plugging our CAE.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003624",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Chemistry",
      "Complementarity (molecular biology)",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Encoder",
      "Engineering",
      "Feature (linguistics)",
      "Fuse (electrical)",
      "Fusion",
      "Fusion mechanism",
      "Genetics",
      "Linguistics",
      "Lipid bilayer fusion",
      "Medicine",
      "Modal",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Polymer chemistry",
      "RGB color model",
      "Radiology",
      "Redundancy (engineering)",
      "Segmentation",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Wei"
      },
      {
        "surname": "Chu",
        "given_name": "Tao"
      },
      {
        "surname": "Liu",
        "given_name": "Qiong"
      }
    ]
  },
  {
    "title": "Artificial life for segmentation of fusion ultrasound images of breast abnormalities",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108838",
    "abstract": "Segmentation of cancerous tumors in ultrasound (US) images of human organs is one of the critical problems in medical imaging. The US images are characterized by low contrast, irregular shapes, high levels of speckle-noise and acoustic shadows, making it difficult to segment the tumor. Yet, US imaging is considered one of the most inexpensive and safe imaging tests available to detect cancer in its early stages. However, an automatic segmentation method applicable to all types of US imagery does not exist. This paper proposes a novel segmentation method that combines image fusion, artificial life (AL) and a genetic algorithm (GA). The new algorithm has been applied to US images of breast cancer. The method is based on tracing agents (TA), which are artificial organisms with memory and the ability to communicate. They live inside a fusion image generated from the US and the elastography (EL) images. The TA can recognize the patterns of strong edges and boundary gaps allowing to outline the tumor. The new model has been tested against six types of segmentation models, i.e., machine learning, active contours, level set models, superpixel models, edge linking models and selected hybrid methods. The experiments include 16 state-of-the-art methods, which outperform 69 recent and classical segmentation routines. The tests were run on 395 breast cancer images from http://onlinemedicalimages.com and https://www.ultrasoundcases.info/. TA training employs a GA. The model has been verified on “hard” cases (complex shapes, boundary leakage, and noisy edge maps). The proposed algorithm produces more accurate results than the reference methods on high complexity images. A video demo of the algorithm is at http://shorturl.at/htBW9.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003193",
    "keywords": [
      "Artificial intelligence",
      "Breast cancer",
      "Breast ultrasound",
      "Cancer",
      "Computer science",
      "Computer vision",
      "Image segmentation",
      "Internal medicine",
      "Level set (data structures)",
      "Level set method",
      "Mammography",
      "Medicine",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Speckle noise",
      "Speckle pattern"
    ],
    "authors": [
      {
        "surname": "Karunanayake",
        "given_name": "Nalan"
      },
      {
        "surname": "Lohitvisate",
        "given_name": "Wanrudee"
      },
      {
        "surname": "Makhanov",
        "given_name": "Stanislav S."
      }
    ]
  },
  {
    "title": "Heterogeneous representation learning and matching for few-shot relation prediction",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108830",
    "abstract": "The recent explosive development of knowledge graphs (KGs) in artificial intelligence tasks coupled with incomplete or partial information has triggered considerable research interest in relation prediction. However, many challenges still remain unsolved: (i) the previous relation prediction methods require a significant amount of training instances (i.e., head-tail entity pairs) for every relation, which is infeasible in practical scenarios; and (ii) the representation learning of entities and relations always assumes that all local neighbors and their features contribute equally to the embedding, not sufficiently considering the heterogeneity of the information; and (iii) the state-of-the-art methods usually require a lot of training time, resulting in a high cost in real-world applications. To overcome these challenges, we propose a heterogeneous representation learning and matching approach, Multi-metric Feature Extraction Network (MFEN for short), for few-shot relation prediction in KGs. Our method focuses on knowledge graphs to sufficiently explore the topological structure and node content in graphs. Rather than taking the average of the embeddings of all relational neighbors, a heterogeneity-aware representation learning method is proposed to generate high-expressive embeddings, which capture the heterogenous roles of the relational neighbors of given entity and all of their features via a convolutional encoder. To learn the expressive representations efficiently, a single-layer CNN architecture with multi-scale filters is devised. In addition, multiple heuristic metrics are combined to efficiently improve the accuracy of similarity calculation. The proposed MFEN model is evaluated on two representative benchmark datasets NELL and Wiki. Extensive experiments have demonstrated that our method gets more than 5 % accuracy improvement and three times speedup to state-of-the-art models. Code is available on https://github.com/summer-funny/MFEN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003119",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Law",
      "Machine learning",
      "Matching (statistics)",
      "Mathematics",
      "Natural language processing",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Relation (database)",
      "Representation (politics)",
      "Shot (pellet)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Tao"
      },
      {
        "surname": "Ma",
        "given_name": "Hongyu"
      },
      {
        "surname": "Wang",
        "given_name": "Chao"
      },
      {
        "surname": "Qiao",
        "given_name": "Shaojie"
      },
      {
        "surname": "Zhang",
        "given_name": "Liang"
      },
      {
        "surname": "Yu",
        "given_name": "Shui"
      }
    ]
  },
  {
    "title": "Adaptive aggregation-distillation autoencoder for unsupervised anomaly detection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108897",
    "abstract": "Anomaly detection (AD) has been receiving great attention as it plays a crucial role in many areas of basic research and industrial applications. However, most existing AD methods not only rely on training on normal data, but also ignore the multi-cluster nature of normal and abnormal patterns. To overcome these limitations, this paper proposes a novel method called Adaptive Aggregation-Distillation AutoEncoder (AADAE) for unsupervised anomaly detection. AADAE is built upon the density-based landmark selection in respect to representing diverse normal patterns. During training, AADAE adaptively updates the location and quantity of landmarks. Then, an aggregation-distillation mechanism is constructed: Firstly, it aggregates the latent representations of normal and anomalous to different landmark-guided regions within the convex polygon with landmarks as vertices, which minimizes the intra-class variation and promotes the separability of normal and abnormal samples. Secondly, the distillation mechanism is applied to obtain reliable detection results when there are anomalies in the training set. The aggregation process motivates AADAE to learn the distribution of multi-cluster normal samples with the help of landmarks, which in turn facilitates the distillation process to differentiate normal from anomalies for training. Extensive empirical studies on ten datasets from different application domains demonstrate the efficiency and generalization ability of the method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003788",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Distillation",
      "Generalization",
      "Landmark",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Process (computing)"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Jiaqi"
      },
      {
        "surname": "Deng",
        "given_name": "Fang"
      },
      {
        "surname": "Zhao",
        "given_name": "Jiachen"
      },
      {
        "surname": "Chen",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "Biometric presentation attacks: Handcrafted features versus deep learning approaches",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.09.013",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002768",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Binary tree",
      "Biometrics",
      "Computation",
      "Computer science",
      "Computer vision",
      "Filter (signal processing)",
      "Image (mathematics)",
      "Interval tree",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Segment tree",
      "Tree (set theory)",
      "Tree structure"
    ],
    "authors": [
      {
        "surname": "Marcialis",
        "given_name": "Gian Luca"
      },
      {
        "surname": "Liu",
        "given_name": "Xiaoming"
      },
      {
        "surname": "Didaci",
        "given_name": "Luca"
      }
    ]
  },
  {
    "title": "Predicting on-street parking violation rate using deep residual neural networks",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.09.023",
    "abstract": "The lack of available parking spaces can be among the most significant issues that can affect the quality of life of citizens in large cities. This has led to the development of on-street parking systems that typically ensure that parking spaces will be available for the local population, as well as provide easy access to parking for visitors, e.g., by providing directions for finding sectors where parking slots are available. Unfortunately, such systems are affected by illegal parking, i.e., parking without paying the parking fee, since in this case, the number of registered parked cars does not match the number of cars that are actually parked, leading to providing incorrect suggestions to drivers. This can discourage drivers from using such systems, potentially further increasing the parking violation rate. Such phenomena can be addressed by using smart sensors that can detect the presence of cars in various areas. However, installing and maintaining such systems is costly, which usually discourage cities from implementing such solutions. The main contribution of this paper is a Deep Learning (DL)-based pipeline that works in an indirect way (i.e., without using sensors) and allows for developing an accurate fine-grained parking violation prediction system, increasing in this way the accuracy of the information provided to on-street parking systems with minimal cost. To deal with missing and noisy data we also propose a data augmentation and smoothing technique that can further improve the accuracy of DL models, when used in such scenarios. The effectiveness of the developed system is validated using experiments on a large-scale dataset, which contains more than 3.9 million scans for illegally parked cars collected by the municipal police in Thessaloniki, Greece.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002938",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Engineering",
      "Epistemology",
      "Installation",
      "Operating system",
      "Parking guidance and information",
      "Philosophy",
      "Quality (philosophy)",
      "Real-time computing",
      "Residual",
      "Smoothing",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Karantaglis",
        "given_name": "Nikolaos"
      },
      {
        "surname": "Passalis",
        "given_name": "Nikolaos"
      },
      {
        "surname": "Tefas",
        "given_name": "Anastasios"
      }
    ]
  },
  {
    "title": "Exploiting key points supervision and grouped feature fusion for multiview pedestrian detection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108866",
    "abstract": "Multiview pedestrian detection detects pedestrians based on the perception of the same environment from multiple perspectives. This task requires feature extraction in a single view with occlusion and aggregation of multiview information. However, existing research is limited by the local occlusion and the multiview feature stitching method, which cannot perform multiview aggregation efficiently. This paper introduces a network that utilizes key points supervision and grouped feature fusion to address these challenges. It uses key points to regress pedestrians in a single view, and augments the pedestrian consistency information in overlapping views by a grouped feature fusion module. Specifically, the proposed key points supervision effectively alleviates false negatives due to occlusion, and the grouped feature fusion module enhances pedestrian location features by computing the similarity and spatial correlation of overlapping views after single view projection to the ground plane, thereby reducing target ambiguity. Quantitative and qualitative results show that the proposed method can reduce false negatives and false positives in multiview pedestrian detection and achieve efficient multiview feature aggregation. Compared to state-of-the-art methods, the proposed model achieves superior performance, achieving the highest MODA of 92.4 and 93.9 on Wildtrack and MultiviewX datasets, respectively. We believe, to the best of our knowledge, that this approach offers a new optimization idea for multiview aggregation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003478",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Engineering",
      "Feature (linguistics)",
      "Feature extraction",
      "Image stitching",
      "Key (lock)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Pedestrian",
      "Pedestrian detection",
      "Philosophy",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Xin"
      },
      {
        "surname": "Xiong",
        "given_name": "Yijin"
      },
      {
        "surname": "Zhang",
        "given_name": "Guoying"
      },
      {
        "surname": "Deng",
        "given_name": "Hui"
      },
      {
        "surname": "Kou",
        "given_name": "Kangkang"
      }
    ]
  },
  {
    "title": "Covid-MANet: Multi-task attention network for explainable diagnosis and severity assessment of COVID-19 from CXR images",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108826",
    "abstract": "The devastating outbreak of Coronavirus Disease (COVID-19) cases in early 2020 led the world to face health crises. Subsequently, the exponential reproduction rate of COVID-19 disease can only be reduced by early diagnosis of COVID-19 infection cases correctly. The initial research findings reported that radiological examinations using CT and CXR modality have successfully reduced false negatives by RT-PCR test. This research study aims to develop an explainable diagnosis system for the detection and infection region quantification of COVID-19 disease. The existing research studies successfully explored deep learning approaches with higher performance measures but lacked generalization and interpretability for COVID-19 diagnosis. In this study, we address these issues by the Covid-MANet network, an automated end-to-end multi-task attention network that works for 5 classes in three stages for COVID-19 infection screening. The first stage of the Covid-MANet network localizes attention of the model to the relevant lungs region for disease recognition. The second stage of the Covid-MANet network differentiates COVID-19 cases from bacterial pneumonia, viral pneumonia, normal and tuberculosis cases, respectively. To improve the interpretation and explainability, three experiments have been conducted in exploration of the most coherent and appropriate classification approach. Moreover, the multi-scale attention model MA-DenseNet201 proposed for the classification of COVID-19 cases. The final stage of the Covid-MANet network quantifies the proportion of infection and severity of COVID-19 in the lungs. The COVID-19 cases are graded into more specific severity levels such as mild, moderate, severe, and critical as per the score assigned by the RALE scoring system. The MA-DenseNet201 classification model outperforms eight state-of-the-art CNN models, in terms of sensitivity and interpretation with lung localization network. The COVID-19 infection segmentation by UNet with DenseNet121 encoder achieves dice score of 86.15% outperforming UNet, UNet++, AttentionUNet, R2UNet, with VGG16, ResNet50 and DenseNet201 encoder. The proposed network not only classifies images based on the predicted label but also highlights the infection by segmentation/localization of model-focused regions to support explainable decisions. MA-DenseNet201 model with a segmentation-based cropping approach achieves maximum interpretation of 96% with COVID-19 sensitivity of 97.75%. Finally, based on class-varied sensitivity analysis Covid-MANet ensemble network of MA-DenseNet201, ResNet50 and MobileNet achieve 95.05% accuracy and 98.75% COVID-19 sensitivity. The proposed model is externally validated on an unseen dataset, yields 98.17% COVID-19 sensitivity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003077",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Disease",
      "Infectious disease (medical specialty)",
      "Internal medicine",
      "Interpretability",
      "Medicine",
      "Paleontology",
      "Pathology",
      "Pneumonia",
      "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)",
      "Stage (stratigraphy)"
    ],
    "authors": [
      {
        "surname": "Sharma",
        "given_name": "Ajay"
      },
      {
        "surname": "Mishra",
        "given_name": "Pramod Kumar"
      }
    ]
  },
  {
    "title": "DARTSRepair: Core-failure-set guided DARTS for network robustness to common corruptions",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108864",
    "abstract": "Network architecture search (NAS), in particular the differentiable architecture search (DARTS) method, has shown a great power to learn excellent model architectures on the specific dataset of interest. In contrast to using a fixed dataset, in this work, we focus on a different but important scenario for NAS: how to refine a deployed network’s model architecture to enhance its robustness with the guidance of a few collected and misclassified examples that are degraded by some real-world unknown corruptions having a specific pattern (e.g., noise, blur, etc..). To this end, we first conduct an empirical study to validate that the model architectures can be definitely related to the corruption patterns. Surprisingly, by just adding a few corrupted and misclassified examples (e.g., 10 3 examples) to the clean training dataset (e.g., 5.0 × 10 4 examples), we can refine the model architecture and enhance the robustness significantly. To make it more practical, the key problem, i.e., how to select the proper failure examples for the effective NAS guidance, should be carefully investigated. Then, we propose a novel core-failure-set guided DARTS that embeds a K -center-greedy algorithm for DARTS to select suitable corrupted failure examples to refine the model architecture. We use our method for DARTS-refined DNNs on the clean as well as 15 corruptions with the guidance of four specific real-world corruptions. Compared with the state-of-the-art NAS as well as data-augmentation-based enhancement methods, our final method can achieve higher accuracy on both corrupted datasets and the original clean dataset. On some of the corruption patterns, we can achieve as high as over 45 % absolute accuracy improvements.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003454",
    "keywords": [
      "Algorithm",
      "Biochemistry",
      "Chemistry",
      "Common core",
      "Computer network",
      "Computer science",
      "Core (optical fiber)",
      "Core network",
      "Gene",
      "Programming language",
      "Robustness (evolution)",
      "Set (abstract data type)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Ren",
        "given_name": "Xuhong"
      },
      {
        "surname": "Chen",
        "given_name": "Jianlang"
      },
      {
        "surname": "Juefei-Xu",
        "given_name": "Felix"
      },
      {
        "surname": "Xue",
        "given_name": "Wanli"
      },
      {
        "surname": "Guo",
        "given_name": "Qing"
      },
      {
        "surname": "Ma",
        "given_name": "Lei"
      },
      {
        "surname": "Zhao",
        "given_name": "Jianjun"
      },
      {
        "surname": "Chen",
        "given_name": "Shengyong"
      }
    ]
  },
  {
    "title": "Similarity activation map for co-salient object detection",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.10.009",
    "abstract": "Co-salient object detection aims to detect the common objects within a group of relevant images, to which the spatial similarity contributes a lot. Existing methods utilize the inner product to compute the pixel-wise correlations, imitating the tracking methods. We present a novel yet effective module (Similarity Activation Module, SAM) to generate the similarity activation maps as the spatial modulator. The similarity activation maps are learned to highlight the common objects across the multiple images while suppressing other objects and the background. Moreover, we propose the Edge Extraction Module (EEM) and Feature Fusion Module (FFM) which can be easily applied to any existing methods without requiring architectural changes. Extensive experiments on different co-salient detection datasets demonstrate that our method (SimiNet) achieves state-of-the-art performance under various evaluation metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003038",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Feature extraction",
      "Image (mathematics)",
      "Linguistics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Philosophy",
      "Pixel",
      "Psychology",
      "Salient",
      "Similarity (geometry)",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yu"
      },
      {
        "surname": "Li",
        "given_name": "Shuxiao"
      }
    ]
  },
  {
    "title": "Rethinking interactive image segmentation: Feature space annotation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108882",
    "abstract": "Despite the progress of interactive image segmentation methods, high-quality pixel-level annotation is still time-consuming and laborious — a bottleneck for several deep learning applications. We take a step back to propose interactive and simultaneous segment annotation from multiple images guided by feature space projection. This strategy is in stark contrast to existing interactive segmentation methodologies, which perform annotation in the image domain. We show that feature space annotation achieves competitive results with state-of-the-art methods in foreground segmentation datasets: iCoSeg, DAVIS, and Rooftop. Moreover, in the semantic segmentation context, it achieves 91.5% accuracy in the Cityscapes dataset, being 74.75 times faster than the original annotation procedure. Further, our contribution sheds light on a novel direction for interactive image annotation that can be integrated with existing methodologies. The supplementary material presents video demonstrations. Code available at https://github.com/LIDS-UNICAMP/rethinking-interactive-image-segmentation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003636",
    "keywords": [
      "Annotation",
      "Artificial intelligence",
      "Automatic image annotation",
      "Biology",
      "Bottleneck",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Embedded system",
      "Feature (linguistics)",
      "Feature vector",
      "Image (mathematics)",
      "Image retrieval",
      "Image segmentation",
      "Linguistics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Bragantini",
        "given_name": "Jordão"
      },
      {
        "surname": "Falcão",
        "given_name": "Alexandre X."
      },
      {
        "surname": "Najman",
        "given_name": "Laurent"
      }
    ]
  },
  {
    "title": "Spatial feature mapping for 6DoF object pose estimation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108835",
    "abstract": "This work aims to estimate 6Dof (6D) object pose in background clutter. Considering the strong occlusion and background noise, we propose to utilize the spatial structure for better tackling this challenging task. Observing that the 3D mesh can be naturally abstracted by a graph, we build the graph using 3D points as vertices and mesh connections as edges. We construct the corresponding mapping from 2D image features to 3D points for filling the graph and fusion of the 2D and 3D features. Afterward, a Graph Convolutional Network (GCN) is applied to help the feature exchange among objects’ points in 3D space. To address the problem of rotation symmetry ambiguity for objects, a spherical convolution is utilized and the spherical features are combined with the convolutional features that are mapped to the graph. Predefined 3D keypoints are voted and the 6DoF pose is obtained via the fitting optimization. Two scenarios of inference, one with the depth information and the other without it are discussed. Tested on the datasets of YCB-Video and LINEMOD, the experiments demonstrate the effectiveness of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003168",
    "keywords": [
      "Artificial intelligence",
      "Clutter",
      "Computer science",
      "Computer vision",
      "Graph",
      "Inference",
      "Pattern recognition (psychology)",
      "Pose",
      "Radar",
      "Telecommunications",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Mei",
        "given_name": "Jianhan"
      },
      {
        "surname": "Jiang",
        "given_name": "Xudong"
      },
      {
        "surname": "Ding",
        "given_name": "Henghui"
      }
    ]
  },
  {
    "title": "Multi-layer manifold learning for deep non-negative matrix factorization-based multi-view clustering",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108815",
    "abstract": "Multi-view data clustering based on Non-negative Matrix Factorization (NMF) has been commonly used for pattern recognition by grouping multi-view high-dimensional data by projecting it to a lower-order dimensional space. However, the NMF framework fails to learn the accurate lower-order representation of the input data if it exhibits complex and non-linear relationships. This paper proposes a deep non-negative matrix factorization-based framework for effective multi-view data clustering by uncovering both the non-linear relationships and the intrinsic components of the data. Both the consensus and complementary information present in multiple views are sufficiently learned in the proposed framework with the effective use of constraints such as normalized cut-type and orthogonal. The optimal manifold of multi-view data is effectively incorporated in all layers of the framework. Extensive experimental results show the proposed method outperforms state-of-the-art multi-view matrix factorization-based methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002965",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Composite material",
      "Computer science",
      "Dimensionality reduction",
      "Eigenvalues and eigenvectors",
      "Engineering",
      "External Data Representation",
      "Factorization",
      "Law",
      "Manifold (fluid mechanics)",
      "Materials science",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Matrix decomposition",
      "Mechanical engineering",
      "Non-negative matrix factorization",
      "Nonlinear dimensionality reduction",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Luong",
        "given_name": "Khanh"
      },
      {
        "surname": "Nayak",
        "given_name": "Richi"
      },
      {
        "surname": "Balasubramaniam",
        "given_name": "Thirunavukarasu"
      },
      {
        "surname": "Bashar",
        "given_name": "Md Abul"
      }
    ]
  },
  {
    "title": "SSMTReID-Net: Multi-Target Unsupervised Domain Adaptation for Person Re-Identification",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.09.014",
    "abstract": "The task of unsupervised domain adaptive (DA) person re-identification (reID) has gained prominence in recent years. Current reID works mostly leverage off-the-shelf single-target domain adaptive (STDA) techniques to reduce any domain gap between a labeled source dataset and an unlabeled target dataset in a single-source single-target training setup in their reID framework. However, directly extending such STDA techniques to multiple target domains suffers from two inherent drawbacks: a) the performance on the source domain deteriorates once the model is fine-tuned for a target domain, and b) posing the single-source multi-target (SSMT) setup as an independent single-source single-target case by blending all target domains to form a single target domain fails to utilize the complementary information present in the different target domains. Hence, to tackle this problem of unsupervised multi-target domain adaptation (MTDA), we propose a novel architecture called SSMTReID-Net. SSMTReID-Net employs the elastic weight consolidation (EWC) regularizer to ensure competitive performance on the source domain after adaptation, and the notion of information bottleneck (IB) to highlight domain invariant target features while suppressing any domain-irrelevant artifacts. Our model is end-to-end trainable, and extensive results on different single-source multi-target combinations of challenging person reID datasets like DukeMTMC-reID, Market-1501 and CUHK03 datasets confirm the superiority of SSMTReID-Net over the other baselines.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552200277X",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Classifier (UML)",
      "Computer science",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Geometry",
      "Identification (biology)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Net (polyhedron)",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Psychology"
    ],
    "authors": [
      {
        "surname": "Mohanty",
        "given_name": "Anwesh"
      },
      {
        "surname": "Banerjee",
        "given_name": "Biplab"
      },
      {
        "surname": "Velmurugan",
        "given_name": "Rajbabu"
      }
    ]
  },
  {
    "title": "Data-attention-YOLO (DAY): A comprehensive framework for mesoscale eddy identification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108870",
    "abstract": "The accurate mesoscale eddy identification methods with deep learning framework depend on either single eddy characteristic from altimeter missions or multi-step eddy examination strategies, disregarding those indistinguishable features from multiple eddy data integration. In this article, we first propose a data-attention-based YOLO (DAY) to precisely recognize mesoscale eddies in the South China Sea (SCS), which can hierarchically unite multiple eddy attributes and efficiently predict eddies with one-step strategy involving detection and classification. It consists of two main components: heterogeneous eddy data integration module and dynamic attention detecting module for eddy identification. The data integration component empirically transforms the field of multi-source eddy data and propagates eddy labels through automatic labeling method, which sustains a good supply for our dynamic attention-base detecting network. To thoroughly identify mesoscale eddies based on spatio-temporal patterns, DAY efficiently learns the characteristics of mesoscale eddies with an improved one-step identification YOLO network. The comparative evaluation results demonstrate that DAY achieves 54% performance improvement over the state-of-the-art methods on single gray SLA data and outperforms two-stage detecting technique Faster R-CNN by 51%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200351X",
    "keywords": [
      "Altimeter",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Data mining",
      "Eddy",
      "Geography",
      "Geology",
      "Identification (biology)",
      "Mesoscale meteorology",
      "Meteorology",
      "Pattern recognition (psychology)",
      "Remote sensing",
      "Turbulence"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xinning"
      },
      {
        "surname": "Wang",
        "given_name": "Xuegong"
      },
      {
        "surname": "Li",
        "given_name": "Chong"
      },
      {
        "surname": "Zhao",
        "given_name": "Yuben"
      },
      {
        "surname": "Ren",
        "given_name": "Peng"
      }
    ]
  },
  {
    "title": "High-order conditional mutual information maximization for dealing with high-order dependencies in feature selection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108895",
    "abstract": "This paper presents a novel feature selection method based on the conditional mutual information (CMI). The proposed High Order Conditional Mutual Information Maximization (HOCMIM) method incorporates high order dependencies into the feature selection procedure and has a straightforward interpretation due to its bottom-up derivation. The HOCMIM is derived from the CMI’s chain expansion and expressed as a maximization optimization problem. The maximization problem is solved using a greedy search procedure, which speeds up the entire feature selection process. The experiments are run on a set of benchmark datasets (20 in total). The HOCMIM is compared with eighteen state-of-the-art feature selection algorithms, from the results of two supervised learning classifiers (Support Vector Machine and K-Nearest Neighbor). The HOCMIM achieves the best results in terms of accuracy and shows to be faster than high order feature selection counterparts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003764",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Conditional mutual information",
      "Data mining",
      "Feature (linguistics)",
      "Feature selection",
      "Geodesy",
      "Geography",
      "Greedy algorithm",
      "Linguistics",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Mutual information",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Selection (genetic algorithm)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Souza",
        "given_name": "Francisco"
      },
      {
        "surname": "Premebida",
        "given_name": "Cristiano"
      },
      {
        "surname": "Araújo",
        "given_name": "Rui"
      }
    ]
  },
  {
    "title": "SO-softmax loss for discriminable embedding learning in CNNs",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108877",
    "abstract": "Convolutional neural networks (CNNs)-based classifiers, trained with the softmax cross-entropy loss, have achieved remarkable success in learning embeddings for pattern recognition. The cosine similarity-based softmax variants further improve the performance by focusing on optimizing the angles between embeddings and class weights. However, embeddings learned by these variants still have significant intra-class variances since these methods only optimize the relative differences between intra- and inter-class cosine similarities. To simultaneously optimize intra- and inter-class cosine similarities, this paper proposes a cosine Similarity Optimization-based softmax (SO-softmax) loss, which is based on a generalized softmax loss formulation that combines both similarities. The proposed loss constrains the intra-class (positive) and inter-class (negative) cosine similarity by quadratic transformations, thus making the embedding representation more compact within classes and more distinguishable between classes. It is verified theoretically that SO-softmax loss can optimize both the similarities simultaneously. Thorough experiments are conducted on typical audio classification, image classification, face verification, image retrieval, and person re-identification tasks, and the results show that SO-softmax loss outperforms the state-of-the-art loss functions in CNNs-based frameworks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003582",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Embedding",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Softmax function"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Qiang"
      },
      {
        "surname": "Yang",
        "given_name": "Jibin"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiongwei"
      },
      {
        "surname": "Cao",
        "given_name": "Tieyong"
      }
    ]
  },
  {
    "title": "Hiding multiple images into a single image via joint compressive autoencoders",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108842",
    "abstract": "Interest in image hiding has been continually growing. Recently, deep learning-based image hiding approaches improve the hidden capacity significantly. However, the major challenges of the existing methods are that they are difficult to balance between the errors of the modified cover image and those of the recovered secret image. To solve this problem, in this paper, we develop an image hiding algorithm based on a joint compressive autoencoder framework. Further, we propose a novel strategy to enlarge the hidden capacity, i.e., hiding multi-images in one container image. Specifically, our approach provides an extremely high image hidden capacity coupled with small reconstruction errors of the secret image. More importantly, we tackle the trade-off problem of earlier approaches by mapping the image representations in the latent spaces of the joint compressive autoencoder models, leading to both high visual quality of the container image and low reconstruction error the secret image. In an extensive set of experiments, we confirm our proposed approach to outperform several state-of-the-art image hiding methods, yielding high imperceptibility and steganalysis resistance of the container images with high recovery quality of the secret images, while improving the image hidden capacity significantly (four times higher than full-image hiding capacity).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003235",
    "keywords": [
      "Architectural engineering",
      "Artificial intelligence",
      "Autoencoder",
      "Computer science",
      "Computer vision",
      "Container (type theory)",
      "Deep learning",
      "Embedding",
      "Engineering",
      "Image (mathematics)",
      "Image quality",
      "Information hiding",
      "Joint (building)",
      "Mechanical engineering",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Xiyao"
      },
      {
        "surname": "Ma",
        "given_name": "Ziping"
      },
      {
        "surname": "Chen",
        "given_name": "Zhihong"
      },
      {
        "surname": "Li",
        "given_name": "Fangfang"
      },
      {
        "surname": "Jiang",
        "given_name": "Ming"
      },
      {
        "surname": "Schaefer",
        "given_name": "Gerald"
      },
      {
        "surname": "Fang",
        "given_name": "Hui"
      }
    ]
  },
  {
    "title": "Sparse random projection isolation forest for outlier detection",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.09.015",
    "abstract": "Isolation Forest has a low computational complexity, hence has been widely applied to detect outliers in large-scale data. However, it suffers from the artifacts caused by the hyperplanes chosen, thereby failing to detect outliers in some specific regions. To tackle this problem, we propose the random-projection-based Isolation Forest, which works in two steps. First, we transform the data using the random projection technique. Then, we employ the Isolation Forest to identify outliers using the transformed data. Experimental results show that the proposed methods outperform 12 state-of-the-art outlier detectors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002847",
    "keywords": [
      "Algorithm",
      "Anomaly detection",
      "Artificial intelligence",
      "Biology",
      "Cartography",
      "Computer science",
      "Geography",
      "Geometry",
      "Hyperplane",
      "Isolation (microbiology)",
      "Mathematics",
      "Microbiology",
      "Outlier",
      "Pattern recognition (psychology)",
      "Projection (relational algebra)",
      "Random forest",
      "Random projection",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Tan",
        "given_name": "Xu"
      },
      {
        "surname": "Yang",
        "given_name": "Jiawei"
      },
      {
        "surname": "Rahardja",
        "given_name": "Susanto"
      }
    ]
  },
  {
    "title": "A novel explainable neural network for Alzheimer’s disease diagnosis",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108876",
    "abstract": "Visual classification for medical images has been dominated by convolutional neural networks (CNNs) for years. Though they have shown great performance on accuracy, some of them provide decisions that are hard to explain while others encode information from irrelevant or noisy regions. In this work, we try to close this gap by proposing an explainable framework which consists of a predictor and an explainable tool, so as to provide accurate diagnoses with intuitive visualization maps and prediction basis. Specifically, the predictor is designed by applying attention mechanisms to multi-scale features so as to learn and discover class discriminative latent representations that are close to each brain volume’s label. Meanwhile, to explain our predictor, we propose the novel explainable tool which includes a high-resolution visualization method and a prediction-basis creation and retrieval module. The former effectively integrates the feature maps of intermediate layers as well as the last convolutional layer, which surpasses state-of-the-art visualization approaches in producing high-resolution representations with more accurate localization of discriminative areas. While the latter provides prediction basis evidence via retrieved volumes with similar latent representations which are accessible to neurologists. Extensive experiments show that the proposed framework achieves higher level of accuracy and explainability over other state-of-the-art solutions. More importantly, it localizes crucial brain areas with clearer boundaries, less noises, which matches background knowledge in the neuroscience literature.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003570",
    "keywords": [
      "Artificial intelligence",
      "Basis (linear algebra)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Convolutional neural network",
      "Discriminative model",
      "ENCODE",
      "Feature (linguistics)",
      "Gene",
      "Geometry",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Lu"
      },
      {
        "surname": "Xiang",
        "given_name": "Wei"
      },
      {
        "surname": "Fang",
        "given_name": "Juan"
      },
      {
        "surname": "Phoebe Chen",
        "given_name": "Yi-Ping"
      },
      {
        "surname": "Zhu",
        "given_name": "Ruifeng"
      }
    ]
  },
  {
    "title": "Target-Cognisant Siamese Network for Robust Visual Object Tracking",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.09.017",
    "abstract": "Siamese trackers have become the mainstream framework for visual object tracking in recent years. However, the extraction of the template and search space features is disjoint for a Siamese tracker, resulting in a limited interaction between its classification and regression branches. This degrades the model capacity accurately to estimate the target, especially when it exhibits severe appearance variations. To address this problem, this paper presents a target-cognisant Siamese network for robust visual tracking. First, we introduce a new target-cognisant attention block that computes spatial cross-attention between the template and search branches to convey the relevant appearance information before correlation. Second, we advocate two mechanisms to promote the precision of obtained bounding boxes under complex tracking scenarios. Last, we propose a max filtering module to utilise the guidance of the regression branch to filter out potential interfering predictions in the classification map. The experimental results obtained on challenging benchmarks demonstrate the competitive performance of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002860",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Computer science",
      "Computer vision",
      "Gene",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Yingjie"
      },
      {
        "surname": "Song",
        "given_name": "Xiaoning"
      },
      {
        "surname": "Xu",
        "given_name": "Tianyang"
      },
      {
        "surname": "Feng",
        "given_name": "Zhenhua"
      },
      {
        "surname": "Wu",
        "given_name": "Xiaojun"
      },
      {
        "surname": "Kittler",
        "given_name": "Josef"
      }
    ]
  },
  {
    "title": "Incremental Learning from Low-labelled Stream Data in Open-Set Video Face Recognition",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108885",
    "abstract": "Deep Learning approaches have brought solutions, with impressive performance, to general classification problems where wealthy of annotated data are provided for training. In contrast, less progress has been made in continual learning of a set of non-stationary classes, mainly when applied to unsupervised problems with streaming data. Here, we propose a novel incremental learning approach which combines a deep features encoder with an Open-Set Dynamic Ensembles of SVM, to tackle the problem of identifying individuals of interest (IoI) from streaming face data. From a simple weak classifier trained on a few video-frames, our method can use unsupervised operational data to enhance recognition. Our approach adapts to new patterns avoiding catastrophic forgetting and partially heals itself from miss-adaptation. Besides, to better comply with real world conditions, the system was designed to operate in an open-set setting. Results show a benefit of up to 15% F1-score increase respect to non-adaptive state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003661",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Data set",
      "Discrete mathematics",
      "Encoder",
      "Face (sociological concept)",
      "Facial recognition system",
      "Forgetting",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Open set",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Set (abstract data type)",
      "Social science",
      "Sociology",
      "Streaming data",
      "Support vector machine",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Lopez-Lopez",
        "given_name": "Eric"
      },
      {
        "surname": "Pardo",
        "given_name": "Xose M."
      },
      {
        "surname": "Regueiro",
        "given_name": "Carlos V."
      }
    ]
  },
  {
    "title": "HCFNN: High-order coverage function neural network for image classification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108873",
    "abstract": "Recent advances in deep neural networks (DNNs) have mainly focused on innovations in network architecture and loss function. In this paper, we introduce a flexible high-order coverage function (HCF) neuron model to replace the fully-connected (FC) layers. The approximation theorem and proof for the HCF are also presented to demonstrate its fitting ability. Unlike the FC layers, which cannot handle high-dimensional data well, the HCF utilizes weight coefficients and hyper-parameters to mine underlying geometries with arbitrary shapes in an n-dimensional space. To explore the power and potential of our HCF neuron model, a high-order coverage function neural network (HCFNN) is proposed, which incorporates the HCF neuron as the building block. Moreover, a novel adaptive optimization method for weights and hyper-parameters is designed to achieve effective network learning. Comprehensive experiments on nine datasets in several domains validate the effectiveness and generalizability of the HCF and HCFNN. The proposed method provides a new perspective for further developments in DNNs and ensures wide application in the field of image classification. The source code is available at https://github.com/Tough2011/HCFNet.git",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003545",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Block (permutation group theory)",
      "Code (set theory)",
      "Computer science",
      "Evolutionary biology",
      "Field (mathematics)",
      "Function (biology)",
      "Generalizability theory",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Pure mathematics",
      "Set (abstract data type)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Ning",
        "given_name": "Xin"
      },
      {
        "surname": "Tian",
        "given_name": "Weijuan"
      },
      {
        "surname": "Yu",
        "given_name": "Zaiyang"
      },
      {
        "surname": "Li",
        "given_name": "Weijun"
      },
      {
        "surname": "Bai",
        "given_name": "Xiao"
      },
      {
        "surname": "Wang",
        "given_name": "Yuebao"
      }
    ]
  },
  {
    "title": "Iterative structure transformation and conditional random field based method for unsupervised multimodal change detection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108845",
    "abstract": "Change detection between heterogeneous images has become an increasingly interesting research topic in remote sensing. The different appearances and statistics of heterogeneous images bring great challenges to this task. In this paper, we propose an unsupervised iterative structure transformation and conditional random field (IST-CRF) based multimodal change detection (MCD) method, combining an imaging modality-invariant based structure transformation method with a random filed framework specifically designed for MCD, to acquire an optimal change map within a global probabilistic model. IST-CRF first constructs graphs to represent the structures of the images, and transforms the heterogeneous images to the same differential domain by using graph based forward and backward structure transformations. Then, the change vectors are calculated to distinguish the changed and unchanged areas. Finally, in order to classify the change vectors and compute the binary change map, a CRF model is designed to fully explore the spectral-spatial information, which incorporates the change information, local spatially-adjacent neighbor information, and global spectrally-similar neighbor information with a random field framework. As the changed samples will influence the structure transformation and reduce the quality of change vectors, we use an iterative framework to propagate the CRF segmentation results back to the structure transformation process that removes the changed samples, and thus improve the accuracy of change detection. Experiments conducted on different real data sets show the effectiveness of IST-CRF. Source code of the proposed method will be made available at https://github.com/yulisun/IST-CRF.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003260",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Change detection",
      "Chemistry",
      "Computer science",
      "Conditional random field",
      "Field (mathematics)",
      "Gene",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Yuli"
      },
      {
        "surname": "Lei",
        "given_name": "Lin"
      },
      {
        "surname": "Guan",
        "given_name": "Dongdong"
      },
      {
        "surname": "Wu",
        "given_name": "Junzheng"
      },
      {
        "surname": "Kuang",
        "given_name": "Gangyao"
      }
    ]
  },
  {
    "title": "Clustering experience replay for the effective exploitation in reinforcement learning",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108875",
    "abstract": "Reinforcement learning is a useful tool for training an agent to effectively achieve the desired goal in the sequential decision-making problem. It trains the agent to make decision by exploiting the experience in the transitions resulting from the different decisions. To exploit this experience, most reinforcement learning methods replay the explored transitions by uniform sampling. But in this way, it is easy to ignore the last explored transitions. Another way to exploit this experience defines the priority of each transition by the estimation error in training and then replays the transitions according to their priorities. But it only updates the priorities of the transitions replayed at the current training time step, thus the transitions with low priorities will be ignored. In this paper, we propose a clustering experience replay, called CER, to effectively exploit the experience hidden in all explored transitions in the current training. CER clusters and replays the transitions by a divide-and-conquer framework based on time division as follows. Firstly, it divides the whole training process into several periods. Secondly, at the end of each period, it uses k -means to cluster the transitions explored in this period. Finally, it constructs a conditional probability density function to ensure that all kinds of transitions will be sufficiently replayed in the current training. We construct a new method, TD3 _ CER, to implement our clustering experience replay on TD3. Through the theoretical analysis and experiments, we illustrate that our TD3 _ CER is more effective than the existing reinforcement learning methods. The source code can be downloaded from https://github.com/grcai/CER-Master.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003569",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Machine learning",
      "Reinforcement learning"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Min"
      },
      {
        "surname": "Huang",
        "given_name": "Tianyi"
      },
      {
        "surname": "Zhu",
        "given_name": "William"
      }
    ]
  },
  {
    "title": "Inter-Attribute awareness for pedestrian attribute recognition",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108865",
    "abstract": "The task of pedestrian attribute recognition (PAR) is to distinguish a series of person semantic attributes. Generally, existing methods adopt multi-label classification algorithms to tackle the PAR task by utilizing multiple attribute labels. Despite remarkable progress, this kind of method normally ignores relations between different attributes. In order to be aware of relations between attributes, we propose an inter-attribute aware network via vector-neuron capsule for PAR (IAA-Caps). Our IAA-Caps method replaces traditional one-dimensional scalar neurons with two-dimensional vector-neuron capsules by embedding them in IAA-Caps. Specifically, during IAA-Caps training, one dimension in capsules is used to recognize different attributes, and the other dimension is used to strengthen the relations of different attributes. Through considering inter-attribute relations, compared with previous methods that use a heavyweight backbone (e.g., ResNet50 or BN-Inception), a more lightweight backbone (i.e., OSNet) can be adopted in our proposed IAA-Caps to achieve better performance. Experiments are conducted on several PAR benchmark datasets, including PETA, PA-100K, RAPv1, and RAPv2, demonstrating the effectiveness of the proposed IAA-Caps. In addition, experiments also show that the proposed method can improve the performance of PAR on different backbones, showing its generalization ability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003466",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Data mining",
      "Dimension (graph theory)",
      "Economics",
      "Embedding",
      "Generalization",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Junyi"
      },
      {
        "surname": "Huang",
        "given_name": "Yan"
      },
      {
        "surname": "Gao",
        "given_name": "Zhipeng"
      },
      {
        "surname": "Hong",
        "given_name": "Yating"
      },
      {
        "surname": "Zhao",
        "given_name": "Jianqiang"
      },
      {
        "surname": "Du",
        "given_name": "Xinsheng"
      }
    ]
  },
  {
    "title": "Editorial of the special section on CIARP 2021",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.10.004",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002975",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Economics",
      "Face (sociological concept)",
      "Facial expression",
      "Facial recognition system",
      "Feature (linguistics)",
      "Frame (networking)",
      "Linguistics",
      "Machine learning",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Social science",
      "Sociology",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Papa",
        "given_name": "João Paulo"
      },
      {
        "surname": "Tavares",
        "given_name": "João Manuel R.S."
      }
    ]
  },
  {
    "title": "Efficient federated multi-view learning",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108817",
    "abstract": "Multi-view learning aims to explore a global common structure shared by different views collected from multiple individual sources. The nascent field of federated learning tries to learn a global model over distributed networks of devices. This paper shows that multi-view learning is naturally suited to address the feature heterogeneity of the federated setting. We propose a novel model, namely robust federated multi-view learning (FedMVL), which is considered in the following formulation: given a dataset with M views, it is required to train machine learning models while the M views are distributed across M devices or nodes. Considering the unique challenges like stragglers and fault tolerance in federated setting, we derive an iterative federated optimization algorithm that allows each node with the flexibility to approximately address its subproblem. To the best of our knowledge, our model for the first time considers the issues including high communication cost, fault tolerance, and stragglers for distributed multi-view learning. The proposed model also achieves encouraging performance on clustering task compared to closely related methods, as we illustrate through simulations on several real-world datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002989",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Distributed computing",
      "Distributed learning",
      "Economics",
      "Engineering",
      "Fault tolerance",
      "Feature (linguistics)",
      "Federated learning",
      "Field (mathematics)",
      "Flexibility (engineering)",
      "Linguistics",
      "Machine learning",
      "Management",
      "Mathematics",
      "Node (physics)",
      "Pedagogy",
      "Philosophy",
      "Psychology",
      "Pure mathematics",
      "Statistics",
      "Structural engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Shudong"
      },
      {
        "surname": "Shi",
        "given_name": "Wei"
      },
      {
        "surname": "Xu",
        "given_name": "Zenglin"
      },
      {
        "surname": "Tsang",
        "given_name": "Ivor W."
      },
      {
        "surname": "Lv",
        "given_name": "Jiancheng"
      }
    ]
  },
  {
    "title": "BRULÈ: Barycenter-Regularized Unsupervised Landmark Extraction",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108816",
    "abstract": "Unsupervised retrieval of image features is vital for many computer vision tasks where the annotation is missing or scarce. In this work, we propose a new unsupervised approach to detect the landmarks in images, validating it on the popular task of human face key-points extraction. The method is based on the idea of auto-encoding the wanted landmarks in the latent space while discarding the non-essential information (and effectively preserving the interpretability). The interpretable latent space representation (the bottleneck containing nothing but the wanted key-points) is achieved by a new two-step regularization approach. The first regularization step evaluates transport distance from a given set of landmarks to some average value (the barycenter by Wasserstein distance). The second regularization step controls deviations from the barycenter by applying random geometric deformations synchronously to the initial image and to the encoded landmarks. We demonstrate the effectiveness of the approach both in unsupervised and semi-supervised training scenarios using 300-W, CelebA, and MAFL datasets. The proposed regularization paradigm is shown to prevent overfitting, and the detection quality is shown to improve beyond the state-of-the-art face models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002977",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Decoding methods",
      "Interpretability",
      "Landmark",
      "Machine learning",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "Bespalov",
        "given_name": "Iaroslav"
      },
      {
        "surname": "Buzun",
        "given_name": "Nazar"
      },
      {
        "surname": "Dylov",
        "given_name": "Dmitry V."
      }
    ]
  },
  {
    "title": "Micro-expression spotting based on optical flow features",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.09.009",
    "abstract": "Expression is the changes of facial organs due to facial muscle movement and an important way of human emotional interaction. Neurophysiological studies show that micro-expressions (MEs) are not controlled by subjective consciousness and reflect people's real emotions. That is why MEs have significant value in public security applications. This paper proposes an automatic ME spotting method of high accuracy and interpretability. Firstly, we design the nose tip location-based image alignment method to remove global displacement caused by head shaking. Secondly, according to the action unit definition in the face coding system (FACS), we select fourteen regions of interest (ROI) to capture subtle facial movements. The dense optical flow is introduced to estimate local movements and time-domain variations of the ROIs. Thirdly, we design a peak detection method on the time-domain variation curves to locate the movement intervals precisely. Lastly, we propose an overlapping index to measure the consistency of changes in different organs. Evaluation on the CAS(ME)2 and SAMM Long Video database shows that our ME spotting method may achieve better accuracy with a relatively lower computation cost and can be applied to similar facial image processing applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002720",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Face (sociological concept)",
      "Facial expression",
      "Ground truth",
      "Image (mathematics)",
      "Interpretability",
      "Optical flow",
      "Pattern recognition (psychology)",
      "Social science",
      "Sociology",
      "Spotting"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Yuhong"
      },
      {
        "surname": "Xu",
        "given_name": "Zhongliang"
      },
      {
        "surname": "Ma",
        "given_name": "Lin"
      },
      {
        "surname": "Li",
        "given_name": "Haifeng"
      }
    ]
  },
  {
    "title": "Fuzzy prototype selection-based classifiers for imbalanced data. Case study",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.07.003",
    "abstract": "Imbalanced data are popular in the machine learning community due to their likelihood of appearing in real-world application areas and the problems they present for classical classifiers. The goal of this work is to extend the capabilities of prototype-based classifiers using fuzzy similarity relations and to make them sensitive to class-imbalanced data classification. This paper proposes two new fuzzy logic-based prototype selection classifiers for imbalanced datasets, Imb-SPBASIR-Fuzzy_V1 (FPS-v1) and Imb-SPBASIR-Fuzzy_V2 (FPS-v1), and shows a comparative study of them with state-of-the-art methods on public datasets from the UCI machine learning repository. The results on the selected datasets suggest that fuzzy logic-based prototype selection classifiers perform well and efficiently, indicating that it is a viable alternative. The fuzzy relationships provided by this approach allow better results than the state-of-the-art models. Further analysis showed that the proposed fuzzy-based prototypes methods permit obtaining more accurate to deal with the correct prophylaxis, timely diagnosis and treatment of postoperative mediastinitis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002148",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Data mining",
      "Fuzzy logic",
      "Image (mathematics)",
      "Machine learning",
      "Selection (genetic algorithm)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Rodríguez Alvarez",
        "given_name": "Yanela"
      },
      {
        "surname": "García Lorenzo",
        "given_name": "María Matilde"
      },
      {
        "surname": "Caballero Mota",
        "given_name": "Yailé"
      },
      {
        "surname": "Filiberto Cabrera",
        "given_name": "Yaima"
      },
      {
        "surname": "García Hilarión",
        "given_name": "Isabel M."
      },
      {
        "surname": "Machado Montes de Oca",
        "given_name": "Daniela"
      },
      {
        "surname": "Bello Pérez",
        "given_name": "Rafael"
      }
    ]
  },
  {
    "title": "Online Adaptive Kernel Learning with Random Features for Large-scale Nonlinear Classification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108862",
    "abstract": "In the field of support vector machines, online random feature map algorithms are very important methods for large-scale nonlinear classification problems. At present, the existing methods have the following shortcomings: (1) If only the hyperplane vector is updated during learning while the random feature components are fixed, there is no guarantee that these online methods can adapt to the change of data distribution shape when the data is coming one by one. (2) When the kernel is selected improperly, the samples mapped to an inappropriate space may not be well classified. In order to overcome these shortcomings, considering the fact that iteratively updating random feature components can make data better fit in the current space and lead to the flexible adjustment of the kernel function, random features based online adaptive kernel learning (RF-OAK) is proposed for large-scale nonlinear classification problems. Theoretical analysis of the proposed algorithm is also provided. The experimental results and the Wilcoxon signed-ranks test show that in terms of test accuracy, the proposed method is significantly better than the state-of-the-art online feature mapping classification methods. Compared with the deep learning algorithms, the training time of RF-OAK is shorter. In terms of test accuracy, RF-OAK is better than online algorithm and comparable with offline algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003430",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Feature (linguistics)",
      "Feature vector",
      "Geometry",
      "Hyperplane",
      "Kernel (algebra)",
      "Kernel method",
      "Linguistics",
      "Machine learning",
      "Mann–Whitney U test",
      "Mathematics",
      "Nonlinear system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Random field",
      "Random forest",
      "Scale (ratio)",
      "Statistics",
      "Support vector machine",
      "Wilcoxon signed-rank test"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yingying"
      },
      {
        "surname": "Yang",
        "given_name": "Xiaowei"
      }
    ]
  },
  {
    "title": "Multi-View correlation distillation for incremental object detection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108863",
    "abstract": "In real applications, new object classes often emerge after the detection model has been trained on a prepared dataset with fixed classes. Fine-tuning the old model with only new data will lead to a well-known phenomenon of catastrophic forgetting, which severely degrades the performance of modern object detectors. Due to the storage burden, data privacy and time consumption, sometimes it is impractical to train the model from scratch with all data of both old and new classes. In this paper, we propose a novel Multi-View Correlation Distillation (MVCD) based incremental object detection method, which explores the intra-feature correlations in the feature space of the object detector. To better transfer the knowledge learned from the old classes and maintain the ability to learn new classes, we select the sample-specific discriminative features from channel-wise, point-wise and instance-wise views. Meanwhile, the correlation distillation losses on the selective features are designed to regularize the learning of the incremental object detector. A new metric named Stability-Plasticity-mAP (SPmAP) is proposed to evaluate the incremental learning performance as a complementary metric to mAP, which integrates the metrics for the stability on old classes and the plasticity on new classes in incremental object detection. The extensive experiments conducted on VOC2007 and COCO demonstrate that MVCD achieves a better trade-off between stability and plasticity than state-of-the-art first-order distillation-based incremental object detection methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003442",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Discriminative model",
      "Distillation",
      "Engineering",
      "Feature (linguistics)",
      "Forgetting",
      "Linguistics",
      "Machine learning",
      "Metric (unit)",
      "Object (grammar)",
      "Object detection",
      "Operations management",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Stability (learning theory)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Dongbao"
      },
      {
        "surname": "Zhou",
        "given_name": "Yu"
      },
      {
        "surname": "Zhang",
        "given_name": "Aoting"
      },
      {
        "surname": "Sun",
        "given_name": "Xurui"
      },
      {
        "surname": "Wu",
        "given_name": "Dayan"
      },
      {
        "surname": "Wang",
        "given_name": "Weiping"
      },
      {
        "surname": "Ye",
        "given_name": "Qixiang"
      }
    ]
  },
  {
    "title": "Multiple-solutions RANSAC for finding axes of symmetry in fragments of objects",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108805",
    "abstract": "The problem of “finding best lines passing through a set of straight lines” has appeared in applications such as archaeological pottery analysis, precision manufacturing, and 3D modelling. In these applications, an instance of this problem is finding the symmetry axis of a symmetrical object from a set of its surface normal lines. We show that the mentioned instance of the problem may have two meaningful local minima, one of which is the symmetry axis, a fact that has been neglected in the literature. A multiple-solutions RANSAC algorithm is proposed for finding initial estimates of both local minima in the presence of outliers. Then, a coordinate-descent algorithm is presented that starts from these initial estimates and finds the local minima of the problem. The proposed coordinate-descent method does not involve any line search procedure, and its convergence is guaranteed. We also provide a proof for the rate of the convergence.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322002862",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convergence (economics)",
      "Coordinate descent",
      "Descent (aeronautics)",
      "Economic growth",
      "Economics",
      "Geometry",
      "Gradient descent",
      "Image (mathematics)",
      "Line (geometry)",
      "Line search",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Maxima and minima",
      "Meteorology",
      "Object (grammar)",
      "Outlier",
      "Path (computing)",
      "Physics",
      "Programming language",
      "RANSAC",
      "Set (abstract data type)",
      "Symmetry (geometry)"
    ],
    "authors": [
      {
        "surname": "Nasiri",
        "given_name": "Seyed-Mahdi"
      },
      {
        "surname": "Hosseini",
        "given_name": "Reshad"
      },
      {
        "surname": "Moradi",
        "given_name": "Hadi"
      }
    ]
  },
  {
    "title": "To Actively Initialize Active Learning",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108836",
    "abstract": "Though much effort has been spent on designing new active learning algorithms, little attention has been paid to the initialization problem of active learning, i.e., how to find a set of labeled samples which contains at least one instance per category. This work identifies the initialization of active learning as a separate and novel research problem, reviews existing methods that can be adapted to be used for this task and, in addition, proposes a new active initialization criterion: the Nearest Neighbor Criterion. Experiments on 16 benchmark datasets verify that the novel method often finds an initialization set with fewer queried samples than other methods do.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200317X",
    "keywords": [
      "Active learning (machine learning)",
      "Artificial intelligence",
      "Computer science",
      "Computer vision"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Yazhou"
      },
      {
        "surname": "Loog",
        "given_name": "Marco"
      }
    ]
  },
  {
    "title": "Adaptive weighted guided image filtering for depth enhancement in shape-from-focus",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108900",
    "abstract": "Existing shape from focus (SFF) techniques cannot preserve depth edges and fine structural details from a sequence of multi-focus images. Moreover, noise in the sequence affects the accuracy of the depth map. In this paper, a novel depth enhancement algorithm for the SFF based on an adaptive weighted guided image filtering (AWGIF) is proposed to address the above issues. The AWGIF is applied to decompose an initial depth map estimated by the traditional SFF into base and detail layers. In order to preserve the edges accurately in the refined depth map, the guidance image is constructed from the sequence, and the coefficient of the AWGIF is utilized to suppress the noise while enhancing the fine depth details. Experiments on real and synthetic objects demonstrate the superiority of our algorithm in terms of anti-noise, and the ability to preserve depth edges and fine structural details w.r.t. existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003818",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Base (topology)",
      "Biology",
      "Computer science",
      "Computer vision",
      "Depth map",
      "Focus (optics)",
      "Genetics",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Noise (video)",
      "Optics",
      "Physics",
      "Sequence (biology)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yuwen"
      },
      {
        "surname": "Li",
        "given_name": "Zhengguo"
      },
      {
        "surname": "Zheng",
        "given_name": "Chaobing"
      },
      {
        "surname": "Wu",
        "given_name": "Shiqian"
      }
    ]
  },
  {
    "title": "2PESNet: Towards online processing of temporal action localization",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108871",
    "abstract": "Existing online video processing methods such as online action detection focus on a frame-level understanding for high responsiveness. However, it has a fundamental limitation in that it lacks instance-level understanding of videos, making it difficult to be applied to higher-level vision tasks. The instance-level action detection, known as Temporal Action Localization (TAL), have limitations when applying to the online settings. In this work, we introduce a new task that aims to detect action instances of videos in an online setting, named Online Temporal Action Localization (OnTAL). To tackle this problem, we propose a 2-Pass End/Start detection Network (2PESNet) that detects action instances by effectively finding the start and end of an action instance. Additionally, we propose a two-stage action end detection method to further improve the performance. Extensive experiments on THUMOS’14 and ActivityNet v1.3 demonstrate that our model is able to take both accuracy and responsiveness when predicting action instances from streaming videos.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003521",
    "keywords": [
      "Action (physics)",
      "Action recognition",
      "Activity detection",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Economics",
      "Focus (optics)",
      "Frame (networking)",
      "Machine learning",
      "Management",
      "Optics",
      "Physics",
      "Quantum mechanics",
      "Task (project management)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Young Hwi"
      },
      {
        "surname": "Nam",
        "given_name": "Seonghyeon"
      },
      {
        "surname": "Kim",
        "given_name": "Seon Joo"
      }
    ]
  },
  {
    "title": "Video summarization with a convolutional attentive adversarial network",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108840",
    "abstract": "With the explosive growth of video data, video summarization, which attempts to seek the minimum subset of frames while still conveying the main story, has become one of the hottest topics. Nowadays, substantial achievements have been made by supervised learning techniques, especially after the emergence of deep learning. However, it is extremely expensive and difficult to construct a large-scale video summarization dataset through human annotation. To address this problem, we propose a convolutional attentive adversarial network (CAAN), whose key idea is to build a deep summarizer in an unsupervised way. Upon the generative adversarial network, our overall framework consists of a generator and a discriminator. The former predicts importance scores for all the frames of a video while the latter tries to distinguish the score-weighted frame features from original frame features. To capture the global and local temporal relationship of video frames, the generator employs a fully convolutional sequence network to build global representation of a video, and an attention-based network to predict normalized importance scores. To optimize the parameters, our objective function is composed of three loss functions, which can guide the frame-level importance score prediction collaboratively. To validate this proposed method, we have conducted extensive experiments on two public benchmarks SumMe and TVSum. The results show the superiority of our proposed method against other state-of-the-art unsupervised approaches. Our method even outperforms some published supervised approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003211",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Automatic summarization",
      "Computer science",
      "Computer security",
      "Construct (python library)",
      "Convolutional neural network",
      "Deep learning",
      "Detector",
      "Discriminator",
      "Feature learning",
      "Frame (networking)",
      "Generator (circuit theory)",
      "Key (lock)",
      "Key frame",
      "Law",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Power (physics)",
      "Programming language",
      "Quantum mechanics",
      "Representation (politics)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Liang",
        "given_name": "Guoqiang"
      },
      {
        "surname": "Lv",
        "given_name": "Yanbing"
      },
      {
        "surname": "Li",
        "given_name": "Shucheng"
      },
      {
        "surname": "Zhang",
        "given_name": "Shizhou"
      },
      {
        "surname": "Zhang",
        "given_name": "Yanning"
      }
    ]
  },
  {
    "title": "A survey of robust adversarial training in pattern recognition: Fundamental, theory, and methodologies",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108889",
    "abstract": "Deep neural networks have achieved remarkable success in machine learning, computer vision, and pattern recognition in the last few decades. Recent studies, however, show that neural networks (both shallow and deep) may be easily fooled by certain imperceptibly perturbed input samples called adversarial examples. Such security vulnerability has resulted in a large body of research in recent years because real-world threats could be introduced due to the vast applications of neural networks. To address the robustness issue to adversarial examples particularly in pattern recognition, robust adversarial training has become one mainstream. Various ideas, methods, and applications have boomed in the field. Yet, a deep understanding of adversarial training including characteristics, interpretations, theories, and connections among different models has remained elusive. This paper presents a comprehensive survey trying to offer a systematic and structured investigation on robust adversarial training in pattern recognition. We start with fundamentals including definition, notations, and properties of adversarial examples. We then introduce a general theoretical framework with gradient regularization for defending against adversarial samples - robust adversarial training with visualizations and interpretations on why adversarial training can lead to model robustness. Connections will also be established between adversarial training and other traditional learning theories. After that, we summarize, review, and discuss various methodologies with defense/training algorithms in a structured way. Finally, we present analysis, outlook, and remarks on adversarial training.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003703",
    "keywords": [
      "Adversarial machine learning",
      "Adversarial system",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Deep learning",
      "Deep neural networks",
      "Gene",
      "Machine learning",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Qian",
        "given_name": "Zhuang"
      },
      {
        "surname": "Huang",
        "given_name": "Kaizhu"
      },
      {
        "surname": "Wang",
        "given_name": "Qiu-Feng"
      },
      {
        "surname": "Zhang",
        "given_name": "Xu-Yao"
      }
    ]
  },
  {
    "title": "Practical protection against video data leakage via universal adversarial head",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108834",
    "abstract": "While online video sharing becomes more popular, it also causes unconscious leakage of personal information in the video retrieval systems like deep hashing. A snoop can collect more users’ private information from the video database by querying similar videos. This paper focuses on bypassing the deep video hashing based retrieval to prevent information from being maliciously collected. We propose universal adversarial head (UAH), which crafts adversarial query videos by prepending the original videos with a sequence of adversarial frames to perturb the normal hash codes in the Hamming space. This adversarial head can be generated only with a few natural videos, and mislead the retrieval system to return irrelevant videos when it is applied to most query videos. Furthermore, to obey the principle of information protection, we expand the proposed method to a data-free paradigm to generate the UAH, without access to users’ original videos. Extensive experiments demonstrate the effectiveness of our method in misleading deep video hashing under both white-box and black-box settings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003156",
    "keywords": [
      "Adversarial system",
      "Algorithm",
      "Artificial intelligence",
      "Block code",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Decoding methods",
      "Hamming code",
      "Hamming space",
      "Hash function",
      "Information leakage",
      "Information retrieval"
    ],
    "authors": [
      {
        "surname": "Bai",
        "given_name": "Jiawang"
      },
      {
        "surname": "Chen",
        "given_name": "Bin"
      },
      {
        "surname": "Gao",
        "given_name": "Kuofeng"
      },
      {
        "surname": "Wang",
        "given_name": "Xuan"
      },
      {
        "surname": "Xia",
        "given_name": "Shu-Tao"
      }
    ]
  },
  {
    "title": "Causal learner: A toolbox for causal structure and Markov blanket learning",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.09.021",
    "abstract": "Causal Learner is a toolbox for learning causal structure and Markov blanket (MB) from data. It integrates functions for generating simulated Bayesian network data, a set of state-of-the-art global causal structure learning algorithms, a set of state-of-the-art local causal structure learning algorithms, a set of state-of-the-art MB learning algorithms, and abundant functions for evaluating algorithms. The data generation part of Causal Learner is written in R, and the rest of Causal Learner is written in MATLAB. Causal Learner aims to provide researchers and practitioners with an open-source platform for causal learning from data and for the development and evaluation of new causal learning algorithms. The Causal Learner project is available at http://bigdata.ahu.edu.cn/causal-learner.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002914",
    "keywords": [
      "Artificial intelligence",
      "Bayesian network",
      "Causal inference",
      "Causal model",
      "Causal structure",
      "Computer science",
      "Econometrics",
      "Machine learning",
      "Markov blanket",
      "Markov chain",
      "Markov model",
      "Markov property",
      "Mathematics",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Set (abstract data type)",
      "Statistics",
      "Toolbox"
    ],
    "authors": [
      {
        "surname": "Ling",
        "given_name": "Zhaolong"
      },
      {
        "surname": "Yu",
        "given_name": "Kui"
      },
      {
        "surname": "Zhang",
        "given_name": "Yiwen"
      },
      {
        "surname": "Liu",
        "given_name": "Lin"
      },
      {
        "surname": "Li",
        "given_name": "Jiuyong"
      }
    ]
  },
  {
    "title": "Kernel embedding transformation learning for graph matching",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.09.016",
    "abstract": "Graph matching, which aims to establish correspondences between two geometrical graphs, is a general and powerful tool for pattern recognition and computer vision. However, many factors degrade the matching accuracy. The graph structure suffering from deformation and rotation variations is a key issue in the process of matching. In this work, we propose a joint framework in the reproducing kernel Hilbert space (RKHS) for graph matching with deformation and rotation variations, which incorporates the kernelized unary alignment and local structure alignment into a joint framework. Specifically, the proposed method is able to enhance the node to node correspondence and the edge to edge correspondence and avoids the effect of deformation and rotation by maximizing the similarities between the source graph and the transformed target graph in the reproducing kernel Hilbert space. Meanwhile, an effective algorithm is presented to solve the joint framework. Comprehensive discussion, involving convergence analysis and parameter sensitive analysis, are as well proposed. Promising experimental results in the variety of graph matching tasks such as deformation and rotation are provided to evidence the superiority of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002872",
    "keywords": [
      "3-dimensional matching",
      "Artificial intelligence",
      "Biochemistry",
      "Bipartite graph",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Embedding",
      "Gene",
      "Graph",
      "Graph kernel",
      "Kernel (algebra)",
      "Kernel embedding of distributions",
      "Kernel method",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Statistics",
      "Support vector machine",
      "Theoretical computer science",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Yu-Feng"
      },
      {
        "surname": "Chen",
        "given_name": "Long"
      },
      {
        "surname": "Huang",
        "given_name": "Ke-Kun"
      },
      {
        "surname": "Zhu",
        "given_name": "Hu"
      },
      {
        "surname": "Xu",
        "given_name": "Guoxia"
      }
    ]
  },
  {
    "title": "Multi-feature deep information bottleneck network for breast cancer classification in contrast enhanced spectral mammography",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108858",
    "abstract": "There is considerable variation in the size, shape and location of tumours, which makes it challenging for radiologists to diagnose breast cancer. Automated diagnosis of breast cancer from Contrast Enhanced Spectral Mammography (CESM) can support clinical decision making. However, existing methods fail to obtain an effective representation of the CESM and ignore the relationships between images. In this paper, we investigated for the first time a novel and flexible multimodal representation learning method, multi-feature deep information bottleneck (MDIB), for breast cancer classification in CESM. Specifically, the method incorporated an information bottleneck (IB)-based module to learn the prominent representation that provide concise input while informative for the classification. In addition, we creatively extended IB theory to multi-feature IB, which facilitates the learning of relevant features for classification between CESM images. To validate our method, experiments were conducted on our private and public datasets. The classification results of our method were also compared with those of state-of-the-art methods. The experiment results proved the effectiveness and the efficiency of the proposed method. We release our code at https://github.com/sjq5263/MDIB-for-CESM-classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003399",
    "keywords": [
      "Artificial intelligence",
      "Bottleneck",
      "Breast cancer",
      "Cancer",
      "Computer science",
      "Contrast (vision)",
      "Embedded system",
      "Feature (linguistics)",
      "Feature learning",
      "Information bottleneck method",
      "Internal medicine",
      "Law",
      "Linguistics",
      "Machine learning",
      "Mammography",
      "Medicine",
      "Mutual information",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Jingqi"
      },
      {
        "surname": "Zheng",
        "given_name": "Yuanjie"
      },
      {
        "surname": "Wang",
        "given_name": "Jing"
      },
      {
        "surname": "Ullah",
        "given_name": "Muhammad Zakir"
      },
      {
        "surname": "Li",
        "given_name": "Xuecheng"
      },
      {
        "surname": "Zou",
        "given_name": "Zhenxing"
      },
      {
        "surname": "Ding",
        "given_name": "Guocheng"
      }
    ]
  },
  {
    "title": "Cubic-cross convolutional attention and count prior embedding for smoke segmentation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108902",
    "abstract": "It is very challenging to accurately segment smoke images because smoke has some adverse properties, such as semi-transparency and blurry boundary. Aiming at solving these problems, we first fuse convolutional results along different axes to equivalently produce a cubic-cross convolutional kernel, which enlarges receptive fields at affordable computational costs for capturing long-range dependency of smoke pixels, and then we propose a Cubic-cross Convolutional Attention (CCA). To embed global category information, we propose a count prior structure to model and supervise the count of smoke pixels. To ensure the network can correctly extract a count prior map, we impose a regression loss on the count prior map and corresponding ideal count map directly calculated from its ground truth. Then we multiply the reshaped input by the count prior map to produce a Count Prior Attention (CPA) map, which is upsampled to generate the final output. A cross entropy loss is used to supervise the final segmentation. Finally, we use ResNet50 for feature encoding, and stack CCA and CPA together to propose a Cubic-cross convolutional attention and Count prior Embedding Network (CCENet) for smoke segmentation. Experiments on both synthetic and real smoke datasets show that our method outperforms existing state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003831",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Embedding",
      "Feature (linguistics)",
      "Ground truth",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Yuan",
        "given_name": "Feiniu"
      },
      {
        "surname": "Dong",
        "given_name": "Zeshu"
      },
      {
        "surname": "Zhang",
        "given_name": "Lin"
      },
      {
        "surname": "Xia",
        "given_name": "Xue"
      },
      {
        "surname": "Shi",
        "given_name": "Jinting"
      }
    ]
  },
  {
    "title": "An improved entity recognition approach to cyber-social knowledge provision of intellectual property using a CRF-LSTM model",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.10.001",
    "abstract": "With the development of cutting-edge IT technologies, e.g. Big Data, Knowledge Engineering, etc., traditional Intellectual Property (IP) services have depicted high redundancy and low efficiency during management of such large-scale of data. Recent advancement of Artificial Intelligence (AI) and Deep Learning (DL) models has been accelerating relevant research activities being investigated on Knowledge Graph (KG) schemes and applications in different domains, such as medical services, social media, etc. However, when IP services and their cyber-social provision are taken into account, relevant approaches suffer from unbalanced labels against training results, and inappropriate evaluation metrics not well reflecting the impact of the unbalance. In this paper, a deep learning model combining Conditional Random Field and Bidirectional LSTM has been proposed, in order to achieve named entity recognition with unbalanced labels. An adaptive metric, G-Score was introduced to compare the fitting ability of models by evaluating the gap between Precision and Recall. According to the results, the proposed model can effectively recognize the potential named entities with outperformance over other relevant models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552200294X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data science",
      "Epistemology",
      "Intellectual property",
      "Knowledge management",
      "Machine learning",
      "Natural language processing",
      "Operating system",
      "Philosophy",
      "Property (philosophy)",
      "Social knowledge",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yang"
      },
      {
        "surname": "Vijayakumar",
        "given_name": "Pandi"
      },
      {
        "surname": "Gupta",
        "given_name": "Brij B."
      },
      {
        "surname": "Alhalabi",
        "given_name": "Wadee"
      },
      {
        "surname": "Sivaraman",
        "given_name": "Audithan"
      }
    ]
  },
  {
    "title": "Multivariate multi-layer classifier",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108896",
    "abstract": "The variance-ratio binary multi-layer classifier (VRBMLC) has been recently proposed and shown to outperform conventional binary decision trees (BDTs). Though effective with better interpretability, the VRBMLC generates deep layers of tree nodes as it employs a one-feature-at-a-time binary split at each layer. To further condense the tree depth and enhance the classification performance, this research proposes a multivariate multi-layer classifier that applies a variance-ratio criterion to enable ternary splits of each tree node and that integrates the oblique discriminant hyperplane in the tree node. We benchmark 16 state-of-the-art univariate and multivariate classifiers on 43 publicly available datasets. The results show that the proposed methods greatly simplify the tree structure and yield a significantly higher average accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003776",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Binary tree",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Decision tree",
      "Gradient boosting",
      "Interpretability",
      "Machine learning",
      "Mathematics",
      "Multivariate statistics",
      "Pattern recognition (psychology)",
      "Random forest",
      "Univariate"
    ],
    "authors": [
      {
        "surname": "Zeng",
        "given_name": "Huanze"
      },
      {
        "surname": "Chen",
        "given_name": "Argon"
      }
    ]
  },
  {
    "title": "DFR-ST: Discriminative feature representation with spatio-temporal cues for vehicle re-identification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108887",
    "abstract": "Vehicle re-identification (re-ID) aims to discover and match the target vehicles from a gallery image set taken by different cameras on a wide range of road networks. It is crucial for lots of applications such as security surveillance and traffic management. The remarkably similar appearances of distinct vehicles and the significant changes in viewpoints and illumination conditions pose grand challenges to vehicle re-ID. Conventional solutions focus on designing global visual appearances without sufficient consideration of vehicles’ spatio-temporal relationships in different images. This paper proposes a discriminative feature representation with spatio-temporal clues (DFR-ST) for vehicle re-ID. It is capable of building robust features in the embedding space by involving appearance and spatio-temporal information. The proposed DFR-ST constructs an appearance model for a multi-grained visual representation by a two-stream architecture and a spatio-temporal metric to provide complementary information based on this multi-modal information. Experimental results on four public datasets demonstrate DFR-ST outperforms the state-of-the-art methods, which validates the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003685",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Embedding",
      "Engineering",
      "Feature (linguistics)",
      "Feature learning",
      "Feature vector",
      "Focus (optics)",
      "Identification (biology)",
      "Law",
      "Linguistics",
      "Metric (unit)",
      "Modal",
      "Operations management",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Polymer chemistry",
      "Programming language",
      "Representation (politics)",
      "Set (abstract data type)",
      "Viewpoints",
      "Visual arts",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Tu",
        "given_name": "Jingzheng"
      },
      {
        "surname": "Chen",
        "given_name": "Cailian"
      },
      {
        "surname": "Huang",
        "given_name": "Xiaolin"
      },
      {
        "surname": "He",
        "given_name": "Jianping"
      },
      {
        "surname": "Guan",
        "given_name": "Xinping"
      }
    ]
  },
  {
    "title": "Learning deep morphological networks with neural architecture search",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108893",
    "abstract": "Deep Neural Networks (DNNs) are generated by sequentially performing linear and non-linear processes. The combination of linear and non-linear procedures is critical for generating a sufficiently deep feature space. Most non-linear operators are derivations of activation functions or pooling functions. Mathematical morphology is a branch of mathematics that provides non-linear operators for various image processing problems. This paper investigates the utility of integrating these operations into an end-to-end deep learning framework. DNNs are designed to acquire a realistic representation for a particular job. Morphological operators give topological descriptors that convey salient information about the shapes of objects depicted in images. We propose a method based on meta-learning to incorporate morphological operators into DNNs. The learned architecture demonstrates how our novel morphological operations significantly increase DNN performance on various tasks, including picture classification, edge detection, and semantic segmentation. Our codes are available at https://nao-morpho.github.io/.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003740",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Deep learning",
      "Deep neural networks",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Image processing",
      "Law",
      "Linear map",
      "Linguistics",
      "Mathematical morphology",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Pooling",
      "Pure mathematics",
      "Representation (politics)",
      "Salient",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Yufei"
      },
      {
        "surname": "Belkhir",
        "given_name": "Nacim"
      },
      {
        "surname": "Angulo",
        "given_name": "Jesus"
      },
      {
        "surname": "Yao",
        "given_name": "Angela"
      },
      {
        "surname": "Franchi",
        "given_name": "Gianni"
      }
    ]
  },
  {
    "title": "Knowledge points navigation based on three-way concept lattice for autonomous learning",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.10.002",
    "abstract": "In view of the ability of the three-way concept lattices to describe the correlations and hierarchical relationships among the knowledge points, this paper proposes a knowledge point navigation approach based on the three-way concept lattices for autonomous learning. First, this paper constructs the formal context of the exercises-knowledge points according to the corresponding relationship between the exercises and knowledge points; then, the three-way concept lattice generation algorithms are used to obtain the AE-concept lattice and OE-concept lattice; finally, this paper provides students with an effective autonomous learning path and knowledge points navigation Hasse diagram to achieve efficient guidance of autonomous learning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002951",
    "keywords": [
      "Acoustics",
      "Algorithm",
      "Artificial intelligence",
      "Autonomous learning",
      "Computer science",
      "Discrete mathematics",
      "Formal concept analysis",
      "Geometry",
      "Hasse diagram",
      "Lattice (music)",
      "Lattice Miner",
      "Machine learning",
      "Mathematics",
      "Mathematics education",
      "Partially ordered set",
      "Physics",
      "Point (geometry)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Hao",
        "given_name": "Fei"
      },
      {
        "surname": "Gong",
        "given_name": "Yanqi"
      },
      {
        "surname": "Yu",
        "given_name": "Wangyang"
      },
      {
        "surname": "Loia",
        "given_name": "Vincenzo"
      }
    ]
  },
  {
    "title": "Effective full-scale detection for salient object based on condensing-and-filtering network",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108904",
    "abstract": "With the development of deep learning, salient object detection methods have made great progress. However, there are still two challenges: 1) The lack of rich features extracted from multiple perspectives at different encoder levels results in the omission of salient objects with varying scales. 2) The ineffective fusion of multi-level features during decoding dilutes the saliency features, which destroys the purity of the predicted maps. In this paper, we design a Condensing-and-Filtering Network (CFNet), in which a saliency pyramid condensing module (SPCM) and a saliency filtering module (SFM) are proposed to solve the above two problems respectively. Specifically, SPCM introduces pyramid convolution as the basic unit to condense full-scale features from global and local perspectives at each level of the encoder. SFM is equipped with an ingenious ‘funnel’ structure to effectively filter multi-level features and supplement details, which makes the fusion of features more robust. The two modules complement each other, so that the full-scale features can be used effectively to predict salient objects. Extensive experimental results on five benchmark datasets demonstrate that our method performs favourably against the state-of-the-art approaches, and also shows superiority in terms of speed (16.18ms) and FLOPs (21.19G). Meanwhile, we extend our CFNet to the task of RGB-D salient object detection and achieve better results, which further demonstrate its effectiveness. The code will be made available.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003855",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Bottleneck",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Decoding methods",
      "Embedded system",
      "Encoder",
      "Filter (signal processing)",
      "Geodesy",
      "Geography",
      "Geometry",
      "Mathematics",
      "Object (grammar)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Pyramid (geometry)",
      "Quantum mechanics",
      "Salient",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Xinyu"
      },
      {
        "surname": "Sun",
        "given_name": "Meijun"
      },
      {
        "surname": "Han",
        "given_name": "Yahong"
      },
      {
        "surname": "Wang",
        "given_name": "Zheng"
      },
      {
        "surname": "Tian",
        "given_name": "Qi"
      }
    ]
  },
  {
    "title": "Reconstruction of fragmented trajectories of collective motion using Hadamard deep autoencoders",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108891",
    "abstract": "Learning dynamics of collectively moving agents such as fish or humans is an essential task in research. Due to phenomena such as occlusion or change of illumination, the multi-object methods tracking such dynamics may lose the tracks of the agents which may result in fragmentations of trajectories. Here, we present an extended deep autoencoder (DA) that we train only on the fully observed segments of the trajectories by defining its loss function as the Hadamard product of a binary indicator matrix with the absolute difference between the outputs and the labels. The trajectory matrix of the agents practicing collective motion is low-rank due to mutual interactions and dependencies between the agents that we utilize as the underlying pattern that our Hadamard deep autoencoder (HDA) codes during its training. The performance of this HDA is compared with that of a low-rank matrix completion scheme in the context of fragmented trajectory reconstruction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003727",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Astronomy",
      "Autoencoder",
      "Biology",
      "Composite material",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Deep learning",
      "Hadamard transform",
      "Materials science",
      "Mathematical analysis",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Motion (physics)",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Physics",
      "Trajectory"
    ],
    "authors": [
      {
        "surname": "Gajamannage",
        "given_name": "Kelum"
      },
      {
        "surname": "Park",
        "given_name": "Yonggi"
      },
      {
        "surname": "Paffenroth",
        "given_name": "Randy"
      },
      {
        "surname": "Jayasumana",
        "given_name": "Anura P."
      }
    ]
  },
  {
    "title": "Multi-scale attention-based pseudo-3D convolution neural network for Alzheimer’s disease diagnosis using structural MRI",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108825",
    "abstract": "Recently, deep learning based Computer-Aided Diagnosis methods have been widely utilized due to their highly effective diagnosis of patients. Although Convolutional Neural Networks (CNNs) are capable of extracting the latent structural characteristics of dementia and of capturing the changes of brain anatomy in Magnetic Resonance Imaging (MRI) scans, the high-dimensional input to a deep CNN usually makes the network difficult to train, and affects its diagnostic accuracy. In this paper, a novel method called the hierarchical pseudo-3D convolution neural network based on a kernel attention mechanism with a new global context block, which is abbreviated as “PKG-Net”, is proposed to accurately predict Alzheimer’s disease even when the input features are complex. Specifically, the proposed network first extracts multi-scale features from pre-processed images. Second, the attention mechanism and global context blocks are applied to combine features from different layers to hierarchically transform the MRI into more compact high-level features. Then, a joint loss function is used to train the proposed network to generate more distinguishing features, which improve the generalization performance of the network. In addition, we combine our method with different architectures. Extensive experiments are conducted to analyze the performance of the PKG-Net with different hyper-parameters and architectures. Finally, in order to verify the effectiveness of our method on Alzheimer’s disease diagnosis, we carry out extensive experiments on the ADNI dataset, and compare the results of our method with that of existing methods in terms of accuracy, recall and precision. Furthermore, our network can fully take advantage of the deep 3D convolutional neural network for automatic feature extraction and representation, and thus can avoid the limitation of low processing efficiency caused by the preprocessing procedure in which a specific area needs to be annotated in advance. Finally, we evaluate our proposed framework using two public datasets, ADNI-1 and ADNI-2, and the experimental results show that our proposed framework can achieve superior performance over state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003065",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Block (permutation group theory)",
      "Combinatorics",
      "Computer science",
      "Context (archaeology)",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Deep learning",
      "Geometry",
      "Kernel (algebra)",
      "Machine learning",
      "Mathematics",
      "Paleontology",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Pei",
        "given_name": "Zhao"
      },
      {
        "surname": "Wan",
        "given_name": "Zhiyang"
      },
      {
        "surname": "Zhang",
        "given_name": "Yanning"
      },
      {
        "surname": "Wang",
        "given_name": "Miao"
      },
      {
        "surname": "Leng",
        "given_name": "Chengcai"
      },
      {
        "surname": "Yang",
        "given_name": "Yee-Hong"
      }
    ]
  },
  {
    "title": "Multi-stage and multi-branch network with similar expressions label distribution learning for facial expression recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.09.007",
    "abstract": "This paper proposes a novel facial expression recognition network, called Multi-Stage and Similar Expressions Label Distribution Learning Network (MSCL). Our method is based on the observations of the labels ambiguity between similar expressions in complex wild scenes, for there are inherently similar features between them that are difficult to distinguish and even manually mislabeled. The proposed network consists of three modules, namely a multi-stage multi-branch classification network (MSB), a multi-branch label distribution learning module (MLD), and a multi-branch similarity preserving module (MSP). MSB aggregates similar expression features through the first-stage prediction, MLD utilizes the aggregation results of MSB to extract the label distribution between similar expressions, and MSP utilizes a consistency relationship to minimize the differences between multiple branches. We propose an end-to-end model, where each module can be integrated with existing network modules. Furthermore, our method achieves 89.44% accuracy on the RAF-DB dataset and achieves state-of-the-art results on the AffectNet dataset with 63.25% and 66.56% accuracy on its two subsets, AffectNet-8 and AffectNet-7, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002719",
    "keywords": [
      "Ambiguity",
      "Artificial intelligence",
      "Computer science",
      "Consistency (knowledge bases)",
      "Expression (computer science)",
      "Facial expression recognition",
      "Facial recognition system",
      "Image (mathematics)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Lang",
        "given_name": "Junjie"
      },
      {
        "surname": "Sun",
        "given_name": "Xiao"
      },
      {
        "surname": "Li",
        "given_name": "Jia"
      },
      {
        "surname": "Wang",
        "given_name": "Meng"
      }
    ]
  },
  {
    "title": "Temporal feature enhancement network with external memory for live-stream video object detection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108847",
    "abstract": "This paper proposes a method exploiting temporal context with an attention mechanism for detecting objects in real-time in a live streaming video. Video object detection is challenging and essential in practical applications such as robotics, smartphones, and surveillance cameras. Although methods have been proposed to improve the accuracy or run-time speed by exploiting temporal information, the trade-off between them tends to be ignored. We thus focus on the trade-off between accuracy and speed, and propose a method to improve the accuracy by aggregating the past information from a lightweight feature extractor with an attention mechanism. Evaluations on the UA-DETRAC and ImageNet VID datasets demonstrate our model’s superior performance to state-of-the-art methods on live streaming real-time object detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003284",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Engineering",
      "Extractor",
      "Feature (linguistics)",
      "Focus (optics)",
      "Linguistics",
      "Object (grammar)",
      "Object detection",
      "Optics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Process engineering",
      "Real-time computing"
    ],
    "authors": [
      {
        "surname": "Fujitake",
        "given_name": "Masato"
      },
      {
        "surname": "Sugimoto",
        "given_name": "Akihiro"
      }
    ]
  },
  {
    "title": "Dual-branch self-attention network for pedestrian attribute recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.10.003",
    "abstract": "Pedestrian attribute recognition (PAR) is still a challenging task in real surveillance scenes, where the difficulties, such as occlusion, complex background, and varying views, degrade the recognition accuracy. To fully exploit attribute correlation and regional context, we propose a dual-branch self-attention network for PAR: (1) For the attribute branch, the second-order self-attention module (SO-SAM) is first introduced to derive the second-order feature maps; they are then fused with the first-order information to learn unique features for each attribute using the constrained loss function. (2) For the context branch, multiple adaptive visual tokens and a group of multi-head context self-attention modules (C-SAM) are exploited to describe the image and explore the relationships between different regions. The experimental results on three main public benchmarks, RAP, PA100K, and PETA datasets, demonstrate the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002963",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer security",
      "Context (archaeology)",
      "Dual (grammatical number)",
      "Economics",
      "Engineering",
      "Evolutionary biology",
      "Exploit",
      "Feature (linguistics)",
      "Function (biology)",
      "Image (mathematics)",
      "Linguistics",
      "Literature",
      "Machine learning",
      "Management",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Pedestrian",
      "Pedestrian detection",
      "Philosophy",
      "Spatial contextual awareness",
      "Task (project management)",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Zhenyu"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhang"
      },
      {
        "surname": "Li",
        "given_name": "Da"
      },
      {
        "surname": "Zhang",
        "given_name": "Peng"
      },
      {
        "surname": "Shan",
        "given_name": "Caifeng"
      }
    ]
  },
  {
    "title": "Polarization-based optical characterization for color texture analysis and segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.09.019",
    "abstract": "Texture characterization is very useful for automatic analysis of object surface images for a plethora of applications in medicine, agriculture, industry or remote sensing. Various texture characterization techniques exist, from the classical Haralick descriptors, Gabor filters, local binary patterns to automatically-extracted features using machine learning models. We propose a new hand-crafted texture characterization technique, based on light polarization property, by deploying a circular polarization filter (rotated from 0° to 360° in steps of 10°) in the image acquisition process. The hypothesis is that different materials and surfaces will exhibit different polarization signatures defined as pixel values variation as a function of polarization angle. Such polarization signature is able to locally characterize texture as a consequence of light reflections captured in every pixel due to the texture intrinsic variations. We show the usefulness of our approach for surface/material classification for the purpose of color image segmentation of natural outdoor scenes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002896",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Feature extraction",
      "Gabor filter",
      "Histogram",
      "Image (mathematics)",
      "Image segmentation",
      "Image texture",
      "Local binary patterns",
      "Optical filter",
      "Optics",
      "Pattern recognition (psychology)",
      "Physical chemistry",
      "Physics",
      "Pixel",
      "Polarization (electrochemistry)",
      "Polarizing filter",
      "Segmentation",
      "Texture (cosmology)",
      "Texture filtering"
    ],
    "authors": [
      {
        "surname": "Oprisescu",
        "given_name": "Serban"
      },
      {
        "surname": "Coliban",
        "given_name": "Radu-Mihai"
      },
      {
        "surname": "Ivanovici",
        "given_name": "Mihai"
      }
    ]
  },
  {
    "title": "DSLA: Dynamic smooth label assignment for efficient anchor-free object detection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108868",
    "abstract": "Anchor-free detectors basically formulate object detection as dense classification and regression. For popular anchor-free detectors, it is common to introduce an individual prediction branch to estimate the quality of localization. The following inconsistencies are observed when we delve into the practices of classification and quality estimation. Firstly, for some adjacent samples which are assigned completely different labels, the trained model would produce similar classification scores. This violates the training objective and leads to performance degradation. Secondly, it is found that detected bounding boxes with higher confidences contrarily have smaller overlaps with the corresponding ground-truth. Accurately localized bounding boxes would be suppressed by less accurate ones in the Non-Maximum Suppression (NMS) procedure. To address the inconsistency problems, the Dynamic Smooth Label Assignment (DSLA) method is proposed. Based on the concept of centerness originally developed in FCOS, a smooth assignment strategy is proposed. The label is smoothed to a continuous value in [ 0 , 1 ] to make a steady transition between positive and negative samples. Intersection-of-Union (IoU) is predicted dynamically during training and is coupled with the smoothed label. The dynamic smooth label is assigned to supervise the classification branch. Under such supervision, quality estimation branch is naturally merged into the classification branch, which simplifies the architecture of anchor-free detector. Comprehensive experiments are conducted on the MS COCO benchmark. It is demonstrated that, DSLA can significantly boost the detection accuracy by alleviating the above inconsistencies for anchor-free detectors. Our codes are released at https://github.com/YonghaoHe/DSLA.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003491",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Su",
        "given_name": "Hu"
      },
      {
        "surname": "He",
        "given_name": "Yonghao"
      },
      {
        "surname": "Jiang",
        "given_name": "Rui"
      },
      {
        "surname": "Zhang",
        "given_name": "Jiabin"
      },
      {
        "surname": "Zou",
        "given_name": "Wei"
      },
      {
        "surname": "Fan",
        "given_name": "Bin"
      }
    ]
  },
  {
    "title": "On the role of question encoder sequence model in robust visual question answering",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108883",
    "abstract": "Generalizing beyond the experiences has a significant role in developing robust and practical machine learning systems. It has been shown that current Visual Question Answering (VQA) models are over-dependent on the language-priors (spurious correlations between question-types and their most frequent answers) from the train set and pose poor performance on Out-of-Distribution (OOD) test sets. This conduct negatively affects the robustness of VQA models and restricts them from being utilized in real-world situations. This paper shows that the sequence model architecture used in the question-encoder has a significant role in the OOD performance of VQA models. To demonstrate this, we performed a detailed analysis of various existing RNN-based and Transformer-based question-encoders, and along, we proposed a novel Graph attention network (GAT)-based question-encoder. Our study found that a better choice of sequence model in the question-encoder reduces the over-fit to language biases and improves OOD performance in VQA even without using any additional relatively complex bias-mitigation approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003648",
    "keywords": [
      "Artificial intelligence",
      "Bayesian probability",
      "Biochemistry",
      "Chemistry",
      "Chip",
      "Computer science",
      "Encoder",
      "Gene",
      "Machine learning",
      "Natural language processing",
      "Operating system",
      "Physics",
      "Prior probability",
      "Quantum mechanics",
      "Question answering",
      "Resolver",
      "Robustness (evolution)",
      "Spurious relationship",
      "Telecommunications",
      "Test set",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "KV",
        "given_name": "Gouthaman"
      },
      {
        "surname": "Mittal",
        "given_name": "Anurag"
      }
    ]
  },
  {
    "title": "Dimension-aware attention for efficient mobile networks",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108899",
    "abstract": "Recently, attention mechanisms have shown great potential in improving the performance of mobile networks. Typically, they involve 2D symmetric convolution operations or generate 2D attention maps. However, such manners usually introduce high computational cost and large memory consumption, increasing the computational burden of mobile networks. To address this problem, we propose a novel lightweight attention mechanism, called Dimension-Aware Attention (DAA) block, by modeling the intra-dependencies of each dimension of the input feature map. Specifically, we factorize the channel and spatial attention by three parallel feature vector encoding branches, where stacked 1D asymmetric convolution operations can be naturally leveraged to capture large receptive fields. In this way, channel-aware, horizontal-aware, and vertical-aware attention vectors are extracted to effectively encode multi-dimensional information and greatly reduce the computational complexity of mobile networks. Experiments on multiple vision tasks demonstrate that our DAA block achieves better accuracy against state-of-the-art attention mechanisms with much lower computational operations. Our code is available at https://github.com/rymo96/DAANet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003806",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Block (permutation group theory)",
      "Channel (broadcasting)",
      "Chemistry",
      "Code (set theory)",
      "Computational complexity theory",
      "Computer network",
      "Computer science",
      "Convolution (computer science)",
      "Dimension (graph theory)",
      "ENCODE",
      "Encoding (memory)",
      "Feature (linguistics)",
      "Gene",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Mobile device",
      "Operating system",
      "Philosophy",
      "Programming language",
      "Pure mathematics",
      "Set (abstract data type)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Mo",
        "given_name": "Rongyun"
      },
      {
        "surname": "Lai",
        "given_name": "Shenqi"
      },
      {
        "surname": "Yan",
        "given_name": "Yan"
      },
      {
        "surname": "Chai",
        "given_name": "Zhenhua"
      },
      {
        "surname": "Wei",
        "given_name": "Xiaolin"
      }
    ]
  },
  {
    "title": "Multi-granularity episodic contrastive learning for few-shot learning",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108820",
    "abstract": "Few-shot learning (FSL) aims at fast adaptation to novel classes with few training samples. Among FSL methods, meta-learning and transfer learning-based methods are the most powerful ones. However, most of them rely to some extent on cross-entropy loss, which leads to representations that are overly concerned with the classes already seen, and in turn leads to sub-optimal generalization on novel classes. In this study, we are inspired by meta-learning and transfer learning-based methods and believe good feature representations are vital for FSL. To this end, we propose a new multi-granularity episodic contrastive learning method (MGECL) that introduces contrastive learning into the episode training process. In particular, by enforcing our proposed contrastive loss on both class and instance granularities, the model is able to extract category-independent discriminative patterns and learn richer and more transferable feature representations. Extensive experiments demonstrate that our proposed method achieves state-of-the-art performance on three popular few-shot benchmarks. Our code is available at https://github.com/z1358/MGECL_PR.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003016",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Code (set theory)",
      "Computer science",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature learning",
      "Generalization",
      "Granularity",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Operating system",
      "Philosophy",
      "Programming language",
      "Set (abstract data type)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Pengfei"
      },
      {
        "surname": "Zhu",
        "given_name": "Zhilin"
      },
      {
        "surname": "Wang",
        "given_name": "Yu"
      },
      {
        "surname": "Zhang",
        "given_name": "Jinglin"
      },
      {
        "surname": "Zhao",
        "given_name": "Shuai"
      }
    ]
  },
  {
    "title": "Covered Style Mining via Generative Adversarial Networks for Face Anti-spoofing",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108957",
    "abstract": "Face anti-spoofing, a biometric authentication method, is a central part of automatic face recognition. Recently, two sets of approaches have performed particularly well against presentation attacks: 1) pixel-wise supervision-based methods, which intend to provide fine-grained pixel information to learn specific auxiliary maps; and 2) anomaly detection-based methods, which regard face anti-spoofing as an open-set training task and learn spoof detectors using only bona fide data, where the detectors are shown to generalize well to unknown attacks. However, these approaches depend on handcrafted prior information to control the generation of intermediate difference maps and easily fall into local optima. In this paper, we propose a novel frame-level face anti-spoofing method, Covered Style Mining-GAN (CSM-GAN), which converts face anti-spoofing detection into a style transfer process without any prior information. Specifically, CSM-GAN has four main components: the Covered Style Encoder (CSE), responsible for mining the difference map containing the photography style and discriminative clues; the Auxiliary Style Classifier (ASC), consisting of several stacked Difference Capture Blocks (DCB) responsible for distinguishing bona fide faces from spoofing faces; and the Style Transfer Generator (STG) and Style Adversarial Discriminator (SAD), which form generative adversarial networks to achieve style transfer. Comprehensive experiments on several benchmark datasets show that the proposed method not only outperforms current state-of-the-art but also produces better visual diversity in difference maps.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200437X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Detector",
      "Discriminator",
      "Encoder",
      "Face (sociological concept)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Social science",
      "Sociology",
      "Spoofing attack",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Yiqiang"
      },
      {
        "surname": "Tao",
        "given_name": "Dapeng"
      },
      {
        "surname": "Luo",
        "given_name": "Yong"
      },
      {
        "surname": "Cheng",
        "given_name": "Jun"
      },
      {
        "surname": "Li",
        "given_name": "Xuelong"
      }
    ]
  },
  {
    "title": "Multi-criteria Selection of Rehearsal Samples for Continual Learning",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108907",
    "abstract": "Retaining a small subset to replay is a direct and effective way to prevent catastrophic forgetting in continual learning. However, due to data complexity and restricted memory, picking a proper subset for rehearsal is challenging and still being explored. In this work, we present a Multi-criteria Subset Selection approach that can stabilize and advance replay-based continual learning. The method picks rehearsal samples by integrating multiple criteria, including distance to prototype, intra-class cluster variation, and classifier loss. By doing so, it maximizes the comprehensive representation power of the sampled subset by ensuring its representativeness, diversity, and discriminability. We empirically find that singular criteria are likely to fail in particular tasks, while multi-criteria minimizes this risk and stabilizes task training throughout the continual learning process. Moreover, our method improves replay-based methods consistently and achieves state-of-the-art performance on both CIFAR100 and Tiny-Imagenet datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003880",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Economics",
      "Forgetting",
      "Linguistics",
      "Machine learning",
      "Management",
      "Mathematics",
      "Philosophy",
      "Representativeness heuristic",
      "Selection (genetic algorithm)",
      "Statistics",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Zhuang",
        "given_name": "Chen"
      },
      {
        "surname": "Huang",
        "given_name": "Shaoli"
      },
      {
        "surname": "Cheng",
        "given_name": "Gong"
      },
      {
        "surname": "Ning",
        "given_name": "Jifeng"
      }
    ]
  },
  {
    "title": "Tensor ring with alternative change mask for multitemporal hyperspectral image change detection",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.10.023",
    "abstract": "Multitemporal hyperspectral image (HSI) change detection (CD) is a prevalent topic in remote sensing image processing. HSI CD usually consists of change feature extraction and classification. Although the high dimensionality of HSIs provides rich spectral information, HSIs are prone to spectral-spatial variability that degrades change detection accuracy. Recently, tensor decomposition has been successfully applied to CD. However, there is still room for improvement. We propose a tensor ring-based CD model with alternative change masks (TRACM-CD) for multitemporal HSIs. TRACM-CD extracts temporal change features using TR decomposition applied to different temporal change vectors. The alternative change masks constrain the temporal change representation and guarantee the temporal symmetry for change features to facilitate recognition of background and changes. Experimental results on four real-world multitemporal HSI datasets confirm the effectiveness and superiority of TR-based CD. The proposed model outperforms its tensor counterparts and classic approaches for multitemporal HSIs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003221",
    "keywords": [
      "Artificial intelligence",
      "Change detection",
      "Computer science",
      "Computer vision",
      "Curse of dimensionality",
      "Feature (linguistics)",
      "Feature extraction",
      "Hyperspectral imaging",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pure mathematics",
      "Tensor (intrinsic definition)"
    ],
    "authors": [
      {
        "surname": "Sohail",
        "given_name": "Muhammad"
      },
      {
        "surname": "Chen",
        "given_name": "Zhao"
      },
      {
        "surname": "Liu",
        "given_name": "Guohua"
      }
    ]
  },
  {
    "title": "Unsupervised knowledge transfer for nonblind image deconvolution",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.11.018",
    "abstract": "Nonblind image deconvolution restores the clear image from a blurred one under a known blur kernel, whose recent development has been boosted by supervised deep learning. Motivated by the inaccessibility of ground-truth images for supervised learning in many application domains, such as scientific imaging, this paper studies the unsupervised knowledge transfer problem for nonblind image deconvolution, which aims at adapting a deep model pre-trained on a source domain, to a ground-truth-scare target domain where image contents or blur kernels are distinct from that of the source domain. We propose to conduct the knowledge transfer regarding both images and kernels, by leveraging the model being adapted itself to generate pairs of a pseudo ground-truth image and a blurred image for self training. The proposed method neither accesses source-domain data, which avoids privacy issues, nor accesses target-domain ground-truths, which avoids ground-truth collection. Its effectiveness is demonstrated with the experiments on three deblurring tasks in different domains.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003452",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Deblurring",
      "Deconvolution",
      "Domain (mathematical analysis)",
      "Ground truth",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Kernel (algebra)",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Zhuojie"
      },
      {
        "surname": "Yao",
        "given_name": "Xin"
      },
      {
        "surname": "Xu",
        "given_name": "Yong"
      },
      {
        "surname": "Wang",
        "given_name": "Junle"
      },
      {
        "surname": "Quan",
        "given_name": "Yuhui"
      }
    ]
  },
  {
    "title": "Towards a category-extended object detector with limited data",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108943",
    "abstract": "Object detectors are typically learned on fully-annotated training data with fixed predefined categories. However, categories are often required to be increased progressively. Usually, only the original training set annotated with old classes and some new training data labeled with new classes are available in such scenarios. Based on the limited datasets, a unified detector that can handle all categories is strongly needed. We propose a practical scheme to achieve it in this work. A conflict-free loss is designed to avoid label ambiguity, leading to an acceptable detector in one training round. To further improve performance, we propose a retraining phase in which Monte Carlo Dropout is employed to calculate the localization confidence to mine more accurate bounding boxes, and an overlap-weighted method is proposed for making better use of pseudo annotations during retraining. Extensive experiments demonstrate the effectiveness of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200423X",
    "keywords": [
      "Ambiguity",
      "Artificial intelligence",
      "Bounding overwatch",
      "Business",
      "Computer science",
      "Data mining",
      "Data set",
      "Detector",
      "Dropout (neural networks)",
      "International trade",
      "Labeled data",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Retraining",
      "Scheme (mathematics)",
      "Set (abstract data type)",
      "Telecommunications",
      "Training set"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Bowen"
      },
      {
        "surname": "Chen",
        "given_name": "Chen"
      },
      {
        "surname": "Xiao",
        "given_name": "Xi"
      },
      {
        "surname": "Xia",
        "given_name": "Shutao"
      }
    ]
  },
  {
    "title": "Tree-based data augmentation and mutual learning for offline handwritten mathematical expression recognition",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108910",
    "abstract": "Recently, thanks to the successful application of the attention-based encoder-decoder framework, handwritten mathematical expression recognition (HMER) has achieved significant improvement. However, HMER is still a challenging task in the handwriting recognition area, which suffers from the ambiguity of handwritten symbols, the two-dimensional structure of mathematical expressions, and the lack of labeled data. In this paper, we attempt to improve the recognition performance and generalization ability of the existing state-of-the-art method from two perspectives: data augmentation and model design. We first propose a tree-based multi-level (including symbol level, sub-expression level, and image level) data augmentation strategy, which can generate many synthetic images. Then, we present a novel encoder-decoder hybrid model via tree-based mutual learning to fully utilize the complementarity between tree decoder and string decoder. Benefitting from our data augmentation strategy, we achieve 58.47%/57.82%/62.67% and 74.45% expression recognition accuracy respectively on the CROHME14/16/19 competition datasets and the OffRaSHME20 competition dataset. Moreover, tree-based data augmentation is a key technology to our champion system for the OffRaSHME20 competition. Our tree-based mutual learning method further improves the recognition accuracy to 61.63%/59.81%/64.38% and 75.68% on these datasets. Further quantitative and qualitative analyses also demonstrate the effectiveness and robustness of our proposed methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003910",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Encoder",
      "Gene",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Speech recognition",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Chen"
      },
      {
        "surname": "Du",
        "given_name": "Jun"
      },
      {
        "surname": "Zhang",
        "given_name": "Jianshu"
      },
      {
        "surname": "Wu",
        "given_name": "Changjie"
      },
      {
        "surname": "Chen",
        "given_name": "Mingjun"
      },
      {
        "surname": "Wu",
        "given_name": "JiaJia"
      }
    ]
  },
  {
    "title": "Privileged multi-task learning for attribute-aware aesthetic assessment",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108921",
    "abstract": "Aesthetic attributes are crucial for aesthetics because they explicitly present some photo quality cues that a human expert might use to evaluate a photo’s aesthetic quality. However, the aesthetic attributes have not been largely and sufficiently exploited for photo aesthetic assessment. In this paper, we propose a novel approach to photo aesthetic assessment with the help of aesthetic attributes. The aesthetic attributes are used as privileged information (PI), which is often available during training phase but unavailable in prediction phase due to the high collection expense. The proposed framework consists of a deep multi-task network as generator and a fully connected network as discriminator. Deep multi-task network learns the aesthetic attributes and score simultaneously to capture their dependencies and extract better feature representations. Specifically, we use ranking constraint in the label space, similarity constraint and prior probabilities loss in the privileged information space to make the output of multi-task network converge to that of ground truth. Adversarial loss is used to identify and distinguish the predicted privileged information of a deep multi-task network from the ground truth PI distribution. Experimental results on two benchmark databases demonstrate the superiority of the proposed method to state-of-the-art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004022",
    "keywords": [
      "Artificial intelligence",
      "Cognitive psychology",
      "Computer science",
      "Engineering",
      "Human–computer interaction",
      "Machine learning",
      "Multi-task learning",
      "Psychology",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Shu",
        "given_name": "Yangyang"
      },
      {
        "surname": "Li",
        "given_name": "Qian"
      },
      {
        "surname": "Liu",
        "given_name": "Lingqiao"
      },
      {
        "surname": "Xu",
        "given_name": "Guandong"
      }
    ]
  },
  {
    "title": "Dynamic self-attention with vision synchronization networks for video question answering",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108959",
    "abstract": "Video Question Answering (VideoQA) has gained increasing attention as an important task in understanding the rich spatio-temporal contents, i.e., the appearance and motion in the video. However, existing approaches mainly use the question to learn attentions over all the sampled appearance and motion features separately, which neglect two properties of VideoQA: (1) the answer to the question is often reflected on a few frames and video clips, and most video contents are superfluous; (2) appearance and motion features are usually concomitant and complementary to each other in time series. In this paper, we propose a novel VideoQA model, i.e., Dynamic Self-Attention with Vision Synchronization Networks (DSAVS), to address these problems. Specifically, a gated token selection mechanism is proposed to dynamically select the important tokens from appearance and motion sequences. These chosen tokens are fed into a self-attention mechanism to model the internal dependencies for more effective representation learning. To capture the correlation between the appearance and motion features, a vision synchronization block is proposed to synchronize the two types of vision features at the time slice level. Then, the visual objects can be correlated with their corresponding activities and the performance is further improved. Extensive experiments conducted on three public VideoQA data sets confirm the effectivity and superiority of our model compared with state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004393",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Economics",
      "Geometry",
      "Law",
      "Machine learning",
      "Management",
      "Mathematics",
      "Motion (physics)",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Security token",
      "Synchronization (alternating current)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yun"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaoming"
      },
      {
        "surname": "Huang",
        "given_name": "Feiran"
      },
      {
        "surname": "Shen",
        "given_name": "Shixun"
      },
      {
        "surname": "Tian",
        "given_name": "Peng"
      },
      {
        "surname": "Li",
        "given_name": "Lang"
      },
      {
        "surname": "Li",
        "given_name": "Zhoujun"
      }
    ]
  },
  {
    "title": "GSIP: Green Semantic Segmentation of Large-Scale Indoor Point Clouds",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.10.014",
    "abstract": "An efficient solution to semantic segmentation of large-scale indoor scene point clouds is proposed in this work. It is named GSIP (Green Segmentation of Indoor Point clouds) and its performance is evaluated on a representative large-scale benchmark — the Stanford 3D Indoor Segmentation (S3DIS) dataset. GSIP has two novel components: 1) a room-style data pre-processing method that selects a proper subset of points for further processing, and 2) a new feature extractor which is extended from PointHop. For the former, sampled points of each room form an input unit. For the latter, the weaknesses of PointHop’s feature extraction when extending it to large-scale point clouds are identified and fixed with a simpler processing pipeline. As compared with PointNet, which is a pioneering deep-learning-based solution, GSIP is green since it has significantly lower computational complexity and a much smaller model size. Furthermore, experiments show that GSIP outperforms PointNet in segmentation performance for the S3DIS dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003075",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Feature extraction",
      "Geodesy",
      "Geography",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pipeline (software)",
      "Point (geometry)",
      "Point cloud",
      "Programming language",
      "Scale (ratio)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Min"
      },
      {
        "surname": "Kadam",
        "given_name": "Pranav"
      },
      {
        "surname": "Liu",
        "given_name": "Shan"
      },
      {
        "surname": "Kuo",
        "given_name": "C.-C. Jay"
      }
    ]
  },
  {
    "title": "ETCNN: Extra Tree and Convolutional Neural Network-based Ensemble Model for COVID-19 Tweets Sentiment Classification",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.11.012",
    "abstract": "Pandemics influence people negatively and people experience fear and disappointment. With the global outspread of COVID-19, the sentiments of the general public are substantially influenced, and analyzing their sentiments could help to devise corresponding policies to alleviate negative sentiments. Often the data collected from social media platforms is unstructured leading to low classification accuracy. This study brings forward an ensemble model where the benefits of handcrafted features and automatic feature extraction are combined by machine learning and deep learning models. Unstructured data is obtained, preprocessed, and annotated using TextBlob and VADER before training machine learning models. Similarly, the efficiency of Word2Vec, TF, and TF-IDF features is also analyzed. Results reveal the better performance of the extra tree classifier when trained with TF-IDF features from TextBlob annotated data. Overall, machine learning models perform better with TF-IDF and TextBlob. The proposed model obtains superior performance using both annotation techniques with 0.97 and 0.95 scores of accuracy using TextBlob and VADER respectively with Word2Vec features. Results reveal that use of machine learning and deep learning models together with a voting criterion tends to yield better results than other machine learning models. Analysis of sentiments indicates that predominantly people possess negative sentiments regarding COVID-19.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003415",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Classifier (UML)",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Embedding",
      "Ensemble learning",
      "Machine learning",
      "Majority rule",
      "Sentiment analysis",
      "Social media",
      "Word2vec",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Umer",
        "given_name": "Muhammad"
      },
      {
        "surname": "Sadiq",
        "given_name": "Saima"
      },
      {
        "surname": "karamti",
        "given_name": "Hanen"
      },
      {
        "surname": "Abdulmajid Eshmawi",
        "given_name": "Ala’"
      },
      {
        "surname": "Nappi",
        "given_name": "Michele"
      },
      {
        "surname": "Usman Sana",
        "given_name": "Muhammad"
      },
      {
        "surname": "Ashraf",
        "given_name": "Imran"
      }
    ]
  },
  {
    "title": "Reparameterized attention for convolutional neural networks",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.10.022",
    "abstract": "The attention mechanism has been widely explored for neural networks as it could effectively model the interdependencies among channels, spatial positions, and frames. A neural network with attention modules has uncertainties in its parameters, but training the models deterministically hardly captures the uncertainties. Modeling the parameters’ uncertainty of the attention module could facilitate flexibly capturing the representative patterns, thus promoting the generalization of the models. In this work, we propose a novel reparameterized attention strategy by modeling the uncertainty of the parameters in the attention module and performing uncertainty-aware optimization. Instead of learning deterministic parameters for the attention modules, our strategy learns variational posterior distributions. The experimental results show that our strategy could consistently improve different models’ accuracy and reduce the generalization gap without extra computation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003154",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computation",
      "Computer science",
      "Convolutional neural network",
      "Generalization",
      "Interdependence",
      "Law",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Political science"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Yiming"
      },
      {
        "surname": "Li",
        "given_name": "Ruixiang"
      },
      {
        "surname": "Yu",
        "given_name": "Yunlong"
      },
      {
        "surname": "Li",
        "given_name": "Xi"
      }
    ]
  },
  {
    "title": "A fixed-point rotation-based feature selection method for micro-expression recognition",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.10.021",
    "abstract": "Micro-expressions (MEs) express spontaneous, subtle, and hard to hide real human emotions. Compared with macro expressions, the occurrence of MEs is characterized by a small number of activated muscles, short duration, and low amplitude of action. Therefore, extracting the sparse spatio-temporal features of MEs is a challenge for ME recognition. In this paper, we try to extract the low-dimensional features of MEs while ensuring a high accuracy. Firstly, considering that ME samples may be inconsistent in the time domain, a differential energy image method is improved to fix the temporal variation of MEs to unit time. An integral projection method is then used to improve the information density. Secondly, a fixed-point rotation-based feature selection method is proposed further select features with large motion variations. Specifically, the features are transformed from RGB to rotation axes in 3D space, and a fixed point is rotated separately to form a point set. The relative position of the points is changed by adjusting the rotation angle thus optimizing the distribution of the point set. The subset of points with large rotation angles is selected as the feature for classification. Finally, the effectiveness of the method is evaluated using SVM as a classifier experimented on three datasets. The experimental results show that the low-dimensional features can perform well for ME recognition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003099",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Expression (computer science)",
      "Feature (linguistics)",
      "Feature selection",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point (geometry)",
      "Programming language",
      "Rotation (mathematics)",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Mingzhong"
      },
      {
        "surname": "Wang",
        "given_name": "Qi"
      },
      {
        "surname": "Wang",
        "given_name": "Qingshan"
      },
      {
        "surname": "Zheng",
        "given_name": "Zhiwen"
      }
    ]
  },
  {
    "title": "A human-in-the-loop recommendation-based framework for reconstruction of mechanically shredded documents",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.10.011",
    "abstract": "The advances in machine learning – particularly in deep learning – have enabled automatizing the reconstruction of shredded documents with significant accuracy. However, despite the recent remarkable results, the state-of-the-art on fully automatic reconstruction still has room for improvement, mainly due to imprecision on the evaluation of how the shreds fit each other (compatibility/cost evaluation). To tackle this problem, we propose a human-in-the-loop reconstruction framework that takes user inputs to improve the solutions (permutation of shreds). In our approach, the user verifies whether adjacent shreds of a solution are also adjacent in the original document. Unlike the current literature, our framework includes a recommender module that automatically selects pairs of shreds to be analyzed by a human. Four recommendation strategies were proposed and evaluated. Results achieved by coupling deep learning reconstruction methods into our framework have shown that introducing the human in the loop can reduce errors by more than 40 % .",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552200304X",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Human-in-the-loop",
      "Machine learning",
      "Permutation (music)",
      "Physics",
      "Recommender system"
    ],
    "authors": [
      {
        "surname": "Paixão",
        "given_name": "Thiago M."
      },
      {
        "surname": "Berriel",
        "given_name": "Rodrigo F."
      },
      {
        "surname": "Boeres",
        "given_name": "Maria C.S."
      },
      {
        "surname": "Koerich",
        "given_name": "Alessandro L."
      },
      {
        "surname": "Badue",
        "given_name": "Claudine"
      },
      {
        "surname": "De Souza",
        "given_name": "Alberto F."
      },
      {
        "surname": "Oliveira-Santos",
        "given_name": "Thiago"
      }
    ]
  },
  {
    "title": "Group-based bi-directional recurrent wavelet neural network for efficient video super-resolution (VSR)",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.11.014",
    "abstract": "Video super-resolution (VSR) is an important technology for enhancing the quality of video frames. The recurrent neural network (RNN)-based approach is suitable for sequential data because it can use accumulated temporal information. However, since existing methods only tend to capture slow and symmetrical motion with low frame rate, there are still limitations to restore the missing details for more dynamic motion. Most of the previous methods using spatial information treat different types of the spatial features identically. It leads to lack of obtaining meaningful information and enhancing the fine details. We propose a group-based bi-directional recurrent wavelet neural network (GBR-WNN) to exploit spatio-temporal information effectively. The proposed group-based bi-directional RNN (GBR) framework is built on the well-structured process with the group of pictures (GOP). In a GOP, we resolves the low-resolution (LR) frames from border frames to center target frame. Because super-resolved features in a GOP are cumulative, neighboring features are improved progressively and asymmetrical motion can be dealt with. Also, we propose a temporal wavelet attention (TWA) adopting attention module for both spatial and temporal features simultaneously based on discrete wavelet transform. Experiments show that the proposed scheme achieves superior performance compared with state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003440",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Exploit",
      "Frame (networking)",
      "Motion (physics)",
      "Pattern recognition (psychology)",
      "Recurrent neural network",
      "Telecommunications",
      "Wavelet",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Choi",
        "given_name": "Young-Ju"
      },
      {
        "surname": "Lee",
        "given_name": "Young-Woon"
      },
      {
        "surname": "Kim",
        "given_name": "Byung-Gyu"
      }
    ]
  },
  {
    "title": "An emotion index estimation based on facial action unit prediction",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.11.019",
    "abstract": "Automated emotion recognition is essential in human-computer interactions. Instant emotion computing based on facial expressions plays a supporting role in understanding interactive intentions. Although continuous emotions can more accurately describe the overall emotional state, most existing research still focuses on discrete facial expression recognition. This paper proposes computing continuous emotions based on action unit (AU) recognition. A continuous emotion representation and computing approach are presented, which is called the emotion index. We first define two polarities as represented by positive and negative emotions. We map compound emotions into continuous space using the relationship between AUs and discrete emotions, which then creates the mapping between AUs and continuous emotion. Second, an end-to-end convolutional neural network (CNN) is used to recognize AU occurrence and intensity. We then build the facial AU matrix. Finally, a function is built to transfer the facial AU occurrence and intensities into the emotion index through the emotion relative matrix. The experiments show that our AU recognition approach can attain a better performance than state-of-the-art approaches. Moreover, our approach has an outstanding average F1 score of 66.9%. The emotion index results are in line with expectations, indicating they can effectively express emotion variations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003488",
    "keywords": [
      "Action (physics)",
      "Affective computing",
      "Artificial intelligence",
      "Composite material",
      "Computer science",
      "Convolutional neural network",
      "Emotion classification",
      "Emotion recognition",
      "Face (sociological concept)",
      "Facial expression",
      "Index (typography)",
      "Law",
      "Materials science",
      "Matrix (chemical analysis)",
      "Negative emotion",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Psychology",
      "Quantum mechanics",
      "Representation (politics)",
      "Social psychology",
      "Social science",
      "Sociology",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Tan",
        "given_name": "Xiaohui"
      },
      {
        "surname": "Fan",
        "given_name": "Yachun"
      },
      {
        "surname": "Sun",
        "given_name": "Mingrui"
      },
      {
        "surname": "Zhuang",
        "given_name": "Meiqi"
      },
      {
        "surname": "Qu",
        "given_name": "Fangbing"
      }
    ]
  },
  {
    "title": "Exploiting shape cues for weakly supervised semantic segmentation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108953",
    "abstract": "Weakly supervised semantic segmentation (WSSS) aims to produce pixel-wise class predictions with only image-level labels for training. To this end, previous methods adopt the common pipeline: they generate pseudo masks from class activation maps (CAMs) and use such masks to supervise segmentation networks. However, it is challenging to derive comprehensive pseudo masks that cover the whole extent of objects due to the local property of CAMs, i.e., they tend to focus solely on small discriminative object parts. In this paper, we associate the locality of CAMs with the texture-biased property of convolutional neural networks (CNNs). Accordingly, we propose to exploit shape information to supplement the texture-biased CNN features, thereby encouraging mask predictions to be not only comprehensive but also well-aligned with object boundaries. We further refine the predictions in an online fashion with a novel refinement method that takes into account both the class and the color affinities, in order to generate reliable pseudo masks to supervise the model. Importantly, our model is end-to-end trained within a single-stage framework and therefore efficient in terms of the training cost. Through extensive experiments on PASCAL VOC 2012, we validate the effectiveness of our method in producing precise and shape-aligned segmentation results. Specifically, our model surpasses the existing state-of-the-art single-stage approaches by large margins. What is more, it also achieves a new state-of-the-art performance over multi-stage approaches, when adopted in a simple two-stage pipeline without bells and whistles.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004332",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Convolutional neural network",
      "Discriminative model",
      "Epistemology",
      "Exploit",
      "Focus (optics)",
      "Linguistics",
      "Locality",
      "Optics",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pipeline (software)",
      "Programming language",
      "Property (philosophy)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Kho",
        "given_name": "Sungpil"
      },
      {
        "surname": "Lee",
        "given_name": "Pilhyeon"
      },
      {
        "surname": "Lee",
        "given_name": "Wonyoung"
      },
      {
        "surname": "Ki",
        "given_name": "Minsong"
      },
      {
        "surname": "Byun",
        "given_name": "Hyeran"
      }
    ]
  },
  {
    "title": "Automatically classifying non-functional requirements using deep neural network",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108948",
    "abstract": "Non-functional requirements are property that software products must have in order to meet the user’s business requirements, and are additional constraints on the quality and characteristics of software systems. They are generally written by software designers and documented in various parts of requirements documentation. When developing systems, developers need to classify non-functional requirements from requirements documents, and classifying these non-functional requirements requires professional skills, experience, and domain knowledge, which is challenging and time-consuming for developers. It would be beneficial to implement automatic classification of non-functional requirements from requirements documents, which could reduce the manual, time, and mental fatigue involved in identifying specific non-functional requirements from a large number of requirements. In this paper, a deep neural network model called NFRNet is designed to automatically classify non-functional requirements from software requirement documents. The network consists of two parts. One is an improved BERT word embedding model based on N-gram masking for learning context representation of the requirement descriptions, and the other is a Bi-LSTM classification network for capture context information of the requirement descriptions. We use a Softmax classifier in the end to classify the requirement descriptions. At the same time, in order to accelerate the training and improve the generalization ability of the model, the network uses multi-sample dropout regularization technology. This new regularization technology can reduce the number of iterations needed for training, accelerate the training of deep neural networks, and the networks trained achieved lower error rates. In addition, we expanded the original non-functional requirements dataset (PROMISE dataset) and designed a new dataset called SOFTWARE NFR. The new dataset far exceeds the original dataset in terms of the number of requirement description sentences and the number of non-functional requirements categories. It can be taken as a new testbed for non-functional requirements classification. Through cross-validation on the new dataset, the experimental results show that the network designed in this paper is significantly better than the other 17 classification methods in terms of Precision, Recall, and F1-score. At the same time, for the training set and the validation set, using the multi-sample dropout regularization technology can accelerate the training speed, reduce the number of iterations, and achieve lower error rates and loss.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004289",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Classifier (UML)",
      "Computer science",
      "Functional requirement",
      "Functional specification",
      "Machine learning",
      "Non-functional requirement",
      "Non-functional testing",
      "Programming language",
      "Requirements analysis",
      "Requirements management",
      "Software",
      "Software construction",
      "Software design",
      "Software development",
      "Software engineering",
      "Software requirements",
      "Software requirements specification"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Bing"
      },
      {
        "surname": "Nong",
        "given_name": "Xiuwen"
      }
    ]
  },
  {
    "title": "Image outpainting guided by prior structure information",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.10.030",
    "abstract": "Deep learning-based image outpainting infers the missing region by using the known parts of images. However, due to lack of fully exploring the known image structure or texture information, most existing methods always produce blurry contents and distorted structures. To generate more natural outpainting results, we propose a two-stage outpainting method guided by prior structure information. It consists of both structure outpainting and texture outpainting, which allows the model to complete structure outpainting and refines the generated image. In Stage-I, we build the structure outpainting network to infer the structural information of the missing regions by utilizing that of the known structure. This could fully explore the global structure information and produce complete structure images. Stage-II builds upon Stage-I results and utilizes both inferred image-level and aggregated multi-scale feature-level structure information to refine results with more authentic and natural texture. Moreover, a multi-level dilated convolution block is presented to significantly enlarge the receptive field of texture outpainting, promoting it to extract more useful feature information for producing finer texture. Compared with the existing state-of-the-art (SOTA) methods, the experimental results on both Places2 and Paris StreetView datasets illustrate that our method exhibits better in terms of qualitative and quantitative comparisons.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003294",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Block (permutation group theory)",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Feature (linguistics)",
      "Geometry",
      "Image (mathematics)",
      "Image processing",
      "Image texture",
      "Information structure",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Texture (cosmology)"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Canghong"
      },
      {
        "surname": "Ren",
        "given_name": "Yongpeng"
      },
      {
        "surname": "Li",
        "given_name": "Xiaojie"
      },
      {
        "surname": "Mumtaz",
        "given_name": "Imran"
      },
      {
        "surname": "Jin",
        "given_name": "Zhiheng"
      },
      {
        "surname": "Ren",
        "given_name": "Hongping"
      }
    ]
  },
  {
    "title": "MAN: Mining Ambiguity and Noise for Facial Expression Recognition in the Wild",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.10.016",
    "abstract": "Due to the ambiguous expressions and the subjectiveness of annotators, annotation ambiguity is a serious obstacle for facial expression recognition (FER). Ambiguous annotation exists in similar and dissimilar classes, which we call ambiguity and noise. The previous state-of-the-art approaches use uncertainty to generalize the two categories, and adopt uncertainty learning to suppress uncertainty samples. However, ambiguous expressions are confused with noisy label expressions may bias the model toward easy samples and hurt the generalization capability. To solve this problem, we propose a novel approach to mine ambiguity and noise (MAN) in FER datasets. Specifically, we design a co-division module, which divides the datasets into clean, ambiguous and noisy label expressions based on the consistency and inconsistency between the predictions of two networks and the given labels. To effectively learn the clean expressions, improve discriminative ability and avoid memorizing noisy labels, the tri-regularization module employs supervised learning, mutuality learning and unsupervised learning for the three subsets, respectively. Extensive experiments have shown that MAN can effectively mine the real ambiguity and noise, and achieve state-of-the-art performance in both synthetic noisy datasets and popular benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003105",
    "keywords": [
      "Ambiguity",
      "Annotation",
      "Artificial intelligence",
      "Computer science",
      "Consistency (knowledge bases)",
      "Discriminative model",
      "Expression (computer science)",
      "Facial expression",
      "Generalization",
      "Image (mathematics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Ziyang"
      },
      {
        "surname": "Sun",
        "given_name": "Xiao"
      },
      {
        "surname": "Li",
        "given_name": "Jia"
      },
      {
        "surname": "Wang",
        "given_name": "Meng"
      }
    ]
  },
  {
    "title": "Feature Nonlinear Transformation Non-Negative Matrix Factorization with Kullback-Leibler Divergence",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108906",
    "abstract": "This paper introduces a Feature Nonlinear Transformation Non-Negative Matrix Factorization with Kullback-Leibler Divergence (FNTNMF-KLD) for extracting the nonlinear features of a matrix in standard NMF. This method uses a nonlinear transformation to act on the feature matrix for constructing a NMF model based on the objective function of Kullback-Leibler Divergence, and the Taylor series expansion and the Newton iteration formula of solving root are used to obtain the iterative update rules of the basis matrix and the feature matrix. Experimental results show that the proposed method obtains the nonlinear features of data matrix in a more efficient way. In object recognition and clustering tasks, better accuracy can be achieved over some typical NMF methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003879",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Cluster analysis",
      "Composite material",
      "Computer science",
      "Divergence (linguistics)",
      "Eigenvalues and eigenvectors",
      "Feature (linguistics)",
      "Gene",
      "Kullback–Leibler divergence",
      "Linguistics",
      "Materials science",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Matrix decomposition",
      "Non-negative matrix factorization",
      "Nonlinear system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Lirui"
      },
      {
        "surname": "Wu",
        "given_name": "Ning"
      },
      {
        "surname": "Li",
        "given_name": "Xiao"
      }
    ]
  },
  {
    "title": "The familiarity hypothesis: Explaining the behavior of deep open set methods",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108931",
    "abstract": "In many object recognition applications, the set of possible categories is an open set, and the deployed recognition system will encounter novel objects belonging to categories unseen during training. Detecting such “novel category” objects is usually formulated as an anomaly detection problem. Anomaly detection algorithms for feature-vector data identify anomalies as outliers, but outlier detection has not worked well in deep learning. Instead, methods based on the computed logits of visual object classifiers give state-of-the-art performance. This paper proposes the Familiarity Hypothesis that these methods succeed because they are detecting the absence of familiar learned features rather than the presence of novelty. This distinction is important, because familiarity-based detection will fail in many situations where novelty is present. For example when an image contains both a novel object and a familiar one, the familiarity score will be high, so the novel object will not be noticed. The paper reviews evidence from the literature and presents additional evidence from our own experiments that provide strong support for this hypothesis. The paper concludes with a discussion of whether familiarity-based detection is an inevitable consequence of representation learning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004125",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Cognitive neuroscience of visual object recognition",
      "Computer science",
      "Discrete mathematics",
      "Feature (linguistics)",
      "Law",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Novelty",
      "Novelty detection",
      "Object (grammar)",
      "Object detection",
      "Open set",
      "Outlier",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Programming language",
      "Psychology",
      "Representation (politics)",
      "Set (abstract data type)",
      "Social psychology"
    ],
    "authors": [
      {
        "surname": "Dietterich",
        "given_name": "Thomas G."
      },
      {
        "surname": "Guyer",
        "given_name": "Alex"
      }
    ]
  },
  {
    "title": "NAS For efficient mobile eyebrow biometrics",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.11.009",
    "abstract": "Occlusions, such as those due to wearing surgical masks, pose a significant challenge to the face recognition systems. Among possible remedies, ocular biometric has proven to be a popular choice. However, the upper ocular regions, especially the patterns presented by the eyebrows, have yet to gain the attention they deserve. In this work, we leverage Neural Architecture Search (NAS) to discover better-performing architectures for eyebrow recognition. To reduce the computational complexity, we apply a zero-shot NAS to assess the exploratory architectures’ performance prior to any training. We were able to discover three new architectures that achieved competitive accuracies in eyebrow recognition. In doing so, we explored depthwise separable convolution, hard-swish, and Arcface loss functions to further enhance the discovered models in terms of accuracy and number of parameters. Our best result provided 0.999 AUC, 0.6% EER, and 98.25% GMR at 10 − 3 FMR over FACES dataset, which is better than the results of state-of-the-art architecture, a 29-layer lightCNN which has 21 × more parameters and 8 × more FLOPS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003385",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biometrics",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Eyebrow",
      "Face (sociological concept)",
      "Facial recognition system",
      "Leverage (statistics)",
      "Machine learning",
      "Medicine",
      "Pattern recognition (psychology)",
      "Social science",
      "Sociology",
      "Surgery"
    ],
    "authors": [
      {
        "surname": "Nguyen",
        "given_name": "Hoang Mark"
      },
      {
        "surname": "Derakhshani",
        "given_name": "Reza"
      }
    ]
  },
  {
    "title": "Unsupervised domain adaptation in homogeneous distance space for person re-identification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108941",
    "abstract": "Data distribution alignment and clustering-based self-training are two feasible solutions to tackle unsupervised domain adaptation (UDA) on person re-identification (re-ID). Most existing alignment-based methods solely learn the source domain decision boundaries and align the data distribution of the target domain to the source domain, thus the re-ID performance on the target domain completely depends on the shared decision boundaries and how well the alignment is performed. However, two domains can hardly be precisely aligned because of the label space discrepancy of two domains, resulting in poor target domain re-ID performance. Although clustering-based self-training approaches could learn independent decision boundaries on the pseudo-labelled target domain data, they ignore both the accurate ID-related information of the labelled source domain data and the underlying relations between two domains. To fully exploit the source domain data to learn discriminative target domain ID-related features, in this paper, we propose a novel cross-domain alignment method in the homogeneous distance space, which is constructed by the newly designed stair-stepping alignment (SSA) matcher. Such alignment method can be integrated into both alignment-based framework and clustering-based framework. Extensive experiments validate the effectiveness of our proposed alignment method in these two frameworks. We achieve superior performance when the proposed alignment module is integrated into the clustering-based framework. Codes will be available at: http://github.com/Dingyuan-Zheng/HDS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004216",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Cluster analysis",
      "Computer science",
      "Computer security",
      "Data mining",
      "Discriminative model",
      "Domain (mathematical analysis)",
      "Exploit",
      "Identification (biology)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Space (punctuation)"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Dingyuan"
      },
      {
        "surname": "Xiao",
        "given_name": "Jimin"
      },
      {
        "surname": "Wei",
        "given_name": "Yunchao"
      },
      {
        "surname": "Wang",
        "given_name": "Qiufeng"
      },
      {
        "surname": "Huang",
        "given_name": "Kaizhu"
      },
      {
        "surname": "Zhao",
        "given_name": "Yao"
      }
    ]
  },
  {
    "title": "Memory‐augmented neural networks based dynamic complex image segmentation in digital twins for self‐driving vehicle",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108956",
    "abstract": "With the continuous increase of the amount of information, people urgently need to identify the information in the image in more detail in order to obtain richer information from the image. This work explores the dynamic complex image segmentation of self-driving vehicle under Digital Twins (DTs) based on Memory-augmented Neural Networks (MANNs), so as to further improve the performance of self-driving in intelligent transportation. In view of the complexity of the environment and the dynamic changes of the scene in intelligent transportation, this work constructs a segmentation model for dynamic complex image of self-driving vehicle under DTs based on MANNs by optimizing the Deep Learning algorithm and further combining with the DTs technology, so as to recognize the information in the environment image during the self-driving. Finally, the performance of the constructed model is analyzed by experimenting with different image datasets (PASCALVOC 2012, NYUDv2, PASCAL CONTEXT, and real self-driving complex traffic image data). The results show that compared with other classical algorithms, the established MANN-based model has an accuracy of about 85.80%, the training time is shortened to 107.00 s, the test time is 0.70 s, and the speedup ratio is high. In addition, the average algorithm parameter of the given energy function α=0.06 reaches the maximum value. Therefore, it is found that the proposed model shows high accuracy and short training time, which can provide experimental reference for future image visual computing and intelligent information processing.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004368",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Civil engineering",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Digital image",
      "Engineering",
      "Image (mathematics)",
      "Image processing",
      "Image segmentation",
      "Intelligent transportation system",
      "Operating system",
      "Paleontology",
      "Pascal (unit)",
      "Programming language",
      "Segmentation",
      "Speedup"
    ],
    "authors": [
      {
        "surname": "Lv",
        "given_name": "Zhihan"
      },
      {
        "surname": "Qiao",
        "given_name": "Liang"
      },
      {
        "surname": "Yang",
        "given_name": "Shuo"
      },
      {
        "surname": "Li",
        "given_name": "Jinhua"
      },
      {
        "surname": "Lv",
        "given_name": "Haibin"
      },
      {
        "surname": "Piccialli",
        "given_name": "Francesco"
      }
    ]
  },
  {
    "title": "Wasserstein distributional harvesting for highly dense 3D point clouds",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108978",
    "abstract": "In this paper, we present a novel 3D point cloud harvesting method, which can harvest 3D points from an estimated surface distribution in an unsupervised manner (i.e., an input is a prior distribution). Our method outputs the surface distribution of a 3D object and samples 3D points from the distribution based on the proposed progressive random sampling strategy. The progressive sampling regards a prior distribution itself as a network input and uses a progressively increasing number of latent variables for training, which can diversify the coordinates of 3D points with fast convergence. Subsequently, our stochastic instance normalization transforms the implicit distribution into other distributions, which enables diverse shapes of 3D objects. Experimental results show that our method is competitive with other state-of-the-art methods. Our method can harvest an arbitrary number of 3D points, wherein the 3D object is represented in detail with highly dense 3D points or a part of it is described with partial sampling.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004587",
    "keywords": [
      "Algorithm",
      "Anthropology",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convergence (economics)",
      "Distribution (mathematics)",
      "Economic growth",
      "Economics",
      "Filter (signal processing)",
      "Geometry",
      "Mathematical analysis",
      "Mathematics",
      "Normalization (sociology)",
      "Object (grammar)",
      "Point (geometry)",
      "Point cloud",
      "Sampling (signal processing)",
      "Sociology",
      "Surface (topology)"
    ],
    "authors": [
      {
        "surname": "Shu",
        "given_name": "Dong Wook"
      },
      {
        "surname": "Park",
        "given_name": "Sung Woo"
      },
      {
        "surname": "Kwon",
        "given_name": "Junseok"
      }
    ]
  },
  {
    "title": "Siamese networks with an online reweighted example for imbalanced data learning",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108947",
    "abstract": "One key challenging problem in data mining and decision-making is to establish a decision support system based on unbalanced datasets. In this study, we propose a novel algorithm to handle unbalanced learning problems that integrates the advantages of Siamese convolutional neural networks (SCNN) and the online reweighted example (ORE) algorithm into a unified method. First, the SCNN model is established for learning and extracting deep feature features at different levels. Second, the ORE algorithm is used to address the problem of data with a class-imbalanced distribution. Compared with baseline approaches, the experimental results show that our proposed method substantially enhances the performance of both within-project defect prediction and cross-project defect prediction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004277",
    "keywords": [
      "Artificial intelligence",
      "Baseline (sea)",
      "Computer science",
      "Computer security",
      "Convolutional neural network",
      "Data mining",
      "Deep learning",
      "Feature (linguistics)",
      "Geology",
      "Key (lock)",
      "Labeled data",
      "Linguistics",
      "Machine learning",
      "Oceanography",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Linchang"
      },
      {
        "surname": "Shang",
        "given_name": "Zhaowei"
      },
      {
        "surname": "Tan",
        "given_name": "Jin"
      },
      {
        "surname": "Zhou",
        "given_name": "Mingliang"
      },
      {
        "surname": "Zhang",
        "given_name": "Mu"
      },
      {
        "surname": "Gu",
        "given_name": "Dagang"
      },
      {
        "surname": "Zhang",
        "given_name": "Taiping"
      },
      {
        "surname": "Tang",
        "given_name": "Yuan Yan"
      }
    ]
  },
  {
    "title": "Structure-based graph convolutional networks with frequency filter",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.11.005",
    "abstract": "Message-passing neural networks (MPNNs) have attracted a lot interesting in academia and industry and have been applied to various graph analytical problems of real-world applications and achieved prominent successes. For the most of current works on MPNNs, they mainly focus on two categories: spectral-based and spatial-based methods. The former seeks to distill useful information (e.g. low-pass and high-pass signals) and the later designs structural schemes. However, it is not enough to only utilize one of them. We tackle this drawback by proposing an efficient and elegant method of taking advantage of the structural graph neural network (GNN) for advanced spectral-based information filtering. Through the learnable frequency components and global virtual neighbor nodes, the proposed scheme is elaborated to obtain informative and structured messages of neighbor nodes and latent space correlational nodes. Experimental results show that our method outperforms the state-of-the-art performance in a series of datasets of graph tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003348",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Attention network",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Filter (signal processing)",
      "Focus (optics)",
      "Graph",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "FeiFei"
      },
      {
        "surname": "Ping",
        "given_name": "Mingzhu"
      },
      {
        "surname": "Mei",
        "given_name": "KuiZhi"
      }
    ]
  },
  {
    "title": "SWIPENET: Object detection in noisy underwater scenes",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108926",
    "abstract": "Deep learning based object detection methods have achieved promising performance in controlled environments. However, these methods lack sufficient capabilities to handle underwater object detection due to these challenges: (1) images in the underwater datasets and real applications are blurry whilst accompanying severe noise that confuses the detectors and (2) objects in real applications are usually small. In this paper, we propose a Sample-WeIghted hyPEr Network (SWIPENET), and a novel training paradigm named Curriculum Multi-Class Adaboost (CMA), to address these two problems at the same time. Firstly, the backbone of SWIPENET produces multiple high resolution and semantic-rich Hyper Feature Maps, which significantly improve small object detection. Secondly, inspired by the human education process that drives the learning from easy to hard concepts, we propose the noise-robust CMA training paradigm that learns the clean data first and then move on to learns the diverse noisy data. Experiments on four underwater object detection datasets show that the proposed SWIPENET+CMA framework achieves better or competitive accuracy in object detection against several state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004071",
    "keywords": [
      "AdaBoost",
      "Artificial intelligence",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Face detection",
      "Facial recognition system",
      "Feature (linguistics)",
      "Geology",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Noise (video)",
      "Object (grammar)",
      "Object detection",
      "Oceanography",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Underwater",
      "Viola–Jones object detection framework"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Long"
      },
      {
        "surname": "Zhou",
        "given_name": "Feixiang"
      },
      {
        "surname": "Wang",
        "given_name": "Shengke"
      },
      {
        "surname": "Dong",
        "given_name": "Junyu"
      },
      {
        "surname": "Li",
        "given_name": "Ning"
      },
      {
        "surname": "Ma",
        "given_name": "Haiping"
      },
      {
        "surname": "Wang",
        "given_name": "Xin"
      },
      {
        "surname": "Zhou",
        "given_name": "Huiyu"
      }
    ]
  },
  {
    "title": "A novel method for fusing graph convolutional network and feature based on feedback connection mechanism for nondestructive testing",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.11.013",
    "abstract": "Data-driven intelligent methods to fusing feature with a priori information have made great progress in the recent times. And there are two limitations to fusion methods. Firstly, the structure of the fusion method cannot be precisely determined. Secondly, the contribution of the final result of the fusion method cannot be confirmed. In this paper, a novel feedback connection mechanism inspired by the traditional feedback control system is proposed to solve the above-mentioned limitations. The overall structure consists of three parts. The first part is a abstract feature extractor, which consists of a multilayer graph neural networks (GCNs). The second part uses memory augmentation of long short-term memory (LSTM) to achieve initial fusion of feature and re-extraction of abstract features. In order to combine the two abstract features more effectively, the third part of the mechanism of feedback recursive update of feature fusion weights is proposed. Our proposed method is first tested for damage detection of photovoltaic (PV) modules. Another experiment is the inverse problem of self-magnetic flux leakage (SMFL) detection with stress imbalance detection on a pipeline experimental platform. The result show that the proposed method can improve the detection efficiency and analyze the contribution of feature to the detection results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003439",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Feature (linguistics)",
      "Feature extraction",
      "Fusion",
      "Fusion mechanism",
      "Graph",
      "Linguistics",
      "Lipid bilayer fusion",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Shaoxuan"
      },
      {
        "surname": "Feng",
        "given_name": "Jian"
      },
      {
        "surname": "Lu",
        "given_name": "Senxiang"
      }
    ]
  },
  {
    "title": "Learning intra-domain style-invariant representation for unsupervised domain adaptation of semantic segmentation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108911",
    "abstract": "In this paper, we aim to tackle the problem of unsupervised domain adaptation (UDA) of semantic segmentation and improve the UDA performance with a novel conception of learning intra-domain style-invariant representation. Previous UDA methods focused on reducing the inter-domain inconsistency between the source domain and the target domain. However, due to the different data distributions of the two domains, reducing the inter-domain inconsistency cannot ensure the generalization ability of the trained model in the target domain. Therefore, to improve the UDA performance, we take into consideration the intra-domain diversity of the target domain for the first time in studies on UDA and aim to train the model to generalize well to the diverse intra-domain styles. To achieve this, we propose a self-ensembling method to learn the intra-domain style-invariant representation and we introduce a semantic-aware multimodal image-to-image translation model to obtain images with diversified intra-domain styles. Our method achieves state-of-the-art performance on two synthetic-to-real adaptation benchmarks, and we demonstrate the effectiveness of our method by conducting extensive experiments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003922",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Generalization",
      "Image (mathematics)",
      "Invariant (physics)",
      "Law",
      "Machine learning",
      "Mathematical analysis",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Zongyao"
      },
      {
        "surname": "Togo",
        "given_name": "Ren"
      },
      {
        "surname": "Ogawa",
        "given_name": "Takahiro"
      },
      {
        "surname": "Haseyama",
        "given_name": "Miki"
      }
    ]
  },
  {
    "title": "In the eye of the beholder: A survey of gaze tracking techniques",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108944",
    "abstract": "Gaze tracking estimates and tracks the user’s gaze by analyzing facial or eye features, it is an important way to realize automated vision-based interaction. This paper introduces the visual information used in gaze tracking, and discusses the commonly used gaze estimation methods and their research dynamics, including: 2D mapping-based methods, 3D model-based methods, and appearance-based methods. In this way, some key issues that need to be solved in these methods are considered, and their research trends are discussed. Their characteristics in system configuration, personal calibration, head motion, gaze accuracy and robustness are also compared. Finally, the applications of gaze tracking techniques are analyzed from various application factors and fields. This paper reviews the latest development of gaze tracking, focuses more on various gaze tracking algorithms and their existing challenges. The development trends of gaze tracking are prospected, which provides ideas for future theoretical research and practical applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004241",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Eye tracking",
      "Gaze",
      "Gene",
      "Human–computer interaction",
      "Kalman filter",
      "Pedagogy",
      "Psychology",
      "Robustness (evolution)",
      "Tracking (education)",
      "Tracking system"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Jiahui"
      },
      {
        "surname": "Chi",
        "given_name": "Jiannan"
      },
      {
        "surname": "Yang",
        "given_name": "Huijie"
      },
      {
        "surname": "Yin",
        "given_name": "Xucheng"
      }
    ]
  },
  {
    "title": "Kernel dependence regularizers and Gaussian processes with applications to algorithmic fairness",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108922",
    "abstract": "Current adoption of machine learning in industrial, societal and economical activities has raised concerns about the fairness, equity and ethics of automated decisions. Predictive models are often developed using biased datasets and thus retain or even exacerbate biases in their decisions and recommendations. Removing the sensitive covariates, such as gender or race, is insufficient to remedy this issue since the biases may be retained due to other related covariates. We present a regularization approach to this problem that trades off predictive accuracy of the learned models (with respect to biased labels) for the fairness in terms of statistical parity, i.e. independence of the decisions from the sensitive covariates. In particular, we consider a general framework of regularized empirical risk minimization over reproducing kernel Hilbert spaces and impose an additional regularizer of dependence between predictors and sensitive covariates using kernel-based measures of dependence, namely the Hilbert-Schmidt Independence Criterion (HSIC) and its normalized version. This approach leads to a closed-form solution in the case of squared loss, i.e. ridge regression. We also provide statistical consistency results for both risk and fairness bound for our approach. Moreover, we show that the dependence regularizer has an interpretation as modifying the corresponding Gaussian process (GP) prior. As a consequence, a GP model with a prior that encourages fairness to sensitive variables can be derived, allowing principled hyperparameter selection and studying of the relative relevance of covariates under fairness constraints. Experimental results in synthetic examples and in real problems of income and crime prediction illustrate the potential of the approach to improve fairness of automated decisions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004034",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Covariate",
      "Econometrics",
      "Hyperparameter",
      "Independence (probability theory)",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Regularization (linguistics)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Zhu"
      },
      {
        "surname": "Pérez-Suay",
        "given_name": "Adrián"
      },
      {
        "surname": "Camps-Valls",
        "given_name": "Gustau"
      },
      {
        "surname": "Sejdinovic",
        "given_name": "Dino"
      }
    ]
  },
  {
    "title": "Uncorrelated feature selection via sparse latent representation and extended OLSDA",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108966",
    "abstract": "Modern unsupervised feature selection methods predominantly obtain the cluster structure and pseudo-labels information through spectral clustering. However, the pseudo-labels obtained by spectral clustering are usually mixed between positive and negative. Moreover, the Laplacian matrix in spectral clustering typically affects feature selection. Additionally, spectral clustering does not consider the interconnection information between data. To address these problems, this paper proposes uncorrelated feature selection via sparse latent representation and extended orthogonal least square discriminant analysis (OLSDA), which we term SLREO). Firstly, SLREO retains the interconnection between data by latent representation learning, and preserves the internal information between the data. In order to remove redundant interconnection information, an l 2,1-norm constraint is applied to the residual matrix of potential representation learning. Secondly, SLREO obtains non-negative pseudo-labels through orthogonal least square discriminant analysis (OLSDA) of embedded non-negative manifold structure. It not only avoids the appearance of negative pseudo-labels, but also eliminates the effect of the Laplacian matrix on feature selection. The manifold information of the data is also preserved. Furthermore, the matrix of the learned latent representation and OLSDA is used as pseudo-labels information. It not only ensures that the generated pseudo-labels are non-negative, but also makes the pseudo-labels closer to the true class labels. Finally, in order to avoid trivial solutions, an uncorrelated constraint and l 2,1-norm constraint are imposed on the feature transformation matrix. These constraints ensure row sparsity of the feature transformation matrix, select low-redundant and discriminative features, and improve the effect of feature selection. Experimental results show that the Clustering Accuracy (ACC) and Normalized Mutual Information (NMI) of SLREO are significantly improved, as compared with six other published algorithms, tested on 11 benchmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004460",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature selection",
      "Laplace operator",
      "Laplacian matrix",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Spectral clustering"
    ],
    "authors": [
      {
        "surname": "Shang",
        "given_name": "Ronghua"
      },
      {
        "surname": "Kong",
        "given_name": "Jiarui"
      },
      {
        "surname": "Zhang",
        "given_name": "Weitong"
      },
      {
        "surname": "Feng",
        "given_name": "Jie"
      },
      {
        "surname": "Jiao",
        "given_name": "Licheng"
      },
      {
        "surname": "Stolkin",
        "given_name": "Rustam"
      }
    ]
  },
  {
    "title": "Intentional-Deception Detection Based on Facial Muscle Movements in an Interactive Social Context",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.10.008",
    "abstract": "Micro-expression, which is generated by facial muscle movements, could be a crucial cue for deception detection. In the existed research investigating the relationship between facial muscles and deception detection, researchers have focused almost exclusively on two muscles, i.e., zygomaticus and corrugator supercilii, based on the theoretical basis that they are highly associated with positive and negative expressions. However, the aim of this study is to demonstrate the direct relationship between facial muscle movements and deception detection. Addressing this issue, this paper proposes an experimental paradigm with high ecological validity that uses electromyography (EMG) signals to precisely examine the role of facial muscle movements in deception detection. Moreover, we propose a vector-based sequential forward selection (VSFS) algorithm to identify the muscle (or muscle combination) most closely associated with lying. Based on our proposed approach, the importance of seven selected facial muscles was explored by comparing the corresponding facial EMG (fEMG) between truth and lying conditions. First, the present study found that the zygomaticus and corrugator supercilii could play important roles in deception detection, and our findings are consistent with existed research. Second, the experiment result verified that the muscles related to deception detection were consistent with those with higher frequency occurring in micro-expression. Moreover, the present study provides a theoretical basis that intelligent micro-expressions analysis could improve the lie detection performance by focusing on the area of the forehead, eyebrows, and cheeks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003014",
    "keywords": [
      "Anatomy",
      "Artificial intelligence",
      "Biology",
      "Cognitive psychology",
      "Communication",
      "Computer science",
      "Context (archaeology)",
      "Deception",
      "Electromyography",
      "Facial electromyography",
      "Facial expression",
      "Facial muscles",
      "Forehead",
      "Lie detection",
      "Lying",
      "Medicine",
      "Neuroscience",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Psychology",
      "Radiology",
      "Social psychology",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Dong",
        "given_name": "Zizhao"
      },
      {
        "surname": "Wang",
        "given_name": "Gang"
      },
      {
        "surname": "Lu",
        "given_name": "Shaoyuan"
      },
      {
        "surname": "Dai",
        "given_name": "Luyao"
      },
      {
        "surname": "Huang",
        "given_name": "Shucheng"
      },
      {
        "surname": "Liu",
        "given_name": "Ye"
      }
    ]
  },
  {
    "title": "Adversarial training of LSTM-ED based anomaly detection for complex time-series in cyber-physical-social systems",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.10.017",
    "abstract": "With the development and maturity of smart cities, more and more Cyber-Physical-Social Systems (CPSSs) need to monitor a variety of time-series data from sensors and network transmissions to ensure the quality and reliability of the Cyber-Physical-Social Services. Time-series anomaly detection is a common search problem in the field of pattern recognition. Existing approaches and models of anomaly detection have solved the problem of simple smooth time-series and perform ideal recognition performance. However, in real scenarios, complex time-series with non-Gaussian noise and complex data distributions are prevalent. Compared to smooth and simple time-series, complex time-series occur more frequently in real-world settings and are difficult to model and label. To address these challenges, this paper proposes an unsupervised anomaly detection algorithm based on Long Short-Term Memory Encoder-Decoder (LSTM-ED) via an adversarial training method for complex time-series in cyber-physical-social systems with high performance. This is a novel method that incorporates adversarial learning to improve the robustness of encoder-decoder architecture, enabling it to obtain good anomaly detection results for complex time-series. In addition, LSTM is employed as the network unit of encoder-decoder architecture, which is also able to extract temporal correlation in time-series to a greater extent. We have conducted extensive experiments on four datasets from real scenarios and the results show that the accuracy of the proposed adversarial training of LSTM-ED is significantly better than that of the state-of-the-art methods, including other unsupervised methods and traditional supervised methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003129",
    "keywords": [
      "Adversarial system",
      "Anomaly detection",
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Cyber-physical system",
      "Data mining",
      "Deep learning",
      "Gene",
      "Machine learning",
      "Operating system",
      "Paleontology",
      "Robustness (evolution)",
      "Series (stratigraphy)",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Haiqi"
      },
      {
        "surname": "Liu",
        "given_name": "Shaohui"
      },
      {
        "surname": "Jiang",
        "given_name": "Feng"
      }
    ]
  },
  {
    "title": "An End-to-end Supervised Domain Adaptation Framework for Cross-Domain Change Detection",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108960",
    "abstract": "Change detection is a crucial but extremely challenging task in remote sensing image analysis, and much progress has been made with the rapid development of deep learning. However, most existing deep learning-based change detection methods try to elaborately design complicated neural networks with powerful feature representations. However, they ignore the universal domain shift induced by time-varying land cover changes, including luminance fluctuations and seasonal changes between pre-event and post-event images, thereby producing suboptimal results. In this paper, we propose an end-to-end supervised domain adaptation framework for cross-domain change detection named SDACD, to effectively alleviate the domain shift between bi-temporal images for better change predictions. Specifically, our SDACD presents collaborative adaptations from both image and feature perspectives with supervised learning. Image adaptation exploits generative adversarial learning with cycle-consistency constraints to perform cross-domain style transformation, which effectively narrows the domain gap in a two-side generation fashion. As for feature adaptation, we extract domain-invariant features to align different feature distributions in the feature space, which could further reduce the domain gap of cross-domain images. To further improve the performance, we combine three types of bi-temporal images for the final change prediction, including the initial input bi-temporal images and two generated bi-temporal images from the pre-event and post-event domains. Extensive experiments and analyses conducted on two benchmarks demonstrate the effectiveness and generalizability of our proposed framework. Notably, our framework pushes several representative baseline models up to new State-Of-The-Art records, achieving 97.34% and 92.36% on the CDD and WHU building datasets, respectively. The source code and models are publicly available at https://github.com/Perfect-You/SDACD.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200440X",
    "keywords": [
      "Artificial intelligence",
      "Change detection",
      "Computer science",
      "Deep learning",
      "Domain (mathematical analysis)",
      "Event (particle physics)",
      "Feature (linguistics)",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Jia"
      },
      {
        "surname": "Xuan",
        "given_name": "Wenjie"
      },
      {
        "surname": "Gan",
        "given_name": "Yuhang"
      },
      {
        "surname": "Zhan",
        "given_name": "Yibing"
      },
      {
        "surname": "Liu",
        "given_name": "Juhua"
      },
      {
        "surname": "Du",
        "given_name": "Bo"
      }
    ]
  },
  {
    "title": "Infrared and visible image fusion via parallel scene and texture learning",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108929",
    "abstract": "Image fusion plays a pivotal role in numerous high-level computer vision tasks. Existing deep learning-based image fusion methods usually leverage an implicit manner to achieve feature extraction, which would cause some characteristics of source images, e.g., contrast and structural information, are unable to be fully extracted and integrated into the fused images. In this work, we propose an infrared and visible image fusion method via parallel scene and texture learning. Our key objective is to deploy two branches of deep neural networks, namely the content branch and detail branch, to synchronously extract different characteristics from source images and then reconstruct the fused image. The content branch focuses primarily on coarse-grained information and is deployed to estimate the global content of source images. The detail branch primarily pays attention to fine-grained information, and we design an omni-directional spatially variant recurrent neural networks in this branch to model the internal structure of source images more accurately and extract texture-related features in an explicit manner. Extensive experiments show that our approach achieves significant improvements over state-of-the-arts on qualitative and quantitative evaluations with comparatively less running time consumption. Meanwhile, we also demonstrate the superiority of our fused results in the object detection task. Our code is available at: https://github.com/Melon-Xu/PSTLFusion.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004101",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Feature extraction",
      "Fusion",
      "Image (mathematics)",
      "Image fusion",
      "Leverage (statistics)",
      "Linguistics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Source code"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Meilong"
      },
      {
        "surname": "Tang",
        "given_name": "Linfeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Hao"
      },
      {
        "surname": "Ma",
        "given_name": "Jiayi"
      }
    ]
  },
  {
    "title": "Distribution alignment for cross-device palmprint recognition",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108942",
    "abstract": "With the development of IoT and mobile devices, cross-device palmprint recognition is becoming an emerging research topic in multimedia for its great application potential. Due to the diverse characteristics of different devices, e.g.resolution or artifacts caused by post-processing, cross-device palmprint recognition remains a challenging problem. In this paper, we make efforts to improve cross-device palmprint recognition in two aspects: (1) we put forward a novel distribution-based loss to narrow the representation gap across devices, and (2) we establish a new cross-device benchmark based on existing palmprint recognition datasets. Different from many recent studies that only utilize instance-level or pairwise-level information between devices, the proposed progressive target distribution loss (PTD loss) uses the distributional information. Moreover, we establish a progressive target mechanism that will be dynamically updated during training, making the optimization easier and smoother. The newly established benchmark contains more samples and more types of IoT devices than previous benchmarks, which can facilitate cross-device palmprint research. Extensive comparisons on several benchmarks reveal that: (1) our method outperforms other cross-device biometric recognition approaches significantly; (2) our method presents superior performance compared to SOTA competitors on several general palmprint recognition benchmarks; Code and data are openly available at https://kaizhao.net/palmprint.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004228",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biometrics",
      "Code (set theory)",
      "Computer science",
      "Computer vision",
      "Geodesy",
      "Geography",
      "Law",
      "Mobile device",
      "Operating system",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Shen",
        "given_name": "Lei"
      },
      {
        "surname": "Zhang",
        "given_name": "Yingyi"
      },
      {
        "surname": "Zhao",
        "given_name": "Kai"
      },
      {
        "surname": "Zhang",
        "given_name": "Ruixin"
      },
      {
        "surname": "Shen",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Self-supervised spectral clustering with exemplar constraints",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108975",
    "abstract": "As a leading graph clustering technique, spectral clustering is one of the most widely used clustering methods that captures complex clusters in data. However, some of its deficiencies, such as the high computational complexity in eigen decomposition and the guidance without supervised information, limit its real applications. To get rid of the deficiencies, we propose a self-supervised spectral clustering algorithm. In this algorithm, we define an exemplar constraint which reflects the relations between objects and exemplars. We provide the related analysis to show that it is more suitable for unsupervised learning. Based on the exemplar constraint, we build an optimization model for self-supervised spectral clustering so that we can simultaneously learn clustering results and exemplar constraints. Furthermore, we propose an iterative method to solve the new optimization problem. Compared to other existing versions of spectral clustering algorithms, the new algorithm can use the low computational costs to discover a high-quality cluster structure of a data set without prior information. Furthermore, we did a number of experiments of algorithm comparison and parameter analysis on benchmark data sets to illustrate that the proposed algorithm is very effective and efficient.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004551",
    "keywords": [
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Constrained clustering",
      "Correlation clustering",
      "Data mining",
      "Data stream clustering",
      "Pattern recognition (psychology)",
      "Spectral clustering"
    ],
    "authors": [
      {
        "surname": "Bai",
        "given_name": "Liang"
      },
      {
        "surname": "Zhao",
        "given_name": "Yunxiao"
      },
      {
        "surname": "Liang",
        "given_name": "Jiye"
      }
    ]
  },
  {
    "title": "Discrete space reinforcement learning algorithm based on twin support vector machine classification",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.11.017",
    "abstract": "Reinforcement learning (RL) has become one of the key component of various machine learning algorithms in recent years. However, traditional RL algorithms lack convergence speed and accuracy in small-scale discrete space environment. Recently An et al. proposed RL algorithm based on support vector machines (SVMs) (Pattern Recognit. Lett. 111 (2018) 30-35) which adopts the Advantage Actor-Critic (A2C) framework and improves the speed and accuracy of convergence in discrete space. Owing to the advantages of twin support vector machines (TWSVMs) over SVMs, in this paper, we propose a RL algorithm based on TWSVM classification. The proposed algorithm adopts a modified A2C framework, where there are multiple Actors and a single Critic. Finally, we compare the performance of the proposed algorithm with some existing algorithms in traditional RL environment. Interestingly, the proposed algorithm outperforms the existing algorithms in terms of convergence speed and accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003476",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Key (lock)",
      "Machine learning",
      "Reinforcement learning",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Wenguo"
      },
      {
        "surname": "Zhou",
        "given_name": "Zhengchun"
      },
      {
        "surname": "Adhikary",
        "given_name": "Avik Ranjan"
      },
      {
        "surname": "Dutta",
        "given_name": "Bapi"
      }
    ]
  },
  {
    "title": "Progressive Deep Non-Negative Matrix Factorization Architecture with Graph Convolution-based Basis Image Reorganization",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108984",
    "abstract": "Deep non-negative matrix factorization is committed to using multi-layer structure to extract underlying parts-based representation. However, the basis images obtained by continuous depth factorization is too sparse, resulting in too fragmented parts reflected by the basis image. This makes the number of factorization layers limited and the underlying local feature representation is inaccurate. Therefore, we propose a novel progressive deep non-negative matrix factorization (PDNMF) architecture that adds a basis image reconstruction step to the successive basis image factorization steps. This helps the basis image in depth factorization to maintain better robustness of feature representation. In the reconstruction step, the attribute similarity graph (ASG) is constructed to describe the semantic expression ability of each basis image. With the help of the ASG, the basis image enhances its own semantic integrity through graph convolution without drastically destroying its representation. The evaluation in image recognition shows that the recognition accuracy of the proposed PDNMF improves with the increase of layers. Our method outperforms the state-of-the-art deep factorization methods in image recognition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004642",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Basis (linear algebra)",
      "Basis function",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Convolution (computer science)",
      "Eigenvalues and eigenvectors",
      "Factorization",
      "Feature (linguistics)",
      "Gene",
      "Geometry",
      "Graph",
      "Incomplete LU factorization",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Matrix decomposition",
      "Non-negative matrix factorization",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Robustness (evolution)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Yang"
      },
      {
        "surname": "Deng",
        "given_name": "Furong"
      },
      {
        "surname": "Pei",
        "given_name": "Jihong"
      },
      {
        "surname": "Yang",
        "given_name": "Xuan"
      }
    ]
  },
  {
    "title": "Enhancement of DNN-based multilabel classification by grouping labels based on data imbalance and label correlation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108964",
    "abstract": "Multilabel classification (MLC) is a challenging task in real-world applications, such as project document classification which led us to conduct this research. In the past decade, deep neural networks (DNNs) have been explored in MLC due to their flexibility in dealing with annotated data. However, DNN-based MLC still suffers many problems. Two critical problems are data imbalance and label correlation. These two problems will become more prominent when a training dataset is limited and with a large label set. In this study, special neural network configurations were developed to enhance the performance of DNN-based MLC based on data imbalance and label correlation. The classification accuracy of minority labels and users-preferred labels was increased using customized label groups. The proposed method was evaluated using river restoration project documents and other fifteen datasets. The results show that the proposed method generally increases f1-score for minority labels up to 10%. Adding label dependence into label groups improves the f1-score of user-preferred majority labels up to 5%. The accuracy increase varies in different datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004447",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Correlation",
      "Data mining",
      "Data set",
      "Deep neural networks",
      "Economics",
      "Flexibility (engineering)",
      "Geometry",
      "Machine learning",
      "Management",
      "Mathematics",
      "Multi-label classification",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Statistics",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Ling"
      },
      {
        "surname": "Wang",
        "given_name": "Yuhong"
      },
      {
        "surname": "Li",
        "given_name": "Hao"
      }
    ]
  },
  {
    "title": "Joint operation and attention block search for lightweight image restoration",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108909",
    "abstract": "Recently, block-based design methods have shown effectiveness in image restoration tasks, which are usually designed in a handcrafted manner and have computation and memory consumption challenges in practice. In this paper, we propose a joint operation and attention block search algorithm for image restoration, which focuses on searching for optimal combinations of operation blocks and attention blocks. Specifically, we first construct two search spaces: operation block search space and attention block search space. The former is used to explore the suitable operation of each layer and aims to construct a lightweight and effective operation search module (OSM). The latter is applied to discover the optimal connection of various attention mechanisms and aims to enhance the feature expression. The searched structure is called the attention search module (ASM). Then we combine OSM and ASM to construct a joint search module (JSM), which serves as the basic module to build the final network. Moreover, we propose a cross-scale fusion module (CSFM) to effectively integrate multiple hierarchical features from JSMs, which helps to mine feature corrections of intermediate layers. Extensive experiments on image super-resolution, gray image denoising, and JPEG image deblocking tasks demonstrate that our proposed network can achieve competitive performance. The source code is available on https://github.com/it-hao/JSNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003909",
    "keywords": [
      "Architectural engineering",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Code (set theory)",
      "Computer science",
      "Computer vision",
      "Construct (python library)",
      "Engineering",
      "Feature (linguistics)",
      "Geometry",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "JPEG",
      "Joint (building)",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Shen",
        "given_name": "Hao"
      },
      {
        "surname": "Zhao",
        "given_name": "Zhong-Qiu"
      },
      {
        "surname": "Liao",
        "given_name": "Wenrui"
      },
      {
        "surname": "Tian",
        "given_name": "Weidong"
      },
      {
        "surname": "Huang",
        "given_name": "De-Shuang"
      }
    ]
  },
  {
    "title": "Online temporal classification of human action using action inference graph",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108972",
    "abstract": "Nowadays, deep learning methods have achieved state-of-the-art results in human action recognition. These methods process a full video sequence to recognize an action, which is unnecessary because many frames are similar. Recently, keyframe-based methods are proposed to overcome this issue. Though keyframe based methods have shown competitive performance in action recognition, both methods still process all the required frames of a video clip and average the results of individual clips/frames to recognize the action of the video. We argue that by simply using the average of the results of the video clips, deep models are not using the motion information of the video and thus leads to an inaccurate recognition of the action. To cope with the aforementioned issue, we propose a new online temporal classification model (OTCM) that classifies an action from a video in an online fashion and addresses the issue of averaging by making decision of each frame of a video sequence. As well, we propose a new action inference graph (AIG) that enables early recognition. Hence, the proposed model can recognize an action early before using all the keyframes or the whole video sequence and thus, requires less computation for recognizing human actions. Moreover, our OTCM can perform online action detection. To the best of our knowledge, this is the first time that the OTCM model along with the AIG is proposed. The experimental results of the benchmark datasets show that the proposed OTCM model has achieved and set a new record of the SOTA results, in particular, without using full video sequences.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004526",
    "keywords": [
      "Action (physics)",
      "Action recognition",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "CLIPS",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Genetics",
      "Geodesy",
      "Geography",
      "Graph",
      "Inference",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Process (computing)",
      "Quantum mechanics",
      "Sequence (biology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "E Elahi",
        "given_name": "G M Mashrur"
      },
      {
        "surname": "Yang",
        "given_name": "Yee-Hong"
      }
    ]
  },
  {
    "title": "Scale-selective and noise-robust extended local binary pattern for texture classification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108901",
    "abstract": "As one of the most successful local feature descriptors, the local binary pattern (LBP) estimates the texture distribution rule of an image based on the signs of differences between neighboring pixels to obtain intensity- and rotation- invariance. In this paper, we propose a novel image descriptor to address scale transformation and noise interference simultaneously. We name it scale-selective and noise-robust extended LBP (SNELBP). First, each image in training sets is transformed into different scale spaces by a Gaussian filter. Second, noise-robust pattern histograms are obtained from each scale space by using our previously proposed median robust extended LBP (MRELBP). Then, scale-invariant histograms are determined by selecting the maximum among all scale levels for a certain image. Finally, the most informative patterns are selected from the dictionary pretrained by the two-stage compact dominant feature selection method (CDFS), maintaining the descriptor more lightweight with sufficiently low time cost. Extensive experiments on five public databases (Outex_TC_00011, TC_00012, KTH-TIPS, UMD and NEU) and one fresh texture database (JoJo) under two kinds of interferences (Gaussian and salt pepper) indicate that our SNELBP yields more competitive results than thirty classical LPB variants as well as eight typical deep learning methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200382X",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Feature vector",
      "Filter (signal processing)",
      "Gaussian",
      "Gaussian noise",
      "Gene",
      "Histogram",
      "Image (mathematics)",
      "Image processing",
      "Linguistics",
      "Local binary patterns",
      "Mathematics",
      "Mixture model",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pixel",
      "Quantum mechanics",
      "Robustness (evolution)",
      "Scale (ratio)",
      "Scale invariance",
      "Scale space",
      "Statistics",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Qiwu"
      },
      {
        "surname": "Su",
        "given_name": "Jiaojiao"
      },
      {
        "surname": "Yang",
        "given_name": "Chunhua"
      },
      {
        "surname": "Silven",
        "given_name": "Olli"
      },
      {
        "surname": "Liu",
        "given_name": "Li"
      }
    ]
  },
  {
    "title": "Neural architecture search via reference point based multi‐objective evolutionary algorithm",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108962",
    "abstract": "For neural architecture search, NSGA-Net has searched a representative neural architecture set of Pareto-optimal solutions to consider both accuracy and computation complexity simultaneously. However, some decision-makers only concentrate on such neural architectures in the subpart regions of Pareto-optimal Frontier that they have interests in. Under the above circumstances, certain uninterested neural architectures may cost many computing resources. In order to consider the preference of decision-makers, we propose the reference point based NSGA-Net (RNSGA-Net) for neural architecture search. The core of RNSGA-Net adopts the reference point approach to guarantee the Pareto-optimal region close to the reference points and also combines the advantage of NSGAII with the fast nondominated sorting approach to split the Pareto front. Moreover, we augment an extra bit value of the original encoding to represent two types of residual block and one type of dense block for residual connection and dense connection in the RNSGA-Net. In order to satisfy the decision-maker preference, the multi-objective is measured to search competitive neural architecture by minimizing an error metric and FLOPs of computational complexity. Experiment results on the CIFAR-10 dataset demonstrate that RNSGA-Net can improve NSGA-Net in terms of the more structured representation space and the preference of decision-makers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004423",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Block (permutation group theory)",
      "Computer science",
      "Engineering",
      "Geometry",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Metric (unit)",
      "Multi-objective optimization",
      "Operations management",
      "Pareto principle",
      "Sorting"
    ],
    "authors": [
      {
        "surname": "Tong",
        "given_name": "Lyuyang"
      },
      {
        "surname": "Du",
        "given_name": "Bo"
      }
    ]
  },
  {
    "title": "Generalized discriminant analysis via kernel exponential families",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108933",
    "abstract": "This paper introduces a novel supervised dimension reduction method for classification and regression problems using reproducing kernel Hilbert spaces. The proposed approach takes advantage of the modeling power of kernel exponential families to extract nonlinear summary statistics of the data that are sufficient to preserve information about the target response. For the special case of finite dimensional exponential family distributions, the proposed method is shown to simplify the known solutions for sufficient dimension reduction. A connection with support vector machines is shown and exploited to obtain efficient estimation procedures. Experiments with simulated and real data illustrate the potential of the proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004058",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Computer science",
      "Dimension (graph theory)",
      "Dimensionality reduction",
      "Discrete mathematics",
      "Exponential family",
      "Exponential function",
      "Hilbert space",
      "Kernel (algebra)",
      "Kernel method",
      "Linear discriminant analysis",
      "Mathematical analysis",
      "Mathematics",
      "Nonlinear system",
      "Pattern recognition (psychology)",
      "Physics",
      "Pure mathematics",
      "Quantum mechanics",
      "Reproducing kernel Hilbert space",
      "Statistics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Ibañez",
        "given_name": "Isaías"
      },
      {
        "surname": "Forzani",
        "given_name": "Liliana"
      },
      {
        "surname": "Tomassi",
        "given_name": "Diego"
      }
    ]
  },
  {
    "title": "Domain consistency regularization for unsupervised multi-source domain adaptive classification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108955",
    "abstract": "Deep learning-based multi-source unsupervised domain adaptation (MUDA) has been actively studied in recent years. Compared with single-source unsupervised domain adaptation (SUDA), domain shift in MUDA exists not only between the source and target domains but also among multiple source domains. Most existing MUDA algorithms focus on extracting domain-invariant representations among all domains whereas the task-specific decision boundaries among classes are largely neglected. In this paper, we propose an end-to-end trainable network that exploits domain Consistency Regularization for unsupervised Multi-source domain Adaptive classification (CRMA). CRMA aligns not only the distributions of each pair of source and target domains but also that of all domains. For each pair of source and target domains, we employ an intra-domain consistency to regularize a pair of domain-specific classifiers to achieve intra-domain alignment. In addition, we design an inter-domain consistency that targets joint inter-domain alignment among all domains. To address different similarities between multiple source domains and the target domain, we design an authorization strategy that assigns different authorities to domain-specific classifiers adaptively for optimal pseudo label prediction and self-training. Extensive experiments show that CRMA tackles unsupervised domain adaptation effectively under a multi-source setup and achieves superior adaptation consistently across multiple MUDA datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004356",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer security",
      "Consistency (knowledge bases)",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Exploit",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Multi-source",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Zhipeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaobing"
      },
      {
        "surname": "Lu",
        "given_name": "Shijian"
      },
      {
        "surname": "Yi",
        "given_name": "Shuai"
      }
    ]
  },
  {
    "title": "Information-theoretic policy learning from partial observations with fully informed decision makers",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.10.025",
    "abstract": "In this work we formulate and treat an extension of the Imitation from Observations problem. Imitation from Observations is a generalisation of the well-known Imitation Learning problem where state-only demonstrations are considered. In our treatment we extend the scope of Imitation from Observations to feature-only demonstrations which could arguably be described as partial observations. Therewith we mean that the full state of the decision makers is unknown and imitation must take place on the basis of a limited set of features. We set out for methods that extract an executable policy directly from those features which, in the literature, would be referred to as Behavioural Cloning methods. Our treatment combines elements from probability and information theory and draws connections with entropy regularized Markov Decision Processes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003245",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Entropy (arrow of time)",
      "Executable",
      "Imitation",
      "Machine learning",
      "Markov decision process",
      "Markov process",
      "Mathematics",
      "Operating system",
      "Physics",
      "Programming language",
      "Psychology",
      "Quantum mechanics",
      "Set (abstract data type)",
      "Social psychology",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Lefebvre",
        "given_name": "Tom"
      }
    ]
  },
  {
    "title": "Single image based 3D human pose estimation via uncertainty learning",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108934",
    "abstract": "In monocular image scenes, 3D human pose estimation exhibits inherent ambiguity due to the loss of depth information and occlusions. Simply regressing body joints with high uncertainties will lead to model overfitting and poor generalization. In this paper, we propose an uncertainty-based framework to jointly learn 3D human poses and the uncertainty of each joint. Our proposed joint estimation framework aims to mitigate the adverse effects of training samples with high uncertainties and facilitate the training procedure. To be specific, we model each body joint as a Laplace distribution for uncertainty representation. Since visual joints often exhibit low uncertainties while occluded ones have high uncertainties, we develop an adaptive scaling factor, named the uncertainty-aware scaling factor, to ease the network optimization in accordance with the joint uncertainties. By doing so, our network is able to converge faster and significantly reduce the adverse effects caused by those ambiguous joints. Furthermore, we present an uncertainty-aware graph convolutional network by exploiting the learned joint uncertainties and the relationships among joints to refine the initial joint localization. Extensive experiments on single-person (Human3.6M) and multi-person (MuCo-3DHP & MuPoTS-3D) 3D human pose estimation datasets demonstrate the effectiveness of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004149",
    "keywords": [
      "3D pose estimation",
      "Ambiguity",
      "Architectural engineering",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Generalization",
      "Joint (building)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Monocular",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Pose",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Han",
        "given_name": "Chuchu"
      },
      {
        "surname": "Yu",
        "given_name": "Xin"
      },
      {
        "surname": "Gao",
        "given_name": "Changxin"
      },
      {
        "surname": "Sang",
        "given_name": "Nong"
      },
      {
        "surname": "Yang",
        "given_name": "Yi"
      }
    ]
  },
  {
    "title": "Unsupervised contrastive unpaired image generation approach for improving tuberculosis screening using chest X-ray images",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.10.026",
    "abstract": "Tuberculosis is an infectious disease that mainly affects the lung tissues. Therefore, chest X-ray imaging can be very useful to diagnose and to understand the evolution of the pathology. This image modality has a poorer quality in contrast with other techniques as the magnetic resonance or the computerized tomography, but chest X-ray is easier and cheaper to perform. Furthermore, data scarcity is challenging in the domain of biomedical imaging. In order to mitigate this problem, the use of Generative Adversarial Network models for image generation has proved to be a powerful approach to train the deep learning models with small datasets, representing an alternative to classic data augmentation strategies. In this work, we propose a fully automatic approach for the generation of novel synthetic chest X-ray images to mitigate the effect of data scarcity in order to improve the tuberculosis screening performance using 3 different publicly available representative datasets: Montgomery County, Shenzhen and TBX11K. Firstly, this approach trains image translation models with a large-sized dataset (TBX11K). Then, these models are used to generate the novel set of synthetic images using small-sized and medium-sized datasets (Montgomery County and Shenzhen, respectively). Finally, the novel set of generated images is added to the training set to improve the performance of an automatic tuberculosis screening. As a result, we obtained an 88.41% ± 5.27% of accuracy for the Montgomery County dataset and a 90.33% ± 1.41% for the Shenzhen dataset. These results demonstrate that the proposed method outperforms previous state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003257",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Image (mathematics)",
      "Image quality",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Morís",
        "given_name": "Daniel I."
      },
      {
        "surname": "de Moura",
        "given_name": "Joaquim"
      },
      {
        "surname": "Novo",
        "given_name": "Jorge"
      },
      {
        "surname": "Ortega",
        "given_name": "Marcos"
      }
    ]
  },
  {
    "title": "GFNet: Automatic segmentation of COVID-19 lung infection regions using CT images based on boundary features",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108963",
    "abstract": "In early 2020, the global spread of the COVID-19 has presented the world with a serious health crisis. Due to the large number of infected patients, automatic segmentation of lung infections using computed tomography (CT) images has great potential to enhance traditional medical strategies. However, the segmentation of infected regions in CT slices still faces many challenges. Specially, the most core problem is the high variability of infection characteristics and the low contrast between the infected and the normal regions. This problem leads to fuzzy regions in lung CT segmentation. To address this problem, we have designed a novel global feature network(GFNet) for COVID-19 lung infections: VGG16 as backbone, we design a Edge-guidance module(Eg) that fuses the features of each layer. First, features are extracted by reverse attention module and Eg is combined with it. This series of steps enables each layer to fully extract boundary details that are difficult to be noticed by previous models, thus solving the fuzzy problem of infected regions. The multi-layer output features are fused into the final output to finally achieve automatic and accurate segmentation of infected areas. We compared the traditional medical segmentation networks, UNet, UNet++, the latest model Inf-Net, and methods of few shot learning field. Experiments show that our model is superior to the above models in Dice, Sensitivity, Specificity and other evaluation metrics, and our segmentation results are clear and accurate from the visual effect, which proves the effectiveness of GFNet. In addition, we verify the generalization ability of GFNet on another “never seen” dataset, and the results prove that our model still has better generalization ability than the above model. Our code has been shared at https://github.com/zengzhenhuan/GFNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004435",
    "keywords": [
      "Artificial intelligence",
      "Boundary (topology)",
      "Computer science",
      "Computer vision",
      "Coronavirus disease 2019 (COVID-19)",
      "Dice",
      "Disease",
      "Feature (linguistics)",
      "Fuzzy logic",
      "Generalization",
      "Image segmentation",
      "Infectious disease (medical specialty)",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Segmentation",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "Chaodong"
      },
      {
        "surname": "Zeng",
        "given_name": "Zhenhuan"
      },
      {
        "surname": "Xiao",
        "given_name": "Leyi"
      },
      {
        "surname": "Qu",
        "given_name": "Xilong"
      }
    ]
  },
  {
    "title": "Conditional motion in-betweening",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108894",
    "abstract": "Motion in-betweening (MIB) is a process of generating intermediate skeletal movement between the given start and target poses while preserving the naturalness of the motion, such as periodic footstep motion while walking. Although state-of-the-art MIB methods are capable of producing plausible motions given sparse key-poses, they often lack the controllability to generate motions satisfying the semantic contexts required in practical applications. We focus on the method that can handle pose or semantic conditioned MIB tasks using a unified model. We also present a motion augmentation method to improve the quality of pose-conditioned motion generation via defining a distribution over smooth trajectories. Our proposed method outperforms the existing state-of-the-art MIB method in pose prediction errors while providing additional controllability. Our code and results are available on our project web page: https://jihoonerd.github.io/Conditional-Motion-In-Betweening.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003752",
    "keywords": [
      "Applied mathematics",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Controllability",
      "Focus (optics)",
      "Key (lock)",
      "Mathematics",
      "Motion (physics)",
      "Naturalness",
      "Operating system",
      "Optics",
      "Physics",
      "Process (computing)",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Jihoon"
      },
      {
        "surname": "Byun",
        "given_name": "Taehyun"
      },
      {
        "surname": "Shin",
        "given_name": "Seungyoun"
      },
      {
        "surname": "Won",
        "given_name": "Jungdam"
      },
      {
        "surname": "Choi",
        "given_name": "Sungjoon"
      }
    ]
  },
  {
    "title": "Mixup gamblers+: Learning interpolated pseudo “uncertainty” in latent feature space for reliable inference",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.09.020",
    "abstract": "We introduce Mixup Gamblers+, a method for building a deep learning classifier capable of self-evaluating the reliability of inference results. In the proposed method, samples with high uncertainty are generated virtually through data interpolation in the feature space embedded by the deep learning model, and the classifier is trained to detect them. Moreover, we introduce metric learning for feature representation learning to correlate the distance in the latent feature space with the similarity of the samples. This enables us to estimate the uncertainty of the pseudo data based on the distance in the latent feature space and enhances the training of our proposed method. The use of data interpolation in the latent feature space makes the proposed method a general-purpose method for learning uncertainty estimation models for inference, independent of the dataset and problem settings. The proposed method improves the accuracy of the inference and achieves state-of-the-art inferential uncertainty estimation results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522002902",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Feature learning",
      "Feature vector",
      "Inference",
      "Interpolation (computer graphics)",
      "Linguistics",
      "Machine learning",
      "Motion (physics)",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Yamaguchi",
        "given_name": "Takumi"
      },
      {
        "surname": "Murakawa",
        "given_name": "Masahiro"
      }
    ]
  },
  {
    "title": "Unsupervised domain adaptation with progressive adaptation of subspaces",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108918",
    "abstract": "Unsupervised Domain Adaptation (UDA) aims to classify unlabeled target domain by transferring knowledge from labeled source domain with domain shift. Most of the existing UDA methods try to mitigate the adverse impact induced by the shift via reducing domain discrepancy. However, such approaches easily suffer a notorious mode collapse issue due to the lack of labels in target domain. Naturally, one of the effective ways to mitigate this issue is to reliably estimate the pseudo labels for target domain, which itself is hard. To overcome this, we propose a novel UDA method named Progressive Adaptation of Subspaces approach (PAS) in which we utilize such an intuition that appears much reasonable to gradually obtain reliable pseudo labels. Specifically, we progressively and steadily refine the shared subspaces as bridge of knowledge transfer by adaptively anchoring/selecting and leveraging those target samples with reliable pseudo labels. Subsequently, the refined subspaces can in turn provide more reliable pseudo-labels of the target domain, making the mode collapse highly mitigated. Our thorough evaluation demonstrates that PAS is not only effective for common UDA, but also outperforms the state-of-the arts for more challenging Partial Domain Adaptation (PDA) situation, where the source label set subsumes the target one.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003995",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Epistemology",
      "Geometry",
      "Intuition",
      "Linear subspace",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Weikai"
      },
      {
        "surname": "Chen",
        "given_name": "Songcan"
      }
    ]
  },
  {
    "title": "Corrigendum to “LiDAR-based localization using universal encoding and memory-aware regression” Pattern Recognition Volume 128 (2022) 108685",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108915",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200396X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Encoding (memory)",
      "Geography",
      "Lidar",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Regression",
      "Remote sensing",
      "Statistics",
      "Volume (thermodynamics)"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Shangshu"
      },
      {
        "surname": "Wang",
        "given_name": "Cheng"
      },
      {
        "surname": "Wen",
        "given_name": "Chenglu"
      },
      {
        "surname": "Cheng",
        "given_name": "Ming"
      },
      {
        "surname": "Liu",
        "given_name": "Minghao"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhihong"
      },
      {
        "surname": "Li",
        "given_name": "Xin"
      }
    ]
  },
  {
    "title": "Counterfactual explanation based on gradual construction for deep networks",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108958",
    "abstract": "To understand the black-box characteristics of deep networks, counterfactual explanation that deduces not only the important features of an input space but also how those features should be modified to classify input as a target class has gained an increasing interest. The patterns that deep networks have learned from a training dataset can be grasped by observing the feature variation among various classes. However, current approaches perform the feature modification to increase the classification probability for the target class irrespective of the internal characteristics of deep networks. This often leads to unclear explanations that deviate from real-world data distributions. To address this problem, we propose a counterfactual explanation method that exploits the statistics learned from a training dataset. Especially, we gradually construct an explanation by iterating over masking and composition steps. The masking step aims to select an important feature from the input data to be classified as a target class. Meanwhile, the composition step aims to optimize the previously selected feature by ensuring that its output score is close to the logit space of the training data that are classified as the target class. Experimental results show that our method produces human-friendly interpretations on various classification datasets and verify that such interpretations can be achieved with fewer feature modification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004381",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Construct (python library)",
      "Counterfactual thinking",
      "Data mining",
      "Epistemology",
      "Feature (linguistics)",
      "Feature vector",
      "Linguistics",
      "Machine learning",
      "Masking (illustration)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Space (punctuation)",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Jung",
        "given_name": "Hong-Gyu"
      },
      {
        "surname": "Kang",
        "given_name": "Sin-Han"
      },
      {
        "surname": "Kim",
        "given_name": "Hee-Dong"
      },
      {
        "surname": "Won",
        "given_name": "Dong-Ok"
      },
      {
        "surname": "Lee",
        "given_name": "Seong-Whan"
      }
    ]
  },
  {
    "title": "Alleviating the over-smoothing of graph neural computing by a data augmentation strategy with entropy preservation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108951",
    "abstract": "The Graph Convolutional Networks (GCN) proposed by Kipf and Welling is an effective model to improve semi-supervised learning of pattern recognition, but faces the obstacle of over-smoothing, which will weaken the representation ability of GCN. Recently some works are proposed to tackle above limitation by randomly perturbing graph topology or feature matrix to generate data augmentations as input for training. However, these operations inevitably do damage to the integrity of information structures and have to sacrifice the smoothness of feature manifold. In this paper, we first introduce a novel graph entropy definition as a measure to quantitatively evaluate the smoothness of a data manifold and then point out that this graph entropy is controlled by triangle motif-based information structures. Considering the preservation of graph entropy, we propose an effective strategy to generate randomly perturbed training data but maintain both graph topology and graph entropy. Extensive experiments have been conducted on real-world datasets and the results verify the effectiveness of our proposed method in improving semi-supervised node classification accuracy compared with a surge of baselines. Beyond that, our proposed approach could significantly enhance the robustness of training process for GCN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004319",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Entropy (arrow of time)",
      "Graph",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Smoothing",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Xue"
      },
      {
        "surname": "Sun",
        "given_name": "Dan"
      },
      {
        "surname": "Wei",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Do deep neural networks contribute to multivariate time series anomaly detection?",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108945",
    "abstract": "Anomaly detection in time series is a complex task that has been widely studied. In recent years, the ability of unsupervised anomaly detection algorithms has received much attention. This trend has led researchers to compare only learning-based methods in their articles, abandoning some more conventional approaches. As a result, the community in this field has been encouraged to propose increasingly complex learning-based models mainly based on deep neural networks. To our knowledge, there are no comparative studies between conventional, machine learning-based and, deep neural network methods for the detection of anomalies in multivariate time series. In this work, we study the anomaly detection performance of sixteen conventional, machine learning-based and, deep neural network approaches on five real-world open datasets. By analyzing and comparing the performance of each of the sixteen methods, we show that no family of methods outperforms the others. Therefore, we encourage the community to reincorporate the three categories of methods in the anomaly detection in multivariate time series benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004253",
    "keywords": [
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Condensed matter physics",
      "Data mining",
      "Deep learning",
      "Engineering",
      "Field (mathematics)",
      "Machine learning",
      "Mathematics",
      "Multivariate statistics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Physics",
      "Pure mathematics",
      "Series (stratigraphy)",
      "Systems engineering",
      "Task (project management)",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Audibert",
        "given_name": "Julien"
      },
      {
        "surname": "Michiardi",
        "given_name": "Pietro"
      },
      {
        "surname": "Guyard",
        "given_name": "Frédéric"
      },
      {
        "surname": "Marti",
        "given_name": "Sébastien"
      },
      {
        "surname": "Zuluaga",
        "given_name": "Maria A."
      }
    ]
  },
  {
    "title": "An Open-Set Recognition and Few-Shot Learning Dataset for Audio Event Classification in Domestic Environments",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.10.019",
    "abstract": "The problem of training with a small set of positive samples is known as few-shot learning (FSL). It is widely known that traditional deep learning algorithms usually show very good performance when trained with large datasets. However, in many applications, it is not possible to obtain such a high number of samples. This paper deals with the application of FSL to the detection of specific and intentional acoustic events given by different types of sound alarms, such as door bells or fire alarms, using a limited number of samples. These sounds typically occur in domestic environments where many events corresponding to a wide variety of sound classes take place. Therefore, the detection of such alarms in a practical scenario can be considered an open-set recognition (OSR) problem. To address the lack of a dedicated public dataset for audio FSL, researchers usually make modifications on other available datasets. This paper is aimed at providing the audio recognition community with a carefully annotated dataset 1 for FSL in an OSR context comprised of 1360 clips from 34 classes divided into pattern sounds and unwanted sounds. To facilitate and promote research on this area, results with state-of-the-art baseline systems based on transfer learning are also presented.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003142",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "CLIPS",
      "Chemistry",
      "Computer science",
      "Context (archaeology)",
      "Event (particle physics)",
      "Machine learning",
      "Organic chemistry",
      "Paleontology",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Set (abstract data type)",
      "Shot (pellet)",
      "Speech recognition",
      "Transfer of learning",
      "Variety (cybernetics)"
    ],
    "authors": [
      {
        "surname": "Naranjo-Alcazar",
        "given_name": "Javier"
      },
      {
        "surname": "Perez-Castanos",
        "given_name": "Sergi"
      },
      {
        "surname": "Zuccarello",
        "given_name": "Pedro"
      },
      {
        "surname": "Torres",
        "given_name": "Ana M."
      },
      {
        "surname": "Lopez",
        "given_name": "Jose J."
      },
      {
        "surname": "Ferri",
        "given_name": "Francesc J."
      },
      {
        "surname": "Cobos",
        "given_name": "Maximo"
      }
    ]
  },
  {
    "title": "Defending malware detection models against evasion based adversarial attacks",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.10.010",
    "abstract": "The last decade has witnessed a massive malware boom in the Android ecosystem. Literature suggests that artificial intelligence/machine learning based malware detection models can potentially solve this problem. But, these detection models are often vulnerable to adversarial samples developed by malware designers. Therefore, we validate the adversarial robustness and evasion resistance of different malware detection models developed using machine learning in this work. We first designed a neural network agent (MalDQN) based on deep reinforcement learning that adds noise via perturbations to the malware applications and converts them into adversarial malware applications. Malware designers can also generate these samples and use them to perform evasion attacks and fool the malware detection models. The proposed MalDQN agent achieved an average 98 % fooling rate against twenty distinct malware detection models based on a variety of classification algorithms (standard, ensemble, and deep neural network) and two different features (android permission and intent). The MalDQN evasion attack reduced the average accuracy from 86.18 % to 55.85 % in the twenty malware detection models mentioned above. Later, we also developed defensive measures to counter such evasion attacks. Our experimental results show that the proposed defensive strategies considerably improve the capability of different malware detection models to detect adversarial applications and build resistance against them.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003026",
    "keywords": [
      "Adversarial machine learning",
      "Adversarial system",
      "Android malware",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Cryptovirology",
      "Deep learning",
      "Evasion (ethics)",
      "Gene",
      "Immune system",
      "Immunology",
      "Machine learning",
      "Malware",
      "Reinforcement learning",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Rathore",
        "given_name": "Hemant"
      },
      {
        "surname": "Sasan",
        "given_name": "Animesh"
      },
      {
        "surname": "Sahay",
        "given_name": "Sanjay K."
      },
      {
        "surname": "Sewak",
        "given_name": "Mohit"
      }
    ]
  },
  {
    "title": "Score-Oriented Loss (SOL) functions",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108913",
    "abstract": "Loss functions engineering and the assessment of prediction performances are two crucial and intertwined aspects of supervised machine learning. This paper focuses on binary classification to introduce a class of loss functions that are defined on probabilistic confusion matrices and that allow an automatic and a priori maximization of the skill scores. These loss functions are tested in various classification experiments, which show that the probability distribution function associated with the confusion matrices significantly impacts the outcome of the score maximization process, and that the proposed functions are competitive with other state-of-the-art probabilistic losses.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003946",
    "keywords": [
      "A priori and a posteriori",
      "Arithmetic",
      "Artificial intelligence",
      "Binary classification",
      "Binary number",
      "Biology",
      "Class (philosophy)",
      "Computer science",
      "Confusion",
      "Confusion matrix",
      "Epistemology",
      "Evolutionary biology",
      "Expectation–maximization algorithm",
      "Function (biology)",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Maximum likelihood",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Probabilistic logic",
      "Process (computing)",
      "Psychoanalysis",
      "Psychology",
      "Statistics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Marchetti",
        "given_name": "F."
      },
      {
        "surname": "Guastavino",
        "given_name": "S."
      },
      {
        "surname": "Piana",
        "given_name": "M."
      },
      {
        "surname": "Campi",
        "given_name": "C."
      }
    ]
  },
  {
    "title": "ProFeat: Unsupervised image clustering via progressive feature refinement",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.10.029",
    "abstract": "Unsupervised image clustering is a chicken-and-egg problem that involves representation learning and clustering. To resolve the inter-dependency between them, many approaches that iteratively perform the two tasks have been proposed, but their accuracy is limited due to inaccurate intermediate representations and clusters. To overcome this, this paper proposes ProFeat, a novel iterative approach to unsupervised image clustering based on progressive feature refinement. To learn discriminative features for clustering while avoiding adversarial influence from inaccurate intermediate clusters, ProFeat rigorously divides representation learning and clustering by modeling a neural network for clustering as a composition of an embedding and a clustering function and introducing an auxiliary embedding function. ProFeat progressively refines representations using confident samples from intermediate clusters using an extended contrastive loss. This paper also proposes ensemble-based feature refinement for more robust clustering. Our experiments demonstrate that ProFeat achieves superior results compared to previous methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003282",
    "keywords": [
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Conceptual clustering",
      "Correlation clustering",
      "Discriminative model",
      "Embedding",
      "Feature (linguistics)",
      "Feature learning",
      "Law",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Jeonghoon"
      },
      {
        "surname": "Im",
        "given_name": "Sunghoon"
      },
      {
        "surname": "Cho",
        "given_name": "Sunghyun"
      }
    ]
  },
  {
    "title": "Motion-blurred image restoration framework based on parameter estimation and fuzzy radial basis function neural networks",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108983",
    "abstract": "The restoration of motion-blurred images has always been a complex problem in image restoration. The current single blurred image algorithm cannot very well solve the estimation error of motion blur parameters. A comprehensive motion-blurred image restoration framework is proposed, which includes motion-blurred data generation, blur parameter estimation, and image quality assessment of restored images. First, we designed and used four image data sets with different degrees of blurring. We innovatively propose a blur parameter estimation algorithm based on the particle swarm optimization (B-PSO) algorithm. The Naturalness Image Quality Evaluator (NIQE) is used as the fitness function of the PSO algorithm. The framework also introduces a polynomial-based radial basis function neural network (P-RBFNN) as a new image quality assessment (IQA) method, with good image classification performance. Test results from public datasets show that the proposed framework can accurately estimate blur parameters. The peak signal-to-noise ratio (PSNR) reaches 29.976 dB, the structural similarity (SSIM) reaches 0.9044, and the classification rate is 96%. The proposed restoration framework produces the best image restoration results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004630",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Basis (linear algebra)",
      "Biology",
      "Computer science",
      "Computer vision",
      "Evolutionary biology",
      "Function (biology)",
      "Fuzzy logic",
      "Geometry",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Mathematics",
      "Motion (physics)",
      "Motion estimation",
      "Pattern recognition (psychology)",
      "Radial basis function",
      "Radial basis function network"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Shengmin"
      },
      {
        "surname": "Oh",
        "given_name": "Sung-Kwun"
      },
      {
        "surname": "Kim",
        "given_name": "Jin-Yul"
      },
      {
        "surname": "Fu",
        "given_name": "Zunwei"
      },
      {
        "surname": "Pedrycz",
        "given_name": "Witold"
      }
    ]
  },
  {
    "title": "Face-mask-aware Facial Expression Recognition based on Face Parsing and Vision Transformer",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.11.004",
    "abstract": "As wearing face masks is becoming an embedded practice due to the COVID-19 pandemic, facial expression recognition (FER) that takes face masks into account is now a problem that needs to be solved. In this paper, we propose a face parsing and vision Transformer-based method to improve the accuracy of face-mask-aware FER. First, in order to improve the precision of distinguishing the unobstructed facial region as well as those parts of the face covered by a mask, we re-train a face-mask-aware face parsing model, based on the existing face parsing dataset automatically relabeled with a face mask and pixel label. Second, we propose a vision Transformer with a cross attention mechanism-based FER classifier, capable of taking both occluded and non-occluded facial regions into account and reweigh these two parts automatically to get the best facial expression recognition performance. The proposed method outperforms existing state-of-the-art face-mask-aware FER methods, as well as other occlusion-aware FER methods, on two datasets that contain three kinds of emotions (M-LFW-FER and M-KDDI-FER datasets) and two datasets that contain seven kinds of emotions (M-FER-2013 and M-CK+ datasets).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003312",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Face (sociological concept)",
      "Facial expression",
      "Facial recognition system",
      "Parsing",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Social science",
      "Sociology",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Bo"
      },
      {
        "surname": "Wu",
        "given_name": "Jianming"
      },
      {
        "surname": "Ikeda",
        "given_name": "Kazushi"
      },
      {
        "surname": "Hattori",
        "given_name": "Gen"
      },
      {
        "surname": "Sugano",
        "given_name": "Masaru"
      },
      {
        "surname": "Iwasawa",
        "given_name": "Yusuke"
      },
      {
        "surname": "Matsuo",
        "given_name": "Yutaka"
      }
    ]
  },
  {
    "title": "Non-rigid point set registration based on local neighborhood information support",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108952",
    "abstract": "Non-rigid point set registration is a crucial task and an unsolved problem in the field of computer vision. One commonly used method for solving the problem is based on the Gaussian mixture model (GMM). In this method, the point set registration is formalized as a probability density estimation problem. Most GMM-based methods achieve registration by maintaining global and local structures of points. However, the previous methods did not filter the neighborhood information in the local structure, and the quality of local neighborhood information directly affects the accuracy of registration. Therefore, extracting effective local neighborhood information is still a challenge. We propose a novel point set registration method based on GMM by extracting local neighborhood information. The two point sets X and Y are regarded as the centroids of GMM and data points produced by GMM, respectively. Our method computes initial correspondences by comparing the feature descriptors of point sets, and the initial correspondences are updated by considering the neighborhood information. Our method then uses the Expectation–Maximization method to solve the GMM. In the experimental results, the efficiency and advantages of our method relative to the current methods are verified by applying five commonly used datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004320",
    "keywords": [
      "Artificial intelligence",
      "Centroid",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Filter (signal processing)",
      "Geometry",
      "Linguistics",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Mixture model",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point (geometry)",
      "Point set registration",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Chuanju"
      },
      {
        "surname": "Niu",
        "given_name": "Dongmei"
      },
      {
        "surname": "Wang",
        "given_name": "Peng"
      },
      {
        "surname": "Zhao",
        "given_name": "Xiuyang"
      },
      {
        "surname": "Yang",
        "given_name": "Bo"
      },
      {
        "surname": "Zhang",
        "given_name": "Caiming"
      }
    ]
  },
  {
    "title": "Digital geometry on a cubic stair-case mesh",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.11.003",
    "abstract": "In this paper, we investigate digital geometry on the rhombille tiling, D(6,3,6,3), that is the dual of the semi-regular tiling called hexadeltille T(6,3,6,3) tiling and also known as trihexagonal tiling. In fact, this tiling can be seen as an oblique mesh of the cubic grid giving practical importance to this specific grid both in image processing and graphics. The properties of the coordinate systems used to address the tiles are playing crucial roles in the simplicity of various algorithms and mathematical formulae of digital geometry that allow to work on the grid in image processing, image analysis and computer graphics, thus we present a symmetric coordinate system. This coordinate system has a strong relation to topological/combinatorial coordinate system of the cubic grid. It is an interesting fact that greedy shortest path algorithm may not be used on this grid, despite to this, we present algorithm to provide a minimal-length path between each pair of tiles, where paths are defined as sequences of neighbor tiles (those are considered to be neighbors which share a side). We also prove closed formula for computing the digital, i.e., path-based distance, the length (the number of steps) of a/the shortest path(s). Some example pictures on this grid are also presented, as well as its possible application as pixel geometry for color images and videos on the hexagonal grid.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003336",
    "keywords": [
      "Combinatorics",
      "Computational science",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Digital geometry",
      "Digital image",
      "Geometry",
      "Image (mathematics)",
      "Image processing",
      "Mathematics",
      "Polygon mesh",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Nagy",
        "given_name": "Benedek"
      },
      {
        "surname": "Saadat",
        "given_name": "MohammadReza"
      }
    ]
  },
  {
    "title": "Learnable dynamic margin in deep metric learning",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108961",
    "abstract": "With the deepening of deep neural network research, deep metric learning has been further developed and achieved good results in many computer vision tasks. Deep metric learning trains the deep neural network by designing appropriate loss functions, and the deep neural network projects the training samples into an embedding space, where similar samples are very close, while dissimilar samples are far away. In the past two years, the proxy-based loss achieves remarkable improvements, boosts the speed of convergence and is robust against noisy labels and outliers due to the introduction of proxies. In the previous proxy-based losses, fixed margins were used to achieve the goal of metric learning, but the intra-class variance of fine-grained images were not fully considered. In this paper, a new proxy-based loss is proposed, which aims to set a learnable margin for each class, so that the intra-class variance can be better maintained in the final embedding space. Moreover, we also add a loss between proxies, so as to improve the discrimination between classes and further maintain the intra-class distribution. Our method is evaluated on fine-grained image retrieval, person re-identification and remote sensing image retrieval common benchmarks. The standard network trained by our loss achieves state-of-the-art performance. Thus, the possibility of extending our method to different fields of pattern recognition is confirmed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004411",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Categorization",
      "Computer science",
      "Deep learning",
      "Economics",
      "Embedding",
      "Machine learning",
      "Margin (machine learning)",
      "Metric (unit)",
      "Operations management",
      "Outlier",
      "Pattern recognition (psychology)",
      "Proxy (statistics)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yifan"
      },
      {
        "surname": "Liu",
        "given_name": "Pingping"
      },
      {
        "surname": "Lang",
        "given_name": "Yijun"
      },
      {
        "surname": "Zhou",
        "given_name": "Qiuzhan"
      },
      {
        "surname": "Shan",
        "given_name": "Xue"
      }
    ]
  },
  {
    "title": "DNN self-embedding watermarking: Towards tampering detection and parameter recovery for deep neural network",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.10.013",
    "abstract": "In recent years, a large number of deep neural networks (DNN) models have been built and deployed, which need to be protected against malicious tampering by the adversary. This work is the first to propose a recoverable, self-embedding fragile watermarking scheme for DNN models to protect the model integrity. This scheme can not only identify and locate the tampered parameter blocks in the model, but can also recover the damaged parameters accurately. Detailedly, through exploiting the characteristics of the to-be-protected DNN model, the authentication data and recovery data are generated, and then the reference sharing mechanism is used to embed these data into the model without affecting its original functionality, which can realize the model parameter recovery under different tampering rates. Experimental results demonstrate that, the proposed scheme can achieve satisfactory performance of tampering detection and parameter recovery with low device requirements and can be effectively adaptable to a variety of existing DNNs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003063",
    "keywords": [
      "Adversary",
      "Artificial intelligence",
      "Artificial neural network",
      "Authentication (law)",
      "Computer science",
      "Computer security",
      "Deep neural networks",
      "Digital watermarking",
      "Embedding",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Scheme (mathematics)"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Gejian"
      },
      {
        "surname": "Qin",
        "given_name": "Chuan"
      },
      {
        "surname": "Yao",
        "given_name": "Heng"
      },
      {
        "surname": "Han",
        "given_name": "Yanfang"
      }
    ]
  },
  {
    "title": "Towards generalizable person re-identification with a bi-stream generative model",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108954",
    "abstract": "Generalizable person re-identification (re-ID) has attracted growing attention due to its powerful adaptation capability in the unseen data domain. However, existing solutions often neglect either crossing cameras (e.g., illumination and resolution differences) or pedestrian misalignments (e.g., viewpoint and pose discrepancies), which easily leads to poor generalization capability when adapted to the new domain. In this paper, we formulate these difficulties as: 1) Camera-Camera (CC) problem, which denotes the various human appearance changes caused by different cameras; 2) Camera-Person (CP) problem, which indicates the pedestrian misalignments caused by the same identity person under different camera viewpoints or changing pose. To solve the above issues, we propose a Bi-stream Generative Model (BGM) to learn the fine-grained representations fused with camera-invariant global feature and pedestrian-aligned local feature, which contains an encoding network and two stream decoding sub-network. Guided by original pedestrian images, one stream is employed to learn a camera-invariant global feature for the CC problem via filtering cross-camera interference factors. For the CP problem, another stream learns a pedestrian-aligned local feature for pedestrian alignment using information-complete densely semantically aligned part maps. Moreover, a part-weighted loss function is presented to reduce the influence of missing parts on pedestrian alignment. Extensive experiments demonstrate that our method outperforms the state-of-the-art methods on the large-scale generalizable re-ID benchmarks, involving domain generalization setting and cross-domain setting.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004344",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Domain (mathematical analysis)",
      "Engineering",
      "Feature (linguistics)",
      "Generalization",
      "Generative grammar",
      "Generative model",
      "Invariant (physics)",
      "Linguistics",
      "Mathematical analysis",
      "Mathematical physics",
      "Mathematics",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Pedestrian",
      "Philosophy",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Xin"
      },
      {
        "surname": "Liu",
        "given_name": "Wei"
      },
      {
        "surname": "Wang",
        "given_name": "Zheng"
      },
      {
        "surname": "Hu",
        "given_name": "Ruimin"
      },
      {
        "surname": "Tian",
        "given_name": "Qi"
      }
    ]
  },
  {
    "title": "Weighting and pruning based ensemble deep random vector functional link network for tabular data classification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108879",
    "abstract": "In this paper, we first integrate normalization to the Ensemble Deep Random Vector Functional Link network (edRVFL). This re-normalization step can help the network avoid divergence of the hidden features. Then, we propose novel variants of the edRVFL network. Weighted edRVFL (WedRVFL) uses weighting methods to give training samples different weights in different layers according to how the samples were classified confidently in the previous layer thereby increasing the ensemble’s diversity and accuracy. Furthermore, a pruning-based edRVFL (PedRVFL) has also been proposed. We prune some inferior neurons based on their importance for classification before generating the next hidden layer. Through this method, we ensure that the randomly generated inferior features will not propagate to deeper layers. Subsequently, the combination of weighting and pruning, called Weighting and Pruning based Ensemble Deep Random Vector Functional Link Network (WPedRVFL), is proposed. We compare their performances with other state-of-the-art classification methods on 24 tabular UCI classification datasets. The experimental results illustrate the superior performance of our proposed methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003600",
    "keywords": [
      "Agronomy",
      "Anthropology",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Data mining",
      "Machine learning",
      "Medicine",
      "Normalization (sociology)",
      "Pattern recognition (psychology)",
      "Pruning",
      "Radiology",
      "Random forest",
      "Sociology",
      "Support vector machine",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Qiushi"
      },
      {
        "surname": "Hu",
        "given_name": "Minghui"
      },
      {
        "surname": "Suganthan",
        "given_name": "Ponnuthurai Nagaratnam"
      },
      {
        "surname": "Katuwal",
        "given_name": "Rakesh"
      }
    ]
  },
  {
    "title": "Metric learning via perturbing hard-to-classify instances",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108928",
    "abstract": "Constraint selection is an effective means to alleviate the problem of a massive amount of constraints in metric learning. However, it is difficult to find and deal with all association constraints with the same hard-to-classify instance (i.e., an instance surrounded by dissimilar instances), negatively affecting metric learning algorithms. To address this problem, we propose a new metric learning algorithm from the perspective of selecting instances, Metric Learning via Perturbing of Hard-to-classify Instances (ML-PHI), which directly perturbs the hard-to-classify instances to reduce over-fitting for the hard-to-classify instances. ML-PHI perturbs hard-to-classify instances to be closer to similar instances while keeping the positions of the remaining instances as constant as possible. As a result, the negative impacts of hard-to-classify instances are effectively reduced. We have conducted extensive experiments on real data sets, and the results show that ML-PHI is effective and outperforms state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004095",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Constant (computer programming)",
      "Constraint (computer-aided design)",
      "Economics",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Programming language",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Xinyao"
      },
      {
        "surname": "Wei",
        "given_name": "Wei"
      },
      {
        "surname": "Liang",
        "given_name": "Jianqing"
      },
      {
        "surname": "Dang",
        "given_name": "Chuangyin"
      },
      {
        "surname": "Liang",
        "given_name": "Jiye"
      }
    ]
  },
  {
    "title": "Multiple geometry representations for 6D object pose estimation in occluded or truncated scenes",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108903",
    "abstract": "Deep learning-based 6D object pose estimation methods from a single RGBD image have recently received increasing attention because of their powerful representation learning capabilities. These methods, however, cannot handle severe occlusion and truncation. In this paper, we present a novel 6D object pose estimation method based on multiple geometry representations. Specifically, we introduce a network to fuse the appearance and geometry features extracted from input color and depth images. Then, we utilize these per-point fusion features to estimate keypoint offsets, edge vectors, and dense symmetry correspondences in the canonical coordinate system. Finally, a two-stage pose regression module is applied to compute the 6D pose of an object. Relative to the unitary 3D keypoint-based strategy, such combination of multiple geometry representations provides sufficient and diverse information, especially for occluded or truncated scenes. To show the robustness to occlusion and truncation of the proposed method, we conduct comparative experiments on the Occlusion LineMOD, Truncation LineMOD, and T-LESS datasets. Results reveal that the proposed method outperforms state-of-the-art techniques by a large margin.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003843",
    "keywords": [
      "3D pose estimation",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Gene",
      "Geometry",
      "Law",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Pose",
      "Representation (politics)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jichun"
      },
      {
        "surname": "Qiu",
        "given_name": "Lemiao"
      },
      {
        "surname": "Yi",
        "given_name": "Guodong"
      },
      {
        "surname": "Zhang",
        "given_name": "Shuyou"
      },
      {
        "surname": "Wang",
        "given_name": "Yang"
      }
    ]
  },
  {
    "title": "MonoPoly: A practical monocular 3D object detector",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108967",
    "abstract": "3D object detection plays a pivotal role in driver assistance systems and has practical requirements for small storage and fast inference. Monocular 3D detection alternatives abandon the complexity of LiDAR setup and pursues the effectiveness and efficiency of the vision scheme. In this work, we propose a set of anchor-free monocular 3D detectors called MonoPoly based on the keypoint paradigm. Specifically, we design a polynomial feature aggregation sampling module to extract multi-scale context features for auxiliary training and alleviate classification and localization misalignment through an attention-aware loss. Extensive experiments show that the proposed MonoPoly series achieves an excellent trade-off between performance and model size while maintaining real-time efficiency on KITTI and nuScenes datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004472",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Detector",
      "Economics",
      "Feature (linguistics)",
      "Geology",
      "Inference",
      "Lidar",
      "Linguistics",
      "Market economy",
      "Mathematical analysis",
      "Mathematics",
      "Monocular",
      "Monocular vision",
      "Monopoly",
      "Object (grammar)",
      "Object detection",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Remote sensing",
      "Scheme (mathematics)",
      "Set (abstract data type)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Guan",
        "given_name": "He"
      },
      {
        "surname": "Song",
        "given_name": "Chunfeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhaoxiang"
      },
      {
        "surname": "Tan",
        "given_name": "Tieniu"
      }
    ]
  },
  {
    "title": "Wrapper feature selection method based differential evolution and extreme learning machine for intrusion detection system",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108912",
    "abstract": "The intrusion detection system (IDS) has gained a rapid increase of interest due to its widely recognized potential in various security fields, however, it suffers from several challenges. Different network datasets have several redundant and irrelevant features that affect the decision of the IDS classifier. Therefore, it is essential to decrease these features to improve the system performance. In this paper, an efficient wrapper feature selection method is proposed for improving the performance and decreasing the processing time of the IDS. The proposed approach employs a differential evaluation algorithm to select the useful features whilst the extreme learning machine classifier is applied after feature selection to evaluate the selected features. Many experiments are performed using the full NSL-KDD dataset to evaluate the performance of the proposed method. The results prove that the proposed approach can efficiently reduce the features, increase the accuracy, reduce the false alarm rates, and improve the processing time of the IDS in comparison to other recent related works.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003934",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Classifier (UML)",
      "Computer science",
      "Constant false alarm rate",
      "Data mining",
      "Differential evolution",
      "Extreme learning machine",
      "False alarm",
      "Feature selection",
      "Intrusion detection system",
      "Machine learning",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Al-Yaseen",
        "given_name": "Wathiq Laftah"
      },
      {
        "surname": "Idrees",
        "given_name": "Ali Kadhum"
      },
      {
        "surname": "Almasoudy",
        "given_name": "Faezah Hamad"
      }
    ]
  },
  {
    "title": "Multi-feature sparse similar representation for person identification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108916",
    "abstract": "Person identification with a single feature (e.g., face recognition, speaker verification, person re-identification, etc.) has been studied extensively for many years, while few works focus on multi-feature person identification. Though promising performance has been achieved by only using the information of facial images, voice, or pedestrian appearance, it is still challenging to recognize a person with only a single feature in some situations (e.g., a person at a distance or occluded by other objects, and a partial person out of view). In this paper, we present a multi-feature sparse similar representation (MFSSR) method to effectively fuse face features, body features, and global image features for the task of person identification. In MFSSR, we designed a reconstructed deep spatial feature for representing the appearance of human body by using the spatial correlation coding of partial deep spatial features. Then we presented a multi-feature sparse similar representation model for jointly using different features, e.g., face, body, and the global image. Besides, considering that the coding coefficients associated with good samples but not outliers should be more similar among different features, we jointly represent different features by imposing a weighted ℓ 1 -norm distance regularization, instead of the conventional ℓ 2 -norm regularization, on the coefficients. Experimental results on several multi-feature person identification databases have clearly shown the superior performance of the proposed model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003971",
    "keywords": [
      "Artificial intelligence",
      "Coding (social sciences)",
      "Computer science",
      "Computer vision",
      "Face (sociological concept)",
      "Facial recognition system",
      "Feature (linguistics)",
      "Linguistics",
      "Mathematics",
      "Neural coding",
      "Outlier",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Regularization (linguistics)",
      "Social science",
      "Sociology",
      "Sparse approximation",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Meng"
      },
      {
        "surname": "Liao",
        "given_name": "Lei"
      },
      {
        "surname": "Ke",
        "given_name": "Kangyin"
      },
      {
        "surname": "Gao",
        "given_name": "Guangwei"
      }
    ]
  },
  {
    "title": "TRL: Transformer based refinement learning for hybrid-supervised semantic segmentation",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.11.015",
    "abstract": "This paper studies a new yet practical setting of semi-supervised semantic segmentation, i.e., hybrid-supervised semantic segmentation, where a small number of pixel-level (strong) annotations and a large number of image-level (weak) annotations are provided. It is a common practice to utilize pseudo labels to mitigate the issue of lacking strong annotations. However, most of the existing works focus on improving the model representation with unlabeled data, while ignoring the quality of pseudo labels, leading to poor segmentation performance. It is difficult to directly learn a model with limited images to produce high-quality pseudo labels. To address this problem, we propose a novel learning method, i.e., Transformer based Refinement Learning (TRL), which explores a learning process under the assistance of weak annotations and the supervision of strong annotations. TRL progressively refines heat maps from the poor qualities to the better ones to obtain satisfactory pseudo labels. Specifically, we propose a Dual-Cross Transformer Network (DCTN) to perform the refinement learning. DCTN extracts the features from both images and heat maps by a dual-stream network. The cross attentions inside DCTN hierarchically fuse the dual-stream features. The experiments on the PASCAL VOC and COCO datasets show that TRL outperforms the state-of-the-art methods for hybrid-supervised semantic segmentation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003427",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Deep learning",
      "Labeled data",
      "Machine learning",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Segmentation",
      "Supervised learning",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Cheng",
        "given_name": "Lin"
      },
      {
        "surname": "Fang",
        "given_name": "Pengfei"
      },
      {
        "surname": "Yan",
        "given_name": "Yan"
      },
      {
        "surname": "Lu",
        "given_name": "Yang"
      },
      {
        "surname": "Wang",
        "given_name": "Hanzi"
      }
    ]
  },
  {
    "title": "Knowledge Transfer and Crowdsourcing in Cyber-Physical-Social Systems",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.10.027",
    "abstract": "The rapid development of cyber-physical systems results in vast amount of heterogeneous data generated every day. To deal with unstructured data and maintain its security and privacy in smart manufacturing, it is necessary to merge social space with cyber-physical systems to develop cyber-physical-social systems. Crowdsourcing and knowledge transfer can be effective approaches to solve problems of manufacturing and product development, such as inviting hackers to break the security bridge to test the efficacy of the measures and handling enormous data generated in cyber-physical-social system. Crowdsourcing is a novel computing paradigm that leverages human effort to tackle computationally difficult issues, whereas knowledge transfer helps complete the new assignment based on quick access to the existing knowledge. As a result, an enhanced annotation for the work may be done at a low cost via suitable knowledge transfer. This paper introduces cyber-physical-social system and highlights the challenges and issues arising from massive data generated over the internet by various sources, and discusses how pattern recognition techniques can be used to identify anomalies or attacks. It also defines the terms knowledge transfer and crowdsourcing and explains how they can be effectively used to solve a problem in cyber-physical-social systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003269",
    "keywords": [
      "Bridge (graph theory)",
      "Computer science",
      "Computer security",
      "Crowdsourcing",
      "Cyber-physical system",
      "Data science",
      "Hacker",
      "Internal medicine",
      "Knowledge management",
      "Knowledge transfer",
      "Medicine",
      "Operating system",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Kumar",
        "given_name": "Gautam"
      },
      {
        "surname": "Narducci",
        "given_name": "Fabio"
      },
      {
        "surname": "Bakshi",
        "given_name": "Sambit"
      }
    ]
  },
  {
    "title": "Memory-efficient distribution-guided experience sampling for policy consolidation",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.10.024",
    "abstract": "Policy consolidation aims at learning new skills in sequential multi-tasks without forgetting acquired skills. Typically, the collected transitions for each task need to be stored for further exploiting. However, it is impractical to store all the transitions with limited memory as the number of tasks increases. To this end, we propose a novel distribution-guided experience sampling method that can efficiently select previously informative transitions and replay them to consolidate the learned policy. Specifically, we learn a distributional neural network to capture the interdependence among transitions, and evaluate the importance of each transition based on the interdependence. Meanwhile, the importance-based prioritized sampling method is presented to periodically replay transitions with higher importance to consolidate previous skills. Therefore, the proposed method can maintain good generalization ability of the policy in a memory-efficient way. Experimental results demonstrate the effectiveness of the proposed method on several benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003233",
    "keywords": [
      "Accounting",
      "Artificial intelligence",
      "Artificial neural network",
      "Business",
      "Cognitive psychology",
      "Computer science",
      "Computer vision",
      "Consolidation (business)",
      "Economics",
      "Filter (signal processing)",
      "Forgetting",
      "Generalization",
      "Hippocampus",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Memory consolidation",
      "Neuroscience",
      "Psychology",
      "Sampling (signal processing)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Fuxian"
      },
      {
        "surname": "Li",
        "given_name": "Weichao"
      },
      {
        "surname": "Lin",
        "given_name": "Yining"
      },
      {
        "surname": "Ji",
        "given_name": "Naye"
      },
      {
        "surname": "Li",
        "given_name": "Shijian"
      },
      {
        "surname": "Li",
        "given_name": "Xi"
      }
    ]
  },
  {
    "title": "Riemannian dynamic generalized space quantization learning",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108932",
    "abstract": "Many existing works represent signals by covariance matrices and then develop learning methods on the Riemannian symmetric positive-definite (SPD) manifold to deal with such data. However, they summarize each instance with a single covariance matrix, omitting some potential important information, such as the time evolution of the correlation in signals. In this paper, we represent each instance by a sequence of covariance matrices and develop a novel dynamic generalized learning Riemannian space quantization (DGLRSQ) method to deal with such data representations. The proposed DGLRSQ method incorporates short-term memory mechanism in generalized learning Riemannian space quantization (GLRSQ), which is an extension of Euclidean generalized learning vector quantization to deal with SPD matrix-valued data. The proposed method can capture the temporal evolution of the correlation in signals and thus provides better performance to its the counterpart – GLRSQ, which treats each instance as a signal covariance matrix. Empirical investigations on synthetic data and motor imagery EEG data show the superior performance of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004137",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Covariance",
      "Covariance matrix",
      "Euclidean space",
      "Learning vector quantization",
      "Mathematics",
      "Pure mathematics",
      "Quantization (signal processing)",
      "Riemannian manifold",
      "Statistics",
      "Vector quantization"
    ],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "MengLing"
      },
      {
        "surname": "Tang",
        "given_name": "Fengzhen"
      },
      {
        "surname": "Guo",
        "given_name": "Yinan"
      },
      {
        "surname": "Zhao",
        "given_name": "Xingang"
      }
    ]
  },
  {
    "title": "Fast Point Cloud Sampling Network",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.11.006",
    "abstract": "The increasing number of points in 3D point clouds has brought great challenges for subsequent algorithm efficiencies. Down-sampling algorithms are adopted to simplify the data and accelerate the computation. Except the well-known random sampling and farthest distance sampling, some recent works have tried to learn a sampling pattern according to the downstream task, which helps generate sampled points by fully-connected networks with fixed output point numbers. In this condition, a progress-net structure covering all resolutions sampling networks or multiple separate sampling networks for different resolutions are required, which is inconvenient. In this work, we propose a novel learning-based point cloud sampling framework, named Fast point cloud sampling network (FPN), which drives initial randomly sampled points to better positions instead of generating coordinates. FPN can be used to sample points clouds to any resolution once trained by changing the number of initial randomly sampled points. Results on point cloud reconstruction and recognition confirm that FPN can reach state-of-the-art performances with much higher sampling efficiency than most existing sampling methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S016786552200335X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Cloud computing",
      "Computation",
      "Computer science",
      "Computer vision",
      "Filter (signal processing)",
      "Geometry",
      "Mathematics",
      "Operating system",
      "Point (geometry)",
      "Point cloud",
      "Sample (material)",
      "Sampling (signal processing)"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Tianxin"
      },
      {
        "surname": "Chen",
        "given_name": "Jun"
      },
      {
        "surname": "Zhang",
        "given_name": "Jiangning"
      },
      {
        "surname": "Liu",
        "given_name": "Yong"
      },
      {
        "surname": "Liang",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "Heterogeneous spatio-temporal relation learning network for facial action unit detection",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.11.010",
    "abstract": "Properties such as temporal relation and action relation of facial Action Units (AUs) make AU detection different from general multi-label classification tasks. Therefore, how to capture the spatial and temporal co-occurrence of AUs becomes the key to improving detection accuracy. Although many works on the spatial relation of AUs have been proposed in recent years, very few works have explored the temporal relation of AUs. In this paper, we propose a Heterogeneous Spatio-Temporal Relation learning network (HSTR-Net) to capture the temporal and spatial relations of AUs. The co-occurrence knowledge graph module guides the network to model spatio-temporal relations by introducing prior relation information, and the spatio-temporal Transformer module adaptively captures spatio-temporal relations through spatio-temporal feature interaction. To further model the temporal relation between AUs, the self-attention mechanism to fuse AU features in the time dimension. Extensive experiments are conducted on two challenging datasets, BP4D and DISFA, and experimental results show that our proposed HSTR-Net achieves the comparable performance of the state-of-the-art in the field of AU detection. The code for our methods is available at https://github.com/saaaaalmon/HSTR-Net.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003403",
    "keywords": [
      "Artificial intelligence",
      "Code (set theory)",
      "Computer science",
      "Data mining",
      "Field (mathematics)",
      "Graph",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Pure mathematics",
      "Relation (database)",
      "Set (abstract data type)",
      "Spatial relation",
      "Temporal database",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Wenyu"
      },
      {
        "surname": "Shi",
        "given_name": "Shuze"
      },
      {
        "surname": "Dong",
        "given_name": "Yu"
      },
      {
        "surname": "An",
        "given_name": "Gaoyun"
      }
    ]
  },
  {
    "title": "Weighted 3D volume reconstruction from series of slice data using a modified Allen–Cahn equation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108914",
    "abstract": "In this study, we develop a fast and accurate computational method for a weighted three-dimensional (3D) volume reconstruction from a series of slice data using a phase-field model. The proposed method is based on a modified Allen–Cahn (AC) equation with a fidelity term. The algorithm automatically generates the necessary slices between the given slices by solving the governing equation. To reconstruct a 3D volume, we first set a source slice and target slice. Next, we set the source slice as the initial condition and the target slice as the fidelity function. Finally, we retain the numerical solutions during an evolution as intermediate slices between the source and target slices. There are two criteria for choosing the intermediate slice: One is based on the area of the symmetric difference between the phase-field solution and the target and the other is based on the change of the phase-field solution relative to the area of the target. We use the weighted average of the two criteria. To validate the efficiency and accuracy of the proposed numerical algorithm, several computational experiments are conducted. Computational test results confirm the superior performance of the proposed algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322003958",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Basis (linear algebra)",
      "Biology",
      "Chemistry",
      "Computer science",
      "Data set",
      "Electrical engineering",
      "Engineering",
      "Evolutionary biology",
      "Field (mathematics)",
      "Function (biology)",
      "Geometry",
      "Mathematical optimization",
      "Mathematics",
      "Organic chemistry",
      "Paleontology",
      "Phase (matter)",
      "Physics",
      "Programming language",
      "Pure mathematics",
      "Quantum mechanics",
      "Series (stratigraphy)",
      "Set (abstract data type)",
      "Volume (thermodynamics)",
      "Wafer"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yibao"
      },
      {
        "surname": "Song",
        "given_name": "Xin"
      },
      {
        "surname": "Kwak",
        "given_name": "Soobin"
      },
      {
        "surname": "Kim",
        "given_name": "Junseok"
      }
    ]
  },
  {
    "title": "JSL3d: Joint subspace learning with implicit structure supervision for 3D pose estimation",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108965",
    "abstract": "Estimating 3D human poses from a single image is an important task in computer graphics. Most model-based estimation methods represent the labeled/detected 2D poses and the projection of approximated 3D poses using vector representations of body joints. However, such lower-dimensional vector representations fail to maintain the spatial relations of original body joints, because the representations do not consider the inherent structure of body joints. In this paper, we propose JSL3d, a novel joint subspace learning approach with implicit structure supervision based on Sparse Representation (SR) model, capturing the latent spatial relations of 2D body joints by an end-to-end autoencoder network. JSL3djointly combines the learned latent spatial relations and 2D joints as inputs for the standard SR inference frame. The optimization is simultaneously processed via geometric priors in both latent and original feature spaces. We have evaluated JSL3dusing four large-scale and well-recognized benchmarks, including Human3.6M, HumanEva-I, CMU MoCap and MPII. The experiment results demonstrate the effectiveness of JSL3d.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004459",
    "keywords": [
      "Algorithm",
      "Architectural engineering",
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Bayesian probability",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Feature (linguistics)",
      "Frame (networking)",
      "Inference",
      "Joint (building)",
      "Law",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Pose",
      "Prior probability",
      "Projection (relational algebra)",
      "Representation (politics)",
      "Subspace topology",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Mengxi"
      },
      {
        "surname": "Zhou",
        "given_name": "Shihao"
      },
      {
        "surname": "Li",
        "given_name": "Cuihua"
      },
      {
        "surname": "Lei",
        "given_name": "Yunqi"
      }
    ]
  },
  {
    "title": "Concordance between facial micro-expressions and physiological signals under emotion elicitation",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.11.001",
    "abstract": "Various modalities have been leveraged for affective computing, alone or combined, such as facial expressions, speech intonations, peripheral physiological signals, and brain activities. Previous studies have shown that the multimodal fusion of affective data usually improves performance. However, the internal interactive mechanism among different modalities is rarely studied. In this paper, we investigated the concordance between facial micro-expressions and physiological signals under high arousal emotion elicitation with strict synchronization. By linking the onset of micro-expressions with physiological signals, a series of epoch durations were created to cover the potential reaction delay that may vary with different physiological signals. The experimental results show a significant correlation between the appearance of micro-expressions and time-domain features of heart rate variability, but not respiration or electrodermal activity related features. These findings indirectly verify the feasibility and reliability of micro-expression as a measure for non-contact genuine emotion recognition and would be beneficial for the fusion of micro-expression and physiological signals for more robust affective computing and their applications in public security and mental health.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003300",
    "keywords": [
      "Artificial intelligence",
      "Bioinformatics",
      "Biology",
      "Cognitive psychology",
      "Communication",
      "Computer science",
      "Concordance",
      "Facial expression",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Psychology",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Zou",
        "given_name": "Bochao"
      },
      {
        "surname": "Wang",
        "given_name": "Yingxue"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaolong"
      },
      {
        "surname": "Lyu",
        "given_name": "Xiangwen"
      },
      {
        "surname": "Ma",
        "given_name": "Huimin"
      }
    ]
  },
  {
    "title": "An objective method for pedestrian occlusion level classification",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.10.028",
    "abstract": "Pedestrian detection is among the most safety-critical features of driver assistance systems for autonomous vehicles. One of the most complex detection challenges is that of partial occlusion, where a target object is only partially available to the sensor due to obstruction by another foreground object. A number of current pedestrian detection benchmarks provide annotation for partial occlusion to assess algorithm performance in these scenarios, however each benchmark varies greatly in their definition of the occurrence and severity of occlusion. In addition, current occlusion level annotation methods contain a high degree of subjectivity by the human annotator. This can lead to inaccurate or inconsistent reporting of an algorithm’s detection performance for partially occluded pedestrians, depending on which benchmark is used. This research presents a novel, objective method for pedestrian occlusion level classification for ground truth annotation. Occlusion level classification is achieved through the identification of visible pedestrian keypoints and through the use of a novel, effective method of 2D body surface area estimation. Experimental results demonstrate that the proposed method reflects the pixel-wise occlusion level of pedestrians in images and is effective for all forms of occlusion, including challenging edge cases such as self-occlusion, truncation and inter-occluding pedestrians.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003270",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cardiology",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Enhanced Data Rates for GSM Evolution",
      "Geodesy",
      "Geography",
      "Medicine",
      "Occlusion",
      "Pattern recognition (psychology)",
      "Pedestrian",
      "Pedestrian detection",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Gilroy",
        "given_name": "Shane"
      },
      {
        "surname": "Glavin",
        "given_name": "Martin"
      },
      {
        "surname": "Jones",
        "given_name": "Edward"
      },
      {
        "surname": "Mullins",
        "given_name": "Darragh"
      }
    ]
  },
  {
    "title": "ℓ p -Norm Support Vector Data Description",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108930",
    "abstract": "The support vector data description (SVDD) approach serves as a de facto standard for one-class classification where the learning task entails inferring the smallest hyper-sphere to enclose target objects while linearly penalising the errors/slacks via an ℓ 1 -norm penalty term. In this study, we generalise this modelling formalism to a general ℓ p -norm ( p ≥ 1 ) penalty function on slacks. By virtue of an ℓ p -norm function, in the primal space, the proposed approach enables formulating a non-linear cost for slacks. From a dual problem perspective, the proposed method introduces a dual norm into the objective function, thus, proving a controlling mechanism to tune into the intrinsic sparsity/uniformity of the problem for enhanced descriptive capability. A theoretical analysis based on Rademacher complexities characterises the generalisation performance of the proposed approach while the experimental results on several datasets confirm the merits of the proposed method compared to other alternatives.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004113",
    "keywords": [
      "Algebra over a field",
      "Artificial intelligence",
      "Computer science",
      "Law",
      "Mathematics",
      "Norm (philosophy)",
      "Political science",
      "Pure mathematics"
    ],
    "authors": [
      {
        "surname": "Rahimzadeh Arashloo",
        "given_name": "Shervin"
      }
    ]
  },
  {
    "title": "A closed-form solution for conditional multidimensional scaling",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.11.007",
    "abstract": "Conditional multidimensional scaling seeks for a low-dimensional configuration from pairwise dissimilarities, in the presence of other known features. This method enables a simpler knowledge discovery process. Thus, it has broad application across different science and engineering domains because prior information of such known features is often available. The current solution of conditional multidimensional scaling is obtained via minimizing its conditional stress objective function with conditional SMACOF, an iterative optimization algorithm. However, iterative optimization is sensitive to starting values and can be time consuming for large problems. This paper proposes an alternative closed-form solution for conditional multidimensional scaling to address these deficits. The proposed method is based on multiple linear regression and eigendecomposition. The proposed algorithm does not necessarily replace conditional SMACOF. The former can be used to initialize the latter to improve its speed and accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003361",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Geometry",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Multidimensional scaling",
      "Pairwise comparison",
      "Scaling"
    ],
    "authors": [
      {
        "surname": "Bui",
        "given_name": "Anh Tuan"
      }
    ]
  },
  {
    "title": "Center Prediction Loss for Re-identification",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108949",
    "abstract": "The training loss function that enforces certain training sample distribution patterns plays a critical role in building a re-identification (ReID) system. Besides the basic requirement of discrimination, i.e., the features corresponding to different identities should not be mixed, additional intra-class distribution constraints, such as features from the same identities should be close to their centers, have been adopted to construct losses. Despite the advances of various new loss functions, it is still challenging to strike the balance between the need of reducing the intra-class variation and allowing certain distribution freedom. Traditional intra-class losses try to shrink samples of the same class into one point in the feature space and may easily drop their intra-class similarity structure. In this paper, we propose a new loss based on center predictivity, that is, a sample must be positioned in a location of the feature space such that from it we can roughly predict the location of the center of same-class samples. The prediction error is then regarded as a loss called Center Prediction Loss (CPL). Unlike most existing metric learning loss functions, CPL involves learnable parameters, i.e., the center predictor, which brings a remarkable change in the properties of the loss. In particular, it allows higher freedom in intra-class distributions. And the parameters in CPL will be discarded after training. Extensive experiments on various real-world ReID datasets show that the proposed loss can achieve superior performance and can also be complementary to existing losses.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004290",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Chemistry",
      "Chromatography",
      "Class (philosophy)",
      "Computer science",
      "Data mining",
      "Engineering",
      "Evolutionary biology",
      "Feature (linguistics)",
      "Feature vector",
      "Function (biology)",
      "Geometry",
      "Identification (biology)",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point (geometry)",
      "Sample (material)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Lu"
      },
      {
        "surname": "Wang",
        "given_name": "Yunlong"
      },
      {
        "surname": "Liu",
        "given_name": "Lingqiao"
      },
      {
        "surname": "Wang",
        "given_name": "Peng"
      },
      {
        "surname": "Zhang",
        "given_name": "Yanning"
      }
    ]
  },
  {
    "title": "Perceived emotions and AU combinations in ambiguous facial expressions",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.10.018",
    "abstract": "Ambiguous facial expressions are common and cannot be classified as specifically prototypical or compound in nature. We have very little understanding of the relationships connecting such expressions to perceived human emotions, such as in terms of compatibility or repulsion. This ignorance also exists with regards to their relationship to action units (AUs). This research employed network analysis to depict the network of perceived emotions and AUs in nearly 5,000 facial expressions obtained from the RAF-AU database, and calculated the centrality indices. We then used a matrix calculation to analyze the relationships between AU combinations and perceived emotions to better understand how people interpret the actions appearing on faces. The results showed that: (1) surprise was the most repulsive to other emotions in the emotion network, (2) AU25 and related open-mouth actions stay in the center of the AU network, (3) AU25 was weighted first in terms of contributing to perceived emotion, and (4) AU4 and AU25 were the most frequently presented in common ambiguous facial expressions. The results were not consistent with those of previous research, mainly due to differences in research methods and materials. The results imply that emotions perceived from ambiguous facial expressions cannot be predicted by core AUs of prototypical facial expressions. The implications and limitations of these conclusions are also discussed herein.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003130",
    "keywords": [
      "Artificial intelligence",
      "Cognitive psychology",
      "Communication",
      "Computer science",
      "Facial expression",
      "Mathematics",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Psychology",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Wen-Jing"
      },
      {
        "surname": "Ruan",
        "given_name": "Qian-Nan"
      },
      {
        "surname": "Fu",
        "given_name": "Xiaolan"
      },
      {
        "surname": "Sun",
        "given_name": "Yu-Qi"
      }
    ]
  },
  {
    "title": "Progressive downsampling and adaptive guidance networks for dynamic scene deblurring",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108988",
    "abstract": "The existing learning-based dynamic scene deblurring methods have made good progress to some extent. However, these methods are usually based on multiscale strategy, which has the following shortcomings: (1) The bilinear downsampling operation will cause some loss of important high-frequency information, e.g., strong edges, which also further affects the network learning a better deblurring mapping. (2) Existing methods only use a single activation function, which limits the ability of the network model to fit data and causes the network performance to be easily saturated. Therefore, we propose an end-to-end progressive downsampling and adaptive guidance network called PDAG-Net for solving above problems. The proposed PDAG-Net can retain more strong edges and other high-frequency information of a blurry image so as to make the network learn a more effective deblurring mapping between the input and label images. In the proposed PDAG-Net, we implement a multiscale blended activation residual block called MSBA-ResBlock for learning the nonlinear characteristics of dynamic scene blur, which can also alleviate the performance saturation problem caused by a single activation function and improve multiscale feature extraction ability. Finally, we propose a multisupervision strategy for obtaining more robust and effective features and making the network possess more stable trainging and faster convergence. Extensive experimental results on a public dataset indicate that the proposed network outperforms the state-of-the-art image deblurring methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S003132032200468X",
    "keywords": [
      "Artificial intelligence",
      "Bilinear interpolation",
      "Computer science",
      "Computer vision",
      "Deblurring",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Upsampling"
    ],
    "authors": [
      {
        "surname": "Cui",
        "given_name": "Jinkai"
      },
      {
        "surname": "Li",
        "given_name": "Weihong"
      },
      {
        "surname": "Guo",
        "given_name": "Wei"
      },
      {
        "surname": "Gong",
        "given_name": "Weiguo"
      }
    ]
  },
  {
    "title": "Local-to-Global Support Vector Machines (LGSVMs)",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108920",
    "abstract": "For supervised classification tasks that involve a large number of instances, we propose and study a new efficient tool, namely the Local-to-Global Support Vector Machine (LGSVM) method. Its background somehow lies in the framework of approximation theory and of local kernel-based models, such as the Partition of Unity (PU) method. Indeed, even if the latter needs to be accurately tailored for classification tasks, such as allowing the use of the cosine semi-metric for defining the patches, the LGSVM is a global method constructed by gluing together the local SVM contributions via compactly supported weights. When the number of instances grows, such a construction of a global classifier enables us to significantly reduce the usually high complexity cost of SVMs. This claim is supported by a theoretical analysis of the LGSVM and of its complexity as well as by extensive numerical experiments carried out by considering benchmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004010",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Classifier (UML)",
      "Combinatorics",
      "Computer science",
      "Economics",
      "Geodesy",
      "Geography",
      "Geometry",
      "Kernel (algebra)",
      "Kernel method",
      "Machine learning",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Partition (number theory)",
      "Pattern recognition (psychology)",
      "Support vector machine",
      "Trigonometric functions"
    ],
    "authors": [
      {
        "surname": "Marchetti",
        "given_name": "F."
      },
      {
        "surname": "Perracchione",
        "given_name": "E."
      }
    ]
  },
  {
    "title": "Table Structure Recognition and Form Parsing by End-to-End Object Detection and Relation Parsing",
    "journal": "Pattern Recognition",
    "year": "2022",
    "doi": "10.1016/j.patcog.2022.108946",
    "abstract": "The recognition of two-dimensional structure of tables and forms from document images is a challenge due to the complexity of document structures and the diversity of layouts. In this paper, we propose a graph neural network (GNN) based unified framework named Table Structure Recognition Network (TSRNet) to jointly detect and recognize the structures of various tables and forms. First, a multi-task fully convolutional network (FCN) is used to segment primitive regions such as text segments and ruling lines from document images, then a GNN is used to classify and group these primitive regions into page objects such as tables and cells. At last, the relationships between neighboring page objects are analyzed using another GNN based parsing module. The parameters of all the modules in the system can be trained end-to-end to optimize the overall performance. Experiments of table detection and structure recognition for modern documents on the POD 2017, cTDaR 2019 and PubTabNet datasets and template-free form parsing for historical documents on the NAF dataset show that the proposed method can handle various table/form structures and achieve superior performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0031320322004265",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Economics",
      "End-to-end principle",
      "Management",
      "Natural language processing",
      "Parsing",
      "Pattern recognition (psychology)",
      "Relation (database)",
      "Table (database)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xiao-Hui"
      },
      {
        "surname": "Yin",
        "given_name": "Fei"
      },
      {
        "surname": "Dai",
        "given_name": "He-Sen"
      },
      {
        "surname": "Liu",
        "given_name": "Cheng-Lin"
      }
    ]
  },
  {
    "title": "Events in crowded places: A smart service management",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.11.011",
    "abstract": "In this work we present eSERVANT (eventS in crowdEd places: a smaRt serVice mANagemenT), an ICT hardware and software infrastructure that allows the management of services tied to big events such as concerts, sport matches or fairs that take place in dedicated facilities. Among the main features of eSERVANT there is a vision-based dynamic routing that exploits computer vision techniques to detect people and track their movements within a facility. Thanks to an estimate of room occupancies and inbound/outbound flows, we propose an algorithm for dynamically selecting the best indoor route and avoid hazardous situations. At the same time, we perform user profiling finalized to promoting social networking through recommendation. To assess the usefulness of our infrastructure we established a test case over a six month experimentation and quantitatively evaluated user satisfaction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003397",
    "keywords": [
      "Business",
      "Computer network",
      "Computer science",
      "Computer security",
      "Engineering",
      "Exploit",
      "Marketing",
      "Mechanical engineering",
      "Multimedia",
      "Operating system",
      "Profiling (computer programming)",
      "Programming language",
      "Real-time computing",
      "Routing (electronic design automation)",
      "Service (business)",
      "Software",
      "Track (disk drive)",
      "Work (physics)"
    ],
    "authors": [
      {
        "surname": "Becattini",
        "given_name": "Federico"
      },
      {
        "surname": "Ferracani",
        "given_name": "Andrea"
      },
      {
        "surname": "Becchi",
        "given_name": "Giuseppe"
      },
      {
        "surname": "Del Bimbo",
        "given_name": "Alberto"
      }
    ]
  },
  {
    "title": "Gaussian distribution-based facial expression feature extraction network",
    "journal": "Pattern Recognition Letters",
    "year": "2022",
    "doi": "10.1016/j.patrec.2022.10.015",
    "abstract": "In this paper, we proposed a plug-and-play Gaussian-based Facial Expression Feature Extraction Network (GFE2N) for effective Facial Expression Recognition (FER). We considered facial expression image as a combination of the common information (facial attribute features) across different images and the unique information (facial movement features) that described expression movements. More specifically, GFE2N consisted of two crucial sub-networks: a Gaussian Sampling Network (GSN) and a Movement Feature Generation Network (MFGN). In particular, GSN first simulated latent facial attributes by a Gaussian distribution with neutral images. Then, MFGN captured the relationship between latent attributes and overall representations of expression images to generate the sample-specific variations of facial movement features. The wide ranges of experiments (including on in-the-wild datasets RAF-DB, AffectNet, FERPLUS and in-the-lab dataset CK+) illustrated that the proposed GFE2N can consistently achieve competitive results. GFE2N can also be fused as a subsidiary strategy module to assist improve the existing FER performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0167865522003087",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Computer vision",
      "Expression (computer science)",
      "Facial expression",
      "Facial expression recognition",
      "Facial recognition system",
      "Feature (linguistics)",
      "Feature extraction",
      "Gaussian",
      "Gaussian network model",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Sample (material)"
    ],
    "authors": [
      {
        "surname": "Wan",
        "given_name": "Fei"
      },
      {
        "surname": "Zhi",
        "given_name": "Ruicong"
      }
    ]
  }
]