[
  {
    "title": "A fuzzy incidence coloring to monitor traffic flow with minimal waiting time",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115664",
    "abstract": "In this paper we measure the Fuzzy incidence chromatic numbers of certain Fuzzy incidence graphs. We apply Fuzzy incidence coloring to real-world problems to monitor human loss during accidents by adhering to traffic flow laws and in particular, reducing traffic flow waiting times.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742101054X",
    "keywords": [
      "Artificial intelligence",
      "Chromatic scale",
      "Combinatorics",
      "Computer network",
      "Computer science",
      "Data mining",
      "Flow (mathematics)",
      "Fuzzy logic",
      "Geometry",
      "Incidence (geometry)",
      "Mathematics",
      "Measure (data warehouse)",
      "Real-time computing",
      "Traffic flow (computer networking)"
    ],
    "authors": [
      {
        "surname": "Yamuna",
        "given_name": "V."
      },
      {
        "surname": "Arun Prakash",
        "given_name": "K."
      }
    ]
  },
  {
    "title": "Towards better subtitles: A multilingual approach for punctuation restoration of speech transcripts",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115740",
    "abstract": "This paper proposes a flexible approach for punctuation prediction that can be used to produce state-of-the-art results in a multilingual scenario. We have performed experiments using transcripts of TED Talks from the IWSLT 2017 and IWSLT 2011 evaluation campaigns. Our experiments show that the recognition errors of the ASR output degrade the performance of our models, in line with related literature. Our monolingual models perform consistently in Human-edited transcripts of German, Dutch, Portuguese and Romanian, suggesting that commas may be more difficult to predict than periods, using pre-trained contextual models. We have trained a single multilingual model that predicts punctuation in multiple languages that achieves results comparable with the ones achieved by monolingual models, revealing evidence of the potential of using a single multilingual model to solve the task for multiple languages. Then, we argue that usage of current punctuation systems in the literature are implicitly dependent on correct segmentation of ASR outputs for they rely on positional information to solve the punctuation task. This is too big of a requirement for use in a real life application. Through several experiments, we show that our method to train and test models is more robust to different segmentation. These contributions are of particular importance in our multilingual pipeline, since they avoid training a different model for each of the involved languages, and they guarantee that the model will be more robust to incorrect segmentation of the ASR outputs in comparison with other methods in the literature. To the best of our knowledge, we report the first experiments using a single multilingual model for punctuation restoration in multiple languages.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011180",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Economics",
      "German",
      "Language model",
      "Linguistics",
      "Management",
      "Natural language processing",
      "Philosophy",
      "Pipeline (software)",
      "Programming language",
      "Punctuation",
      "Segmentation",
      "Speech recognition",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Guerreiro",
        "given_name": "Nuno Miguel"
      },
      {
        "surname": "Rei",
        "given_name": "Ricardo"
      },
      {
        "surname": "Batista",
        "given_name": "Fernando"
      }
    ]
  },
  {
    "title": "Experimental analysis of tardiness in preemptive single machine scheduling",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114947",
    "abstract": "This paper studies the scheduling of a finite set of jobs on a single machine. Jobs can be preempted. They are associated with respective release and due dates. In a more general case, each job has a priority weight. The objective is to minimize the total weighted tardiness. An optimal schedule providing the minimal total weighted tardiness can be found by our Boolean linear programming model, but it becomes computationally intractable as the problem size grows. Thus, we analyze the recently designed heuristic to minimize total weighted tardiness based on using the remaining available time and remaining processing time. It is a rapid online scheduling algorithm but the heuristic may return schedules whose tardiness deviates from an optimal value by 50% and more. We extend the heuristic into four branches with a purpose to improve it by excluding options of job selection uncertainty. Depending on the scheduling problem type, we render the metaheuristic to a single branch which is run after the main heuristic algorithm. This allows building schedules online. Our extensive computational study shows that our online metaheuristic returns high quality schedules.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003882",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Heuristic",
      "Job shop scheduling",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Operating system",
      "Schedule",
      "Scheduling (production processes)",
      "Single-machine scheduling",
      "Tardiness"
    ],
    "authors": [
      {
        "surname": "Goldengorin",
        "given_name": "Boris"
      },
      {
        "surname": "Romanuke",
        "given_name": "Vadim"
      }
    ]
  },
  {
    "title": "Adversarial machine learning in Network Intrusion Detection Systems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115782",
    "abstract": "Adversarial examples are inputs to a machine learning system intentionally crafted by an attacker to fool the model into producing an incorrect output. These examples have achieved a great deal of success in several domains such as image recognition, speech recognition and spam detection. In this paper, we study the nature of the adversarial problem in Network Intrusion Detection Systems (NIDS). We focus on the attack perspective, which includes techniques to generate adversarial examples capable of evading a variety of machine learning models. More specifically, we explore the use of evolutionary computation (particle swarm optimization and genetic algorithm) and deep learning (generative adversarial networks) as tools for adversarial example generation. To assess the performance of these algorithms in evading a NIDS, we apply them to two publicly available data sets, namely the NSL-KDD and UNSW-NB15, and we contrast them to a baseline perturbation method: Monte Carlo simulation. The results show that our adversarial example generation techniques cause high misclassification rates in eleven different machine learning models, along with a voting classifier. Our work highlights the vulnerability of machine learning based NIDS in the face of adversarial perturbation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011507",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Artificial neural network",
      "Classifier (UML)",
      "Computer science",
      "Intrusion detection system",
      "Machine learning"
    ],
    "authors": [
      {
        "surname": "Alhajjar",
        "given_name": "Elie"
      },
      {
        "surname": "Maxwell",
        "given_name": "Paul"
      },
      {
        "surname": "Bastian",
        "given_name": "Nathaniel"
      }
    ]
  },
  {
    "title": "An ontology engineering approach to measuring city education system performance",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115734",
    "abstract": "This paper addresses the problem of how to represent education system measurement definitions and the data used to derive them. ISO 37120 is standard for measuring city performance. It defines 100 indicators of which seven focus on Education. As cities adopt the standard and begin publishing their indicator values, citizens, city bureaucrats and politicians will be able to see how well their education systems perform relative to other cities. Hence, education systems will be subject to greater scrutiny and will have to provide evidence-based explanations for their performance. In order to enable the automated analysis of city performance, four problems must be solved: 1) how are indicator definitions represented? 2) how is the data used to derive an indicator value represented? 3) how is educational system specific knowledge represented, and 4) how is city specific educational system knowledge represented? In this paper we present an Education Ontology designed to represent ISO 37120 education indicators and the information delineated above. The ontology is designed to enable an intelligent agent to perform consistency and root cause analysis of city indicators. It can also be used by cities to publish their education indicators and data, using Semantic Web standards, on their Open Data portals.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011143",
    "keywords": [
      "Advertising",
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Consistency (knowledge bases)",
      "Data science",
      "Economics",
      "Epistemology",
      "Knowledge management",
      "Law",
      "Linked data",
      "Management",
      "Ontology",
      "Performance indicator",
      "Philosophy",
      "Political science",
      "Publication",
      "Scrutiny",
      "Semantic Web",
      "Subject (documents)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Fox",
        "given_name": "Mark S."
      }
    ]
  },
  {
    "title": "Elitist non-dominated sorting Harris hawks optimization: Framework and developments for multi-objective problems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115747",
    "abstract": "This paper proposed a novel multi-objective non-sorted Harris Hawks Optimizer (NSHHO) part of the recently developed Harris Hawks Optimizer (HHO) based on an elitist non dominated sorting mechanism. The same Harris Hawks Optimizer methodology was issued for converging towards optimum solutions in a multiple-objective criterion search space. For obtained well-distributed Pareto optimal front and their solution has better coverage. However, a non-dominated ranking with the crowding distance strategy is applied to HHO. To check the performance of NSHHO, a total of 46 case studies include 13 un-constrained, 11 constrained, and 22 real-world design multiple-objective highly nonlinear constraint problems. The obtained results of the proposed NSHHO are compared with NSGA-II, MOPSO and MOEA/D, quantitatively, and different performance metrics are compared qualitatively, which represents the advantage of the newly proposed NSHHO algorithm in solving the unconstrained, constrained, and real-world problems with different linear, nonlinear, continuous and discrete characteristics based Pareto front. We think the proposed NSHHO can be viral as an effective multi-objective optimizer within the field. The open-source software of NSHHO is publicly provided at https://codeocean.com/capsule/2034037/tree and https://aliasgharheidari.com/HHO.html.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011246",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Constraint (computer-aided design)",
      "Field (mathematics)",
      "Geometry",
      "Mathematical optimization",
      "Mathematics",
      "Multi-objective optimization",
      "Pareto principle",
      "Pure mathematics",
      "Ranking (information retrieval)",
      "Sorting"
    ],
    "authors": [
      {
        "surname": "Jangir",
        "given_name": "Pradeep"
      },
      {
        "surname": "Heidari",
        "given_name": "Ali Asghar"
      },
      {
        "surname": "Chen",
        "given_name": "Huiling"
      }
    ]
  },
  {
    "title": "Application of data-driven models to predictive maintenance: Bearing wear prediction at TATA steel",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115699",
    "abstract": "Industries that are in transition to Industry 4.0 often face challenges in applying data-driven methods to improve performance. While ample methods are available in literature, knowledge on how to select and apply them is scarce. This study aims to address this gap reported on the design and implementation of data-driven models for predictive maintenance at TATA Steel, Shotton. The objective of the project is to predict the wearing behaviour of the components in the steel production line for maintenance activity decision support. To achieve the predictive maintenance goal, the approach applied can be summarized as follows: 1. business understanding and data collection, 2. literature review, 3. data preparation and exploration, 4. modelling and result analysis and 5. conclusion and recommendation. The data-driven methods that were analysed and compared are: Partial Least Squares Regression (PLSR), Artificial Neu- ral Network (ANN) and Random Forest(RF). After cleaning and analysing the production line data, predictive maintenance with the current available data in TATA Steel, Shotton is best feasible with PLSR. The study further concludes that, predictive maintenance is likely to be feasible in similar industries that are in transition to industry 4.0 and have growing volumes of production data with varying quality and detail. However, as illustrated in this case study, careful understanding of the industrial process, thorough modeling and cleaning of the data as well as careful method selection and tuning are required. Moreover, the resulting model needs to be packaged in a user friendly way to find its way to the job floor.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010836",
    "keywords": [
      "Computer science",
      "Data mining",
      "Data quality",
      "Economics",
      "Engineering",
      "Epistemology",
      "Machine learning",
      "Macroeconomics",
      "Mechanical engineering",
      "Metric (unit)",
      "Operating system",
      "Operations management",
      "Partial least squares regression",
      "Philosophy",
      "Predictive maintenance",
      "Predictive modelling",
      "Process (computing)",
      "Production (economics)",
      "Production line",
      "Quality (philosophy)",
      "Reliability engineering"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "X."
      },
      {
        "surname": "Van Hillegersberg",
        "given_name": "Jos"
      },
      {
        "surname": "Topan",
        "given_name": "E."
      },
      {
        "surname": "Smith",
        "given_name": "S."
      },
      {
        "surname": "Roberts",
        "given_name": "M."
      }
    ]
  },
  {
    "title": "Building condition assessment using artificial neural network and structural equations",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115743",
    "abstract": "Building facilities condition assessment is considered a fundamental aspect of an effective decision-making maintenance management plan to fulfill service requirements. A noticeable dearth of studies is believed to have delivered condition assessment approaches for existing buildings; however, these approaches are still deemed premature, with some limitations in demand enhancement. This paper presents a novel physical condition assessment framework for existing educational buildings that contribute to the body of knowledge by offering a state-of-the-art approach incorporating an Artificial Neural Network (ANN) predictive model and a Structural Equation Model (SEM). The ANN predictive model aims to forecast the future condition-rating states for each facility component in various building spaces. Simultaneously, the SEM determines the proportionate weights of building facilities components. The primary objectives of this paper are to prioritize building components for maintenance purposes and record the potential effects of several parameters influencing the condition state of building components. These objectives can be achieved via four sequential modules: 1) scan to BIM module; 2) condition assessment prediction module; 3) proportionate weight determination module; and 4) entire space rating value module. Condition-monitoring data on six different buildings' internal components are analyzed to anticipate their future condition. The components carried out are: 1) wooden flooring tiles; 2) gypsum board ceiling tiles; 3) wooden doors; 4) wooden windows, 5) split air conditioner units; and 6) desktop computers. The overall coefficient of determination (R2) of the developed ANN models for the predicted six components conditions are 0.99, 0.99, 0.927, 0.88, 0.97, and 0.972, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011210",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Ceiling (cloud)",
      "Component (thermodynamics)",
      "Computer science",
      "Doors",
      "Engineering",
      "Operating system",
      "Physics",
      "Reliability engineering",
      "Structural engineering",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Gouda Mohamed",
        "given_name": "Ahmed"
      },
      {
        "surname": "Marzouk",
        "given_name": "Mohamed"
      }
    ]
  },
  {
    "title": "Visualizing high-dimensional industrial process based on deep reinforced discriminant features and a stacked supervised t-distributed stochastic neighbor embedding network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115389",
    "abstract": "Visual process monitoring is to monitor industrial processes by projecting the high-dimensional process data into the two-dimensional space, which provides powerful insight for industrial processes, and accelerates fault diagnosis. The challenge of visual process monitoring lies in how to project the complex process data into the two-dimensional plane and separate different classes as much as possible. In this paper, a new visual process monitoring method is proposed. First, a stacked reinforced discriminant auto-encoder (SRDAE) which consists of multiple reinforced discriminant auto-encoders (RDAEs) is proposed to extract discriminant features. In SRDAE, the useful features in the original data and the hidden output of the previous RDAE are combined together as the input of the latter RDAE, and the error of class label is added into the loss function of RDAE. Therefore, SRDAE can prevent the loss of useful information in the original data in the high layers and make the extracted features have the powerful ability to separate different classes. Furthermore, in order to extract the more informative discriminant features, minimal redundancy maximal relevance (MRMR) technology is utilized to select important neurons from all layers of the SRDAE as the final feature representation of the original data. Finally, a stacked supervised t-distributed stochastic neighbor embedding network is proposed to visualize the discriminant features for process monitoring. The effectiveness of the proposed method is validated on the Tennessee Eastman process, the experiments show that the proposed method can effectively separate different classes to achieve intuitive and efficient process monitoring.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008137",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Dimensionality reduction",
      "Discriminant",
      "Embedding",
      "Encoder",
      "Feature (linguistics)",
      "Linear discriminant analysis",
      "Linguistics",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Redundancy (engineering)"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Weipeng"
      },
      {
        "surname": "Yan",
        "given_name": "Xuefeng"
      }
    ]
  },
  {
    "title": "CovH2SD: A COVID-19 detection approach based on Harris Hawks Optimization and stacked deep learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115805",
    "abstract": "Starting from Wuhan in China at the end of 2019, coronavirus disease (COVID-19) has propagated fast all over the world, affecting the lives of billions of people and increasing the mortality rate worldwide in few months. The golden treatment against the invasive spread of COVID-19 is done by identifying and isolating the infected patients, and as a result, fast diagnosis of COVID-19 is a critical issue. The common laboratory test for confirming the infection of COVID-19 is Reverse Transcription Polymerase Chain Reaction (RT-PCR). However, these tests suffer from some problems in time, accuracy, and availability. Chest images have proven to be a powerful tool in the early detection of COVID-19. In the current study, a hybrid learning and optimization approach named CovH2SD is proposed for the COVID-19 detection from the Chest Computed Tomography (CT) images. CovH2SD uses deep learning and pre-trained models to extract the features from the CT images and learn from them. It uses Harris Hawks Optimization (HHO) algorithm to optimize the hyperparameters. Transfer learning is applied using nine pre-trained convolutional neural networks (i.e. ResNet50, ResNet101, VGG16, VGG19, Xception, MobileNetV1, MobileNetV2, DenseNet121, and DenseNet169). Fast Classification Stage (FCS) and Compact Stacking Stage (CSS) are suggested to stack the best models into a single one. Nine experiments are applied and results are reported based on the Loss, Accuracy, Precision, Recall, F1-Score, and Area Under Curve (AUC) performance metrics. The comparison between combinations is applied using the Weighted Sum Method (WSM). Six experiments report a WSM value above 96.5%. The top WSM and accuracy reported values are 99.31% and 99.33% respectively which are higher than the eleven compared state-of-the-art studies",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011738",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Coronavirus disease 2019 (COVID-19)",
      "Deep learning",
      "Disease",
      "Hyperparameter",
      "Infectious disease (medical specialty)",
      "Machine learning",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)",
      "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Balaha",
        "given_name": "Hossam Magdy"
      },
      {
        "surname": "El-Gendy",
        "given_name": "Eman M."
      },
      {
        "surname": "Saafan",
        "given_name": "Mahmoud M."
      }
    ]
  },
  {
    "title": "A simulated annealing-based recommender system for solving the tourist trip design problem",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115723",
    "abstract": "Tourism recommender system (TRS) experienced high growth over the last decades given its significant role in tourist satisfaction. Tourists are usually worried about planning their trips since it requires a considerable effort and it is a time consuming task. The main purpose of TRS is to select points of interest (POIs) that match users preferences and to suggest personalized daily sightseeing Designing such a suitable traveling tour is modeled as a Tourist Trip Design Problem (TTDP). Numerous variants of the problem are presented in the literature such as the Team Orienteering Problem (TOP) evoked in this paper. The aim is to maximize the total collected score of visited POIs subject to distance and time constraints. For solving the TTDP, we propose a metaheuristic based on two steps (i) grouping the POIs into clusters based on the start and end locations and (ii) solving the routing problem for each cluster to generate the optimal routes. The proposed approach is a combination between k-means and simulated annealing algorithms, namely, KSA. Besides, we develop StayPlan which is a TRS that integrates KSA algorithm to tackle the TTDP. StayPlan incorporates three main modules which are, user preferences, data retrieving & scoring and TTDP solving & optimization. Computational experiments compare the performance of our proposed algorithm with four state-of-the-art methods. The results show the efficiency and effectiveness of KSA algorithm in solving the benchmark suite from the literature. KSA algorithm reaches the best-known solutions for 100% of tested instances. Additionally, our method outperforms all the considered algorithms in terms of CPU time, it is quite fast with an average run time of 31.49 s. Ultimately, an illustrative example of the adapted scenario is applied to Tunis city as a real case.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011040",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer network",
      "Computer science",
      "Engineering",
      "Geodesy",
      "Geography",
      "History",
      "Law",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Operations research",
      "Orienteering",
      "Parallel computing",
      "Point of interest",
      "Political science",
      "Recommender system",
      "Routing (electronic design automation)",
      "Simulated annealing",
      "Suite",
      "TRIPS architecture",
      "Tourism",
      "Vehicle routing problem"
    ],
    "authors": [
      {
        "surname": "Tlili",
        "given_name": "Takwa"
      },
      {
        "surname": "Krichen",
        "given_name": "Saoussen"
      }
    ]
  },
  {
    "title": "Efficient algorithms for mining closed high utility itemsets in dynamic profit databases",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115741",
    "abstract": "The problem of discovering high-utility itemsets (HUIs) in transaction databases, which is an extension of Frequent Itemset Mining, is a commonly encountered mining task. Researchers have proposed algorithms to efficiently mine highly profitable itemsets in customer transaction databases, in which the unit profits of items are fixed. However, this assumption does not reflect the true nature of the utility measure of items in real-life transaction databases, which might vary over time. Moreover, since this important characteristic is ignored by all the current HUI mining algorithms, they are either not applicable to this type of database, or they generate inaccurate results. In addition, the HUI mining algorithms’ traditional limitation is that they produce a huge number of HUIs for users. In this paper, we define the problem of mining a lossless, concise and compact representation of HUIs, called closed HUIs (CHUIs), in dynamic unit profit databases. Based on newly defined of utility measure, a novel algorithm, called iEFIM-Closed, is introduced. This relies on this new utility measure, a novel compact database format to reduce the cost of database scans and increase the efficiency of the mining process. Experimental evaluations show that iEFIM-Closed significantly outperforms state-of-the-art algorithms for mining CHUIs on sparse databases with dynamic profit, and has competitive performance in dense databases in terms of runtime, the cost of database scans and scalability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011192",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Data mining",
      "Database",
      "Database transaction",
      "Economics",
      "Microeconomics",
      "Profit (economics)",
      "Scalability"
    ],
    "authors": [
      {
        "surname": "Nguyen",
        "given_name": "Trinh D.D."
      },
      {
        "surname": "Nguyen",
        "given_name": "Loan T.T."
      },
      {
        "surname": "Vu",
        "given_name": "Lung"
      },
      {
        "surname": "Vo",
        "given_name": "Bay"
      },
      {
        "surname": "Pedrycz",
        "given_name": "Witold"
      }
    ]
  },
  {
    "title": "Predicting stock market crisis via market indicators and mixed frequency investor sentiments",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115844",
    "abstract": "At present, integrating investor sentiment into the prediction of stock market crisis has attracted more and more attention. However, the existing researches only considered the impact of the market indicators and the micro investor sentiment on stock market, while ignored the impact of the macro one. Therefore, in this paper, we develop an early warning system for predicting stock market crisis via market indicators and mixed frequency investor sentiments. The proposed early warning system consists of five components, which includes the construction of mixed frequency investor sentiments that consider both macro and micro investor sentiments, the identification of stock market crisis, the determination of the forecast horizon using Ensemble Empirical Mode Decomposition (EEMD), the definition of the early warning signal, and the building of prediction model using artificial neural network (ANN). Lastly, we apply the developed warning system to China’s stock markets. Experimental results show that mixed frequency investor sentiments can improve the early warning ability in predicting the stock market crisis, and the ANN model has better performance than other methods including Support Vector Machine (SVM), Decision Tree (DT), Random Forest (RF), Gradient Boosting Decision Tree (GBDT), K-Nearest Neighbor (KNN), and Logistic Regression (LR).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421012057",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Decision tree",
      "Econometrics",
      "Economics",
      "Engineering",
      "Filter (signal processing)",
      "Financial crisis",
      "Gradient boosting",
      "Hilbert–Huang transform",
      "Horse",
      "Logistic regression",
      "Machine learning",
      "Macroeconomics",
      "Mechanical engineering",
      "Paleontology",
      "Random forest",
      "Stock (firearms)",
      "Stock market",
      "Support vector machine",
      "Telecommunications",
      "Warning system"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Shan"
      },
      {
        "surname": "Liu",
        "given_name": "Chenhui"
      },
      {
        "surname": "Chen",
        "given_name": "Zhensong"
      }
    ]
  },
  {
    "title": "A compact compound sinusoidal differential evolution algorithm for solving optimisation problems in memory-constrained environments",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115705",
    "abstract": "In this paper, a new compact algorithm is proposed. Two sinusoidal formulas are used to automatically adjust the crossover rate and the mutation scaling factor in the compact Differential Evolution (cDE) metaheuristic. The proposed algorithm, called Compound Sinusoidal cDE, CScDE, is compared to seven state-of-the-art compact algorithms on the well-known BBOB test-bed, the CEC-2014 test suite for continuous optimisation, as well as five real-world optimisation problems chosen from the CEC-2011 benchmarks. The CScDE algorithm outperformed its competitors for most problem categories and over most dimensions. It is also compared to some well-established population-based metaheuristics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010873",
    "keywords": [
      "Algorithm",
      "Archaeology",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Crossover",
      "Demography",
      "Differential evolution",
      "Evolutionary algorithm",
      "Gene",
      "History",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Mutation",
      "Population",
      "Regression analysis",
      "Sociology",
      "Suite",
      "Test case",
      "Test suite"
    ],
    "authors": [
      {
        "surname": "Khalfi",
        "given_name": "Souheila"
      },
      {
        "surname": "Draa",
        "given_name": "Amer"
      },
      {
        "surname": "Iacca",
        "given_name": "Giovanni"
      }
    ]
  },
  {
    "title": "Expert system dedicated to condition-based maintenance based on a knowledge graph approach: Application to an aeronautic system",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115767",
    "abstract": "Condition Based Maintenance (CBM) has become the focus of many research topics over the past decades. This is mostly related to the development of new machine learning algorithms and the ever increasing capacity to collect data allowing failures to be detected and the system’s remaining lifetime to be estimated while requiring few or no expert knowledge. However, current machine learning based CBM solutions have limitations. They require an extensive and relevant data set to train on and are performed at the component level, not system-wide. Conversely, Expert Systems (ES) do not have these restrictions but should be used on systems with available expert knowledge and are currently suffering from efficiency, scalability and applicability limits. In this paper, an ES solution for CBM based on an heterogeneous information network will be presented to address the efficiency, scalability and applicability issues of modern ES. An application to an aircraft system will be used as case study to illustrate the process and performance of this solution for anomaly detection and diagnostics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011398",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Component (thermodynamics)",
      "Computer science",
      "Data mining",
      "Database",
      "Engineering",
      "Expert system",
      "Focus (optics)",
      "Graph",
      "Machine learning",
      "Operating system",
      "Optics",
      "Physics",
      "Process (computing)",
      "Programming language",
      "Scalability",
      "Set (abstract data type)",
      "Systems engineering",
      "Theoretical computer science",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Sarazin",
        "given_name": "Alexandre"
      },
      {
        "surname": "Bascans",
        "given_name": "Jérémy"
      },
      {
        "surname": "Sciau",
        "given_name": "Jean-Baptiste"
      },
      {
        "surname": "Song",
        "given_name": "Jiefu"
      },
      {
        "surname": "Supiot",
        "given_name": "Bruno"
      },
      {
        "surname": "Montarnal",
        "given_name": "Aurélie"
      },
      {
        "surname": "Lorca",
        "given_name": "Xavier"
      },
      {
        "surname": "Truptil",
        "given_name": "Sébastien"
      }
    ]
  },
  {
    "title": "UIFDBC: Effective density based clustering to find clusters of arbitrary shapes without user input",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115746",
    "abstract": "Density-based clustering has the ability to detect arbitrary shaped clusters in any dataset. In recent years, several density peak clustering methods have been reported. Among these, a few need user input(s), but majority use cluster validity indices to provide the best results. In this paper, we propose a density-based user-input-free clustering method named UIFDBC, which is capable of detecting clusters of arbitrary shapes, without depending on any specific cluster validity index. The method is evaluated on 16 synthetic and 7 real-world datasets and compared with 8 recent density-based clustering methods. The results show our method is superior, in general, to its counterparts in terms of discovering arbitrary shaped clusters on tested datasets. The approach also has the ability to handle low-density instances in a special manner to minimize error propagation. Our method is available as an R package and can be downloaded by clicking the link https://sites.google.com/view/hussinchowdhury/software.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011234",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Data mining"
    ],
    "authors": [
      {
        "surname": "Chowdhury",
        "given_name": "Hussain Ahmed"
      },
      {
        "surname": "Bhattacharyya",
        "given_name": "Dhruba Kumar"
      },
      {
        "surname": "Kalita",
        "given_name": "Jugal Kumar"
      }
    ]
  },
  {
    "title": "Extended Hapicare: A telecare system with probabilistic diagnosis and self-adaptive treatment",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115749",
    "abstract": "The massive growth of the population with chronic diseases calls for a telecare system to enhance their quality of life and reduce their treatment costs. Most of the current solutions depend on reliable data, deterministic rules, or the similarity of patients, while studies have shown otherwise. To this end, in this paper, we have extended our previous work on the Hapicare framework to integrate probabilistic diagnosis and self-adaptive treatment. Our new framework enables sensors’ datastream analysis and online decision-making. Its ontology-based reasoning uses Systematized Nomenclature of Medicine - Clinical Terms (SNOMED-CT) ontology to add contextual information to the collected data. Moreover, probabilistic reasoning is applied for diagnosis and screening to manage the uncertainty and unreliability of data as well as the indeterministic medical rules. The treatment system is designed to be modifiable by the experts and automatically adaptable to patients’ needs. The probabilistic diagnosis performance has been evaluated based on two public datasets regarding symptoms and risk factors of two chronic diseases: chronic kidney disease and dermatologic disease. The results show that our solution outperforms a classical classifier specifically when more than 40% of the data are missing. The proposed framework is also validated using four scenarios. The evaluation results demonstrate the ability of the proposed framework to help patients and doctors diagnose and treat medical conditions and episodes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742101126X",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Decision support system",
      "Economic growth",
      "Economics",
      "Environmental health",
      "Epistemology",
      "Health care",
      "Linguistics",
      "Machine learning",
      "Medicine",
      "Ontology",
      "Philosophy",
      "Population",
      "Probabilistic logic",
      "SNOMED CT",
      "Telecare",
      "Telemedicine",
      "Terminology"
    ],
    "authors": [
      {
        "surname": "Kordestani",
        "given_name": "Hossain"
      },
      {
        "surname": "Mojarad",
        "given_name": "Roghayeh"
      },
      {
        "surname": "Chibani",
        "given_name": "Abdelghani"
      },
      {
        "surname": "Barkaoui",
        "given_name": "Kamel"
      },
      {
        "surname": "Amirat",
        "given_name": "Yacine"
      },
      {
        "surname": "Zahran",
        "given_name": "Wagdy"
      }
    ]
  },
  {
    "title": "Robust clustering with sparse corruption via ℓ 2 , 1 , ℓ 1 norm constraint and Laplacian regularization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115704",
    "abstract": "Clustering has been applied in machine learning, data mining and so on, and has received extensive attention. However, since some data has noise or outliers, these noise or outliers easily bring about the objective function with large errors. In this paper, a robust clustering model with ℓ 2 , 1 , ℓ 1 norm and Laplacian regularization (RCLR) is proposed, on which, sparse error matrix is introduced to express sparse noise, and ℓ 1 norm is introduced to alleviate the sparse noise. In addition, the ℓ 2 , 1 norm is also introduced to achieves space robust by virtue of its nice rotation invariance property. Therefore, our RCLR is insensitive to data noise and outliers. More importantly, the Laplacian regularization is introduced into the RCLR to improve the clustering accuracy. In order to solve the optimization objective of clustering problem, we propose an iterative updating algorithm, named alternating direction method of multipliers (ADMM), to update each optimization variable alternatively, and the convergence of the proposed algorithm is also proved in theory. Finally, experimental results on a total of eleven datasets of three types of datasets, elaborate the superiority of this method over six existing classical clustering methods. Three types of datasets include face images dataset, handwritten recognition dataset, and UCI dataset. In particular, our RCLR clustering approach has the best effect on face image dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010861",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Compressed sensing",
      "Computer science",
      "Data mining",
      "Graph",
      "Image (mathematics)",
      "Laplacian matrix",
      "Law",
      "Noise (video)",
      "Norm (philosophy)",
      "Optimization problem",
      "Outlier",
      "Pattern recognition (psychology)",
      "Political science",
      "Regularization (linguistics)",
      "Spectral clustering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Min"
      },
      {
        "surname": "Liu",
        "given_name": "Jinglei"
      }
    ]
  },
  {
    "title": "Dynamic time warp-based clustering: Application of machine learning algorithms to simulation input modelling",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115684",
    "abstract": "Effective input modelling in stochastic simulation is essential in driving and understanding underlying system behaviours. Current approaches to input modelling either consider all input data as a homogeneous data set, resulting in simulation models that ignore idiosyncratic systems characteristics, or alternatively, treat individual data sets independently, leading to more complex analysis. In this article we propose a novel approach based on exploratory machine learning techniques to generate representative system behaviours with just adequate scenario experiments by grouping input data into clusters. Dynamic time warping measures the similarity between input sources and silhouette indices are used to determine the optimal number of clusters. This approach provides more targeted analysis to characterize underlying systems behaviours driven by factors such as socio-economics, demographics or geography. Results from two simulation case studies demonstrated the effectiveness of the proposed approach, in that system output behaviours remain invariant based on several statistical tests.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010691",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Dynamic time warping",
      "Image (mathematics)",
      "Invariant (physics)",
      "Machine learning",
      "Mathematical physics",
      "Mathematics",
      "Programming language",
      "Set (abstract data type)",
      "Silhouette",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "James"
      },
      {
        "surname": "Johnstone",
        "given_name": "Michael"
      },
      {
        "surname": "Le",
        "given_name": "Vu"
      },
      {
        "surname": "Khan",
        "given_name": "Burhan"
      },
      {
        "surname": "Anwar Hosen",
        "given_name": "Mohammad"
      },
      {
        "surname": "Creighton",
        "given_name": "Doug"
      },
      {
        "surname": "Carney",
        "given_name": "Jessica"
      },
      {
        "surname": "Wilson",
        "given_name": "Andy"
      },
      {
        "surname": "Lynch",
        "given_name": "Michael"
      }
    ]
  },
  {
    "title": "Novel distributed load balancing algorithms in cloud storage",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115713",
    "abstract": "From last decade, there is a rapid expanded upon in the data in cyberspace. In order to manage them efficiently, distributed storage came into the world. Cloud storage is a type of distributed storage based on cloud computing technology. Cloud storage acts as a repository in which data stored, managed and made accessible to users. Largest generated application datasets can flexibly be stored or deleted in the cloud and from here end users access this data by using cloud storage services interface, without accessing any storage server in real. Cloud storage system is considered of hundreds of independent storage servers which are distributed geographically, and handle millions client requests concurrently. Some of the storage servers get huge clients requests and some servers remain under loaded. Due to this unequal distribution of load on storage servers degrades the performance of overall system and increases the response time. This work addresses issues regarding to efficient utilization of storage servers in cloud storage. Handling various challenges regarding to the load balancing in the cloud storage is the one of the main objectives of this research. Though analyzing the contribution of other authors in this area, in this work two distributed load balancing algorithm CDLB and DDLB are proposed by exploiting the different parameter of storage server. The first proposed algorithm considers the service rate and queue length as a main parameter of the server. The second proposed algorithm considers extra server parameter such as service time and deadline time of the client request. This work monitors various aspects which leverage the overall performance of cloud storage. Both the algorithms try to balance the load of storage servers as well as effectively utilize the server capabilities. From the simulation results, it can be concluded that proposed algorithms balance the load, efficiently utilize the server capabilities, reduce the response time and leverage the overall system performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010952",
    "keywords": [
      "Cloud computing",
      "Cloud storage",
      "Computer data storage",
      "Computer network",
      "Computer science",
      "Converged storage",
      "Database",
      "Distributed computing",
      "Distributed data store",
      "File server",
      "Geometry",
      "Grid",
      "Information repository",
      "Load balancing (electrical power)",
      "Mathematics",
      "Operating system",
      "Queue",
      "Response time",
      "Server"
    ],
    "authors": [
      {
        "surname": "Gupta",
        "given_name": "Yogesh"
      }
    ]
  },
  {
    "title": "Unsupervised multi-subepoch feature learning and hierarchical classification for EEG-based sleep staging",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115759",
    "abstract": "As the medium of developing brain–computer interface system, the recognition of EEG signals is complicated and difficult due to the complex nonstationary characteristics and the individual difference between subjects. In this paper, we investigate the EEG signal classification problem and propose a novel unsupervised multi-subepoch feature learning and hierarchical classification method for automatic sleep staging. First, we divide the EEG epoch into multiple signal subepochs, and each subepoch is mapped to amplitude axis and time axis respectively to obtain two kinds of feature information with amplitude–time dynamic characteristics. Then, the statistical classification features are extracted from the mapped feature information. Furthermore, we conduct unsupervised feature learning for consistent and specific classification features from the perspective of time series. Finally, according to the differences and similarities of EEG signals in different sleep stages, a hierarchical weighted support vector machine-based classification model (H-WSVM) is established, which can use different feature subsets at each classification level and different weighting parameters for unbalanced data samples. To select the optimal feature subset for detecting each sleep stage, we propose a novel evaluation criterion for feature classification ability based on rough set theory. Experimental results on the most commonly used dataset show that the proposed method has better sleep staging performance and can effectively promote the development and application of EEG sleep staging system.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011337",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Electroencephalography",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature vector",
      "Linguistics",
      "Machine learning",
      "Medicine",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Polysomnography",
      "Psychiatry",
      "Psychology",
      "Radiology",
      "Sleep Stages",
      "Support vector machine",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "An",
        "given_name": "Panfeng"
      },
      {
        "surname": "Yuan",
        "given_name": "Zhiyong"
      },
      {
        "surname": "Zhao",
        "given_name": "Jianhui"
      }
    ]
  },
  {
    "title": "Lane change strategy analysis and recognition for intelligent driving systems based on random forest",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115781",
    "abstract": "The development of intelligent connected technology provides a platform for multidimensional information interaction and makes the recognition of a specific lane change strategy become a reality. Intelligent driving systems cannot make human-like decisions by only recognizing left or right lane change behaviors; in fact, poor environmental cognitive competence is the main cause of accidents of intelligent driving systems during road tests. Therefore, the recognition of specific lane change strategies can provide support for safety performance improvement. This research aims to identify the different lane change strategies of a subject vehicle with a rear approaching vehicle in the target lane. For the purpose of acquiring different strategies, a total of 42 experienced drivers including 37 males and 5 females participated in the on-road vehicle experiments, and we divided lane change strategies into three categories: mandatory, yielding, and waiting for lane change. The lane change duration time, lane crossing time, and characteristic parameters including distance to lane line, steering wheel angle, relative distance, and relative speed under different lane change strategies were compared and analyzed. Then the random forest classifier was employed to construct a lane change strategy identification model. Results indicated that the maximum value of global recognition accuracy under three input parameters reached 88.78% when the time window was 1.2 s, which was higher than the maximum value of recognition accuracy under four input parameters, but the identification time was 0.1 s later than the model with four input parameters. In addition, the recognition performance of the random forest model, attention-bidirectional long short-term memory (Atten-BiLSTM) model and genetic algorithm-support vector machine (GA-SVM) model was compared. The results demonstrated that the global recognition accuracy and identification time of the proposed model were better than that of GA-SVM model under different input parameters, and the performance was comparable to the Atten-BiLSTM model. The findings provide the basis for the intelligent driving system to make more human-like decisions and improve safety performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011490",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Change detection",
      "Classifier (UML)",
      "Computer science",
      "Identification (biology)",
      "Pattern recognition (psychology)",
      "Random forest",
      "Simulation"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Qinyu"
      },
      {
        "surname": "Wang",
        "given_name": "Chang"
      },
      {
        "surname": "Fu",
        "given_name": "Rui"
      },
      {
        "surname": "Guo",
        "given_name": "Yingshi"
      },
      {
        "surname": "Yuan",
        "given_name": "Wei"
      },
      {
        "surname": "Li",
        "given_name": "Zhen"
      }
    ]
  },
  {
    "title": "A dynamic predictor selection algorithm for predicting stock market movement",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115836",
    "abstract": "Although training a deep network with financial time series is not hard, the important issue is, how much the prediction for the truly new data can be trusted with a trained network. In this study, we propose a dynamic predictor selection algorithm (DPSA) that dynamically evaluates and selects the prediction model (predictor) for stock daily movement trend prediction. We first build an initial set of potential candidate predictors based on the convolutional long short-term memory networks (ConvLSTMs) by using different values of parameters. To evaluate the candidate predictors, we propose a kernel time-weighted fuzzy c-means clustering algorithm (KTFCM), which improves the kernel FCM algorithm (KFCM), to organize the historical samples according to their relevance to the target sample, which makes the historical samples that are closely related to the target sample have more influence on the predictors. Then, we use the well-organized historical samples to evaluate the candidate predictors. The predictor that yields the best accuracy is selected to predict the target sample. The proposed DPSA algorithm takes less than one minute in total for training the networks, evaluating and selecting the predictors, and performing prediction, which greatly shortens the time of the deep learning prediction. We perform the comparative experiments for the proposed DPSA algorithm and seven popular methods. These experiments test a large real-life financial time series data of various stock markets. The experiment results show that DPSA achieves the best accuracy and the highest return compared to the seven other popular methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011982",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Chemistry",
      "Chromatography",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Horse",
      "Machine learning",
      "Paleontology",
      "Sample (material)",
      "Stock market",
      "Stock market prediction"
    ],
    "authors": [
      {
        "surname": "Dong",
        "given_name": "Shuting"
      },
      {
        "surname": "Wang",
        "given_name": "Jianxin"
      },
      {
        "surname": "Luo",
        "given_name": "Hongze"
      },
      {
        "surname": "Wang",
        "given_name": "Haodong"
      },
      {
        "surname": "Wu",
        "given_name": "Fang-Xiang"
      }
    ]
  },
  {
    "title": "A multi-objective formulation of maximal covering location problem with customers’ preferences: Exploring Pareto optimality-based solutions",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115830",
    "abstract": "The maximal covering location problem (MCLP) is a well-known combinatorial optimization problem with several applications in emergency and military services as well as in public services. Traditionally, MCLP is a single objective problem where the objective is to maximize the sum of the demands of customers which are served by a fixed number of open facilities. In this article, a multi-objective MCLP is proposed where each customer has a preference for each facility. The multi-objective MCLP with customers’ preferences (MOMCLPCP) deals with the opening of a fixed number of facilities from a given set of potential facility locations and then customers are assigned to these opened facilities such that both (i) the sum of the demands of customers and (ii) the sum of the preferences of the customers covered by these opened facilities are maximized. A Pareto-based multi-objective harmony search algorithm (MOHSA), which utilizes a harmony refinement strategy for faster convergence, is proposed to solve MOMCLPCP. The proposed MOHSA is terminated based on the stabilization of the density of non-dominated solutions. For experimental purposes, 82 new test instances of MOMCLPCP are generated from the existing single objective MCLP benchmark data sets. The performance of the proposed MOHSA is compared with the well-known non-dominated sorting genetic algorithm II (NSGA-II), and it has been observed that the proposed MOHSA always outperforms NSGA-II in terms of computation time. Moreover, statistical tests show that the objective values obtained from both algorithms are comparable.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011945",
    "keywords": [
      "Algorithm",
      "Benchmark (surveying)",
      "Computation",
      "Computer science",
      "Genetic algorithm",
      "Geodesy",
      "Geography",
      "Harmony search",
      "Mathematical optimization",
      "Mathematics",
      "Multi-objective optimization",
      "Pareto principle",
      "Sorting"
    ],
    "authors": [
      {
        "surname": "Atta",
        "given_name": "Soumen"
      },
      {
        "surname": "Mahapatra",
        "given_name": "Priya Ranjan Sinha"
      },
      {
        "surname": "Mukhopadhyay",
        "given_name": "Anirban"
      }
    ]
  },
  {
    "title": "Audiovisual speaker indexing for Web-TV automations",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115833",
    "abstract": "The current paper introduces a multimodal framework to provide Web-TV automations for live broadcasting and overall big streaming data management. The term indexing refers to the spatiotemporal localization of speakers participating in a discussion panel. Multiple modalities acting in parallel form the data-driven decision-making pipeline. The automated workflow includes the tasks of active speaker detection and localization, frame selection, and creation of a semantically annotated database. For improved performance and robustness, an information fusion model is proposed, which makes use of different audio and visual modalities. Audio-driven Voice Activity Detection follows the Enhanced Temporal Integration methodology applied on a standard audio feature set. For the localization of the dominant audio source, the argument that maximizes the General Cross-Correlation method is calculated. The visual modalities include face and mouth detection and Visual Voice Activity Detection. A Long Short Term Memory network is trained with mouth image sequences to determine voice activity. The values of the audio and visual Voice Activity Detection modules, as well as the General Cross-Correlation result, are used to train an Adaptive Neuro-Fuzzy model, which is responsible for the final decision. Experimental results prove the superiority of the information fusion approach compared to unimodal audio or visual models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011969",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Gene",
      "Modalities",
      "Robustness (evolution)",
      "Search engine indexing",
      "Social science",
      "Sociology",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Vryzas",
        "given_name": "Nikolaos"
      },
      {
        "surname": "Vrysis",
        "given_name": "Lazaros"
      },
      {
        "surname": "Dimoulas",
        "given_name": "Charalampos"
      }
    ]
  },
  {
    "title": "A multi-objective formulation of maximal covering location problem with customers’ preferences: Exploring Pareto optimality-based solutions",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115830",
    "abstract": "The maximal covering location problem (MCLP) is a well-known combinatorial optimization problem with several applications in emergency and military services as well as in public services. Traditionally, MCLP is a single objective problem where the objective is to maximize the sum of the demands of customers which are served by a fixed number of open facilities. In this article, a multi-objective MCLP is proposed where each customer has a preference for each facility. The multi-objective MCLP with customers’ preferences (MOMCLPCP) deals with the opening of a fixed number of facilities from a given set of potential facility locations and then customers are assigned to these opened facilities such that both (i) the sum of the demands of customers and (ii) the sum of the preferences of the customers covered by these opened facilities are maximized. A Pareto-based multi-objective harmony search algorithm (MOHSA), which utilizes a harmony refinement strategy for faster convergence, is proposed to solve MOMCLPCP. The proposed MOHSA is terminated based on the stabilization of the density of non-dominated solutions. For experimental purposes, 82 new test instances of MOMCLPCP are generated from the existing single objective MCLP benchmark data sets. The performance of the proposed MOHSA is compared with the well-known non-dominated sorting genetic algorithm II (NSGA-II), and it has been observed that the proposed MOHSA always outperforms NSGA-II in terms of computation time. Moreover, statistical tests show that the objective values obtained from both algorithms are comparable.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011945",
    "keywords": [
      "Algorithm",
      "Benchmark (surveying)",
      "Computation",
      "Computer science",
      "Genetic algorithm",
      "Geodesy",
      "Geography",
      "Harmony search",
      "Mathematical optimization",
      "Mathematics",
      "Multi-objective optimization",
      "Pareto principle",
      "Sorting"
    ],
    "authors": [
      {
        "surname": "Atta",
        "given_name": "Soumen"
      },
      {
        "surname": "Mahapatra",
        "given_name": "Priya Ranjan Sinha"
      },
      {
        "surname": "Mukhopadhyay",
        "given_name": "Anirban"
      }
    ]
  },
  {
    "title": "Audiovisual speaker indexing for Web-TV automations",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115833",
    "abstract": "The current paper introduces a multimodal framework to provide Web-TV automations for live broadcasting and overall big streaming data management. The term indexing refers to the spatiotemporal localization of speakers participating in a discussion panel. Multiple modalities acting in parallel form the data-driven decision-making pipeline. The automated workflow includes the tasks of active speaker detection and localization, frame selection, and creation of a semantically annotated database. For improved performance and robustness, an information fusion model is proposed, which makes use of different audio and visual modalities. Audio-driven Voice Activity Detection follows the Enhanced Temporal Integration methodology applied on a standard audio feature set. For the localization of the dominant audio source, the argument that maximizes the General Cross-Correlation method is calculated. The visual modalities include face and mouth detection and Visual Voice Activity Detection. A Long Short Term Memory network is trained with mouth image sequences to determine voice activity. The values of the audio and visual Voice Activity Detection modules, as well as the General Cross-Correlation result, are used to train an Adaptive Neuro-Fuzzy model, which is responsible for the final decision. Experimental results prove the superiority of the information fusion approach compared to unimodal audio or visual models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011969",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Gene",
      "Modalities",
      "Robustness (evolution)",
      "Search engine indexing",
      "Social science",
      "Sociology",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Vryzas",
        "given_name": "Nikolaos"
      },
      {
        "surname": "Vrysis",
        "given_name": "Lazaros"
      },
      {
        "surname": "Dimoulas",
        "given_name": "Charalampos"
      }
    ]
  },
  {
    "title": "MFeature: Towards high performance evolutionary tools for feature selection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115655",
    "abstract": "Feature selection commonly refers to a process of using the candidate algorithm to detect the optimal feature sets during the preprocessing steps in machine learning and data mining. This procedure can optimize the dataset's features to be analyzed and maximize the classification performance based on the selected optimal feature combination. In this work, a hybridization model is developed and utilized to select optimal feature subset based on an innovative binary version of moth-flame optimizer and the K-Nearest Neighbor Classifier (KNN) for classification tasks. The proposed new technique, abbreviated as MFeature or ESAMFO, applies several strategies, including two types of transfer functions, ensemble strategy, simulated annealing (SA) disturbance mechanism, and crossover scheme to improve the equilibrium between the global exploration and local exploitation capabilities of the basic MFO. Each individual in the proposed algorithm is evaluated by the size of selected features and the error rate of the KNN classifier. The proposed model's efficacy is assessed on 30 benchmark datasets with different dimensions from the UCI repository and compared with other KNN based feature selection algorithms from the literature. The comprehensive results via various comparisons reveal the efficiency of the proposed technique in decreasing the classification error rate compared with other feature selection algorithms, ensuring the capability of ESAMFO in exploring the feature space and selecting the most informative features for classification purposes. For post publications that support this research, readers can refer to https://aliasgharheidari.com.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010460",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Classifier (UML)",
      "Computer science",
      "Crossover",
      "Data mining",
      "Data pre-processing",
      "Feature (linguistics)",
      "Feature selection",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Preprocessor",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Yueting"
      },
      {
        "surname": "Huang",
        "given_name": "Hui"
      },
      {
        "surname": "Heidari",
        "given_name": "Ali Asghar"
      },
      {
        "surname": "Gui",
        "given_name": "Wenyong"
      },
      {
        "surname": "Ye",
        "given_name": "Xiaojia"
      },
      {
        "surname": "Chen",
        "given_name": "Ying"
      },
      {
        "surname": "Chen",
        "given_name": "Huiling"
      },
      {
        "surname": "Pan",
        "given_name": "Zhifang"
      }
    ]
  },
  {
    "title": "A GWO-based multi-robot cooperation method for target searching in unknown environments",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115795",
    "abstract": "To solve static and dynamic target searching problems involving multiple robots in unknown environments, a novel adaptive robotic grey wolf optimizer (GWO) algorithm, named the RGWO, is proposed. First, an optimal learning strategy is introduced to improve the position updating formula of the GWO to make the algorithm suitable for use in actual mobile situations involving robots, allowing the searching robots to move towards the target (prey) in a step-by-step manner. Then, an adaptive inertial weighting scheme is adopted. By increasing the “aggregation degree” or decreasing the “evolution speed”, the influence of the inertial weight can be increased, which is helpful for maintaining the search diversity of the robots and avoiding premature convergence. In addition, due to the ability of the prey to escape, the pursuing robots are likely to fall into local optima. To avoid this issue, an adaptive speed adjustment strategy and an escape mechanism are adopted. The RGWO is verified and compared with other methods. The RGWO has obvious advantages over other methods in terms of the number of required iterations, success rate and efficiency, and it is superior in both static and dynamic target searching. However, the search trajectories generated with the RGWO are not smoother than those generated with the other investigated methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011635",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Finance",
      "Local optimum",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Medicine",
      "Mobile robot",
      "Position (finance)",
      "Radiology",
      "Robot",
      "Scheme (mathematics)",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Hongwei"
      },
      {
        "surname": "Sun",
        "given_name": "Wei"
      },
      {
        "surname": "Lin",
        "given_name": "Anping"
      },
      {
        "surname": "Xue",
        "given_name": "Min"
      },
      {
        "surname": "Zhang",
        "given_name": "Xing"
      }
    ]
  },
  {
    "title": "Online updating extended belief rule-based system for sensor-based activity recognition",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115737",
    "abstract": "Sensor-based activity recognition (AR) is a core problem with the research domain of smart environments. It has, however, the potential to provide solutions to address the problems associated with the growing size and ageing profile of the global population. The work presented within this paper focuses on the extended belief rule-based system (EBRBS), which offered promising performance compared with popular benchmark AR models and exhibited a high robustness in the situation of sensor failure. Nevertheless, efficiency remains one of the major issues to be improved for determining and updating the extended belief rule base (EBRB) within the EBRBS. This is critical for further utilizing the EBRBS in AR situations within dynamic smart environments. An eigendecomposition-based sensor selection method is firstly proposed to select an effective subset of sensors and to also enable efficient implementation to facilitate online AR. A novel domain division-based rule generation method is also proposed to generate and update an EBRB efficiently when new sensor data are available or when some sensors should be included or excluded in the EBRB. The combination of these two methods leads to an enhanced EBRBS, called online updating EBRBS. Two datasets (in a balanced class situation) obtained from simulation and actual environments are studied to provide detailed experimental analysis as a preliminary study and basis to handle further the imbalanced situation of real AR. The experimental results demonstrate an enhanced performance of the online updating EBRBS compared with the original EBRBS and some benchmark AR models, in terms of efficiency and effectiveness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011167",
    "keywords": [
      "Activity recognition",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Machine learning",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Long-Hao"
      },
      {
        "surname": "Liu",
        "given_name": "Jun"
      },
      {
        "surname": "Wang",
        "given_name": "Ying-Ming"
      },
      {
        "surname": "Nugent",
        "given_name": "Chris"
      },
      {
        "surname": "Martínez",
        "given_name": "Luis"
      }
    ]
  },
  {
    "title": "Attentional matrix factorization with context and co-invocation for service recommendation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115698",
    "abstract": "Easy accessibility of data and functions are the main advantages to develop mashups from abundant sources of Web APIs. However, it simultaneously brings difficulties to choose suitable APIs for a mashup. Existing probabilistic matrix factorization (PMF) recommender systems can effectively exploit the latent features of the invocations with the same weight but not all features are equally significant and predictive, and the useless features may bring noises to the model. In this work, we propose the Attentional PMF Model (AMF), which leverages a neural attentional network to learn the significance of latent features. We then inject the attentional scores and the mashup-API context similarity into the matrix factorization structure for training. Furthermore, our model exploits the relationship between APIs from both their context and co-invocation history as regularization terms to improve its prediction performance. Our experiments are evaluated on ProgrammableWeb. The results show that our model outperforms some state-of-art recommender systems in mashup service applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010824",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Collaborative filtering",
      "Computer science",
      "Computer security",
      "Context (archaeology)",
      "Eigenvalues and eigenvectors",
      "Exploit",
      "Information retrieval",
      "Machine learning",
      "Mashup",
      "Matrix decomposition",
      "Paleontology",
      "Physics",
      "Probabilistic logic",
      "Quantum mechanics",
      "Recommender system",
      "Web 2.0",
      "Web service",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Nguyen",
        "given_name": "Mo"
      },
      {
        "surname": "Yu",
        "given_name": "Jian"
      },
      {
        "surname": "Nguyen",
        "given_name": "Tung"
      },
      {
        "surname": "Han",
        "given_name": "Yanbo"
      }
    ]
  },
  {
    "title": "Customer classification: A Mamdani fuzzy inference system standpoint for modifying the failure mode and effect analysis based three dimensional approach",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115753",
    "abstract": "Customer categorization using a three dimensional loyalty matrix, based on failure mode and effect analysis (FMEA), is an innovative approach for customer classification but is vulnerable to FMEA limitations. The main purpose of this research is to utilize a multi input single output Mamdani fuzzy inference system (FIS) to cope with the traditional FMEA inherited shortcomings. Besides, the classification logic and classes of the Loyalty Matrix methodology have been adopted for the purpose. We have also identified four potential market scenarios and evaluated the performance of the proposed methodology within these contexts. Correspondingly, four tailored FIS’s consisting of a total of 108 fuzzy rules have been developed. Empirical results indicate that the new approach successfully resolved serious issues such as data uncertainty, weight ignorance, the same output value computation from different input values and the discontinued output.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011295",
    "keywords": [
      "Adaptive neuro fuzzy inference system",
      "Art",
      "Artificial intelligence",
      "Categorization",
      "Computer science",
      "Data mining",
      "Dual (grammatical number)",
      "Engineering",
      "Epistemology",
      "Failure mode and effects analysis",
      "Fuzzy control system",
      "Fuzzy inference system",
      "Fuzzy logic",
      "Ignorance",
      "Inference",
      "Law",
      "Literature",
      "Loyalty",
      "Machine learning",
      "Mode (computer interface)",
      "Operating system",
      "Philosophy",
      "Political science",
      "Reliability engineering"
    ],
    "authors": [
      {
        "surname": "Geramian",
        "given_name": "Arash"
      },
      {
        "surname": "Abraham",
        "given_name": "Ajith"
      }
    ]
  },
  {
    "title": "An efficient image super resolution model with dense skip connections between complex filter structures in Generative Adversarial Networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115780",
    "abstract": "Generative Adversarial Network (GAN) based models have gained a lot of popularity due to their outstanding performance in image super resolution tasks. However, these networks have few inherent problems; such as, high computational complexity, large depth and vanishing gradient. Present work proposes a novel GAN based architecture, namely, Super Resolution with Inception Network (SRINet) to solve the above mentioned problems. Generator architecture of SRINet uses complex filter structure rather than the linear filter structure to increase the depth and width of network without increasing the computational cost. Complex filter settings, present in the architecture, helps it to attain locally distributed information along with hierarchical global information in an image. Hence, the proposed method approximates the most favorable sparse structures to foster the learning capability of network. Additionally, progressive two stage upscaling approach with dense skip connections are introduced in the generator architecture. This technique helps the proposed network to learn precise mapping to generate an output image from the low resolution image. To measure visual quality of an image, we use human visual system based visual information fidelity metric. Proposed method outperforms all the state-of-the-art methods qualitatively (perceptually) and quantitatively on other GAN based methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011489",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer engineering",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Economics",
      "Filter (signal processing)",
      "Generator (circuit theory)",
      "Image (mathematics)",
      "Metric (unit)",
      "Network architecture",
      "Operations management",
      "Pattern recognition (psychology)",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Sharma",
        "given_name": "Shailza"
      },
      {
        "surname": "Kumar",
        "given_name": "Vinay"
      }
    ]
  },
  {
    "title": "A Novel Minutiae Triangulation Technique for Non-invertible Fingerprint Template Generation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115832",
    "abstract": "The fingerprint is the readily available and most widely used biometrics for the security of a system. In order to impart the security of the template database, it is important to make the fingerprint template robust to fingerprint reconstruction, i.e., invertibility. In this paper, a fingerprint template is proposed using a novel minutiae triangulation technique. In this triangulation technique, the set of minutiae of a template is taken to generate a set of minutiae triplets each forming a triangle. After this, a minutiae transformation is applied to obtain a set of triplets of internal angles of the triangle which also removes the minutiae location and orientation information. In the absence of this information, the orientation map of the fingerprint cannot be estimated. This set of triplets together with some other information is used as a proposed fingerprint template. The proposed system is experimented using standard FVC2000, FVC2004 and FVC2006 databases and yields better results in terms of EER (Equal Error Rate) as compared with the state-of-art techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011957",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Computer vision",
      "Fingerprint (computing)",
      "Fingerprint recognition",
      "Geometry",
      "Mathematics",
      "Minutiae",
      "Orientation (vector space)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Triangulation"
    ],
    "authors": [
      {
        "surname": "Trivedi",
        "given_name": "Amit Kumar"
      },
      {
        "surname": "Thounaojam",
        "given_name": "Dalton Meitei"
      },
      {
        "surname": "Pal",
        "given_name": "Shyamosree"
      }
    ]
  },
  {
    "title": "An advanced TOPSIS method with new fuzzy metric based on interval type-2 fuzzy sets",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115770",
    "abstract": "Decision-making techniques are among important topics applied in operations research. Multi-Criteria Decision-Making (MCDM) is one of the very commonly used subjects of this issue. Technique for Order Preference by Similarity to Ideal Solutions (TOPSIS) is among the most utilized MCDM approaches in terms of both convenience and efficiency. To provide the reality and ease of application of these methods as much as possible Interval Type-2 Fuzzy Numbers (IT2FN)s, a special kind of type-2 fuzzy sets (T2FS)s, are frequently used together with MCDM methods. However, before the final stage of the solution to the problem, defuzzification techniques are applied for obtaining the optimum solution. Switching to crisp numbers before the last step of the procedure reduces the effectiveness of using the IT2FNs. The aim of this paper is to contribute to the literature with a new idea of fuzzy metric function whose result is again a fuzzy number. The result of this study will be a serious contribution to the literature with its metric function structure that is a fuzzy number. A metric function whose result will be a fuzzy set can also be made for different fuzzy structures. Also, a new partial order relation will be given for IT2FNs to define this fuzzy metric function. Moreover, this new fuzzy metric will be adapted to the TOPSIS method. Hence, it will be assured that the defuzzification operations are used only in the final sorting stage and the advantage of using fuzzy numbers will be able to continue in further steps. The advanced TOPSIS with a new fuzzy metric will be applied to a video chat program selection problem as an example.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011416",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Defuzzification",
      "Economics",
      "Fuzzy classification",
      "Fuzzy logic",
      "Fuzzy mathematics",
      "Fuzzy number",
      "Fuzzy set",
      "Fuzzy set operations",
      "Ideal solution",
      "Mathematical optimization",
      "Mathematics",
      "Membership function",
      "Metric (unit)",
      "Multiple-criteria decision analysis",
      "Operations management",
      "Operations research",
      "Physics",
      "TOPSIS",
      "Thermodynamics",
      "Type-2 fuzzy sets and systems"
    ],
    "authors": [
      {
        "surname": "Meni̇z",
        "given_name": "Büşra"
      }
    ]
  },
  {
    "title": "A weighted word embedding based approach for extractive text summarization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115867",
    "abstract": "Automatic text summarization (ATS) is a method to condense a long size text document into abridging form by enveloping all the primary information and central theme. Numerous ATS models have already prospected in this direction. However, many of those do not capture the semantic features and latent meanings of the text documents. In this paper, we present a weighted word vector representation method concerning TF-IDF for ATS. The proposed model is a prospective method for huge data on the internet that can catch all possible semantic meanings from the text along with the statistical and linguistic features. The proposed word vectors help to strengthen the diversity of the generated summary by discriminating semantically dissimilar sentences. Besides, we evaluate the proposed model on news articles taken from DUC 2007 dataset using the ROUGE summary evaluation metric. Moreover, we compare the proposed model against the four state-of-the-art summarization models and observe that our proposed approach outperforms among all the baselines and able to produce coherent, meaningful, diverse, and least redundant summaries.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421012264",
    "keywords": [
      "Artificial intelligence",
      "Automatic summarization",
      "Computer science",
      "Economics",
      "Embedding",
      "Information retrieval",
      "Latent semantic analysis",
      "Law",
      "Linguistics",
      "Metric (unit)",
      "Natural language processing",
      "Operations management",
      "Philosophy",
      "Political science",
      "Politics",
      "Probabilistic latent semantic analysis",
      "Representation (politics)",
      "Word (group theory)",
      "Word embedding"
    ],
    "authors": [
      {
        "surname": "Rani",
        "given_name": "Ruby"
      },
      {
        "surname": "Lobiyal",
        "given_name": "Daya K."
      }
    ]
  },
  {
    "title": "Unsupervised multi-subepoch feature learning and hierarchical classification for EEG-based sleep staging",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115759",
    "abstract": "As the medium of developing brain–computer interface system, the recognition of EEG signals is complicated and difficult due to the complex nonstationary characteristics and the individual difference between subjects. In this paper, we investigate the EEG signal classification problem and propose a novel unsupervised multi-subepoch feature learning and hierarchical classification method for automatic sleep staging. First, we divide the EEG epoch into multiple signal subepochs, and each subepoch is mapped to amplitude axis and time axis respectively to obtain two kinds of feature information with amplitude–time dynamic characteristics. Then, the statistical classification features are extracted from the mapped feature information. Furthermore, we conduct unsupervised feature learning for consistent and specific classification features from the perspective of time series. Finally, according to the differences and similarities of EEG signals in different sleep stages, a hierarchical weighted support vector machine-based classification model (H-WSVM) is established, which can use different feature subsets at each classification level and different weighting parameters for unbalanced data samples. To select the optimal feature subset for detecting each sleep stage, we propose a novel evaluation criterion for feature classification ability based on rough set theory. Experimental results on the most commonly used dataset show that the proposed method has better sleep staging performance and can effectively promote the development and application of EEG sleep staging system.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011337",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Electroencephalography",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature vector",
      "Linguistics",
      "Machine learning",
      "Medicine",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Polysomnography",
      "Psychiatry",
      "Psychology",
      "Radiology",
      "Sleep Stages",
      "Support vector machine",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "An",
        "given_name": "Panfeng"
      },
      {
        "surname": "Yuan",
        "given_name": "Zhiyong"
      },
      {
        "surname": "Zhao",
        "given_name": "Jianhui"
      }
    ]
  },
  {
    "title": "A novel structural adaptive discrete grey prediction model and its application in forecasting renewable energy generation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115761",
    "abstract": "The rapidly growing renewable energy generation instigates stochastic volatility of electricity supply that may compromise the power grid's stability and increase the grid imbalance cost. Therefore, accurate mid-to-long term renewable energy generation forecasting is of great significance for integrating renewable energy systems with smart grid and energy strategic planning. For this purpose, a new structural adaptive discrete grey prediction model is proposed. Overall, the proposed model possesses three main contributions. Firstly, the introduction of nonlinear term and periodic term strengthens the ability of the traditional DGM (1,1) model to capture the nonlinear and linear development trend of time series and improves the adaptability of the grey prediction model to arbitrary periodic time series. Secondly, the emerging coefficients are determined by the particle swarm optimization algorithm and hold-out cross-validation method, and the adaptive selection of the model structure is realized. From the perspective of expert system, it reduces the need for modeling knowledge. Thirdly, the consistency of stretching, unbiasedness, and compatibility with other grey models are discussed, which further verified the feasibility and practicability of the proposed model. Besides, the performance of the proposed model is compared with those of a series of grey prediction models and non-grey prediction methods to verify the feasibility and superiority of this new approach by three real cases. The results indicate that the proposed model benefits from its adaptive structure and produces reliable predictions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011350",
    "keywords": [
      "Computer science",
      "Electric power system",
      "Electrical engineering",
      "Engineering",
      "Geometry",
      "Grid",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Nonlinear system",
      "Particle swarm optimization",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Renewable energy"
    ],
    "authors": [
      {
        "surname": "Qian",
        "given_name": "Wuyong"
      },
      {
        "surname": "Sui",
        "given_name": "Aodi"
      }
    ]
  },
  {
    "title": "Shared-view and specific-view information extraction for recommendation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115752",
    "abstract": "In various recommender systems, ratings and reviews are the main information to show user preferences. However, recommendation models that only use ratings, such as collaborative filtering, are vulnerable to data sparsity. And models only using review information will also suffer from the sparsity of reviews. On one hand, most ratings and reviews are interrelated and complementary, reviews may explain why a user gives a high or low rating to an item. On the other hand, ratings and reviews are numerical and textual information, respectively, and they reflect the preference of the user from a coarse-grained level and a fine-grained level A user may comment positively about some aspects of an item, even he gives a very low score to this item. There are specific information among each of them because of their heterogeneity. Therefore, it is possible to learn more accurate representation of users and items by effectively integrating ratings and text reviews from different views, that is, shared-view and specific-view. In this paper, we propose a Shared-view and Specific-view Information extraction model for Recommendation (SSIR), which integrates the information from reviews and interaction matrix to predict ratings Our model has two key components, including shared-view information extraction and specific-view exploitation. From the perspective of shared-view, SSIR jointly minimizes the loss of confusion adversarial and rating prediction loss to extract the shared information from reviews and user–item interaction matrix. For the specific-view part, SSIR applies orthogonal constraints on shared-view and specific-view modules to extract the discriminative features from reviews and interaction data. We fuse the features extracted from these two views to predict the final ratings. In addition, we use auxiliary reviews to deal with the sparsity problem of reviews. Experimental results on eight datasets show the effectiveness and robustness of our method, which could adapt to the recommendation scenarios with fewer reviews and ratings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011283",
    "keywords": [
      "Artificial intelligence",
      "Collaborative filtering",
      "Computer science",
      "Data science",
      "Economics",
      "Information extraction",
      "Information retrieval",
      "Interaction information",
      "Law",
      "Mathematics",
      "Microeconomics",
      "Perspective (graphical)",
      "Political science",
      "Politics",
      "Preference",
      "Recommender system",
      "Representation (politics)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Huiting"
      },
      {
        "surname": "Zhao",
        "given_name": "Jindou"
      },
      {
        "surname": "Li",
        "given_name": "Peipei"
      },
      {
        "surname": "Zhao",
        "given_name": "Peng"
      },
      {
        "surname": "Wu",
        "given_name": "Xindong"
      }
    ]
  },
  {
    "title": "Identifying vital nodes from local and global perspectives in complex networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115778",
    "abstract": "Recognition of vital nodes in complex networks retains great importance in the improvement of network’s robustness and vulnerability. Consistent research proposed various approaches like local-structure-based methods, e.g., degree centrality, pagerank, etc., and global-structure-based methods, e.g., betweenness, closeness centrality, etc., to evaluate the concerned nodes. Though their performance is amazingly well, these methods have undergone some intrinsic limitations. For instance, local-structure-based methods lose some sort of global information and global-structure-based methods are too complicated to measure the important nodes, particularly in networks where sizes become large. To tackle these challenges, we propose a Local-and-Global-Centrality (LGC) measuring algorithm to identify the vital nodes through handling local as well as global topological aspects of a network simultaneously. In order to assess the performance of the proposed algorithm with respect to the state-of-the-art methodologies, we performed experiments through LCG, Betweenness (BNC), Closeness (CNC), Gravity (GIC), Page-Rank (PRC), Eigenvector (EVC), Global and Local Structure (GLS), Global Structure Model (GSM), and Profit-leader (PLC) methods on differently sized real-world networks. Our experiments disclose that LGC outperformed many of the compared techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011477",
    "keywords": [
      "Artificial intelligence",
      "Complex network",
      "Computer science",
      "Data science",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Ullah",
        "given_name": "Aman"
      },
      {
        "surname": "Wang",
        "given_name": "Bin"
      },
      {
        "surname": "Sheng",
        "given_name": "JinFang"
      },
      {
        "surname": "Long",
        "given_name": "Jun"
      },
      {
        "surname": "Khan",
        "given_name": "Nasrullah"
      },
      {
        "surname": "Sun",
        "given_name": "ZeJun"
      }
    ]
  },
  {
    "title": "An efficient Nyström spectral clustering algorithm using incomplete Cholesky decomposition",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115813",
    "abstract": "Nyström method can estimate the eigenvectors of a large kernel matrix with the eigenvectors of a small sampled sub-matrix. However, we may encounter two problems when using Nyström method to speed up spectral clustering: one problem is the approximate eigenvectors generated by standard Nyström method are sub-optimal, so they may impair the performance of spectral clustering; another one is the accurate Nyström approximation needs a sufficient number of samples, which will increase the eigen-decomposition cost on the sampled sub-matrix. To solve these problems, this paper proposes an efficient Nyström spectral clustering algorithm using incomplete Cholesky decomposition, in which a new matrix factorization strategy is designed for Nyström spectral clustering to meet the orthogonal constraints, and an efficient eigensolver based on incomplete Cholesky decomposition is developed to accelerate the Nyström approximation. In this way, the obtained approximate orthogonal eigenvectors will help to improve the clustering quality, and the developed eigenvector calculation method will help to reduce the clustering complexity. The experimental results show that the proposed algorithm performs well on many challenging data sets, and it can accomplish more complex clustering tasks with limited computing resources.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011817",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Chemistry",
      "Cholesky decomposition",
      "Chromatography",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Decomposition",
      "Eigenvalues and eigenvectors",
      "Incomplete Cholesky factorization",
      "Integral equation",
      "Kernel (algebra)",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Matrix decomposition",
      "Minimum degree algorithm",
      "Nyström method",
      "Organic chemistry",
      "Physics",
      "Quantum mechanics",
      "Spectral clustering"
    ],
    "authors": [
      {
        "surname": "Jia",
        "given_name": "Hongjie"
      },
      {
        "surname": "Wang",
        "given_name": "Liangjun"
      },
      {
        "surname": "Song",
        "given_name": "Heping"
      },
      {
        "surname": "Mao",
        "given_name": "Qirong"
      },
      {
        "surname": "Ding",
        "given_name": "Shifei"
      }
    ]
  },
  {
    "title": "Hierarchical control of multi-agent reinforcement learning team in real-time strategy (RTS) games",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115707",
    "abstract": "Coordinated control of multi-agent teams is an important task in many real-time strategy (RTS) games. In most prior work, micromanagement is the commonly used strategy whereby individual agents operate independently and make their own combat decisions. On the other extreme, some employ a macromanagement strategy whereby all agents are controlled by a single decision model. In this paper, we propose a hierarchical command and control architecture, consisting of a single high-level and multiple low-level reinforcement learning agents operating in a dynamic environment. This hierarchical model enables the low-level unit agents to make individual decisions while taking commands from the high-level commander agent. Compared with prior approaches, the proposed model provides the benefits of both flexibility and coordinated control. The performance of such hierarchical control model is demonstrated through empirical experiments in a real-time strategy game known as StarCraft: Brood War (SCBW).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010897",
    "keywords": [
      "Artificial intelligence",
      "Autonomous agent",
      "Computer science",
      "Control (management)",
      "Engineering",
      "Flexibility (engineering)",
      "Machine learning",
      "Mathematics",
      "Reinforcement learning",
      "Statistics",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Weigui Jair"
      },
      {
        "surname": "Subagdja",
        "given_name": "Budhitama"
      },
      {
        "surname": "Tan",
        "given_name": "Ah-Hwee"
      },
      {
        "surname": "Ong",
        "given_name": "Darren Wee-Sze"
      }
    ]
  },
  {
    "title": "Bead geometry modeling on uneven base metal surface by fuzzy systems for multi-pass welding",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115356",
    "abstract": "This paper presents a modeling method of weld bead profiles deposited on uneven base metal surfaces and its application in multi-pass welding. The robotized multi-pass tungsten inert gas welding requires precise positioning of the weld beads to avoid welding defects and achieve the desirable welding join since the weld bead shapes depend on the surface of the previously deposited beads. The proposed model consists of fuzzy systems to estimate the coefficients of the profile function. The characteristic points of the trapezoidal membership functions in the rule bases are tuned by the Bacterial Memetic Algorithm during supervised training. The fuzzy systems are structured as multiple-input-single-output systems, where the inputs are the welding process variables and the coefficients of the shape functions of the segments underlying the modeled bead; the outputs are the coefficients of the bead shape function. Each segment surface is approximated by a second-order polynomial function defined in the weld bead’s local coordinate system. The model is developed from empirical data collected from single and multi-pass welding. The performance of the proposed model is compared with a multiple linear regression model. During the experimental validation, first, the individual beads are evaluated by comparing the estimated coefficients of the profile function and other bead characteristics (bead area, width, contact angles, and position of the toe points) with the measurements, and the estimations of a multiple linear regression model. Second, the sequential placement of the weld beads is evaluated while filling a straight V-groove by comparing the estimated bead characteristics with the measurements and calculating the accumulated error of the filled groove cross-section. The results show that the proposed model provides a good estimation of the bead shapes during deposition on uneven base metal surfaces and outperforms the regression model with low error in both validation cases. Furthermore, it is experimentally validated that the derived bead characteristics provide a suitable measure to identify locations sensitive to welding defects.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007843",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bead",
      "Biology",
      "Composite material",
      "Computer science",
      "Economics",
      "Engineering",
      "Evolutionary biology",
      "Finance",
      "Function (biology)",
      "Fuzzy logic",
      "Materials science",
      "Mechanical engineering",
      "Position (finance)",
      "Welding"
    ],
    "authors": [
      {
        "surname": "Horváth",
        "given_name": "Csongor Márk"
      },
      {
        "surname": "Botzheim",
        "given_name": "János"
      },
      {
        "surname": "Thomessen",
        "given_name": "Trygve"
      },
      {
        "surname": "Korondi",
        "given_name": "Péter"
      }
    ]
  },
  {
    "title": "Portfolio formation and optimization with continuous realignment: A suggested method for choosing the best portfolio of stocks using variable length NSGA-II",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115732",
    "abstract": "In this paper, we introduce a method of portfolio formation through vertical and horizontal clustering. The clustering algorithm incorporates sufficiently diversified number of stocks in the portfolio with exposure limits on each stock. On obtaining the near Pareto optimal portfolios by using the proposed variable-length Non-dominated Sorting based Genetic Algorithm (NSGA-II), quarter-wise weights of each portfolio’s constituent stocks are determined through the proposed single objective Genetic Algorithm (GA) based Markowitz model. This enables dynamic realignment of the portfolios and can incorporate the macroeconomic environment of the time. The performance of the portfolios is then compared with a benchmark portfolio. Our results show that returns from each of our portfolios, dynamically realigned each quarter, have been able to beat the benchmark index return over our study’s time period. The performance of the clustering algorithm is validated with 4 well-known clustering algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742101112X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cluster analysis",
      "Computer science",
      "Econometrics",
      "Economics",
      "Engineering",
      "Financial economics",
      "Genetic algorithm",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Mechanical engineering",
      "Medicine",
      "Multi-objective optimization",
      "Pareto principle",
      "Portfolio",
      "Portfolio optimization",
      "Radiology",
      "Rate of return on a portfolio",
      "Sorting",
      "Stock (firearms)",
      "Variable (mathematics)",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Pal",
        "given_name": "Ramen"
      },
      {
        "surname": "Chaudhuri",
        "given_name": "Tamal Datta"
      },
      {
        "surname": "Mukhopadhyay",
        "given_name": "Somnath"
      }
    ]
  },
  {
    "title": "Drug-target continuous binding affinity prediction using multiple sources of information",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115810",
    "abstract": "Drug-target binding affinity prediction has a significant role in the search for new drugs or novel targets for existing drugs. The vast majority of recent computational approaches, presented for the task of drug-target binding affinity prediction, make use of a single source to measure drug-drug or protein-protein similarities. Incorporating various information sources is of the essence for improving the accuracy of drug-target prediction. The main objective of this research is to propose a method for combining the information provided from various similarity measures for drug-drug and protein-protein similarities and to show that this leads to better prediction performance. For this purpose, we propose a method that makes use of five drug-drug and five protein-protein similarity measures simultaneously to predict the binding affinity value of an input query drug-protein interaction. In the proposed method, using each pair of drug-drug and protein-protein similarity measures, k-nearest neighbor algorithm is used to find k drug-protein pairs most similar to the input interaction. The information regarding the binding affinity values of neighbors and their similarities are fed as features to a gradient boosting machine to construct the regression model. To assess the performance of our method in comparison with state-of-the-art methods in the literature, three related benchmark datasets were used. The experimental results in various settings (pairwise, new drug, and new target scenarios) indicate the superiority of the proposed method in comparison with other methods proposed in the literature.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011787",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Bioinformatics",
      "Biology",
      "Computer science",
      "Data mining",
      "Drug",
      "Drug discovery",
      "Drug target",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Machine learning",
      "Medicine",
      "Pairwise comparison",
      "Pharmacology",
      "Similarity (geometry)",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Tanoori",
        "given_name": "Betsabeh"
      },
      {
        "surname": "Jahromi",
        "given_name": "Mansoor Zolghadri"
      },
      {
        "surname": "Mansoori",
        "given_name": "Eghbal G."
      }
    ]
  },
  {
    "title": "Reinforcement learning-based register renaming policy for simultaneous multithreading CPUs",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115717",
    "abstract": "Simultaneous multithreading (SMT) improves the performance of superscalar CPUs by exploiting thread-level parallelism with shared entries for better utilization of resources. A key issue for this out-of-order execution is that the occupancy latency of a physical rename register can be undesirably long due to many program execution-dependent factors that result in performance degradation. Such an issue becomes even more problematic in an SMT environment in which these registers are shared among concurrently running threads. Smartly managing this critical shared resource to ensure that slower threads do not block faster threads’ execution is essential to the advancement of SMT performance. In this paper, an actor–critic style reinforcement learning (RL) algorithm is proposed to dynamically assigning an upper-bound (cap) of the rename registers any thread is allowed to use according to the threads’ real-time demand. In particular, a critic network projects the current Issue Queues (IQ) usage, register file usage, and the cap value to a reward; an actor network is trained to project the current IQ usage and register file usage to the optimal real-time cap value via ascending the instructions per cycle (IPC) gradient within the trajectory distribution. The proposed method differs from the state-of-the-art (Wang and Lin, 2018) as the cap for the rename registers for each thread is adjusted in real-time according to the policy and state transition from self-play. The proposed method shows an improvement in IPC up to 162.8% in a 4-threaded system, 154.8% in a 6-threaded system and up to 101.7% in an 8-threaded system. The code is now available open source at https://github.com/98k-bot/RL-based-SMT-Register-Renaming-Policy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742101099X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Instruction set",
      "Latency (audio)",
      "Multithreading",
      "Operating system",
      "Parallel computing",
      "Register file",
      "Reinforcement learning",
      "Simultaneous multithreading",
      "Telecommunications",
      "Thread (computing)"
    ],
    "authors": [
      {
        "surname": "Zhan",
        "given_name": "Huixin"
      },
      {
        "surname": "Sheng",
        "given_name": "Victor S."
      },
      {
        "surname": "Lin",
        "given_name": "Wei-Ming"
      }
    ]
  },
  {
    "title": "Robotic Hierarchical Graph Neurons. A novel implementation of HGN for swarm robotic behaviour control",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115675",
    "abstract": "Simple rule-based robot behaviours, such as those utilised for swarming robots, typically excel in only the niche conditions for which they were designed. Behaviour selection allows robots to switch between these specialised behaviours in accordance with the observed conditions. This paper explores the use of a novel form of Hierarchical Graph Neurons (HGN) for such behaviour selection within a swarm of robotic agents. This new HGN is called Robotic-HGN (R-HGN) as it allows pattern matching of mixed datasets of robot observations. R-HGN matches said patterns to labelled environments and allows appropriate robot behaviours to be utilised throughout an operation in a ‘society of mind’ approach to task flexibility in robots. This approach is novel to the HGN field as it expands the application beyond discrete categorical data inputs. Additionally, this research is novel to the field of robotic swarming as it explores a new method to temporal agent diversity for overcoming localised environment challenges. This R-HGN for behaviour selection is validated against individual behaviour implementations and a random behaviour selection. The comparison is made via statistical distribution of swarm fitnesses in multiple instances of a non-trivial swarming task. From this comparison R-HGN is found to enable appropriate behaviour selection in both environments known and unknown a priori, resulting in a median swarm performance improvement of up to 389%. Finally, in environments prior observed, the R-HGN environment prediction one-versus-all accuracy is up to 99.1% and F 1 scores reach a maximum of 97.15%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010629",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computation",
      "Computer science",
      "Control (management)",
      "Graph",
      "Robot",
      "Selection (genetic algorithm)",
      "Swarm behaviour",
      "Swarm robotics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Smith",
        "given_name": "Phillip"
      },
      {
        "surname": "Aleti",
        "given_name": "Aldeida"
      },
      {
        "surname": "Lee",
        "given_name": "Vincent C.S."
      },
      {
        "surname": "Hunjet",
        "given_name": "Robert"
      },
      {
        "surname": "Khan",
        "given_name": "Asad"
      }
    ]
  },
  {
    "title": "Research on the consistency of additive trapezoidal fuzzy preference relations",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115837",
    "abstract": "Trapezoidal fuzzy numbers offer a suitable and flexible way to express vague judgments of decision makers. This paper studies decision making with additive trapezoidal fuzzy preference relations. It first analyzes the issues encountered in previous consistency concepts for additive trapezoidal fuzzy preference relations. Then, two new consistency concepts, namely, an additive consistency concept and a multiplicative consistency concept, are introduced. To verify the rationality, several of their properties are discussed. To efficiently assess the consistency of additive trapezoidal fuzzy preference relations, several corresponding optimization models are formed. When the objective function values of the built models are zero, the associated additive trapezoidal fuzzy preference relations are consistent. Considering the case where additive trapezoidal fuzzy preference relations are incomplete, optimization models based on the consistency analysis are built, by which identified unknown judgments have the highest consistency level with the known ones. In general, additive trapezoidal fuzzy preference relations are unacceptably consistent. To rank objects from additive trapezoidal fuzzy preference relations, several optimization models for improving the consistency level are built, which consider the self-confidence of the decision makers, the total adjustment and the number of adjusted elements. According to the above discussion, two frameworks for ranking objects from additive trapezoidal fuzzy preference relations are provided that are based on the additive and multiplicative consistency analysis, respectively. Finally, numerical examples are provided to highlight the concrete application and to deliver the comparative analysis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011994",
    "keywords": [
      "Additive model",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Consistency (knowledge bases)",
      "Discrete mathematics",
      "Fuzzy logic",
      "Fuzzy number",
      "Fuzzy set",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Multiplicative function",
      "Preference",
      "Preference relation",
      "Rank (graph theory)",
      "Ranking (information retrieval)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Meng",
        "given_name": "Fanyong"
      },
      {
        "surname": "Pedrycz",
        "given_name": "Witold"
      },
      {
        "surname": "Tang",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "An automated slice sorting technique for multi-slice computed tomography liver cancer images using convolutional network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115686",
    "abstract": "An early detection and diagnosis of liver cancer can help the radiation therapist in choosing the target area and the amount of radiation dose to be delivered to the patients. The radiologists usually spend a lot of time in selecting the most relevant slices from thousands of scans, which are usually obtained from multi-slice CT scanners. The purpose of this paper multi-organ classification of 3D CT images of liver cancer suspected patients by convolution network. A dataset consisting of 63503 CT images of liver cancer patients taken from The Cancer Imaging Archive (TCIA) has been used to validate the proposed method. The method is a CNN for classification of CT liver cancer images. The classification results in terms of accuracy, precision, sensitivity, specificity, true positive rate, false negative rate, and F1 score have been computed. The results manifest a high validation accuracy of 99.1%, when convolution network is trained with the data augmented volume slices as compared to accuracy of 98.7% with that obtained original volume slices. The overall test accuracy for data augmented volume slice dataset is 93.1% superior to other volume slices. The main contribution of this work is that it will help the radiation therapist to focus on a small subset of CT image data. This is achieved by segregating the whole set of 63503 CT images into three categories based on the likelihood of the spread of cancer to other organs in liver cancer suspected patients. Consequently, only 19453 CT images had liver visible in them, making rest of 44050 CT images less relevant for liver cancer detection. The proposed method will help in the rapid diagnosis and treatment of liver cancer patients.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742101071X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Cancer",
      "Cancer detection",
      "Computed tomography",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Data set",
      "Focus (optics)",
      "Internal medicine",
      "Liver cancer",
      "Medicine",
      "Nuclear medicine",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Radiology",
      "Volume (thermodynamics)"
    ],
    "authors": [
      {
        "surname": "Kaur",
        "given_name": "Amandeep"
      },
      {
        "surname": "Chauhan",
        "given_name": "Ajay Pal Singh"
      },
      {
        "surname": "Aggarwal",
        "given_name": "Ashwani Kumar"
      }
    ]
  },
  {
    "title": "Brain inspired lifelong learning model based on neural based learning classifier system for underwater data classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115798",
    "abstract": "The general benchmark for success of an artificial intelligence system is its ability to imitate learning of the human brain. The human brain is capable of continuous learning over a lifespan. The learned knowledge is retained, augmented, fine-tuned and reused to perform new future tasks. At present, machine learning models perform well when carefully arranged, balanced and homogenized data is presented. However, most of these models undergo performance degradation when multiple tasks with incremental data are presented. Inspired by learning of the brain, in this study, we propose a lifelong learning model which extracts knowledge and utilizes the previously learned knowledge to solve the current problem. In the proposed model, firstly, we exploit various deep convolution blocks to extract non-trivial features from images, then a code fragment based learning classifier system with a rich knowledge encoding scheme is devised for knowledge extraction, transfer and reuse. We validate the proposed model with 2 incremental learning scenarios: (i) new instances (ii) new classes, on underwater synsets of the benchmark ImageNet dataset. Experiments results which are analyzed by using paired sampled statistical t-test, show that the proposed model outperforms baseline methods as well as deep convolution neural network based methods, with respect to classification accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011660",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Biology",
      "Classifier (UML)",
      "Computer science",
      "Deep learning",
      "Ecology",
      "Geodesy",
      "Geography",
      "Lifelong learning",
      "Machine learning",
      "Pedagogy",
      "Psychology",
      "Reuse"
    ],
    "authors": [
      {
        "surname": "Irfan",
        "given_name": "Muhammad"
      },
      {
        "surname": "Jiangbin",
        "given_name": "Zheng"
      },
      {
        "surname": "Iqbal",
        "given_name": "Muhammad"
      },
      {
        "surname": "Masood",
        "given_name": "Zafar"
      },
      {
        "surname": "Arif",
        "given_name": "Muhammad Hassan"
      },
      {
        "surname": "Hassan",
        "given_name": "Syed Rauf ul"
      }
    ]
  },
  {
    "title": "Towards noninvasive and fast detection of Glycated hemoglobin levels based on ECG using convolutional neural networks with multisegments fusion and Varied-weight",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115846",
    "abstract": "Glycated hemoglobin A1c (HbA1c) is regarded as a gold standard to evaluate long-term blood glucose control, and it is also a crucial metric in diabetes screening, diagnosis, and management. However, thus far, the HbA1c measurement methods are invasive and painful. Considering that HbA1c levels are associated with cardiovascular autonomic neuropathy, in this paper, a novel Electrocardiogram (ECG)-based approach was presented for noninvasive and fast detection of HbA1c levels using 60-second, single-lead ECG waveform. For this purpose, a total of 317,105 ECG datasets encompassing 370 patients with diabetes were obtained using wearable devices. Furthermore, the ECG preprocessing was based on autocorrelation analysis. The convolutional neural networks with multisegment fusion and varied-weight (CNN-MFVW) were proposed to achieve ECG feature extraction and HbA1c detection. The results showed that the average accuracy, precision, recall, and F1-score of the proposed algorithm were 0.9015, 0.9051, 0.8991 and 0.9013 respectively. Moreover, the area under the curve (AUC) was up to 0.9899, which was higher than other algorithms of conventional CNN and CNN-LSTM. Therefore, we conclude that the proposed approach for noninvasive and fast detection of HbA1c levels has potentials in practical applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421012070",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Diabetes mellitus",
      "Endocrinology",
      "F1 score",
      "Glycated hemoglobin",
      "Gold standard (test)",
      "Internal medicine",
      "Medicine",
      "Pattern recognition (psychology)",
      "Preprocessor",
      "Type 2 diabetes"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Jingzhen"
      },
      {
        "surname": "Lu",
        "given_name": "Jingyi"
      },
      {
        "surname": "Tobore",
        "given_name": "Igbe"
      },
      {
        "surname": "Liu",
        "given_name": "Yuhang"
      },
      {
        "surname": "Kandwal",
        "given_name": "Abhishek"
      },
      {
        "surname": "Wang",
        "given_name": "Lei"
      },
      {
        "surname": "Zhou",
        "given_name": "Jian"
      },
      {
        "surname": "Nie",
        "given_name": "Zedong"
      }
    ]
  },
  {
    "title": "Performance analysis of the integration between Portfolio Optimization and Technical Analysis strategies in the Brazilian stock market",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115687",
    "abstract": "This article proposes a fusion between Technical Analysis indicators and Multiobjective Portfolio Optimization. It considers four indicators and the optimization performed over two risk measures and the expected return, subject to cardinality constraint, self-financing, and investment limits. The fusion occurs using two scenarios. The first one generates an optimal investment portfolio at the beginning of each month and uses Technical Analysis indicators to carry out the transactions. The second one performs the optimization monthly, considering just the assets filtered by the indicators. Numerical simulations consider an insightful comparison concerning the performance of proposed approaches with indicators and optimization in isolation and with standard benchmarks, covering six years of data from the Brazilian Stock Exchange, as a robust analysis. The portfolios are evaluated under metrics return, maximum drawdown and drawup, Return Over Maximum Drawdown index, and the Draw Ratio index. The results show that these fusions can improve the portfolio performance, providing optimal strategies to the investor giving a higher return for a certain level of risk, even considering realistic constraints.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010721",
    "keywords": [
      "Biology",
      "Cardinality (data modeling)",
      "Computer science",
      "Constraint (computer-aided design)",
      "Data mining",
      "Econometrics",
      "Economics",
      "Expected return",
      "Finance",
      "Financial economics",
      "Geometry",
      "Horse",
      "Index (typography)",
      "Mathematical optimization",
      "Mathematics",
      "Paleontology",
      "Portfolio",
      "Portfolio optimization",
      "Rate of return on a portfolio",
      "Stock exchange",
      "Stock market",
      "Stock market index",
      "Technical analysis",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Barroso",
        "given_name": "B.C."
      },
      {
        "surname": "Cardoso",
        "given_name": "R.T.N."
      },
      {
        "surname": "Melo",
        "given_name": "M.K."
      }
    ]
  },
  {
    "title": "Macroeconomic forecasting with statistically validated knowledge graphs",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115765",
    "abstract": "This study leverages narrative from global newspapers to construct theme-based knowledge graphs about world events, demonstrating that features extracted from such graphs improve forecasts of industrial production in three large economies compared to a number of benchmarks. Our analysis relies on a filtering methodology that extracts “backbones” of statistically significant edges from large graph data sets. We find that changes in the eigenvector centrality of nodes in such backbones capture shifts in relative importance between different themes significantly better than graph similarity measures. We supplement our results with an interpretability analysis, showing that the theme categories “disease” and “economic” have the strongest predictive power during the time period that we consider. Our work serves as a blueprint for the construction of parsimonious – yet informative – theme-based knowledge graphs to monitor in real time the evolution of relevant phenomena in socio-economic systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011386",
    "keywords": [],
    "authors": [
      {
        "surname": "Tilly",
        "given_name": "Sonja"
      },
      {
        "surname": "Livan",
        "given_name": "Giacomo"
      }
    ]
  },
  {
    "title": "A review on social spam detection: Challenges, open issues, and future directions",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115742",
    "abstract": "Online Social Networks are perpetually evolving and used in plenteous applications such as content sharing, chatting, making friends/followers, customer engagements, commercials, product reviews/promotions, online games, and news, etc. The substantial issues related to the colossal flood of social spam in social media are polarizing sentiments, impacting users’ online interaction time, degrading available information quality, network bandwidth, computing power, and speed. Simultaneously, groups of coordinated automated accounts/bots often use social networking sites to spread spam, rumors, bogus reviews, and fake news for targeted users or mass communication. The latest developments in the form of artificial intelligence-enabled Deepfakes have exacerbated these issues at large. Consequently, it becomes extremely relevant to review recent work concerning social spam and spammer detection to counter this issue and its effect. This paper provides a brief introduction to social spam, the spamming process, and social spam taxonomy. The comprehensive review entails several dimensionality reduction techniques used for feature selection/extraction, features used, various machine learning and deep learning techniques used for social spam and spammer detection, and their merits and demerits. Artificial intelligence and deep learning empowered Deepfake (text, image, and video) spam, and their countermeasures are also explored. Furthermore, meticulous discussions, existing challenges, and emerging issues such as robustness of detection systems, scalability, real-time datasets, evade strategies used by spammers, coordinated inauthentic behavior, and adversarial attacks on machine learning-based spam detectors, etc., have been discussed with possible directions for future research.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011209",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Data science",
      "Database",
      "Forum spam",
      "Internet privacy",
      "Lexical analysis",
      "Machine learning",
      "Scalability",
      "Social media",
      "Spambot",
      "Spamming",
      "The Internet",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Rao",
        "given_name": "Sanjeev"
      },
      {
        "surname": "Verma",
        "given_name": "Anil Kumar"
      },
      {
        "surname": "Bhatia",
        "given_name": "Tarunpreet"
      }
    ]
  },
  {
    "title": "Interval valued intuitionistic fuzzy AHP-WASPAS based public transportation service quality evaluation by a new extension of SERVQUAL Model: P-SERVQUAL 4.0",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115757",
    "abstract": "Service quality is one of the most important factors that increase the use of public transportation, especially in metropolitan cities such as Istanbul. When evaluating service quality in public transportation systems, one of the most important factors is customer satisfaction. The increase in service quality increases the usage of public transport system utilization and it helps to solve many problems such as traffic congestion, air and noise pollution and energy consumption. For this purpose, the SERVQUAL model is extended with two new criteria related to Industry 4.0 and the pandemic to understand and evaluate the service quality of public transport systems. New criteria added into the SERVQUAL model and a novel P-SERVQUAL 4.0 (Pandemic SERVQUAL 4.0) model is presented. The novel service quality evaluation model is constructed as a three level hierarchical structure to evaluate public transport systems during the pandemic. Then, the evaluation model is modeled as a multi criteria decision making problem and a novel AHP (Analytic Hierarchy Process) integrated WASPAS (Weighted Aggregated Sum Product Assessment) under interval valued intuitionistic fuzzy (IVIF) environment methodology is employed. A real case application to evaluate Istanbul public transport systems is presented in this study. The proposed novel model is applied to evaluate the most used public transportation alternatives such as IETT Bus, Metrobus, Tram, Metro and Marmaray, in Istanbul. By the application of P-SERVQUAL 4.0 model; as a result of this study, Marmaray is determined as the best alternative. The proposed methodology can be used by public or private organizations to improve their strategies and operations to adapt Industry 4.0 and prevent the spread of SARS-CoV-2 (COVID-19). Also, the main limitations and the practical and managerial implications of the study are included in the conclusions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011325",
    "keywords": [
      "Analytic hierarchy process",
      "Business",
      "Computer science",
      "Engineering",
      "Epistemology",
      "Marketing",
      "Mathematics",
      "Operations research",
      "Philosophy",
      "Public transport",
      "Quality (philosophy)",
      "SERVQUAL",
      "Service (business)",
      "Service quality",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Tumsekcali",
        "given_name": "Ecem"
      },
      {
        "surname": "Ayyildiz",
        "given_name": "Ertugrul"
      },
      {
        "surname": "Taskin",
        "given_name": "Alev"
      }
    ]
  },
  {
    "title": "An enhanced separable reversible and secure patient data hiding algorithm for telemedicine applications",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115721",
    "abstract": "In telemedicine industry, a standout among the most significant issues is the exchange of Electronic Patient Information (EPI) between patient and a doctor that are remotely connected. A minute change to EPI may result in wrong diagnosis to the patient. With the aim to ensure secure and safe communications in telemedicine framework, an enhanced separable reversible data hiding technique in encrypted domain has been presented here that gives high embedding rate by embedding k , ( k ≥ 1 ) binary bits of patient data in b a s e 2 k numeral framework using Average Pixel Repetition (APR) method. APR method converts original image of size M × N into encrypted scaled-up image of size ( 2 × M − 1 ) × ( 2 × N − 1 ) such that non-seed pixels are used for data embedding where as seed pixels remain as it is to facilitate reversibility. The experimental study shown that the proposed method gave extremely high embedding rates on both natural and medical images. For all test images, the proposed methodology altogether beated all the compared methodologies in its ability to embed patient information and precisely recover it with maintaining the visual quality of stego images too.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011027",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Economic growth",
      "Economics",
      "Embedding",
      "Encryption",
      "Health care",
      "Image (mathematics)",
      "Information hiding",
      "Mathematical analysis",
      "Mathematics",
      "Pixel",
      "Separable space",
      "Steganography",
      "Telemedicine"
    ],
    "authors": [
      {
        "surname": "Bhardwaj",
        "given_name": "Rupali"
      },
      {
        "surname": "Aggarwal",
        "given_name": "Ashutosh"
      }
    ]
  },
  {
    "title": "Fuzzy Request Set Modelling for Detecting Multiplexed Asymmetric DDoS Attacks on HTTP/2 servers",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115697",
    "abstract": "The introduction of HTTP/2 has led to a dramatic change in web traffic. The steady flow of requests in HTTP/1.1 has been replaced by bursts of multiple requests, largely due to the introduction of multiplexing in HTTP/2 which allows users to send multiple requests through a single connection. This feature was introduced in order to reduce the page loading time by multiplexing a web page and its associated resources in a single connection. While this feature has significantly improved user experience, it can be misused to launch sophisticated application layer DDoS attacks against HTTP/2 servers. Instead of the intended use of multiplexing, attackers can force the web server to process multiple random requests simultaneously, leading to increased server usage. The use of computationally intensive requests can further exacerbate the situation. These attacks, called Multiplexed Asymmetric Attacks, pose a dangerous threat to HTTP/2 servers and stem from the lack of verification of the multiplexed requests. In this work, an approach to model an HTTP/2 request set as a fuzzy multiset is presented. The proposed approach uses a combination of relative cardinality and request workload to detect multiplexed AL-DDoS attacks. Experiments on open source datasets demonstrate that the proposed approach is able to detect multiplexed AL-DDoS attacks with an accuracy of around 95%, while maintaining a low False Positive Rate (FPR) of around 3%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010812",
    "keywords": [
      "Application layer DDoS attack",
      "Blocking (statistics)",
      "Computer network",
      "Computer science",
      "Denial-of-service attack",
      "Multiplexing",
      "Operating system",
      "Programming language",
      "Server",
      "Set (abstract data type)",
      "Telecommunications",
      "The Internet",
      "Web server",
      "Web traffic"
    ],
    "authors": [
      {
        "surname": "Praseed",
        "given_name": "Amit"
      },
      {
        "surname": "Thilagam",
        "given_name": "P. Santhi"
      }
    ]
  },
  {
    "title": "An automatic feature construction method for salient object detection: A genetic programming approach",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115726",
    "abstract": "Over the last two decades, salient object detection (SOD) has received increasingly more attention due to its ability to handle complex natural scenes and its various real-world applications. The performance of an SOD method mainly relies on saliency features that are extracted with different levels of information. Low-level saliency features are often effective in simple scenarios, but they are not always robust in challenging scenarios. With the recent prevalence of high-level saliency features such as deep convolutional neural networks (CNNs) features, a remarkable progress has been achieved in the SOD field. However, CNN-based constructed high-level features unavoidably drop the location information and low-level fine details (e.g., edges and corners) of salient object(s), leading to unclear/blurry boundary predictions. In addition, deep CNN methods have difficulties to generalize and accurately detect salient objects when they are trained with limited number of images (e.g. small datasets). This paper proposes a new automatic feature construction method using Genetic Programming (GP) to construct informative high-level saliency features for SOD. The proposed method takes low-level and hand-crafted saliency features as input to construct high-level features. The constructed GP-based high-level features not only detect the general objects, but they are also good at capturing details and edges/boundaries. The GP-based constructed features have better interpretability compared to CNN-based features. The proposed GP-based method can potentially cope with a small number of samples for training to obtain a good generalization as long as the given training data has enough information to represent the distribution of the data. The experiments on six datasets reveal that the new method achieves consistently high performance compared to twelve state-of-the-art SOD methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011076",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Construct (python library)",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Generalization",
      "Genetic programming",
      "Interpretability",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Moghaddam",
        "given_name": "Shima Afzali Vahed"
      },
      {
        "surname": "Al-Sahaf",
        "given_name": "Harith"
      },
      {
        "surname": "Xue",
        "given_name": "Bing"
      },
      {
        "surname": "Hollitt",
        "given_name": "Christopher"
      },
      {
        "surname": "Zhang",
        "given_name": "Mengjie"
      }
    ]
  },
  {
    "title": "Image super-resolution based on adaptive cascading attention network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115815",
    "abstract": "Single image super-resolution (SISR) refers to the task restoring a high-resolution (HR) image from its low-resolution (LR) counterpart. Deep convolutional neural networks (CNNs) have significantly improved SISR performance. However, the improvements usually come at the cost of the increased network size, which is not practical for the resource constrained mobile devices. In this paper, we propose a lightweight adaptive cascading attention network (ACAN) for SISR. Our contributions are threefold. First, ACAN can assign different weights to each pixel on each channel, so as to select the important information to reconstruct a high quality HR image. Second, ACAN can adaptively combine hierarchical features and effectively reuse the features. Third, ACAN can adaptively reconstruct an HR image using multi-scale global features. Compared with the other state of the art methods, ACAN achieves a better balance between performance and computational cost.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011829",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Convolutional neural network",
      "Ecology",
      "Economics",
      "Image (mathematics)",
      "Management",
      "Pattern recognition (psychology)",
      "Pixel",
      "Resolution (logic)",
      "Reuse",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Dengwen"
      },
      {
        "surname": "Chen",
        "given_name": "Yiming"
      },
      {
        "surname": "Li",
        "given_name": "Wenbin"
      },
      {
        "surname": "Li",
        "given_name": "Jinxin"
      }
    ]
  },
  {
    "title": "An analytical approach for big social data analysis for customer decision-making in eco-friendly hotels",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115722",
    "abstract": "Sustainable tourism is an emerging trend around the world. Eco-friendly (green) hotels are environmentally friendly properties that are becoming more popular among green travellers. Electronic Word-of-Mouth (e-WOM) is a method of communicating with customers to share their experiences and is a powerful marketing tool for hotel marketing. This paper investigates the role of online reviews of eco-friendly hotels for preference learning using multi-criteria decision-making and machine learning techniques. We develop a new method using multi-criteria decision making, supervised and unsupervised learning techniques. The Expectation-Maximization (EM) algorithm is used as an unsupervised learning technique to cluster travellers’ online reviews. We use the Higher-Order Singular-Value Decomposition technique along with a similarity measure to find the most similar customers based on their preference. To predict travellers’ preference for eco-friendly hotels, we employ a neuro-fuzzy system, the Adaptive Neuro-Fuzzy Inference System, as a supervised learning technique. To select the most important criteria, we use the entropy-weight approach in each segment. Several experiments were performed on the collected data from the Czech Republic's eco-friendly hotels on the TripAdvisor platform. The results demonstrated that the hybrid approach is effective for customers’ segmentation, and preference learning and prediction in eco-friendly hotels.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011039",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Cluster analysis",
      "Computer science",
      "Ecology",
      "Entropy (arrow of time)",
      "Environmentally friendly",
      "Fuzzy logic",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Pairwise comparison",
      "Physics",
      "Preference",
      "Quantum mechanics",
      "Statistics",
      "Unsupervised learning",
      "User Friendly"
    ],
    "authors": [
      {
        "surname": "Nilashi",
        "given_name": "Mehrbakhsh"
      },
      {
        "surname": "Minaei-Bidgoli",
        "given_name": "Behrouz"
      },
      {
        "surname": "Alrizq",
        "given_name": "Mesfer"
      },
      {
        "surname": "Alghamdi",
        "given_name": "Abdullah"
      },
      {
        "surname": "Alsulami",
        "given_name": "Abdulaziz A."
      },
      {
        "surname": "Samad",
        "given_name": "Sarminah"
      },
      {
        "surname": "Mohd",
        "given_name": "Saidatulakmal"
      }
    ]
  },
  {
    "title": "HeartMan DSS: A decision support system for self-management of congestive heart failure",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115688",
    "abstract": "Congestive heart failure is a chronic medical condition that affects about 2% of the adult population. Even though it cannot be cured, it can be relieved by a proper, long-term, complex and personalized disease management. In this paper we present the HeartMan Decision Support System (DSS), aimed at supporting individual patients in their uptake of well-established clinical guidelines (i.e., both medication and behaviour based) for disease management. The HeartMan DSS is a central component of the wider HeartMan mobile-health platform that employs mobile phones, wristband sensors and a web application for communication with patients, their physicians and caregivers. The DSS itself provides recommendations for (1) managing patient’s physical health in terms of exercise, nutrition, medications and self-monitoring, (2) psychological support, and (3) managing environmental parameters. The DSS employs a variety of methods: rule-based decision models and adaptable workflows developed using literature and in collaboration with medical experts, classification models developed by machine learning from data, and optimization algorithms. Taken together, they provide a comprehensive, personalized and user-friendly disease management platform. The system was evaluated in a clinical proof-or-concept trial, involving 56 patients in four hospitals. The results confirmed that the system was successful in improving self-care behaviour, decreased patients' levels of depression and anxiety, and improved the overall predicted 1-year mortality risk.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010733",
    "keywords": [
      "Anxiety",
      "Artificial intelligence",
      "Clinical decision support system",
      "Computer science",
      "Database",
      "Decision support system",
      "Depression (economics)",
      "Disease",
      "Economics",
      "Environmental health",
      "Heart failure",
      "Intensive care medicine",
      "Internal medicine",
      "Macroeconomics",
      "Medical emergency",
      "Medicine",
      "Population",
      "Psychiatry",
      "Variety (cybernetics)",
      "Workflow"
    ],
    "authors": [
      {
        "surname": "Bohanec",
        "given_name": "Marko"
      },
      {
        "surname": "Tartarisco",
        "given_name": "Gennaro"
      },
      {
        "surname": "Marino",
        "given_name": "Flavia"
      },
      {
        "surname": "Pioggia",
        "given_name": "Giovanni"
      },
      {
        "surname": "Puddu",
        "given_name": "Paolo Emilio"
      },
      {
        "surname": "Schiariti",
        "given_name": "Michele Salvatore"
      },
      {
        "surname": "Baert",
        "given_name": "Anneleen"
      },
      {
        "surname": "Pardaens",
        "given_name": "Sofie"
      },
      {
        "surname": "Clays",
        "given_name": "Els"
      },
      {
        "surname": "Vodopija",
        "given_name": "Aljoša"
      },
      {
        "surname": "Luštrek",
        "given_name": "Mitja"
      }
    ]
  },
  {
    "title": "Benchmarking the efficiency of water and sewerage companies: Application of the stochastic non-parametric envelopment of data (stoned) method",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115711",
    "abstract": "Many regulators employ frontier techniques to measure the performance of regulated monopolistic companies. This study estimates the cost efficiency of the water sector using the stochastic non-parametric envelopment of data (StoNED) method which brings together the qualities of both data envelopment analysis (DEA) and stochastic frontier analysis (SFA). This technique is applied to several Chilean private and public water companies without and with the inclusion of several environmental variables. The results indicated that the water industry demonstrated high levels of cost efficiency. Based on average results, it appeared that the public water company performed slightly better than private ones, with full private water companies being more efficient than concessionary. However, the analysis of the trend in companies’ efficiency revealed that full private water companies’ efficiency increased over the years 2010–2018, whereas the opposite was true for the public water company. It is also found that environmental variables might have a higher impact on public water company costs and inefficiency than private ones.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010939",
    "keywords": [
      "Benchmarking",
      "Business",
      "Computer science",
      "Data envelopment analysis",
      "Data mining",
      "Environmental engineering",
      "Environmental science",
      "Marketing",
      "Mathematical optimization",
      "Mathematics",
      "Operations research",
      "Parametric statistics",
      "Sewerage",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Molinos-Senante",
        "given_name": "Maria"
      },
      {
        "surname": "Maziotis",
        "given_name": "Alexandros"
      }
    ]
  },
  {
    "title": "Bi-objective dynamic multiprocessor open shop scheduling for maintenance and healthcare diagnostics",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115777",
    "abstract": "This paper addresses a bi-objective dynamic multiprocessor open shop scheduling problem in which the simultaneous objectives of minimizing both the mean weighted flow time and the makespan are considered. This problem is commonly encountered in maintenance and healthcare diagnostic systems. Since it is NP-hard for both objectives, efficient heuristics are needed to quickly generate a set of non-dominated solutions that a decision maker would choose from. For this sake, two metaheuristic approaches based on the non-dominated sorting genetic algorithm (NSGA-II) and the multi-objective grey wolf optimizer (MOGWO) are developed in this paper. Both metaheuristics are hybridized with simulated annealing (SA) local search. Parameter tuning computational experiments are conducted first on a set of 30 small instances from the literature for which Pareto optimal solutions are known. Then, computational experiments on large randomly generated instances are conducted. Computational results for small instances show that the NSGA-II is capable of generating non-dominated solutions that are very close to the optimal Pareto front. Results also reveal that the performance of the NSGA-II is better in most of the cases compared to the MOGWO under different settings of the studied problem for both small and large instances. However, for large instances with large number of workstations and jobs, low loading level and high percentage of busy machines at the beginning of the schedule, the difference in performance between both metaheuristics is minor.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011465",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Flow shop scheduling",
      "Heuristics",
      "Job shop scheduling",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Multi-objective optimization",
      "Multiprocessing",
      "Operating system",
      "Parallel computing",
      "Pareto principle",
      "Schedule",
      "Scheduling (production processes)",
      "Simulated annealing",
      "Sorting",
      "Tardiness"
    ],
    "authors": [
      {
        "surname": "Abdelmaguid",
        "given_name": "Tamer F."
      }
    ]
  },
  {
    "title": "Unified distributed robust regression and variable selection framework for massive data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115701",
    "abstract": "This paper proposes a unified distributed robust regression framework for distributed massive data, which can include many robust regressions in one setting. Specifically, we first transfer different types of robust regressions into an asymptotically equivalent least-squares problem. Then the resulting estimator can be calculated as a weighted average of robust local estimators, and the communication cost is reduced, since it involves only one round of communication. In addition, since the local data information is incorporated sufficiently, it is adaptive to the heterogeneity. The new estimator is proven to be equivalent with the corresponding global robust regression estimator. Furthermore, we conduct variable selection based on the unified robust regression framework and adaptive LASSO, and the path of solution can also be conveniently obtained by LARS algorithm. It is theoretically shown that the new variable selection method can select true relevant variables consistently by using a new distributed BIC-type tuning parameter selector. The simulation results confirm the effectiveness of the new methods and the correctness of the theoretical results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010848",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Correctness",
      "Estimator",
      "Feature selection",
      "Gene",
      "Lasso (programming language)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Regression",
      "Regression analysis",
      "Robust regression",
      "Robust statistics",
      "Robustness (evolution)",
      "Selection (genetic algorithm)",
      "Statistics",
      "Variable (mathematics)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Kangning"
      }
    ]
  },
  {
    "title": "Explainable prediction of electric energy demand using a deep autoencoder with interpretable latent space",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115842",
    "abstract": "Recently, many studies have exploited the potential of deep learning to forecast energy demand, but they cannot explain the results. They only analyze the simple correlations between the input and output to discover the most important input features, or they depend on the manual investigation of the latent space embedded with power demand patterns. In this paper, to overcome these shortcomings, we propose a deep autoencoder that can explain the prediction results by manipulating the latent space. It consists of 1) a power encoder that embeds power information, 2) an auxiliary encoder that embeds auxiliary information for an interpretable latent space in two dimensions, 3) a predictor that predicts power demand by using concatenated values of the latent variables extracted from the two encoders, and 4) an explainer that provides the most important input features in predicting the future demand by utilizing the interpretable latent variables. Several experiments on a dataset of household electric energy demand show that the proposed model not only performs better than conventional models, with a mean squared error of 0.376 in predicting electricity demand for 60 min, but also provides the capacity to explain the results by analyzing the correlation of inputs, latent variables, and energy demand predicted.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421012033",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Computer science",
      "Deep learning",
      "Demand response",
      "Electrical engineering",
      "Electricity",
      "Encoder",
      "Energy (signal processing)",
      "Engineering",
      "Latent heat",
      "Latent variable",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Statistics",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Jin-Young"
      },
      {
        "surname": "Cho",
        "given_name": "Sung-Bae"
      }
    ]
  },
  {
    "title": "A novel version of Cuckoo search algorithm for solving optimization problems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115669",
    "abstract": "In this paper, a Cuckoo search algorithm, namely the New Movement Strategy of Cuckoo Search (NMS-CS), is proposed. The novelty is in a random walk with step lengths calculated by Lévy distribution. The step lengths in the original Cuckoo search (CS) are significant terms in simulating the Cuckoo bird's movement and are registered as a scalar vector. In NMS-CS, step lengths are modified from the scalar vector to the scalar number called orientation parameter. This parameter is controlled by using a function established from the random selection of one of three proposed novel functions. These functions have diverse characteristics such as; convex, concave, and linear, to establish a new strategy movement of Cuckoo birds in NMS-CS. As a result, the movement of NMS-CS is more flexible than a random walk in the original CS. By using the proposed functions, NMS-CS achieves the distance of movement long enough at the first iterations and short enough at the last iterations. It leads to the proposed algorithm achieving a better convergence rate and accuracy level in comparison with CS. The first 23 classical benchmark functions are selected to illustrate the convergence rate and level of accuracy of NMS-CS in detail compared with the original CS. Then, the other Algorithms such as Particle Swarm Optimization (PSO), Gravitational Search Algorithm (GSA), and Grey Wolf Optimizer (GWO) are employed to compare with NMS-CS in a ranking of the best accuracy. In the end, three engineering design problems (tension/compression spring design, pressure vessel design and welded beam design) are employed to demonstrate the effect of NMS-CS for solving various real-world problems. The statistical results show the potential performance of NMS-CS in a widespread class of optimization problems and its excellent application for optimization problems having many constraints. Source codes of NMS-CS is publicly available at http://goldensolutionrs.com/codes.html.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010599",
    "keywords": [
      "Algorithm",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Cuckoo search",
      "Geometry",
      "Lévy flight",
      "Mathematical optimization",
      "Mathematics",
      "Particle swarm optimization",
      "Random search",
      "Random walk",
      "Rate of convergence",
      "Scalar (mathematics)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Cuong-Le",
        "given_name": "Thanh"
      },
      {
        "surname": "Minh",
        "given_name": "Hoang-Le"
      },
      {
        "surname": "Khatir",
        "given_name": "Samir"
      },
      {
        "surname": "Wahab",
        "given_name": "Magd Abdel"
      },
      {
        "surname": "Tran",
        "given_name": "Minh Thi"
      },
      {
        "surname": "Mirjalili",
        "given_name": "Seyedali"
      }
    ]
  },
  {
    "title": "Distance transform based text-line extraction from unconstrained handwritten document images",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115666",
    "abstract": "Text-line extraction (TLE) is the process of segmenting a document page into lines of text for processing by modules such as language and writer identification or Optical Character Recognition (OCR). Designing of an appropriate TLE method is always a challenging research problem especially in the domain of unconstrained handwritten documents. This is because of the vast number of potential interactions between the text lines. For example, these lines are not always straight, lines written close to each other can overlap with ascenders and descenders, and lines can interact with other content on the page. In this paper, we present a novel language-independent text-line extraction method for unconstrained handwritten documents which handles complexities such as touching and multi-skewed text lines, overlapping characters and irregular inter-line spacing. Our method preprocesses the document pages using a distance transform based method and uses a novel path detection algorithm to separate individual text-line. The proposed method has been tested on six standard datasets which are publicly available and the experimental results show that our method achieves a promising accuracy over state-of-the-art TLE methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010563",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Character (mathematics)",
      "Computer science",
      "Document processing",
      "Domain (mathematical analysis)",
      "Geometry",
      "Identification (biology)",
      "Image (mathematics)",
      "Information retrieval",
      "Line (geometry)",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Operating system",
      "Optical character recognition",
      "Path (computing)",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Bera",
        "given_name": "Suman Kumar"
      },
      {
        "surname": "Kundu",
        "given_name": "Soumyadeep"
      },
      {
        "surname": "Kumar",
        "given_name": "Neeraj"
      },
      {
        "surname": "Sarkar",
        "given_name": "Ram"
      }
    ]
  },
  {
    "title": "KR-DBSCAN: A density-based clustering algorithm based on reverse nearest neighbor and influence space",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115763",
    "abstract": "Density-based clustering is one of the most commonly used analysis methods in data mining and machine learning, with the advantage of locating non-ball-shaped clusters without specifying the number of clusters in advance. However, it has notable shortcomings, such as an inability to distinguish adjacent clusters of different densities. We propose a density-based clustering algorithm, KR-DBSCAN, which is based on the reverse nearest neighbor and influence space. The core objects are identified according to the reverse nearest neighborhood, and their influence spaces are determined by calculating the k-nearest neighborhood and reverse nearest neighborhood for each data object under the Euclidean distance metric. In particular, a new cluster expansion condition is defined using the reverse nearest neighborhood and its influence space, and when the core objects are within their influence spaces, they are added to the cluster by breadth-first traversal. As a result, adjacent clusters with different densities are effectively distinguished, and the computational load is substantially reduced. Boundary objects and noise objects are identified, also using k-nearest neighbors. KR-DBSCAN is experimentally validated on the UCI dataset and some synthetic datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011374",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "DBSCAN",
      "Data mining",
      "Nearest-neighbor chain algorithm",
      "Operating system",
      "Pattern recognition (psychology)",
      "Space (punctuation)",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Lihua"
      },
      {
        "surname": "Liu",
        "given_name": "Hongkai"
      },
      {
        "surname": "Zhang",
        "given_name": "Jifu"
      },
      {
        "surname": "Liu",
        "given_name": "Aiqin"
      }
    ]
  },
  {
    "title": "Analysis of sentiment in tweets addressed to a single domain-specific Twitter account: Comparison of model performance and explainability of predictions",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115771",
    "abstract": "Many institutions and companies find it valuable to know how people feel about their ventures; hence, scientific research in sentiment analysis has been intensely developed over time. Automated sentiment analysis can be considered as a machine learning (ML) prediction task, with classes representing human affective states. Due to the rapid development of ML and deep learning (DL), improvements in automatic sentiment analysis performance are achieved almost every year. Since 2013, Semantic Evaluation (SemEval) has hosted a worldwide community-acknowledged competition that allows for comparisons of recent innovations. The sentiment analysis tasks focus on assessing sentiment in Twitter posts authored by various publishers and addressing multiple subjects. Our study aimed to compare selected popular and recent natural language processing methods using a new data set of Twitter posts sent to a single Twitter account. For improved comparability of our experiments with SemEval, we adopted their metrics and also deployed our models on data published for SemEval-2017. In addition, we investigated if an unsupervised ML technique applied for the detection of topics in tweets can be leveraged to improve the predictive performance of a selected transformer model. We also demonstrated how a recent explainable artificial intelligence technique can be used in Twitter sentiment analysis to gain a deeper understanding of the models’ predictions. Our results show that the most recent DL language modeling approach provides the highest quality; however, this quality comes at reduced model transparency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011428",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Comparability",
      "Computer science",
      "Computer security",
      "Deep learning",
      "Economics",
      "Machine learning",
      "Management",
      "Mathematics",
      "Natural language processing",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "SemEval",
      "Sentiment analysis",
      "Set (abstract data type)",
      "Task (project management)",
      "Transformer",
      "Transparency (behavior)",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Fiok",
        "given_name": "Krzysztof"
      },
      {
        "surname": "Karwowski",
        "given_name": "Waldemar"
      },
      {
        "surname": "Gutierrez",
        "given_name": "Edgar"
      },
      {
        "surname": "Wilamowski",
        "given_name": "Maciej"
      }
    ]
  },
  {
    "title": "GL-GCN: Global and Local Dependency Guided Graph Convolutional Networks for aspect-based sentiment classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115712",
    "abstract": "Aspect-based sentiment classification, which aims at identifying the sentiment polarity of a sentence towards the specified aspect, has become a crucial task for sentiment analysis. Existing methods have proposed effective models and achieved satisfactory results, but they mainly focus on exploiting local structure information of a given sentence, such as locality, sequentiality or syntactical dependency constraints within the sentence. Recently, some research works, which utilizes global dependency information, has attracted increasing interest and significantly boosts the performance of text classification. In this paper, we simultaneously introduce both global structure information and local structure information into the task of aspect-based sentiment classification, and propose a novel aspect-based sentiment classification approach, i.e., Global and Local Dependency Guided Graph Convolutional Networks (GL-GCN). In particular, we exploit the syntactic dependency structure as well as sentence sequential information (e.g., the output of BiLSTM) to mine the local structure information of a sentence. On the other hand, we construct a word-document graph using the entire corpus to reveal the global dependency information between words. In addition, an attention mechanism is leveraged to effectively fuse both global and local dependency structure signals. Extensive experiments are conducted on five benchmark datasets in terms of both Accuracy and F1-Score, and the results illustrate that our proposed framework outperforms state-of-the-art methods for aspect-based sentiment classification. The model is implemented using PyTorch and is trained on GPU GeForce GTX 2080 Ti.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010940",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer security",
      "Dependency (UML)",
      "Dependency grammar",
      "Dependency graph",
      "Exploit",
      "Geodesy",
      "Geography",
      "Graph",
      "Linguistics",
      "Locality",
      "Natural language processing",
      "Philosophy",
      "Sentence",
      "Sentiment analysis",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Xiaofei"
      },
      {
        "surname": "Zhu",
        "given_name": "Ling"
      },
      {
        "surname": "Guo",
        "given_name": "Jiafeng"
      },
      {
        "surname": "Liang",
        "given_name": "Shangsong"
      },
      {
        "surname": "Dietze",
        "given_name": "Stefan"
      }
    ]
  },
  {
    "title": "Fast prediction of complicated temperature field using Conditional Multi-Attention Generative Adversarial Networks (CMAGAN)",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115727",
    "abstract": "Generative Adversarial Networks (GANs) have been applied to solve simple physics and engineering problems without complex governing equations. However, existing models can not characterize details and overall dependence in predicting a temperature field. In this paper, we specially propose a new GAN, conditional multi-attention generative adversarial network (CMAGAN), for temperature prediction. CMAGAN can provide reasonably accurate results of complicated heat transfer phenomena rapidly. In our network, a conditional multi-attention module (CMAM) is proposed to captures long-range dependencies between temperature in different regions. An improved multi-scale discriminator is designed to improve the accuracy of temperature results and reduce the randomness by judging from multiple scales. We evaluated the proposed model by simulating the temperature field in a bare sandy land scene with random distribution of signal stations. The temperature images generated by CMAGAN show high accuracy in wake regions (MAPE < 0.079%), shadow regions (MPAE < 0.083%) and signal station regions (MAPE < 0.173%). Compared with previous networks, our GAN has the best performance in temperature prediction via different image similarity evaluation criteria. Finally, CMAGAN's performance in expressing thermodynamics phenomenon is discussed via the visualization of CMAM in our experiment case. The results show that CMAGAN can directly generate robust solutions to complicated heat transfer phenomena.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011088",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Conditional random field",
      "Deep learning",
      "Field (mathematics)",
      "Generative adversarial network",
      "Generative grammar",
      "Machine learning",
      "Mathematics",
      "Pure mathematics"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Jincheng"
      },
      {
        "surname": "Zhu",
        "given_name": "Feiding"
      },
      {
        "surname": "Han",
        "given_name": "Yuge"
      },
      {
        "surname": "Chen",
        "given_name": "Chen"
      }
    ]
  },
  {
    "title": "A visualized bibliometric analysis of mapping research trends of machine learning in engineering (MLE)",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115728",
    "abstract": "In this work, we conducted a visualized bibliometric analysis to map the research trends of machine learning in engineering (MLE) based on articles indexed in the Web of Science Core Collection published between 2000 and 2019. The research distributions, knowledge bases, research hotspots, and research frontiers for MLE studies are revealed by using VOSviewer software and visualization technology. The growth of the literature related to MLE averaged 24.3% in the past two decades. A total of 3057 peer-reviewed papers from 96 countries published in 1299 different journals were identified. The USA was the most productive country, with 23.73% of the overall articles and 32.25% of the overall citations. The most active research organization was MIT, with 41 publications and 1079 citations, and the Journal of Machine Learning Research had the largest number of citations in the field of MLE. In particular, our findings indicate that the research issues of “random forests”, “support vector machine”, “extreme learning machine”, “deep learning”, “statistical learning theory”, and “Python machine learning” formed the knowledge bases of MLE from 2000 to 2019, while the research hotspots focused on applications of machine learning benchmark algorithms. Burst detection analysis results showed that more burst keywords emerged and had a higher frequency of change after 2010. This study provides an insight view of the overall research trends of MLE and may help researchers better understand this research field and predict its dynamic directions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742101109X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Data science",
      "Information retrieval",
      "Machine learning"
    ],
    "authors": [
      {
        "surname": "Su",
        "given_name": "Miao"
      },
      {
        "surname": "Peng",
        "given_name": "Hui"
      },
      {
        "surname": "Li",
        "given_name": "Shaofan"
      }
    ]
  },
  {
    "title": "A new single-chromosome evolutionary algorithm for community detection in complex networks by combining content and structural information",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115854",
    "abstract": "Community detection is an important step in perceiving network structure and performance for complex network analysis. The rapid growth of network data in recent years has piqued the interest of many researchers in community detection. The majority of community detection methods only consider the network structure. Nonetheless, real-world network nodes may have some characteristics that can be useful for community detection. This study proposed a novel single-chromosome evolutionary algorithm with a distinctive architecture modification operator for community detection in complex networks using a combination of structural and content information. To this end, a novel virtual network was created by taking into account the structure and content of nodes, and communities were discovered for this network by optimizing the objective function (and using the combinatorial adjacency matrix instead of the structural adjacency matrix) in a series of steps. The nodes in this network were the same as the nodes in the main network; however, the links were developed based on similarities between nodes and their structural neighborhood. The proposed algorithm also included a method for sorting new nodes in order to determine the analysis order of nodes along with the local improvement of solution, as well as a new criterion, CS, for measuring the content similarity of nodes. The proposed algorithm was evaluated in real-networks and compared to various state-of-the-art and widely used methods. The Friedman rank algorithm was then used to rank the proposed algorithm and the existing methods using six real networks. According to the NMI criterion used in the Friedman rank test, the rank of the proposed algorithms increased by 96.8762%, 70.2693%, 26.0005%, 23.5294%, 46.5109%, and 23.5294% compared respectively with ASCD-ARC, BTLSC, Adapt-SA, PSB-PG, RSECD, and NEMBP, which have all been proposed in recent years.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421012148",
    "keywords": [
      "Adjacency list",
      "Adjacency matrix",
      "Algorithm",
      "Biochemistry",
      "Chemistry",
      "Chromosome",
      "Combinatorics",
      "Community structure",
      "Complex network",
      "Computer science",
      "Data mining",
      "Gene",
      "Graph",
      "Mathematics",
      "Rank (graph theory)",
      "Sorting",
      "Theoretical computer science",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Pourabbasi",
        "given_name": "Elmira"
      },
      {
        "surname": "Majidnezhad",
        "given_name": "Vahid"
      },
      {
        "surname": "Taghavi Afshord",
        "given_name": "Saeid"
      },
      {
        "surname": "Jafari",
        "given_name": "Yasser"
      }
    ]
  },
  {
    "title": "Biased autoencoder for collaborative filtering with temporal signals",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115775",
    "abstract": "Recommendation systems are used in various types of online platforms and in e-commerce. Collaborative filtering (CF) is one of the most popular approaches for recommendation systems and has been widely studied in academia. In recent years, several models based on neural networks that can discover nonlinear relationships have been proposed and compared to traditional CF models. The results showed that they performed better in terms of their prediction accuracy. However, these models do not consider user bias and item bias together, and they do not include temporal signals. This paper proposes a biased autoencoder model (Biased AutoRec) for CF, which is built on the well-known AutoRec CF approach. Several approaches are also proposed to integrate temporal signals into the Biased AutoRec model to merge the power of nonlinearity and temporal signals. Experiments on several public datasets showed that the new models outperformed the AutoRec model, which outperformed the prediction accuracy of previous state-of-the-art CF models (i.e., biased matrix factorization, RBM-CF, LLORMA).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011453",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Collaborative filtering",
      "Computer science",
      "Deep learning",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Recommender system"
    ],
    "authors": [
      {
        "surname": "Dou",
        "given_name": "Runliang"
      },
      {
        "surname": "Arslan",
        "given_name": "Oguzhan"
      },
      {
        "surname": "Zhang",
        "given_name": "Ce"
      }
    ]
  },
  {
    "title": "Medical decision support system for cancer treatment in precision medicine in developing countries",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115725",
    "abstract": "In many developing countries and regions, there are medical problems such as dense populations, lack of medical resources, and shortage of doctors, making it impossible to provide patients with more convenient full-cycle services. Non-small cell lung cancer is a malignant tumor with the highest morbidity and mortality in the world. Research on the auxiliary treatment of intelligent systems is helpful in understanding and evaluate the disease. The system can help doctors provide patients with effective drug treatments and personalized medical services by actively learning the experience of outstanding experts. According to expert knowledge, through quantitative efficacy scores and specific analysis of evaluation indicators, based on the efficacy evaluation matrix and the extraction of key features of patients and drugs, a predictive model of drug efficacy evaluation for adjuvant therapy is established. The model divides into latent feature extraction and curative effect collaborative prediction modules. In the feature extraction module, adding noise to the original data in model training process helps reduce the impact of the sparseness of the patient's medication data. By considering the uncertainty of experts in drug efficacy evaluation modeling, based on probability analysis and efficacy prediction, the proposed method demonstrates the potential options in the face of hesitating choices. According to the predicted efficacy score, candidate drugs are selected to assist doctors in disease analysis and secondary diagnosis. Experiments have shown that drug efficacy prediction methods can provide adjuvant treatments for diseases and quantify the therapeutic effects of targeted drugs. The efficacy information and detection information of patient-drug pairs are helpful to improve decision-making ability, and the proposed medical decision support system framework is superior to other deep learning methods. By adding data, the performance can be significantly improved.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011064",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Disease",
      "Economic shortage",
      "Government (linguistics)",
      "Internal medicine",
      "Linguistics",
      "Machine learning",
      "Medicine",
      "Pathology",
      "Philosophy",
      "Precision medicine"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Genghua"
      },
      {
        "surname": "Chen",
        "given_name": "Zhigang"
      },
      {
        "surname": "Wu",
        "given_name": "Jia"
      },
      {
        "surname": "Tan",
        "given_name": "Yanlin"
      }
    ]
  },
  {
    "title": "Medical decision support system for cancer treatment in precision medicine in developing countries",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115725",
    "abstract": "In many developing countries and regions, there are medical problems such as dense populations, lack of medical resources, and shortage of doctors, making it impossible to provide patients with more convenient full-cycle services. Non-small cell lung cancer is a malignant tumor with the highest morbidity and mortality in the world. Research on the auxiliary treatment of intelligent systems is helpful in understanding and evaluate the disease. The system can help doctors provide patients with effective drug treatments and personalized medical services by actively learning the experience of outstanding experts. According to expert knowledge, through quantitative efficacy scores and specific analysis of evaluation indicators, based on the efficacy evaluation matrix and the extraction of key features of patients and drugs, a predictive model of drug efficacy evaluation for adjuvant therapy is established. The model divides into latent feature extraction and curative effect collaborative prediction modules. In the feature extraction module, adding noise to the original data in model training process helps reduce the impact of the sparseness of the patient's medication data. By considering the uncertainty of experts in drug efficacy evaluation modeling, based on probability analysis and efficacy prediction, the proposed method demonstrates the potential options in the face of hesitating choices. According to the predicted efficacy score, candidate drugs are selected to assist doctors in disease analysis and secondary diagnosis. Experiments have shown that drug efficacy prediction methods can provide adjuvant treatments for diseases and quantify the therapeutic effects of targeted drugs. The efficacy information and detection information of patient-drug pairs are helpful to improve decision-making ability, and the proposed medical decision support system framework is superior to other deep learning methods. By adding data, the performance can be significantly improved.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011064",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Disease",
      "Economic shortage",
      "Government (linguistics)",
      "Internal medicine",
      "Linguistics",
      "Machine learning",
      "Medicine",
      "Pathology",
      "Philosophy",
      "Precision medicine"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Genghua"
      },
      {
        "surname": "Chen",
        "given_name": "Zhigang"
      },
      {
        "surname": "Wu",
        "given_name": "Jia"
      },
      {
        "surname": "Tan",
        "given_name": "Yanlin"
      }
    ]
  },
  {
    "title": "Image super-resolution based on adaptive cascading attention network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115815",
    "abstract": "Single image super-resolution (SISR) refers to the task restoring a high-resolution (HR) image from its low-resolution (LR) counterpart. Deep convolutional neural networks (CNNs) have significantly improved SISR performance. However, the improvements usually come at the cost of the increased network size, which is not practical for the resource constrained mobile devices. In this paper, we propose a lightweight adaptive cascading attention network (ACAN) for SISR. Our contributions are threefold. First, ACAN can assign different weights to each pixel on each channel, so as to select the important information to reconstruct a high quality HR image. Second, ACAN can adaptively combine hierarchical features and effectively reuse the features. Third, ACAN can adaptively reconstruct an HR image using multi-scale global features. Compared with the other state of the art methods, ACAN achieves a better balance between performance and computational cost.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011829",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Convolutional neural network",
      "Ecology",
      "Economics",
      "Image (mathematics)",
      "Management",
      "Pattern recognition (psychology)",
      "Pixel",
      "Resolution (logic)",
      "Reuse",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Dengwen"
      },
      {
        "surname": "Chen",
        "given_name": "Yiming"
      },
      {
        "surname": "Li",
        "given_name": "Wenbin"
      },
      {
        "surname": "Li",
        "given_name": "Jinxin"
      }
    ]
  },
  {
    "title": "Auto-detection of acoustic emission signals from cracking of concrete structures using convolutional neural networks: Upscaling from specimen",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115863",
    "abstract": "Acoustic emission (AE) monitoring has gained significant interest as a promising method for monitoring of changes in structural integrity and durability. Long-term AE monitoring needs to detect and distinguish crack signals from ambient noise (or dummy) signals; however, it is still a daunting task which currently limits field implementation of the AE method. Herein, we explore the feasibility of using convolutional neural network (CNN) models to detect AE crack signals from ambient signals. The trained models are validated both with noise-embedded synthesized signals and with upscaled physical model experiments simulating earthquake loading to a scaled model foundation by using a large-scale shaking table. The 2D CNN model trained the laboratory-synthesized signal sets effectively captured the crack and crack-free signals in all cases including the upscaled physical model experiments. This study presents a simple but robust CNN model for pre-filtering of crack signals and a novel training method for enhanced accuracy, which can be applied for real-time structural health monitoring of concrete-based structures.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421012239",
    "keywords": [
      "Acoustic emission",
      "Acoustics",
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Database",
      "Durability",
      "Earthquake shaking table",
      "Engineering",
      "Image (mathematics)",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "SIGNAL (programming language)",
      "Structural engineering",
      "Structural health monitoring"
    ],
    "authors": [
      {
        "surname": "Han",
        "given_name": "Gyeol"
      },
      {
        "surname": "Kim",
        "given_name": "Yong-Min"
      },
      {
        "surname": "Kim",
        "given_name": "Hyunwoo"
      },
      {
        "surname": "Oh",
        "given_name": "Tae-Min"
      },
      {
        "surname": "Song",
        "given_name": "Ki-Il"
      },
      {
        "surname": "Kim",
        "given_name": "Ayoung"
      },
      {
        "surname": "Kim",
        "given_name": "Youngchul"
      },
      {
        "surname": "Cho",
        "given_name": "Youngtae"
      },
      {
        "surname": "Kwon",
        "given_name": "Tae-Hyuk"
      }
    ]
  },
  {
    "title": "An empirical analysis of the cardinality constrained expectile-based VaR portfolio optimization problem",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115724",
    "abstract": "Expectiles are asymmetric generalizations of mean that are extensively employed by statisticians in regression analysis. In the last decade, the coherence and elicitability characteristics of expectiles have attracted attention of the researchers in risk management field. Recently, expectile has been recommended as an alternative risk measure to value-at-risk (VaR) and conditional value-at-risk (CVaR). As an analogy to VaR and CVaR, expectile is defined as a risk measure called expectile-based value-at-risk (EVaR). In this study, EVaR optimization model is extended with a set of practical constraints such as no short-selling, target return, proportional bounds, and portfolio cardinality constraints. The ex-ante and ex-post risk-adjusted return performances of the proposed model are compared with those of CVaR model by using historical data of the stocks listed in the BIST 100 and the S&P 100 indices. Furthermore, we perform an extensive numerical investigation to reveal the impact of important parameters on the performances of the models. The obtained results show the potential benefits of using EVaR model in practical investment decisions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011052",
    "keywords": [
      "CVAR",
      "Cardinality (data modeling)",
      "Computer science",
      "Data mining",
      "Downside risk",
      "Econometrics",
      "Economics",
      "Efficient frontier",
      "Expected shortfall",
      "Finance",
      "Mathematical optimization",
      "Mathematics",
      "Portfolio",
      "Portfolio optimization",
      "Risk management",
      "Risk measure",
      "Value at risk"
    ],
    "authors": [
      {
        "surname": "Avci",
        "given_name": "Mualla Gonca"
      },
      {
        "surname": "Avci",
        "given_name": "Mustafa"
      }
    ]
  },
  {
    "title": "Representation learning with collaborative autoencoder for personalized recommendation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115825",
    "abstract": "In the past decades, recommendation systems have provided lots of valuable personalized suggestions for the users to address the problem of information over-loaded. Collaborative Filtering (CF) is one of the most commonly applied and successful recommendation approaches, which refers to using the preferences of groups with similar interests to recommend information to other users. Recently, in addition to the traditional matrix factorization techniques, deep learning methods have been proposed to learn more abstract and higher-level representations for recommendation. However, most previous deep recommendation methods learn the higher-level feature representations of users and items through an identical model structure, which ignores the different characteristics of the user-based and item-based data. In addition, the rating matrix is usually sparse which may result in a significant degradation of recommendation performance. To address these problems, we propose a representation learning method with Collaborative Autoencoder for Personalized Recommendation (CAPR for short). In this method, user-based and item-based feature representations are learned by two different autoencoders for capturing different features of the data. Meanwhile, items’ attributions are combined into the feature representations with semi-autoencoder for alleviating the sparsity problem. Extensive experimental results confirm the effectiveness of our proposed method compared to other state-of-the-art matrix factorization methods and deep recommendation methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011908",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Collaborative filtering",
      "Computer science",
      "Deep learning",
      "Feature learning",
      "Information retrieval",
      "Law",
      "Machine learning",
      "Political science",
      "Politics",
      "Recommender system",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Yi"
      },
      {
        "surname": "Wu",
        "given_name": "Xindong"
      },
      {
        "surname": "Qiang",
        "given_name": "Jipeng"
      },
      {
        "surname": "Yuan",
        "given_name": "Yunhao"
      },
      {
        "surname": "Li",
        "given_name": "Yun"
      }
    ]
  },
  {
    "title": "Research on knowledge graph alignment model based on deep learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115768",
    "abstract": "The construction of large-scale knowledge graphs from heterogeneous sources is fundamental to knowledge-driven applications. To solve the problem of redundancy and inconsistency in the process of domain knowledge fusion, this paper reports studies of domain knowledge alignment from the perspective of a knowledge graph. A novel knowledge graph alignment (KGA) model is proposed, based on knowledge graph deep representation learning. To assess the validity of the model, comparative experiments are conducted on the datasets of heterogeneous, cross-lingual, and domain-specific knowledge graphs. Our results of experiments suggest significant improvement on all of these datasets. We discuss the implications for improving the alignment effect of knowledge graph entities, enhancing the coverage and correctness of knowledge graphs, and promoting the performance of knowledge graphs in knowledge-driven applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011404",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Graph",
      "Knowledge graph",
      "Knowledge management",
      "Machine learning",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Chuanming"
      },
      {
        "surname": "Wang",
        "given_name": "Feng"
      },
      {
        "surname": "Liu",
        "given_name": "Ying-Hsang"
      },
      {
        "surname": "An",
        "given_name": "Lu"
      }
    ]
  },
  {
    "title": "Real-time prediction of nuclear power plant parameter trends following operator actions",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115848",
    "abstract": "Operators in the main control room of a nuclear power plant (NPP) oversee all plant operations, and thus any human error committed by the operators can be critical. If the operators can be informed about the future trends of the significant plant parameters that will follow their actions, they will be able to detect a human error in a short time or even prevent it. Future parameter trends, in addition, can be used to confirm the appropriate operational plan and support accident diagnosis. To achieve fast and accurate future parameter trend prediction in NPPs, we propose a data-driven prediction model composed of a multi-step prediction strategy and artificial neural networks. To find the optimal model performance, we applied a multilayered perceptron, vanilla recurrent neural network, and long short-term memory (LSTM) network, and trained the various candidate models with emergency operation data generated from an NPP simulator. Application results showed that the prediction model with the multi-input multi-output strategy and LSTM networks was able to successfully address the multivariate problem of future parameter trend estimation considering operator action in multiple emergency situations. It is believed that the proposed model may support NPP operators in coping with human errors and diagnosing accidents.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421012094",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Ecology",
      "Epistemology",
      "Gene",
      "Machine learning",
      "Multilayer perceptron",
      "Nuclear physics",
      "Nuclear power",
      "Nuclear power plant",
      "Operator (biology)",
      "Perceptron",
      "Philosophy",
      "Physics",
      "Predictive power",
      "Repressor",
      "Transcription factor"
    ],
    "authors": [
      {
        "surname": "Bae",
        "given_name": "Junyong"
      },
      {
        "surname": "Kim",
        "given_name": "Geunhee"
      },
      {
        "surname": "Lee",
        "given_name": "Seung Jun"
      }
    ]
  },
  {
    "title": "Selection of bus chassis for large fleet operators in India: An AHP-TOPSIS approach",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115760",
    "abstract": "Public transport by bus fleet is the most sought-after mode of transportation in India, which is owned and operated by government bodies as well as private players. In general, these operators purchase chassis from different OEMs (Original Equipment Manufacturer) and build bodies according to the statutory norms and conditions prevailing at the place of usage. The fleet operators have their selection criteria for chassis. However, the chassis supplied by distinct OEMs satisfy these selection criteria differently. Hence, the selection of appropriate chassis becomes a complex decision-making problem for fleet organizations. The selection criteria of bus chassis are developed through literature as well as interactions with fleet operators and chassis OEMs. A framework for the selection of the best chassis among alternatives is developed based on a hybrid methodology of AHP (Analytic Hierarchy Process) and TOPSIS (Technique for order preference by similarity to ideal solution). The hybrid methodology will rank various OEM chassis, which will help the bus fleet owners to choose the right chassis that satisfy their requirements in the most optimum manner.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011349",
    "keywords": [
      "Aerospace engineering",
      "Analytic hierarchy process",
      "Artificial intelligence",
      "Automotive engineering",
      "Automotive industry",
      "Business",
      "Chassis",
      "Computer science",
      "Engineering",
      "Manufacturing engineering",
      "Operating system",
      "Operations research",
      "Original equipment manufacturer",
      "Process (computing)",
      "Selection (genetic algorithm)",
      "Structural engineering",
      "TOPSIS",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "James",
        "given_name": "Ajith Tom"
      },
      {
        "surname": "Vaidya",
        "given_name": "Dhruval"
      },
      {
        "surname": "Sodawala",
        "given_name": "Mustufa"
      },
      {
        "surname": "Verma",
        "given_name": "Sunny"
      }
    ]
  },
  {
    "title": "Deep-HR: Fast heart rate estimation from face video under realistic conditions",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115596",
    "abstract": "This paper presents a novel method for remote heart rate (HR) estimation. Recent studies have proved that blood pumping by the heart is highly correlated to the intense color of face pixels, and surprisingly can be utilized for remote HR estimation. Researchers successfully proposed several methods for this task, but making it work in realistic situations is still a challenging problem in computer vision community. Furthermore, learning to solve such a complex task on a dataset with very limited annotated samples is not reasonable. Consequently, researchers do not prefer to use the deep learning approaches for this problem. In this paper, we propose a simple yet efficient approach to benefit the advantages of the Deep Neural Network (DNN) by simplifying HR estimation from a complex task to learning from very correlated representation to HR. Inspired by previous work, we learn a component called Front-End (FE) to provide a discriminative representation of face videos, afterward a light deep regression auto-encoder as Back-End (BE) is learned to map the FE representation to HR. Regression task on the informative representation is simple and could be learned efficiently on limited training samples. Beside of this, to be more accurate and work well on low-quality videos, two deep encoder–decoder networks are trained to refine the output of FE. We also introduce a challenging dataset (HR-D) to show that our method can efficiently work in realistic conditions. Experimental results on HR-D and MAHNOB datasets confirm that our method could run as a real-time method and estimate the average HR better than state-of-the-art ones.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009969",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Deep learning",
      "Discriminative model",
      "Economics",
      "Encoder",
      "Face (sociological concept)",
      "Law",
      "Machine learning",
      "Management",
      "Operating system",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Psychoanalysis",
      "Psychology",
      "Regression",
      "Representation (politics)",
      "Social science",
      "Sociology",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Sabokrou",
        "given_name": "Mohammad"
      },
      {
        "surname": "Pourreza",
        "given_name": "Masoud"
      },
      {
        "surname": "Li",
        "given_name": "Xiaobai"
      },
      {
        "surname": "Fathy",
        "given_name": "Mahmood"
      },
      {
        "surname": "Zhao",
        "given_name": "Guoying"
      }
    ]
  },
  {
    "title": "Prediction of stock price direction using a hybrid GA-XGBoost algorithm with a three-stage feature engineering process",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115716",
    "abstract": "The stock market has performed one of the most important functions in a laissez-faire economic system by gathering people, companies, and flows of money for several centuries. There have been numerous studies on the stock market among researchers to predict stock prices, and a growing number of studies employed machine learning or deep learning techniques on the stock market predictions with the advent of big data and the rapid development of artificial intelligence techniques. However, making accurate predictions of stock price direction remains difficult because stock prices are inherently complex, nonlinear, nonstationary, and sometimes too irrational to be predictable. Despite the wealth of information, previous prediction systems often overlooked key indicators and the importance of feature engineering. This study proposes a hybrid GA-XGBoost prediction system with an enhanced feature engineering process consisting of feature set expansion, data preparation, and optimal feature set selection using the hybrid GA-XGBoost algorithm. This study experimentally verifies the importance of feature engineering process in stock price direction prediction by comparing obtained feature sets to original dataset as well as improving prediction performance to outperform benchmark models. Specifically, the most significant accuracy increment comes from feature expansion that adds 67 technical indicators to the original historical stock price data. This study also produces a parsimonious optimal feature set using the GA-XGBoost algorithm that can achieve the desired performance with substantially fewer features. Consequently, this study empirically proves that a successful prediction performance largely depends on a deliberate combination of feature engineering processes with a baseline learning model to make a good balance and harmony between the curse of dimensionality and the blessing of dimensionality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010988",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Engineering",
      "Feature (linguistics)",
      "Feature engineering",
      "Linguistics",
      "Machine learning",
      "Mechanical engineering",
      "Operating system",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Series (stratigraphy)",
      "Stage (stratigraphy)",
      "Stock (firearms)",
      "Stock price"
    ],
    "authors": [
      {
        "surname": "Yun",
        "given_name": "Kyung Keun"
      },
      {
        "surname": "Yoon",
        "given_name": "Sang Won"
      },
      {
        "surname": "Won",
        "given_name": "Daehan"
      }
    ]
  },
  {
    "title": "TrafficBERT: Pre-trained model with large-scale data for long-range traffic flow forecasting",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115738",
    "abstract": "Traffic flow prediction has various applications such as in traffic systems and autonomous driving. Road conditions have become increasingly complex, and this, in turn, has increased the demand for effective traffic volume predictions. Statistical models and conventional machine-learning models have been employed for this purpose more recently, deep learning has been widely used. However, most deep learning-based models require data additional to traffic information, such as information on adjacent roads or road weather conditions. Therefore, the effectiveness of these models is typically restricted to certain roads. Even if such information were available, there is a possibility of bias toward a specific road. To overcome this limitation, based on the bidirectional encoder representations from transformers (BERT), we propose trafficBERT, a model that is suitable for use on various roads because it is pre-trained with large-scale traffic data. Our model captures time-series information by employing multi-head self-attention in place of the commonly used recurrent neural network. In addition, the autocorrelation between the states before and after each time step is determined more efficiently via factorized embedding parameterization. Our results indicate that trafficBERT outperforms models trained using data for specific roads, as well as commonly used statistical and deep learning models, such as Stacked Autoencoder, and models based on long short-term memory, in terms of accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011179",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Composite material",
      "Computer science",
      "Computer security",
      "Data mining",
      "Deep learning",
      "Embedding",
      "Generality",
      "Machine learning",
      "Materials science",
      "Psychology",
      "Psychotherapist",
      "Range (aeronautics)",
      "Traffic flow (computer networking)"
    ],
    "authors": [
      {
        "surname": "Jin",
        "given_name": "KyoHoon"
      },
      {
        "surname": "Wi",
        "given_name": "JeongA"
      },
      {
        "surname": "Lee",
        "given_name": "EunJu"
      },
      {
        "surname": "Kang",
        "given_name": "ShinJin"
      },
      {
        "surname": "Kim",
        "given_name": "SooKyun"
      },
      {
        "surname": "Kim",
        "given_name": "YoungBin"
      }
    ]
  },
  {
    "title": "Is the suggested food your desired?: Multi-modal recipe recommendation with demand-based knowledge graph",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115708",
    "abstract": "Personalized recipe recommender systems help users mine certain dishes they want to find and even really desire, which play a significant role in matching dishes, balancing nutrients, and preventing non-communicable diseases. Generally, customer’s preferences or needs vary from person to person, and people are often reluctant to accept recommended food without reasonable explanation, especially when their demands are not explicitly addressed. In this paper, we are devoted to providing recipe suggestions accompanied by rational interpretations generated from images or videos. First, we construct a recipe knowledge graph (RcpKG) through the use of multi-modality and hierarchical thought, which focuses on the underlying demands of users and the consideration of multiple fine-grained factors. On this basis, a novel multi-modal recipe recommendation method via the knowledge graph (RcpMKR) is proposed, which represents nodes in multiple aspects and performs multi-relational graph structure extraction of the RcpKG. It not only takes into account local associations within the graph but also global information, and incorporates user concerns at different levels. Then, we adopt BERT-based multi-modal models and generative adversarial networks to generate interpretations. Additionally, dynamic convolution and random synthetic attention are utilized in our work to discriminate among features. Experimental results show that the proposed method and BERT-based fusion models improve recipe recommendation performance and explanation generation. Specifically, the precision of the RcpMKR method through RcpKG, user concerns and graph convolutional network improves by 7.82%, and the viExpCBTBERT method via 2D&3D convolutional neural networks for developing text interpretations enhances the F1-score by 10% compared with the baseline.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010903",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Construct (python library)",
      "Convolutional neural network",
      "Food science",
      "Graph",
      "Machine learning",
      "Matching (statistics)",
      "Mathematics",
      "Modal",
      "Polymer chemistry",
      "Programming language",
      "Recipe",
      "Recommender system",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Lei",
        "given_name": "Zhenfeng"
      },
      {
        "surname": "Ul Haq",
        "given_name": "Anwar"
      },
      {
        "surname": "Zeb",
        "given_name": "Adnan"
      },
      {
        "surname": "Suzauddola",
        "given_name": "Md"
      },
      {
        "surname": "Zhang",
        "given_name": "Defu"
      }
    ]
  },
  {
    "title": "The similarities and divergences between grey and fuzzy theory",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115812",
    "abstract": "This article aims to provide the reader with a clearer understanding of the similarities and the divergences that exist between grey and fuzzy theory. The notion of fuzzy is considerably more established to that of grey, however, in recent years grey theory has garnered traction and has become more noticeable to wider audiences. Both can be seen as being proponents of uncertainty modelling, within their repertoires there are indeed facets that are similar in nature, and it is precisely these considerations that will often cause confusion. As grey theory generates more popularity, there will evidently be a need to address the issues regarding the similarities. The differences that exist in some cases are intrinsically subtle, hence the need for a definitive explanation. They are two distinctly separate paradigms, however, one could be forgiven for assuming that grey theory utilises fuzzy notions, simply presented with a different name. This article will aim to provide the reader with a coherent pairwise comparison of instances where similarities occur, describing the perspective adopted from both approaches. What will be covered will include the differences that exist between an interval-valued fuzzy set to that of a grey set; interval and grey numbers — both the discrete and the generalised grey number. Also highlighted will be the links to that of probability theory; imprecise and precise; belief and possibility functions. In describing the differences one will also gain an understanding of the similarities that indeed exist between the two.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011805",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Confusion",
      "Epistemology",
      "Fuzzy logic",
      "Fuzzy set",
      "Gray (unit)",
      "Mathematical economics",
      "Mathematics",
      "Medicine",
      "Pairwise comparison",
      "Perspective (graphical)",
      "Philosophy",
      "Popularity",
      "Programming language",
      "Psychoanalysis",
      "Psychology",
      "Radiology",
      "Set (abstract data type)",
      "Social psychology",
      "Vagueness"
    ],
    "authors": [
      {
        "surname": "Khuman",
        "given_name": "Arjab Singh"
      }
    ]
  },
  {
    "title": "Fully component selection: An efficient combination of feature selection and principal component analysis to increase model performance",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115678",
    "abstract": "Principal component analysis (PCA) is one the most effective and widely used dimensionality-reduction techniques which aggregates the maximal variance in the first few components. In many applications, the first few components of PCA are used in place of the original variables in statistical and machine learning models; nevertheless, such use does not guarantee the selection of the most relevant components to the target variable. This paper presents an efficient approach in which all components, rather than simply the first few, are considered as input for the random forest (RF) model, and a feature selection algorithm is integrated with the RF to select the most relevant components, hence called fully component selection (FCS). The proposed method was evaluated on spectroscopic data comprising 70 soil samples with 2050 spectral bands to estimate soil organic carbon (SOC). The FCS approach was compared to the RF model once considering the original features, then again using only the first few components, and again using all components without feature selection. The results showed that RF-FCS substantially outperformed other approaches, such that the R2 was increased between 25.2% and 55.5%. Furthermore, the findings indicated that the most relevant principal components (PCs) to the target variable were not the first few, but PC6, PC7, PC8, PC15, and PC43. Using the FCS approach increased model performance substantively, and coupling it with machine learning and statistical models is strongly recommended for high dimensional data applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010642",
    "keywords": [
      "Artificial intelligence",
      "Component (thermodynamics)",
      "Computer science",
      "Curse of dimensionality",
      "Data mining",
      "Dimensionality reduction",
      "Feature (linguistics)",
      "Feature selection",
      "Independent component analysis",
      "Linguistics",
      "Machine learning",
      "Model selection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Principal component analysis",
      "Random forest",
      "Selection (genetic algorithm)",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Shafizadeh-Moghadam",
        "given_name": "Hossein"
      }
    ]
  },
  {
    "title": "A hierarchical multi-agent architecture based on virtual identities to explain black-box personalization policies",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115731",
    "abstract": "Hyper-personalization policies entail a considerable improvement regarding previous personalization approaches. However, they present several issues that need to be addressed, such as minimal explainability and privacy invasion. A hierarchical Multi-Agent System (MAS) is presented in this work to provide a solution to these concerns. The system is formulated as a hybrid approach, where some of the agents work autonomously, while the user input triggers the remaining. At the autonomous level, a set of Virtual Identities (VIs) representing different user profiles interact with Black-Box Hyper-Personalization Online Systems (BBHOS), gathering a set of targeted responses. Associative patterns and profile aggregations can then be inferred from the analysis of these responses. In the user-triggered level, the real user is virtualized as an identity that represents their features. The virtual identity serves as an intermediary between the personalization system and the real user. This virtualization hinders the personalization service from extracting sensitive contextual information about the real user, protecting their privacy. The results obtained by the user identity on its interaction with the personalization service are then analyzed, adjusting the content of the response to fit the user’s requests instead of their features. A use case on the functioning of the analysis of search engines is presented to illustrate the complete behavior of the proposed architecture.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011118",
    "keywords": [
      "Acoustics",
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Black box",
      "Computer science",
      "Economics",
      "Economy",
      "Human–computer interaction",
      "Identity (music)",
      "Operating system",
      "Personalization",
      "Physics",
      "Programming language",
      "Service (business)",
      "Set (abstract data type)",
      "User interface",
      "User modeling",
      "Visual arts",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Amador-Domínguez",
        "given_name": "Elvira"
      },
      {
        "surname": "Serrano",
        "given_name": "Emilio"
      },
      {
        "surname": "Manrique",
        "given_name": "Daniel"
      }
    ]
  },
  {
    "title": "A multi-objective simulation–optimization for a joint problem of strategic facility location, workforce planning, and capacity allocation: A case study in the Royal Australian Navy",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115751",
    "abstract": "The availability of assets crucially depends on the interplay between the facility locations, workforce planning, and capacity allocation. Therefore, in this paper, we model and solve a novel multi-objective optimization problem for strategic facility location, workforce planning, and capacity allocation in the context of the military. The developed model deviates from more common approaches and adapts a dynamic capacity scheme for both the workforce and facilities to capture the dynamic nature of future demand, such as maintenance requirements for assets. The capacity allocation and workforce planning problem focus on mainly strategic decisions involving from workforce and facilities. A system dynamics (SD) simulation model enables a decision-maker to analyze the effects of decision variables by modeling the system complexities, uncertainties, and interactions between facilities, workforce, and assets. However, the simulation model neither suggests nor seeks the best solution strategy or strategies. To overcome this shortcoming, we propose a simulation–optimization approach that uses a Non-dominated Sorting Genetic Algorithm-II (NSGA-II). NSGA-II generates feasible solution strategies (candidate solutions) such as time and amount of capacity expansion and downsizing for both the workforce and facilities as well as the amount of crew recruitment. These solution strategies are fed to the simulation model where it evaluates the fitness of candidate solutions. We test the applicability of the proposed method on a realistic case study from the Royal Australian Navy. Finally, we present scenario-based sensitivity analysis arising from the decision variables to support decision-makers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011271",
    "keywords": [
      "Archaeology",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Economic growth",
      "Economics",
      "Engineering",
      "Genetic algorithm",
      "History",
      "Machine learning",
      "Multi-objective optimization",
      "Navy",
      "Operations research",
      "Paleontology",
      "Programming language",
      "Sorting",
      "Workforce",
      "Workforce management",
      "Workforce planning"
    ],
    "authors": [
      {
        "surname": "Turan",
        "given_name": "Hasan Hüseyin"
      },
      {
        "surname": "Kahagalage",
        "given_name": "Sanath Darshana"
      },
      {
        "surname": "Jalalvand",
        "given_name": "Fatemeh"
      },
      {
        "surname": "El Sawah",
        "given_name": "Sondoss"
      }
    ]
  },
  {
    "title": "A new wrapper feature selection method for language-invariant offline signature verification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115756",
    "abstract": "Among various biometric systems, an offline signature verification system has been widely used in all fields such as in banks, educational institutes, legal procedures and, criminal investigation where authentication and verification are utmost required. Despite the popularity of the online signature verification system, its offline counterpart still has great importance in developing countries, especially in rural areas, where easy availability of smart devices along with fast internet connection is not available. In this work, we have developed a language invariant offline signature verification model which is almost equally applicable for both writer dependent and writer independent scenarios. At first, an offline signature is collected as an image, following which a corresponding signal is generated using singular value decomposition. Then four different kinds of features namely, statistical, shape-based, similarity-based, and frequency-based are extracted from the transformed signal of the signature image. Next, to reduce the feature dimension, we have designed a novel wrapper feature selection method based on Red Deer Algorithm, a recently proposed meta-heuristic method, to keep only the relevant features to be used during signature authentication and verification process. Finally, a confidence score from the Naïve Bayes classifier has been used to perform the authentication and verification process. Our model has been evaluated on CEDAR (English), UTSig (Persian), Sigcomp 2011 Dutch, Sigcomp 2011 Chinese, and SigWIcomp 2015 Bengali signature datasets. Obtained results confirm that the proposed model can outperform many of its predecessors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011313",
    "keywords": [
      "Artificial intelligence",
      "Authentication (law)",
      "Biometrics",
      "Classifier (UML)",
      "Computer science",
      "Computer security",
      "Data mining",
      "Feature extraction",
      "Feature selection",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Naive Bayes classifier",
      "Pattern recognition (psychology)",
      "Signature (topology)",
      "Singular value decomposition",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Banerjee",
        "given_name": "Debanshu"
      },
      {
        "surname": "Chatterjee",
        "given_name": "Bitanu"
      },
      {
        "surname": "Bhowal",
        "given_name": "Pratik"
      },
      {
        "surname": "Bhattacharyya",
        "given_name": "Trinav"
      },
      {
        "surname": "Malakar",
        "given_name": "Samir"
      },
      {
        "surname": "Sarkar",
        "given_name": "Ram"
      }
    ]
  },
  {
    "title": "Multimodel anomaly detection on spatio-temporal logistic datastream with open anomaly detection architecture",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115755",
    "abstract": "Logistic companies rely on heavily Vehicle Tracking Systems (VTS) to provide instant and valuable data about the vehicle’s condition, the cargoes’ statuses, and the trips. The data that is collected from the vehicles and drivers are generally not fully utilized to its full potential other than day-to-day business activities because of the difficulties created by the big data aspects of the logistic data in storing, processing, analyzing, and reporting which require expertise and infrastructure. Real-time analysis of the data coming from VTS and mobile devices provides enormous value in terms of businesses. In this research, by using real-world data supplied by an international logistic company, we have established a multimodel streaming analytic framework that detects trajectory anomalies and identifies drivers in parallel and near-real-time. The framework can be enhanced with any number of trajectory or driver identification models, all of which can be run in parallel and contribute to the overall anomaly detection process by the “consensus” of the included models. In this study, we provide a sample implementation of the framework with one trajectory and one behavior anomaly detection model suitable for logistic spatio-temporal datastream. Provided models do not require any business-specific data and only depend on the basic VTS and location data. An isolation-based model is used for trajectory anomaly detection which does not need the source and the destination while identifying sub-trajectories. The outputs of both models are combined on a dashboard which shows the vehicles’ abnormal sub-trajectories along with the notification of whether or not the vehicles are being driven by the assigned driver.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011301",
    "keywords": [
      "Anomaly (physics)",
      "Anomaly detection",
      "Astronomy",
      "Big data",
      "Biology",
      "Botany",
      "Computer science",
      "Condensed matter physics",
      "Dashboard",
      "Data mining",
      "Data science",
      "Identification (biology)",
      "Operating system",
      "Physics",
      "Process (computing)",
      "Real-time computing",
      "Trajectory"
    ],
    "authors": [
      {
        "surname": "Oktay",
        "given_name": "Talha"
      },
      {
        "surname": "Yoğurtçuoğlu",
        "given_name": "Erdenay"
      },
      {
        "surname": "Sarıkaya",
        "given_name": "Ramazan Nejdet"
      },
      {
        "surname": "Karaca",
        "given_name": "Ali Recep"
      },
      {
        "surname": "Kömürcü",
        "given_name": "Mehmet Fırat"
      },
      {
        "surname": "Sayar",
        "given_name": "Ahmet"
      }
    ]
  },
  {
    "title": "Deep learning with multiple scale attention and direction regularization for asset price prediction",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115796",
    "abstract": "Forecasting the stock price is a challenging task due to its complex dynamic behaviors, affected by long-term trends, seasonal changes, cyclical changes, and irregular changes. Although many deep learning techniques have been applied to stock price forecasting, few of them have a deep insight into these complex behaviors. In this work, we propose a four-step hybrid model, named ESTA-Net, to adaptively extract these behavior patterns for stock price forecasting. Firstly, the empirical mode decomposition is applied to decompose a closing price sequence into intrinsic mode functions (IMFs). The goal of this step is to extract multiple quasi-stationary features of different time scales from the historical closing price sequence. Secondly, each IMF is modeled and forecasted by a temporal attention long short term memory (TALSTM) network. The TALSTM network is designed to capture the long term dependency of each IMF. Thirdly, the learned deep representations of IMFs are fed into a scale attention network (SANet), which adaptively selects relevant deep representations of multiple time scale features extracted from the historical price sequence. Finally, these learned deep features are fed into a fully connected layer to predict the future closing price. In addition, to make the proposed model learn the movement direction of the closing price, we propose a novel regularization term, i.e. the direction regularization term, to train the proposed model. This regularization term measures the inconsistency between the predicted movement direction and the actual movement direction of the closing price. Experiments show that the proposed model significantly outperforms benchmark models. Particularly, on seven financial market indices, the proposed model with the direction regularization term achieves the highest POCID (32.04% higher than that of CNN) and the lowest MAPE (37.36% lower than that of DA-RNN).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011647",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Closing (real estate)",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Econometrics",
      "Economics",
      "Filter (signal processing)",
      "Finance",
      "Hilbert–Huang transform",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Paleontology",
      "Regularization (linguistics)",
      "Sequence learning",
      "Series (stratigraphy)",
      "Sliding window protocol",
      "Stock price",
      "Window (computing)"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Fucui"
      },
      {
        "surname": "Tan",
        "given_name": "Shan"
      }
    ]
  },
  {
    "title": "Identifying complaints based on semi-supervised mincuts",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115668",
    "abstract": "In today’s competitive business world, customer service is often at the heart of businesses that can help strengthen their brands. Customer complaint resolution in a timely and efficient manner is key to improving customer satisfaction. Moreover, customers’ complaints play important roles in identifying their requirements which offer a starting point for effective and efficient planning of the company’s overall R&D and new product or service development activities. That said, businesses face challenges towards automatically identifying complaints buried deep in massive online content. In this paper, we propose a graph-based semi-supervised learning paradigm leveraging syntactic and semantic representations of tweets. Intrinsic evaluation results on a benchmark dataset illustrate that the proposed approach outperforms state-of-the-art supervised non-graph based classification models for solving the complaints identification task, and confirms the efficacy of the proposed approach. Experimental results also show that the performance of the state-of-the-art supervised complaint classification model trained over hand-crafted features extracted from several linguistic resources can be reached with less than 50% of the training data with the proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010587",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Business",
      "Complaint",
      "Computer science",
      "Customer satisfaction",
      "Economics",
      "Geodesy",
      "Geography",
      "Graph",
      "Information retrieval",
      "Law",
      "Machine learning",
      "Management",
      "Marketing",
      "Natural language processing",
      "Political science",
      "Task (project management)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Singh",
        "given_name": "Apoorva"
      },
      {
        "surname": "Saha",
        "given_name": "Sriparna"
      },
      {
        "surname": "Hasanuzzaman",
        "given_name": "Mohammed"
      },
      {
        "surname": "Jangra",
        "given_name": "Anubhav"
      }
    ]
  },
  {
    "title": "Brain network topology unraveling epilepsy and ASD Association: Automated EEG-based diagnostic model",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115762",
    "abstract": "The co-occurrence of Autism Spectrum Disorder and Epilepsy, i.e., ASD + E is very common, however, the underlying mechanism associating both cohorts has not been unraveled quantitatively yet. Using resting-state EEG, the present work investigates whether brain functional topology can explain and differentiate ASD + E from ASD and E. A weighted visibility graph (VG) algorithm is employed to map brain EEG signals of 120 age-matched ASD (30), E (30), ASD + E (30) and Typically Developing (TD; 30) individuals into a complex network. Afterward, the complex graph metrics were evaluated to measure brain connectivity at local and global level. The proposed methodology is compared with state-of-art-methods in disorder detection. The statistical and probabilistic investigations have shown that ASD + E affects the brain network topology majorly in frontal, temporal, and parietal regions, but it shows a functional overlap with ASD (49%) and epilepsy (55%). Furthermore, the evaluation of the metrics using Support Vector Machine (SVM) classifier reflected that a combination of metrics (average weighted degree, local efficiency, characteristic path length, and eigenvector centrality) can distinguish the groups with an accuracy of 98.2%. The paper also demonstrated that variation in brain topology with maturation (from child (5–11 years) to adolescent (11–18 years)) is the reason for heterogeneity and unexplained behavioral abnormalities in affected individuals. In contrast to existing theoretical studies, the present study has quantitatively ruled out ASD + E condition by tracing variations in brain regions and network topology. In future, the studies can target the identified regions to detect evident clinical markers responsible for ASD, E and ASD + E as well.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011362",
    "keywords": [
      "Artificial intelligence",
      "Autism",
      "Autism spectrum disorder",
      "Combinatorics",
      "Computer science",
      "Connectome",
      "Developmental psychology",
      "Electroencephalography",
      "Epilepsy",
      "Functional connectivity",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Psychology",
      "Regular polygon",
      "Resting state fMRI",
      "Support vector machine",
      "Topology (electrical circuits)",
      "Visibility graph"
    ],
    "authors": [
      {
        "surname": "Wadhera",
        "given_name": "Tanu"
      }
    ]
  },
  {
    "title": "Natural Language Processing for the identification of Human factors in aviation accidents causes: An application to the SHEL methodology",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115694",
    "abstract": "Accidents in aviation are rare events. From them, aviation safety management systems take fast and effective remedy actions by performing the analysis of the root causes of accidents, most of which are proved to be human factors. Since the current standard relies on the manual classification performed by trained staff, there are no technical standards already defined for automated human factors identification. This paper considers this issue, proposing machine learning techniques by leveraging on the state-of-the-art technologies of Natural Language Processing. The techniques are then adapted to the Software Hardware Environment Liveware (SHEL) standard accident causality model and tested on a set of real accidents. The computational results show the accuracy and effectiveness of the proposed methodology. Furthermore, the application of the methodology to real documents checked by experts estimates a reduction of the time needed for at least 30% compared to the standard methods of human factors identification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010782",
    "keywords": [
      "Aerospace engineering",
      "Artificial intelligence",
      "Aviation",
      "Aviation accident",
      "Aviation safety",
      "Biology",
      "Botany",
      "Computer science",
      "Engineering",
      "Human error",
      "Identification (biology)",
      "Machine learning",
      "Medicine",
      "Programming language",
      "Risk analysis (engineering)",
      "Set (abstract data type)",
      "Software"
    ],
    "authors": [
      {
        "surname": "Perboli",
        "given_name": "Guido"
      },
      {
        "surname": "Gajetti",
        "given_name": "Marco"
      },
      {
        "surname": "Fedorov",
        "given_name": "Stanislav"
      },
      {
        "surname": "Giudice",
        "given_name": "Simona Lo"
      }
    ]
  },
  {
    "title": "Application of modified Black-Litterman model for active portfolio management",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115719",
    "abstract": "An active policy for portfolio optimization is developed based on repetitive application of modified Black-Litterman (BL) portfolio model and new formal definition of the expert views. New subjective views are defined which are based on the differences between the historical mean asset returns and their implied return values. An algorithm for the implementation of active management with the modified BL model is derived. The active management policy allows using short time series of historical data of assets, providing portfolio optimization with limited set of assets. New market point is evaluated, because the small set of assets does not allow market index to be used as characteristics of the market. The new formalization of the expert views allows to be compared the Mean Variance and BL portfolios on common basis. The experiments and comparisons between the Mean Variance optimization and the modified BL problem give advantages to the last one.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011015",
    "keywords": [
      "Accounting",
      "Active management",
      "Asset (computer security)",
      "Asset allocation",
      "Black–Litterman model",
      "Computer science",
      "Computer security",
      "Econometrics",
      "Economics",
      "Financial economics",
      "Index (typography)",
      "Management",
      "Mathematical optimization",
      "Mathematics",
      "Portfolio",
      "Portfolio optimization",
      "Programming language",
      "Project management",
      "Project portfolio management",
      "Replicating portfolio",
      "Set (abstract data type)",
      "Variance (accounting)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Stoilov",
        "given_name": "Todor"
      },
      {
        "surname": "Stoilova",
        "given_name": "Krasimira"
      },
      {
        "surname": "Vladimirov",
        "given_name": "Miroslav"
      }
    ]
  },
  {
    "title": "Short-term load forecasting with dense average network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115748",
    "abstract": "As an important part of the power system, power load forecasting directly affects the national economy. Small improvements in power load forecasts can save millions of dollars for the power industry. Therefore, improving the accuracy of power load forecasting has always been the pursuing goal for a power system. Based on this goal, this paper proposes a novel connection, the dense average connection, in which the outputs of all preceding layers are averaged as the input of the next layer in a feed-forward fashion. Dense average connection can alleviate the problem of gradient explosion without introducing new parameters. Based on dense average connection, we construct the dense average network (DaNet) for power load forecasting. On two public datasets (ISO-NE dataset and NAU dataset), we use MAPE, MAE and RMSE to evaluate the performance of DaNet. The predictions of DaNet are better than those of existing benchmarks. On this basis, this paper uses the ensemble method to reduce the peak value of prediction bias, which helps to alleviate the dispatching problem caused by unexpected loads. To verify the reliability of the model predictions, the robustness is analyzed and verified by adding input disturbances. The experimental results show that the proposed model is effective and robust for power load forecasting.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011258",
    "keywords": [
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Electric power system",
      "Engineering",
      "Gene",
      "Mathematics",
      "Mean squared error",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Reliability (semiconductor)",
      "Reliability engineering",
      "Robustness (evolution)",
      "Statistics",
      "Term (time)"
    ],
    "authors": [
      {
        "surname": "Liao",
        "given_name": "Zhifang"
      },
      {
        "surname": "Pan",
        "given_name": "Haihui"
      },
      {
        "surname": "Huang",
        "given_name": "Xuechun"
      },
      {
        "surname": "Mo",
        "given_name": "Ronghui"
      },
      {
        "surname": "Fan",
        "given_name": "Xiaoping"
      },
      {
        "surname": "Chen",
        "given_name": "Huanwen"
      },
      {
        "surname": "Liu",
        "given_name": "Limin"
      },
      {
        "surname": "Li",
        "given_name": "Yan"
      }
    ]
  },
  {
    "title": "A grid anchor based cropping approach exploiting image aesthetics, geometric composition, and semantics",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115852",
    "abstract": "Image cropping aims at the selection of the relevant part of an image maximizing its aesthetic quality and composition. The part of the image that needs to be removed is highly dependent on user preferences and can be related to image aesthetics, composition, informativeness, or other criteria. Since the concept of the perfect crop does not exist, but there are several cropping possibilities, recent cropping algorithms are trained to rank a set of crop candidates based on their compositional quality. To this end, several benchmark databases have been released that provide for each image a series of human-annotated crop candidates with corresponding scores. Many of the image cropping methods rely on a single criterion to define the best crop or crops in an image. However, a single criterion misses the complexity of human opinions which can differ in personal preferences and backgrounds. Motivated by this, we formulate the cropping problem as a ranking problem of candidate crop regions using a grid anchor based approach and multiple criteria. To evaluate the goodness of a crop region, we design a cropping method by combining three efficient and lightweight neural networks specifically designed to evaluate the quality of a crop in terms of aesthetics, composition, and semantics. Our results on standard datasets show that using more criteria yields better crops than state-of-the-art approaches. This result is also confirmed by a subjective study on user preferences that involved a panel of users.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421012112",
    "keywords": [
      "Agricultural engineering",
      "Agriculture",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Combinatorics",
      "Computer science",
      "Cropping",
      "Database",
      "Ecology",
      "Engineering",
      "Epistemology",
      "Geodesy",
      "Geography",
      "Geometry",
      "Grid",
      "Image (mathematics)",
      "Mathematics",
      "Philosophy",
      "Programming language",
      "Quality (philosophy)",
      "Rank (graph theory)",
      "Ranking (information retrieval)",
      "Selection (genetic algorithm)",
      "Semantics (computer science)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Celona",
        "given_name": "Luigi"
      },
      {
        "surname": "Ciocca",
        "given_name": "Gianluigi"
      },
      {
        "surname": "Napoletano",
        "given_name": "Paolo"
      }
    ]
  },
  {
    "title": "Automatic detection of user trajectories from social media posts",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115733",
    "abstract": "Social media represents a rich environment to collect huge amounts of data containing useful information about people’s behaviors and interactions. In particular, such information has been widely exploited for analyzing the mobility of people, as geotagged social media posts allow to extract accurate patterns on movements of people. This paper presents AUDESOME (AUtomatic Detection of user trajEctories from SOcial MEdia), an automatic method for discovering user mobility patterns from social media posts. In particular, the method includes two new unsupervised algorithms: ( i ) a text mining algorithm, which analyzes social media posts to automatically extract the main keywords identifying the Places-of-Interest (PoI) in a given area; and ( i i ) a geospatial clustering algorithm, which detects the Regions-of-Interest (RoIs) by using both geotagged posts and extracted keywords. We experimentally evaluated the performance of AUDESOME taking into account the following aspects: identification of keywords, detection of RoIs, and extraction of user trajectories. The experiments, performed on a real dataset containing about 3 million of geotagged items published in Flickr, demonstrate that AUDESOME achieves better results than existing techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011131",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Geology",
      "Geospatial analysis",
      "Identification (biology)",
      "Information retrieval",
      "Remote sensing",
      "Social media",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Belcastro",
        "given_name": "Loris"
      },
      {
        "surname": "Marozzo",
        "given_name": "Fabrizio"
      },
      {
        "surname": "Perrella",
        "given_name": "Emanuele"
      }
    ]
  },
  {
    "title": "A grid anchor based cropping approach exploiting image aesthetics, geometric composition, and semantics",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115852",
    "abstract": "Image cropping aims at the selection of the relevant part of an image maximizing its aesthetic quality and composition. The part of the image that needs to be removed is highly dependent on user preferences and can be related to image aesthetics, composition, informativeness, or other criteria. Since the concept of the perfect crop does not exist, but there are several cropping possibilities, recent cropping algorithms are trained to rank a set of crop candidates based on their compositional quality. To this end, several benchmark databases have been released that provide for each image a series of human-annotated crop candidates with corresponding scores. Many of the image cropping methods rely on a single criterion to define the best crop or crops in an image. However, a single criterion misses the complexity of human opinions which can differ in personal preferences and backgrounds. Motivated by this, we formulate the cropping problem as a ranking problem of candidate crop regions using a grid anchor based approach and multiple criteria. To evaluate the goodness of a crop region, we design a cropping method by combining three efficient and lightweight neural networks specifically designed to evaluate the quality of a crop in terms of aesthetics, composition, and semantics. Our results on standard datasets show that using more criteria yields better crops than state-of-the-art approaches. This result is also confirmed by a subjective study on user preferences that involved a panel of users.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421012112",
    "keywords": [
      "Agricultural engineering",
      "Agriculture",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Combinatorics",
      "Computer science",
      "Cropping",
      "Database",
      "Ecology",
      "Engineering",
      "Epistemology",
      "Geodesy",
      "Geography",
      "Geometry",
      "Grid",
      "Image (mathematics)",
      "Mathematics",
      "Philosophy",
      "Programming language",
      "Quality (philosophy)",
      "Rank (graph theory)",
      "Ranking (information retrieval)",
      "Selection (genetic algorithm)",
      "Semantics (computer science)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Celona",
        "given_name": "Luigi"
      },
      {
        "surname": "Ciocca",
        "given_name": "Gianluigi"
      },
      {
        "surname": "Napoletano",
        "given_name": "Paolo"
      }
    ]
  },
  {
    "title": "COVID-19 and other viruses: Holding back its spreading by massive testing",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115710",
    "abstract": "The experience of Singapore and South Korea makes it clear that under certain circumstances massive testing is an effective way for containing the advance of the COVID-19. In this paper, we propose a modified SEIR model which takes into account tracing and massive testing, proving theoretically that more tracing and testing implies a reduction of the total number of infected people in the long run. We apply this model to the spreading of the first wave of the disease in Spain, obtaining numerical results. After that, we introduce a heuristic approach in order to minimize the COVID-19 spreading by planning effective test distributions among the populations of a region over a period of time. As an application, the impact of distributing tests among the counties of New York according to this method is computed in terms of the number of saved infected individuals.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010927",
    "keywords": [
      "2019-20 coronavirus outbreak",
      "Artificial intelligence",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Disease",
      "Economics",
      "Finance",
      "Geometry",
      "Heuristic",
      "Infectious disease (medical specialty)",
      "Mathematics",
      "Medicine",
      "Operating system",
      "Order (exchange)",
      "Outbreak",
      "Pathology",
      "Reduction (mathematics)",
      "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)",
      "Tracing",
      "Virology"
    ],
    "authors": [
      {
        "surname": "Sainz-Pardo",
        "given_name": "José L."
      },
      {
        "surname": "Valero",
        "given_name": "José"
      }
    ]
  },
  {
    "title": "Two-stage convolutional neural network for road crack detection and segmentation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115718",
    "abstract": "Automatic detection of road cracks is an important task to support road inspection for transport infrastructure. Various methods have been proposed for road crack detection and segmentation, however, there is no established method for handling real road images that are noisy and of low quality. In this paper, a new method utilising a two-stage convolutional neural network (CNN) is proposed for road crack detection and segmentation in images at the pixel level. Our novel contribution is a framework where the first stage serves to remove noise or artifacts and isolate the potential cracks to a small area, and the second stage is able to learn the context of cracks in the detected area. This is hence more effective than learning over the entire original noisy image. Extensive experiments on real datasets including public sources and our collected dataset have been conducted. The experimental results show that the two-stage CNN model outperformed existing approaches, especially for noisy, low-resolution images, and imbalanced datasets. Our approach achieves an F1-measure of over 0.91 on three datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011003",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Convolutional neural network",
      "Deep learning",
      "Economics",
      "Image (mathematics)",
      "Management",
      "Noise (video)",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Pixel",
      "Segmentation",
      "Stage (stratigraphy)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Nguyen",
        "given_name": "Nhung Hong Thi"
      },
      {
        "surname": "Perry",
        "given_name": "Stuart"
      },
      {
        "surname": "Bone",
        "given_name": "Don"
      },
      {
        "surname": "Le",
        "given_name": "Ha Thanh"
      },
      {
        "surname": "Nguyen",
        "given_name": "Thuy Thi"
      }
    ]
  },
  {
    "title": "Explaining anomalies detected by autoencoders using Shapley Additive Explanations",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115736",
    "abstract": "Deep learning algorithms for anomaly detection, such as autoencoders, point out the outliers, saving experts the time-consuming task of examining normal cases in order to find anomalies. Most outlier detection algorithms output a score for each instance in the database. The top-k most intense outliers are returned to the user for further inspection; however, the manual validation of results becomes challenging without justification or additional clues. An explanation of why an instance is anomalous enables the experts to focus their investigation on the most important anomalies and may increase their trust in the algorithm. Recently, a game theory-based framework known as SHapley Additive exPlanations (SHAP) was shown to be effective in explaining various supervised learning models. In this paper, we propose a method that uses Kernel SHAP to explain anomalies detected by an autoencoder, which is an unsupervised model. The proposed explanation method aims to provide a comprehensive explanation to the experts by focusing on the connection between the features with high reconstruction error and the features that are most important in terms of their affect on the reconstruction error. We propose a black-box explanation method, because it has the advantage of being able to explain any autoencoder without being aware of the exact architecture of the autoencoder model. The proposed explanation method extracts and visually depicts both features that contribute the most to the anomaly and those that offset it. An expert evaluation using real-world data demonstrates the usefulness of the proposed method in helping domain experts better understand the anomalies. Our evaluation of the explanation method, in which a “perfect” autoencoder is used as the ground truth, shows that the proposed method explains anomalies correctly, using the exact features, and evaluation on real-data demonstrates that (1) our explanation model, which uses SHAP, is more robust than the Local Interpretable Model-agnostic Explanations (LIME) method, and (2) the explanations our method provides are more effective at reducing the anomaly score than other methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011155",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Autoencoder",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Machine learning",
      "Outlier",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Antwarg",
        "given_name": "Liat"
      },
      {
        "surname": "Miller",
        "given_name": "Ronnie Mindlin"
      },
      {
        "surname": "Shapira",
        "given_name": "Bracha"
      },
      {
        "surname": "Rokach",
        "given_name": "Lior"
      }
    ]
  },
  {
    "title": "Solving the battery swap station location-routing problem with a mixed fleet of electric and conventional vehicles using a heuristic branch-and-price algorithm with an adaptive selection scheme",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115683",
    "abstract": "In this paper, a battery swap station location and routing problem with time windows and a mixed fleet of electric and conventional vehicles (BSS–MF–LRPTW) is proposed. This problem is motivated by a real-life logistics application by extending the existing electric vehicle battery swap stations location routing problem (BSS–EV–LRP). The BSS–MF–LRPTW aims to simultaneously determine the locations of battery swap stations (BSSs) and the routing plan of a mixed fleet under the driving range, the load capacity limitation, and time windows. An integer programming (IP) model is developed for the proposed BSS–MF–LRPTW. As there are a large number of variables and complicating constraints of the IP model, we break it up into the master problem and the subproblem, based on Danzig–Wolfe decomposition. To enhance the tractability of the problem, we follow up with a heuristic branch-and-price algorithm with an adaptive selection scheme (HBP-ASS), which integrates the exact policy with a heuristic strategy. The HBP-ASS develops heuristic versions of the dynamic programming algorithm by designing seven operators for heuristic label extension and dominance. An adaptive selection scheme is presented to decide which operator is employed. The performance of the proposed HBP-ASS is evaluated based on an extensive computational study. The results show that the HBP-ASS can obtain the exact solution to small-scale instances much more quickly than commercial branch-and-bound/cut solvers such as CPLEX. Also, for all tested cases, the HBP-ASS can find better solutions to large-scale instances within a shorter time than the existing heuristics – adaptive large neighborhood search. Furthermore, sensitivity analyses are carried out to provide managerial insights.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742101068X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Economics",
      "Finance",
      "Heuristic",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Routing (electronic design automation)",
      "Scheme (mathematics)",
      "Selection (genetic algorithm)",
      "Swap (finance)",
      "Vehicle routing problem"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yanru"
      },
      {
        "surname": "Li",
        "given_name": "Decheng"
      },
      {
        "surname": "Zhang",
        "given_name": "Zongcheng"
      },
      {
        "surname": "Wahab",
        "given_name": "M.I.M."
      },
      {
        "surname": "Jiang",
        "given_name": "Yangsheng"
      }
    ]
  },
  {
    "title": "An attention-based CNN-BiLSTM hybrid neural network enhanced with features of discrete wavelet transformation for fetal acidosis classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115714",
    "abstract": "Cardiotocography (CTG) is widely used in fetal monitoring, especially in the diagnosis of fetal acidosis. However, the manual interpretation of CTG analysis may easily lead to a low diagnostic rate, usually caused by various subjective factors. In order to reduce misdiagnosis, we propose an attention-based CNN-BiLSTM hybrid neural network enhanced with features of discrete wavelet transformation (DWT) for fetal acidosis classification. A joint model of convolutional neural network (CNN) and bi-directional long short-term memory (BiLSTM) is established to capture the complex nonlinear spatial and temporal relations of fetal heart rate (FHR) signals. The attention mechanism is then adopted to focus on important input features. And DWT is used to obtain FHR signals transformation coefficient features in order to reduce overfitting. Two features are fused together to classify fetal acidosis. This study uses signals from the public databases of the CTU-UHB for evaluation. A ten different verifications yields average sensitivity (SE), specificity (SP), and quality index (QI) of 75.23%, 70.82% and 72.29%, respectively. Our approach achieves better experimental results than previous works. Moreover, Our hybrid model is an end to end one, with a much simpler DWT feature extraction. With the advent of the big data era, our hybrid model will have great advantages.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010964",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Gene",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Transformation (genetics)",
      "Wavelet"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Mujun"
      },
      {
        "surname": "Lu",
        "given_name": "Yaosheng"
      },
      {
        "surname": "Long",
        "given_name": "Shun"
      },
      {
        "surname": "Bai",
        "given_name": "Jieyun"
      },
      {
        "surname": "Lian",
        "given_name": "Wanmin"
      }
    ]
  },
  {
    "title": "Sperm Motility Analysis by using Recursive Kalman Filters with the smartphone based data acquisition and reporting approach",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115774",
    "abstract": "Semen analysis is currently performed by using two techniques. Visual assessment technique is manual observation based technique and strongly depends on the experiences of the observer. Therefore, the reliability of the results is skeptical. On the other hand, computer based expert systems are more consistent and reliable. However, they are very expensive systems, therefore, cannot be utilized in many laboratories. In this study, we proposed a hybrid expert system utilizing visual assessment environment with the computerized analyzing part to eliminate the disadvantages of each technique. In the proposed system, smartphone based data acquisition approach is used to provide more modular and practical expert system for the sperm analysis. The records are, then, transferred to the server to analyze by developed software. In this analyzing software, we proposed multi-stage hybrid analyzing approach in terms of video stabilization, sperm concentration and motility analysis. Each video was initially fixed by the Speed Up Robust Features based matching technique. Then, Kalman Filter was employed for sperm tracking. After tracking step, trajectories have been divided into 3 s length to prevent possible incorrect assignments due to sudden changes in sperm motions. In the experimental tests, we combined all trajectories obtained from a total of 18 videos of 6 different subjects. We clustered a total of 89438 trajectories into 4 cluster as fast progressive, progressive, non-progressive and immotile according to extracted seven features. In order to compare the results, we also analyzed the same semen sample in another expert system, SQA-Vision. The difference was measured 3.4% and 4.8% in the determination of total and motile sperm concentration, and 2.1%, 7.4%, 5.3% for progressive, non-progressive and immotile movement type analysis respectively. The significance and impact of the proposed system are capability of reporting more detailed results in a variety of situations and having more advantages than any expert systems utilized for sperm analysis in terms of portability, cost and modularity. Additionally, to the best of our knowledge, this is the first study reporting use of the smartphone in an expert system for the sperm analysis in terms of data acquisition and result reporting.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421011441",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Kalman filter",
      "Operating system",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Real-time computing",
      "Reliability (semiconductor)",
      "Software",
      "Tracking system"
    ],
    "authors": [
      {
        "surname": "Ilhan",
        "given_name": "Hamza Osman"
      },
      {
        "surname": "Yuzkat",
        "given_name": "Mecit"
      },
      {
        "surname": "Aydin",
        "given_name": "Nizamettin"
      }
    ]
  },
  {
    "title": "Novel implicit-trust-network-based recommendation methodology",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115709",
    "abstract": "Recommender systems are known for helping e-commerce and entertainment sits to provide excellent services into their clients by finding those interests in a short time as possible. Recently, researchers have studied on trust relationships of people for improving user-based recommender systems. Hence, this study proposed an implicit approach for constructing a trust-network, which contains two parts. i. constructing an incipient trust-network by examining Similarity, Confidence and Identical Opinion of users; ii. Reconstructing the incipient trust-network as a telic trust-network by appraising its precision for predicting unknown-items rating. The study also presented methods for determining each of the similarity, the confidence, and the identical opinion. Furthermore, this study presented a recommendation strategy by merging two ways of ranking items based on their predicted rating and users' score to them, considering the trust-network. The study was evaluated on datasets of Ciao and FilmTrust for the trust-network correctness, the predicted-ratings error, and the quality of the recommendations. The results have shown, the scheme of this study has significantly improved the performance compared to some state-of-the-art techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010915",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Correctness",
      "Data mining",
      "Epistemology",
      "Image (mathematics)",
      "Information retrieval",
      "Mathematical analysis",
      "Mathematics",
      "Philosophy",
      "Quality (philosophy)",
      "Ranking (information retrieval)",
      "Recommender system",
      "Scheme (mathematics)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Barzegar Nozari",
        "given_name": "Reza"
      },
      {
        "surname": "Koohi",
        "given_name": "Hamidreza"
      }
    ]
  },
  {
    "title": "Integer programming approach and application of reformulation-linearization technique to liver exchange problem",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115599",
    "abstract": "Organ transplants are essential for many end-stage organic disease patients. Unfortunately, because of medical or biological incompatibilities, not all donors can donate to their intended recipients. These incompatibilities can be overcome by organ exchange programs, which find new compatible donor–patient pairs by exchanging donors between patients. Organ exchange programs have become prevalent in the last decade for kidneys, and liver exchanges have also been increasing steadily. However, despite the growing number of liver exchanges, since the procedure is relatively new, there is a lack of studies attempting to optimize exchange plans through mathematical programming. This paper develops a new integer programming model for liver exchange programs that takes into account the unique characteristics of liver transplantation. In addition, a new enhanced model is obtained by applying the reformulation-linearization technique (RLT), which provides tight linear programming (LP) relaxation bounds and is computationally efficient.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009994",
    "keywords": [
      "Algorithm",
      "Art",
      "Computer science",
      "Dual (grammatical number)",
      "Integer (computer science)",
      "Integer programming",
      "Internal medicine",
      "Linearization",
      "Literature",
      "Mathematical optimization",
      "Mathematics",
      "Medicine",
      "Nonlinear system",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Relaxation (psychology)"
    ],
    "authors": [
      {
        "surname": "Yuh",
        "given_name": "Junsang"
      },
      {
        "surname": "Eun",
        "given_name": "Joonyup"
      },
      {
        "surname": "Cheong",
        "given_name": "Taesu"
      }
    ]
  },
  {
    "title": "A hybrid matheuristic for the Two-Stage Capacitated Facility Location problem",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115501",
    "abstract": "In the class of supply chain problems, the Two-Stage Capacitated Facility Location (TSCFL) is defined by optimal locations for installing factories and warehouses to meet the demand of customers. The problem aims to minimize operating costs: opening facilities and the flow of products from factories to customers, passing through warehouses, meeting the capacity constraints of factories and warehouses and customers’ demand. To solve this problem, a hybridization of Clustering Search (CS), Adaptive Large Neighborhood Search (ALNS) and Local Branching (LB) is proposed. This hybridization is a new and interesting approach which has found high quality solutions in low computational time. To show that, computational experiments were performed using benchmark instances. The results showed that the proposed method outperforms the current state-of-art for the TSCFL for 40 out of 50 instances.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009118",
    "keywords": [
      "Biology",
      "Computer science",
      "Facility location problem",
      "Mathematical optimization",
      "Mathematics",
      "Paleontology",
      "Stage (stratigraphy)"
    ],
    "authors": [
      {
        "surname": "Souto",
        "given_name": "Gabriel"
      },
      {
        "surname": "Morais",
        "given_name": "Igor"
      },
      {
        "surname": "Mauri",
        "given_name": "Geraldo Regis"
      },
      {
        "surname": "Ribeiro",
        "given_name": "Glaydston Mattos"
      },
      {
        "surname": "González",
        "given_name": "Pedro Henrique"
      }
    ]
  },
  {
    "title": "Improving fairness of artificial intelligence algorithms in Privileged-Group Selection Bias data settings",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115667",
    "abstract": "An increasing number of decisions regarding the daily lives of human beings are being controlled by artificial intelligence (AI) algorithms. Since they now touch on many aspects of our lives, it is crucial to develop AI algorithms that are not only accurate but also objective and fair. Recent studies have shown that algorithmic decision-making may be inherently prone to unfairness, even when there is no intention for it. In this paper, we study the fairness of AI algorithms in data settings for which unprivileged groups are extremely under-represented compared to privileged groups. A typical domain which often presents such Privileged Group Selection Bias (PGSB) is AI-based hiring, which stems from an inherent lack of labeled information for rejected applicants. We first demonstrate that such a selection bias can lead to a high algorithmic bias, even if privileged and unprivileged groups are treated exactly the same. We then propose several methods to overcome this type of bias. In particular, we suggest three in-process and pre-process fairness mechanisms, combined with both supervised and semi-supervised learning algorithms. An extensive evaluation that was conducted using two real-world datasets, reveals that the proposed methods are able to improve fairness considerably, with only a minimal compromise in accuracy. This is despite the limited information available for unprivileged groups and the inherent trade-off between fairness and accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010575",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Compromise",
      "Computer science",
      "Domain (mathematical analysis)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Process (computing)",
      "Selection (genetic algorithm)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Pessach",
        "given_name": "Dana"
      },
      {
        "surname": "Shmueli",
        "given_name": "Erez"
      }
    ]
  },
  {
    "title": "Recovery solutions for ecotourism centers during the Covid-19 pandemic: Utilizing Fuzzy DEMATEL and Fuzzy VIKOR methods",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115594",
    "abstract": "Obviously, the Covid-19 pandemic has huge impact on most businesses and has caused serious and countless problems for them. Therefore, providing solutions for affected businesses to recover and improve their activities during pandemic times is inevitable. In this regard, ecotourism centers are one of the businesses that went through this problem and have faced significant dilemmas in their activities. Also, reportedly, there is no related research focusing on the recovery approaches to address these obstacles relating to these kinds of businesses during the pandemic. Therefore, all of these exhorted us to do the current research. In this paper, some practical and useful action plans for ecotourism centers are firstly developed to help these businesses. To obtain the action plans, some brainstorming sessions were held consisting of tourism experts, university professors, managers, owners, and some personnel of eco-tourism centers. In order to prioritize the defined action plans, four criteria are considered. Firstly, we compute the weights of the considered criteria by the Fuzzy DEMATEL and then they are prioritized using the Fuzzy VIKOR. The findings of the current study divulge that the AP2 “Standardization of the centers” and AP3 “Estimating demand number and increasing the capacity” and AP7 “Identifying other natural tourist attractions of the region” have the highest and lowest priority to be implemented.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009945",
    "keywords": [
      "Action (physics)",
      "Artificial intelligence",
      "Brainstorming",
      "Business",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Disease",
      "Economics",
      "Ecotourism",
      "Environmental economics",
      "Finance",
      "Fuzzy logic",
      "Infectious disease (medical specialty)",
      "Law",
      "Marketing",
      "Mathematics",
      "Medicine",
      "Operating system",
      "Operations research",
      "Order (exchange)",
      "Pandemic",
      "Pathology",
      "Physics",
      "Political science",
      "Process management",
      "Quantum mechanics",
      "Risk analysis (engineering)",
      "Standardization",
      "Tourism",
      "VIKOR method"
    ],
    "authors": [
      {
        "surname": "Hosseini",
        "given_name": "Seyyed Mehdi"
      },
      {
        "surname": "Paydar",
        "given_name": "Mohammad Mahdi"
      },
      {
        "surname": "Hajiaghaei-Keshteli",
        "given_name": "Mostafa"
      }
    ]
  },
  {
    "title": "Minimum training sample size requirements for achieving high prediction accuracy with the BN model: A case study regarding seismic liquefaction",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115702",
    "abstract": "The complexity of a Bayesian network (BN) model and the number of training samples used have significant impacts on the prediction accuracy of the model regarding seismic liquefaction. The required training sample size for ensuring that a BN model has high generalization ability is a critical issue in parameter learning. To address this issue, this study analyses the relationship between the predictive performance of the BN model and the complexity of the model, training sample size, and average discrete intervals. Taking seismic liquefaction prediction as an example, 4536 statistical experiments are designed to investigate the training and testing performances of 21 different BN models under the conditions of nine different training sample size ratios (5%, 10%, 20%, 30%, 40%, 50%, 60%, 70%, and 80% of all data) and testing samples using 20% of all data. The results reveal that the learning performance of a BN is not sensitive to the training sample size but is related to the complexity of the model. The larger the sample size is, the stronger the generalization ability of the model. The minimum training sample requirements are related to the maximum in-degrees and the average discrete intervals, not the numbers of nodes and edges and the maximum out-degree of the BN structure. In addition, a modified structural entropy can characterize the complexity of a BN structure better than the existing structural entropy, but it has a worse relationship with the minimum training sample requirements than that of the maximum in-degree. To quickly determine the minimum training sample size requirements of a BN model with a predictive accuracy of 80%, a fitting function that considers the effects of the maximum in-degree and the average discrete intervals is presented, and its effectiveness is validated by two examples.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742101085X",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Engineering",
      "Generalization",
      "Geotechnical engineering",
      "Liquefaction",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Meteorology",
      "Physics",
      "Principle of maximum entropy",
      "Sample (material)",
      "Sample size determination",
      "Statistics",
      "Training (meteorology)"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Jilei"
      },
      {
        "surname": "Zou",
        "given_name": "Wenjun"
      },
      {
        "surname": "Wang",
        "given_name": "Jing"
      },
      {
        "surname": "Pang",
        "given_name": "Luou"
      }
    ]
  },
  {
    "title": "Ordering of interval-valued Fermatean fuzzy sets and its applications",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115613",
    "abstract": "Fermatean fuzzy sets (FFSs) introduced by Senapati and Yager (2019) can handle the problems with imprecise and incomplete information more effectively than that of intuitionistic fuzzy sets introduced by Atanassov (1986). Senapati and Yager (2019) have established some basic Mathematical operations over the Fermatean fuzzy sets. They have also introduced the concept of score and accuracy function on the class of Fermatean fuzzy sets for comparing any two arbitrary FFSs. In this paper, firstly, we introduce the concept of interval-valued Fermatean fuzzy sets and establish some Mathematical operations on the class of IVFFSs. Secondly, we introduce various score functions in the class of IVFFSs and study their properties. Thirdly, we compare different ranking methods with the proposed score functions for showing the efficacy of the proposed score functions. Fourthly, we establish the interval-valued Fermatean fuzzy TOPSIS (IVFFTOPSIS) method for solving multi-criteria decision-making problems. Finally, we show the applicability of proposed score functions on IVFFSs and the IVFFTOPSIS method in solving MCDM problems using numerical examples.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010095",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Class (philosophy)",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Evolutionary biology",
      "Function (biology)",
      "Fuzzy classification",
      "Fuzzy logic",
      "Fuzzy number",
      "Fuzzy set",
      "Fuzzy set operations",
      "Interval (graph theory)",
      "Mathematical optimization",
      "Mathematics",
      "Membership function",
      "Ranking (information retrieval)",
      "Score",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "S",
        "given_name": "Jeevaraj"
      }
    ]
  },
  {
    "title": "Applying deep neural networks for the automatic recognition of sign language words: A communication aid to deaf agriculturists",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115601",
    "abstract": "One of the major challenges that deaf people face in modern societal life is communication. For those engaged in agricultural jobs, efficiency at work and productivity are deeply related to the quality of deciphering the sign language used by the deaf farmers. Employing sign language interpreters is not a pragmatic solution to this problem. There comes the need for developing a reliable system for automatic sign language recognition (SLR). This paper reports a work on the recognition of hand gestures for the Indian sign language (ISL) words commonly used by deaf farmers. A hybrid deep learning model with convolutional long short term memory (LSTM) network has been exploited for gesture classification. The model has attained an average classification accuracy of 76.21% on the proposed dataset of ISL words from the agricultural domain.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010009",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Domain (mathematical analysis)",
      "Gesture",
      "Gesture recognition",
      "Interpreter",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Philosophy",
      "Programming language",
      "Sign (mathematics)",
      "Sign language",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Venugopalan",
        "given_name": "Adithya"
      },
      {
        "surname": "Reghunadhan",
        "given_name": "Rajesh"
      }
    ]
  },
  {
    "title": "Incremental semi-supervised Extreme Learning Machine for Mixed data stream classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115591",
    "abstract": "With an explosive growth of data generated in the Internet and other fields, the data stream classification has sparked broad interest recently. Nowadays, some of the challenges in data streams, such as concept drift detection and supervised data stream classification, have been well-developed. However, when confronted with mixed data streams (containing categorical and numerical values) or limited available labeled samples, many data stream methods cannot achieve a satisfying performance or even cannot work. To tackle these two problems, we proposed an Incremental Semi-supervised Extreme Learning Machine for Mixed data stream classification (MIS-ELM). To be specific, for the issue of mixed data in data streams, we designed a novel soft one-hot encoding method by combining the coupling object similarity method and the one-hot encoding method, which can embed categorical data into high-quality numerical data and is used in the data preprocessing phase of MIS-ELM; for the issue of limited labeled samples, we introduced an incremental learning method based on unlabeled data, which is employed in the training classifier phase of MIS-ELM. When no concept drift occurs in the data stream, MIS-ELM uses only unlabeled data for incremental learning to fine-tune the classifier trained in the previous sliding window. Also, MIS-ELM instinctively inherits the fast computability of ELM, so it is very suitable for the real-time processing of data streams. Finally, we evaluated the representation performance of the soft one-hot encoding and the classification performance of MIS-ELM, within real data streams. The experimental results demonstrate the superiority of the proposed methods over the state-of-the-art techniques in their areas, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100991X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Categorical variable",
      "Classifier (UML)",
      "Computer science",
      "Concept drift",
      "Data mining",
      "Data pre-processing",
      "Data stream",
      "Data stream mining",
      "Encoding (memory)",
      "External Data Representation",
      "Extreme learning machine",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Preprocessor",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Qiude"
      },
      {
        "surname": "Xiong",
        "given_name": "Qingyu"
      },
      {
        "surname": "Ji",
        "given_name": "Shengfen"
      },
      {
        "surname": "Yu",
        "given_name": "Yang"
      },
      {
        "surname": "Wu",
        "given_name": "Chao"
      },
      {
        "surname": "Gao",
        "given_name": "Min"
      }
    ]
  },
  {
    "title": "Markov features based DTCWS algorithm for online image forgery detection using ensemble classifier in the pandemic",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115630",
    "abstract": "The unexpected blast of the COVID has been found everywhere in the world, and everyone has been shocked unusually. In this critical situation, the number of online activities increases, whether related to online education, research, business meetings, virtual conferences, or virtual court. Under this pandemic situation, digital images are the only source of information that can be generally shared and visualized in virtual conferences and social media, and it's challenging to share the document for forgery detection. Today, it's straight forward to forge these images using image-editing software, and it's essential to detect image forgery for such images. In this paper, an efficient novel Discrete-Time Cosine Wavelet and Spatial (DTCWS) Markov feature-based algorithm has been designed for the detection of such forgery, especially for this pandemic situation. For this work, high-dimensional Markov features have been extracted in the DTCWS domain, and the dimensionality of these Markov features has been reduced with Principal Component Analysis (PCA). Furthermore, the co-occurrence matrix has increased the correlation among coefficients. For classification, an optimized ensemble classifier is used for evaluating the results instead of using a support vector machine classifier. Due to the time constraint in online activities, the proposed algorithm shows the best accuracy of 99.9% without taking too much time and fewer complexes compared to the current work.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010241",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Feature extraction",
      "Feature vector",
      "Hidden Markov model",
      "Machine learning",
      "Markov chain",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Mehta",
        "given_name": "Rachna"
      },
      {
        "surname": "Aggarwal",
        "given_name": "Karan"
      },
      {
        "surname": "Koundal",
        "given_name": "Deepika"
      },
      {
        "surname": "Alhudhaif",
        "given_name": "Adi"
      },
      {
        "surname": "Polat",
        "given_name": "Kemal"
      }
    ]
  },
  {
    "title": "Applications of artificial intelligence in COVID-19 pandemic: A comprehensive review",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115695",
    "abstract": "During the current global public health emergency caused by novel coronavirus disease 19 (COVID-19), researchers and medical experts started working day and night to search for new technologies to mitigate the COVID-19 pandemic. Recent studies have shown that artificial intelligence (AI)has been successfully employed in the health sector for various healthcare procedures. This study comprehensively reviewed the research and development on state-of-the-art applications of artificial intelligence for combating the COVID-19 pandemic. In the process of literature retrieval, the relevant literature from citation databases including ScienceDirect, Google Scholar, and Preprints from arXiv, medRxiv, and bioRxiv was selected. Recent advances in the field of AI-based technologies are critically reviewed and summarized. Various challenges associated with the use of these technologies are highlighted and based on updated studies and critical analysis, research gaps and future recommendations are identified and discussed. The comparison between various machine learning (ML) and deep learning (DL) methods, the dominant AI-based technique, mostly used ML and DL methods for COVID-19 detection, diagnosis, screening, classification, drug repurposing, prediction, and forecasting, and insights about where the current research is heading are highlighted. Recent research and development in the field of artificial intelligence has greatly improved the COVID-19 screening, diagnostics, and prediction and results in better scale-up, timely response, most reliable, and efficient outcomes, and sometimes outperforms humans in certain healthcare tasks. This review article will help researchers, healthcare institutes and organizations, government officials, and policymakers with new insights into how AI can control the COVID-19 pandemic and drivemore research and studies for mitigating the COVID-19 outbreak.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010794",
    "keywords": [
      "Applications of artificial intelligence",
      "Artificial intelligence",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Data science",
      "Disease",
      "Engineering",
      "Field (mathematics)",
      "Government (linguistics)",
      "Health care",
      "Infectious disease (medical specialty)",
      "Law",
      "Linguistics",
      "Mathematics",
      "Medicine",
      "Pandemic",
      "Pathology",
      "Philosophy",
      "Political science",
      "Pure mathematics",
      "Repurposing",
      "Waste management"
    ],
    "authors": [
      {
        "surname": "Khan",
        "given_name": "Muzammil"
      },
      {
        "surname": "Mehran",
        "given_name": "Muhammad Taqi"
      },
      {
        "surname": "Haq",
        "given_name": "Zeeshan Ul"
      },
      {
        "surname": "Ullah",
        "given_name": "Zahid"
      },
      {
        "surname": "Naqvi",
        "given_name": "Salman Raza"
      },
      {
        "surname": "Ihsan",
        "given_name": "Mehreen"
      },
      {
        "surname": "Abbass",
        "given_name": "Haider"
      }
    ]
  },
  {
    "title": "Machine Learning analysis of the human infant gut microbiome identifies influential species in type 1 diabetes",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115648",
    "abstract": "Diabetes is a disease that is closely linked to genetics and epigenetics, yet mechanisms for clarifying the onset and/or progression of the disease have sometimes not been fully managed. In recent years and due to the large number of recent studies, it is known that changes in the balance of the microbiota can cause a high battery of diseases, including diabetes. Machine Learning (ML) techniques are able to identify complex, non-linear patterns of expression and relationships within the data set to extract intrinsic knowledge without any biological assumptions about the data. At the same time, mass sequencing techniques allow us to obtain the metagenomic profile of an individual, whether it is a body part, organ or tissue, and thus identify the composition of a given microbe. The great increase in the development of both technologies in their respective fields of study leads to the logical union of both to try to identify the bases of a complex disease such as diabetes. To this end, a Random Forest model has been developed at different taxonomic levels, obtaining results above 0.80 in AUC for families and above 0.98 at species level, following a strict experimental design to ensure that results are compared under equal conditions. It is identified how, in infants, the species Bacteroides uniformis, Bacteroides dorei and Bacteroides thetaiotaomicron are reduced in the microbiota of those with T1D, while, the populations of Prevotella copri increase slightly and that of Bacteroides vulgatus is much higher. Finally, thanks to the more specific metagenomic signature at species level, a model has been generated to predict those seroconverted patients not previously diagnosed with diabetes but who have expressed at least two of the autoantibodies analysed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010393",
    "keywords": [
      "Artificial intelligence",
      "Bacteria",
      "Bacteroides",
      "Bacteroides thetaiotaomicron",
      "Bioinformatics",
      "Biology",
      "Computational biology",
      "Computer science",
      "Diabetes mellitus",
      "Disease",
      "Endocrinology",
      "Gene",
      "Genetics",
      "Human microbiome",
      "Internal medicine",
      "Machine learning",
      "Medicine",
      "Metagenomics",
      "Microbiome",
      "Prevotella",
      "Type 1 diabetes"
    ],
    "authors": [
      {
        "surname": "Fernández-Edreira",
        "given_name": "Diego"
      },
      {
        "surname": "Liñares-Blanco",
        "given_name": "Jose"
      },
      {
        "surname": "Fernandez-Lozano",
        "given_name": "Carlos"
      }
    ]
  },
  {
    "title": "Nonlinear classification of emotion from EEG signal based on maximized mutual information",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115605",
    "abstract": "Recent research reveals that continuous efforts are being made to explore the relationship between EEG signals and manually scored emotions through feature extraction or emotion extraction. In order to achieve emotion extraction, wavelet transforms, Support Vector Machine (SVM), higher-order crossing, short term Fourier transform and ANOVA as classifiers are commonly used. This paper presents for the first time, the determination of maximally informative dimensions from EEG signals, the application of the same for the prediction of human emotions and assessment of the prediction for manually scored emotions. This is an alternative approach of emotion extraction compared to traditional approaches such as Support Vector Machines (SVM) and random forest. The information space, that is available in an EEG database does not usually map into the emotion space entirely. Thus a relevant subspace needs to be developed, which satisfactorily defines the target emotional space. Feature vectors of the dataset are reoriented to the directions, which are relevant informative directions for identifiable emotion. The correlation of manually scored emotion with EEG signal is assessed using mutual Information concerning emotional space. There is no hidden assumption in this method and hence the method is generic in nature. Our method predicts 82% and 72% in ‘two-class emotional scoring’ and ‘three-class emotional scoring’ methods of emotions respectively in a limited dataset of 32 subjects. The maximum prediction recorded is 95.87% for the dominance component of emotion.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010046",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Electroencephalography",
      "Emotion classification",
      "Feature extraction",
      "Feature vector",
      "Machine learning",
      "Mutual information",
      "Pattern recognition (psychology)",
      "Programming language",
      "Psychiatry",
      "Psychology",
      "Random forest",
      "SIGNAL (programming language)",
      "Speech recognition",
      "Subspace topology",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Ghosh",
        "given_name": "Snigdha Madhab"
      },
      {
        "surname": "Bandyopadhyay",
        "given_name": "Sharba"
      },
      {
        "surname": "Mitra",
        "given_name": "Debjani"
      }
    ]
  },
  {
    "title": "Automatic recommendation of feature selection algorithms based on dataset characteristics",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115589",
    "abstract": "Feature selection in real-world data mining problems is essential to make the learning task efficient and more accurate. Identifying the best feature selection algorithm, among the many available, is a complex activity that still relies heavily on human experts or some random trial-and-error procedure. Thus, the automated machine learning community has taken some steps towards the automation of this process. In this paper, we address the metalearning challenge of recommending feature selection algorithms by proposing a novel meta-feature engineering model. Our model considers a broad collection of meta-features that enable the study of the relationship between the dataset properties and the feature selection algorithm performance in terms of several criteria. We arrange the input meta-features into eight categories: (i) simple, (ii) statistical, (iii) information-theoretical, (iv) complexity, (v) landmarking, (vi) based on symbolic models, (vii) based on images, and (viii) based on complex networks (graphs). The target meta-features emerge from a multi-criteria performance measure, based on five individual performance indexes, that assesses feature selection methods grounded in information, distance, dependence, consistency, and precision measures. We evaluate our proposal using a recently developed framework that extracts the input meta-features from 213 benchmark datasets, and ranks the assessed feature selection algorithms, to fill in the target meta-features in meta-bases. This evaluation uses five state-of-the-art classification methods to induce recommendation models from meta-bases: C4.5, Random Forest, XGBoost, ANN, and SVM. The results showed that it is possible to reach an average accuracy of up to 90% applying our meta-feature engineering model. This work is the first to use an extensive empirical evaluation to provide a careful discussion of the strengths and limitations of more than 160 meta-features. These meta-features, while designed to aid the task of feature selection algorithm recommendation, can readily be employed in other metalearning scenarios. Therefore, we believe our findings are a valuable contribution to the fields of automated machine learning and data mining, as well as to the feature extraction and pattern recognition communities.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009908",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Automation",
      "Benchmark (surveying)",
      "Computer science",
      "Consistency (knowledge bases)",
      "Data mining",
      "Deep learning",
      "Economics",
      "Engineering",
      "Feature (linguistics)",
      "Feature engineering",
      "Feature selection",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Machine learning",
      "Management",
      "Mechanical engineering",
      "Meta learning (computer science)",
      "Philosophy",
      "Random forest",
      "Selection (genetic algorithm)",
      "Support vector machine",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Parmezan",
        "given_name": "Antonio Rafael Sabino"
      },
      {
        "surname": "Lee",
        "given_name": "Huei Diana"
      },
      {
        "surname": "Spolaôr",
        "given_name": "Newton"
      },
      {
        "surname": "Wu",
        "given_name": "Feng Chung"
      }
    ]
  },
  {
    "title": "An efficient multilevel thresholding segmentation method for thermography breast cancer imaging based on improved chimp optimization algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115651",
    "abstract": "Thermography images are a helpful screening tool that can detect breast cancer by showing the body parts that indicate an abnormal change in temperature. Various segmentation methods are proposed to extract regions of interest from breast cancer images to enhance the classification. Many issues were solved using thresholding. In this paper, a new efficient version of the recent chimp optimization algorithm (ChOA), namely opposition-based Lévy Flight chimp optimizer (IChOA), was proposed. The original ChOA algorithm can stagnate in local optima and needs varied exploration with an adequate blending of exploitation. Therefore, the convergence is accelerated by improving the initial diversity and good exploitation capability at a later stage of generations. Opposition-based learning (OBL) is applied at the initialization phase of ChOA to boost its population diversity in the search space, and the Lévy Flight is used to enhance its exploitation. Moreover, the IChOA is applied to tackle the image segmentation problem using multilevel thresholding. The proposed method tested using Otsu and Kapur methods over a dataset from Mastology Research with Infrared Image (DMR-IR) database during the optimization process. Furthermore, compared against seven other meta-heuristic algorithms, namely Gray wolf optimization (GWO), Moth flame optimization (MFO), Whale optimization algorithm (WOA), Sine–cosine algorithm (SCA), Slap swarm algorithm (SSA), Equilibrium optimization (EO), and original Chimp optimization algorithm (ChOA). Results based on the fitness values of obtained best solutions revealed that the IChOA achieved valuable and accurate results in terms of quality, consistency, accuracy, and the evaluation matrices such as PSNR, SSIM, and FSIM. Eventually, IChOA obtained robustness for the segmentation of various positive and negative cases compared to the methods of its counterparts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010423",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Image (mathematics)",
      "Image segmentation",
      "Particle swarm optimization",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Thresholding"
    ],
    "authors": [
      {
        "surname": "Houssein",
        "given_name": "Essam H."
      },
      {
        "surname": "Emam",
        "given_name": "Marwa M."
      },
      {
        "surname": "Ali",
        "given_name": "Abdelmgeid A."
      }
    ]
  },
  {
    "title": "Bayesian based lifetime prediction for high-power white LEDs",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115627",
    "abstract": "The introduction of high-power white LEDs has revolutionized the lighting industry in the past few decades due to the multiple benefits in terms of high reliability, environmental friendliness and versatile applications. However, challenges have arisen in assessing the reliability and lifetime prediction because it is difficult to record the failure data in a short period of time. Currently, the nonlinear least squares (NLS) regression-based method is used in industry for projecting the lumen maintenance lifetime from degradation data. The model parameters estimated using the NLS regression approach are deterministic and introduce high prediction errors. In this paper, a Bayesian method is proposed to estimate the remaining useful lifetimes (RULs) of both high-power white LED packages and lamps. The accelerated degradation tests conducted for gathering lumen degradation data are used to validate the proposed method. The exponential decay model is used as the degradation model and the parameters are estimated based on Markov Chain Monte Carlo (MCMC) sampling and using the Metropolis-Hasting (MH) algorithm. The lifetime prediction results showed that the Bayesian method has better prediction accuracy compared to the NLS method. Thus, the proposed Bayesian method is shown to be a promising approach to address the lifetime prediction issue for high-power white LEDs with improved prediction accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010216",
    "keywords": [
      "Artificial intelligence",
      "Bayesian probability",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Gene",
      "Light-emitting diode",
      "Materials science",
      "Optoelectronics",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "White (mutation)"
    ],
    "authors": [
      {
        "surname": "Ibrahim",
        "given_name": "Mesfin S."
      },
      {
        "surname": "Jing",
        "given_name": "Zhou"
      },
      {
        "surname": "Yung",
        "given_name": "Winco K.C."
      },
      {
        "surname": "Fan",
        "given_name": "Jiajie"
      }
    ]
  },
  {
    "title": "Triplet loss based metric learning for closed loop detection in VSLAM system",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115646",
    "abstract": "Closed loop detection can alleviate the error accumulation during the operation of Visual Simultaneous Localization and Mapping (VSLAM) system, which is of great significance to the accuracy and robustness of the robot. Triplet loss based metric learning has been proposed for closed loop detection in this paper. Firstly, a triplet selection strategy has been constructed. The Softplus function is applied to triplet loss so that the loss in the negative axis will be soft margin. The adaptive margin has been proposed to maintain the mapping distribution in metric space. Metric learning converts keyframes into feature vectors, evaluating the similarity of keyframes by calculating the Euclidean distance between feature vectors, which is utilized to determine whether a closed loop is formed. Secondly, detection strategy of candidate keyframes for loop detection is introduced according to Euclidean distance. Finally, triplet loss based metric learning is applied to closed loop detection for VSLAM system. VSLAM datasets have been applied to evaluate the precision and recall of metric learning model, and then the established VSLAM system has been implemented in real-time application. The experimental results illustrate the feasibility and effectiveness of proposed method, which can be further applied to practical VSLAM systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010381",
    "keywords": [
      "Artificial intelligence",
      "Closed loop",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Control engineering",
      "Engineering",
      "Loop (graph theory)",
      "Mathematics",
      "Metric (unit)",
      "Operations management"
    ],
    "authors": [
      {
        "surname": "Chang",
        "given_name": "Jianfang"
      },
      {
        "surname": "Dong",
        "given_name": "Na"
      },
      {
        "surname": "Li",
        "given_name": "Donghui"
      },
      {
        "surname": "Qin",
        "given_name": "Minghui"
      }
    ]
  },
  {
    "title": "A self-organizing incremental neural network for continual supervised learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115662",
    "abstract": "Continual learning algorithms can adapt to changes of data distributions, new classes, and even completely new tasks without catastrophically forgetting previously acquired knowledge. Here, we present a novel self-organizing incremental neural network, GSOINN+, for continual supervised learning. GSOINN+ learns a topological mapping of the input data to an undirected network and uses a weighted nearest-neighbor rule with fractional distance for classification. GSOINN+ learns incrementally—new classification tasks do not need to be specified a priori, and no rehearsal of previously learned tasks with stored training sets is required. In a series of sequential learning experiments, we show that GSOINN+ can mitigate catastrophic forgetting, even when completely new tasks are to be learned.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010526",
    "keywords": [
      "A priori and a posteriori",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Epistemology",
      "Forgetting",
      "Incremental learning",
      "Linguistics",
      "Machine learning",
      "Philosophy",
      "Self-organizing map",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Wiwatcharakoses",
        "given_name": "Chayut"
      },
      {
        "surname": "Berrar",
        "given_name": "Daniel"
      }
    ]
  },
  {
    "title": "Attribute-based Neural Collaborative Filtering",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115539",
    "abstract": "The core task of recommendation systems is to capture user preferences for items. Dot product operations are usually used to mine user preferences for items. However, the dot product can only capture the low-order linear relationships between users and items. In addition, to alleviate the data sparsity problem, current methods mainly introduce auxiliary information, such as user/item attribute information. This attribute information is often treated equivalently. In fact, the importance of this information has different effects on the recommendation results. Therefore, in this paper, we propose a novel Attribute-based Neural Collaborative Filtering (ANCF) method to solve the above problems. Specifically, we use the attention mechanism to distinguish the importance of attribute information and integrate it into the corresponding user and item feature representations to obtain a complete feature representation of users and items. To further capture the high-order interactive relationship between users and items, we use a multi-layer perceptron in ANCF to fully learn the high-order nonlinear relationship between users and items. Extensive experiments on four publicly available datasets demonstrate the effectiveness of the proposed ANCF framework.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009465",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Collaborative filtering",
      "Computer science",
      "Data mining",
      "Economics",
      "Feature (linguistics)",
      "Finance",
      "Geometry",
      "Information retrieval",
      "Law",
      "Linguistics",
      "Machine learning",
      "Management",
      "Mathematics",
      "Order (exchange)",
      "Perceptron",
      "Philosophy",
      "Political science",
      "Politics",
      "Product (mathematics)",
      "Recommender system",
      "Representation (politics)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Hai"
      },
      {
        "surname": "Qian",
        "given_name": "Fulan"
      },
      {
        "surname": "Chen",
        "given_name": "Jie"
      },
      {
        "surname": "Zhao",
        "given_name": "Shu"
      },
      {
        "surname": "Zhang",
        "given_name": "Yanping"
      }
    ]
  },
  {
    "title": "A novel representation in genetic programming for ensemble classification of human motions based on inertial signals",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115624",
    "abstract": "The use of sensing technologies and novel computational methods for automated motion detection can play a major role in improving the quality of life. Recently, researchers have become interested in employing the inertial sensor technology to record human motion signals as well as the new machine learning methods for signal-based motion detection. This manuscript proposes a novel method for human motion detection based on inertial sensors. The spatial information of a motion is first used in this method for geometric feature extraction. This manuscript also aims to introduce a novel ensemble learning approach through the genetic programing paradigm. To reduce the general complexity in the process of designing the proposed classifier, an initial population of binary trees (genes) is first created and then enhanced through genetic programing to select the best classifier. A complete experiment was conducted to evaluate the proposed ensemble classifier for the classification of inertial signals of human motions. According to the experimental results based on several well-known datasets of inertial signals, the proposed approach performed appropriately in comparison with the existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010186",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Demography",
      "Ensemble learning",
      "Feature extraction",
      "Genetic programming",
      "Inertial frame of reference",
      "Inertial measurement unit",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Population",
      "Quantum mechanics",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Sepahvand",
        "given_name": "Majid"
      },
      {
        "surname": "Abdali-Mohammadi",
        "given_name": "Fardin"
      }
    ]
  },
  {
    "title": "A Gaussian Process model for UAV localization using millimetre wave radar",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115563",
    "abstract": "The detection and positioning of unmanned aerial vehicles has become essential for both automation and surveillance tasks, in recent years. The design of accurate drone localization systems is challenging, especially in cluttered environments, where the target may be partially or even completely obscured. This paper proposes a precise detection and 3D localization system for drones, by means of a millimetre wave radar. Drone locations are estimated from spatial heatmaps of the received radar signals, which are obtained by applying the super-resolution MUSIC algorithm. These estimates are improved by analysis of the micro-Doppler effect, generated by the rotating propellers, which aids detection in poor visibility conditions. A novel Gaussian Process Regression model is developed, in order to compensate for systematic biases in the radar data. The complete system produces accurate estimates of the target range and direction, and is shown to outperform direct spectral analysis methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009696",
    "keywords": [
      "Aerospace engineering",
      "Artificial intelligence",
      "Astronomy",
      "Biology",
      "Computer science",
      "Computer vision",
      "Doppler effect",
      "Drone",
      "Engineering",
      "Gaussian",
      "Genetics",
      "Geography",
      "Meteorology",
      "Millimetre wave",
      "Operating system",
      "Optics",
      "Physics",
      "Process (computing)",
      "Quantum mechanics",
      "Radar",
      "Range (aeronautics)",
      "Real-time computing",
      "Remote sensing",
      "Telecommunications",
      "Visibility"
    ],
    "authors": [
      {
        "surname": "Paredes",
        "given_name": "José A."
      },
      {
        "surname": "Álvarez",
        "given_name": "Fernando J."
      },
      {
        "surname": "Hansard",
        "given_name": "Miles"
      },
      {
        "surname": "Rajab",
        "given_name": "Khalid Z."
      }
    ]
  },
  {
    "title": "A novel temporal recommender system based on multiple transitions in user preference drift and topic review evolution",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115626",
    "abstract": "Recommender systems are challenging research problems being exploited to suggest new items or services, such as books, music and movies, and even people, to users based on information about the user profile or the recommended items. To date, collaborative filtering (CF) has become one of the most widely used approaches for recommendations. However, traditional CF methods usually cannot track temporal dynamic user preferences and topic changes to make appropriate suggestions. Moreover, the performance of CF is limited in the case of sparse data. In this paper, we propose a novel temporal recommender system based on multiple transitions in user preference drift, called MTUPD, which employs a multitransition factor and a forgetting time function to investigate the evolution of user preferences. In addition, we consider addressing the rating sparsity issue by using text reviews. Understanding the reviews can facilitate the system grasping whether or not a user is attracted by the appearance of an item and whether the facet of an item’s appearance contributes the most to its ratings. To achieve this, we apply a topic model that automatically classifies hidden topic factors in each time period and incorporate the transition method for both user preferences and relevant review topics. Experiments show that our proposed model outperforms the compared models on eight promising datasets for temporal recommender systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010204",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Collaborative filtering",
      "Computer science",
      "Data mining",
      "Economics",
      "Evolutionary biology",
      "Forgetting",
      "Function (biology)",
      "Information retrieval",
      "Linguistics",
      "Machine learning",
      "Microeconomics",
      "Philosophy",
      "Preference",
      "Recommender system"
    ],
    "authors": [
      {
        "surname": "Wangwatcharakul",
        "given_name": "Charinya"
      },
      {
        "surname": "Wongthanavasu",
        "given_name": "Sartra"
      }
    ]
  },
  {
    "title": "A sentiment analysis-based expert weight determination method for large-scale group decision-making driven by social media data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115629",
    "abstract": "In the big data era, it is practical and generalizable to use social media data for decision-making. Thus, determining how to effectively use this kind of data to support large-scale group decision-making (LSGDM) is a direction worth studying. In this paper, for the first time, the sentiment analysis of social media data is used to evaluate the quality of LSGDM and then to dynamically determine the weights of experts from the statistical perspective. The proposed method helps improve the objectivity, accuracy, and acceptability of LSGDM. A case application demonstrates the feasibility of this method, and the results of comparative analyses reveal its superior accuracy and stability to a certain extent.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742101023X",
    "keywords": [
      "Artificial intelligence",
      "Big data",
      "Computer science",
      "Data mining",
      "Data science",
      "Epistemology",
      "Machine learning",
      "Objectivity (philosophy)",
      "Perspective (graphical)",
      "Philosophy",
      "Physics",
      "Quality (philosophy)",
      "Quantum mechanics",
      "Scale (ratio)",
      "Sentiment analysis",
      "Social media",
      "Stability (learning theory)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Wan",
        "given_name": "Qifeng"
      },
      {
        "surname": "Xu",
        "given_name": "Xuanhua"
      },
      {
        "surname": "Zhuang",
        "given_name": "Jun"
      },
      {
        "surname": "Pan",
        "given_name": "Bin"
      }
    ]
  },
  {
    "title": "ARF: A hybrid model for credit scoring in complex systems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115634",
    "abstract": "Numerous quantitative models have been developed for credit scoring of bank customers, by which banks assess the risk of their credit customers. These models mainly determine customer rating using previous trends. Due to the changing economic conditions and various social and political developments and its impact on the economy and economic enterprises, quantitative models alone do not have the appropriate accuracy and there is a need for a comprehensive model taking into account quantitative and qualitative conditions and expert opinions. In this article, 5C criteria have been used to score the customer, which are: character, capacity, capital, collateral and conditions. The customer condition criterion is highly affected by economic shocks. The status of the corresponding listed companies has been used to identify and determine the impact of the customer's economic shocks. A hybrid model is presented that detects and predicts the shocks of different stock market segments based on adaptive neuro-fuzzy inference systems (ANFIS) and recurrent neural network (RNN) using historical data and indicators. Then the results along with other customer criteria are entered into a fuzzy rule base (FRB) which finalizes the customer score. Based on the obtained results and pattern, it is possible to repay the loan on time for the customers who are expected so, and for suspicious customers, the loan will be prevented or more closely monitored.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010289",
    "keywords": [
      "Adaptive neuro fuzzy inference system",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Business",
      "Collateral",
      "Computer science",
      "Customer base",
      "Econometrics",
      "Economics",
      "Engineering",
      "Finance",
      "Fuzzy control system",
      "Fuzzy logic",
      "Horse",
      "Loan",
      "Mechanical engineering",
      "Paleontology",
      "Stock (firearms)",
      "Stock market"
    ],
    "authors": [
      {
        "surname": "Yousofi Tezerjan",
        "given_name": "Mostafa"
      },
      {
        "surname": "Safi Samghabadi",
        "given_name": "Azamdokht"
      },
      {
        "surname": "Memariani",
        "given_name": "Azizollah"
      }
    ]
  },
  {
    "title": "A quantum-clustering optimization method for COVID-19 CT scan image segmentation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115637",
    "abstract": "The World Health Organization (WHO) has declared Coronavirus Disease 2019 (COVID-19) as one of the highly contagious diseases and considered this epidemic as a global health emergency. Therefore, medical professionals urgently need an early diagnosis method for this new type of disease as soon as possible. In this research work, a new early screening method for the investigation of COVID-19 pneumonia using chest CT scan images has been introduced. For this purpose, a new image segmentation method based on K-means clustering algorithm (KMC) and novel fast forward quantum optimization algorithm (FFQOA) is proposed. The proposed method, called FFQOAK (FFQOA+KMC), initiates by clustering gray level values with the KMC algorithm and generating an optimal segmented image with the FFQOA. The main objective of the proposed FFQOAK is to segment the chest CT scan images so that infected regions can be accurately detected. The proposed method is verified and validated with different chest CT scan images of COVID-19 patients. The segmented images obtained using FFQOAK method are compared with various benchmark image segmentation methods. The proposed method achieves mean squared error, peak signal-to-noise ratio, Jaccard similarity coefficient and correlation coefficient of 712.30, 19.61, 0.90 and 0.91 in case of four experimental sets, namely Experimental_Set_1, Experimental_Set_2, Experimental_Set_3 and Experimental_Set_4, respectively. These four performance evaluation metrics show the effectiveness of FFQOAK method over these existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010319",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Image segmentation",
      "Jaccard index",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Singh",
        "given_name": "Pritpal"
      },
      {
        "surname": "Bose",
        "given_name": "Surya Sekhar"
      }
    ]
  },
  {
    "title": "EMG pattern recognition via Bayesian inference with scale mixture-based stochastic generative models",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115644",
    "abstract": "Electromyogram (EMG) has been utilized to interface signals for prosthetic hands and information devices owing to its ability to reflect human motion intentions. Although various EMG classification methods have been introduced into EMG-based control systems, they do not fully consider the stochastic characteristics of EMG signals. This paper proposes an EMG pattern classification method incorporating a scale mixture-based generative model. A scale mixture model is a stochastic EMG model in which the EMG variance is considered as a random variable, enabling the representation of uncertainty in the variance. This model is extended in this study and utilized for EMG pattern classification. The proposed method is trained by variational Bayesian learning, thereby allowing the automatic determination of the model complexity. Furthermore, to optimize the hyperparameters of the proposed method with a partial discriminative approach, a mutual information-based determination method is introduced. Simulation and EMG analysis experiments demonstrated the relationship between the hyperparameters and classification accuracy of the proposed method as well as the validity of the proposed method. The comparison using public EMG datasets revealed that the proposed method outperformed the various conventional classifiers. These results indicated the validity of the proposed method and its applicability to EMG-based control systems. In EMG pattern recognition, a classifier based on a generative model that reflects the stochastic characteristics of EMG signals can outperform the conventional general-purpose classifier.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742101037X",
    "keywords": [
      "Artificial intelligence",
      "Bayesian inference",
      "Bayesian probability",
      "Classifier (UML)",
      "Computer science",
      "Discriminative model",
      "Generative grammar",
      "Generative model",
      "Hyperparameter",
      "Inference",
      "Machine learning",
      "Mixture model",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Furui",
        "given_name": "Akira"
      },
      {
        "surname": "Igaue",
        "given_name": "Takuya"
      },
      {
        "surname": "Tsuji",
        "given_name": "Toshio"
      }
    ]
  },
  {
    "title": "Deep Learning for predicting neutralities in Offensive Language Identification Dataset",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115458",
    "abstract": "Deep learning is advancing rapidly; it has aided in solving problems that were thought impossible. Natural language understanding is one such task that has evolved with the advancement of deep learning systems. There have been several sentiment analysis attempts, but they aim to classify it as a single emotion. Human emotion in natural language is generally a complex combination of emotions, which may be indeterminate or neutral at times. Neutrosophy is a branch of philosophy that identifies neutralities and uses membership functions (positive, negative, neutral) to quantify a sample into Single Valued Neutrosophic Set (SVNS) values. Our work aims to combine the power of deep learning with SVNS to represent a sample’s sentiment into membership functions of SVNS. We have worked on the Offensive Language Identification Dataset (OLID). Combining the power of state-of-the-art neural network techniques with neutrosophy allowed us to quantify the sentiments and identify the transition phase between positive and negative ones. We used the transition phase to capture neutral samples, which is beneficial if we want to obtain purely positive/negative samples. We performed experiments using Bi-directional Long Short Term Memory (BiLSTM) with attention, Bidirectional Encoder Representations from Transformers (BERT), A Lite BERT (ALBERT), A Robustly Optimised BERT Approach (RoBERTa), and MPNet. Our SVNS model performed equivalent to state-of-the-art neural network models on the OLID dataset. Here, we propose a novel framework that can integrate with any neural network model and quantify sentiments using SVNS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100871X",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Deep learning",
      "Identification (biology)",
      "Language identification",
      "Machine learning",
      "Mathematics",
      "Natural language",
      "Natural language processing",
      "Offensive",
      "Operations research"
    ],
    "authors": [
      {
        "surname": "Sharma",
        "given_name": "Mayukh"
      },
      {
        "surname": "Kandasamy",
        "given_name": "Ilanthenral"
      },
      {
        "surname": "Kandasamy",
        "given_name": "Vasantha"
      }
    ]
  },
  {
    "title": "DeepBatch: A hybrid deep learning model for interpretable diagnosis of breast cancer in whole-slide images",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115586",
    "abstract": "The gold standard for breast cancer diagnosis, treatment, and management is the histological analysis of a suspected section. Histopathology consists in analyzing the characteristics of the lesions using tissue sections stained with hematoxylin and eosin. However, pathologists are currently subjected to high workloads, mainly due to the fundamental role of histological analysis in the patient’s treatment. In this context, methods able to reduce histological analysis time, provide a second opinion, or even point out suspicious locations as a screening tool become increasingly important for pathologists. This article proposes a model based on Convolutional Neural Networks (CNN) to provide a refined and multiclass segmentation of Whole Slide Imaging (WSI) for breast cancer. The methodology is divided into four modules: pre-processing, ROI detection, ROI sampling, and region segmentation. These modules are organized to decode the information learned using CNNs in interpretable predictions for pathologists. The preprocessing module is responsible for removing background and noise from WSI. At ROI detection, we use the U-Net convolutional architecture to identify suspicious regions in low magnification WSI. The sampling module maps the identified suspected areas from low magnifications to 40× magnifications. region segmentation module segments high-magnification areas using a ResNet50/U-Net. To validate the methodology, we use data sets from different sources that can be used together or separately in each module, depending on its purpose. We used 205 breast cancer WSI for training, validation, and testing. For the detection of suspicious regions by ROI detection, we obtained an IoU of 93.43%, accuracy of 91.27%, sensitivity of 90.77%, specificity of 94.03%, F1 score of 84.17%, and an AUC of 0.93. For the refined segmentation of WSI by the region segmentation module, we obtained an IoU of 88.23%, accuracy of 96.10%, sensitivity of 71.83%, specificity of 96.19%, F1 score of 82.94%, and an AUC of 0.88. In short, the model provides refined segmentation of breast cancer WSIs using a cascade of CNNs. This segmentation can assist pathologists in interpreting the diagnosis by accurately presenting the regions considered during the inference of WSI. Our results indicate the possibility of using the model as a second screening system.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100988X",
    "keywords": [
      "Artificial intelligence",
      "Breast cancer",
      "Cancer",
      "Computer science",
      "Deep learning",
      "Internal medicine",
      "Machine learning",
      "Medicine",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Zeiser",
        "given_name": "Felipe André"
      },
      {
        "surname": "da Costa",
        "given_name": "Cristiano André"
      },
      {
        "surname": "Ramos",
        "given_name": "Gabriel de Oliveira"
      },
      {
        "surname": "Bohn",
        "given_name": "Henrique C."
      },
      {
        "surname": "Santos",
        "given_name": "Ismael"
      },
      {
        "surname": "Roehe",
        "given_name": "Adriana Vial"
      }
    ]
  },
  {
    "title": "Multiscale quantum harmonic oscillator optimization algorithm with multiple quantum perturbations for numerical optimization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115615",
    "abstract": "Multi-scale quantum harmonic oscillator algorithm (MQHOA) is a recent proposed intelligent algorithm, in which the optimization process can be regarded as the multi-scale quantum annealing process with respect to the constraint of the harmonic oscillator potential well. It has been proved effective and efficient to deal with unimodal and multimodal numerical optimization problems. However, it takes a long time for the particles system to reach the ground-state equilibrium at each annealing scale. Motivated by this situation, a diffusion Monte Carlo method based dynamic sampling regulation strategy is proposed to enhances the sampling efficiency by dynamically adjusting the sampling frequency. Moreover, the particles with different kinetic energies are introduced into the optimization system as quantum perturbations, which reduces the probability of the algorithm falling into a local optimum by keeping the different searching scales simultaneously. The theoretical derivations of this method are presented. The effectiveness of the algorithm is studied by comparing it with previous versions of the MQHOA and several famous intelligent algorithms on uni- and multimodal benchmark functions. The experimental results illustrate that the proposed method has a comparable performance for numerical optimization problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010101",
    "keywords": [
      "Adaptive simulated annealing",
      "Algorithm",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Filter (signal processing)",
      "Geodesy",
      "Geography",
      "Harmonic oscillator",
      "Local optimum",
      "Mathematical optimization",
      "Mathematics",
      "Physics",
      "Quantum",
      "Quantum harmonic oscillator",
      "Quantum mechanics",
      "Sampling (signal processing)",
      "Simulated annealing"
    ],
    "authors": [
      {
        "surname": "Xin",
        "given_name": "Gang"
      },
      {
        "surname": "Wang",
        "given_name": "Peng"
      },
      {
        "surname": "Jiao",
        "given_name": "Yuwei"
      }
    ]
  },
  {
    "title": "Graph signal recovery using restricted Boltzmann machines",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115635",
    "abstract": "Given: (a) clean training dataset, (b) trained graph machine learning pipeline, and (c) noisy test dataset; how do we make the pipeline robust to such noise? In an attempt to answer this question, we propose a modification to the pipeline that exploits both the content addressability of a restricted Boltzmann machine and the message passing capabilities of a graph neural network. We investigate which of the hidden layers of the pipeline is best suited for our proposed modification. By establishing that a decrease in mutual information indicates an increase in prediction accuracy which is in turn driven by progressive geometric clustering of the samples belonging to the same class, we attempt to explain the behaviour exhibited by the modified pipeline. Empirical results demonstrate that our approach makes the pipeline robust to some real-world noisy scenarios in citation datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010290",
    "keywords": [],
    "authors": [
      {
        "surname": "Mohan",
        "given_name": "Ankith"
      },
      {
        "surname": "Nakano",
        "given_name": "Aiichiro"
      },
      {
        "surname": "Ferrara",
        "given_name": "Emilio"
      }
    ]
  },
  {
    "title": "Modeling spatiotemporal patterns of gait anomaly with a CNN-LSTM deep neural network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115582",
    "abstract": "In this work, we propose an end-to-end deep learning model that uses the skeleton data recorded by Kinect to capture spatiotemporal patterns for gait anomaly recognition. Via considering the entire skeleton, the proposed model captures the relationship between different body joints in locomotion. Unlike the common two-class or one-class approaches in skeleton-based methods, the proposed model considers a multi-class framework. Such a multi-class technique can be easily adapted for a more frequent and less expensive gait assessment outside of motion capture facilities. The proposed deep learning model is trained and evaluated on the publicly available Walking gait dataset and achieves an average accuracy of 90.57% in identifying nine different walking patterns. Through transfer learning, we also evaluate our model on two other publicly available datasets, acquiring an average accuracy of 83.64% on a dataset of three classes and 90.83% on a dataset with six classes of normal/pathological gait patterns. The results of this work indicate the potential of markerless modalities such as Kinect for designing less costly and more convenient health infrastructures for assisted living. Additionally, an automatic and non-invasive gait assessment can further augment the clinical diagnosis for an extensive list of ailments that cause different types of gait disorders.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009842",
    "keywords": [
      "Anomaly (physics)",
      "Artificial intelligence",
      "Artificial neural network",
      "Class (philosophy)",
      "Computer science",
      "Condensed matter physics",
      "Convolutional neural network",
      "Deep learning",
      "Gait",
      "Machine learning",
      "Medicine",
      "Modalities",
      "Pattern recognition (psychology)",
      "Physical medicine and rehabilitation",
      "Physics",
      "Social science",
      "Sociology",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Sadeghzadehyazdi",
        "given_name": "Nasrin"
      },
      {
        "surname": "Batabyal",
        "given_name": "Tamal"
      },
      {
        "surname": "Acton",
        "given_name": "Scott T."
      }
    ]
  },
  {
    "title": "An ensemble method for nuclei detection of overlapping cervical cells",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115642",
    "abstract": "The Pap test is a preventive approach that requires specialized and labor-intensive examination of cytological preparations to track potentially cancerous cells from the internal and external cervix surface. A cytopathologist must analyze many microscopic fields while screening for abnormal cells. Therefore there is hope that a support decision system could assist with clinical diagnosis, for example, by identifying sub-cellular abnormalities, such as changes in the nuclei features. This work proposes an ensemble method for cervical nuclei detection aiming to reduce the workload of cytopathologists. First, a preprocessing phase divides the original image into superpixels, which are input to feature extraction and selection algorithms. The proposed ensemble method combines three classifiers: Decision Tree (DT), Nearest Centroid (NC), and k-Nearest Neighbors (k-NN), which are evaluated against the ISBI’14 Overlapping Cervical Cytology Image Segmentation Challenge dataset. Experiments show that the proposed method is the state-of-the-art algorithm of the literature for recall (0.999) and F1 values (0.993). It produced a recall very close to the optimum value and also kept high precision (0.988).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010356",
    "keywords": [
      "Artificial intelligence",
      "Centroid",
      "Computer science",
      "Decision tree",
      "Feature selection",
      "Pattern recognition (psychology)",
      "Precision and recall",
      "Preprocessor",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Diniz",
        "given_name": "Débora Nasser"
      },
      {
        "surname": "Vitor",
        "given_name": "Rafael Ferreira"
      },
      {
        "surname": "Bianchi",
        "given_name": "Andrea Gomes Campos"
      },
      {
        "surname": "Delabrida",
        "given_name": "Saul"
      },
      {
        "surname": "Carneiro",
        "given_name": "Cláudia Martins"
      },
      {
        "surname": "Ushizima",
        "given_name": "Daniela Mayumi"
      },
      {
        "surname": "de Medeiros",
        "given_name": "Fátima Nelsizeuma Sombra"
      },
      {
        "surname": "Souza",
        "given_name": "Marcone Jamilson Freitas"
      }
    ]
  },
  {
    "title": "A deep-learning-based framework for severity assessment of COVID-19 with CT images",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115616",
    "abstract": "Millions of positive COVID-19 patients are suffering from the pandemic around the world, a critical step in the management and treatment is severity assessment, which is quite challenging with the limited medical resources. Currently, several artificial intelligence systems have been developed for the severity assessment. However, imprecise severity assessment and insufficient data are still obstacles. To address these issues, we proposed a novel deep-learning-based framework for the fine-grained severity assessment using 3D CT scans, by jointly performing lung segmentation and lesion segmentation. The main innovations in the proposed framework include: 1) decomposing 3D CT scan into multi-view slices for reducing the complexity of 3D model, 2) integrating prior knowledge (dual-Siamese channels and clinical metadata) into our model for improving the model performance. We evaluated the proposed method on 1301 CT scans of 449 COVID-19 cases collected by us, our method achieved an accuracy of 86.7% for four-way classification, with the sensitivities of 92%, 78%, 95%, 89% for four stages. Moreover, ablation study demonstrated the effectiveness of the major components in our model. This indicates that our method may contribute a potential solution to severity assessment of COVID-19 patients using CT images and clinical metadata.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010113",
    "keywords": [
      "2019-20 coronavirus outbreak",
      "Artificial intelligence",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Deep learning",
      "Disease",
      "Infectious disease (medical specialty)",
      "Machine learning",
      "Medicine",
      "Metadata",
      "Operating system",
      "Outbreak",
      "Pathology",
      "Radiology",
      "Segmentation",
      "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Zhidan"
      },
      {
        "surname": "Zhao",
        "given_name": "Shixuan"
      },
      {
        "surname": "Chen",
        "given_name": "Yang"
      },
      {
        "surname": "Luo",
        "given_name": "Fuya"
      },
      {
        "surname": "Kang",
        "given_name": "Zhiqing"
      },
      {
        "surname": "Cai",
        "given_name": "Shengping"
      },
      {
        "surname": "Zhao",
        "given_name": "Wei"
      },
      {
        "surname": "Liu",
        "given_name": "Jun"
      },
      {
        "surname": "Zhao",
        "given_name": "Di"
      },
      {
        "surname": "Li",
        "given_name": "Yongjie"
      }
    ]
  },
  {
    "title": "Persona analytics: Analyzing the stability of online segments and content interests over time using non-negative matrix factorization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115611",
    "abstract": "Personified big data and rapidly developing data science techniques enable previously unforeseen methodological developments for longitudinal analysis of online audiences. Applying data-driven persona generation on online customer statistics from a real organizational social media channel, we demonstrate how personas can be deployed to understand online customer patterns over time. We conduct 32 monthly rounds of data collection of customer demographics and content consumption patterns on the YouTube channel of a major publishing organization posting thousands of items of content and then algorithmically generate 15 personas monthly. We analyze the data-driven persona for changes monthly, yearly, and lifetime (period). Results show an average 40% change in the personas, and 78% of the personas experience more change than consistency for topic interests. The implications are that organizations frequently publishing online content should employ automatic data collection and periodic persona creation to ensure their customer understanding is current. For this, algorithmic data-driven systems that leverage methods for persona creation are recommended.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010083",
    "keywords": [
      "Analytics",
      "Artificial intelligence",
      "Big data",
      "Business",
      "Computer science",
      "Customer base",
      "Data collection",
      "Data mining",
      "Data science",
      "Human–computer interaction",
      "Leverage (statistics)",
      "Marketing",
      "Mathematics",
      "Persona",
      "Statistics",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Jansen",
        "given_name": "Bernard J."
      },
      {
        "surname": "Jung",
        "given_name": "Soon-gyo"
      },
      {
        "surname": "Chowdhury",
        "given_name": "Shammur A."
      },
      {
        "surname": "Salminen",
        "given_name": "Joni"
      }
    ]
  },
  {
    "title": "Quantile cross-spectral density: A novel and effective tool for clustering multivariate time series",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115677",
    "abstract": "Clustering of multivariate time series is a central problem in data mining with applications in many fields. Frequently, the clustering target is to identify groups of series generated by the same multivariate stochastic process. Most of the approaches to address this problem include a prior step of dimensionality reduction which may result in a loss of information or consider dissimilarity measures based on correlations and cross-correlations but ignoring the serial dependence structure. We propose a novel approach to measure dissimilarity between multivariate time series aimed at jointly capturing both cross dependence and serial dependence. Specifically, each series is characterized by a set of matrices of estimated quantile cross-spectral densities, where each matrix corresponds to a pair of quantile levels. Then the dissimilarity between every couple of series is evaluated by comparing their estimated quantile cross-spectral densities, and the pairwise dissimilarity matrix is taken as starting point to develop a partitioning around medoids algorithm. Since the quantile-based cross-spectra capture dependence in quantiles of the joint distribution, the proposed metric has a high capability to discriminate between high-level dependence structures. An extensive simulation study shows that our clustering procedure outperforms a wide range of alternative methods and exhibits robustness to noise distribution besides being computationally efficient. A real data application involving bivariate financial time series illustrates the usefulness of the proposed approach. The procedure is also applied to cluster nonstationary series from the UEA multivariate time series classification archive.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010630",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Bivariate analysis",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Mathematics",
      "Multivariate statistics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Quantile",
      "Series (stratigraphy)",
      "Statistics",
      "Univariate"
    ],
    "authors": [
      {
        "surname": "López-Oriona",
        "given_name": "Ángel"
      },
      {
        "surname": "Vilar",
        "given_name": "José A."
      }
    ]
  },
  {
    "title": "Self-supervised multimodal reconstruction pre-training for retinal computer-aided diagnosis",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115598",
    "abstract": "Computer-aided diagnosis using retinal fundus images is crucial for the early detection of many ocular and systemic diseases. Nowadays, deep learning-based approaches are commonly used for this purpose. However, training deep neural networks usually requires a large amount of annotated data, which is not always available. In practice, this issue is commonly mitigated with different techniques, such as data augmentation or transfer learning. Nevertheless, the latter is typically faced using networks that were pre-trained on additional annotated data. An emerging alternative to the traditional transfer learning source tasks is the use of self-supervised tasks that do not require manually annotated data for training. In that regard, we propose a novel self-supervised visual learning strategy for improving the retinal computer-aided diagnosis systems using unlabeled multimodal data. In particular, we explore the use of a multimodal reconstruction task between complementary retinal imaging modalities. This allows to take advantage of existent unlabeled multimodal data in the medical domain, improving the diagnosis of different ocular diseases with additional domain-specific knowledge that does not rely on manual annotation. To validate and analyze the proposed approach, we performed several experiments aiming at the diagnosis of different diseases, including two of the most prevalent impairing ocular disorders: glaucoma and age-related macular degeneration. Additionally, the advantages of the proposed approach are clearly demonstrated in the comparisons that we perform against both the common fully-supervised approaches in the literature as well as current self-supervised alternatives for retinal computer-aided diagnosis. In general, the results show a satisfactory performance of our proposal, which improves existing alternatives by leveraging the unlabeled multimodal visual data that is commonly available in the medical field.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009982",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Computer-aided",
      "Computer-aided diagnosis",
      "Machine learning",
      "Meteorology",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Training (meteorology)"
    ],
    "authors": [
      {
        "surname": "Hervella",
        "given_name": "Álvaro S."
      },
      {
        "surname": "Rouco",
        "given_name": "José"
      },
      {
        "surname": "Novo",
        "given_name": "Jorge"
      },
      {
        "surname": "Ortega",
        "given_name": "Marcos"
      }
    ]
  },
  {
    "title": "AutoWeka4MCPS-AVATAR: Accelerating automated machine learning pipeline composition and optimisation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115643",
    "abstract": "Automated machine learning pipeline (ML) composition and optimisation aim at automating the process of finding the most promising ML pipelines within allocated resources (i.e., time, CPU and memory). Existing methods, such as Bayesian-based and genetic-based optimisation, which are implemented in Auto-Weka, Auto-sklearn and TPOT, evaluate pipelines by executing them. Therefore, the pipeline composition and optimisation of these methods frequently require a tremendous amount of time that prevents them from exploring complex pipelines to find better predictive models. To further explore this research challenge, we have conducted experiments showing that many of the generated pipelines are invalid in the first place, and attempting to execute them is a waste of time and resources. To address this issue, we propose a novel method to evaluate the validity of ML pipelines, without their execution, using a surrogate model (AVATAR). The AVATAR generates a knowledge base by automatically learning the capabilities and effects of ML algorithms on datasets’ characteristics. This knowledge base is used for a simplified mapping from an original ML pipeline to a surrogate model which is a Petri net based pipeline. Instead of executing the original ML pipeline to evaluate its validity, the AVATAR evaluates its surrogate model constructed by capabilities and effects of the ML pipeline components and input/output simplified mappings. Evaluating this surrogate model is less resource-intensive than the execution of the original pipeline. As a result, the AVATAR enables the pipeline composition and optimisation methods to evaluate more pipelines by quickly rejecting invalid pipelines. We integrate the AVATAR into the sequential model-based algorithm configuration (SMAC). Our experiments show that when SMAC employs AVATAR, it finds better solutions than on its own. This is down to the fact that the AVATAR can evaluate more pipelines within the same time budget and allocated resources.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010368",
    "keywords": [],
    "authors": [
      {
        "surname": "Nguyen",
        "given_name": "Tien-Dung"
      },
      {
        "surname": "Musial",
        "given_name": "Katarzyna"
      },
      {
        "surname": "Gabrys",
        "given_name": "Bogdan"
      }
    ]
  },
  {
    "title": "Automated design of search algorithms: Learning on algorithmic components",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115493",
    "abstract": "This paper proposes AutoGCOP, a new general framework for automated design of local search algorithms. In a recently established General Combinatorial Optimisation Problem (GCOP) model, the problem of algorithm design itself is defined as a combinatorial optimisation problem. AutoGCOP defines a general framework to optimise the composition of elementary algorithmic components as decision variables in GCOP. By modelling various well-known local search meta-heuristics within a general framework, AutoGCOP supports automatic design of new novel algorithms which may be highly different from those manually designed in the literature. Within the consistent AutoGCOP framework, various elementary algorithmic components are analysed for solving the benchmark vehicle routing problem with time window constraints and different characteristics. Furthermore, two learning models based on reinforcement learning and Markov chain are investigated to learn and enhance the compositions of algorithmic components towards the automated design of search algorithms. The Markov chain model presents superior performance learning the compositions of algorithmic components during the search, demonstrating its effectiveness designing new algorithms automatically.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009039",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Beam search",
      "Benchmark (surveying)",
      "Combinatorial search",
      "Computer science",
      "Geodesy",
      "Geography",
      "Heuristics",
      "Hyper-heuristic",
      "Local search (optimization)",
      "Machine learning",
      "Markov chain",
      "Markov decision process",
      "Markov process",
      "Mathematics",
      "Mobile robot",
      "Operating system",
      "Reinforcement learning",
      "Robot",
      "Robot learning",
      "Search algorithm",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Meng",
        "given_name": "Weiyao"
      },
      {
        "surname": "Qu",
        "given_name": "Rong"
      }
    ]
  },
  {
    "title": "Combining deep learning with geometric features for image-based localization in the Gastrointestinal tract",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115631",
    "abstract": "Tracking monocular colonoscope in the GastroIntestinal (GI) tract is challenging as the obtained images suffer from deformation, blurred textures, and significant changes in appearance. These drawbacks greatly restrict the tracking ability of conventional geometry-based methods, which are heavily dependent on the performance of corner points extraction from the image. Even though end-to-end Deep Learning (DL) can overcome these issues, limited labeling data is a roadblock to the state-of-the-art DL-based method. To handle these drawbacks, we propose a novel approach to combine the DL-based method with the traditional geometry-based approach to achieve better localization with small training data. In this work, a DL network is trained with the images of the pre-operative endoscopy/colonoscopy. Siamese architecture is introduced to perform the zone labeling of the image based on the anatomical segmentation with expert knowledge. Then, using the image in the therapeutic intervention, our method predicts the 6 degrees of freedom scope pose and recover geometric reference to the images from the pre-operative endoscopy/colonoscopy. The DL network predicts the zone of the testing image, and the pre-generated triangulated map points within the zone in the training set are registered with the bundle adjustment algorithm. The proposed hybrid method is tested on the synthetic data sets and the real-world in-vivo data sets. Further, the results achieved through various experiments validate that the proposed method outperforms traditional geometry-based only or DL-based only localization techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010253",
    "keywords": [
      "Artificial intelligence",
      "Bundle adjustment",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Ground truth",
      "Image (mathematics)",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Programming language",
      "Psychology",
      "Segmentation",
      "Set (abstract data type)",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Jingwei"
      },
      {
        "surname": "Patel",
        "given_name": "Mitesh"
      },
      {
        "surname": "Girgensohn",
        "given_name": "Andreas"
      },
      {
        "surname": "Kim",
        "given_name": "Chelhwon"
      }
    ]
  },
  {
    "title": "Bank strategic asset allocation under a unified risk measure",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115574",
    "abstract": "Most available bank asset allocation models use several risk measures as constraints; as a consequence, the comparison of the risk between different asset allocation strategies is often difficult, since each strategy is subject to several risks. With this research, we create a simulation–optimization methodology that measures interest rate, credit and liquidity risks in a unified manner. The associated risk events, such as interest rate increases, liquidity outflows or spikes in defaults are generated using the same simulation engine, giving as output a single risk measure (the probability of failure, used by ratings agencies) that aggregates those risks under the same simulation engine. Finally, we use our methodology to determine Pareto fronts for the optimal balance sheet allocations and minimum-risk strategies. As a result, several findings emerge, such as: 1) Risk is dependent on the income stream; 2) Allocation to book value assets is preferable; 3) Under low rate environments, a full allocation to cash is very risky and is not the minimum risk strategy; 4) Banks can make investments in stocks in environments of high prospective returns and low leverage.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009787",
    "keywords": [
      "Actuarial science",
      "Asset (computer security)",
      "Asset allocation",
      "Cash flow",
      "Computer science",
      "Computer security",
      "Econometrics",
      "Economics",
      "Expected shortfall",
      "Finance",
      "Interest rate",
      "Interest rate risk",
      "Leverage (statistics)",
      "Liquidity risk",
      "Machine learning",
      "Market liquidity",
      "Portfolio",
      "Risk management",
      "Risk measure"
    ],
    "authors": [
      {
        "surname": "Júdice",
        "given_name": "Pedro"
      },
      {
        "surname": "Pinto",
        "given_name": "Luís"
      },
      {
        "surname": "Santos",
        "given_name": "José Luís"
      }
    ]
  },
  {
    "title": "Water end-use consumption in low-income households: Evaluation of the impact of preprocessing on the construction of a classification model",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115623",
    "abstract": "The challenge of transforming massive water flow data into desegregated smart information according to water end uses is an issue that has motivated many researchers. This challenge is even more difficult in low-income regions owing to the high variability of data because predominant hydraulic devices offer many activation possibilities for users as they are controlled by globe valves. Devices with standardized flow rates such as washing machines or dishwashers are exceptions. A common practice is to apply commercial software that classifies events at the end-use level and then to develop a personalized classification model with enhanced alignment with the database. If the preprocessing step is not performed properly, it can affect perceived device behaviors, which may lead to incorrect conclusions. To evaluate how this variability can interfere with commercial software responses, we developed classification models using a dataset preprocessed by Trace Wizard® as training data and then applied the trained models to a test dataset consisting of events that were authenticated by individual flow sensors. Our goal was to identify the degree of difference between the two datasets. The results demonstrate that when Trace Wizard® is applied, the features of each device differ from the original water consumption flow, indicating that data variability interferes with the credibility of feedback. Additionally, preprocessing tended to increase the volume, duration, and flow rates, giving the impression that the consumption was higher than the real scenario. The constructed models were not able to overcome the distortions introduced by Trace Wizard® classification. For example, fixtures had poor matches for several houses, with statistical measures below 50%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010174",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Data pre-processing",
      "Linguistics",
      "Machine learning",
      "Philosophy",
      "Preprocessor",
      "Programming language",
      "Software",
      "TRACE (psycholinguistics)",
      "Wizard",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Oliveira-Esquerre",
        "given_name": "Karla"
      },
      {
        "surname": "Mello",
        "given_name": "Mariza"
      },
      {
        "surname": "Botelho",
        "given_name": "Gabriella"
      },
      {
        "surname": "Deng",
        "given_name": "Zikang"
      },
      {
        "surname": "Koushanfar",
        "given_name": "Farinaz"
      },
      {
        "surname": "Kiperstok",
        "given_name": "Asher"
      }
    ]
  },
  {
    "title": "Utilizing the evidential reasoning approach to determine a suitable wireless sensor network orientation for asset integrity monitoring of an offshore gas turbine driven generator",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115583",
    "abstract": "This research proposes the most ideal Wireless Sensor Network (WSN) topology for remote integrity monitoring of an offshore gas turbine driven generator. The intention is to design the structure of a number of WSNs within the electrical generation system with varying connection types and methods of relaying data. The research is concerned only with the design of the WSNs, i.e. the hardware and orientation of the sensor nodes and not the software, programming or data protection. This will potentially provide a good base, once an ideal WSN design is determined, to expand the network further incorporating more criteria and develop the necessary software to complete the WSN. The work applies the Evidential Reasoning approach to a number of WSN topologies in order to determine the most suitable based upon an outlined set of performance criteria. Axiom based validation of the methodology is also provided within the analysis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009854",
    "keywords": [
      "Computer network",
      "Computer science",
      "Data mining",
      "Distributed computing",
      "Electrical engineering",
      "Embedded system",
      "Engineering",
      "Generator (circuit theory)",
      "Geometry",
      "Mathematics",
      "Network topology",
      "Orientation (vector space)",
      "Physics",
      "Power (physics)",
      "Programming language",
      "Quantum mechanics",
      "Software",
      "Topology (electrical circuits)",
      "Wireless sensor network"
    ],
    "authors": [
      {
        "surname": "Loughney",
        "given_name": "Sean"
      },
      {
        "surname": "Wang",
        "given_name": "Jin"
      },
      {
        "surname": "Matellini",
        "given_name": "Dante B."
      },
      {
        "surname": "Nguyen",
        "given_name": "Trung Thanh"
      }
    ]
  },
  {
    "title": "Enhancing topic-detection in computerized assessments of constructed responses with distributional models of language",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115621",
    "abstract": "Usually, computerized assessments of constructed responses use a predictive-centered approach instead of a validity-centered one. Here, we compared the convergent and discriminant validity of two computerized assessment methods designed to detect semantic topics in constructed responses: Inbuilt Rubric (IR) and Partial Contents Similarity (PCS). While both methods are distributional models of language and use the same Latent Semantic Analysis (LSA) prior knowledge, they produce different semantic representations. PCS evaluates constructed responses using non-meaningful semantic dimensions (this method is the standard LSA assessment of constructed responses), but IR endows original LSA semantic space coordinates with meaning. In the present study, 255 undergraduate and high school students were allocated one of three texts and were tasked to make a summary. A topic-detection task was conducted comparing IR and PCS methods. Evidence from convergent and discriminant validity was found in favor of the IR method for topic-detection in computerized constructed response assessments. In this line, the multicollinearity of PCS method was larger than the one of IR method, which means that the former is less capable of discriminating between related concepts or meanings. Moreover, the semantic representations of both methods were qualitatively different, that is, they evaluated different concepts or meanings. The implications of these automated assessment methods are also discussed. First, the meaningful coordinates of the Inbuilt Rubric method can accommodate expert rubrics for computerized assessments of constructed responses improving computer-assisted language learning. Second, they can provide high-quality computerized feedback accurately detecting topics in other educational constructed response assessments. Thus, it is concluded that: (1) IR method can represent different concepts and contents of a text, simultaneously mapping a considerable variability of contents in constructed responses; (2) IR method semantic representations have a qualitatively different meaning than the LSA ones and present a desirable multicollinearity that promotes the discriminant validity of the scores of distributional models of language; and (3) IR method can extend the performance and the applications of current LSA semantic representations by endowing the dimensions of the semantic space with semantic meanings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010150",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Latent semantic analysis",
      "Machine learning",
      "Mathematics",
      "Mathematics education",
      "Natural language processing",
      "Probabilistic latent semantic analysis",
      "Rubric",
      "Semantic similarity"
    ],
    "authors": [
      {
        "surname": "Martínez-Huertas",
        "given_name": "José Á."
      },
      {
        "surname": "Olmos",
        "given_name": "Ricardo"
      },
      {
        "surname": "León",
        "given_name": "José A."
      }
    ]
  },
  {
    "title": "Deep neural network approach for a serendipity-oriented recommendation system",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115660",
    "abstract": "Most of the available recommender systems focus on the accuracy of recommendations. As a result, their recommendations are often popular and very close to user preferences, which make them repetitious and predictable, hence adversely affecting user satisfaction. Recent studies on recommender systems, however, aim for factors beyond accuracy as accuracy alone cannot ensure the satisfaction of all users. One of the most important criteria beyond the accuracy is serendipity, which includes relevant, unexpected, and novel recommendations that cannot be easily discovered by users themselves. In this paper, a Convolutional Neural Network (CNN) is integrated with the Particle Swarm Optimization (PSO) algorithm to generate serendipitous recommendations. The proposed method is based on the focus shift points, consisting of unexpectedness and relevance parameters. In this approach, these points are considered as the factors showing whether recommendations are serendipitous. The CNN is employed to predict the focus shift points for each user. Then, the PSO is utilized to search for recommendations close to the predicted focus shift points and generate the list of candidate recommendations. After that, the Serendipitous Personalized Ranking (SPR) method is employed to re-rank the candidate recommendations and generate the final list. According to the evaluation results, the proposed approach outperforms other state-of-the-art methods in SRDP, Hit Ratio, and NDCG factors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010514",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Deep learning",
      "Epistemology",
      "Focus (optics)",
      "Information retrieval",
      "Law",
      "Learning to rank",
      "Machine learning",
      "Mathematics",
      "Optics",
      "Philosophy",
      "Physics",
      "Political science",
      "Rank (graph theory)",
      "Ranking (information retrieval)",
      "Recommender system",
      "Relevance (law)",
      "Serendipity"
    ],
    "authors": [
      {
        "surname": "Ziarani",
        "given_name": "Reza Jafari"
      },
      {
        "surname": "Ravanmehr",
        "given_name": "Reza"
      }
    ]
  },
  {
    "title": "Single and Multi-label Fault Classification in rotors from unprocessed multi-sensor data through deep and parallel CNN architectures",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115565",
    "abstract": "An attempt has been made in this study to address two issues related to fault classification in machinery. The first concerns the need of pre-processing raw data collected by sensors and the other is identification in cases where more than one fault exist simultaneously, i.e. where multi-label faults are present. We describe a Convolution Neural Network (CNN) based Deep architecture for classification of Single faults from raw time domain data and a similar but shallower Parallel Multiple Binary Classifier Network for Multi-label Fault Classification. Data from previously conducted experiments on a Rotor-Bearing Fault Simulator are used. In the case of Single-Label fault classification, Support Vector Machines (SVMs), Clustering, Artificial Neural Networks (ANNs) and other algorithms have been used in the past . These procedures require the raw time-domain data collected by sensors, to be first processed and handcrafted into parameters like Fast Fourier Transform (FFT) coefficients, Statistical Moments, etc. before being fed as inputs. Usage of ANN with raw time domain data is inadequate as it suffers from the vanishing gradient problem. We propose a Deep Learning Multi-channel Convolutional Neural Network (McCNN) architecture here, which eliminates this problem and employs the RGB image analogy for channelizing raw time-domain vibration signals from various sub-systems of the rotor-bearing installation. It is shown that this architecture effectively works on raw time-domain data and recognizes all kinds of Single-Label Faults. For Multi-Label faults also, which generally get classified as erroneous Single-Label type through routine codes, the parallel architecture is demonstrated to give good results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009714",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Data mining",
      "Deep learning",
      "Fault (geology)",
      "Geology",
      "Pattern recognition (psychology)",
      "Programming language",
      "Raw data",
      "Seismology",
      "Support vector machine",
      "Time domain"
    ],
    "authors": [
      {
        "surname": "Sonkul",
        "given_name": "Nikhil A."
      },
      {
        "surname": "Dhage",
        "given_name": "Gaurav S."
      },
      {
        "surname": "Vyas",
        "given_name": "Nalinaksh S."
      }
    ]
  },
  {
    "title": "Assessing the competency of a semi-parametric expert system in the realms of response characterization uncertainty in premixed methanol dual fuel diesel combustion strategies: In critique to RSM",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115516",
    "abstract": "Engine response characterization endeavours of the day are faced with the contradictory obligations of developing an accurate engine response map in face of an ever-expanding parametric space and decreasing test-bench based prototype development time cycles. Under such imperatives of sparse experimental regimens, the present study focuses on the pivotal notions of reliability in a data-driven multi-variate response modelling endeavour in the complex realms of a premixed methanol dual fuel diesel operation. In the study, the Gene Expression Programming (GEP) technique with its distinguishing features of adaptivity in model evolution has been critically examined for its credibility as an expert system in such emerging combustion strategies. The relevance and competency of the GEP method has been rationalized to this end by comparing its performance with the traditional Response Surface Methodology which has been the mainstay in such engine response characterization objectives. Model integrity has been analysed and compared through a multitude of conventional and improvised statistical goodness-of-fit measures encompassing absolute and relative error metrics. Model reliability was adjudged through an exhaustive uncertainty analysis. Information theoretic measures of Kullback-Leibler divergence and improvised Thiel uncertainty coefficients were employed to compare the quality of system knowledge gained by the competing techniques. Generalization capability was reckoned through all the respective metrics applied across a test data set held blind to the meta-model training sequence. Comparable regression scores across conventional and standardized correlation measures notwithstanding, all GEP evolved meta-models registered substantially lower footprints in all statistical error metrics together with lower uncertainty bandwidths of estimation and information loss across both training and test data simultaneously for all responses explored. The outcome of the study arguably evokes considerable reconsideration on the choice of conventional RSM based characterization strategies in the paradigms of system response uncertainty prevalent in the emergent engine combustion strategies involving significant yet unknown degrees of non-linearity in its parametric operational space.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100926X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Divergence (linguistics)",
      "Generalization",
      "Law",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Parametric statistics",
      "Philosophy",
      "Physics",
      "Political science",
      "Power (physics)",
      "Quantum mechanics",
      "Relevance (law)",
      "Reliability (semiconductor)",
      "Statistics",
      "Uncertainty quantification"
    ],
    "authors": [
      {
        "surname": "Kakati",
        "given_name": "Dipankar"
      },
      {
        "surname": "Banerjee",
        "given_name": "Rahul"
      }
    ]
  },
  {
    "title": "DeepAssociate: A deep learning model exploring sequential influence and history-candidate association for sequence recommendation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115587",
    "abstract": "A remarkable progress in sequential recommendation field lies on deep learning techniques, where deep learning was widely used to capture user preference from behavior records. However, researchers usually place emphasis on sequential change while ignore the correlation between user’s historical behaviors and candidate item’s characteristics (history-candidate association), which leads to the inaccurate matching between target users and candidate items. In this paper, we proposed a deep learning model to explore both sequential influence and history-candidate association in sequential recommendation, namely DeepAssociate. First, we considered the history-candidate association in user preference representation and obtained it by two steps, including sequential influence extraction and association feature extraction. Then, by defining a weighted objective function, we introduced an integrated framework which makes a combination of sequential and association features extraction and prediction module to enhance recommendation performance. Experimental results on four real-world datasets demonstrated that DeepAssociate model outperformed state-of-the-art methods on recommendation performance. Furthermore, a series of extensive experiments indicated the benefit of utilizing history-candidate association feature in reducing model complexity and accelerating model convergence.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009891",
    "keywords": [
      "Artificial intelligence",
      "Association (psychology)",
      "Association rule learning",
      "Biology",
      "Computer science",
      "Convergence (economics)",
      "Data mining",
      "Deep learning",
      "Economic growth",
      "Economics",
      "Epistemology",
      "Feature (linguistics)",
      "Field (mathematics)",
      "Genetics",
      "Law",
      "Linguistics",
      "Machine learning",
      "Matching (statistics)",
      "Mathematics",
      "Philosophy",
      "Political science",
      "Politics",
      "Preference",
      "Pure mathematics",
      "Representation (politics)",
      "Sequence (biology)",
      "Sequence learning",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Yingxue"
      },
      {
        "surname": "Gan",
        "given_name": "Mingxin"
      }
    ]
  },
  {
    "title": "Enhanced Harris hawks optimization with multi-strategy for global optimization tasks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115499",
    "abstract": "Harris Hawks Optimization (HHO) algorithm is a newly proposed meta-heuristic optimization algorithm that simulates the hunting process of the Harris hawks. It has the characteristics of fewer adjustment parameters and a strong optimization effect, resulting in strong competitiveness in similar optimization algorithms. However, HHO is prone to premature convergence and low convergence accuracy when dealing with specific complex optimization problems. Therefore, our work integrates two novel strategies into the standard HHO to gain enhanced exploration and exploitation capabilities. Specifically, our work firstly proposed an exploration strategy based on logarithmic spiral and opposition-based learning to improve the exploration ability of HHO. Secondly, the local search technique for Rosenbrock Method (RM) is modified to dynamically fuse into the standard HHO to enhance the algorithm’s local search capability and improve the convergence accuracy. The novel meta-heuristic algorithm proposed in this paper is called RLHHO. Finally, to validate the algorithm’s effectiveness, the proposed RLHHO algorithm is fully performance tested with eight other traditional meta-heuristic optimization algorithms on 23 benchmark functions and 30 IEEE CEC2014 test functions. Besides, another six advanced meta-heuristics algorithms are also compared in the 30 CEC’2014 test functions. The experimental results show that RLHHO performs significantly better than HHO as well as other traditional and advanced meta-heuristic algorithms in most of the test functions. To test the scalability of RLHHO in complex real-world problems, it was used to optimize the solution of three constrained real-world engineering problems, and the experimental results show that RLHHO’s powerful performance can be used as an effective tool for solving constrained engineering problems. Also, an effective hybrid model of kernel extreme learning machine is developed on the basis of RLHHO to cope with bankruptcy prediction problem. The experimental results show that this hybrid model is highly competitive with other mainstream classifiers regarding stability and prediction accuracy. The supplementary info and answers to possiblequeries will be publicly available at https://www.researchgate.net/profile/Chenyang_Li39/research.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100909X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Convergence (economics)",
      "Database",
      "Economic growth",
      "Economics",
      "Geodesy",
      "Geography",
      "Heuristic",
      "Heuristics",
      "Mathematical optimization",
      "Mathematics",
      "Multi-swarm optimization",
      "Optimization problem",
      "Scalability",
      "Test functions for optimization"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "ChenYang"
      },
      {
        "surname": "Li",
        "given_name": "Jun"
      },
      {
        "surname": "Chen",
        "given_name": "HuiLing"
      },
      {
        "surname": "Jin",
        "given_name": "Ming"
      },
      {
        "surname": "Ren",
        "given_name": "Hao"
      }
    ]
  },
  {
    "title": "Remora optimization algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115665",
    "abstract": "In this paper, Remora Optimization Algorithm (ROA) is proposed, which is a new bionics-based, natural-inspired, and meta-heuristic algorithm. The inspiration for ROA is mainly due to the parasitic behavior of remora. Different locations are updated in different hosts: In some large hosts, remora feeds on the host's ectoparasites or wreckage and evades natural enemies, for example in the case of giant whales. In some small hosts, remora follows the host to move to the bait-rich area to prey, taking the fast-moving swordfish as an example. In the case of these two update methods, remora also makes some judges based on experience. If it takes the initiative to prey, it updates the host, makes a global update. If it eat around the host, remora does not change the host, and continues to local update. This algorithm is more inclined to provide a new idea for memetic algorithm, because the host in ROA can be reasonably replaced, such as ships, turtles, etc. The above dynamic mode and behavior are simulated mathematically and the validity of the ROA is tested with 29 benchmark questions and 5 actual engineering questions. Parallel comparisons are made with 10 other natural heuristics. The statistical results and comparisons show that ROA provides a very promising prospect and a strong competitive ability compared to other state-of-the-art heuristic techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010551",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Ecology",
      "Geodesy",
      "Geography",
      "Heuristic",
      "Heuristics",
      "Host (biology)",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Particle swarm optimization",
      "Swarm intelligence"
    ],
    "authors": [
      {
        "surname": "Jia",
        "given_name": "Heming"
      },
      {
        "surname": "Peng",
        "given_name": "Xiaoxu"
      },
      {
        "surname": "Lang",
        "given_name": "Chunbo"
      }
    ]
  },
  {
    "title": "Combating hate speech using an adaptive ensemble learning model with a case study on COVID-19",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115632",
    "abstract": "Social media platforms generate an enormous amount of data every day. Millions of users engage themselves with the posts circulated on these platforms. Despite the social regulations and protocols imposed by these platforms, it is difficult to restrict some objectionable posts carrying hateful content. Automatic hate speech detection on social media platforms is an essential task that has not been solved efficiently despite multiple attempts by various researchers. It is a challenging task that involves identifying hateful content from social media posts. These posts may reveal hate outrageously, or they may be subjective to the user or a community. Relying on manual inspection delays the process, and the hateful content may remain available online for a long time. The current state-of-the-art methods for tackling hate speech perform well when tested on the same dataset but fail miserably on cross-datasets. Therefore, we propose an ensemble learning-based adaptive model for automatic hate speech detection, improving the cross-dataset generalization. The proposed expert model for hate speech detection works towards overcoming the strong user-bias present in the available annotated datasets. We conduct our experiments under various experimental setups and demonstrate the proposed model’s efficacy on the latest issues such as COVID-19 and US presidential elections. In particular, the loss in performance observed under cross-dataset evaluation is the least among all the models. Also, while restricting the maximum number of tweets per user, we incur no drop in performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010265",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Economics",
      "Ensemble learning",
      "Generalization",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Process (computing)",
      "Social media",
      "Task (project management)",
      "User-generated content",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Agarwal",
        "given_name": "Shivang"
      },
      {
        "surname": "Chowdary",
        "given_name": "C. Ravindranath"
      }
    ]
  },
  {
    "title": "Data augmentation approaches using cycle-consistent adversarial networks for improving COVID-19 screening in portable chest X-ray images",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115681",
    "abstract": "The current COVID-19 pandemic, that has caused more than 100 million cases as well as more than two million deaths worldwide, demands the development of fast and accurate diagnostic methods despite the lack of available samples. This disease mainly affects the respiratory system of the patients and can lead to pneumonia and to severe cases of acute respiratory syndrome that result in the formation of several pathological structures in the lungs. These pathological structures can be explored taking advantage of chest X-ray imaging. As a recommendation for the health services, portable chest X-ray devices should be used instead of conventional fixed machinery, in order to prevent the spread of the pathogen. However, portable devices present several problems (specially those related with capture quality). Moreover, the subjectivity and the fatigue of the clinicians lead to a very difficult diagnostic process. To overcome that, computer-aided methodologies can be very useful even taking into account the lack of available samples that the COVID-19 affectation shows. In this work, we propose an improvement in the performance of COVID-19 screening, taking advantage of several cycle generative adversarial networks to generate useful and relevant synthetic images to solve the lack of COVID-19 samples, in the context of poor quality and low detail datasets obtained from portable devices. For validating this proposal for improved COVID-19 screening, several experiments were conducted. The results demonstrate that this data augmentation strategy improves the performance of a previous COVID-19 screening proposal, achieving an accuracy of 98.61% when distinguishing among NON-COVID-19 (i.e. normal control samples and samples with pathologies others than COVID-19) and genuine COVID-19 samples. It is remarkable that this methodology can be extrapolated to other pulmonary pathologies and even other medical imaging domains to overcome the data scarcity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010666",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Coronavirus disease 2019 (COVID-19)",
      "Disease",
      "Epistemology",
      "Infectious disease (medical specialty)",
      "Medicine",
      "Operating system",
      "Paleontology",
      "Pathology",
      "Philosophy",
      "Process (computing)",
      "Quality (philosophy)"
    ],
    "authors": [
      {
        "surname": "Morís",
        "given_name": "Daniel Iglesias"
      },
      {
        "surname": "de Moura Ramos",
        "given_name": "José Joaquim"
      },
      {
        "surname": "Buján",
        "given_name": "Jorge Novo"
      },
      {
        "surname": "Hortas",
        "given_name": "Marcos Ortega"
      }
    ]
  },
  {
    "title": "Prioritizing and aggregating interacting requirements for product-service system development",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115636",
    "abstract": "Requirements evaluation is critically important for the successful development of a product-service system (PSS). The requirements of a PSS often interact with each other, hence significantly influencing requirements evaluation and decision-making processes. The recent literature has proposed some methods such as fuzzy ANP and rough DEMATEL to evaluate interacting PSS requirements and to focus on requirements prioritization. However, aggregation with respect to interacting PSS requirements is seldomly considered. Alternatively, the weighted arithmetic mean method is implicitly used as the aggregation operator to aggregate PSS requirements. Hence, different effects of interactions among any subset of PSS requirements are not considered. This may result in sub-optimal alternatives being adopted for further PSS development. In order to solve this specific problem, a systematic method based on rough-fuzzy DEMATEL, 2-additive fuzzy measures, and the Choquet integral is proposed for aggregating interacting requirements for PSS development along with requirements prioritization. The proposed method utilizes the rough-fuzzy DEMATEL method to determine the weights of interacting PSS requirements when there is a group of experts providing subjective and linguistic assessments of influence strengths. By integrating the Choquet integral with 2-additive fuzzy measures, the proposed method can aggregate interacting PSS requirements non-additively by considering 2-order interactions between any two requirements. To demonstrate its feasibility and advantages, the proposed method is applied to evaluate requirement interactions for a smart wearable medical system.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010307",
    "keywords": [
      "Aggregate (composite)",
      "Artificial intelligence",
      "Choquet integral",
      "Composite material",
      "Computer science",
      "Economics",
      "Economy",
      "Finance",
      "Functional requirement",
      "Fuzzy logic",
      "Geometry",
      "Materials science",
      "Mathematics",
      "Medicine",
      "Order (exchange)",
      "Product (mathematics)",
      "Programming language",
      "Requirement prioritization",
      "Requirements analysis",
      "Requirements management",
      "Risk analysis (engineering)",
      "Service (business)",
      "Software",
      "Software engineering"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Xinwei"
      },
      {
        "surname": "Li",
        "given_name": "Jing"
      },
      {
        "surname": "Eres",
        "given_name": "Hakki"
      },
      {
        "surname": "Zheng",
        "given_name": "Chen"
      }
    ]
  },
  {
    "title": "Matrix completion on learnt graphs: Application to collaborative filtering",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115652",
    "abstract": "There are two broad frameworks for collaborative filtering. Chronologically, the neighborhood based models came first – they were based on linear interpolation where the interpolation weights were proportional to the similarities between users’ and items’. Latent factor models were introduced later; they were based on the underlying assumption that the data matrix is low-rank. Both the approaches essentially completed the partially observed rating matrix. These two approaches were later combined by regularizing the latent factor model with graph Laplacian computed from the similarities defined in the neighborhood based models. However, this graph was computed from partially observed data and may have failed to represent the true similarities. This work addresses this issue; instead of computing the graph and assuming it to be fixed during the matrix completion process, we will alternate between updating (refining) the graph assuming the matrix to be fully observed, and completing the matrix, assuming the graph to be fixed. Benchmarking with fixed graph techniques as well as state-of-the-art collaborative filtering algorithms on popular Movielens datasets have shown that our proposed approach indeed improves over the rest.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010435",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmarking",
      "Business",
      "Collaborative filtering",
      "Composite material",
      "Computer science",
      "Gaussian",
      "Graph",
      "Laplacian matrix",
      "Machine learning",
      "Marketing",
      "Materials science",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Matrix completion",
      "MovieLens",
      "Physics",
      "Quantum mechanics",
      "Recommender system",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Mongia",
        "given_name": "Aanchal"
      },
      {
        "surname": "Majumdar",
        "given_name": "Angshul"
      }
    ]
  },
  {
    "title": "Machine learning model for diagnostic method prediction in parasitic disease using clinical information",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115658",
    "abstract": "Diagnosing a parasitic disease is a very difficult job in clinical practice. In this study, we constructed a machine learning model for diagnosis prediction using patient information. First, we diagnosed whether a patient has a parasitic disease. Next, we predicted the proper diagnosis method among the six types of diagnostic terms (biopsy, endoscopy, microscopy, molecular, radiology, and serology) if the patient has a parasitic disease. To make the datasets, we extracted patient information from PubMed abstracts from 1956 to 2019. We then used two datasets: the prediction for parasite-infected patient dataset (N = 8748) and the prediction for diagnosis method dataset (N = 3780). We then compared four machine learning models: support vector machine, random forest, multi-layered perceptron, and gradient boosting. To solve the data imbalance problem, the synthetic minority over-sampling technique and TomekLinks were used. In the parasite-infected patient dataset, the random forest, random forest with synthetic minority over-sampling technique, gradient boosting, gradient boosting with synthetic minority over-sampling technique, and gradient boosting with TomekLinks demonstrated the best performances (AUC: 79%). In predicting the diagnosis method dataset, gradient boosting with synthetic minority over-sampling technique was the best model (AUC: 87%). For the class prediction, gradient boosting demonstrated the best performances in biopsy (AUC: 88%). In endoscopy (AUC: 94%), molecular (AUC: 90%), and radiology (AUC: 88%), gradient boosting with synthetic minority over-sampling technique demonstrated the best performance. Random forest demonstrated the best performances in microscopy (AUC: 82%) and serology (AUC: 85%). We calculated feature importance using gradient boosting; age was the highest feature importance. In conclusion, this study demonstrated that gradient boosting with synthetic minority over-sampling technique can predict a parasitic disease and serve as a promising diagnosis tool for binary classification and multi-classification schemes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010496",
    "keywords": [
      "Artificial intelligence",
      "Biopsy",
      "Boosting (machine learning)",
      "Computer science",
      "Gradient boosting",
      "Machine learning",
      "Medicine",
      "Pattern recognition (psychology)",
      "Radiology",
      "Random forest",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "You Won"
      },
      {
        "surname": "Choi",
        "given_name": "Jae Woo"
      },
      {
        "surname": "Shin",
        "given_name": "Eun-Hee"
      }
    ]
  },
  {
    "title": "Evidential reasoning for preprocessing uncertain categorical data for trustworthy decisions: An application on healthcare and finance",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115597",
    "abstract": "The uncertainty attributed by discrepant data in AI-enabled decisions is a critical challenge in highly regulated domains such as health care and finance. Ambiguity and incompleteness due to missing values in output and input attributes, respectively, is ubiquitous in these domains. It could have an adverse impact on a certain unrepresented set of people in the training data without a developer’s intention to discriminate. The inherently non-numerical nature of categorical attributes than numerical attributes and the presence of incomplete and ambiguous categorical attributes in a dataset increases the uncertainty in decision-making. This paper addresses the challenges in handling categorical attributes as it is not addressed comprehensively in previous research. Three sources of uncertainties in categorical attributes are recognised in this research. The informational uncertainty, unforeseeable uncertainty in the decision task environment, and the uncertainty due to lack of pre-modelling explainability in categorical attributes are addressed in the proposed methodology on maximum likelihood evidential reasoning (MAKER). It can transform and impute incomplete and ambiguous categorical attributes into interpretable numerical features. It utilises a notion of weight and reliability to include subjective expert preference over a piece of evidence and the quality of evidence in a categorical attribute, respectively. The MAKER framework strives to integrate the recognised uncertainties in the transformed input data that allow a model to perceive data limitations during the training regime and acknowledge doubtful predictions by supporting trustworthy pre-modelling and post modelling explainability. The ability to handle uncertainty and its impact on explainability is demonstrated on a real-world healthcare and finance data for different missing data scenarios in three types of AI algorithms: deep-learning, tree-based, and rule-based model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009970",
    "keywords": [
      "Artificial intelligence",
      "Categorical variable",
      "Computer science",
      "Computer security",
      "Data mining",
      "Data pre-processing",
      "Data science",
      "Economic growth",
      "Economics",
      "Health care",
      "Machine learning",
      "Preprocessor",
      "Trustworthiness"
    ],
    "authors": [
      {
        "surname": "Sachan",
        "given_name": "Swati"
      },
      {
        "surname": "Almaghrabi",
        "given_name": "Fatima"
      },
      {
        "surname": "Yang",
        "given_name": "Jian-Bo"
      },
      {
        "surname": "Xu",
        "given_name": "Dong-Ling"
      }
    ]
  },
  {
    "title": "Random sampling with fuzzy replacement",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115602",
    "abstract": "In this study, the probability of selecting unit again of the selecting sample is determined by the fuzzification parameter. Accordingly, a random sampling method has been proposed providing a transition between with replacement or without replacement of selecting sample. Especially, this paper provides a bridge between sampling with and without replacement with the fuzzification parameter which takes values in the range [0,1]. When it is zero, the proposed method performs as 'without replacement', and when it is one, it performs 'with replacement'. A simulation study is carried out to examine the performance of the proposed method and it is carefully studied how is the behavior of the difference between the effective sample size, the expectation of sample size, and the actual sample size. In real life problems modelling, only two different random sampling is performed in the literature. However, this study provides more reliable modelling by adding a variability in the variance. Lastly, a scenario is presented for which the introduced sampling design is sensible to be implemented and it produces some desire properties.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010010",
    "keywords": [
      "Accounting",
      "Artificial intelligence",
      "Business",
      "Chemistry",
      "Chromatography",
      "Composite material",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Demography",
      "Filter (signal processing)",
      "Fuzzy logic",
      "Fuzzy set",
      "Materials science",
      "Mathematics",
      "Population",
      "Programming language",
      "Range (aeronautics)",
      "Sample (material)",
      "Sample size determination",
      "Sampling (signal processing)",
      "Set (abstract data type)",
      "Simple random sample",
      "Sociology",
      "Statistics",
      "Stratified sampling",
      "Variance (accounting)"
    ],
    "authors": [
      {
        "surname": "Kesemen",
        "given_name": "Orhan"
      },
      {
        "surname": "Tiryaki",
        "given_name": "Buğra Kaan"
      },
      {
        "surname": "Tezel",
        "given_name": "Özge"
      },
      {
        "surname": "Özkul",
        "given_name": "Eda"
      },
      {
        "surname": "Naz",
        "given_name": "Ebru"
      }
    ]
  },
  {
    "title": "Web usage analysis of Pillar 3 disclosed information by deposit customers in turbulent times",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115503",
    "abstract": "Market discipline has been a scrutinized area since the last financial crisis in 2008. Regulators strengthened their role particularly through Pillar 3 in Basel III. However, there are still some aspects of market discipline that deserve special attention to avoid future failures. This study focuses on the analysis of the interest and behaviour of deposit stakeholders based on website data dedicated to disclosures of commercial bank in Slovakia during and after turbulent times (period 2009–2012). The data consists of log files, and web mining techniques were applied (the modelling of web user behaviour in dependence on time - based on the proposals of the authors). The results show that also in turbulent times, stakeholders’ interest in Pillar 3 disclosures is low (in line with (Munk, Pilkova, Benko, & Blažeková, 2017)) and the highest interest was identified for the Pricing List category. After turbulent times, Pillar 3 categories (Pillar 3 related information and Pillar 3 disclosures) have weak interest, with peaks at the beginning of the year, and the highest increase was in the Business Conditions category. The results suggest that the enhancement of interest of key stakeholders in disclosures inevitably requires changes to deliver sufficient disclosure data structures and to design a disclosure policy that fulfils regulatory expectations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009131",
    "keywords": [
      "Accounting",
      "Business",
      "Computer science",
      "Engineering",
      "Pillar",
      "Structural engineering",
      "Web application",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Munk",
        "given_name": "Michal"
      },
      {
        "surname": "Pilkova",
        "given_name": "Anna"
      },
      {
        "surname": "Benko",
        "given_name": "Lubomir"
      },
      {
        "surname": "Blazekova",
        "given_name": "Petra"
      },
      {
        "surname": "Svec",
        "given_name": "Peter"
      }
    ]
  },
  {
    "title": "MFFNet: Multi-dimensional Feature Fusion Network based on attention mechanism for sEMG analysis to detect muscle fatigue",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115639",
    "abstract": "Muscle fatigue detection based on surface Electromyography (sEMG) is one of the essential goals of human–computer interaction. The main challenge is that the sEMG signal is unstable and complex. Meanwhile, the individual’s difference in fatigue tolerance will increase the detection difficulty. In order to reduce the impact of the above challenges, in this article, we use the sEMG signal to detect muscle fatigue based on the Multi-dimensional Feature Fusion Network (MFFNet), which is composed of Attention Frequency domain Network (AFNet) and Attention Time domain Network (ATNet). Precisely, AFNet consists of the convolutional neural network, channel attention network and spatial attention network. ATNet is composed of a two-way long and short-term memory network and time attention network. Furthermore, through the filter and Gaussian short-time Fourier transform, we can analyze the feature of the time domain and frequency domain of sEMG. Subsequently, fuse features of different dimensions are used to predict fatigue detection in many muscle fatigue detection experiments based on sEMG. The proposed method has better performance and interpretability. Experimental results prove that the proposed method can promote the development of sEMG in the field of muscle fatigue detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010332",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Electrical engineering",
      "Electromyography",
      "Engineering",
      "Feature (linguistics)",
      "Frequency domain",
      "Fuse (electrical)",
      "Interpretability",
      "Linguistics",
      "Medicine",
      "Muscle fatigue",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physical medicine and rehabilitation",
      "Programming language",
      "SIGNAL (programming language)",
      "Time domain"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yongqing"
      },
      {
        "surname": "Chen",
        "given_name": "Siyu"
      },
      {
        "surname": "Cao",
        "given_name": "Wenpeng"
      },
      {
        "surname": "Guo",
        "given_name": "Peng"
      },
      {
        "surname": "Gao",
        "given_name": "Dongrui"
      },
      {
        "surname": "Wang",
        "given_name": "Manqing"
      },
      {
        "surname": "Zhou",
        "given_name": "Jiliu"
      },
      {
        "surname": "Wang",
        "given_name": "Ting"
      }
    ]
  },
  {
    "title": "Data-driven modeling of technology acceptance: A machine learning perspective",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115584",
    "abstract": "Understanding, explaining, and predicting technology acceptance have dominated the research of information systems (IS) for more than two decades. Past research has favored explanatory modeling, considering it a prediction-oriented approach; until recently, predictive analytics has been poorly understood and widely underappreciated. Research on IS for prediction-oriented modeling like predictive analytics remains rare, despite its potential for development and utility. Our research addresses the capacity of predictive analytics for advancing technology acceptance modeling by assessing predictive power, evaluating the current frameworks, and introducing new constructs. This research formulates a unique data-driven approach that utilizes machine learning (ML) and predictive analytics-based modeling to empirically predict end users’ acceptance of consumer-use technology in a non-organizational setting using the following steps. First, a thorough analysis of IS literature was conducted to explore the constructs of technology use in various contexts. Second, the Twitter API and interviews were utilized to extract new constructs and evaluate the content of current models of technology acceptance. Third, a unique technology acceptance model of thirty-seven constructs was developed and tested on thirty-two personal technologies with heterogeneous subjects. Fourth, ML algorithms estimated the predictive power of the model and ranked the influence of its variables, achieving an R 2 of 0.97 and an error rate of 0.04. Thirteen new constructs were successfully introduced, including eight technology characteristics. Four other constructs were reinstated, presenting the utility of the ML approach to contribute to research. Ranking the thirty-seven constructs by applying a sensitivity analysis on the basis of partial derivatives showed the differences between the predictive model and the explanatory model of personal technology acceptance. The proposed approach demonstrates the capacity of ML to formulate a complex model of personal technology acceptance, which further develops technology acceptance models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009866",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data science",
      "Human–computer interaction",
      "Knowledge management",
      "Machine learning",
      "Perspective (graphical)",
      "Technology acceptance model",
      "Usability"
    ],
    "authors": [
      {
        "surname": "Alwabel",
        "given_name": "Asim Suleman A."
      },
      {
        "surname": "Zeng",
        "given_name": "Xiao-Jun"
      }
    ]
  },
  {
    "title": "Representation learning using Attention Network and CNN for Heterogeneous networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115628",
    "abstract": "Network embedding (NE), also known as network representation learning (NRL), is a method to learn a low-dimensional latent representation of nodes in an information network. The real-world data is usually presented in the form of heterogeneous information network (HIN) with multiple types of nodes and edges. Because of the rich information in HINs, it is necessary for a network embedding method to incorporate this information into the low-dimensional potential representation of the nodes as much as possible. In this paper, we propose a semi-supervised representation learning model using a graph attention network and a convolutional neural network (CNN) for HINs, called RANCH. In the part of the graph attention network, we construct a heterogeneous graph attention network using heterogeneous edges to preserve the features of nodes and the structure of network. In the part of the CNN, we leverage a 1D-CNN sentence classification model from natural language processing (NLP) community by adopting edge-constrained truncated random walks to generate node sequences, which can be treated as a corpus of words and sentences. The latter part further integrates the structural information of the network on the basis of the previous part and strengthens the influence of the node’s label information on the node representation. We have performed experiments of node classification on three real-world datasets, and the result shows that our model performs better than the state-of-the-arts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010228",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Embedding",
      "Engineering",
      "Feature learning",
      "Graph",
      "Heterogeneous network",
      "Law",
      "Leverage (statistics)",
      "Machine learning",
      "Node (physics)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Structural engineering",
      "Telecommunications",
      "Theoretical computer science",
      "Wireless",
      "Wireless network"
    ],
    "authors": [
      {
        "surname": "Tong",
        "given_name": "Ning"
      },
      {
        "surname": "Tang",
        "given_name": "Ying"
      },
      {
        "surname": "Chen",
        "given_name": "Bo"
      },
      {
        "surname": "Xiong",
        "given_name": "Lirong"
      }
    ]
  },
  {
    "title": "Automated classification of remote sensing images using multileveled MobileNetV2 and DWT techniques",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115659",
    "abstract": "Automated classification of remote sensing images is one of the complex issues in robotics and machine learning fields. Many models have been proposed for remote sensing image classification (RSIC) to obtain high classification performance. The objective of this study are twofold. First, to create a new space object image collection as such a dataset is not currently available. Second, propose a novel RSIC model to yield highest classification performance using our newly created dataset. Our presented automated classification model consists of multilevel deep feature generation, iterative feature selection, and classification steps. The features are extracted from the images using pre-trained MobileNetV2 and discrete wavelet transform (DWT) methods. The combination of DWT and MobileNetV2 generates large number of features. Then, iterative neighborhood component analysis (INCA) is used to select the best features. Finally, selected features are fed to support vector machine (SVM) for automated classification. The presented model is validated using two RSIC datasets: UC-Merced, and newly created space object images (publicly available at: http://web.firat.edu.tr/turkertuncer/space_object.rar). The developed model has obtained an accuracy of 98.10% and 95.95% using UC-Merced, and newly generated space object image datasets, respectively with 10-fold cross-validation strategy. It can be concluded from the results that, the presented RSIC model is accurate and ready for real-world applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010502",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Contextual image classification",
      "Data mining",
      "Discrete wavelet transform",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature vector",
      "Image (mathematics)",
      "Linguistics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Support vector machine",
      "Wavelet",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Karadal",
        "given_name": "Can Haktan"
      },
      {
        "surname": "Kaya",
        "given_name": "M. Cagri"
      },
      {
        "surname": "Tuncer",
        "given_name": "Turker"
      },
      {
        "surname": "Dogan",
        "given_name": "Sengul"
      },
      {
        "surname": "Acharya",
        "given_name": "U. Rajendra"
      }
    ]
  },
  {
    "title": "Improving exploration efficiency of deep reinforcement learning through samples produced by generative model",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115680",
    "abstract": "Deep reinforcement learning (DRL) has made remarkable achievements in artificial intelligence. However, it relies on stochastic exploration that suffers from low efficiency, especially in the early learning stages, of which the time complexity is nearly exponential. To solve the problem, an algorithm, referred to as Generative Action Selection through Probability (GRASP), is proposed to improve exploration in reinforcement learning. The primary insight is to reshape exploration spaces to limit the choice of exploration behaviors. More specifically, GRASP trains a generator to generate the exploration spaces from demonstrations by generative adversarial network (GAN). And then the agent selects actions from new exploration spaces via modified ϵ -greedy algorithm to incorporate GRASP with existing standard deep reinforcement learning algorithms. Experiment results showed that deep reinforcement learning equipped with GRASP demonstrated significant improvements in simulated environments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010654",
    "keywords": [
      "Action (physics)",
      "Action selection",
      "Artificial intelligence",
      "Biology",
      "Cartography",
      "Computer science",
      "GRASP",
      "Generative grammar",
      "Generative model",
      "Generator (circuit theory)",
      "Geography",
      "Machine learning",
      "Neuroscience",
      "Perception",
      "Physics",
      "Power (physics)",
      "Programming language",
      "Quantum mechanics",
      "Reinforcement learning",
      "Train"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Dayong"
      },
      {
        "surname": "Zhu",
        "given_name": "Fei"
      },
      {
        "surname": "Liu",
        "given_name": "Quan"
      },
      {
        "surname": "Zhao",
        "given_name": "Peiyao"
      }
    ]
  },
  {
    "title": "Data fusion based on Searchlight analysis for the prediction of Alzheimer’s disease",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115549",
    "abstract": "In recent years, several computer-aided diagnosis (CAD) systems have been proposed for an early identification of dementia. Although these approaches have mostly used the transformation of data into a different feature space, more precise information can be gained from a Searchlight strategy. The current study presents a data fusion classification system that employs magnetic resonance imaging (MRI) and neuropsychological tests to distinguish between Mild-Cognitive Impairment (MCI) patients that convert to Alzheimer’s disease (AD) and those that remain stable. Specifically, this method uses a nested cross-validation procedure to compute the optimum contribution of each data modality in the final decision. The model employs Support-Vector Machine (SVM) classifiers for both data modalities and is combined with Searchlight when applied to neuroimaging. We compared the performance of our system with an alternative based on Principal Component Analysis (PCA) for dimensionality reduction. Results show that Searchlight outperformed PCA both for uni/multimodal classification, obtaining a maximum accuracy of 80.9% when combining data from six and twelve months before patients converted to AD. Moreover, Searchlight allowed the identification of the most informative regions at different stages of the longitudinal study, which can be crucial for a better understanding of the development of AD. Additionally, results do not depend on the parcellations provided by a specific brain atlas, which manifests the robustness and the spatial precision of the method proposed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009556",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Dimensionality reduction",
      "Gene",
      "Machine learning",
      "Medicine",
      "Neuroimaging",
      "Pattern recognition (psychology)",
      "Principal component analysis",
      "Psychiatry",
      "Robustness (evolution)",
      "Sensor fusion",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Arco",
        "given_name": "Juan E."
      },
      {
        "surname": "Ramírez",
        "given_name": "Javier"
      },
      {
        "surname": "Górriz",
        "given_name": "Juan M."
      },
      {
        "surname": "Ruz",
        "given_name": "María"
      }
    ]
  },
  {
    "title": "Control chart recognition based on the parallel model of CNN and LSTM with GA optimization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115689",
    "abstract": "Quality control process has become one of the most critical issues in intelligent manufacturing. As the most practical and prevalent tools for continuously monitoring, control chart patterns (CCPs) can be automatically recognized to judge the fault of production process. However, there are three shortcomings in the previous researches. 1) Insufficient or superfluous features are considered; 2) Few studies simultaneously take the local features and time sequence information into account; 3) With the exception of empiricism, little work has been done on searching for the optimal hyper-parameters for the neural networks. Accordingly, FFR-GACLN, a method for ten CCPs recognition is proposed in this paper. Such a method is comprised of two sections including the fusion feature reduction (FFR) and GACLN model construction. Firstly, the feature extraction comprising of features fusion and feature dimensionality reduction by convolutional auto-encoder is applied to enhance the reliability and performance of the extracted features. Then, such features are input into GACLN, a network combining the convolutional neural network (CNN) and the long-short term memory (LSTM) for achieving the final recognition task. Moreover, genetic algorithm (GA) is applied to search for the optimal hyper-parameters for FFR-GACLN. The quantitative and simulation results demonstrate that the performance of the proposed method is superior over the previous techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010745",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Dimensionality reduction",
      "Feature (linguistics)",
      "Genetic algorithm",
      "Geometry",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Power (physics)",
      "Process (computing)",
      "Quantum mechanics",
      "Reduction (mathematics)",
      "Reliability (semiconductor)"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Yaoxiang"
      },
      {
        "surname": "Zhang",
        "given_name": "Min"
      }
    ]
  },
  {
    "title": "A two-stage evolutionary strategy based MOEA/D to multi-objective problems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115654",
    "abstract": "The balance of convergence and diversity plays a significant role to the performance of multi-objective evolutionary algorithms (MOEAs). The MOEA/D is a very popular multi-objective optimization algorithm and has been used to solve various real world problems. Like many other algorithms, the MOEA/D also has insufficient ability of convergence and diversity when tackling certain complex multi-objective optimization problems (MOPs). In this paper, a novel algorithm named MOEA/D-TS is proposed for effectively solving MOPs. The new algorithm adopts two stages evolution strategies, the first stage is focused on pushing the solutions into the area of the Pareto front and speeding up its convergence ability, after that, the second stage conducts in the operating solution’s diversity and makes the solutions distributed uniformly. The performance of MOEA/D-TS is validated in the ZDT, DTLZ and IMOP problems. Compared with others popular and variants algorithms, the experimental results demonstrate that the proposed algorithm has advantage over other algorithms with regard to the convergence and diversity in most of the tested problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010459",
    "keywords": [
      "Algorithm",
      "Anthropology",
      "Artificial intelligence",
      "Computer science",
      "Convergence (economics)",
      "Diversity (politics)",
      "Economic growth",
      "Economics",
      "Evolutionary algorithm",
      "Mathematical optimization",
      "Mathematics",
      "Multi-objective optimization",
      "Optimization problem",
      "Pareto principle",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Cao",
        "given_name": "Jie"
      },
      {
        "surname": "Zhang",
        "given_name": "Jianlin"
      },
      {
        "surname": "Zhao",
        "given_name": "Fuqing"
      },
      {
        "surname": "Chen",
        "given_name": "Zuohan"
      }
    ]
  },
  {
    "title": "Fraudulent traffic detection in online advertising with bipartite graph propagation algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115573",
    "abstract": "Online advertising fraud has brought huge economic losses to global advertisers. We propose and compare a set of graph models to detect fraudulent Internet Protocol addresses (IPs) in online advertising. We use the classic label propagation algorithm (LPA) on the IP-campaign bipartite graph as our baseline. Based on two behavioral patterns of fraudulent IPs, the power-law distribution and behavioral similarity, we propose two new methods to calculate IP fraud scores and three new methods to calculate campaign fraud scores. Their combinations produce six variations of LPA. Finally, we integrate the six variations with the random forest algorithm. Results show that all six variations outperform the basic bipartite graph model. The integrated random forest algorithm outperforms the basic bipartite graph model in precision, ROC, F1 score, and kappa by 17.9%, 15.0%, 17.9%, and 23.1%, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009775",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bipartite graph",
      "Computer science",
      "Data mining",
      "Graph",
      "Machine learning",
      "Random forest",
      "The Internet",
      "Theoretical computer science",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Yue"
      },
      {
        "surname": "Xu",
        "given_name": "Yunjie"
      },
      {
        "surname": "Li",
        "given_name": "Jiaoyang"
      }
    ]
  },
  {
    "title": "EA-based hyperparameter optimization of hybrid deep learning models for effective drug-target interactions prediction",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115525",
    "abstract": "The identification of drug-target interactions (DTIs) is an important process in drug repositioning and drug discovery. However, it is very expensive and time-consuming to determine all possible DTIs with experimental approaches. Most existing machine learning-based methods formulate the DTIs prediction problem as a binary classification problem. Nevertheless, the lack of experimentally validated negative samples results in imbalanced class distribution within the datasets, which may have a negative influence on the DTI prediction performance. Casting DTI prediction task as a regression problem seems an interesting alternative to avoid this issue especially with the recent increase in protein structural data and DTI datasets. Within this context, a twofold contribution is described in this paper. First, we propose a novel deep learning model for predicting drug-target binding affinities called “Convolution Neural Network with Attention-based bidirectional Long Short-Term Memory network” (CNN-AbiLSTM), which combines a CNN with an attention-based biLSTM. Second, building a powerful hybrid CNN-AbiLSTM model can be highly complicated and requires a suitable setting of the model’s hyperparameters. To handle this problem, we propose an evolutionary algorithm-based framework more specifically a Differential Evolution (DE) algorithm to find the optimal configuration of the proposed model. Experimental results show that the proposed DE-based CNN-AbiLSTM model achieves better performance compared with baseline methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009349",
    "keywords": [
      "Artificial intelligence",
      "Binary classification",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Convolutional neural network",
      "Deep learning",
      "Hyperparameter",
      "Machine learning",
      "Operating system",
      "Paleontology",
      "Process (computing)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Mahdaddi",
        "given_name": "Abla"
      },
      {
        "surname": "Meshoul",
        "given_name": "Souham"
      },
      {
        "surname": "Belguidoum",
        "given_name": "Meriem"
      }
    ]
  },
  {
    "title": "Fraudulent traffic detection in online advertising with bipartite graph propagation algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115573",
    "abstract": "Online advertising fraud has brought huge economic losses to global advertisers. We propose and compare a set of graph models to detect fraudulent Internet Protocol addresses (IPs) in online advertising. We use the classic label propagation algorithm (LPA) on the IP-campaign bipartite graph as our baseline. Based on two behavioral patterns of fraudulent IPs, the power-law distribution and behavioral similarity, we propose two new methods to calculate IP fraud scores and three new methods to calculate campaign fraud scores. Their combinations produce six variations of LPA. Finally, we integrate the six variations with the random forest algorithm. Results show that all six variations outperform the basic bipartite graph model. The integrated random forest algorithm outperforms the basic bipartite graph model in precision, ROC, F1 score, and kappa by 17.9%, 15.0%, 17.9%, and 23.1%, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009775",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bipartite graph",
      "Computer science",
      "Data mining",
      "Graph",
      "Machine learning",
      "Random forest",
      "The Internet",
      "Theoretical computer science",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Yue"
      },
      {
        "surname": "Xu",
        "given_name": "Yunjie"
      },
      {
        "surname": "Li",
        "given_name": "Jiaoyang"
      }
    ]
  },
  {
    "title": "EA-based hyperparameter optimization of hybrid deep learning models for effective drug-target interactions prediction",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115525",
    "abstract": "The identification of drug-target interactions (DTIs) is an important process in drug repositioning and drug discovery. However, it is very expensive and time-consuming to determine all possible DTIs with experimental approaches. Most existing machine learning-based methods formulate the DTIs prediction problem as a binary classification problem. Nevertheless, the lack of experimentally validated negative samples results in imbalanced class distribution within the datasets, which may have a negative influence on the DTI prediction performance. Casting DTI prediction task as a regression problem seems an interesting alternative to avoid this issue especially with the recent increase in protein structural data and DTI datasets. Within this context, a twofold contribution is described in this paper. First, we propose a novel deep learning model for predicting drug-target binding affinities called “Convolution Neural Network with Attention-based bidirectional Long Short-Term Memory network” (CNN-AbiLSTM), which combines a CNN with an attention-based biLSTM. Second, building a powerful hybrid CNN-AbiLSTM model can be highly complicated and requires a suitable setting of the model’s hyperparameters. To handle this problem, we propose an evolutionary algorithm-based framework more specifically a Differential Evolution (DE) algorithm to find the optimal configuration of the proposed model. Experimental results show that the proposed DE-based CNN-AbiLSTM model achieves better performance compared with baseline methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009349",
    "keywords": [
      "Artificial intelligence",
      "Binary classification",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Convolutional neural network",
      "Deep learning",
      "Hyperparameter",
      "Machine learning",
      "Operating system",
      "Paleontology",
      "Process (computing)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Mahdaddi",
        "given_name": "Abla"
      },
      {
        "surname": "Meshoul",
        "given_name": "Souham"
      },
      {
        "surname": "Belguidoum",
        "given_name": "Meriem"
      }
    ]
  },
  {
    "title": "E-Res U-Net: An improved U-Net model for segmentation of muscle images",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115625",
    "abstract": "In this paper, we propose a new semantic segmentation network called ’E-Res U-Net’, to achieve better segmentation results of deep and superficial muscles in ultrasonic muscle images. This model is based on U-Net, and its structure has been modified to improve the performance of the algorithm. There are three aspects of improvement based on U-Net, including E-Res layer, dilated convolution module, and E-Res path. Additional experiments demonstrate that each designed module in our proposed network is effective, can improve the accuracy compared to the original U-Net. When compared with other algorithms which are state-of-the-art, the experimental result under the overall network structure is even more excellent.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010198",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Elastic net regularization",
      "Feature selection",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Net (polyhedron)",
      "Path (computing)",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Junsheng"
      },
      {
        "surname": "Lu",
        "given_name": "Yiwen"
      },
      {
        "surname": "Tao",
        "given_name": "Siyi"
      },
      {
        "surname": "Cheng",
        "given_name": "Xuan"
      },
      {
        "surname": "Huang",
        "given_name": "Chenxi"
      }
    ]
  },
  {
    "title": "Integrated framework of process mining and simulation–optimization for pod structured clinical layout design",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115696",
    "abstract": "This paper proposes a three-phase framework to leverage hospital tracking data of patient visits while designing healthcare layouts with pod structures. The first phase proposes a process mining algorithm that modifies the Probabilistic Determining Finite Automata (PDFA) with Particle Swarm Optimization (PDFA-PSO) algorithm to predict the significant patient workflows from hospital historical data. The second phase employs simulation modeling to solve a right-sizing problem to determine the optimal size of the layout pods and the frequency of flows between the different clinical locations. The final phase uses an Unequal Area Facility Layout Problem (UAFLP) to determine the layout typology. The proposed process mining and simulation model are vital steps to measure the frequency between spaces and pod areas, which are needed to solve the UAFLP for outpatient settings. The proposed framework is validated using a case study for a renovation project of a large heart and vascular clinic in the US. The research shows that process mining is an efficient tool to extract a subset of significant patient pathways among 90 pathway variants and build a more realistic simulation that reflects behavioral and operational aspects. The research shows that the PSO algorithm is efficient in estimating the PDFA parameters and improving the prediction accuracy of the extracted patient pathways. In addition, the research shows that Genetic Algorithm with Placement Staretegy is an efficient algorithm for layout automation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010800",
    "keywords": [
      "Computer science",
      "Data mining",
      "Database",
      "Genetic algorithm",
      "Leverage (statistics)",
      "Machine learning",
      "Operating system",
      "Particle swarm optimization",
      "Process (computing)",
      "Workflow"
    ],
    "authors": [
      {
        "surname": "Halawa",
        "given_name": "Farouq"
      },
      {
        "surname": "Chalil Madathil",
        "given_name": "Sreenath"
      },
      {
        "surname": "Khasawneh",
        "given_name": "Mohammad T."
      }
    ]
  },
  {
    "title": "An attention-driven convolutional neural network-based multi-level spectral–spatial feature learning for hyperspectral image classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115663",
    "abstract": "Recently, convolutional neural networks (CNNs) are successfully applied to extract abstract features of hyperspectral image (HSI), and they obtained competitive performances in HSI classification. However, HSI has inhomogeneous pixels or inherent spectral correlation, and the classification performance of CNN on HSI data will be degraded by modeling all information with equal importance. To address the above issues, we propose an attention mechanism-based method termed multi-level feature network with spectral–spatial attention model (MFNSAM), which consists of a multi-level feature CNN (MFCNN) and a spectral–spatial attention module (SSAM). Due to rich spectral information and spatial distribution in HSI data, MFCNN is employed as multi-scale fusion architecture to bridge the gaps between multi-level features. Specifically, the MFCNN extracts diverse information by compounding the representations generated by each tunnel of multi-scale filter group. To improve the representational capacity in spatial and spectral domains, the channel-wise attention branch is exploited to suppress redundant spectral information, and the spatial-wise attention is designed to explore the contextual information for better refinement. Thus, the SSAM is formed by merging the two branches to adaptively recalibrate the nonlinear interdependence of deep spectral–spatial features. Experiments on University of Pavia, Heihe, and Kennedy Space Center hyperspectral data sets demonstrate that the proposed model provide competitive results to state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010538",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Filter (signal processing)",
      "Geography",
      "Hyperspectral imaging",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Remote sensing",
      "Spatial analysis"
    ],
    "authors": [
      {
        "surname": "Pu",
        "given_name": "Chunyu"
      },
      {
        "surname": "Huang",
        "given_name": "Hong"
      },
      {
        "surname": "Yang",
        "given_name": "Liping"
      }
    ]
  },
  {
    "title": "CBCapsNet: A novel writer-independent offline signature verification model using a CNN-based architecture and capsule neural networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115649",
    "abstract": "Offline Signature verification is a biometric method with important applications in financial, legal and administrative procedures. The verification process includes comparing the extracted features of a questioned signature with those of genuine signatures of a certain individual. There are many challenges in designing offline signature verification as dynamic temporal features of signatures are not available. Deep Convolutional Neural Networks (DCNNs) have the great capability of extracting features from signature images. Despite the important advantages of these networks, they are unable to recognize the spatial properties of each feature in a signature. In addition, max-pooling layers usually eliminate some features that are crucial for forgery detection. In this paper, we propose a novel signature verification model with a combination of a CNN and Capsule Neural Networks (CapsNet) in order to capture spatial properties of signature features, improve the feature extraction phase, and reduce the complexity of the network. Moreover, we designed a new training mechanism in which a single network is trained simultaneously by two images at the same level so that the training parameters are reduced by half. Such mechanism does not require two separate networks for learning the features. Finally, a composite backbone architecture is presented with the hybrid of the proposed CNN-CapsNet models which we name CBCapsNet. The evaluation results demonstrate that our proposed model can improve accuracy and outperform prevalent signature verification methods in the community.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742101040X",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Deep learning",
      "Feature (linguistics)",
      "Feature extraction",
      "Geometry",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pooling",
      "Process (computing)",
      "Signature (topology)"
    ],
    "authors": [
      {
        "surname": "Parcham",
        "given_name": "Ebrahim"
      },
      {
        "surname": "Ilbeygi",
        "given_name": "Mahdi"
      },
      {
        "surname": "Amini",
        "given_name": "Mohammad"
      }
    ]
  },
  {
    "title": "Forecasting influenza epidemics in Hong Kong using Google search queries data: A new integrated approach",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115604",
    "abstract": "Forecasting influenza epidemics has important practical implications. However, the performance of traditional methods adopting in Hong Kong influenza forecasting is limited due to its particularity. This paper proposes an integrated approach for Hong Kong influenza epidemics forecasting. The novelties of our approach mainly include: firstly, we adopt a model for Google search queries data collection and selection in Hong Kong to substitute Google Correlate. Secondly, we adopt the stacked autoencoder (SAE) to reduce the dimensionality of Google search queries data. Thirdly, we adopt a signal decomposition method named variational mode decomposition (VMD) to decompose the influenza data into modes with different frequencies, which can extract the characteristic. Fourthly, we use artificial neural networks (ANN) to forecast these modes of influenza epidemics extracted by VMD respectively, then these forecasts of each mode are added to generate the final forecasting results. From the perspective of forecasting accuracy and hypothesis tests, the empirical results show that our proposed integrated approach SAE-VMD-ANN significantly outperforms some other benchmark models both in the whole period and influenza season. The performance of our proposed model during the COVID-19 pandemic is checked too.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010034",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Coronavirus disease 2019 (COVID-19)",
      "Curse of dimensionality",
      "Data mining",
      "Disease",
      "Filter (signal processing)",
      "Geodesy",
      "Geography",
      "Hilbert–Huang transform",
      "Infectious disease (medical specialty)",
      "Machine learning",
      "Medicine",
      "Mode (computer interface)",
      "Operating system",
      "Pathology"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yunhao"
      },
      {
        "surname": "Feng",
        "given_name": "Gengzhong"
      },
      {
        "surname": "Tsui",
        "given_name": "Kwok-Leung"
      },
      {
        "surname": "Sun",
        "given_name": "Shaolong"
      }
    ]
  },
  {
    "title": "Development and application of evaporation rate water cycle algorithm for optimal coordination of directional overcurrent relays",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115538",
    "abstract": "An optimization algorithm called modified evaporation rate water cycle algorithm (MERWCA) is proposed in this paper to find the optimal solutions for the coordination problem of directional overcurrent relays (DOCRs). The proposed MERWCA improves the performance of a conventional evaporation rate water cycle algorithm (ERWCA) by enhancing the balance between exploitation (search locally) and exploration (search globally) phases to find the best optimum solution. MERWCA includes the Opposition Based Learning (OBL) and Levy Flight (LF) component to address the shortcomings that the original ERWCA may exhibit, to avoid falling on the local optimal and improve the convergence rate. The proposed MERWCA is verified on the CEC’2017 test suite and its performance are compared with those of ten common metaheuristic algorithms (MAs). Both MERWCA and ERWCA are evaluated in the case of non-conventional and conventional relay curves. The feasibility of MERWCA technique is assessed using the conventional IEEE 39-bus meshed distribution test system. The results prove the viability of the MERWCA in solving DOCRs coordination problems in both cases (conventional and non-conventional characteristic relay curve) without any miscoordination between DOCRs pair. Moreover, the reduction ratio in minimizing the total operating using MERWCA reaches 46% with respect to the ERWCA and about 66% using the non-conventional relay characteristic. Finally, the MERWCA is assessed using benchmark DIgSILENT PowerFactory.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009453",
    "keywords": [
      "Algorithm",
      "Benchmark (surveying)",
      "Computer science",
      "Computer security",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Electrical engineering",
      "Engineering",
      "Geodesy",
      "Geography",
      "Key (lock)",
      "Local search (optimization)",
      "Mathematical optimization",
      "Mathematics",
      "Overcurrent",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Rate of convergence",
      "Relay",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Korashy",
        "given_name": "Ahmed"
      },
      {
        "surname": "Kamel",
        "given_name": "Salah"
      },
      {
        "surname": "Houssein",
        "given_name": "Essam H."
      },
      {
        "surname": "Jurado",
        "given_name": "Francisco"
      },
      {
        "surname": "Hashim",
        "given_name": "Fatma A."
      }
    ]
  },
  {
    "title": "Particle distance rank feature selection by particle swarm optimization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115620",
    "abstract": "This paper presents a feature selection method in multi-objective particle swarm optimization space. For this task, a novel particle ranking is proposed based on particle distance from dominated and non-dominated particles and then used for feature rank computation. Position and velocity of particles are updated by a new update rule relies in feature ranks encoded in a vector. Properties of the proposed method are proven mathematically and supported in experiments. The proposed feature selection method is evaluated on 12 UCI datasets and 4 datasets from real-world applications compared with 5 state-of-the-art feature selection methods. As a visual comparison, the proposed method finds better non-dominated particles in two-dimensional optimization space with lower run time. Experiments also showed that the proposed method outperforms existing feature selection methods with regard to Success Counting Measure, C_Metric, Hyper-Volume Indicator and Statistical Analysis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010149",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Economics",
      "Feature (linguistics)",
      "Feature selection",
      "Feature vector",
      "Finance",
      "Linguistics",
      "Mathematics",
      "Measure (data warehouse)",
      "Metric (unit)",
      "Multi-swarm optimization",
      "Operations management",
      "Particle swarm optimization",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Position (finance)",
      "Rank (graph theory)",
      "Ranking (information retrieval)",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Shafipour",
        "given_name": "Milad"
      },
      {
        "surname": "Rashno",
        "given_name": "Abdolreza"
      },
      {
        "surname": "Fadaei",
        "given_name": "Sadegh"
      }
    ]
  },
  {
    "title": "New closed-loop approximate dynamic programming for solving stochastic decentralized multi-project scheduling problem with resource transfers",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115593",
    "abstract": "Resource transfers and uncertain activity durations are two practical factors that are difficult to cope with in real project management. This paper studies a new decentralized multi-project scheduling problem with both resource transfers and uncertain activity durations, we name it the stochastic decentralized multi-project scheduling problem with resource transfers (SDRCMPSPTT). To tackle the curse-of-dimensionality of an exact solution approach, we develop a new and effective rollout policy-based approximate dynamic programming (ADP) algorithm for SDRCMPSPTT. Based on the benchmark instances, we build a new data set and examine the performance of 12 priority rule heuristics, then select the two best ones as the base policy for our rollout algorithm. The proposed approach is verified under a stochastic environment by assessing different base heuristic policies, computational results show that the rollout algorithm can further improve the solution quality of the base heuristics. Compared with the state-of-the-art algorithm for solving the decentralized multi-project scheduling problem under a deterministic environment, our rollout policy-based ADP algorithm can also obtain competitive solutions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009933",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Dynamic priority scheduling",
      "Dynamic programming",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Schedule",
      "Scheduling (production processes)",
      "Stochastic programming"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Song"
      },
      {
        "surname": "Xu",
        "given_name": "Zhe"
      }
    ]
  },
  {
    "title": "A level set method based on additive bias correction for image segmentation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115633",
    "abstract": "Intensity inhomogeneity brings great difficulties to image segmentation. This problem is partly solved by the multiplicative bias field correction model. However, some other problems still exist, such as slow segmentation speed and narrow application field. In this paper, an additive bias correction (ABC) model based on intensity inhomogeneity is proposed. The model divides the observed image into three parts: additive bias function, reflection edge structure function and Gaussian noise. Firstly, the local area and local clustering criterion of intensity inhomogeneity are defined. Secondly, by introducing the level set function, the local clustering criterion is transformed into an energy function based on the level set model. Finally, the structure of the estimated bias field and the reflection edge is computed through the process of minimizing the energy function while the image is segmented. In order to improve the stability of the system, a de-parameterized regularization function and an adaptive data-driven term function are designed. Compared with the traditional multiplicative model, the addition model has faster calculation speed. The proposed model can obtain ideal segmentation effect for images with intensity inhomogeneity. Experiment results show that the proposed method is more robust, faster and more accurate than traditional piecewise and multiplicative models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010277",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Image segmentation",
      "Level set (data structures)",
      "Mathematical analysis",
      "Mathematics",
      "Multiplicative function",
      "Pattern recognition (psychology)",
      "Piecewise",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Weng",
        "given_name": "Guirong"
      },
      {
        "surname": "Dong",
        "given_name": "Bin"
      },
      {
        "surname": "Lei",
        "given_name": "Yu"
      }
    ]
  },
  {
    "title": "Seq2Img-DRNET: A travel time index prediction algorithm for complex road network at regional level",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115554",
    "abstract": "Nowadays, traffic situation prediction is a major concern. Travel time index prediction is one of the representative tasks in traffic situation prediction. Most research of the current prediction focuses on road-level speed, but the travel time index prediction of complex road network at the regional level has not been well solved. In this paper, we proposed a novel model with convolutional neural network called Seq2Img-DRNET. The sequence data with 10 min interval are converted into image data as the input of our model. The model is constructed by fusing dense connection network and residual network to solve the travel time index prediction of complex road network at the regional level. Taking two actual traffic networks of commercial district and scenic district in Chengdu as the example, we compared our model with common sequential model and convolution neural network model, and the results show that our model can effectively capture the spatial–temporal relationship characteristics of complex road network at the regional level and make accurate prediction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100960X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Data mining",
      "Engineering",
      "Index (typography)",
      "Residual",
      "Time sequence",
      "Transport engineering",
      "Travel time",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Xiujuan"
      },
      {
        "surname": "Sun",
        "given_name": "Yuzhi"
      },
      {
        "surname": "Bai",
        "given_name": "Yulin"
      },
      {
        "surname": "Zhang",
        "given_name": "Kai"
      },
      {
        "surname": "Liu",
        "given_name": "Yu"
      },
      {
        "surname": "Zhao",
        "given_name": "Xiaowei"
      }
    ]
  },
  {
    "title": "Cross-subject EEG emotion classification based on few-label adversarial domain adaption",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115581",
    "abstract": "Emotion classification signal based on the electroencephalogram (EEG) is an important part of big data associated with health. One of the main challenges in this regard is the varying patterns of EEG indifferent subjects. Domain adaptation is an effective method to reduce the data difference between the source domain and the target domain. However, it is an enormous challenge to make a discriminator-based domain adaptation with a small target data and transform the target domain to the source domain. In the present study, a novel method called “few-label adversarial domain adaption” (FLADA) is proposed for cross-subject emotion classification tasks with small EEG data. The proposed method involves three steps: (a) Selecting subjects of the close source domain forming an adapted list. Few labeled target data are tested based on each emotion model of the source subject to get the subject list of the source domain. (b)Training three models based on each selected subject and the target subject. Three loss functions and six groups’ dataset are designed to get a domain adaption model for each selected source subject. (c) Distilling all classifiers for classifying the target emotion. In general, the main purpose of the proposed method, which originates from the Meta-learning, is to find a feature representation that is broadly suitable for the target subject and source subject with limited labels. The proposed method can be applied to all deep learning oriented models. In order to evaluate the performance of the proposed method, extensive experiments are carried out on SEED and DEAP datasets, which are public datasets. It is found that with a small amount of target data, the proposed FLADA model outperforms the state-of-art methods in terms of accuracy and AUC-ROC. All codes generated in this article are available at github: https://github.com/heibaipei/FLADA.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009830",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Detector",
      "Discriminator",
      "Domain (mathematical analysis)",
      "Electroencephalography",
      "Feature (linguistics)",
      "Library science",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Psychiatry",
      "Psychology",
      "Speech recognition",
      "Subject (documents)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yingdong"
      },
      {
        "surname": "Liu",
        "given_name": "Jiatong"
      },
      {
        "surname": "Ruan",
        "given_name": "Qunsheng"
      },
      {
        "surname": "Wang",
        "given_name": "Shuocheng"
      },
      {
        "surname": "Wang",
        "given_name": "Chen"
      }
    ]
  },
  {
    "title": "Smart training: Mask R-CNN oriented approach",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115595",
    "abstract": "This paper is aimed at the usage of an augmented reality assisted system set up on the smart-glasses for training activities. Literature review leads us to a comparison among related technologies, yielding that Mask Regions with Convolutional Neural Network (R-CNN) oriented approach fits the study needs. The proposed method including (1) pointing gesture capture, (2) finger-pointing analysis, and (3) virtual tool positioning and rotation angle are developed. Results show that the recognition of object detection is 95.5%, the Kappa value of recognition of gesture detection is 0.93, and the average time for detecting pointing gesture is 0.26 seconds. Furthermore, even under different lighting, such as indoor and outdoor, the pointing analysis accuracy is up to 79%. The error between the analysis angle and the actual angle is only 1.32 degrees. The results proved that the system is well suited to present the effect of augmented reality, making it applicable for real world usage.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009957",
    "keywords": [
      "Artificial intelligence",
      "Augmented reality",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Gesture",
      "Gesture recognition",
      "Pattern recognition (psychology)",
      "Programming language",
      "Rotation (mathematics)",
      "Set (abstract data type)",
      "Virtual reality"
    ],
    "authors": [
      {
        "surname": "Su",
        "given_name": "Mu-Chun"
      },
      {
        "surname": "Chen",
        "given_name": "Jieh-Haur"
      },
      {
        "surname": "Trisandini Azzizi",
        "given_name": "Vidya"
      },
      {
        "surname": "Chang",
        "given_name": "Hsiang-Ling"
      },
      {
        "surname": "Wei",
        "given_name": "Hsi-Hsien"
      }
    ]
  },
  {
    "title": "Dependence modeling of multivariate longitudinal hybrid insurance data with dropout",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115552",
    "abstract": "Financial services industries, such as insurance, increasingly use data from their broad cross-section of customers and follow these customers over time. In other areas such as medicine, engineering, and communication systems, it is well known that following subjects over time may result in biased data, for example, the so-called ”dropout effect”. This paper introduces techniques to address dropout commonly encountered in the insurance domain. Specifically, in the insurance context, multivariate claims outcomes may be related to a customer’s dropout or decision to lapse a policy. Insurance claims outcomes are also naturally a hybrid with both discrete and continuous components, which adds complexity to model calibration. Decision makers in the insurance industry will find our work provides helpful guidance in integrating customer loyalty, especially with bundled coverages. This paper introduces a generalized method of moments technique to estimate dependence parameters where associations are represented using copulas. This is especially useful for large data sets. The paper describes how the joint model provides new information that insurers can use to better manage their portfolios of risks. An application to a Spanish insurer data set is presented.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009581",
    "keywords": [
      "Actuarial science",
      "Biology",
      "Business",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Dropout (neural networks)",
      "Econometrics",
      "Economics",
      "Finance",
      "Financial services",
      "Loyalty",
      "Machine learning",
      "Marketing",
      "Multivariate statistics",
      "Paleontology"
    ],
    "authors": [
      {
        "surname": "Frees",
        "given_name": "Edward W."
      },
      {
        "surname": "Bolancé",
        "given_name": "Catalina"
      },
      {
        "surname": "Guillen",
        "given_name": "Montserrat"
      },
      {
        "surname": "Valdez",
        "given_name": "Emiliano A."
      }
    ]
  },
  {
    "title": "An efficient multi-objective evolutionary approach for solving the operation of multi-reservoir system scheduling in hydro-power plants",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115638",
    "abstract": "This paper tackles the short-term hydro-power unit commitment problem in a multi-reservoir system — a cascade-based operation scenario. For this, we propose a new mathematical modeling in which the goal is to maximize the total energy production of the hydro-power plant in a sub-daily operation, and, simultaneously, to maximize the total water content (volume) of reservoirs. For solving the problem, we discuss the Multi-objective Evolutionary Swarm Hybridization (MESH) algorithm, a recently proposed multi-objective swarm intelligence-based optimization method which has obtained very competitive results when compared to existing evolutionary algorithms in specific applications. The MESH approach has been applied to find the optimal water discharge and the power produced at the maximum reservoir volume for all possible combinations of turbines in a hydro-power plant. The performance of MESH has been compared with that of well-known evolutionary approaches such as NSGA-II, NSGA-III, SPEA2, and MOEA/D in a realistic problem considering data from a hydro-power energy system with two cascaded hydro-power plants in Brazil. Results indicate that MESH showed a superior performance than alternative multi-objective approaches in terms of efficiency and accuracy, providing a profit of $412,500 per month in a projection analysis carried out.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010320",
    "keywords": [
      "Computer science",
      "Electric power system",
      "Evolutionary algorithm",
      "Mathematical optimization",
      "Mathematics",
      "Physics",
      "Power (physics)",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Marcelino",
        "given_name": "C.G."
      },
      {
        "surname": "Leite",
        "given_name": "G.M.C."
      },
      {
        "surname": "Delgado",
        "given_name": "C.A.D.M."
      },
      {
        "surname": "de Oliveira",
        "given_name": "L.B."
      },
      {
        "surname": "Wanner",
        "given_name": "E.F."
      },
      {
        "surname": "Jiménez-Fernández",
        "given_name": "S."
      },
      {
        "surname": "Salcedo-Sanz",
        "given_name": "S."
      }
    ]
  },
  {
    "title": "ISSP-tree: An improved fast algorithm for constructing a complete prefix tree using single database scan",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115603",
    "abstract": "The researchers have explored the frequent pattern mining problem by considering the fact that the complete set of information to be processed can be accommodated in systems main memory, and databases are static. However, any transactional or online database may get modified in real-life scenarios due to new transactions or deleting previous obsolete records. Moreover, the support threshold may get updated over time to generate a new set of frequent patterns from the updated database. An inefficient but straightforward method to deal with this problem is recomputing the fresh set of patterns for the updated database or updated support threshold. Most of the existing algorithms perform pattern mining using multiple database scans, which requires a massive amount of main memory and computational time to retain tedious candidate itemsets and prune out the unnecessary itemsets. The research community has developed a few methods to handle the incremental scenario without re-computation from scratch, and those methods are efficient in terms of database scan point of view. Although the approaches have solved the re-computation problem by constructing a complete pattern-tree data structure using only one database scan, they have significant issues such as massive disk I/O and colossal search space high tree construction time. Therefore, to improve the tree construction time, we propose an efficient tree data structure called ISSP-tree (Improved Single Scan Pattern Tree), which creates a complete tree to retain all the database transactions irrespective of the item frequencies using only one database scan. Moreover, the method is also adaptive to incremental and interactive mining.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010022",
    "keywords": [
      "Algorithm",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Data structure",
      "Database",
      "Linguistics",
      "Mathematics",
      "Philosophy",
      "Prefix",
      "Programming language",
      "Search algorithm",
      "Search tree",
      "Tree (set theory)",
      "Trie"
    ],
    "authors": [
      {
        "surname": "Ahmed",
        "given_name": "Shafiul Alom"
      },
      {
        "surname": "Nath",
        "given_name": "Bhabesh"
      }
    ]
  },
  {
    "title": "Grey models for short-term queue length predictions for adaptive traffic signal control",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115618",
    "abstract": "Traffic congestion at a signalized intersection greatly reduces the travel time reliability in urban areas. Adaptive signal control system (ASCS) is the most advanced traffic signal technology that regulates the signal phasing and timings considering the traffic patterns in real-time to reduce traffic congestion. Real-time prediction of traffic queue length can be used to adjust the signal phasing and timings for different traffic movements at a signalized intersection with ASCS. The accuracy of the queue length prediction model varies based on many factors, such as the stochastic nature of the vehicle arrival rates at an intersection, time of the day, weather, and driver characteristics. In addition, accurate queue length prediction for multilane, undersaturated and saturated traffic scenarios at signalized intersections is challenging. Thus, the objective of this study is to develop short-term queue length prediction models for signalized intersections that can be leveraged by adaptive traffic signal control systems using six variations of Grey systems: (i) the first-order single variable Grey model (GM(1,1)); (ii) GM(1,1) with Fourier error corrections (EGM); (iii) the Grey Verhulst model (GVM), (iv) GVM with Fourier error corrections (EGVM), (v) the Grey model with cosine term (GM(1,1 | c o s ( ω t ) or GMC), and (vi) GMC with Fourier error corrections (EGMC). The efficacy of the Grey models is that they facilitate fast processing; as these models do not require a large amount of data; as would be needed in artificial intelligence models; and they can adapt to stochastic changes, unlike statistical models. We have conducted a case study using queue length data from five intersections with adaptive traffic signal control on a calibrated roadway network in Lexington, South Carolina. Grey models were compared with linear, nonlinear time series models, Adaboost (ADA), Bagging (BAG), Gradient Boost (GB), Neural Network (NN), Random Forest (RF), Support Vector Regression (SVR), and Long Short Term Memory (LSTM) models. Based on our analyses, we found that GMC can produce better or competing performance compared to more complex (i.e., LSTM and NN models) in predicting average and maximum queue lengths in terms of root mean squared and mean absolute errors. GMC model needs only 4 data values and can be used in real-time as the computational time to generate a prediction in less than 0.001 s.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010125",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer network",
      "Computer science",
      "Electrical engineering",
      "Engineering",
      "Intersection (aeronautics)",
      "Phaser",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Queue",
      "Real-time computing",
      "SIGNAL (programming language)",
      "Signal timing",
      "Term (time)",
      "Traffic signal",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Comert",
        "given_name": "Gurcan"
      },
      {
        "surname": "Khan",
        "given_name": "Zadid"
      },
      {
        "surname": "Rahman",
        "given_name": "Mizanur"
      },
      {
        "surname": "Chowdhury",
        "given_name": "Mashrur"
      }
    ]
  },
  {
    "title": "Bias and variance residuals for machine learning nonlinear simplex regressions",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115656",
    "abstract": "We propose two new residuals that can be used to evaluate the bias and variance of nonlinear simplex regressions for machine learning. Such models are supervised learning tools for problems with high complexity, since they involve nonlinearity, and are used with univariate responses that assume values in the standard unit interval. The residuals we introduce are obtained from Fisher’s iterative scoring algorithm used for estimating the mean and dispersion regression coefficients. A novel feature of the proposed residuals is that they do not require the computation of projection matrices. As is well known, the computation of such matrices can be computationally costly when the sample size is large. We present and discuss three empirical applications, two that use real data and one that is based on a simulated dataset. The results from all three empirical analyses favor the proposed residuals.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010472",
    "keywords": [
      "Accounting",
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Econometrics",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Nonlinear regression",
      "Nonlinear system",
      "Physics",
      "Quantum mechanics",
      "Regression analysis",
      "Simplex",
      "Statistics",
      "Variance (accounting)"
    ],
    "authors": [
      {
        "surname": "Espinheira",
        "given_name": "Patrícia L."
      },
      {
        "surname": "Silva",
        "given_name": "Luana C.M."
      },
      {
        "surname": "Cribari-Neto",
        "given_name": "Francisco"
      }
    ]
  },
  {
    "title": "Predicting combat outcomes and optimizing armies in StarCraft II by deep learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115592",
    "abstract": "Real-time strategy (RTS) games’ nature that, more complex than the turn-based, tabletop games such as Go, has been spotlighted in the field of artificial intelligence (AI) due to its similarity with real-world problems. In StarCraft II, agents cannot make decisions and control until they evaluate and compare the expected outcome of a choice. Among the ways to evaluate outcomes, combat models are one of the active areas of research to this problem, which is a basis for decision-making. The battlefield of combat needs to be considered in combat models because they have enough influence to overturn the outcome of the battle. However, its effect has not been sufficiently examined. We introduce a combat winner predictor that utilizes battlefield and troop information. Furthermore, we propose a constrained optimization framework with gradient updates to optimize unit-combinations based on the combat winner predictor. Experiments demonstrate the robustness and rapidness of the proposed methods in large-scale combat datasets on various battlefields of StarCraft II. The proposed framework achieved better accuracy in prediction and retrieved winning unit-combinations faster. Incorporating these frameworks into AI agents can improve the AI’s decision-making power.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009921",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Machine learning"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Donghyeon"
      },
      {
        "surname": "Kim",
        "given_name": "Man-Je"
      },
      {
        "surname": "Ahn",
        "given_name": "Chang Wook"
      }
    ]
  },
  {
    "title": "Deep learning based bi-level approach for proactive loan prospecting",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115607",
    "abstract": "A fundamental component to managing a marketing campaign is identifying prospects and selection of leads. Current lead generation models focus on predicting the intention of a customer to purchase a product, however with financial products, particularly loans, this can be insufficient as there are many factors to consider, such as risk, utility, and financial maturity. Developing a marketing campaign for loan prospecting should consider not only customers who need a loan, but rather clients who need a loan and will also be approved. Otherwise, the marketing effort is deemed ineffective, if the lead cannot be converted into a sale. Although, a low response rate is expected for a marketing campaign for loans, we highlight a better approach in managing resources while maintaining a shortlist of high-quality leads. This manuscript introduces a bi-level approach to handle the complex nature of loan products. Two classifiers are built, one modelling loan intention and the other one modelling loan eligibility. We adopt convex combination to control weights of both problems, which in most cases results in an improved performance for identifying future successful loan applicants when compared to baseline models. Rank-based evaluation measures are also adopted to explore the performance of customer rankings. We find that soft classifiers, such as deep learning techniques, are ideal for ranking customers, achieving a superior performance when compared to other machine learning techniques. In addition, we conclude that an ideal cut-off for K customers is estimated to be between 20 to 25 customers, however our best model can maintain an Average Precision of greater than 0.85 when K approaches 50.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010058",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Finance",
      "Geology",
      "Loan",
      "Machine learning",
      "Mining engineering",
      "Prospecting"
    ],
    "authors": [
      {
        "surname": "Munoz",
        "given_name": "Justin"
      },
      {
        "surname": "Rezaei",
        "given_name": "Ahmad Asgharian"
      },
      {
        "surname": "Jalili",
        "given_name": "Mahdi"
      },
      {
        "surname": "Tafakori",
        "given_name": "Laleh"
      }
    ]
  },
  {
    "title": "A diversified group teaching optimization algorithm with segment-based fitness strategy for unmanned aerial vehicle route planning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115690",
    "abstract": "The complexity and diversity of the flight environment pose great challenges to unmanned aerial vehicle route planning, which demands feasible flight strategies and efficient route planning algorithms. To address the issue, this paper constructs a 3-D flight environment model with multiple obstacles, and designs a novel diversified group teaching optimization algorithm for the generation of flight routes of unmanned aerial vehicles. In the environment model, a variety of obstacles are taken into consideration to make the flying scenarios more realistic, including mountain, cuboid, cylinder and triangular prism, and corresponding strategies are presented for unmanned aerial vehicles to safely avoid these obstacles. In the proposed algorithm, three novel teaching methods are introduced to balance the exploitation and exploration phases. Besides, a novel constrained optimization strategy is adopted, in which constraints are incrementally added to the fitness function to avoid the premature phenomenon in the initial iteration stage of algorithm. The experimental results show that compared with several state-of-the-art optimization algorithms, the proposed algorithm is significantly superior and can always generate the optimal flight route in complicated environments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010757",
    "keywords": [
      "Aerospace engineering",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Engineering",
      "Fitness function",
      "Flight planning",
      "Genetic algorithm",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Motion planning",
      "Optimization algorithm",
      "Robot",
      "Route planning"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Yuxin"
      },
      {
        "surname": "Wu",
        "given_name": "Qing"
      },
      {
        "surname": "Zhang",
        "given_name": "Guozhong"
      },
      {
        "surname": "Zhu",
        "given_name": "Shenke"
      },
      {
        "surname": "Xing",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Bankruptcy prediction on the base of the unbalanced data using multi-objective selection of classifiers",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115559",
    "abstract": "The goal of the paper is to develop a new algorithm for predicting whether the company will go bankrupt on the base of unbalanced data. To do it, we propose to consider the classification as a multi-objective optimization problem and construct a prediction model as an ensemble while minimizing the parameters FPR (False Positive Rate) and FNR (False Negative Rate) at the same time. To create the ensemble, the proposed algorithm of a Multi-Objective Classifier Selection (MOCS) selects only classifiers that belong to the Pareto-optimal set in FPR/FNR space; that is, there is no dominance between them, and they satisfy some additional conditions. In the general case, MOCS is determined by three parameters: two threshold values that limit false rates (FNR and FPR), and the crowding distance, which defines the uniqueness of the classifier's results. We tested the proposed algorithm on data collected from 2457 Russian companies, 456 of which went bankrupt, and 5910 Polish companies, 410 of which received bankruptcy status. Datasets contain features such as financial ratios and business environment factors. In the testing, we used more than 70 combinations of under-sampling, over-sampling, and no sampling methods with static and dynamic classification models. Final ensembles include seven classifiers for the Russian dataset and four classifiers for the Polish dataset combined by soft voting rule. In both cases, the proposed algorithm produces a significant improvement of prediction results as in terms of standard metrics (geometric mean, the area under the ROC curve) and in the visual representation in the FNR/FPR space, namely in the shift from a Pareto-optimal set of classifiers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009659",
    "keywords": [
      "Artificial intelligence",
      "Bankruptcy",
      "Bankruptcy prediction",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Economics",
      "Finance",
      "Machine learning",
      "Majority rule",
      "Mathematics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Zelenkov",
        "given_name": "Yuri"
      },
      {
        "surname": "Volodarskiy",
        "given_name": "Nikita"
      }
    ]
  },
  {
    "title": "SDFNet: Automatic segmentation of kidney ultrasound images using multi-scale low-level structural feature",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115619",
    "abstract": "Due to speckle noise, changes in kidney shape and size between patients, and similar regions, segmenting kidneys in ultrasound images is challenging. To alleviate this challenge, we proposed a novel CNN model, namely multi-scale fusion network of structural features and detailed features (SDFNet), to segment kidneys accurately and robustly. Specifically, the architecture includes structure feature extraction network (S-Net), detail information extraction network (D-Net) and multi-scale fusion block (MCBlock), which are in charge of extracting structural features, capturing texture details and merging features, respectively. In S-Net, we designed a boundary detection (BD) module to obtain more complete kidney structural features. In addition, this paper also designed a step-by-step training mechanism to improve the generalization ability of the SDFNet. We validated the proposed method and compared the same kidney ultrasound dataset with several state-of-the-art methods using six quantitative indicators. The results demonstrate that the proposed approach achieves the best overall performance and outperforms all other approaches on kidney ultrasound image segmentation. It is worth noting that this paper quantitatively analyzes the loss function of segment kidney ultrasound images. This work is a good reference for future research.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010137",
    "keywords": [
      "3D ultrasound",
      "Acoustics",
      "Artificial intelligence",
      "Artificial neural network",
      "Block (permutation group theory)",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Feature extraction",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Segmentation",
      "Speckle noise",
      "Ultrasound"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Gongping"
      },
      {
        "surname": "Dai",
        "given_name": "Yu"
      },
      {
        "surname": "Li",
        "given_name": "Rui"
      },
      {
        "surname": "Zhao",
        "given_name": "Yu"
      },
      {
        "surname": "Cui",
        "given_name": "Liang"
      },
      {
        "surname": "Yin",
        "given_name": "Xiaotao"
      }
    ]
  },
  {
    "title": "A stochastic approximation approach to simultaneous feature weighting and selection for nearest neighbour learners",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115671",
    "abstract": "Nearest neighbour (NN) learners are some of the most popular methods in supervised machine learning with feature selection and weighting being two fundamental tools for improving their performance. In this study, we introduce a novel methodology for simultaneous feature selection and weighting for NN learners based on simultaneous perturbation stochastic approximation (SPSA). This is a pseudo-gradient descent optimisation algorithm that approximates gradient information from noisy objective function measurements without a need for an explicit functional form of the objective function nor its derivatives. In particular, we show how the process of simultaneous feature selection and weighting can be optimised within a stochastic approximation framework with repeated cross-validation (CV) performance as the objective function, which we call SPSA-FWS. We provide extensive computational experiments for assessment of this approach and we compare performance of SPSA-FWS to other feature weighting methods. Our results indicate that SPSA-FWS outperforms existing feature weighting algorithms for the most part and it stands as a competitive new method for this task. Specifically, when compared against its unweighted counterpart and feature weighting alone with 5-repeated 5-fold CV accuracy being the performance metric, SPSA-FWS provides improvements of up to 177.88% with a mean improvement of 20.52% for classification tasks and a mean improvement of 0.19 in the R-Squared metric for regression tasks respectively. In addition to its superior performance, SPSA-FWS has two attractive features: (1) it can be used in conjunction with any performance metric and any variant of nearest neighbour learners, and (2) it can be hybridised with other feature weighting methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010605",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Economics",
      "Feature (linguistics)",
      "Feature selection",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Medicine",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Radiology",
      "Simultaneous perturbation stochastic approximation",
      "Statistics",
      "Stochastic gradient descent",
      "Stochastic process",
      "Weighting",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Yeo",
        "given_name": "Guo Feng Anders"
      },
      {
        "surname": "Aksakalli",
        "given_name": "Vural"
      }
    ]
  },
  {
    "title": "Towards comprehensive profile aggregation methods for group recommendation based on the latent factor model",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115585",
    "abstract": "The aggregation of group members’ profiles is an extremely important step in group recommender systems, as it represents the whole group as a single virtual user, and is the input to traditional recommendation techniques. This paper focuses on proposing methods of aggregating profiles of group members. The highlight of the proposed methods lies in incorporating the latent factor matrices formed in the latent factor recommendation model into the profile aggregation for group recommendation. As a result, the process of the profile aggregation is ensured to more fully reflect the interests of the group members. Experimental results show that profile aggregation, enriched by latent factor matrices, can improve group recommendation performance in terms of F1-score and Normalized Discounted Cumulative Gain Measure.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009878",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Factor (programming language)",
      "Group (periodic table)",
      "Machine learning",
      "Organic chemistry",
      "Probabilistic latent semantic analysis",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Nam",
        "given_name": "Le Nguyen Hoai"
      }
    ]
  },
  {
    "title": "Gait monitoring system for patients with Parkinson’s disease",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115653",
    "abstract": "Background Wearable monitoring devices based on inertial sensors have the potential to be used as a quantitative method in clinical practice for continuous assessment of gait disabilities in Parkinson’s disease (PD). Methods This manuscript introduces a new gait monitoring system adapted to patients with PD, based on a wearable monitoring device. To eliminate inter- and intra-subject variability, the computational method was based on heuristic rules with adaptive thresholds and ranges and a motion compensation strategy. The experimental trials involved repeated measurements of walking trials from two cross-sectional studies: the first study was performed in order to validate the effectiveness of the system against a robust 3D motion analysis with 10 healthy subjects; and the second-one aimed to validate our approach against a well-studied wearable IMU-based system on a hospital environment with 20 patients with PD. Results The proposed system proved to be efficient (Experiment I: sensitivity = 95,09% and accuracy = 93,64%; Experiment II: sensitivity = 99,53% and accuracy = 97,42%), time-effective (Experiment I: earlies = 13,71 ms and delays = 12,91 ms; Experiment II: earlies = 12,94 ms and delays = 12,71 ms), user and user-motion adaptable and a low computational-load strategy for real-time gait events detection. Further, it was measured the percentage of absolute error classified with a good acceptability (Experiment I: 3,02 ≤ ε%≤12,94; Experiment II: 2,81 ≤ ε%≤13,45). Lastly, regarding the measured gait parameters, it was observed a reflection of the typical levels of motor impairment for the different disease stages. Conclusion The achieved outcomes enabled to verify that the proposed system can be suitable for gait analysis in the assistance and rehabilitation fields.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010447",
    "keywords": [
      "Accelerometer",
      "Artificial intelligence",
      "Compensation (psychology)",
      "Computer science",
      "Electronic engineering",
      "Embedded system",
      "Engineering",
      "Gait",
      "Inertial measurement unit",
      "Medicine",
      "Motion (physics)",
      "Motion capture",
      "Operating system",
      "Physical medicine and rehabilitation",
      "Psychoanalysis",
      "Psychology",
      "Sensitivity (control systems)",
      "Simulation",
      "Wearable computer"
    ],
    "authors": [
      {
        "surname": "Gonçalves",
        "given_name": "Helena R."
      },
      {
        "surname": "Rodrigues",
        "given_name": "Ana"
      },
      {
        "surname": "Santos",
        "given_name": "Cristina P."
      }
    ]
  },
  {
    "title": "Combined ensemble multi-class SVM and fuzzy NSGA-II for trend forecasting and trading in Forex markets",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115566",
    "abstract": "Foreign exchange (Forex) market is the biggest currency exchange market in the world. Existing trading systems in Forex markets based on technical analysis use crisp technical indicators to provide Buy/Sell signals to the trader, only when the indicator value crosses a given threshold level. This strict and noise-sensitive condition can be replaced through uncertainty handling of indicators using fuzzy numbers to generate Buy/Sell signals with fuzzy memberships functions. To achieve this purpose, this paper presents a combined technique based on ensemble multi-class support vector machine (EmcSVM) and fuzzy NSGA-II for efficient trend classification and trading in Forex markets. At first, EmcSVM is used to forecast and classify the future market trend into uptrend, sideway, and downtrend. Then, NSGA-II is applied to optimize the hyperparameters of the proposed fuzzy trading system comprising multiple AND-OR Buy/Sell technical rules for uptrend/downtrend markets. The hyperparameters include indicator selection within each rule, importance weights of the different rules, and final decision thresholds for Buy/Sell models, while the objective is to maximize average return on investment (ROI) and minimize average draw-down of all transactions. The proposed method has been successfully developed and tested on real data from the Forex market for EUR/USD currency pair in a 6-year timeframe from 2014 to 2019. Obtained results show that the proposed method outperforms the existing crisp trading systems, with 80.8% precision, 72.4% recall, 94.1% annual ROI, and 0.58% draw down.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009726",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Currency",
      "Data mining",
      "Econometrics",
      "Economics",
      "Financial economics",
      "Foreign exchange market",
      "Fuzzy logic",
      "Hyperparameter",
      "Machine learning",
      "Monetary economics",
      "Support vector machine",
      "Technical analysis",
      "Trading strategy"
    ],
    "authors": [
      {
        "surname": "Sadeghi",
        "given_name": "Alireza"
      },
      {
        "surname": "Daneshvar",
        "given_name": "Amir"
      },
      {
        "surname": "Madanchi Zaj",
        "given_name": "Mahdi"
      }
    ]
  },
  {
    "title": "Weighted Clusterwise Linear Regression based on adaptive quadratic form distance",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115609",
    "abstract": "The standard approach to Clusterwise Regression is the Clusterwise Linear Regression method. This approach can lead to data over-fitting, and it is not able to distinguish linear relationships in groups of observations well separated in the space of explanatory variables. This paper presents a Weighted Clusterwise Linear Regression method to obtain homogeneous clusters of observations while maintaining a proper fitting for the response variable, by the minimization of an optimization criterion that combines a k-means-like criterion (based on an adaptive quadratic form dissimilarity) in x-space and the criterion of minimum squared residuals of Regression Analysis. The adaptive metric allows automatic weighing or take into account the correlation between explanatory variables under multiple constraints types. We explore six constraints types. Experiments with synthetic and benchmark datasets corroborate the usefulness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742101006X",
    "keywords": [
      "Bayesian multivariate linear regression",
      "Benchmark (surveying)",
      "Computer science",
      "Data mining",
      "Economics",
      "Geodesy",
      "Geography",
      "Geometry",
      "Linear predictor function",
      "Linear regression",
      "Mathematical optimization",
      "Mathematics",
      "Metric (unit)",
      "Minification",
      "Operations management",
      "Proper linear model",
      "Quadratic equation",
      "Regression",
      "Regression analysis",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "da Silva",
        "given_name": "Ricardo A.M."
      },
      {
        "surname": "de Carvalho",
        "given_name": "Francisco de A.T."
      }
    ]
  },
  {
    "title": "A graph-based recommendation approach for highly interactive platforms",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115555",
    "abstract": "Highly interactive and dynamic marketplaces shake the underlying hypothesis of established recommendation approaches assuming static offerings and sparse interaction data. Such markets show different dynamics due to goods/contents volatility and public accessibility. This imposes new challenges to recommender systems since they are required to handle unbounded data streams with high velocity, volume and variability while operating at scale and in real-time. Moreover, due to the high rates of new offerings introduction and obsolescence, low latency modeling and inference are needed to keep an up-to-date understanding of the market. In this paper, we propose a recommendation approach addressing the specific challenges of real-time stream recommendation in highly interactive marketplaces. The approach is based on several psychological theories to model consumers’ perceived value towards items. Besides, a ranking measure is defined on a heterogeneous information network to infer the factors that drive consumers’ decisions. With data relationships at its center, this data model is strongly efficient while ensuring free and flexible knowledge evolution as data evolves. It allows low latency incremental learning at scale while providing dynamic recommendations. Several comparative experiments were conducted to validate the potential of this approach in different use cases requiring offline, online, static or dynamic recommendations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009611",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Data science",
      "Inference",
      "Machine learning",
      "Obsolescence",
      "Paleontology",
      "Recommender system"
    ],
    "authors": [
      {
        "surname": "Ficel",
        "given_name": "Hemza"
      },
      {
        "surname": "Haddad",
        "given_name": "Mohamed Ramzi"
      },
      {
        "surname": "Baazaoui Zghal",
        "given_name": "Hajer"
      }
    ]
  },
  {
    "title": "Embedding-based real-time change point detection with application to activity segmentation in smart home time series data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115641",
    "abstract": "Human activity recognition systems are essential to enable many assistive applications. Those systems can be sensor-based or vision-based. When sensor-based systems are deployed in real environments, they must segment sensor data streams on the fly in order to extract features and recognize the ongoing activities. This segmentation can be done with different approaches. One effective approach is to employ change point detection (CPD) algorithms to detect activity transitions (i.e. determine when activities start and end). In this paper, we present a novel real-time CPD method to perform activity segmentation, where neural embeddings (vectors of continuous numbers) are used to represent sensor events. Through empirical evaluation with 3 publicly available benchmark datasets, we conclude that our method is useful for segmenting sensor data, offering significant better performance than state of the art algorithms in two of them. Besides, we propose the use of retrofitting, a graph-based technique, to adjust the embeddings and introduce expert knowledge in the activity segmentation task, showing empirically that it can improve the performance of our method using three graphs generated from two sources of information. Finally, we discuss the advantages of our approach regarding computational cost, manual effort reduction (no need of hand-crafted features) and cross-environment possibilities (transfer learning) in comparison to others.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010344",
    "keywords": [
      "Activity recognition",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Business",
      "Change detection",
      "Computer science",
      "Data mining",
      "Embedding",
      "Geodesy",
      "Geography",
      "Graph",
      "Machine learning",
      "Market segmentation",
      "Marketing",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Bermejo",
        "given_name": "Unai"
      },
      {
        "surname": "Almeida",
        "given_name": "Aitor"
      },
      {
        "surname": "Bilbao-Jayo",
        "given_name": "Aritz"
      },
      {
        "surname": "Azkune",
        "given_name": "Gorka"
      }
    ]
  },
  {
    "title": "Wavelet and deep learning-based detection of SARS-nCoV from thoracic X-ray images for rapid and efficient testing",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115650",
    "abstract": "This paper proposes a wavelet and artificial intelligence-enabled rapid and efficient testing procedure for patients with Severe Acute Respiratory Coronavirus Syndrome (SARS-nCoV) through a deep learning approach from thoracic X-ray images. Presently, the virus infection is diagnosed primarily by a process called the real-time Reverse Transcriptase–Polymerase Chain Reaction (rRT-PCR) based on its genetic prints. This whole procedure takes a substantial amount of time to identify and diagnose the patients infected by the virus. The proposed research uses a wavelet-based convolution neural network architectures to detect SARS-nCoV. CNN is pre-trained on the ImageNet and trained end-to-end using thoracic X-ray images. To execute Discrete Wavelet Transforms (DWT), the available mother wavelet functions from different families, namely Haar, Daubechies, Symlet, Biorthogonal, Coiflet, and Discrete Meyer, were considered. Two-level decomposition via DWT is adopted to extract prominent features peripheral and subpleural ground-glass opacities, often in the lower lobes explicitly from thoracic X-ray images to suppress noise effect, further enhancing the signal to noise ratio. The proposed wavelet-based deep learning models of both, two-class instances (COVID vs. Normal) and four-class instances (COVID-19 vs. PNA bacterial vs. PNA viral vs. Normal) were validated from publicly available databases using k-Fold Cross Validation (k-Fold CV) technique. In addition to these X-ray images, images of recent COVID-19 patients were further used to examine the model’s practicality and real-time feasibility in combating the current pandemic situation. It was observed that the Symlet 7 approximation component with two-level manifested the highest test accuracy of 98.87%, followed by Biorthogonal 2.6 with an efficiency of 98.73%. While the test accuracy for Symlet 7 and Biorthogonal 2.6 is high, Haar and Daubechies with two levels have demonstrated excellent validation accuracy on unseen data. It was also observed that the precision, the recall rate, and the dice similarity coefficient for four-class instances were 98%, 98%, and 99%, respectively, using the proposed algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010411",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Coronavirus disease 2019 (COVID-19)",
      "Deep learning",
      "Discrete wavelet transform",
      "Disease",
      "Image (mathematics)",
      "Infectious disease (medical specialty)",
      "Medicine",
      "Noise (video)",
      "Pathology",
      "Pattern recognition (psychology)",
      "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)",
      "Thresholding",
      "Wavelet",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Verma",
        "given_name": "Amar Kumar"
      },
      {
        "surname": "Vamsi",
        "given_name": "Inturi"
      },
      {
        "surname": "Saurabh",
        "given_name": "Prerna"
      },
      {
        "surname": "Sudha",
        "given_name": "Radhika"
      },
      {
        "surname": "G.R.",
        "given_name": "Sabareesh"
      },
      {
        "surname": "S.",
        "given_name": "Rajkumar"
      }
    ]
  },
  {
    "title": "Data-driven decision support system for managing item allocation in an ASRS: A framework development and a case study",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115622",
    "abstract": "When dealing with Automated Storage and Retrieval Systems (ASRS), the allocation of items to the most convenient storage location depends on the vast amount of data produced internally (e.g., Enterprise Resource Planning, Manufacturing Enterprise Systems) and externally (e.g. Supply Chain Management). Moreover, a proper item allocation in the warehouse has a strong influence on the warehouse saturation levels and picking times. In this perspective, the present work proposes the application of data-driven algorithms for managing items in an Automated Storage and Retrieval System (ASRS) in order to reduce the picking times and storage space. Specifically, a four-layer framework is adopted for collecting data produced by different information sources and analyzing them through a data-driven approach. The analytics layer is performed by combining the Association Rule Mining (ARM) technique, to investigate the network of influences among data collected, and a simulation approach for assessing the feasibility of the proposed implementation. The Association Rule Mining allows company managers to identify the components that should be located on the same tray in the ASRS, defining the couples of items frequently picked together in order to reduce the total picking time. The proposed approach is applied to the case study of a shoe manufacturing company to explain the research approach and show how the implementation of the data-driven methodology can provide valuable support in defining item allocation and picking rules. The proposed Association Rule Mining method is new in this context and it has shown a positive impact in comparison to traditional solutions of warehouse management, providing a complete overview of the items’ interactions and identifying communities of items that define local and global patterns and locate influential entities.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010162",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Data mining",
      "Decision support system",
      "Knowledge management",
      "Machine learning",
      "Process management"
    ],
    "authors": [
      {
        "surname": "Antomarioni",
        "given_name": "Sara"
      },
      {
        "surname": "Lucantoni",
        "given_name": "Laura"
      },
      {
        "surname": "Ciarapica",
        "given_name": "Filippo Emanuele"
      },
      {
        "surname": "Bevilacqua",
        "given_name": "Maurizio"
      }
    ]
  },
  {
    "title": "CS-ResNet: Cost-sensitive residual convolutional neural network for PCB cosmetic defect detection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115673",
    "abstract": "In the printed circuit board (PCB) industry, cosmetic defect detection is an essential process to ensure product quality. However, existing PCB cosmetic defect detection approaches have a high false alarm rate, which lead to expensive labor costs of manual confirmation. To solve this problem, some traditional machine learning-based approaches have been proposed, but they just utilize hand-crafted features to build classifiers and thus are rough and sub-optimal. Recently, due to its powerful capability in automatic feature extraction, convolutional neural network (CNN) has been widely used in PCB cosmetic defect detection. However, few of them pay attention to the imbalanced class distribution as well as the different misclassification costs of real and pseudo defects, both of which are common problems in the PCB industry. To this end, in this study, we propose a novel model called cost-sensitive residual convolutional neural network (CS-ResNet) by adding a cost-sensitive adjustment layer in the standard ResNet. Specifically, we assign larger weights to minority real defects based on the class-imbalance degree and then optimize CS-ResNet by minimizing the weighted cross-entropy loss function. We conducted a series of experiments by comparing CS-ResNet with the standard ResNet, state-of-the-art CNN-based approach Auto-VRS and traditional machine learning-based approach HOG-SVM on a real-world PCB cosmetic defect dataset. Experimental results show that CS-ResNet achieves the highest S e n s i t i v i t y (0.89), G - m e a n (0.91) and the lowest misclassification costs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010617",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Cross entropy",
      "Deep learning",
      "Electronic engineering",
      "Engineering",
      "False alarm",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Residual",
      "Residual neural network",
      "Sensitivity (control systems)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Huan"
      },
      {
        "surname": "Jiang",
        "given_name": "Liangxiao"
      },
      {
        "surname": "Li",
        "given_name": "Chaoqun"
      }
    ]
  },
  {
    "title": "Survey of similarity functions on neighborhood-based collaborative filtering",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115482",
    "abstract": "Today, recommender systems play a vital role in the acceleration of searches by internet users to find what they are interested in. Among the strategies proposed for recommender systems, collaborative filtering has received due attention regarding its simplicity and efficiency. The key factor for the success of this strategy returns to the similarity calculation methods that affect the accuracy of its recommendations. Regarding the large volume of articles published in the field of collaborative filtering for the development of recommender systems, it is necessary to provide a comprehensive review of the similarity functions and their efficiency presented in the field. Of course, several surveys have already been published to investigate the similarity functions proposed for collaborative filtering, but these articles either have looked briefly at these functions or reviewed a few numbers of them. In this study, the effort was to provide a comprehensive study on the similarity functions proposed for collaborative filtering with a special focus on the rating-based and neighbor-based approaches. After a brief explanation of each similarity function, some popular evaluation metrics were used for their evaluation using the MovieLens datasets. The comparative evaluation results presented in this article provide a highly useful reference for researchers in this field to choose their appropriate similarity function.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008939",
    "keywords": [
      "Artificial intelligence",
      "Collaborative filtering",
      "Computer science",
      "Data mining",
      "Image (mathematics)",
      "Information retrieval",
      "Recommender system",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Khojamli",
        "given_name": "Halime"
      },
      {
        "surname": "Razmara",
        "given_name": "Jafar"
      }
    ]
  },
  {
    "title": "An automatic and efficient technique for tumor location identification and classification through breast MR images",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115580",
    "abstract": "Aim The basic objective of this paper is to develop a single structured algorithm to classify the breast tissues into normal or abnormal. Material & Methodology For this study, the breast MR images dataset of 448 images collected from Healthmap diagnostics centre, PGIMS, Rohtak, India. The proposed algorithm consists of several steps i.e. an integrated (Median Wiener & Median) filtering technique is used for de-noising; breast boundary region extraction via selection of nipple and mid- sternum points to make the image rotation invariant; determined the tumor region intensity by using morphological operations & hole filling; classify the normal and abnormal breast tissues by SVM using 14 texture features extracted through GLCM & 13 morphological or kinetic features; evaluated the exact location as well as area of abnormal tissues. Results The proposed algorithm has been evaluated statistically as well as visually. The quality parameters achieved are accuracy, sensitivity and specificity with values 0.937, 0.956 and 0.872 respectively. The Jaccard Index coefficient achieved is 0.921, which indicates promising overlap between the predicted tumor and the manually done image by the radiologist so called ground truth image. Conclusion This work may be taken as a second opinion by the radiologists. The evaluated results may give a basic foundation for optimization by selecting the features more precisely and also different evolutionary algorithms using multi-classifiers can be designed in future.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009829",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Ground truth",
      "Invariant (physics)",
      "Jaccard index",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Jaglan",
        "given_name": "Poonam"
      },
      {
        "surname": "Dass",
        "given_name": "Rajeshwar"
      },
      {
        "surname": "Duhan",
        "given_name": "Manoj"
      }
    ]
  },
  {
    "title": "A bidirectional LSTM deep learning approach for intrusion detection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115524",
    "abstract": "The rise in computer networks and internet attacks has become alarming for most service providers. It has triggered the need for the development and implementation of intrusion detection systems (IDSs) to help prevent and or mitigate the challenges posed by network intruders. Over the years, intrusion detection systems have played and continue to play a very significant role in spotting network attacks and anomalies. Numerous researchers around the globe have proposed many IDSs to combat the threat of network invaders. However, most of the previously proposed IDSs have high rates of raising false alarms. Additionally, most existing models suffer the difficulty of detecting the different attack types, especially User-to-Root (U2R) and Remote-to-Local (R2L) attacks. These two types of attacks often appear to have lower detection accuracy for the existing models. Hence, in this paper, we propose a bidirectional Long-Short-Term-Memory (BiDLSTM) based intrusion detection system to handle the challenges mentioned above. To train and measure our model’s performance, we use the NSL-KDD dataset, a benchmark dataset for most IDSs. Experimental results show and validate the effectiveness of the BiDLSTM approach. It outperforms conventional LSTM and other state-of-the-art models in terms of accuracy, precision, recall, and F-score values. It also has a much more reduced false alarm rate than the existing models. Furthermore, the BiDLSTM model achieves a higher detection accuracy for U2R and R2L attacks than the conventional LSTM.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009337",
    "keywords": [
      "Anomaly-based intrusion detection system",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Computer security",
      "Data mining",
      "False alarm",
      "Geodesy",
      "Geography",
      "Intrusion detection system",
      "Long short term memory",
      "Machine learning",
      "Recurrent neural network"
    ],
    "authors": [
      {
        "surname": "Imrana",
        "given_name": "Yakubu"
      },
      {
        "surname": "Xiang",
        "given_name": "Yanping"
      },
      {
        "surname": "Ali",
        "given_name": "Liaqat"
      },
      {
        "surname": "Abdul-Rauf",
        "given_name": "Zaharawu"
      }
    ]
  },
  {
    "title": "A cluster-based approach to predict serious adverse events in surgery",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115506",
    "abstract": "Managing risks related to the actions and conditions of the various elements that make up an operating room is a major concern during surgery. Determining alert thresholds is one of the main challenges. In this document, we propose to focus on the causes that lead to incidents as well as their prediction, which are essential elements in the determination of alerts. For that purpose, we have designed an architecture that couples a Multi-Agent System (MAS) with Case-Based Reasoning (CBR). The ability to emulate a large number of situations thanks to MAS, combined with analytical data management thanks to CBR is an efficient way of analyzing the state of the system and predicting its evolution. Beyond this architecture, decision support tools have been integrated in order to classify the behavior of entities and predict their evolution. This paper presents and analyzes the performance of our original cluster-based method (similVar) dedicated to the determination of unpredefined alert thresholds and risk prediction in surgery rooms. The obtained results prove the ability of our approach to analyze and predict the evolution of variables as disparate as the constants of a patient (“CAPNIA”, “HYPOTHERMIA”, “FeCO2”, “SpO2” etc.) or human fatigue.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009167",
    "keywords": [
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Case-based reasoning",
      "Cluster (spacecraft)",
      "Computer science",
      "Data mining",
      "Focus (optics)",
      "Machine learning",
      "Medicine",
      "Optics",
      "Physics",
      "Programming language",
      "Risk analysis (engineering)",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Perez",
        "given_name": "Bruno"
      },
      {
        "surname": "Lang",
        "given_name": "Christophe"
      },
      {
        "surname": "Henriet",
        "given_name": "Julien"
      },
      {
        "surname": "Philippe",
        "given_name": "Laurent"
      },
      {
        "surname": "Auber",
        "given_name": "Frédéric"
      }
    ]
  },
  {
    "title": "Triple band-notched UWB antenna design using a novel hybrid optimization technique based on DE and NMR algorithms",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115299",
    "abstract": "In this article, a novel hybrid optimization technique based on differential evolution (DE) and naked mole-rat (NMR) algorithms, has been introduced to solve various engineering global optimization problems. The hybridization introduced to overcome the poor exploration and local optima stagnation problems of DE and basic NMR versions. Therefore, the DE global search equations have been incorporated into the NMR worker phase for enhancing the exploration. For this, a Levy based scaling factor and simulated annealing based mating factor has been utilized. The hybrid performance of DE and NMR (HDN algorithm) is tested on twelve CEC 2005 and ten complexes CEC 2019 benchmark functions. In order to check the capability and effectiveness, HDN is introduced in the field of electromagnetics for designing a unique trapezoidal-shaped ultrawideband monopole antenna with triple-band rejection characteristics using an optimization interface of HDN and CST microwave simulator. In resultant, the size of the optimized antenna has been reduced to 20 mm × 28 mm, working over the frequency range of 2.88–13 GHz (fractional bandwidth of 126%). Additionally, band rejection characteristics were achieved by inserting two inverted U-slots and one C-strip pair inbuilt to the patch geometry. Hence, it rejected 3.3–3.6, 5.15–5.85 and 7.8–8.9 GHz frequency-bands to prevent the unwanted signal interference of the corresponding WI-MAX, WLAN, and X-bands, respectively. After that, the prototype was fabricated and measured on RF experimental test-bed to perform hardware benchmarking. Therefore, output electrical performance validates the HDN is capable enough to solve the complex antenna problems and able to adapt modern wireless communication and advanced engineering optimization tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007296",
    "keywords": [
      "Algorithm",
      "Bandwidth (computing)",
      "Computer science",
      "Electronic engineering",
      "Engineering",
      "Simulated annealing",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Singh",
        "given_name": "Gurdeep"
      },
      {
        "surname": "Singh",
        "given_name": "Urvinder"
      }
    ]
  },
  {
    "title": "HAOP-Miner: Self-adaptive high-average utility one-off sequential pattern mining",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115449",
    "abstract": "One-off sequential pattern mining (SPM) (or SPM under the one-off condition) is a kind of repetitive SPM with gap constraints, and has been widely applied in many fields. However, current research on one-off SPM ignores the utility (can be price or profit) of items, resulting in some low-frequency but extremely important patterns being ignored. To solve this issue, this paper addresses self-adaptive High-Average utility One-off sequential Pattern (HAOP) mining which has following three characteristics. Any two occurrences cannot share any letter in the sequence. The support (number of occurrences), utility and length of the pattern are considered simultaneously. The HAOP mining discovers patterns with a self-adaptive gap which means that users do not need to set the gap constraints. We propose an effective algorithm called HAOP-Miner that involves two key steps: support calculation and candidate pattern generation. For the support calculation, we propose a heuristic algorithm named the Reverse filling (Rf) algorithm that can effectively calculate the support by avoiding creating redundant nodes and pruning the redundant and useless nodes after finding an occurrence. Since HAOP mining does not satisfy the Apriori property, a support lower bound method combined with the pattern growth strategy is adopted to generate the candidate patterns. The experimental results first validate the effectiveness of HAOP-Miner, and then demonstrate that HAOP-Miner has better performance than other state-of-the-art algorithms. More importantly, HAOP-Miner is easier to mine valuable patterns. The algorithms and datasets are available at https://github.com/wuc567/Pattern-Mining/tree/master/HAOP-Miner.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008630",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer security",
      "Data mining",
      "Heuristic",
      "Key (lock)",
      "Programming language",
      "Pruning",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Youxi"
      },
      {
        "surname": "Lei",
        "given_name": "Rong"
      },
      {
        "surname": "Li",
        "given_name": "Yan"
      },
      {
        "surname": "Guo",
        "given_name": "Lei"
      },
      {
        "surname": "Wu",
        "given_name": "Xindong"
      }
    ]
  },
  {
    "title": "Parallel efficient global optimization method: A novel approach for time-dependent reliability analysis and applications",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115494",
    "abstract": "Time-dependent failure may result from material properties deterioration, random load, and other uncertain factors, which are widespread among practical engineering applications. However, in previous studies of time-dependent reliability analysis (TRA), parallel computing, which can speed up the optimization process significantly, has not been fully considered. Therefore, this study proposed a parallel efficient global optimization (PEGO) integrated with the adaptive Kriging-Monte Carlo simulation (AK-MCS) for the TRA problems. It was shown that the proposed method was superior to the existing TRA methods in computing efficiency and maintained a high accuracy to solve the high-dimension of the TRA problems. Besides, it exhibited an extensive application scope due to the excellent performances (i.e., efficiency, accuracy), which were verified via six representative cases, including four low-dimension and two high-dimension, by comparing with the existing TRA methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009040",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Dimension (graph theory)",
      "Engineering",
      "Global optimization",
      "Kriging",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Monte Carlo method",
      "Operating system",
      "Physics",
      "Power (physics)",
      "Process (computing)",
      "Programming language",
      "Pure mathematics",
      "Quantum mechanics",
      "Reliability (semiconductor)",
      "Reliability engineering",
      "Scope (computer science)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Jiawei"
      },
      {
        "surname": "Jiang",
        "given_name": "Zhenliang"
      },
      {
        "surname": "Song",
        "given_name": "Huaming"
      },
      {
        "surname": "Wan",
        "given_name": "Liangqi"
      },
      {
        "surname": "Huang",
        "given_name": "Fu"
      }
    ]
  },
  {
    "title": "Motivation detection using EEG signal analysis by residual-in-residual convolutional neural network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115548",
    "abstract": "While we know that motivated students learn better than non-motivated students but detecting motivation is challenging. Here we present a game-based motivation detection approach from the EEG signals. We take an original approach of using EEG-based brain computer interface to assess if motivation state is manifest in physiological EEG signals as well, and what are suitable conditions in order to achieve the goal? To the best of our knowledge, detection of motivation level from brain signals is proposed for the first time in this paper. In order to resolve the central obstacle of small EEG datasets containing deep features, we propose a novel and unique ‘residual-in-residual architecture of convolutional neural network (RRCNN)’ that is capable of reducing the problem of over-fitting on small datasets and vanishing gradient. Having accomplished this, several aspects of using EEG signals for motivation detection are considered, including channel selection and accuracy obtained using alpha or beta waves of EEG signals. We also include a detailed validation of the different aspects of our methodology, including detailed comparison with other works as relevant. Our approach achieves 89% accuracy in using EEG signals to detect motivation state while learning, where alpha wave signals of frontal asymmetry channels are employed. A more robust (less sensitive to learning conditions) 88% accuracy is achieved using beta waves signals of frontal asymmetry channels. The results clearly indicate the potential of detecting motivation states using EEG signals, provided suitable methodologies such as proposed in this paper, are employed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009544",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Electroencephalography",
      "Machine learning",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Programming language",
      "Psychology",
      "Residual",
      "SIGNAL (programming language)",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Chattopadhyay",
        "given_name": "Soham"
      },
      {
        "surname": "Zary",
        "given_name": "Laila"
      },
      {
        "surname": "Quek",
        "given_name": "Chai"
      },
      {
        "surname": "Prasad",
        "given_name": "Dilip K."
      }
    ]
  },
  {
    "title": "Creating navigation map in semi-open scenarios for intelligent vehicle localization using multi-sensor fusion",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115543",
    "abstract": "In order to pursue high-accuracy localization for intelligent vehicles (IVs) in semi-open scenarios, this study proposes a new map creation method based on multi-sensor fusion technique. In this new method, the road scenario fingerprint (RSF) was employed to fuse the visual features, three-dimensional (3D) data and trajectories in the multi-view and multi-sensor information fusion process. The visual features were collected in the front and downward views of the IVs; the 3D data were collected by the laser scanner and the downward camera and a homography method was proposed to reconstruct the monocular 3D data; the trajectories were computed from the 3D data in the downward view. Moreover, a new plane-corresponding calibration strategy was developed to ensure the fusion quality of sensory measurements of the camera and laser. In order to evaluate the proposed method, experimental tests were carried out in a 5 km semi-open ring route. A series of nodes were found to construct the RSF map. The experimental results demonstrate that the mean error of the nodes between the created and actual maps was 2.7 cm, the standard deviation of the nodes was 2.1 cm and the max error was 11.8 cm. The localization error of the IV was 10.8 cm. Hence, the proposed RSF map can be applied to semi-open scenarios in practice to provide a reliable basic for IV localization.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009507",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Fusion",
      "Linguistics",
      "Philosophy",
      "Real-time computing",
      "Sensor fusion"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yicheng"
      },
      {
        "surname": "Cai",
        "given_name": "Yingfeng"
      },
      {
        "surname": "Malekian",
        "given_name": "Reza"
      },
      {
        "surname": "Wang",
        "given_name": "Hai"
      },
      {
        "surname": "Sotelo",
        "given_name": "Miguel Angel"
      },
      {
        "surname": "Li",
        "given_name": "Zhixiong"
      }
    ]
  },
  {
    "title": "Detection of tuberculosis from chest X-ray images: Boosting the performance with vision transformer and transfer learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115519",
    "abstract": "Tuberculosis (TB) caused by Mycobacterium tuberculosis is a contagious disease which is among the top deadly diseases in the world. Research in Medical Imaging has been done to provide doctors with techniques and tools to early detect, monitor and diagnose the disease using Artificial Intelligence. Recently, many attempts have been made to automatically recognize TB from chest X-ray (CXR) images. Still, while the obtained performance is encouraging, according to our investigation, many of the existing approaches have been evaluated on small and undiverse datasets. We suppose that such a good performance might not hold for heterogeneous data sources, which originate from real world scenarios. Our present work aims to fill the gap and improve the prediction performance on larger datasets. In particular, we present a practical solution for the detection of tuberculosis from CXR images, making use of cutting-edge Machine Learning and Computer Vision algorithms. We conceptualize a framework by adopting three recent deep neural networks as the main classification engines, namely modified EfficientNet, modified original Vision Transformer, and modified Hybrid EfficientNet with Vision Transformer. Moreover, we also empower the learning process with various augmentation techniques. We evaluated the proposed approach using a large dataset which has been curated by merging various public datasets. The resulting dataset has been split into training, validation, and testing sets which account for 80%, 10%, and 10% of the original dataset, respectively. To further study our proposed approach, we compared it with two state-of-the-art systems. The obtained results are encouraging: the maximum accuracy of 97.72% with AUC of 100% is achieved with ViT_Base_EfficientNet_B1_224. The experimental results demonstrate that our conceived tool outperforms the considered baselines with respect to different quality metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009295",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Boosting (machine learning)",
      "Computer science",
      "Deep learning",
      "Ensemble learning",
      "Machine learning",
      "Medicine",
      "Pathology",
      "Physics",
      "Quantum mechanics",
      "Transfer of learning",
      "Transformer",
      "Tuberculosis",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Duong",
        "given_name": "Linh T."
      },
      {
        "surname": "Le",
        "given_name": "Nhi H."
      },
      {
        "surname": "Tran",
        "given_name": "Toan B."
      },
      {
        "surname": "Ngo",
        "given_name": "Vuong M."
      },
      {
        "surname": "Nguyen",
        "given_name": "Phuong T."
      }
    ]
  },
  {
    "title": "A new hybrid matheuristic of GRASP and VNS based on constructive heuristics, set-covering and set-partitioning formulations applied to the capacitated vehicle routing problem",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115556",
    "abstract": "This paper develops a hybrid matheuristic in two stages to solve Capacitated Vehicle Routing Problem (CVRP) by applying Greedy Randomized Adaptive Search Procedure (GRASP), mathematical models and Variable Neighborhood Search (VNS). The CVRP consists of designing a set of routes for a fleet of identical vehicles to attend a set of customers at shortest distance traveled. In the proposed method, a routing is performed using constructive heuristics and the Set-covering problem (SCP). SCP employs local optima solutions found in previous iterations of VNS to create a partial tour which is filled by a constructive heuristic if needed. Then, the built solution undergoes a local search phase by VNS. This process is repeated as the main loop of the GRASP. As last step of the method, the Set-partitioning problem (SPP) provides a new improved solution with regard to solutions found in the GRASP. We tested our algorithm with seven benchmarks and compared it with some other heuristics in the literature. Computational experiments showed that the proposed algorithm is competitive in terms of the quality of the solutions reported in recent works.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009623",
    "keywords": [
      "Benchmark (surveying)",
      "Computer network",
      "Computer science",
      "Constructive",
      "GRASP",
      "Geodesy",
      "Geography",
      "Greedy algorithm",
      "Greedy randomized adaptive search procedure",
      "Heuristic",
      "Heuristics",
      "Local search (optimization)",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Operating system",
      "Process (computing)",
      "Programming language",
      "Routing (electronic design automation)",
      "Set (abstract data type)",
      "Variable neighborhood search",
      "Vehicle routing problem"
    ],
    "authors": [
      {
        "surname": "Machado",
        "given_name": "André Manhães"
      },
      {
        "surname": "Mauri",
        "given_name": "Geraldo Regis"
      },
      {
        "surname": "Boeres",
        "given_name": "Maria Claudia Silva"
      },
      {
        "surname": "Rosa",
        "given_name": "Rodrigo de Alvarenga"
      }
    ]
  },
  {
    "title": "BIRCHSCAN: A sampling method for applying DBSCAN to large datasets",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115518",
    "abstract": "The DBSCAN algorithm is a traditional density-based clustering method. This algorithm allows to identify clusters of different shapes, with the ability to manage noisy patterns in the data. DBSCAN usually presents good results, however it performs several distance calculations in the clustering process. This leads to a low efficiency and its application in large datasets is not recommended. This work presents a new method to apply DBSCAN to a reduced set of elements in order to cluster the entire dataset. Therefore, the method allows clustering large datasets with similar results to the outcoe of DBSCAN on the entire dataset. Experiments results suggest that the proposed technique presents good results and consistency compared to other algorithms with similar approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009283",
    "keywords": [
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Correlation clustering",
      "DBSCAN",
      "Data mining",
      "Data set",
      "Filter (signal processing)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Programming language",
      "Sampling (signal processing)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "de Moura Ventorim",
        "given_name": "Igor"
      },
      {
        "surname": "Luchi",
        "given_name": "Diego"
      },
      {
        "surname": "Rodrigues",
        "given_name": "Alexandre Loureiros"
      },
      {
        "surname": "Varejão",
        "given_name": "Flávio Miguel"
      }
    ]
  },
  {
    "title": "A possibilistic fuzzy Gath-Geva clustering algorithm using the exponential distance",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115550",
    "abstract": "As a famous clustering algorithm, Gath-Geva (GG) clustering has been widely applied in many fields. Unlike fuzzy c-means (FCM) clustering, GG clustering uses the exponential distance with the fuzzy covariance matrix instead of Euclidean distance to cluster hyperellipsoidal data accurately. Due to the existence of probabilistic constraint, GG clustering is sensitive to noise data. For possibilistic fuzzy c-means (PFCM), the type of clustering data limits its application. To solve the noise sensitivity problem of GG clustering and extend PFCM for clustering hyperellipsoidal data, a possibilistic fuzzy Gath-Geva (PFGG) clustering algorithm is proposed. The PFGG clustering is derived from GG clustering in combination with PFCM clustering. FCM, GG, PFCM and PFGG are run on the several datasets to compare their clustering results. The experimental results show that the performance of PFGG is significantly better than the other clustering algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009568",
    "keywords": [
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Data stream clustering",
      "FLAME clustering",
      "Fuzzy clustering",
      "Fuzzy logic",
      "Mathematics",
      "Pattern recognition (psychology)",
      "k-medians clustering"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Xiaohong"
      },
      {
        "surname": "Zhou",
        "given_name": "Haoxiang"
      },
      {
        "surname": "Wu",
        "given_name": "Bin"
      },
      {
        "surname": "Zhang",
        "given_name": "Tingfei"
      }
    ]
  },
  {
    "title": "Knowledge-driven fuzzy consensus model for team formation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115522",
    "abstract": "The correct allocation of human resources is of utmost importance for any kind of enterprise and organization. Many approaches have been defined so far to support team formation leveraging on different techniques, from knowledge engineering to operational research and computational intelligence. Unfortunately, these approaches are often specifically thought for large organizations owning the right set of technological assets and human resources able to manage and use these approaches. In this work, we propose an original approach to team formation, namely the KnowMIS-Team approach, specifically designed for knowledge-intensive small and medium enterprises. This is a lightweight hybrid approach that combines three different techniques: a knowledge-driven technique for finding the most competent team for a given project based on a lightweight semantic model of knowledge, skills and attitudes; a top-down, leader-selected approach wherein the competent members selected in the previous phase can propose their candidate teams; a bottom-up fuzzy consensus-based mechanism in which the employees of the organization can express their preferences on the candidate teams. A conceptual architecture of an intelligent system implementing the approach is also presented. The KnowMIS-Team approach is the overall result of many years of experience in team formation and management for a research center and embeds all the best practices therein adopted, and it has been experimented in the same center and in other university spin-offs for many years, contributing to the realization of successful projects.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009313",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Fuzzy logic",
      "Knowledge management",
      "Process management",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "D’Aniello",
        "given_name": "Giuseppe"
      },
      {
        "surname": "Gaeta",
        "given_name": "Matteo"
      },
      {
        "surname": "Lepore",
        "given_name": "Mario"
      },
      {
        "surname": "Perone",
        "given_name": "Maria"
      }
    ]
  },
  {
    "title": "F-RRT*: An improved path planning algorithm with improved initial solution and convergence rate",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115457",
    "abstract": "During the last decades, sampling-based algorithms have been used to solve the problem of motion planning. RRT*, as an optimal variant of RRT, provides asymptotic optimality. However, the slow convergence rate and costly initial solution make it inefficient. To overcome these limitations, this paper proposes a modified RRT* algorithm, F-RRT*, which generates a better initial solution and converges faster than RRT*. F-RRT* optimizes the cost of paths by creating a parent node for the random point, instead of selecting it among the existing vertices. The creation process can be divided into two steps, the FindReachest, and CreatNode procedures, which require few calculations, and the triangle inequality is used repeatedly throughout the process, thus, resulting in paths with higher performance than those of RRT*. Since the algorithm proposed in this paper is a tree extending algorithm, its performance can be further enhanced when combined with other sampling strategies. The advantages of the proposed algorithm in the initial solution and fast convergence rate are demonstrated by comparing with RRT*, RRT*-Smart, and Q-RRT* through numerical simulations in this paper.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008708",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Filter (signal processing)",
      "Mathematical optimization",
      "Mathematics",
      "Motion planning",
      "Operating system",
      "Path (computing)",
      "Process (computing)",
      "Programming language",
      "Random tree",
      "Rate of convergence",
      "Robot",
      "Sampling (signal processing)"
    ],
    "authors": [
      {
        "surname": "Liao",
        "given_name": "Bin"
      },
      {
        "surname": "Wan",
        "given_name": "Fangyi"
      },
      {
        "surname": "Hua",
        "given_name": "Yi"
      },
      {
        "surname": "Ma",
        "given_name": "Ruirui"
      },
      {
        "surname": "Zhu",
        "given_name": "Shenrui"
      },
      {
        "surname": "Qing",
        "given_name": "Xinlin"
      }
    ]
  },
  {
    "title": "Evaluation of online emoji description resources for sentiment analysis purposes",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115279",
    "abstract": "Emoji sentiment analysis is a relevant research topic nowadays, for which emoji sentiment lexica are key assets. Manual annotation affects directly their quality (where high quality usually corresponds to high self-agreement and inter-agreement). In this work we present an unsupervised methodology to evaluate emoji sentiment lexica generated from online resources, based on a correlation analysis between a gold standard and the scores resulting from the sentiment analysis of the emoji descriptions in those resources. We consider in our study four such online resources of emoji descriptions: Emojipedia, Emojis.wiki, CLDR emoji character annotations and iEmoji. These resources provide knowledge about real (intended) emoji meanings from different author approaches and perspectives. We also present the automatic creation of a joint lexicon where the sentiment of a given emoji is obtained by averaging its scores from the unsupervised analysis of all the resources involved. The results for the joint lexicon are highly promising, suggesting that valuable subjective information can be inferred from authors’ descriptions in online resources.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007107",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data science",
      "Emoji",
      "Information retrieval",
      "Natural language processing",
      "Sentiment analysis",
      "Social media",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Fernández-Gavilanes",
        "given_name": "Milagros"
      },
      {
        "surname": "Costa-Montenegro",
        "given_name": "Enrique"
      },
      {
        "surname": "García-Méndez",
        "given_name": "Silvia"
      },
      {
        "surname": "González-Castaño",
        "given_name": "Francisco J."
      },
      {
        "surname": "Juncal-Martínez",
        "given_name": "Jonathan"
      }
    ]
  },
  {
    "title": "Advanced AI-based techniques to predict daily energy consumption: A case study",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115508",
    "abstract": "In this paper, we compare the efficiency of three different techniques used to predict the daily power consumption for a local industrial region (the studied case). At first, a variant of the Multiple Model Particle Filter is suggested as a probabilistic approach. Then, two different ANNs with one and two hidden layers respectively are designed and tested. Finally, we demonstrate a developed ANN-based design that has the ability to adapt its own structure according to the historical fluctuations provided by a given dataset that contains the consumed power for the same regarded region between 2011 and 2015; 1825 days. The potential of AI-based techniques will be emphasized by summarizing a complement heuristic study that employs the genetic algorithm to suggest an optimal outage schedule for the generators supplying the upper-mentioned region to accomplish maintenance activities that could be needed from time to time or to rest some of the units if the predicted consumption for a given period doesn’t require the total produced power.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009180",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Complement (music)",
      "Complementation",
      "Computer science",
      "Data mining",
      "Ecology",
      "Energy consumption",
      "Gene",
      "Genetic algorithm",
      "Heuristic",
      "Machine learning",
      "Operating system",
      "Phenotype",
      "Physics",
      "Power (physics)",
      "Power consumption",
      "Probabilistic logic",
      "Quantum mechanics",
      "Schedule"
    ],
    "authors": [
      {
        "surname": "Baba",
        "given_name": "Abdullatif"
      }
    ]
  },
  {
    "title": "A novel hesitant-fuzzy-based group decision approach for outsourcing risk",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115517",
    "abstract": "Outsourcing is recognized as a strategic instrument for companies to move towards diversified operation advantages and efficient global market. However, outsourcing agreements may fail due to insufficient risk consideration and evaluation. This requires an effective risk evaluation approach that necessitates an entire comprehension of the system, its requirements and dimensions. This paper proposes a novel fuzzy group multiple-criteria decision-making approach through integrating triangular fuzzy hesitant sets (TFHS), Failure Mode and Effect Analysis (FMEA) and combined compromise solution (CoCoSo) algorithm. This approach (F-FMEA-CoCoSo, henceforth) analyzes and evaluates the possible risks level of alternative outsourcing providers. It hereby provides measurable information for managers according to the linguistic opinions of industrial experts. Utilization of hesitant fuzzy variables allows decision making participants to state their opinion more precisely. A case study of an Iranian chemical company is used to exemplify the applicability and suitability of the proposed F-FMEA-CoCoSo approach. The model reflects an analytical approach while experts are experiencing considerable uncertainty in risky conditions of outsourcing operations. CoCoSo contains an integrated approach of compromise solutions and its utilization enables investigators to assure results reliability which is proven via the sensitivity analysis. Through this study, we found that risk evaluation of outsourcing providers must consider four key-factors: multi-experts, multi-criteria, multi-uncertainties and measurability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009271",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Compromise",
      "Computer science",
      "Computer security",
      "Engineering",
      "Failure mode and effects analysis",
      "Fuzzy logic",
      "Fuzzy set",
      "Group decision-making",
      "Key (lock)",
      "Law",
      "Marketing",
      "Operations research",
      "Outsourcing",
      "Physics",
      "Political science",
      "Power (physics)",
      "Quantum mechanics",
      "Reliability (semiconductor)",
      "Reliability engineering",
      "Risk analysis (engineering)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Yazdani",
        "given_name": "Morteza"
      },
      {
        "surname": "Mohammed",
        "given_name": "Ahmed"
      },
      {
        "surname": "Bai",
        "given_name": "Chunguang"
      },
      {
        "surname": "Labib",
        "given_name": "Ashraf"
      }
    ]
  },
  {
    "title": "Chaos and intensification enhanced flower pollination algorithm to solve mechanical design and unconstrained function optimization problems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115496",
    "abstract": "Nature-inspired computation has enjoyed a visible position among the soft computational techniques. Flower Pollination Algorithm (FPA), which is known as one of the outstanding algorithms in this domain, has been shown to be promising in numerous publications. FPA is comprised of two main phases, which are referred to as abiotic and biotic pollination that correspond to local and global search, respectively. It makes use of a user-supplied parameter to switch between them. This parameter is referred to as the switching probability. Thus, it can be put forward that switching probability defines the search characteristic of FPA. The present work introduces several FPA modifications that adopt chaotic maps. Moreover, the developed modifications are further enhanced by using intensifying step sizing procedure that allows a more intensified search towards the end of search. With the help of the introduced chaos in switching probability and incrementally intensifying search, developed FPA modifications are expected to find the hard-to-detect promising regions. Next, such capabilities of chaotic maps are utilized in various building blocks of FPA. Performances of all developed FPA modifications are analysed on the well-known unconstrained real-valued function minimization and mechanical design problems. Finally, appropriate non-parametric statistical analysis is carried out to observe possible statistically significant improvements over the standard FPA. As shown by the experimental study, obtained results induce success of the developed procedures, which clearly add to the capability of the canonical FPA.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009064",
    "keywords": [
      "Algorithm",
      "Biology",
      "Botany",
      "CHAOS (operating system)",
      "Computer science",
      "Computer security",
      "Evolutionary biology",
      "Function (biology)",
      "Function optimization",
      "Genetic algorithm",
      "Mathematical optimization",
      "Mathematics",
      "Optimization algorithm",
      "Pollen",
      "Pollination"
    ],
    "authors": [
      {
        "surname": "Ozsoydan",
        "given_name": "Fehmi Burcin"
      },
      {
        "surname": "Baykasoglu",
        "given_name": "Adil"
      }
    ]
  },
  {
    "title": "Big data analytics and machine learning: A retrospective overview and bibliometric analysis",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115561",
    "abstract": "The research and practice of information systems have progressed by leaps and bounds due to the emergence of big data analytics and machine learning. The paper undertakes a bibliometric study to analyze the contributions of major authors, universities/organizations, and countries in terms of productivity, citations, and bibliographic coupling. A sample of 2160 articles from the Scopus for the period 2006–2020 is the basis of the study. The publications are grouped into five clusters, of which Cluster 1 is consistently dominant in the information systems publication landscape. Cluster 2 includes published studies on the Internet of Things, security, and cloud computing, which have also been widely researched. Cluster 3, the third-largest cluster, has attempted to investigate social media analytics. Cluster 4 aims to look into the impact of classification and predictive, which is found to have sustained research interest. Topics with scant coverage in terms of papers are primarily in Cluster 5, indicating saturation in the area and the need for conducting inter-disciplinary studies. The results of our study provide valuable insights for potential contributors and global audiences in terms of emerging topics for research.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009672",
    "keywords": [
      "Analytics",
      "Artificial intelligence",
      "Bibliographic coupling",
      "Bibliometrics",
      "Big data",
      "Chemistry",
      "Chromatography",
      "Citation",
      "Cloud computing",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Data science",
      "Discipline",
      "Economics",
      "Financial economics",
      "LEAPS",
      "Law",
      "Learning analytics",
      "MEDLINE",
      "Operating system",
      "Political science",
      "Programming language",
      "Sample (material)",
      "Scopus",
      "Social science",
      "Sociology",
      "The Internet",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Justin Zuopeng"
      },
      {
        "surname": "Srivastava",
        "given_name": "Praveen Ranjan"
      },
      {
        "surname": "Sharma",
        "given_name": "Dheeraj"
      },
      {
        "surname": "Eachempati",
        "given_name": "Prajwal"
      }
    ]
  },
  {
    "title": "An efficient method to determine sample size in oversampling based on classification complexity for imbalanced data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115442",
    "abstract": "Resampling, one of the approaches to handle class imbalance, is widely used alone or in combination with other approaches, such as cost-sensitive learning and ensemble learning because of its simplicity and independence in learning algorithms. Oversampling methods, in particular, alleviate class imbalance by increasing the size of the minority class. However, previous studies related to oversampling generally have focused on where to add new samples, how to generate new samples, and how to prevent noise and they rarely have investigated how much sampling is sufficient. In many cases, the oversampling size is set so that the minority class has the same size as the majority class. This setting only considers the size of the classes in sample size determination, and the balanced training set can induce overfitting with the addition of too many minority samples. Moreover, the effectiveness of oversampling can be improved by adding synthetics into the appropriate locations. To address this issue, this study proposes a method to determine the oversampling size less than the sample size needed to obtain a balance between classes, while considering not only the absolute imbalance but also the difficulty of classification in a dataset on the basis of classification complexity. The effectiveness of the proposed sample size in oversampling is evaluated using several boosting algorithms with different oversampling methods for 16 imbalanced datasets. The results show that the proposed sample size achieves better classification performance than the sample size for attaining class balance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008563",
    "keywords": [
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Chemistry",
      "Chromatography",
      "Computer network",
      "Computer science",
      "Data mining",
      "Machine learning",
      "Mathematics",
      "Oversampling",
      "Pattern recognition (psychology)",
      "Sample (material)",
      "Sample complexity",
      "Sample size determination",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Dohyun"
      },
      {
        "surname": "Kim",
        "given_name": "Kyoungok"
      }
    ]
  },
  {
    "title": "A hybrid feedforward neural network algorithm for detecting outliers in non-stationary multivariate time series",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115545",
    "abstract": "To understand the behavior of complex phenomena, data collection and data analysis are the two basic key issues in this process. The most significant hard problem experimenters may face is the optimality selection of the dataset which provides valuable information about the behavior of the phenomena under the experimentation. An experiment with an optimal dataset allows more significant parameters to be estimated with minimum variance and without bias. Unfortunately, the collected datasets of many real-life experiments are not optimal due to the existence of outliers. An outlier is an observation that deviates significantly from other experimental data points arouse suspicions that it was generated by a different mechanism. The presence of even a few outliers leads to misspecification of model, biased estimation of parameters, and poor forecasts. Therefore, removing the outliers from the collected datasets is the critical and significant step before analyzing the data. This paper gives a hybrid feedforward neural network algorithm for detecting outliers as single points as well as small and large clusters in non-stationary multivariate time series using robust measures of location and dispersion matrix. From various perspectives, the performance of the proposed algorithm is compared with the existing algorithms under different scenarios using simulated datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009520",
    "keywords": [
      "Accounting",
      "Algorithm",
      "Anomaly detection",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Business",
      "Computer science",
      "Control engineering",
      "Data mining",
      "Data point",
      "Engineering",
      "Feed forward",
      "Feedforward neural network",
      "Machine learning",
      "Multivariate statistics",
      "Outlier",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Series (stratigraphy)",
      "Time series",
      "Variance (accounting)"
    ],
    "authors": [
      {
        "surname": "Vishwakarma",
        "given_name": "Gajendra K."
      },
      {
        "surname": "Paul",
        "given_name": "Chinmoy"
      },
      {
        "surname": "Elsawah",
        "given_name": "A.M."
      }
    ]
  },
  {
    "title": "A knowledge-based differential covariance matrix adaptation cooperative algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115495",
    "abstract": "In this paper, a knowledge-based differential covariance matrix adaptation cooperative algorithm (DCMAC) is proposed for continuous problems. On the basis of combining successful history-based adaptive DE variants with linear population size reduction (LSHADE) and covariance matrix adaptation evolutionary strategy (CMA-ES), DCMAC proposes a strategy based on knowledge reward and punishment to achieve the purpose of collaborative optimization. Afterward, an adaptive learning mechanism is introduced to optimize the parameters to balance the exploitation and exploration of DCMAC. This process enables the algorithm to have global search capability. Finally, the niching-based population size reduction mechanism is introduced to improve the local search ability of the DCMAC. A weighted mutation strategy with dynamic greedy p value and covariance matrix adaptation (CMA) sampled based on differential vector are presented. Meanwhile, the knowledge acquired in the previous iteration process is adopted in the algorithm to select a mutation strategy for generating the new candidate solutions in the next iteration. The niching population size reduction mechanism is introduced to maintain the diversity of the population and compared with the other classical population size reduction methods. The optimal combination of parameters in the DCMAC algorithm is testified by the design of the experiment. Furthermore, the DCMAC is testified on the CEC2017 benchmark functions. The effectiveness and efficiency of the DCMAC are demonstrated by the experimental results in solving complex continuous problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009052",
    "keywords": [
      "Algorithm",
      "Benchmark (surveying)",
      "CMA-ES",
      "Computer science",
      "Covariance matrix",
      "Demography",
      "Differential evolution",
      "Evolution strategy",
      "Evolutionary algorithm",
      "Geodesy",
      "Geography",
      "Geometry",
      "Mathematical optimization",
      "Mathematics",
      "Population",
      "Reduction (mathematics)",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Zuo",
        "given_name": "Yang"
      },
      {
        "surname": "Zhao",
        "given_name": "Fuqing"
      },
      {
        "surname": "Li",
        "given_name": "Zekai"
      }
    ]
  },
  {
    "title": "Multimodal video-text matching using a deep bifurcation network and joint embedding of visual and textual features",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115541",
    "abstract": "Video-text matching is of high importance in the field of machine vision and artificial intelligence. The main challenging issue in video-text matching is the projection of the video and textual features into a common semantic space, which is called video-text joint embedding. The proper functionality of video-text joint embedding depends on two important factors: the effectiveness of the extracted information for video-text matching and the suitability of network structure for the projection of the extracted features into a common space. Generally, existing approaches do not leverage all the audio and visual information of a video for video-text matching. This study proposes a new approach for video-text matching by extracting a comprehensive set of visual and textual features and projecting them into a common semantic space using an effective structure. We use a new deep network with two textual and video branches that extracts several informative high-level visual and textual features and maps them into a shared space. In the video branch, we extract several descriptors comprising appearance-based, concept-based, and action-based features as well as an audio encoder to extract sound features in the video clip. We also utilize several feature extraction approaches in the textual branch for the effective transformation of an input sentence to a common semantic space. Furthermore, an image description database is used to pre-train and initialize network weights. We evaluated the proposed architecture with two popular video description datasets and compared the results with the results of several state-of-the-art approaches. The comparison results showed the effectiveness of the proposed algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009489",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Embedding",
      "Leverage (statistics)",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Nabati",
        "given_name": "Masoomeh"
      },
      {
        "surname": "Behrad",
        "given_name": "Alireza"
      }
    ]
  },
  {
    "title": "Visual enhanced gLSTM for image captioning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115462",
    "abstract": "For reducing the negative impact of the gradient diminishing on guiding long-short term memory (gLSTM) model in image captioning, we propose a visual enhanced gLSTM model for image caption generation. In this paper, the visual features of image’s region of interest (RoI) are extracted and used as guiding information in gLSTM, in which visual information of RoI is added to gLSTM for generating more accurate image captions. Two visual enhanced methods based on region and entire image are proposed respectively. Among them the visual features from the important semantic region by CNN and the full image visual features by visual words are extracted to guide the LSTM for generating the most important semantic words. Then the visual features and text features of similar images are respectively projected to the common semantic space to obtain visual enhancement guiding information by canonical correlation analysis, and added to each memory cell of gLSTM for generating caption words. Compared with the original gLSTM method, visual enhanced gLSTM model focuses on important semantic region, which is more in line with human perception of images. Experiments on Flickr8k dataset illustrate that the proposed method can achieve more accurate image captions, and outperform the baseline gLSTM algorithm and other popular image captioning methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008757",
    "keywords": [
      "Artificial intelligence",
      "Closed captioning",
      "Computer science",
      "Computer vision",
      "Human visual system model",
      "Image (mathematics)",
      "Image retrieval",
      "Pattern recognition (psychology)",
      "Salient",
      "Semantic gap",
      "Visual Word",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Jing"
      },
      {
        "surname": "Li",
        "given_name": "Kangkang"
      },
      {
        "surname": "Wang",
        "given_name": "Zhenkun"
      },
      {
        "surname": "Zhao",
        "given_name": "Xianwen"
      },
      {
        "surname": "Wang",
        "given_name": "Zhe"
      }
    ]
  },
  {
    "title": "Service chatbots: A systematic review",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115461",
    "abstract": "Chatbots or Conversational agents are the next significant technological leap in the field of conversational services, that is, enabling a device to communicate with a user upon receiving user requests in natural language. The device uses artificial intelligence and machine learning to respond to the user with automated responses. While this is a relatively new area of study, the application of this concept has increased substantially over the last few years. The technology is no longer limited to merely emulating human conversation but is also being increasingly used to answer questions, either in academic environments or in commercial uses, such as situations requiring assistants to seek reasons for customer dissatisfaction or recommending products and services. The primary purpose of this literature review is to identify and study the existing literature on cutting-edge technology in developing chatbots in terms of research trends, their components and techniques, datasets and domains used, as well as evaluation metrics most used between 2011 and 2020. Using the standard SLR guidelines designed by Kitchenham, this work adopts a systematic literature review approach and utilizes five prestigious scientific databases for identifying, extracting, and analyzing all relevant publications during the search. The related publications were filtered based on inclusion/exclusion criteria and quality assessment to obtain the final review paper. The results of the review indicate that the exploitation of deep learning and reinforcement learning architecture is the most used technique to understand users’ requests and to generate appropriate responses. Besides, we also found that the Twitter dataset (open domain) is the most popular dataset used for evaluation, followed by Airline Travel Information Systems (ATIS) (close domain) and Ubuntu Dialog Corpora (technical support) datasets. The SLR review also indicates that the open domain provided by the Twitter dataset, airline and technical support are the most common domains for chatbots. Moreover, the metrics utilized most often for evaluating chatbot performance (in descending order of popularity) were found to be accuracy, F1-Score, BLEU (Bilingual Evaluation Understudy), recall, human-evaluation, and precision.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008745",
    "keywords": [
      "Business",
      "Computer science",
      "Marketing",
      "Service (business)"
    ],
    "authors": [
      {
        "surname": "Mohamad Suhaili",
        "given_name": "Sinarwati"
      },
      {
        "surname": "Salim",
        "given_name": "Naomie"
      },
      {
        "surname": "Jambli",
        "given_name": "Mohamad Nazim"
      }
    ]
  },
  {
    "title": "Gated relational stacked denoising autoencoder with localized author embedding for global citation recommendation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115359",
    "abstract": "Citation recommendation is an effective and efficient way to facilitate authors finding desired references. This paper presents a novel neural network based model, called gated relational probabilistic stacked denoising autoencoder with localized author (GRSLA) embedding, for global citation recommendation task. Our model is comprised of two modules with different neural network architecture. For each citing and cited papers, we use a gated paper embedding module, which is extended from probabilistic stacked denoising autoencoder (PSDAE) by adding gated units, to obtain their paper vectors. The added gated units are able to utilize text information of cited paper to refine the vector representation of citing paper in multiple semantic levels. For an author in papers, we first apply topic model to obtain his/her semantic neighbors, and then use a localized author embedding (LAE) module to excavate author vector representation from semantic and explicit neighbors. Unlike most graph convolutional network (GCN) based methods, the LAE module is able to avoid computing global Laplacian in whole graph by taking limited neighbors. Moreover, the LAE module can also be stacked to absorb more neighbors, which makes our model have high extendibility. Based on the generation process of GRSLA, we also derive a learning algorithm of our model by maximum a posteriori (MAP) estimation. We conduct experiments on the AAN, DBLP and CORD-19 datasets, and the results show that GRSLA model works well than previous global citation recommendation methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007879",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Citation",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Embedding",
      "Graph",
      "Information retrieval",
      "Law",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Probabilistic logic",
      "Representation (politics)",
      "Theoretical computer science",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Dai",
        "given_name": "Tao"
      },
      {
        "surname": "Yan",
        "given_name": "Wenjun"
      },
      {
        "surname": "Zhang",
        "given_name": "Kaiqi"
      },
      {
        "surname": "Qiu",
        "given_name": "Chen"
      },
      {
        "surname": "Zhao",
        "given_name": "Xiangmo"
      },
      {
        "surname": "Pan",
        "given_name": "Shirui"
      }
    ]
  },
  {
    "title": "Supporting digital content marketing and messaging through topic modelling and decision trees",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115546",
    "abstract": "This paper presents a machine learning approach involving tourists’ electronic word of mouth (eWOM) to support destination marketing campaigns. This approach enhances optimisation of a critical aspect of marketing campaigns, that is, the communication of the right content to the right consumers. The proposed method further considers aggregate cultural and economic-related information of the tourists’ country of origin with topic modelling and Decision Tree (DT) models. Each DT addresses different dimensions of culture and purchasing power and the way these dimensions are associated with the topics discussed in eWOM, thus revealing patterns relating tourists’ experiences with potential explanations for their dissatisfaction/satisfaction. The method is implemented in a case study in the context of tourism in Cyprus focusing on two hotel groups (2/3 and 4/5 stars) to account for their differences. Patterns emerged from the extraction of rules from DTs illuminate combinations of variables associated with tourist experience (negative or positive) for each of the two hotel categories and verify the asymmetric relationship between service performance and satisfaction. The approach can be used by management during marketing campaigns to design messages to better address the desires and needs of tourists from different cultural and economic backgrounds, as these emerge from the data analysis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009532",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Content (measure theory)",
      "Data science",
      "Decision tree",
      "Information retrieval",
      "Mathematical analysis",
      "Mathematics",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Gregoriades",
        "given_name": "Andreas"
      },
      {
        "surname": "Pampaka",
        "given_name": "Maria"
      },
      {
        "surname": "Herodotou",
        "given_name": "Herodotos"
      },
      {
        "surname": "Christodoulou",
        "given_name": "Evripides"
      }
    ]
  },
  {
    "title": "Fault simulations and diagnostics for a Boeing 747 Auxiliary Power Unit",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115504",
    "abstract": "Health monitoring of aircraft systems is of great interest to aircraft manufacturers and operators because it minimises the aircraft downtime (due to avoiding unscheduled maintenance), which in turn reduces the operating costs. The work that is presented in this paper explores, for a Boeing 747 APU, fault simulation and diagnostics for single and multiple component faults. Data that corresponds to healthy and faulty conditions is generated by a calibrated simulation model, and a set of performance parameters (symptom vector) are selected to characterise the components health state. For each component under examination, a classification algorithm is used to identify its health state (healthy or faulty) and the training strategy that is used considers the existence of multiple faults in the system. The proposed diagnostic technique is tested against single and multiple fault cases and shows good results for the compressor, turbine, Load Control Valve (LCV) and Fuel Metering Valve (FMV), even though these faults present similar fault patterns. On the contrary, the classifiers for the Speed Sensor (SS) and the generator do not provide reliable predictions. As regards the SS, the sensitivity assessment for this component showed that the existence of faults in the other components can sometimes mask the SS fault. The reason that the generator diagnosis fails under the proposed diagnostic technique is attributed to the fact that it has only a very slight influence on the other symptom vector parameters. In both cases, additional diagnostic strategies are suggested.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009143",
    "keywords": [
      "Actuator",
      "Algorithm",
      "Artificial intelligence",
      "Component (thermodynamics)",
      "Computer science",
      "Downtime",
      "Engineering",
      "Fault (geology)",
      "Fault detection and isolation",
      "Generator (circuit theory)",
      "Geology",
      "Physics",
      "Power (physics)",
      "Programming language",
      "Quantum mechanics",
      "Reliability engineering",
      "Seismology",
      "Set (abstract data type)",
      "State (computer science)",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Skliros",
        "given_name": "Christos"
      },
      {
        "surname": "Ali",
        "given_name": "Fakhre"
      },
      {
        "surname": "Jennions",
        "given_name": "Ian"
      }
    ]
  },
  {
    "title": "Real-time machine learning-based approach for pothole detection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115562",
    "abstract": "Potholes are symptoms of a poorly maintained road, pointing to an underlying structural issue. A vehicle's impact with a pothole not only makes for an uncomfortable journey, but it can also cause damage to the vehicle's wheels, tyres and suspension system resulting in high repair bills. This study presents a comparative study of machine learning models for pothole detection. The data was collected from multiple android devices/routes/cars and pre-processed using a 2-second non-overlapping moving window to extract relevant statistical features for training a binary classifier. The Test dataset was isolated entirely from the Training and Validation datasets, and a stratified K-fold cross-validation was applied to the Training dataset. The Random Forest Tree and KNN showed the best performance on the Test dataset with a similar accuracy of 0.8889. The model performance increased when random search hyperparameter tuning was applied to optimise the Random Forest Tree model's hyperparameters. The Random Forest Tree model's performance after hyperparameter tuning is 0.9444, 1.0000, 0.8889 and 0.9412 for accuracy, precision, recall, and F-score, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009684",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Geology",
      "Hyperparameter",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Motion planning",
      "Pattern recognition (psychology)",
      "Petrology",
      "Pothole (geology)",
      "Random forest",
      "Random tree",
      "Robot",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Egaji",
        "given_name": "Oche Alexander"
      },
      {
        "surname": "Evans",
        "given_name": "Gareth"
      },
      {
        "surname": "Griffiths",
        "given_name": "Mark Graham"
      },
      {
        "surname": "Islas",
        "given_name": "Gregory"
      }
    ]
  },
  {
    "title": "Prediction error distribution with dynamic asymmetry for reversible data hiding",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115475",
    "abstract": "Prediction-error expansion (PEE) by employing asymmetric prediction-error distribution (PED) provides an opportunity to decrease the embedding distortion in reversible data hiding (RDH). Nevertheless, its performance is completely dependent on the asymmetry rate of utilized PED which has never been studied before despite its crucial role in the performance of RDH algorithms. In this paper, a new prediction scheme called dynamic asymmetric distribution of error (DADE) is proposed for PEE. DADE predictor produces error distribution with dynamic asymmetry rate proportionate to embedding capacity (EC). In the proposed predictor several PEDs with a variety of asymmetry rates are composed and for a given EC, the PED with the highest asymmetry rate is selected, provided that it fulfills the EC. By increasing the asymmetry rate of the PED, the length of its short tail and the number of prediction errors in that part are reduced. Consequently, data embedding through single side expansion of prediction-error in the shorter side significantly decreases the number of shifted pixels which leads to the reduction of embedding distortion. Experimental results prove that the proposed method reduces the embedding distortion while slightly increases the EC of PEE based methods with asymmetric PED, and outperforms some state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008861",
    "keywords": [
      "Algorithm",
      "Amplifier",
      "Artificial intelligence",
      "Asymmetry",
      "Bandwidth (computing)",
      "Biological system",
      "Biology",
      "Computer science",
      "Distortion (music)",
      "Distribution (mathematics)",
      "Embedding",
      "Information hiding",
      "Mathematical analysis",
      "Mathematics",
      "Mean squared prediction error",
      "Physics",
      "Quantum mechanics",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Kouhi",
        "given_name": "Abolfazl"
      },
      {
        "surname": "Sedaaghi",
        "given_name": "Mohammad Hossein"
      }
    ]
  },
  {
    "title": "Interval–valued fuzzy and intuitionistic fuzzy–KNN for imbalanced data classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115510",
    "abstract": "Imbalanced data classification aims to categorize the complex data that have a very different distribution in the number of data samples in its classes. Since the traditional classifiers do not consider the imbalanced class distribution, they exhibit poor behavior when faced with this kind of data. There are four main kinds of solutions to deal with this problem: modifying the data distribution, modifying the learning algorithm to consider the imbalanced representation, utilizing the costs of misclassifying the data samples, and ensemble methods. In this paper, we adopt the first type of solution to resample the imbalanced data. There are two phases for resampling, in the first phase, the data is over-sampled, and then the noisy and borderline samples are chosen and removed (under–sampling phase). In the under–sampling phase, we introduce two robust extensions of KNN classifiers based on the concepts of interval-valued fuzzy and intuitionistic fuzzy sets for imbalanced data to filter the noisy and borderline samples. The process of choosing and removing these samples is an iterative procedure (i.e., the Iterative Partitioning Filter). Thus, the proposed filter exploits the classifier ensemble. We propose a new rule for voting concerning the new interval-valued intuitionistic fuzzy-based classifiers. The characteristics of the proposed method are debated in a comprehensive experimental verification comparing by SMOTE and its most well–known generalizations. The experiments on both synthetic data sets with different levels of noise and shapes of borderlines as well as real–world data sets show the potentiality of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009209",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Filter (signal processing)",
      "Fuzzy logic",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Zeraatkar",
        "given_name": "Saeed"
      },
      {
        "surname": "Afsari",
        "given_name": "Fatemeh"
      }
    ]
  },
  {
    "title": "Knowledge-based approach for dimensionality reduction solving repetitive combinatorial optimization problems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115502",
    "abstract": "This paper proposes a knowledge-based approach that combines case-based reasoning and operational research methodologies for solving repetitive combinatorial optimization problems. The novel approach makes use of past experience which is generally neglected in solving current optimization problems, and introduces this knowledge into operational research techniques for problem-solving, especially when the dimension of the problem is large and conventional schemes cannot solve it within a reasonable time limit. It greatly reduces the dimension of the problem and provides near-optimal solutions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100912X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorial explosion",
      "Combinatorial optimization",
      "Combinatorics",
      "Computer science",
      "Curse of dimensionality",
      "Dimension (graph theory)",
      "Dimensionality reduction",
      "Geometry",
      "Limit (mathematics)",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Optimization problem",
      "Pure mathematics",
      "Reduction (mathematics)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Qing"
      }
    ]
  },
  {
    "title": "The impact of word sense disambiguation on stock price prediction",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115568",
    "abstract": "State-of-the-art decision support systems for stock price prediction incorporate pattern-based event detection in text into their predictions. These systems typically fail to account for word meaning, even though word sense disambiguation is crucial for text understanding. Therefore, we propose an advanced natural language processing pipeline for event-based stock price prediction, that allows for word sense disambiguation to be incorporated in the event detection process. We identify events in natural language news messages and subsequently weight these events for their historical impact on stock prices. We assess the merit of word sense disambiguation in event-based stock price prediction in two evaluation scenarios for NASDAQ-100 companies, based on historical stock prices and news articles retrieved from Dow Jones Newswires over a 2-year period. We evaluate the precision of generated buy and sell signals based on our predicted stock price movements, as well as the excess returns generated by a trading strategy that acts upon these signals. Event-based stock price predictions seem most reliable about 2 days into the future. The number of detected events tends to reduce with over 30% when graph-based word sense disambiguation using a degree centrality measure is applied in the event detection process, thus reducing the noise introduced into the stock price movement predictions by high-impact ambiguous events. As a result, modest improvements in the precision of buy and sell signals generated based on these predictions tend to lead to vast improvements of on average about 70% in the associated excess returns.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009738",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Economics",
      "Management",
      "Natural language processing",
      "Paleontology",
      "SemEval",
      "Series (stratigraphy)",
      "Stock price",
      "Task (project management)",
      "Word-sense disambiguation",
      "WordNet"
    ],
    "authors": [
      {
        "surname": "Hogenboom",
        "given_name": "Alexander"
      },
      {
        "surname": "Brojba-Micu",
        "given_name": "Alex"
      },
      {
        "surname": "Frasincar",
        "given_name": "Flavius"
      }
    ]
  },
  {
    "title": "SkillNER: Mining and mapping soft skills from any text",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115544",
    "abstract": "In today's digital world, there is an increasing focus on soft skills. On the one hand, they facilitate innovation at companies, but on the other, they are unlikely to be automated soon. Researchers struggle with accurately approaching quantitatively the study of soft skills due to the lack of data-driven methods to retrieve them. This limits the possibility for psychologists and HR managers to understand the relation between humans and digitalisation. This paper presents SkillNER, a novel data-driven method for automatically extracting soft skills from text. It is a named entity recognition (NER) system trained with a support vector machine (SVM) on a corpus of more than 5000 scientific papers. We developed this system by measuring the performance of our approach against different training models and validating the results together with a team of psychologists. Finally, SkillNER was tested in a real-world case study using the job descriptions of ESCO (European Skill/Competence Qualification and Occupation) as textual source. The system enabled the detection of communities of job profiles based on their shared soft skills and communities of soft skills based on their shared job profiles. This case study demonstrates that the tool can automatically retrieve soft skills from a large corpus in an efficient way, proving useful for firms, institutions, and workers. The tool is open and available online to foster quantitative methods for the study of soft skills.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009519",
    "keywords": [
      "Artificial intelligence",
      "Competence (human resources)",
      "Computer science",
      "Data mining",
      "Data science",
      "Human–computer interaction",
      "Knowledge management",
      "Medical education",
      "Medicine",
      "Psychology",
      "Relation (database)",
      "Social psychology",
      "Soft skills",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Fareri",
        "given_name": "Silvia"
      },
      {
        "surname": "Melluso",
        "given_name": "Nicola"
      },
      {
        "surname": "Chiarello",
        "given_name": "Filippo"
      },
      {
        "surname": "Fantoni",
        "given_name": "Gualtiero"
      }
    ]
  },
  {
    "title": "Object oriented time series exploration: Applied to power consumption analysis of embedded systems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115531",
    "abstract": "Performance monitoring and anomaly detection are major issues in designing and maintaining electronic devices and systems. In recent years, they become more difficult due to the increased complexity of hardware and software. Hence, an important point is to collect representative signal samples and reveal characteristic features allowing to evaluate device operational profiles. This results in the need of an efficient time series analysis. This problem is considered in relevance to embedded systems and Internet of Things devices. The paper presents a new scheme of decomposing time series by introducing higher level objects targeted at the searched system properties. They create a compact state model which facilitates deriving knowledge on system behaviour to validate correctness of its operation. The collected samples are aggregated into objects according to predefined similarity metrics, these objects can be traced, correlated, and merged with relevant operational log events. For this purpose, a set of original algorithms have been composed and included in the developed software tool. The presented approach has been evaluated on a representative dataset obtained from commercial Holter devices and was used to explore their energy consumption efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009398",
    "keywords": [
      "Algorithm",
      "Anomaly detection",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Correctness",
      "Data mining",
      "Ecology",
      "Embedded system",
      "Energy consumption",
      "Image (mathematics)",
      "Law",
      "Operating system",
      "Paleontology",
      "Physics",
      "Political science",
      "Power (physics)",
      "Power consumption",
      "Programming language",
      "Quantum mechanics",
      "Real-time computing",
      "Relevance (law)",
      "Series (stratigraphy)",
      "Set (abstract data type)",
      "Similarity (geometry)",
      "Software"
    ],
    "authors": [
      {
        "surname": "Krosman",
        "given_name": "Kazimierz"
      },
      {
        "surname": "Sosnowski",
        "given_name": "Janusz"
      },
      {
        "surname": "Gawkowski",
        "given_name": "Piotr"
      }
    ]
  },
  {
    "title": "Feature selection in jump models",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115558",
    "abstract": "Jump models switch infrequently between states to fit a sequence of data while taking the ordering of the data into account We propose a new framework for joint feature selection, parameter and state-sequence estimation in jump models. Feature selection is necessary in high-dimensional settings where the number of features is large compared to the number of observations and the underlying states differ only with respect to a subset of the features. We develop and implement a coordinate descent algorithm that alternates between selecting the features and estimating the model parameters and state sequence, which scales to large data sets with large numbers of (noisy) features. We demonstrate the usefulness of the proposed framework by comparing it with a number of other methods on both simulated and real data in the form of financial returns, protein sequences, and text. By leveraging information embedded in the ordering of the data, the resulting sparse jump model outperforms all other methods considered and is remarkably robust to noise.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009647",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Coordinate descent",
      "Data mining",
      "Feature (linguistics)",
      "Feature selection",
      "Genetics",
      "Image (mathematics)",
      "Jump",
      "Linguistics",
      "Model selection",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Selection (genetic algorithm)",
      "Sequence (biology)",
      "State (computer science)"
    ],
    "authors": [
      {
        "surname": "Nystrup",
        "given_name": "Peter"
      },
      {
        "surname": "Kolm",
        "given_name": "Petter N."
      },
      {
        "surname": "Lindström",
        "given_name": "Erik"
      }
    ]
  },
  {
    "title": "Predicting Ethereum prices with machine learning based on Blockchain information",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115480",
    "abstract": "With the growing interest in cryptocurrency and its fundamental algorithm, studies of cryptocurrency price predictions have been actively conducted in various academic disciplines. Since cryptocurrency is generated and consumed by Blockchain systems, Blockchain-specific information can be considered as the main component in forecasting the values of cryptocurrency. This perspective has been widely adopted in studies of Bitcoin price predictions. However, we find that Ethereum, a popular and leading cryptocurrency in the market, has Blockchain information that differs from that of Bitcoin. Hence, this study investigates the relationship between inherent Ethereum Blockchain information and Ethereum prices. Furthermore, we investigate how Blockchain information concerning other publicly available coins on the market is associated with Ethereum prices. Our key findings reveal that macro-economy factors, Ethereum-specific Blockchain information, and the Blockchain information of other cryptocurrency play important roles in the prediction of Ethereum prices.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008915",
    "keywords": [
      "Blockchain",
      "Computer science",
      "Computer security",
      "Cryptocurrency",
      "Key (lock)",
      "Macro",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Han-Min"
      },
      {
        "surname": "Bock",
        "given_name": "Gee-Woo"
      },
      {
        "surname": "Lee",
        "given_name": "Gunwoong"
      }
    ]
  },
  {
    "title": "Bengali text document categorization based on very deep convolution neural network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115394",
    "abstract": "In recent years, the amount of digital text contents or documents in the Bengali language has increased enormously on online platforms due to the effortless access of the Internet via electronic gadgets. As a result, an enormous amount of unstructured data is created that demands much time and effort to organize, search or manipulate. To manage such a massive number of documents effectively, an intelligent text document classification system is proposed in this paper. Intelligent classification of text document in a resource-constrained language (like Bengali) is challenging due to unavailability of linguistic resources, intelligent NLP tools, and larger text corpora. Moreover, Bengali texts are available in two morphological variants (i.e., Sadhu-bhasha and Cholito-bhasha) making the classification task more complicated. The proposed intelligent text classification model comprises GloVe embedding and Very Deep Convolution Neural Network (VDCNN) classifier. Due to the unavailability of standard corpus, this work develops a large Embedding Corpus (EC) containing 969 , 000 unlabelled texts and Bengali Text Classification Corpus (BDTC) containing 156 , 207 labelled documents arranged into 13 categories. Moreover, this work proposes the Embedding Parameters Identification (EPI) Algorithm, which selects the best embedding parameters for low-resource languages (including Bengali). Evaluation of 165 embedding models with intrinsic evaluators (semantic & syntactic similarity measures) shows that the GloVe model is more suitable (regarding Spearman & Pearson correlation) than other embeddings (Word2Vec, FastText, m-BERT) in Bengali text. Experimental results on the test dataset confirm that the proposed GloVe + VDCNN model outperformed (achieving the highest 96.96 % accuracy) the other classification models and existing methods to perform the Bengali text classification task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008174",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Bengali",
      "Categorization",
      "Classifier (UML)",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Document classification",
      "Embedding",
      "Engineering",
      "Information retrieval",
      "Machine learning",
      "Natural language processing",
      "Reliability engineering",
      "Unavailability",
      "Word2vec",
      "WordNet"
    ],
    "authors": [
      {
        "surname": "Hossain",
        "given_name": "Md. Rajib"
      },
      {
        "surname": "Hoque",
        "given_name": "Mohammed Moshiul"
      },
      {
        "surname": "Siddique",
        "given_name": "Nazmul"
      },
      {
        "surname": "Sarker",
        "given_name": "Iqbal H."
      }
    ]
  },
  {
    "title": "Deep4SNet: deep learning for fake speech classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115465",
    "abstract": "Fake speech consists on voice recordings created even by artificial intelligence or signal processing techniques. Among the methods for generating false voice recordings are Deep Voice and Imitation. In Deep voice, the recordings sound slightly synthesized, whereas in Imitation, they sound natural. On the other hand, the task of detecting fake content is not trivial considering the large number of voice recordings that are transmitted over the Internet. In order to detect fake voice recordings obtained by Deep Voice and Imitation, we propose a solution based on a Convolutional Neural Network (CNN), using image augmentation and dropout. The proposed architecture was trained with 2092 histograms of both original and fake voice recordings and cross-validated with 864 histograms. 476 new histograms were used for external validation, and Precision (P) and Recall (R) were calculated. Detection of fake audios reached P = 0.997 , R = 0.997 for Imitation-based recordings, and P = 0.985 , R = 0.944 for Deep Voice-based recordings. The global accuracy was 0.985. According to the results, the proposed system is successful in detecting fake voice content.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008770",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Dropout (neural networks)",
      "Histogram",
      "Image (mathematics)",
      "Imitation",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Psychology",
      "Social psychology",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Ballesteros",
        "given_name": "Dora M."
      },
      {
        "surname": "Rodriguez-Ortega",
        "given_name": "Yohanna"
      },
      {
        "surname": "Renza",
        "given_name": "Diego"
      },
      {
        "surname": "Arce",
        "given_name": "Gonzalo"
      }
    ]
  },
  {
    "title": "Sarcasm Detection using Cognitive Features of Visual Data by Learning Model",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115476",
    "abstract": "The objective of the paper is to detect sarcasm in human communication. The methodology uses basic cognitive features of human utterances by capturing three modes of data viz., voice, text, and temporal facial features. The captured data is unstructured as it consists of parameters of feelings and emotions to generate sarcasm which affects expressions through glottal and facial organs. The data capturing method is equally challenging as compared to the method of data processing to acquire features. The significant work is aligned to make natural decisions in the prediction processes using cognitive information in the data lineage. Sarcasm detection in natural human communication is a challenging process. The Linguistic features of natural language processing (NLP) methods help identify sentiment as negative and positive sentences based on polarity using the pre-labelled samples. The multiclass neural network model is used as a soft cognition method for the detection of sarcasm under cloud resources. Identified cognitive features have information like voice cues and eye movements, they tend to influence the decision of detecting sarcasm. The visual data are found to be quite interesting and can establish a strong platform in the area of NLP for further research work.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008873",
    "keywords": [
      "Artificial intelligence",
      "Cognition",
      "Computer science",
      "Irony",
      "Linguistics",
      "Machine learning",
      "Natural language processing",
      "Neuroscience",
      "Philosophy",
      "Psychology",
      "Sarcasm",
      "Sentiment analysis"
    ],
    "authors": [
      {
        "surname": "Hiremath",
        "given_name": "Basavaraj N."
      },
      {
        "surname": "Patil",
        "given_name": "Malini M."
      }
    ]
  },
  {
    "title": "A machine learning algorithm for stock picking built on information based outliers",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115497",
    "abstract": "Stock picking based on regularities in time series is one of the most studied topics in the financial industry. Various machine learning techniques have been employed for this task. We build a trading strategy algorithm that receives as input indicators defined through outliers in the time series of stocks (return, volume, volatility, bid-ask spread). The procedure identifies the most relevant subset of indicators for the prediction of stock returns by combining an heuristic search strategy, guided from the Information Gain Criterium, with the Naive-Bayes classification algorithm. We apply the methodology to two sets of stocks belonging respectively to the EUROSTOXX50 and the DOW JONES index. Performance is smoother than in the Buy&Hold strategy and yields a higher risk-adjusted return, in particular in a turbulent period. However, outperformance vanishes when transaction costs (5–15 basis points) are inserted. Asset return and return/volume serial correlation turn out to be the most relevant indicators to build the trading algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009076",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Database",
      "Database transaction",
      "Econometrics",
      "Economics",
      "Engineering",
      "Machine learning",
      "Mechanical engineering",
      "Naive Bayes classifier",
      "Outlier",
      "Stock (firearms)",
      "Support vector machine",
      "Volatility (finance)"
    ],
    "authors": [
      {
        "surname": "Barucci",
        "given_name": "Emilio"
      },
      {
        "surname": "Bonollo",
        "given_name": "Michele"
      },
      {
        "surname": "Poli",
        "given_name": "Federico"
      },
      {
        "surname": "Rroji",
        "given_name": "Edit"
      }
    ]
  },
  {
    "title": "AMFB: Attention based multimodal Factorized Bilinear Pooling for multimodal Fake News Detection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115412",
    "abstract": "Fake news is the information or stories that are intentionally created to deceive or mislead the readers. In recent times, Fake news detection has attracted the attention of researchers and practitioners due to its many-fold benefits, including bringing in preventive measures to tackle the dissemination of misinformation that could otherwise disturb the social fabrics. Social media in recent times are heavily loaded with multimedia news and information. People prefer online news reading and find it more informative and convenient if they have access to multimedia content in the forms of text, images, audio, and videos. In early studies, researchers have proposed several fake news detection mechanisms that mostly utilize the textual features and not proper to learn multimodal (textual + visual) shared representation. To overcome these limitations, in this paper, we propose a multimodal fake news detection framework with appropriate multimodal feature fusion that leverages information from text and image and tries to maximize the correlation between them to get the efficient multimodal shared representation. We empirically show that text, when combined with the image, can improve the performance of the model. The model detects the post once it is introduced into the network in an early stage. At the early stage of a news post’s introduction into the network, the model takes the text and image of the post as input and decides whether this is fake or genuine. Since this model only analyzes news contents, It does not require any prior information regarding the user and network details. This framework has four different sub-modules viz. Attention Based Stacked Bidirectional Long Short Term Memory (ABS-BiLSTM) for textual feature representation, Attention Based Multilevel Convolutional Neural Network–Recurrent Neural Network (ABM-CNN–RNN) for visual feature extraction, multimodal Factorized Bilinear Pooling (MFB) for feature fusion and finally Multi-Layer Perceptron (MLP) for the classification. We perform experiments on two publicly available datasets, viz. Twitter and Weibo. Evaluation results show the efficacy of our proposed approach that performs significantly better compared to the state-of-the-art models. It shows to outperform the current state-of-the-art by approximately 10 points for the Twitter dataset. In contrast, the Weibo dataset achieves an overall better performance with balanced F1-scores between fake and real classes. Furthermore, the complexity of our proposed model is significantly lower than the state-of-the-art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008320",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Feature (linguistics)",
      "Information retrieval",
      "Law",
      "Linguistics",
      "Misinformation",
      "News aggregator",
      "Philosophy",
      "Political science",
      "Politics",
      "Pooling",
      "Reading (process)",
      "Representation (politics)",
      "Social media",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Kumari",
        "given_name": "Rina"
      },
      {
        "surname": "Ekbal",
        "given_name": "Asif"
      }
    ]
  },
  {
    "title": "A minority oversampling approach for fault detection with heterogeneous imbalanced data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115492",
    "abstract": "Between-class imbalance and feature heterogeneity commonly coexist in monitoring data collected from engineering systems. The decision hyperplanes of data-driven methods when adopted for fault detection with imbalanced data may be biased to the majority class, resulting in a low fault-detection rate. Various data- and algorithm-level methods have been proposed, with minority oversampling methods among the most popular and successful. However, state-of-the-art minority oversampling methods are unsuitable for imbalanced data with heterogeneous features, including both numeric and nominal variables. There are two main drawbacks: 1) taking a nominal variable as a numeric variable is not trivial, and synthetic minority samples may exceed the value range of nominal variables; 2) conventional distance measures, e.g., Euclidean distance, cannot properly measure the similarity of samples with heterogeneous features. For these considerations, this work proposes new fault-detection methods. The methodological contributions include: 1) two different distance measures adopted for heterogeneous features in the minority class; 2) a new method for coordinate calculation of synthetic samples considering feature heterogeneity; and 3) a new strategy to encode nominal variables into numeric data for data-driven models. Several public heterogeneous imbalanced datasets and a real case study considering fault detection in high-speed trains are considered to verify the effectiveness of the proposed methods. To the knowledge of the author, this is also the first time that the effectiveness of diverse oversampling methods on heterogeneous imbalance data are specifically discussed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009027",
    "keywords": [
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Computer network",
      "Computer science",
      "Data mining",
      "Fault (geology)",
      "Geology",
      "Machine learning",
      "Oversampling",
      "Seismology"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "Kinematic adaptive frequency sampling combined spatio temporal features for snow monitoring in aerospace applications",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115472",
    "abstract": "A new era of aerospace systems has instigated highly coupled frameworks, leading to a significant rise in design complexity. The lack of present-day design systems to govern this complexity has resulted in considerable time and schedule overruns compromising the accuracy during the development of military and commercial platforms. This work presents the framework for a new design process to reduce the complexity and improve accuracy using Spatio Temporal-based Kinematic Adaptive Sampling (ST-KAS). First, dynamic modeling of the Time Factor Matrix (TFM) and Spatial Association Matrix (SAM) based on the location and time is performed to extract relevant features. Second, the Kinematic Adaptive Frequency Sampling Algorithm is designed through a dynamic model and a Probability Uncertainty Measure. However, an adaptive control measure is required to flexibly cope with the uncertainty because the operating environment of the TFM and SAM is varied, and uncertainty exists depending on the number of locations to be analyzed for monitoring snow in aerospace applications. The performance of the Kinematic Adaptive Frequency Sampling is also verified through a numerical simulation according to computational overhead, computational time, and probability of fatality. Simulation experiments show that the suggested solution can minimize the complexity rate for sensing while maintaining the error rate at acceptable levels.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008836",
    "keywords": [
      "Adaptive sampling",
      "Aerospace",
      "Aerospace engineering",
      "Algorithm",
      "Classical mechanics",
      "Computational complexity theory",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Filter (signal processing)",
      "Kinematics",
      "Mathematics",
      "Monte Carlo method",
      "Operating system",
      "Overhead (engineering)",
      "Physics",
      "Real-time computing",
      "Sampling (signal processing)",
      "Schedule",
      "Simulation",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Ramalingam",
        "given_name": "Parameshwaran"
      },
      {
        "surname": "Gopalakrishnan",
        "given_name": "Lakshminarayanan"
      },
      {
        "surname": "Ramachandran",
        "given_name": "Manikandan"
      },
      {
        "surname": "Patan",
        "given_name": "Rizwan"
      }
    ]
  },
  {
    "title": "Evaluation of the impact of blockchain technology on supply chain using cognitive maps",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115455",
    "abstract": "Blockchain technology have gained importance in the supply chain with its transparency, robustness, and elimination of intermediaries. Different impacts are expected with the integration of blockchain technology in supply chain processes. Comprehensive evaluation methods are required to take important strategic decisions about blockchain technology. To examine the impact of blockchain technology, factors of cost, risks, business, and customer related benefits should be considered comprehensively. The purpose of this paper is to investigate the causal relationships among the factors to evaluate blockchain technology impact on supply chain. Cognitive maps (CM) methods which are hesitant fuzzy cognitive map (HFCM), probabilistic linguistic fuzzy cognitive map (PL-FCM) and rough set cognitive map (RS-CM) are used in this study. Also, PL-FCM that is generated based on probabilistic linguistic term sets is used for the first time in this study as a novel cognitive model. This study provides a contribution by developing cognitive map methods to measure and examine the impact of blockchain technology on supply chain for the first time. Firstly, the impacts of blockchain technology on supply chain and their relations are formed according to expert views and literature review. Then, five scenarios are considered using cognitive mapping methods, future predictions in terms of supply chain management are determined. Sensitivity analysis is performed. In this way, firms can analyze various implications and under what conditions how blockchain technology will have an impact on the supply chain.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100868X",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Blockchain",
      "Business",
      "Chemistry",
      "Cognition",
      "Cognitive map",
      "Computer science",
      "Computer security",
      "Fuzzy cognitive map",
      "Fuzzy logic",
      "Fuzzy number",
      "Fuzzy set",
      "Gene",
      "Marketing",
      "Neuroscience",
      "Probabilistic logic",
      "Process management",
      "Robustness (evolution)",
      "Supply chain",
      "Supply chain management"
    ],
    "authors": [
      {
        "surname": "Budak",
        "given_name": "Ayşenur"
      },
      {
        "surname": "Çoban",
        "given_name": "Veysel"
      }
    ]
  },
  {
    "title": "Selection of projects for automotive assembly structures using a hybrid method composed of the group-input compatible, best-worst method for criteria weighting and TrBF-TOPSIS",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115557",
    "abstract": "The constant search for the optimal outcome, and its associated decision-making schemes within automotive assembly projects, can be greatly enhanced by utilizing informed decision methods that access relevant information. When simulated project alternatives are developed and benchmarks established, decision makers must select the appropriate project. When differences in opinion arise between decision makers, or when selection criteria are numerous, it can be difficult to be sure the correct choice was made. This highlights the need for a method that assists decision makers, when presented with such circumstances, so that they can be sure their choice is informed and optimal. Therefore, this paper proposes the use of a hybrid best-worst method (BWM) for weighting group decision making; and a Trapezoidal Bipolar Fuzzy TOPSIS (TrBF-TOPSIS) decision making strategy. The proposed method is used in ranking twenty-one design alternatives for automotive door assembly structures. The observed results indicate that the proposed method can be a relevant tool for variable selection in automotive assembly projects that involve multiple criteria and alternative designs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009635",
    "keywords": [
      "Aerospace engineering",
      "Artificial intelligence",
      "Automotive industry",
      "Computer science",
      "Data mining",
      "Engineering",
      "Fuzzy logic",
      "Group decision-making",
      "Law",
      "Machine learning",
      "Medicine",
      "Multiple-criteria decision analysis",
      "Operations research",
      "Political science",
      "Radiology",
      "Ranking (information retrieval)",
      "Selection (genetic algorithm)",
      "TOPSIS",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Biscaia",
        "given_name": "Ricardo Vinícius Bubna"
      },
      {
        "surname": "Braghini Junior",
        "given_name": "Aldo"
      },
      {
        "surname": "Colmenero",
        "given_name": "João Carlos"
      }
    ]
  },
  {
    "title": "Z-relation equation-based decision making",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115387",
    "abstract": "Fuzzy relations were a main tool of fuzzy set theory in decision making, control and other fields. However, partial reliability of decision-relevant information is missed in these approaches. To deal with fuzziness and partial reliability of information, Zadeh introduced the concept of Z-number. The purpose of research presented in this paper is to develop an approach to decision making under Z-number-valued information. We introduce a definition of Z-number-valued relation (Z-relation) and some operations. The reason is to use Z-relations for evaluation of alternatives w.r.t. multiple criteria under imperfect information provided by a decision maker. In view of this, a Z-relation equation is formulated and some results on its solvability are given. These results are a basis of solving decision problems starting from multicriteria evaluation till final ranking of alternatives. The major conclusion is that this approach allows to deal with fusion of fuzzy and probabilistic information at a feasible level of computational complexity. The main limitation of the approach is difficulty of identification of a Z-relation. No expert knowledge (as it requires intensive involvement of experts) or data-driven information (when data quality is low) may exist. At the same time, computational complexity will grow with the high increase of a number of alternatives. A numerical example on decision making for project selection is considered to illustrate applicability of the study.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008113",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Fuzzy logic",
      "Fuzzy set",
      "Mathematics",
      "Physics",
      "Power (physics)",
      "Probabilistic logic",
      "Quantum mechanics",
      "Ranking (information retrieval)",
      "Relation (database)",
      "Reliability (semiconductor)"
    ],
    "authors": [
      {
        "surname": "Aliev",
        "given_name": "Rafik A."
      },
      {
        "surname": "Guirimov",
        "given_name": "Babek G."
      },
      {
        "surname": "Huseynov",
        "given_name": "Oleg H."
      },
      {
        "surname": "Aliyev",
        "given_name": "Rafig R."
      }
    ]
  },
  {
    "title": "Using enhanced crow search algorithm optimization-extreme learning machine model to forecast short-term wind power",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115579",
    "abstract": "The strong volatility and randomness of wind power impact the grid and reduce the voltage quality of the grid when wind power is connected to the grid in large scale. The power sector takes the wind abandonment measures to ensure the grid voltage stability. The enhanced crow search algorithm optimization-extreme learning machine (ENCSA-ELM) model is proposed to accurately forecast short-term wind power to improve the utilization efficiency of clean energy. (1) The enhanced crow search algorithm (ENCSA) is proposed and applied to short-term wind power forecast. The convergence performance test revealed that the local development and global exploration capabilities of the ENCSA were enhanced, and the test result of the proposed ENCSA algorithm outperformed other well-known nature inspired algorithms and state-of-the-art CSA variants; (2) The output and input of the forecasting models were determined by analysis of the wind power impact factors and the wind power samples in autumn, winter and spring were forecasted by the ENCSA-ELM model; and (3) The forecast results were analyzed by multiple evaluation indexes. The simulation experiments revealed that the error interval and evaluation indexes of the ENCSA-ELM model outperformed the state-of-the-art wind power forecast methods, traditional machine learning models and ELM optimized by other algorithms. The RMSE value and MAPE value of the proposed model were controlled below 20% and 4%. Accurate wind power prediction maintains the voltage stability of power grid and increases the utilization efficiency of clean energy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009817",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Electric power system",
      "Electrical engineering",
      "Engineering",
      "Extreme learning machine",
      "Machine learning",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Stability (learning theory)",
      "Wind power",
      "Wind power forecasting"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Ling-Ling"
      },
      {
        "surname": "Liu",
        "given_name": "Zhi-Feng"
      },
      {
        "surname": "Tseng",
        "given_name": "Ming-Lang"
      },
      {
        "surname": "Jantarakolica",
        "given_name": "Korbkul"
      },
      {
        "surname": "Lim",
        "given_name": "Ming K."
      }
    ]
  },
  {
    "title": "TripMD: Driving patterns investigation via motif analysis",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115527",
    "abstract": "Processing driving data and investigating driving behavior has been receiving an increasing interest in the last decades, with applications ranging from car insurance pricing to policy making. A common strategy to analyze driving behavior is to study the maneuvers being performance by the driver. In this paper, we propose TripMD, a system that extracts the most relevant driving patterns from sensor recordings (such as acceleration) and provides a visualization that allows for an easy investigation. Additionally, we test our system using the UAH-DriveSet dataset, a publicly available naturalistic driving dataset. We show that (1) our system can extract a rich number of driving patterns from a single driver that are meaningful to understand driving behaviors and (2) our system can be used to identify the driving behavior of an unknown driver from a set of drivers whose behavior we know.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009350",
    "keywords": [
      "Computer science",
      "Data mining",
      "Programming language",
      "Ranging",
      "Set (abstract data type)",
      "Telecommunications",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Silva",
        "given_name": "Maria Inês"
      },
      {
        "surname": "Henriques",
        "given_name": "Roberto"
      }
    ]
  },
  {
    "title": "A new data filling approach based on probability analysis in incomplete soft sets",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115358",
    "abstract": "Incomplete information is a common phenomenon in practical situations, and even happens in the uncertain problems. Soft set theory shows good performance in dealing with uncertain problems. While the problem of incomplete information appears in soft set. The existing approach can only handle with incomplete data in some special cases with low accuracy. For these deficiencies, we propose a new approach. The proposed method can solve some cases which can’t be solved by the existing approach and avoid the influence of the threshold with subjective factors. Further error rate of prediction results can be reduced to the minimum by using the proposed approach. To verify the effectiveness and feasibility of new method, we compare proposed method with existing approach. Experimental results based on UCI benchmark database are shown to demonstrate the performance of new approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007867",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Kong",
        "given_name": "Zhi"
      },
      {
        "surname": "Zhao",
        "given_name": "Jie"
      },
      {
        "surname": "Wang",
        "given_name": "Lifu"
      },
      {
        "surname": "Zhang",
        "given_name": "Junjie"
      }
    ]
  },
  {
    "title": "Towards zero-shot cross-lingual named entity disambiguation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115542",
    "abstract": "In cross-Lingual Named Entity Disambiguation (XNED) the task is to link Named Entity mentions in text in some native language to English entities in a knowledge graph. XNED systems usually require training data for each native language, limiting their application for low resource languages with small amounts of training data. Prior work have proposed so-called zero-shot transfer systems which are only trained in English training data, but required native prior probabilities of entities with respect to mentions, which had to be estimated from native training examples, limiting their practical interest. In this work we present a zero-shot XNED architecture where, instead of a single disambiguation model, we have a model for each possible mention string, thus eliminating the need for native prior probabilities. Our system improves over prior work in XNED datasets in Spanish and Chinese by 32 and 27 points, and matches the systems which do require native prior information. We experiment with different multilingual transfer strategies, showing that better results are obtained with a purpose-built multilingual pre-training method compared to state-of-the-art generic multilingual models such as XLM-R. We also discovered, surprisingly, that English is not necessarily the most effective zero-shot training language for XNED into English. For instance, Spanish is more effective when training a zero-shot XNED system that disambiguates Basque mentions with respect to an English knowledge graph.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009490",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Entity linking",
      "Knowledge base",
      "Linguistics",
      "Natural language processing",
      "Organic chemistry",
      "Philosophy",
      "Shot (pellet)",
      "Zero (linguistics)"
    ],
    "authors": [
      {
        "surname": "Barrena",
        "given_name": "Ander"
      },
      {
        "surname": "Soroa",
        "given_name": "Aitor"
      },
      {
        "surname": "Agirre",
        "given_name": "Eneko"
      }
    ]
  },
  {
    "title": "A probabilistic graph-based method to solve precision-diversity dilemma in recommender systems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115485",
    "abstract": "Over the last few years, with significant growth of information on the web, users are swamped with a huge amount of information. Recommender system (RS) is an information retrieval technology that aims to provide relevant items to users by considering their preferences. Although several studies in the literature have proposed new RSs to improve the precision of recommendations, some studies have shown that increasing the diversity of recommendations is one way to increase the quality of the user’s experience and to solve the over-fitting problem. While some previous works have tried to make a trade-off between precision and diversity, they could not provide a linear and deterministic trade-off between precision and diversity. As such, these methods are either impractical or infeasible in a real RS. In this paper, we propose a new method to recommend items which are fit to users’ preferences. The method consists of following steps: i) finding users’ preferences using a limited user study, ii) proposing a probabilistic graph-based recommender system (PGBRS) to recommend items with a desirable level of precision and diversity by considering items that are likely to be preferred by the user in the future. PGBRS is a dataset-independent method which is significantly flexible to update with a low implementation complexity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008964",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Dilemma",
      "Diversity (politics)",
      "Geometry",
      "Graph",
      "Information retrieval",
      "Machine learning",
      "Mathematics",
      "Probabilistic logic",
      "RSS",
      "Recommender system",
      "Sociology",
      "Theoretical computer science",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Joorabloo",
        "given_name": "Nima"
      },
      {
        "surname": "Jalili",
        "given_name": "Mahdi"
      },
      {
        "surname": "Ren",
        "given_name": "Yongli"
      }
    ]
  },
  {
    "title": "IoT-based hybrid optimized fuzzy threshold ELM model for localization of elderly persons",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115500",
    "abstract": "Due to the quickly aging population, the number of elderly persons is rapidly increasing, posing significant challenges for monitoring and assisting them in indoor and outdoor settings. Although some techniques are available for the indoor localization of elderly persons, in the coming years, outdoor localization will be an essential part of society. Different approaches such as GPS, range-based, and range-free have been developed for outdoor localization. However, the localization accuracy and precision is still a significant challenge. For accurate and low-cost localization, we propose a novel IoT-based range-based localization for smart city applications. Using the extreme learning machine (ELM), fuzzy system, and modified swarm intelligence, a hybrid optimized fuzzy threshold ELM (HOFTELM) algorithm is developed. The particle swarm grey wolf optimization is used to identify the direction of the moving sensor node. A fuzzy weighted centroid is used to optimize the consequences of irregular movement of the nodes. Lastly, an optimized threshold extreme learning machine and weighted mean are applied to localize the moving nodes accurately. Our algorithm outperforms the existing algorithms in terms of average location error ratio (ALER), the number of localized nodes, and the computational time. The results show that ALER reduces by at least 48.07% in comparison with the other algorithms. The proposed algorithm also localizes at least 7.25% additional nodes and has a computationally efficient operation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009106",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Fuzzy logic",
      "Machine learning"
    ],
    "authors": [
      {
        "surname": "Ghorpade",
        "given_name": "Sheetal N."
      },
      {
        "surname": "Zennaro",
        "given_name": "Marco"
      },
      {
        "surname": "Chaudhari",
        "given_name": "Bharat S."
      }
    ]
  },
  {
    "title": "A deep neural network approach to QRS detection using autoencoders",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115528",
    "abstract": "Objective: In this paper, a stacked autoencoder deep neural network is proposed to extract the QRS complex from raw ECG signals without any conventional feature extraction phase. Methods: A simple architecture has been deeply trained on many datasets to ensure the generalization of the network at inference. Results: The proposed method achieved a QRS detection accuracy of 99.6% using more than 1042000 beats which is competitive with all state-of-the-art QRS detectors. Moreover, the proposed method produced only 0.82% of Detection Error Rate using six unseen datasets containing more than 1470000 beats. Thus confirms the high performance of our method to detect QRSs. Conclusion: Stacked autoencoder neural networks are very effective in QRS detection. At inference, our algorithm processes 1042309 beats in less than 25.32 s. Thus, it is favorably comparable with state-of-the-art deep learning methods. Significance: The stacked autoencoder is an efficient tool for QRS detection, which could replace conventional systems to help practitioners make fast and accurate decisions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009362",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Cardiology",
      "Computer science",
      "Deep learning",
      "Deep neural networks",
      "Machine learning",
      "Medicine",
      "Pattern recognition (psychology)",
      "QRS complex"
    ],
    "authors": [
      {
        "surname": "Belkadi",
        "given_name": "Mohamed Amine"
      },
      {
        "surname": "Daamouche",
        "given_name": "Abdelhamid"
      },
      {
        "surname": "Melgani",
        "given_name": "Farid"
      }
    ]
  },
  {
    "title": "Efficient algorithms for decision making and coverage deployment of connected multi-low-altitude platforms",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115529",
    "abstract": "Unmanned aerial vehicles (UAVs) have gained significantly in popularity in recent years, and they are coupled with the emerging Internet-of-Things, 5G, and mobile edge computing. Their application domains will continue to expand. UAVs can deliver various IoT-driven applications (e.g., video surveillance UAVs-based applications) anytime and anywhere. Indeed, the rapid deployment of such applications in different environments (e.g., urban or wildlife surveillance/sensing environments) is one of the critical factors to support the emergence of new services and applications (e.g., 5G/B5G UAVs base station, civil and commercial IoT applications). However, several research challenges need to be addressed before the latter can be deployed in the real world. In this paper, we propose a novel approach that enables efficient coverage using a set of UAVs. We define two multi-objective sub-problems. The first sub-problem allows selecting a minimum number of appropriate UAVs for deployment as Low-Altitude-Platforms (LAP) over an area of interest from an extensive collection of options. We model this sub-problem as multi-criteria decision-making (MCDM) problem considering various UAV features and user constraints and solve it using a novel multi-criteria selection algorithm named SAGA. The second sub-problem is a coverage optimization problem that enables controlling UAVs’ hover locations and use gimbal-mounted rotating cameras or multi-lens cameras to increase the coverage accordingly. We propose a strategy aiming at optimizing hovering locations of UAVs and then rotating their cameras. The process comprises two techniques to solve the latter sub-problem: (1) an improved preference-guided genetic algorithm named NSPGGA; (2) a hybrid heuristic DCXGA built upon three algorithmic techniques: divide-and-conquer, greedy search, and exhaustive search. We carried out comparative analyses with seven MCDM methods and ten multi-objective evolutionary/swarm intelligence algorithms. The proposed algorithms outperformed the benchmarking techniques and showed remarkable results, e.g., the selection algorithm SAGA exhibits a high success rate, accuracy, and consistency. The preference-guided genetic algorithm NSPGGA achieves better efficiency, and it is four times faster than the famed NSGA-II. Finally, the hybrid heuristic DCXGA allows having more significant imaging coverages with few camera rotations. The aforementioned vital results are validated through diverse and intensive simulation scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009374",
    "keywords": [
      "Algorithm",
      "Base station",
      "Computer network",
      "Computer science",
      "Operating system",
      "Process (computing)",
      "Programming language",
      "Real-time computing",
      "Set (abstract data type)",
      "Software deployment"
    ],
    "authors": [
      {
        "surname": "Mohamadi",
        "given_name": "Houssem Eddine"
      },
      {
        "surname": "Kara",
        "given_name": "Nadjia"
      },
      {
        "surname": "Lagha",
        "given_name": "Mohand"
      }
    ]
  },
  {
    "title": "An automatic retinal vessel segmentation approach based on Convolutional Neural Networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115459",
    "abstract": "The image segmentation concept is a mechanism of image processing that allows cutting an image into several image sections or parts. Numerous techniques are applied to achieve image segmentation, such as color, pixel value, deep Convolutional Neural Networks, and others. We are interested in Convolutional Neural Network (CNN) based approaches. In this paper, we present an automatic method for blood vessel segmentation in retina images; we propose the use of deep CNNs to achieve this goal. Our proposed model is a multi-encoder decoder architecture composed of two encoder units with convolutional and maxpoling layers and a decoder unit with convolutional and deconvolutional layers; it takes an RGB retina image directly as input. The model, then, is trained to build feature maps from convolution, deconvolution operations and nonlinear activation function in order to reconstruct the feature map and ultimately obtain the output layer as segmentation of retina vessels. Structured retina analysis (STARE) and Digital images of the retina for vessel extraction (DRIVE) are used as datasets for training and evaluation. We implement the model with keras in python. An overview of the model architecture, training with data augmentation techniques, validation and prediction is presented. The results are compared to existing labeled data; and the performance of our model with different metrics like F1 score, accuracy, sensitivity, specificity and precision is evaluated. The results obtained for the different metrics listed above are respectively 0.8321, 0.9716, 0.8214, 0.9860 and 0.8466. In addition, the results of our work are submitted into Drive Challenge in order to get an external evaluation based on dice coefficient calculation with the dice-mean score being 0.8283. The results show that our method is, to a great extent, much better than other CNN-based methods, with a precision of 0.9707 for DRIVE datasets versus 0.9568 and 0.9469 for the others. The results obtained from our proposed model indicate that our method has a promising potential for practical applications like computer analysis of retinal images such as automated screening for precocious detection of diabetic retinopathy. These results are due to the model’s segmentation performance, and, in particular, its enhanced specificity, accuracy and precision.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008721",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Machine learning",
      "Medicine",
      "Ophthalmology",
      "Pattern recognition (psychology)",
      "Retinal",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Chala",
        "given_name": "Mohamed"
      },
      {
        "surname": "Nsiri",
        "given_name": "Benayad"
      },
      {
        "surname": "El yousfi Alaoui",
        "given_name": "My Hachem"
      },
      {
        "surname": "Soulaymani",
        "given_name": "Abdelmajid"
      },
      {
        "surname": "Mokhtari",
        "given_name": "Abdelrhani"
      },
      {
        "surname": "Benaji",
        "given_name": "Brahim"
      }
    ]
  },
  {
    "title": "Gaussian process regression-based learning rate optimization in convolutional neural networks for medical images classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115357",
    "abstract": "This research proposes a series of novel learning rate optimization algorithms with two versions for Adaptive Moment Estimation (Adam), which is a common optimizer in Convolutional Neural Networks (CNNs). Optimizer that is used to control the training efficiency and prediction accuracy by controlling the convergence progress plays an important role in CNNs. However, optimizers such as Adam are usually not hyperparameter-free and very sensitive to the hyperparameters embedded in CNNs. For example, the learning rate is a hyperparameter that represents a step size in the calculations. The learning rate has the most significant influence on prediction accuracy, so optimizing the learning rate is the best way to improve accuracy. In this research, a series of Gaussian Process Regression (GPR)-based learning rate optimization (GLRO) algorithms are proposed to increase the classification accuracy. To be specific, the relationship between the learning rate and corresponding accuracy is studied and the potential learning rate is predicted by the GPR model which is built with previous learning rates and corresponding accuracies. Also, two strategies of the algorithm to select the input learning rate are tested separately. AlexNet, which is a state-of-the-art CNN, is used as a framework to evaluate the proposed algorithms. AlexNet is widely used in the healthcare system as medical imaging classification framework. The Stimulated Raman Scattering (SRS) images of human brain tumors are used to classify cells and non-cells in this research. The proposed GLRO are compared to the conventional learning rate annealing algorithm and the constant learning rate algorithm. The algorithms’ classifications of SRS images are evaluated in terms of accuracy, sensitivity, specificity, and precision. To further validate GLRO, multiple benchmark medical images and CNN frameworks are tested. The experimental results illustrate that the proposed GLRO algorithms outperform other algorithms by showing a 96% classification accuracy on SRS images and achieve promising classification results on the other datasets and CNN frameworks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007855",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Convolutional neural network",
      "Deep learning",
      "Gaussian",
      "Gaussian process",
      "Hyperparameter",
      "Key (lock)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Rate of convergence"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yuanyuan"
      },
      {
        "surname": "Zhang",
        "given_name": "Qianqian"
      },
      {
        "surname": "Yoon",
        "given_name": "Sang Won"
      }
    ]
  },
  {
    "title": "Multimodal sentiment and emotion recognition in hyperbolic space",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115507",
    "abstract": "Prior approaches for multimodal sentiment and emotion recognition (SER) exploit input data representations and neural networks based on the classical Euclidean geometry. Recently, however, the hyperbolic metric proved to be a powerful tool for data mapping, being able to capture the hierarchical structure of the relations among elements in the data. In this paper we propose the use of hyperbolic learning for SER, and show that the inclusion in the neural network of hyperbolic structures mapping the input into the hyperbolic space can improve the quality of the predictions. The benefits brought by the hyperbolic features are evaluated by developing extensions of existing methods following two approaches. From one side, we modified state-of-the-art models by including hyperbolic output layers. From the other, we generated hybrid neural network architectures by combining hyperbolic and Euclidean layers according to different schemes. The proposed hyperbolic models were tested on several classification tasks applied to benchmark multimodal SER datasets. Experiments gave strong evidence that in both simple and complex networks the introduction of a hyperbolic structure results in an improvement of the model accuracy. Specifically, the combined use of hyperbolic and Euclidean layers showed superior performance in almost all the classification tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009179",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Economics",
      "Euclidean distance",
      "Euclidean geometry",
      "Euclidean space",
      "Geodesy",
      "Geography",
      "Geometry",
      "Hyperbolic space",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Araño",
        "given_name": "Keith April"
      },
      {
        "surname": "Orsenigo",
        "given_name": "Carlotta"
      },
      {
        "surname": "Soto",
        "given_name": "Mauricio"
      },
      {
        "surname": "Vercellis",
        "given_name": "Carlo"
      }
    ]
  },
  {
    "title": "On the relevance of the metadata used in the semantic segmentation of indoor image spaces",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115486",
    "abstract": "The study of artificial learning processes in the area of computer vision context has mainly focused on achieving a fixed output target rather than on identifying the underlying processes as a means to develop solutions capable of performing as good as or better than the human brain. This work reviews the well-known segmentation efforts in computer vision. However, our primary focus is on the quantitative evaluation of the amount of contextual information provided to the neural network. In particular, the information used to mimic the tacit information that a human is capable of using, like a sense of unambiguous order and the capability of improving its estimation by complementing already learned information. Our results show that, after a set of pre and post-processing methods applied to both the training data and the neural network architecture, the predictions made were drastically closer to the expected output in comparison to the cases where no contextual additions were provided. Our results provide evidence that learning systems strongly rely on contextual information for the identification task process.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008976",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Image (mathematics)",
      "Image retrieval",
      "Information retrieval",
      "Law",
      "Metadata",
      "Political science",
      "Relevance (law)",
      "Segmentation",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Vasquez-Espinoza",
        "given_name": "Luis"
      },
      {
        "surname": "Castillo-Cara",
        "given_name": "Manuel"
      },
      {
        "surname": "Orozco-Barbosa",
        "given_name": "Luis"
      }
    ]
  },
  {
    "title": "Short-term prediction of fishing effort distributions by discovering fishing chronology among trawlers based on VMS dataset",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115512",
    "abstract": "Short-term prediction of fishing effort distributions will guide fishery management in a dynamic way. However, it meets two unique challenges: the randomness of fishers’ behaviors and the diversity of marine meteorology such as sea storms in the short period. This study proposes short-term prediction system of fishing effort distribution by mining a new kind of knowledge: fishing chronology among trawlers. We first define, quantify, and dig out chronological fishing relations among trawlers based on the VMS dataset. Then the system extracts the optimal early bird set from chronological fishing relations, whose current fishing behaviors can serve as indicators of future fishing effort distributions. Based on the knowledge of fishing chronology, we further design a Convolution Neural Network (CNN) to predict the short-term fishing effort distribution, only taking the current fishing behaviors of early birds as input. We evaluate the system performance on the VMS dataset of 1589 trawlers in the East China Sea from October 2015 to April 2017. The system uses the VMS traces in the first half period to calculate fishing chronology among trawlers, to extract early birds, and to train the CNN model. The traces in the last half period is used to evaluate the prediction accuracy. The results confirm a low prediction error ratio of 6.95% across all the weeks only by tracking 19 early birds. More importantly, our prediction system keeps its accuracy during the week of a sea storm in Feb. 8 th to 10 th , 2017. The application of our system for fishery management is encouraging: tracking only 1% trawlers suffices to predict short-term fishing effort distributions in the near future.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009222",
    "keywords": [
      "Archaeology",
      "Artificial neural network",
      "Biology",
      "Chronology",
      "Computer science",
      "Convolutional neural network",
      "Fishery",
      "Fishing",
      "Geography",
      "Machine learning",
      "Physics",
      "Quantum mechanics",
      "Term (time)"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Zhongning"
      },
      {
        "surname": "Hong",
        "given_name": "Feng"
      },
      {
        "surname": "Huang",
        "given_name": "Haiguang"
      },
      {
        "surname": "Liu",
        "given_name": "Chao"
      },
      {
        "surname": "Feng",
        "given_name": "Yuan"
      },
      {
        "surname": "Guo",
        "given_name": "Zhongwen"
      }
    ]
  },
  {
    "title": "A novel hybrid differential evolution and symbiotic organisms search algorithm for size and shape optimization of truss structures under multiple frequency constraints",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115534",
    "abstract": "Although a large number of metaheuristic algorithms and their variants have been proposed for many engineering optimization problems, no paradigms hybridized by differential evolution (DE) and symbiotic organisms search (SOS) to concurrently improve the optimal solution quality and the convergence speed have been published thus far, especially for size and shape optimization of truss structures with multiple frequency constraints. Therefore, this article aims to propose a novel optimization algorithm as a cross-breed of the DE and the SOS, named HDS, for such problems. This algorithm can simultaneously and effectively enhance both global and local searching abilities by utilizing newly developed operators hybridized from the DE and SOS. An automatically adapted parameter is suggested for a better trade-off between those two capabilities. Furthermore, an elitist scheme is used in the selection phase to extract the best solutions for the next generation. As a consequence, the proposed methodology results in high-quality optimal solutions with a lower computational effort in comparison with two original methods, even many other optimization paradigms available in the literature. 26 benchmark mathematical functions are examined first. 5 numerical examples of shape and size optimization of truss structures are then investigated to validate the feasibility of the current paradigm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009416",
    "keywords": [
      "Algorithm",
      "Benchmark (surveying)",
      "Computer science",
      "Continuous optimization",
      "Convergence (economics)",
      "Differential evolution",
      "Economic growth",
      "Economics",
      "Engineering",
      "Geodesy",
      "Geography",
      "Global optimization",
      "Local search (optimization)",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Multi-swarm optimization",
      "Optimization problem",
      "Structural engineering",
      "Truss"
    ],
    "authors": [
      {
        "surname": "Nguyen-Van",
        "given_name": "Sy"
      },
      {
        "surname": "Nguyen",
        "given_name": "Khoa T."
      },
      {
        "surname": "Luong",
        "given_name": "Van Hai"
      },
      {
        "surname": "Lee",
        "given_name": "Seunghye"
      },
      {
        "surname": "Lieu",
        "given_name": "Qui X."
      }
    ]
  },
  {
    "title": "Content-based group recommender systems: A general taxonomy and further improvements",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115444",
    "abstract": "Group recommender systems have emerged as a solution to recommend interesting, suitable, and useful items that are consumed socially by groups of people, rather than individually. Such systems have pushed for the use of new recommendation methods within such an emerging scenario, in which the use of the collaborative filtering paradigm is the core of the recommender algorithm. However, collaborative filtering presents several drawbacks and limitations in this scenario, such as the need for lots of rating values, as well as their co-occurrence across several items and users (scarcity). In order to overcome these drawbacks, this research explores a taxonomy for content-based group recommendation systems (CB-GRS), and subsequently the paper discusses and analyzes three specific models that can be used to build CB-GRS, which are (1) CB-GRSs supported by recommendation aggregation and individual ranking, (2) CB-GRSs supported by recommendation aggregation and user-item matching, and (3) CB-GRSs supported by the aggregation of user profiles. Furthermore, the paper presents a hybrid CB-GRS that combines the models (2) and (3) and integrates feature weighting and aggregation function switching. An experimental protocol over well-known datasets is then developed in order to evaluate the proposals. The current study aims at providing a basis to develop a research branch concerning content-based group recommender systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008587",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Group (periodic table)",
      "Information retrieval",
      "Organic chemistry",
      "Recommender system",
      "Taxonomy (biology)",
      "World Wide Web",
      "Zoology"
    ],
    "authors": [
      {
        "surname": "Pérez-Almaguer",
        "given_name": "Yilena"
      },
      {
        "surname": "Yera",
        "given_name": "Raciel"
      },
      {
        "surname": "Alzahrani",
        "given_name": "Ahmad A."
      },
      {
        "surname": "Martínez",
        "given_name": "Luis"
      }
    ]
  },
  {
    "title": "Time-series interval prediction under uncertainty using modified double multiplicative neuron network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115478",
    "abstract": "This paper presents a hybrid intelligent approach for constructing prediction intervals (PIs) of terrain profiles over time under uncertainty. It utilizes the double multiplicative neuron (DMN) model and the modified particle swarm optimization (MPSO) algorithm to calculate the upper and lower bounds of unknown elevations ahead on terrain profiles based on the vehicles’ track. MPSO withholds particles generating the positive PIs in the training epochs, in order to prevent the occurrence of unreasonable upside-down PIs that are brought by conventional methods. MPSO adjusts the parameters of the DMN model iteratively by minimizing the value of the proposed cost function. The fitness function aims to enhance DMN’s capability of forecasting terrain trends by integrating a trend indicator with PIs coverage probability and interval widths. This study utilizes the terrain profiles of 3 arc-seconds resolution to verify the effectiveness of the proposed MPSO-DMN T approach for one-step and multi-step PIs estimation. Experimental results demonstrate that the proposed approach (1) overcomes the limitations of the conventional PIs indicators; (2) improves the prediction accuracy for terrain trends by 18.8% in the training data and 15.4% in the testing data, and reduces the computational burden by 31.6% in the training data and 8% in the testing data over the lower upper bound estimation (LUBE) method; (3) achieves comparative coverage probability and interval widths to LUBE using a low-complexity single-layered network. The proposed hybrid approach can be used as an auxiliary decision-making tool for terrain avoidance and terrain following in flight.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008897",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Ecology",
      "Evolutionary biology",
      "Function (biology)",
      "Interval (graph theory)",
      "Mathematics",
      "Particle swarm optimization",
      "Terrain"
    ],
    "authors": [
      {
        "surname": "Pan",
        "given_name": "Wenping"
      },
      {
        "surname": "Feng",
        "given_name": "Liuyang"
      },
      {
        "surname": "Zhang",
        "given_name": "Limao"
      },
      {
        "surname": "Cai",
        "given_name": "Liang"
      },
      {
        "surname": "Shen",
        "given_name": "Chunlin"
      }
    ]
  },
  {
    "title": "Person-dependent seizure detection using statistical CUSUM detector: Preliminary results",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115551",
    "abstract": "Epilepsy is a chronic brain disorder, and for at least one- third of epilepsy patients, medications do not adequately control seizures and surgery is the only potential cure. An automated seizure detector that requires a short period of normal EEG would shorten the seizure monitoring durations, decrease the need for manual assessment of large amount of recorded EEG data, and accordingly assist the neurologists focus on improving quality of care. In this work, we propose a novel approach based on the extended cumulative sum test to detect seizures using intracranial EEG. Different than the existing machine learning approaches, this method requires only a short period of normal EEG for training. In addition to a new proposed feature based on partial directed coherence and random graph theory, in this work, previously developed features used to characterize seizures are used as features in cumulative sum test for seizure detection. A total of 33 intracranial EEG recordings collected from a total of 9 patients corresponding to three different datasets have been used for the analysis purposes: (i) the first dataset includes 11 recorded EEG files (~12.5 min) from 4 patients; (ii) the second dataset includes 20 recorded EEG files (~30 min) from 4 patients; and (iii) two 24-hour long EEG files from a single patient. The proposed detector achieved a mean sensitivity and mean specificity for the first (0.77, 0.86), second (0.88, 0.9) and third datasets (0.92, 0.94) respectively. Statistical detection of seizures has shown success in detecting seizures without the need for highly customizable parameters or previously labelled EEG data. The proposed method could be used in real-time at hospital settings with a minimum requirement of training data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100957X",
    "keywords": [
      "Artificial intelligence",
      "CUSUM",
      "Computer science",
      "Detector",
      "Electroencephalography",
      "Epilepsy",
      "Mathematics",
      "Medicine",
      "Pattern recognition (psychology)",
      "Psychiatry",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Eldeeb",
        "given_name": "Safaa"
      },
      {
        "surname": "Sybeldon",
        "given_name": "Matthew"
      },
      {
        "surname": "Susam",
        "given_name": "Busra"
      },
      {
        "surname": "Akcakaya",
        "given_name": "Murat"
      },
      {
        "surname": "Wozny",
        "given_name": "Thomas"
      },
      {
        "surname": "Pan",
        "given_name": "Jullie"
      },
      {
        "surname": "Mark Richardson",
        "given_name": "Robert"
      },
      {
        "surname": "Bagic",
        "given_name": "Anto"
      },
      {
        "surname": "Antony",
        "given_name": "Arun"
      }
    ]
  },
  {
    "title": "A deep learning-based time series model with missing value handling techniques to predict various types of liquid cargo traffic",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115532",
    "abstract": "The authors propose a time series model that predicts future values of various types of liquid cargo traffic based on long short-term memory (LSTM), a deep learning technique. Existing liquid cargo traffic prediction models are based on traditional time series models, such as autoregressive integrated moving average (ARIMA) and vector autoregression (VAR). Some of these models, which do not consider linear dependencies among the values of different types of liquid cargo traffic, have limitations on their prediction performance, because the values of different types of liquid cargo traffic are dependent on one another. These models’ prediction performance are also limited due to the problem of vanishing gradients, which hinders the learning of long-range time series records. Missing values that exist on real-world liquid cargo traffic records reduce prediction performance as well. The proposed LSTM-based time series model handles missing values on liquid cargo traffic records and predicts future values of liquid cargo traffic. In addition, additional indices, such as inflation rates, exchange rates for dollars, GDP values, and the international prices of oil, are used to improve prediction performance. A case study involving real-world liquid cargo traffic records at the Port of Ulsan, Republic of Korea, for 216 months is used to validate prediction performance of the proposed LSTM-based prediction model compared with traditional ARIMA-based and VAR-based prediction models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009404",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Engineering",
      "Industrial engineering",
      "Machine learning",
      "Missing data",
      "Paleontology",
      "Series (stratigraphy)",
      "Time series",
      "Value (mathematics)"
    ],
    "authors": [
      {
        "surname": "Lim",
        "given_name": "Sunghoon"
      },
      {
        "surname": "Kim",
        "given_name": "Sun Jun"
      },
      {
        "surname": "Park",
        "given_name": "YoungJae"
      },
      {
        "surname": "Kwon",
        "given_name": "Nahyun"
      }
    ]
  },
  {
    "title": "Artificial neural networks for classifying the time series sensor data generated by medical detection dogs",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115564",
    "abstract": "The aim of this research was to discover if artificial neural networks can be used to classify pressure sensor data generated by medical detection dogs as they sniff biological samples. A detection dog can be trained to recognise the odour emitted by one of a wide range of diseases such as prostate cancer, malaria or, potentially, COVID-19. The dog searches a row of sample pots and indicates a positive sample by sitting in front of it. This offers a non-invasive means of diagnosing the specific cancer or disease that the dog has been trained to recognise. For this study, pressure sensors were attached to the sample pots to generate time series data pertaining to the dog’s searching behaviour as they press their nose against the sample pot to sniff its content. Automatic classification could provide a second form of indication, to support or refute the dog’s explicit signal (to sit at a positive sample), which is not always correct. Ultimately, classification software could eliminate the need for the dog to perform an indication gesture, making the dog’s task easier and training quicker. Four different neural network architectures were evaluated: multilayer perceptron (MLP), a convolutional neural network (CNN), a fully convolutional network (FCN) and ResNet (a deep convolutional neural network). Each model was trained to classify the pressure data generated by medical detection dogs. To achieve a useful level of accuracy, it was found that the models needed to be trained using only those data samples where the dog had correctly classified the scent sample. Model hyperparameters were tuned to improve accuracy. We found that the best performing model was MLP. When tested on previously unseen data, where the dog was not always correct, the classification performance of the MLP approached that of the medical detection dogs. For our particular dataset, the model’s true positive rate (i.e. recall) was 59%, matching that of the dogs. The model’s true negative rate was 79%, compared to the dogs’ 91%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009702",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Electronic nose",
      "Machine learning",
      "Multilayer perceptron",
      "Pattern recognition (psychology)",
      "Sample (material)"
    ],
    "authors": [
      {
        "surname": "Withington",
        "given_name": "Lucy"
      },
      {
        "surname": "Diaz Pardo de Vera",
        "given_name": "David"
      },
      {
        "surname": "Guest",
        "given_name": "Claire"
      },
      {
        "surname": "Mancini",
        "given_name": "Clara"
      },
      {
        "surname": "Piwek",
        "given_name": "Paul"
      }
    ]
  },
  {
    "title": "Involvement of controllable lead time and variable demand for a smart manufacturing system under a supply chain management",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115464",
    "abstract": "The concept of controllable lead time and variance is critical issues for the smart supply chain management. This study concerns about variable lead time and variance under controllable production rate and advertise-dependent demand. Managers of any supply chain always improve their performance by reducing lead time and its variance. This paper explores and quantifies these benefits of such lead time reduction for commonly used lot size quantity, production rate, safety factor, reorder point, advertisement cost, vendor’s setup cost. Instead of expected total cost equations, this study provides an exact total cost equation built on an inherent relationship between on-hand inventory and backorder. The marginal value analysis on lead time and its variance achieve more accurate results. The analytical results show that the total supply chain cost is a convex function of both lead time and variance. In other words, the cost savings on both lead time and its variance reduction decrease when lead time becomes larger. Two continuous investments are implied to reduce setup costs and improve the reliability of the production process. The expected backorder and inventory for the buyer uniformly distributed throughout reorder point. Moreover, a smart production process is developed under stochastic demand and flexible production rates. The global optimality of the cost function and decision variables are validated through classical optimization. The numerical examples confirm analytical results and sensitivity analysis is provided for different parameters. Some special cases along with graphical representations are given to validate the model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008769",
    "keywords": [
      "Business",
      "Computer science",
      "Geology",
      "Geomorphology",
      "Industrial organization",
      "Lead (geology)",
      "Lead time",
      "Marketing",
      "Mathematical analysis",
      "Mathematics",
      "Risk analysis (engineering)",
      "Service management",
      "Supply chain",
      "Supply chain management",
      "Variable (mathematics)"
    ],
    "authors": [
      {
        "surname": "Dey",
        "given_name": "Bikash Koli"
      },
      {
        "surname": "Bhuniya",
        "given_name": "Shaktipada"
      },
      {
        "surname": "Sarkar",
        "given_name": "Biswajit"
      }
    ]
  },
  {
    "title": "A diagnostic framework for imbalanced classification in business process predictive monitoring",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115536",
    "abstract": "One of the use cases of business process predictive monitoring is predicting the next activity in a running case, which results in a multi-class classification problem. Approaches to this use case are usually evaluated considering average performance across all classes. This often masks poor performance on minority classes, particularly when classes to be predicted are imbalanced. This is the natural case in next activity prediction, where exceptions or optional activities occur, by design, less frequently than others. In this paper we propose a framework to diagnose poor predictive performance on the minority class in the next activity prediction use case that comprises two tools: an empirical comparison of different resampling techniques in the data preparation phase and a novel classification performance measure. The proposed performance measure aims at highlighting the poor recall on the minority class of a classifier, which is a particularly important performance in the context of next activity prediction, whereas the benchmark helps understanding which resampling technique would be the best at mitigating the poor recall. We also discuss how the two tools of the proposed framework can be combined from an AutoML perspective. The proposed framework has been evaluated on a set of publicly available event logs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100943X",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Machine learning",
      "Operating system",
      "Paleontology",
      "Philosophy",
      "Process (computing)",
      "Recall",
      "Resampling"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Jongchan"
      },
      {
        "surname": "Comuzzi",
        "given_name": "Marco"
      }
    ]
  },
  {
    "title": "On the use of summarization and transformer architectures for profiling résumés",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115521",
    "abstract": "Profiling professional figures is becoming more and more crucial, as companies and recruiters face the challenges of Industry 4.0. On the one hand, demand for specific knowledge in professional figures is rising. On the other hand, workers try to broaden the spectrum of their skills in order to remain appealing in the job market. Therefore, research related to these topics is receiving more and more attention. In this paper, we propose a methodology to profile résumés based on summarization and transformer architectures for generating résumé embeddings and on hierarchical clustering algorithms for grouping these embeddings. We evaluate different strategies and show that our approach achieves promising results on a public domain dataset containing 1202 résumés.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009301",
    "keywords": [
      "Artificial intelligence",
      "Automatic summarization",
      "Computer science",
      "Data mining",
      "Electrical engineering",
      "Engineering",
      "Profiling (computer programming)",
      "Programming language",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Bondielli",
        "given_name": "Alessandro"
      },
      {
        "surname": "Marcelloni",
        "given_name": "Francesco"
      }
    ]
  },
  {
    "title": "CEPchain: A graphical model-driven solution for integrating complex event processing and blockchain",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115578",
    "abstract": "Blockchain provides an immutable distributed ledger for storing transactions. One of the challenges of blockchain is the particular processing of dynamic queries due to accumulating costs. Complex Event Processing (CEP) provides efficient and effective support for this in a way, however, that is difficult to integrate with blockchain. This paper addresses the research challenges of integrating blockchain with CEP. More specifically, we envision an effective development environment in which (i) event-driven smart contracts are modeled in a graphical way, which are, in turn, (ii) automatically transformed into complementary code that is deployed in both a CEP engine and a blockchain network, and then (iii) executed on off-chain CEP applications which, connected to different data sources and sinks, automatically invoke smart contracts when event pattern conditions are met. We follow a classic systems engineering approach for defining the concepts of our system, called CEPchain, which addresses the described requirements. CEPchain was evaluated using a real-world case study for vaccine delivery, which requires an unbroken cold chain. The results demonstrate that our approach can be applied without requiring experts on event processing and smart contract languages. Our contribution simplifies the design of integrated CEP and blockchain functionality by hiding implementation details and supporting efficient deployment.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009805",
    "keywords": [
      "Blockchain",
      "Code (set theory)",
      "Complex event processing",
      "Computer science",
      "Computer security",
      "Database",
      "Distributed computing",
      "Event (particle physics)",
      "Physics",
      "Process (computing)",
      "Programming language",
      "Quantum mechanics",
      "Set (abstract data type)",
      "Smart contract",
      "Software deployment",
      "Software engineering"
    ],
    "authors": [
      {
        "surname": "Boubeta-Puig",
        "given_name": "Juan"
      },
      {
        "surname": "Rosa-Bilbao",
        "given_name": "Jesús"
      },
      {
        "surname": "Mendling",
        "given_name": "Jan"
      }
    ]
  },
  {
    "title": "An improved ensemble learning method for exchange rate forecasting based on complementary effect of shallow and deep features",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115569",
    "abstract": "In prior studies, either the shallow feature or deep feature has been extracted for accurate exchange rate forecasting. However, the complementary effect and distinguished predictive power of multiple features have rarely been investigated, which limits the utilization of comprehensive predictive information. Therefore, a novel ensemble learning method, Adaptive Linear Sparse Random Subspace (ALS-RS), is proposed based on the complementary effect of shallow and deep features. Concretely, in the first stage, the shallow feature is constructed manually combined with expert knowledge and the deep feature is extracted automatically by Bidirectional Gated Recurrent Units (Bi-GRU), then the features obtained are used as model inputs. After that, the improved RS with a feature weighting mechanism is designed to discriminate the importance of each feature and make an accurate ensemble prediction. The experimental results on four exchange rate datasets validate the superiority of our proposed ALS-RS. Besides, the enhanced forecasting capability of fusing multiple features including shallow and deep features is confirmed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100974X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Ensemble learning",
      "Feature (linguistics)",
      "Linguistics",
      "Machine learning",
      "Medicine",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Radiology",
      "Subspace topology",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Gang"
      },
      {
        "surname": "Tao",
        "given_name": "Tao"
      },
      {
        "surname": "Ma",
        "given_name": "Jingling"
      },
      {
        "surname": "Li",
        "given_name": "Hui"
      },
      {
        "surname": "Fu",
        "given_name": "Huimin"
      },
      {
        "surname": "Chu",
        "given_name": "Yan"
      }
    ]
  },
  {
    "title": "Coarse-refinement dilemma: On generalization bounds for data clustering",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115399",
    "abstract": "The data clustering problem is of central importance for the area of machine learning, given its usefulness to represent data structural similarities from input spaces. Although, data clustering counts on scarse literature of a theoretical framework with generalization guarantees. In this context, this manuscript introduces a new concept, based on multidimensional persistent homology, to analyze the conditions on which a clustering model is capable of generalizing data. As a first step, we propose a more general definition of DC problem by relying on topological spaces, instead of metric ones as typically approached in the literature. From that, we show that the data clustering problem presents an analogous dilemma to the bias-variance one, which is here referred to as the coarse-refinement dilemma, from which we conclude that: (i) highly-refined partitions and the clustering instability (overfitting); and (ii) over-coarse partitions and the lack of representativeness (underfitting). The coarse-refinement dilemma suggests the need of a relaxation of Kleinberg’s richness axiom, as such axiom allows the production of unstable or unrepresentative partitions. Experimental exploration considering different clustering refinements can, then, depict such partitions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008228",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Dilemma",
      "Generalization",
      "Geometry",
      "Mathematical analysis",
      "Mathematics"
    ],
    "authors": [
      {
        "surname": "Vaz",
        "given_name": "Yule"
      },
      {
        "surname": "de Mello",
        "given_name": "Rodrigo Fernandes"
      },
      {
        "surname": "Grossi Ferreira",
        "given_name": "Carlos Henrique"
      }
    ]
  },
  {
    "title": "Applications of deep learning in stock market prediction: Recent progress",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115537",
    "abstract": "Stock market prediction has been a classical yet challenging problem, with the attention from both economists and computer scientists. With the purpose of building an effective prediction model, both linear and machine learning tools have been explored for the past couple of decades. Lately, deep learning models have been introduced as new frontiers for this topic and the rapid development is too fast to catch up. Hence, our motivation for this survey is to give a latest review of recent works on deep learning models for stock market prediction. We not only category the different data sources, various neural network structures, and common used evaluation metrics, but also the implementation and reproducibility. Our goal is to help the interested researchers to synchronize with the latest progress and also help them to easily reproduce the previous studies as baselines. Based on the summary, we also highlight some future research directions in this topic.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009441",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Data science",
      "Deep learning",
      "Deep neural networks",
      "Engineering",
      "Horse",
      "Machine learning",
      "Mechanical engineering",
      "Paleontology",
      "Stock (firearms)",
      "Stock market",
      "Stock market prediction"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Weiwei"
      }
    ]
  },
  {
    "title": "Smart, comfortable wearable system for recognizing Arabic Sign Language in real-time using IMUs and features-based fusion",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115448",
    "abstract": "Automatic sign language recognition systems have been presented in the literature to avail as a helpful device for deaf and mute individuals to incorporate them into the community without impediments. Gesture recognition is usually done by using vision-based techniques or hand-attached sensors that track hand and finger movements. This paper presents a sensors-based Arabic Sign Language recognition system. The proposed system depends on the information fusion from a 3-axis accelerometer and gyroscope Inertial Measurement Units (IMUs) sensors. The architecture of the proposed system consists of two main modules: a wearable device (glove), which includes sensors, a communication unit, and micro-controller unit, and a software module running on a mobile device. The proposed glove system utilizes six IMUs sensors where five of them placed at the fingers and the sixth sensor placed at the back of the palm. The collected data from sensors are transmitted to a mobile device through a Bluetooth low-energy wireless transmission for data analysis and gesture recognition. For each gesture, the beginning and endpoints of the significative gesture part are discovered automatically using the intensity of the gyroscope signal. The system employs a features-based sensor fusion that combines accelerometer and gyroscope time-domain and frequency-domain features. In a comprehensive set of experiments with seven participants, the system was trained and tested with 28 Arabic alphabets, and the best recognition accuracy achieved was 98.6% for the user-dependent data and 96% for the user-independent data under support vector machine classifier. The achieved results are outperforming existing systems that have been presented in recent decades for alphabet-level gesture recognition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008629",
    "keywords": [
      "Arabic",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Embedded system",
      "Fusion",
      "Human–computer interaction",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Philosophy",
      "Sign (mathematics)",
      "Sign language",
      "Speech recognition",
      "Wearable computer"
    ],
    "authors": [
      {
        "surname": "Qaroush",
        "given_name": "Aziz"
      },
      {
        "surname": "Yassin",
        "given_name": "Sara"
      },
      {
        "surname": "Al-Nubani",
        "given_name": "Ali"
      },
      {
        "surname": "Alqam",
        "given_name": "Ameer"
      }
    ]
  },
  {
    "title": "A simple and fast method for Named Entity context extraction from patents",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115570",
    "abstract": "The process of extracting relevant technical information from patents or technical literature is as valuable as it is challenging. It deals with highly relevant information extraction from a corpus of documents with particular structure, and a mix of technical and legal jargon. Patents are the wider free source of technical information where homogeneous entities can be found. From a technical perspective the approaches refer to Named Entity Recognition (NER) and make use of Machine Learning techniques for Natural Language Processing (NLP). However, due to the large amount of data, to the complexity of the lexicon, the peculiarity of the structure and the scarcity of the examples to be used to feed the machine learning system, new approaches should be studied. NER methods are increasing their performances in many contexts, but a gap still exists when dealing with technical documentation. The aim of this work is to create an automatic training sets for NER systems by exploiting the nature and structure of patents, an open and massive source of technical documentation. In particular, we focus on collecting the context where users of the invention appear within patents. We then measure to which extent we achieve our goal and discuss how much our method is generalizable to other entities and documents.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009751",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Data science",
      "Documentation",
      "Economics",
      "Focus (optics)",
      "Information extraction",
      "Information retrieval",
      "Jargon",
      "Lexicon",
      "Linguistics",
      "Management",
      "Named-entity recognition",
      "Natural language processing",
      "Operating system",
      "Optics",
      "Paleontology",
      "Philosophy",
      "Physics",
      "Process (computing)",
      "Programming language",
      "Task (project management)",
      "Technical documentation"
    ],
    "authors": [
      {
        "surname": "Puccetti",
        "given_name": "Giovanni"
      },
      {
        "surname": "Chiarello",
        "given_name": "Filippo"
      },
      {
        "surname": "Fantoni",
        "given_name": "Gualtiero"
      }
    ]
  },
  {
    "title": "Bi-level programming model and KKT penalty function solution approach for reliable hub location problem",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115505",
    "abstract": "Tactical and operational decisions must nowadays be made in production and distribution systems to allocate the best possible locations for the establishment of service centers. These systems seek to provide their services the fastest and the most reliably. In the meantime, hub location problems are classified as the most important categories of such decisions. These problems include locating hub facilities and establishing communication networks between facilities and demand centers. This paper aims to design a bi-level programming model to minimize the costs of establishing a hub network at the first decision-making level and reduce service loss due to disruption and failure in service processes at the second decision-making level. Therefore, the reliable bi-level hub location problem was analyzed, and an integer programming model was developed. The KKT method was then employed to solve the model, whereas a two-step heuristic method with a penalty function was proposed to first offer a feasible solution through an innovative algorithm. After that, a process was formulated to improve the feasible solution through the penalty function logic. Data of 37 major cities were collected from the Civil Aviation Organization of Islamic Republic of Iran to validate the proposed model. In brief, the developed hub location problem model managed to efficiently solve some real-world distribution problems through a bi-level programming approach. Moreover, the traffic reliability and total location routing cost of the network were incorporated into a mathematical model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009155",
    "keywords": [
      "Aerospace engineering",
      "Algorithm",
      "Artificial intelligence",
      "Aviation",
      "Biology",
      "Computer science",
      "Economics",
      "Economy",
      "Engineering",
      "Evolutionary biology",
      "Function (biology)",
      "Heuristic",
      "Integer programming",
      "Karush–Kuhn–Tucker conditions",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Operations research",
      "Penalty method",
      "Physics",
      "Power (physics)",
      "Process (computing)",
      "Programming language",
      "Programming paradigm",
      "Quantum mechanics",
      "Reliability (semiconductor)",
      "Service (business)",
      "Service level",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Korani",
        "given_name": "Ehsan"
      },
      {
        "surname": "Eydi",
        "given_name": "Alireza"
      }
    ]
  },
  {
    "title": "Adaptive MLP neural network controller for consensus tracking of Multi-Agent systems with application to synchronous generators",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115460",
    "abstract": "In this study a novel cooperative controller design is developed to tackle the consensus tracking problem of Multi-Agent systems (MAS) based on multilayer perceptron neural network (MLPNN), applied on a distributed synchronous generator (SG) Multi-Agent system in presence of model uncertainties and external disturbances. Application of MLPNN in controller design can lead to smoother system response and can neutralize the impacts of model uncertainties. Furthermore, the proposed method benefits from a novel algorithm formerly known as error backpropagation (BP) algorithm to update and to regulate the weights of MLPNN adaptively based on the principles of consensus error. The proposed strategy can be very effective in control of the distributed SG Multi-Agent system due to its ability for system identification, parameter estimation, and disturbance approximation. Moreover, the utilization of neural networks can meet the criterion to make the consensus error uniformly ultimately bounded. Ultimately, simulation results illustrate the applicability and effectiveness of the novel MLPNN controller to model the system uncertainties and to deal with external disturbances of the distributed SG Multi-Agent system.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008733",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Artificial neural network",
      "Backpropagation",
      "Biology",
      "Botany",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Controller (irrigation)",
      "Identification (biology)",
      "Multi-agent system",
      "Multilayer perceptron",
      "Tracking error"
    ],
    "authors": [
      {
        "surname": "Sharifi",
        "given_name": "Alireza"
      },
      {
        "surname": "Sharafian",
        "given_name": "Amin"
      },
      {
        "surname": "Ai",
        "given_name": "Qian"
      }
    ]
  },
  {
    "title": "Social robot motion planning using contextual distances observed from 3D human motion tracking",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115515",
    "abstract": "The social robot motion planning involves robots operating within the workspace and alongside humans and it is thus necessary for the robots to behave socially. The current literatures make trajectory databases to learn the human trajectory, which has a limited application since only very few out of large possibilities can be recoded. Another approach to the problem is by modelling the macroscopic behaviors, which is limited by parameters that are practically hard to set. Our approach to the problem is thus to understand human motion through a set of simple and well-known primitives, which are observed from the actual human motion. In this paper, we first perform human detection and tracking in a 3D environment. We use a stationary 3D Lidar sensor for the detection and tracking of all moving humans. Our approach detects all moving people and it also solves the problem of occlusion in several cases. We further consider several research hypotheses regarding human navigation noting how much distances the humans prefer to maintain with other humans and obstacles under different common scenarios. The experimentation is done with several subjects and their behavior is used to answer the research hypotheses. A Social Robot Motion Planning algorithm is developed by using a social potential field algorithm as a base. New social forces are added to model the different behaviors displayed by humans. The motion planning algorithm makes the robot maintain the same distances as observed with the humans and perturbating the robots makes them reach the equilibrium distance again.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009258",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Human motion",
      "Match moving",
      "Motion (physics)",
      "Motion planning",
      "Pedagogy",
      "Psychology",
      "Robot",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Malviya",
        "given_name": "Abhinav"
      },
      {
        "surname": "Kala",
        "given_name": "Rahul"
      }
    ]
  },
  {
    "title": "Automated classification of lung sound signals based on empirical mode decomposition",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115456",
    "abstract": "Chronic obstructive pulmonary disease (COPD) is a chronic non-reversible lung disease. Other acute respiratory illness due to infections are termed as non-chronic. In general, the pulmonologist carries preliminary screening by accessing lung sounds. In this paper, we propose a methodology to automatically classify lung sounds associated with non-chronic and chronic categories. To accomplish the task, at first, the empirical mode decomposition (EMD) is applied to lung sound signals to obtain intrinsic mode functions (IMFs). Considering the availability of IMFs in entire dataset and by using a hybrid strategy, first four IMFs have been selected for feature extraction purpose. The IMFs are then further processed to construct two-dimensional (2D) and higher-dimensional (HD) phase space representation (PSR). The feature space includes the 95% confidence ellipse area from 2D-PSR and interquartile range (IQR), mean, median, standard deviation, skewness and kurtosis of Euclidian distances computed from HD-PSR. The process is carried out for the first four IMFs corresponding to the non-chronic and chronic categories of the lung sounds. Neighborhood component analysis is used to select best performing features. The computed and selected features depict a significant ability to discriminate the two categories of lung sound signals. To perform classification, we use ensemble classifiers. Simulation outcomes on ICBHI 2017 lung sound dataset show the ability of the proposed method in effectively classifying non-chronic and chronic lung sound signals. Ensemble of Bagged tree provides the highest classification accuracy of 97.14% over feature space constituted by 10D-PSR of fourth IMF.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008691",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Filter (signal processing)",
      "Hilbert–Huang transform",
      "Kurtosis",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Skewness",
      "Speech recognition",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Khan",
        "given_name": "Sibghatullah I."
      },
      {
        "surname": "Pachori",
        "given_name": "Ram Bilas"
      }
    ]
  },
  {
    "title": "Blockchain for consortium: A practical paradigm in agricultural supply chain system",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115425",
    "abstract": "Trading aspect of agricultural supply chain system is sophisticated since it consists of many stages and involves various entities/agencies. Recently, blockchain technology could prove its effectiveness to solve some of the concerns in agricultural supply chain systems. Nevertheless, maximizing profit for producers (in our study farmers) is another important concern that can be addressed by consortium establishment which blockchain technology is the best solution for this purpose due to the following reasons. First, since all the nodes in the blockchain keep the verified and synchronized version of the chain, each node can verify the transactions’ transparency. Second, blockchain technology is temper-proof that means no one can change the history of the transactions. These two main features of blockchain technology can provide a suitable ground to construct a consortium among the producers. However, there are other specific requirements that a successful consortium in agricultural supply chain system should address them that motivate us to a new design of blockchain technology. More precisely, in our design we consider the problems of trustability, scalability, and share amount assignment. For trustability, we utilize cyber physical system to ensure the quantity and quality of the products. Scalability is being addressed by adopting the concept of public service platform and proposing a new consensus algorithm. And finally share amount assignment is being solved by our improved version of ant colony optimization algorithm. Experimental results and analysis prove the effectiveness and accuracy of our proposed design for blockchain technology.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008435",
    "keywords": [
      "Agriculture",
      "Biology",
      "Blockchain",
      "Business",
      "Computer science",
      "Computer security",
      "Ecology",
      "Marketing",
      "Supply chain"
    ],
    "authors": [
      {
        "surname": "Eluubek kyzy",
        "given_name": "Indra"
      },
      {
        "surname": "Song",
        "given_name": "Huaming"
      },
      {
        "surname": "Vajdi",
        "given_name": "Ahmadreza"
      },
      {
        "surname": "Wang",
        "given_name": "Yongli"
      },
      {
        "surname": "Zhou",
        "given_name": "Junlong"
      }
    ]
  },
  {
    "title": "Dilated-ResUnet: A novel deep learning architecture for building extraction from medium resolution multi-spectral satellite imagery",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115530",
    "abstract": "In today's world, satellite images are being utilized for the identification of built-up area, urban planning, disaster management, insurance & tax assessment in an area, and many other social-economic activities. The extraction of the accurate building footprints in densely populated urban areas from medium resolution satellite images is still a challenging task which requires the development of the new methods to solve such problem. In this paper, a novel Dilated-ResUnet deep learning architecture for building extraction from Sentinel-2 satellite images has been proposed. The proposed model has been tested on three novel building datasets that are prepared for three densely populated cities of India (viz. Delhi, Hyderabad and Bengaluru) using Sentinel-2 satellite images and Planet OSM. First FCC (false colour composite) dataset prepared by merging NIR, Red, Green bands, second FCC dataset prepared by merging NIR, Red, Green and Blue bands and third is TCC (true colour composite) dataset by merging red, green and blue bands. The proposed architecture is applied to both the FCC datasets and TCC dataset separately; it has been identified that the proposed model has obtained better building extraction results using FCC (NIR, Red, Green) dataset. The input satellite image enhancement and extensive experimentations to identify the optimal deep learning hyper-parameters using FCC spatial dataset have also been carried out to further improve the performance of the proposed model. The results of the experimentations reveal that the proposed model has out-performed the state of the art models available in literature by achieving the F1-score of 0.4718 and Mean IoU of 0.582 for building extraction from Sentinel-2 satellite images. The outcome of the research work can be utilized for urban planning and management, generate more ground truths for Sentinel-2 satellite images which further can be useful for other societal applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009386",
    "keywords": [
      "Aerospace engineering",
      "Archaeology",
      "Architecture",
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Deep learning",
      "Engineering",
      "Extraction (chemistry)",
      "Geography",
      "Pattern recognition (psychology)",
      "Remote sensing",
      "Satellite",
      "Satellite imagery"
    ],
    "authors": [
      {
        "surname": "Dixit",
        "given_name": "Mayank"
      },
      {
        "surname": "Chaurasia",
        "given_name": "Kuldeep"
      },
      {
        "surname": "Kumar Mishra",
        "given_name": "Vipul"
      }
    ]
  },
  {
    "title": "Deep triplet residual quantization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115467",
    "abstract": "Quantization techniques have been widely used in the approximate near neighbor similarity search, data compression, etc. Recently, metric learning based deep hashing methods take advantage of quantization techniques to accelerate the computation with minimal accuracy loss. However, most of the existing deep quantization methods are designed for Euclidean distance, which may not lead to good performance in maximum inner product search (MIPS). In addition, metric learning requires an elaborated training strategy for sample selection, which matters in learning high-quality feature representation and boosting the convergent speed of the network. In this paper, we propose a novel deep triplet residual quantization (DTRQ) model that integrates the residual quantization (RQ) into the triplet selection strategy and the quantization error control of MIPS. Specifically, instead of randomly grouping the samples as in DTQ, we group the samples based on the geographical information provided by RQ so that each group can generate more high-quality triplets for faster convergence. Furthermore, we decompose the triplet quantization loss into the norm and angle aspect, which especially reduce the codeword redundancy in MIPS ranking. By stringing the residual quantization through the triplet selection stage and quantization error control, DTRQ can generate high-quality and compact binary codes, which yields promising image retrieval performance on three benchmark datasets, NUS-WIDE, CIFAR-10, and MS-COCO.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008794",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Quantization (signal processing)",
      "Residual"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Chang"
      },
      {
        "surname": "Po",
        "given_name": "Lai-Man"
      },
      {
        "surname": "Ou",
        "given_name": "Wei-Feng"
      },
      {
        "surname": "Xian",
        "given_name": "Peng-Fei"
      },
      {
        "surname": "Cheung",
        "given_name": "Kwok-Wai"
      }
    ]
  },
  {
    "title": "Expert decision-making: A Markovian approach to studying the agency problem",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115451",
    "abstract": "In this paper, we study the agency problem in an organisation within a Markovian framework. More specifically, the paper presents the case of a principal imposing an incentive-control structure upon an agent to force him to follow the principal’s interests for which he was hired, against the tendency of the agent to follow his own interests. Findings point toward the principal’s difficulty in controlling the behaviour of the agent through incentives and monitoring; instead, best results are obtained when hiring agents who care for their reputation and refrain from unprofessional behaviours. The implication is that if we consider that it might be difficult to identify this characteristic at the time of the agent’s hiring, the best criterion will be to look for low levels of greed in the agent. This conclusion goes in some way against current practices of looking for aggressive agents for the generation of higher profits. Nevertheless, it should be noted that these potential benefits might actually fade away if the agent follows his own interests, instead of the principal’s. Another interesting result points to the restricted, although necessary, role of monitoring to control the agent’s behaviour, a result that goes against current research interests on measures of corporate governance. The paper is a contribution to expert decision-making.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008642",
    "keywords": [
      "Agency (philosophy)",
      "Artificial intelligence",
      "Computer science",
      "Economics",
      "Epistemology",
      "Machine learning",
      "Management science",
      "Markov process",
      "Mathematical optimization",
      "Mathematics",
      "Operations research",
      "Philosophy",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Charles",
        "given_name": "Vincent"
      },
      {
        "surname": "Chión",
        "given_name": "Sergio"
      },
      {
        "surname": "Gherman",
        "given_name": "Tatiana"
      }
    ]
  },
  {
    "title": "Feature weighting methods: A review",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115424",
    "abstract": "In the last decades, a wide portfolio of Feature Weighting (FW) methods have been proposed in the literature. Their main potential is the capability to transform the features in order to contribute to the Machine Learning (ML) algorithm metric proportionally to their estimated relevance for inferring the output pattern. Nevertheless, the extensive number of FW related works makes difficult to do a scientific study in this field of knowledge. Therefore, in this paper a global taxonomy for FW methods is proposed by focusing on: (1) the learning approach (supervised or unsupervised), (2) the methodology used to calculate the weights (global or local), and (3) the feedback obtained from the ML algorithm when estimating the weights (filter or wrapper). Among the different taxonomy levels, an extensive review of the state-of-the-art is presented, followed by some considerations and guide points for the FW strategies selection regarding significant aspects of real-world data analysis problems. Finally, a summary of conclusions and challenges in the FW field is briefly outlined.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008423",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Data mining",
      "Economics",
      "Feature (linguistics)",
      "Feature selection",
      "Field (mathematics)",
      "Law",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Medicine",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Pure mathematics",
      "Radiology",
      "Relevance (law)",
      "Taxonomy (biology)",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Niño-Adan",
        "given_name": "Iratxe"
      },
      {
        "surname": "Manjarres",
        "given_name": "Diana"
      },
      {
        "surname": "Landa-Torres",
        "given_name": "Itziar"
      },
      {
        "surname": "Portillo",
        "given_name": "Eva"
      }
    ]
  },
  {
    "title": "Assessment of a failure prediction model in the European energy sector: A multicriteria discrimination approach with a PROMETHEE based classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115513",
    "abstract": "This study presents the implementation of a non-parametric multiple criteria decision aiding (MCDA) model, the Multi-group Hierarchy Discrimination (M.H.DIS) model, through the application of an outranking MCDA approach, namely the PROMETHEE II, on a dataset of 114 European unlisted companies operating in the energy sector. Firstly, the M.H.DIS model has been developed following a five-fold cross validation procedure to analyze whether the model explains and replicates a two-group pre-defined classification of companies in the considered sample, provided by Bureau van Dijk's Amadeus database. Since the M.H.DIS method achieves a quite limited satisfactory accuracy in predicting the Amadeus classification in the holdout sample, the PROMETHEE II method has been performed then to provide a benchmark sorting procedure useful for comparison purposes. The analysis indicates that in terms of average accuracy, M.H.DIS model development with the PROMETHEE based classification provides consistently better results than the ones obtained with the Amadeus classification in most of combinations, which have been built with the financial variables covering the main firm’s dimensions such as profitability, financial structure, liquidity and turnover. The better results of the proposed model in terms of accuracy rate are also confirmed by the comparison to the most three applied machine-learning methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009234",
    "keywords": [
      "Analytic hierarchy process",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Economics",
      "Energy (signal processing)",
      "Energy sector",
      "Environmental economics",
      "Hierarchy",
      "Machine learning",
      "Market economy",
      "Mathematics",
      "Multiple-criteria decision analysis",
      "Operations research",
      "Parametric statistics",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Angilella",
        "given_name": "Silvia"
      },
      {
        "surname": "Pappalardo",
        "given_name": "Maria Rosaria"
      }
    ]
  },
  {
    "title": "Personal Protection Equipment detection system for embedded devices based on DNN and Fuzzy Logic",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115447",
    "abstract": "The large extension and complex structure of most industrial and construction areas very often make it unfeasible or inconvenient for human operators to constantly survey all the workers to detect those who do not properly wear their Personal Protection Equipment (PPE) devices. However, such a detection is of utmost importance to reduce the number of worker injuries. Consequently, the adoption of a computer vision system based on Deep Neural Networks (DNNs) that performs PPE detection by analysing the video streams from surveillance cameras is an appealing option. For instance, smart video cameras placed in the workplace might process the video frames at run-time and trigger alarms whenever they detect workers not correctly wearing PPE devices. However, in order to be sufficiently accurate, DNN-based object detection requires a high computational power that is difficult to embed in cameras. Moreover, DNN training has to be done on a large dataset with thousands of labeled image samples, and therefore the creation of a customized DNN to detect special PPE devices requires a huge effort in finding and labeling images to train the network. This paper proposes a PPE detection framework that combines DNN-based object detection with human judgement through fuzzy logic filtering. The proposed framework runs in near real-time on embedded devices and can be trained with a low number of images (i.e., few hundreds), still providing good accuracy results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008617",
    "keywords": [
      "Artificial intelligence",
      "Computer hardware",
      "Computer science",
      "Embedded system",
      "Fuzzy control system",
      "Fuzzy electronics",
      "Fuzzy logic",
      "Machine learning",
      "Neuro-fuzzy",
      "Real-time computing"
    ],
    "authors": [
      {
        "surname": "Iannizzotto",
        "given_name": "Giancarlo"
      },
      {
        "surname": "Lo Bello",
        "given_name": "Lucia"
      },
      {
        "surname": "Patti",
        "given_name": "Gaetano"
      }
    ]
  },
  {
    "title": "Comments on “Stacking ensemble based deep neural networks modeling for effective epileptic seizure detection”",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115488",
    "abstract": "This short communication provides a discourse emerged after reading “Stacking ensemble based deep neural networks modeling for effective epileptic seizure detection, Expert Systems with Applications, 148, 113239, 2020.” The discussed paper proposes a novel application of stacking-based ensemble for seizure detection, where several deep neural networks were used as base classifiers. The ensemble design and experimental results presented by the author show some weaknesses, which is indicated by, one of which, an inability of the proposed model to outperform previous studies. In this note, controversy of the discussed paper is explained and an improved version of stacking-based deep neural network is also further introduced and detailed to prevent it in the future.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008988",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Computer science",
      "Deep neural networks",
      "Ensemble learning",
      "Epilepsy",
      "Law",
      "Machine learning",
      "Neuroscience",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Political science",
      "Psychology",
      "Reading (process)",
      "Stacking"
    ],
    "authors": [
      {
        "surname": "Tama",
        "given_name": "Bayu Adhi"
      },
      {
        "surname": "Lee",
        "given_name": "Seungchul"
      }
    ]
  },
  {
    "title": "Opposition-based moth swarm algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115481",
    "abstract": "Nowadays, resource-optimizing techniques are required in many engineering areas to obtain the most appropriate solutions for complex problems. For this reason, there is a trend among researchers to improve existing swarm-based algorithms through different evolutionary techniques and to create new population-based methods that can accurately explore the feature space. The recently proposed Moth swarm algorithm (MSA) inspired by the orientation of moths towards moonlight is an associative learning mechanism with immediate memory that uses Lévy mutation to cross-population diversity and spiral movement. The MSA is a population-based method used for tackling complex optimization problems. It presents an adequate capacity for exploration and exploitation trends; however, due to its nature of operators, this type of method is prone to get stuck in sub-optimal locations, which affects the speed of convergence and the computational effort to reach better solutions. To mitigate these shortcomings, this paper proposes an improved MSA that combines opposition-based learning (OBL) as a mechanism to enhance the exploration drifts of the basic version and increase the speed of convergence to obtain more accurate solutions. The proposed approach is called OBMSA. It has been tested for solving three classic engineering design problems (welded beam, tension/compression spring, and pressure vessel designs) with constraints, 19 benchmark functions comprising 7 unimodal, 6 multimodal, and 6 composite functions. Experimental results and comparisons provide evidence that the performance and accuracy of the proposed method are superior to the original MSA. We hope the community utilizes the proposed MSA-based approach for solving other complex problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008927",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Demography",
      "Evolutionary algorithm",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Population",
      "Sociology",
      "Swarm behaviour"
    ],
    "authors": [
      {
        "surname": "Oliva",
        "given_name": "Diego"
      },
      {
        "surname": "Esquivel-Torres",
        "given_name": "Sara"
      },
      {
        "surname": "Hinojosa",
        "given_name": "Salvador"
      },
      {
        "surname": "Pérez-Cisneros",
        "given_name": "Marco"
      },
      {
        "surname": "Osuna-Enciso",
        "given_name": "Valentín"
      },
      {
        "surname": "Ortega-Sánchez",
        "given_name": "Noé"
      },
      {
        "surname": "Dhiman",
        "given_name": "Gaurav"
      },
      {
        "surname": "Heidari",
        "given_name": "Ali Asghar"
      }
    ]
  },
  {
    "title": "An integrated decision-making approach for sustainable supplier selection in the chemical industry",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115553",
    "abstract": "Due to an increased awareness of ecological protection and resultant stringent legislations, business organizations are highly motivated to improve the sustainable performance of their supply chain in order to achieve sustainable development goals. The chemical industry is a high-risk, high-pollution, and high-efficiency industry, that would benefit from a systematic and sustainability focused evaluation system for supplier selection. Yet, to date, few studies have conducted the necessary in-depth analysis of the characteristics of this industry from the economic, social, and environmental perspectives. Despite the many methods and models that have been proposed to resolve the sustainable supplier selection (SSS) problem, no research has yet considered the different characteristics of each triple bottom line dimension. Accordingly, this paper addresses this problem by proposing a hybrid multi-method and multi-criteria decision-making framework for SSS in the chemical industry. Based on specific characteristics of the chemical industry, this study applies Fuzzy Grey Relational Analysis (FGRA), Failure Mode and Effects Analysis (FMEA), and cloud computing-entropy weight method (EWM) to analyze the economic, social, and environmental dimensions, respectively. Finally, this study integrates the evaluation results of the three dimensions using the Decision-making Trial and Evaluation Laboratory (DEMATEL). The proposed approach and decision-making model can help managers of sustainable supply chains in the chemical industry to choose more sustainable suppliers, respond to market demands quickly, and maintain high competitiveness in the market. An illustrative application of the proposed framework and model is undertaken in one of the biggest Chinese petrochemical companies to verify its practicality and reliability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009593",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Business",
      "Chemical industry",
      "Chemistry",
      "Computer science",
      "Decision-making models",
      "Ecology",
      "Economics",
      "Environmental economics",
      "Fuzzy logic",
      "Grey relational analysis",
      "Marketing",
      "Mathematical economics",
      "Organic chemistry",
      "Risk analysis (engineering)",
      "Supply chain",
      "Sustainability",
      "Triple bottom line"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Chong"
      },
      {
        "surname": "Lin",
        "given_name": "Yang"
      },
      {
        "surname": "Barnes",
        "given_name": "David"
      }
    ]
  },
  {
    "title": "Incremental learning using generative-rehearsal strategy for fault detection and classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115477",
    "abstract": "In this study, we propose a novel pseudorehearsal method for modeling fault detection and classification. As manufacturing processes become increasingly advanced, it is often necessary to model the architecture when the data change over time. Particularly, learning with the addition of new fault types is called class incremental learning. Although learning systems must acquire new information from new data, this includes problems that can lead to catastrophic forgetting and class imbalance, wherein the number of instances in a particular class is greater than those in the other classes. Classification performance degrades when the existing model is trained under such conditions. Therefore, we propose a generative-rehearsal strategy that combines a pseudorehearsal strategy with independent generative models for each fault type. This method overcomes catastrophic forgetting and enables incremental learning with unbalanced data. The performance of the proposed method was superior to that of existing incremental and nonincremental methods while being memory efficient.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008885",
    "keywords": [
      "Actuator",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Fault (geology)",
      "Fault detection and isolation",
      "Forgetting",
      "Generative grammar",
      "Generative model",
      "Geology",
      "Incremental learning",
      "Linguistics",
      "Machine learning",
      "Philosophy",
      "Seismology"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Subin"
      },
      {
        "surname": "Chang",
        "given_name": "Kyuchang"
      },
      {
        "surname": "Baek",
        "given_name": "Jun-Geol"
      }
    ]
  },
  {
    "title": "A Nash bargaining solution for a multi period competitive portfolio optimization problem: Co-evolutionary approach",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115509",
    "abstract": "This study focuses on proposing a Nash bargaining model to solve a novel multi-period competitive portfolio optimization problem for large investors in the stock market who want to maximize their terminal wealth while taking into account competitors' profits. In this study, a Competitive Portfolio Model (CPM) is developed in accordance with the Cournot competition principle for a static, non-cooperative, and non-zero-sum game with complete information. Transaction costs, risk-free assets and cash are also included to match real-world conditions. Also, three criteria including the average value at risk, the mean absolute semi-deviation, and entropy are considered to control the investment risk in the model. Moreover, due to common constraints between players (free floating shares of risky assets) in stock markets, this study falls into the category of Generalized Nash Equilibrium Problems (GNEP). Therefore, to overcome the problem, a Cooperative Co-evolutionary Algorithm (CCA) based on Particle Swarm Optimization (PSO) is customized and used. A few experimental tests and a numerical example with descriptive analytics (using real data from two large mutual funds who invest in the Iranian Stock Exchange market) are used to evaluate the feasibility of the proposed model and the efficiency of the design algorithm. After solving the model, for each time period, investors' trading strategies (trading signals and stock volume in their portfolio) are determined. The results show that the volume of transactions due to the market power of an investor has a significant effect on the terminal wealth of competitors. Also, the results of sensitivity analysis show that profit-making is inversely related to the degree of risk aversion of investors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009192",
    "keywords": [
      "Biology",
      "Computer science",
      "Cournot competition",
      "Econometrics",
      "Economics",
      "Financial economics",
      "Horse",
      "Mathematical optimization",
      "Mathematics",
      "Microeconomics",
      "Nash equilibrium",
      "Paleontology",
      "Portfolio",
      "Portfolio optimization",
      "Stock market",
      "Trading strategy"
    ],
    "authors": [
      {
        "surname": "Nokhandan",
        "given_name": "Behnaz Pourvalikhan"
      },
      {
        "surname": "Khalili-Damghani",
        "given_name": "Kaveh"
      },
      {
        "surname": "Hafezalkotob",
        "given_name": "Ashkan"
      },
      {
        "surname": "Didehkhani",
        "given_name": "Hosein"
      }
    ]
  },
  {
    "title": "A static semi-kitting strategy system of JIT material distribution scheduling for mixed-flow assembly lines",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115523",
    "abstract": "The green scheduling plays an increasingly important role in many manufacturing enterprises and trading off the Just-in-Time (JIT) and energy efficiency is a complex optimization problem. A static semi-kitting strategy is proposed to deal with a bi-objective JIT material distribution scheduling problem considering energy consumption (BJMDSP-EC) for mixed-flow assembly lines. A mathematical model that aims to minimize the total energy consumption and the total line-side inventory is established, which meets the request of energy saving and JIT. Due to the NP-hard nature of the proposed problem, a Memory-based Bi-objective Chaotic Gravitational Search Algorithm (MBCGSA) is developed to solve the BJMDSP-EC, which hybridizes the Gravitational Search Algorithm (GSA) and the non-dominated sorting genetic algorithm-II (NSGA-II). The memory-based search strategy and the chaotic gravity constant are applied to the bi-objective optimization algorithm to balance exploration and exploitation. Moreover, clustering and Cauchy deviates are utilized to initialize the population, and two local search optimization operators are designed to optimize two objectives. Finally, computational experiments are performed to evaluate the performance of the MBCGSA by being compared with two other bi-objective optimization algorithms, the multi-objective particle swarm optimization (MOPSO) and NSGA-II, and the results reveal the effectiveness and efficiency of the MBCGSA when solving BJMDSP-EC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009325",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Demography",
      "Electrical engineering",
      "Energy consumption",
      "Engineering",
      "Flow shop scheduling",
      "Job shop scheduling",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Particle swarm optimization",
      "Population",
      "Schedule",
      "Scheduling (production processes)",
      "Sociology",
      "Sorting"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Binghai"
      },
      {
        "surname": "He",
        "given_name": "Zhaoxu"
      }
    ]
  },
  {
    "title": "A link2vec-based fake news detection model using web search results",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115491",
    "abstract": "Today, the world is under siege from various kinds of fake news ranging from politics to COVID-19. Thus, many scholars have been researching automatic fake news detection based on artificial intelligence and machine learning (AI/ML) to prevent the spread of fake news. The mainstream research on detecting fake news so far has been text-based detection approaches, but they have inherent limitations such as the difficulty of short text processing and language dependency. Thus, as an alternative to the text-based approach, the context-based approach is emerging. The most common context-based approach the use of distributors’ network information in social media. However, such information is difficult to obtain, and only propagation within a single social media can be traced. Under this background, we propose the use of composition pattern of web links containing news content as a new source of information for fake news detection. To properly vectorize the composition pattern of web links, this study proposes a novel embedding technique, which is called link2vec, an extension of word2vec. To test the effectiveness and language independency of our link2vec-based model, we applied it to two real-world fake news datasets in different languages (English and Korean). As comparison models, we adopted the conventional text-based model and a hybrid model that combined text and whitelist-based link information proposed by a prior study. Results revealed that in the datasets in two languages, the link2vec-based detection models outperformed all the comparison models with statistical significance. Our research is expected to contribute to suggesting a completely new path for effective fake news detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009015",
    "keywords": [
      "Computer science",
      "Data mining",
      "Fake news",
      "Information retrieval",
      "Internet privacy",
      "Search engine",
      "Spamdexing",
      "Web search engine",
      "Web search query",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Shim",
        "given_name": "Jae-Seung"
      },
      {
        "surname": "Lee",
        "given_name": "Yunju"
      },
      {
        "surname": "Ahn",
        "given_name": "Hyunchul"
      }
    ]
  },
  {
    "title": "Using deep learning neural networks to predict the knowledge economy index for developing and emerging economies",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115514",
    "abstract": "Missing values and the inconsistency of the measures of the knowledge economy remain vexing problems that hamper policy-making and future research in developing and emerging economies. This paper contributes to the new and evolving literature that seeks to advance better understanding of the importance of the knowledge economy for policy and further research in developing and emerging economies. In this paper we use a supervised machine deep learning neural network (DLNN) approach to predict the knowledge economy index of 71 developing and emerging economies during the 1995–2017 period. Applied in combination with a data imputation procedure based on the K-closest neighbor algorithm, DLNN is capable of handling missing data problems better than alternative methods. A 10-fold validation of the DLNN yielded low quadratic and absolute error (0,382 +- 0,065). The results are robust and efficient, and the model’s predictive power is high. There is a difference in the predictive power when we disaggregate countries in all emerging economies versus emerging Central European countries. We explain this result and leave the rest to future endeavors. Overall, this research has filled in gaps due to missing data thereby allowing for effective policy strategies. At the aggregate level development agencies, including the World Bank that originated the KEI, would benefit from our approach until substitutes come along.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009246",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Deep learning",
      "Economics",
      "Emerging markets",
      "Index (typography)",
      "Knowledge economy",
      "Knowledge management",
      "Machine learning",
      "Macroeconomics",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Andrés",
        "given_name": "Antonio Rodríguez"
      },
      {
        "surname": "Otero",
        "given_name": "Abraham"
      },
      {
        "surname": "Amavilah",
        "given_name": "Voxi Heinrich"
      }
    ]
  },
  {
    "title": "A systematic analysis and guidelines of graph neural networks for practical applications",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115466",
    "abstract": "A graph neural network (GNN) draws attention to deal with many problems in social networks and bioinformatics, as graph data proliferate in a wide variety of applications. Despite the large amount of investigation, it is still difficult to choose the most suitable method for a given problem due to the lack of a thorough analysis on the feasible methods. An anatomical comparison of GNNs would help to devise a prospective method for better solution to real-world problems. In order to give guidelines to make full use of the GNN for graph classification, this paper attempts to analyze the state-of-the-art methods of the GNN and provide practicable guidelines for applications. The representative methods are described with a systematic scheme in four phases for GNN: 1) pre-processing, 2) aggregation, 3) readout, and 4) classification with graph embedding, resulting in a large coverage of more than 1300 methods. The 13 well-known benchmark datasets are categorized into three types with respect to the properties of graph data such as connectivity. In total, more than 3600 runs are executed to systematically analyze and compare the GNN models while changing only one method for each phase. Experimental reproducibility and replicability are also verified by comparing the results with the performance from the literature. Finally, five guidelines for an appropriate model are deduced according to the graph characteristics such as complexity on connectivity and node feature.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008782",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Data mining",
      "Embedding",
      "Geodesy",
      "Geography",
      "Graph",
      "Graph embedding",
      "Machine learning",
      "Power graph analysis",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Jin-Young"
      },
      {
        "surname": "Cho",
        "given_name": "Sung-Bae"
      }
    ]
  },
  {
    "title": "Eigenvector-based centralities for multilayer temporal networks under the framework of tensor computation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115471",
    "abstract": "In recent years, many centralities have been developed to identify the key nodes in multilayer and temporal networks. Among these centrality measures, eigenvector-based centralities are very efficient ranking algorithms. In the real world, some complex systems have multilayer structure and edges are dynamic, i.e., they appear and disappear over time, referred to as the multilayer temporal networks. Moreover, some eigenvector-based centralities have been extended to rank the nodes in multilayer temporal networks. However, these existing eigenvector-based centralities ignore the inter-layer interactions between different time stamps and may get incorrect ranking results of nodes. In order to better describe the multilayer and temporal features of networks, in this paper, we construct a sixth-order tensor to represent multilayer temporal networks. In particular, cosine similarity is used to depict the relationships of inter-layer. Then, we propose multilayer temporal eigenvector and PageRank centralities based on the sixth-order tensor and cosine similarity for ranking the nodes, layers and time stamps in networks, referred to as MTEIGBC and MTPRBC centralities. Furthermore, the existence and uniqueness of our centrality measures can be guaranteed under very reasonable conditions. Finally, numerical experiments on three networks are performed to demonstrate the effectiveness and superiority of our proposed ranking methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008824",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Centrality",
      "Combinatorics",
      "Complex network",
      "Computation",
      "Computer science",
      "Cosine similarity",
      "Data mining",
      "Eigenvalues and eigenvectors",
      "Image (mathematics)",
      "Mathematics",
      "PageRank",
      "Pattern recognition (psychology)",
      "Physics",
      "Pure mathematics",
      "Quantum mechanics",
      "Rank (graph theory)",
      "Ranking (information retrieval)",
      "Similarity (geometry)",
      "Tensor (intrinsic definition)",
      "Theoretical computer science",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Lv",
        "given_name": "Laishui"
      },
      {
        "surname": "Zhang",
        "given_name": "Kun"
      },
      {
        "surname": "Zhang",
        "given_name": "Ting"
      },
      {
        "surname": "Li",
        "given_name": "Xun"
      },
      {
        "surname": "Sun",
        "given_name": "Qi"
      },
      {
        "surname": "Zhang",
        "given_name": "Lilinqing"
      },
      {
        "surname": "Xue",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "A new PC-PSO algorithm for Bayesian network structure learning with structure priors",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115237",
    "abstract": "Bayesian network structure learning is the basis of parameter learning and Bayesian inference. However, it is a NP-hard problem to find the optimal structure of Bayesian networks because the computational complexity increases exponentially with the increasing number of nodes. Hence, numerous algorithms have been proposed to obtain feasible solutions, while almost all of them are of certain limits. In this paper, we adopt a heuristic algorithm to learn the structure of Bayesian networks, and this algorithm can provide a reasonable solution to combine the PC and Particle Swarm Optimization (PSO) algorithms. Moreover, we consider structure priors to improve the performance of our PC-PSO algorithm. Meanwhile, we utilize a new mutation operator called Uniform Mutation by Addition and Deletion (UMAD) and a crossover operator called Uniform Crossover. Experiments on different networks show that the approach proposed in this paper has achieved better Bayesian Information Criterion (BIC) scores than other algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006692",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bayesian network",
      "Bayesian probability",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Crossover",
      "Gene",
      "Heuristic",
      "Inference",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Operator (biology)",
      "Particle swarm optimization",
      "Prior probability",
      "Repressor",
      "Transcription factor"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Baodan"
      },
      {
        "surname": "Zhou",
        "given_name": "Yun"
      },
      {
        "surname": "Wang",
        "given_name": "Jianjiang"
      },
      {
        "surname": "Zhang",
        "given_name": "Weiming"
      }
    ]
  },
  {
    "title": "An intelligent system for focused crawling from Big Data sources",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115560",
    "abstract": "Nowadays, the proper management of data is a key business enabler and booster for companies, so as to increase their competitiveness. Typically, companies hold massive amounts of data within their servers, which might include previously offered services, proposals, bids, and so on. They rely on their expert managers to manually analyse them in order to make strategic decisions. However, given the huge amount of information to be analysed and the necessity of making timely decisions, they often exploit a small amount of the available data, which often does not yield effective choices. For instance, this happens in the context of the e-procurement domain, where bids for new calls for tender are often formulated by looking at some past proposals from a company. Driven by an extensive experience on the e-procurement domain, in this paper we propose an intelligent system to support organisations in the focused crawling of artefacts (calls for tender, BIMs, equipment, policies, market trends, and so on) of interest from the web, semantically matching them against internal Big Data and knowledge sources, so as to let companies analysts make better strategic decisions. The novel contribution consists of a proper extension of the K-means algorithm used by a web crawler within the proposed system, and a semantic module exploiting search patterns to find relevant data within the crawled artefacts. The proposed solution has been implemented and extensively assessed in the e-procurement domain. It has been successively extended to other domains, such as robot programming, cloud providing, and several other domains. Since to the best of our knowledge in the literature do not exists similar systems, in order to prove its effectiveness we have compared its crawling component against similar crawlers, by plugging them within our system.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009660",
    "keywords": [
      "Anatomy",
      "Big data",
      "Biology",
      "Business",
      "Computer science",
      "Computer security",
      "Context (archaeology)",
      "Crawling",
      "Data mining",
      "Data science",
      "Domain (mathematical analysis)",
      "Exploit",
      "Knowledge management",
      "Marketing",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Paleontology",
      "Procurement",
      "Semantic Web",
      "Web crawler",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Bifulco",
        "given_name": "Ida"
      },
      {
        "surname": "Cirillo",
        "given_name": "Stefano"
      },
      {
        "surname": "Esposito",
        "given_name": "Christian"
      },
      {
        "surname": "Guadagni",
        "given_name": "Roberta"
      },
      {
        "surname": "Polese",
        "given_name": "Giuseppe"
      }
    ]
  },
  {
    "title": "Real-time human pose estimation on a smart walker using convolutional neural networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115498",
    "abstract": "Rehabilitation is important to improve quality of life for mobility-impaired patients. Smart walkers are a commonly used solution that should embed automatic and objective tools for data-driven human-in-the-loop control and monitoring. However, present solutions focus on extracting few specific metrics from dedicated sensors with no unified full-body approach. We investigate a general, real-time, full-body pose estimation framework based on two RGB+D camera streams with non-overlapping views mounted on a smart walker equipment used in rehabilitation. Human keypoint estimation is performed using a two-stage neural network framework. The 2D-Stage implements a detection module that locates body keypoints in the 2D image frames. The 3D-Stage implements a regression module that lifts and relates the detected keypoints in both cameras to the 3D space relative to the walker. Model predictions are low-pass filtered to improve temporal consistency. A custom acquisition method was used to obtain a dataset, with 14 healthy subjects, used for training and evaluating the proposed framework offline, which was then deployed on the real walker equipment. An overall keypoint detection error of 3.73 pixels for the 2D-Stage and 44.05 mm for the 3D-Stage were reported, with an inference time of 26.6 ms when deployed on the constrained hardware of the walker. We present a novel approach to patient monitoring and data-driven human-in-the-loop control in the context of smart walkers. It is able to extract a complete and compact body representation in real-time and from inexpensive sensors, serving as a common base for downstream metrics extraction solutions, and Human-Robot interaction applications. Despite promising results, more data should be collected on users with impairments, to assess its performance as a rehabilitation tool in real-world scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009088",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Context (archaeology)",
      "Convolutional neural network",
      "Focus (optics)",
      "Inference",
      "Law",
      "Optics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel",
      "Political science",
      "Politics",
      "Pose",
      "RGB color model",
      "Real-time computing",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Palermo",
        "given_name": "Manuel"
      },
      {
        "surname": "Moccia",
        "given_name": "Sara"
      },
      {
        "surname": "Migliorelli",
        "given_name": "Lucia"
      },
      {
        "surname": "Frontoni",
        "given_name": "Emanuele"
      },
      {
        "surname": "Santos",
        "given_name": "Cristina P."
      }
    ]
  },
  {
    "title": "Automatic stimuli classification from ERP data for augmented communication via Brain–Computer Interfaces",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115572",
    "abstract": "This work presents a supervised machine-learning approach to build an expert system that provides support to the neuroscientist in automatically classifying ERP data and matching them with a multisensorial alphabet of stimuli. To do this, two different approaches are considered: a hierarchical tree-based algorithm, XGBoost, and feedfoward neural networks, highlighting the pros and cons of both approaches in the different steps of the classification task. Moreover, the sensitivity of the classification capabilities of the tool as a function of the number of available electrodes is also studied, highlighting what can be achieved by applying the method using commercial, wearable EEG systems. The main novelty of this work consists in significantly enlarging the pool of stimuli that the expert system can recognize and comprising different, possibly mixed, sensorial domains. The obtained results open the way to the design of portable devices for augmented communication systems, which can be of particular interest for the development of advanced Brain–Computer Interfaces (BCI) for communication with different types of neurologically impaired patients.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009763",
    "keywords": [
      "Artificial intelligence",
      "Brain–computer interface",
      "Computer science",
      "Economics",
      "Electroencephalography",
      "Electronic engineering",
      "Embedded system",
      "Engineering",
      "Human–computer interaction",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Novelty",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Psychiatry",
      "Psychology",
      "Sensitivity (control systems)",
      "Task (project management)",
      "Theology",
      "Tree (set theory)",
      "Wearable computer"
    ],
    "authors": [
      {
        "surname": "Leoni",
        "given_name": "Jessica"
      },
      {
        "surname": "Strada",
        "given_name": "Silvia Carla"
      },
      {
        "surname": "Tanelli",
        "given_name": "Mara"
      },
      {
        "surname": "Jiang",
        "given_name": "Kaijun"
      },
      {
        "surname": "Brusa",
        "given_name": "Alessandra"
      },
      {
        "surname": "Proverbio",
        "given_name": "Alice Mado"
      }
    ]
  },
  {
    "title": "An iterated greedy algorithm for the parallel blocking flow shop scheduling problem and sequence-dependent setup times",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115535",
    "abstract": "This paper deals with the problem of scheduling jobs in a parallel flow shop configuration under the blocking constraint, in which the setup time of machines depends not only on the job to be processed but also on the previously processed one, i.e., there are sequence-dependent setup times. The performance analysis of several iterated greedy algorithms with different initial solution procedures and local searches lets us define an efficient algorithm to minimize the maximum job completion time. Moreover, the computational evaluation showed the efficiency of searching in different neighborhood structures and noted the significant influence of the initial solution. However, contrary to other scheduling problems, starting with a high quality solution does not guarantee better performance of the algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009428",
    "keywords": [
      "Algorithm",
      "Biology",
      "Blocking (statistics)",
      "Computer network",
      "Computer science",
      "Flow shop scheduling",
      "Genetics",
      "Greedy algorithm",
      "Iterated function",
      "Job shop scheduling",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Schedule",
      "Scheduling (production processes)",
      "Sequence (biology)"
    ],
    "authors": [
      {
        "surname": "Ribas",
        "given_name": "Imma"
      },
      {
        "surname": "Companys",
        "given_name": "Ramon"
      },
      {
        "surname": "Tort-Martorell",
        "given_name": "Xavier"
      }
    ]
  },
  {
    "title": "Acoustic recognition of noise-like environmental sounds by using artificial neural network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115484",
    "abstract": "In spite of establishing audio perception techniques and tremendous progress of computer capabilities, artificial perception of sound videlicet natural ability to hear is at the initial stage. The latest phenomena in the evolution of expert systems, like internet of things, put a focus to the mass applications that run on embedded platforms often with pure computational capacity. Modern sensing applications require simplicity, universality, and excellent performance. Insects, obviously, realize limited but satisfactory interaction with the environment using minimum resources. The experiment is motivated by the assumption that the similar principle of control can be applied in artificial control systems. In such a manner, theoretical and practical research was conducted in order of defining optimal procedure for the recognition of short, cognitively unpretentious noise-like sounds, in real conditions, on the basis of previous experience. The result is optimal hybrid procedure for the recognition of noise-like environmental sounds built of heuristic algorithms completely. The experiment reports the success in recognizing unfavourable sounds using frequency spectrum as feature vector. The ability of abstraction was tested by employing samples of different abstraction level and robustness with respect to white noise and confusing sounds. Optimal preprocessing was suggested for the improving accuracy, employing white noise and confusing sounds for estimating preprocessing parameters. Built of ultimate algorithms, the procedure is useful for a broad range of research. This is of exceptional importance because acoustic perception in its full complexity can be approached only if the problem is observed multidisciplinary.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008952",
    "keywords": [
      "Abstraction",
      "Acoustics",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Environmental noise",
      "Epistemology",
      "Gene",
      "Image (mathematics)",
      "Machine learning",
      "Neuroscience",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Perception",
      "Philosophy",
      "Physics",
      "Preprocessor",
      "Robustness (evolution)",
      "Sound (geography)",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Simonović",
        "given_name": "Miloš"
      },
      {
        "surname": "Kovandžić",
        "given_name": "Marko"
      },
      {
        "surname": "Ćirić",
        "given_name": "Ivan"
      },
      {
        "surname": "Nikolić",
        "given_name": "Vlastimir"
      }
    ]
  },
  {
    "title": "Solving in real-time the dynamic and stochastic shortest path problem for electric vehicles by a prognostic decision making strategy",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115489",
    "abstract": "The adoption of Electric Vehicles (EVs) has substantially increased during the last decade, creating the need for customized EV-oriented routing strategies capable of using the enormous amount of historical, and real-time, traffic data that is collected through Intelligent Transport Systems (ITSs). Existing EV routing algorithms, however, concentrate mostly on the usage historical data to compute offline optimal paths, whereas the use of real-time traffic information to compute en-route path updates is still an almost unexplored topic; mainly due to its inherent computational challenges. This research effort proposes a Prognostic Decision Making (PDM) strategy to solve in real-time the Electric Vehicle Dynamic Stochastic Shortest Path Problem (EV-DSSPP); aiming at the simultaneous utilization of historical and real-time traffic data. Factors such as recurring and non-recurring traffic congestion, elevation, velocity and EV’s parameters are incorporated into the decision-making process. The proposed strategy has two hierarchical functional layers. The lower layer consists of a fast-computing routing algorithm that, by construction, guarantees a real-time execution. The higher layer organizes the periodic en-route execution of the first layer to compute en-route path updates during a trip. This strategy can hence serve as an expert router that works jointly with an ITS to assist EV drivers on route. The proposal is validated through a simulation study based on real-world traffic data collected in Santiago, Chile. The results show that periodic en-route path updates can generate a reduction in both travel time and energy consumption, which evidences the benefits of incorporating real-time traffic information into the EV-routing problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100899X",
    "keywords": [
      "Computer network",
      "Computer science",
      "Constrained Shortest Path First",
      "Graph",
      "K shortest path routing",
      "Mathematical optimization",
      "Mathematics",
      "Operations research",
      "Path (computing)",
      "Shortest path problem",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Rozas",
        "given_name": "Heraldo"
      },
      {
        "surname": "Muñoz-Carpintero",
        "given_name": "Diego"
      },
      {
        "surname": "Saéz",
        "given_name": "Doris"
      },
      {
        "surname": "Orchard",
        "given_name": "Marcos E."
      }
    ]
  },
  {
    "title": "A probabilistic linguistic dominance score method considering individual semantics and psychological behavior of decision makers",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115372",
    "abstract": "Psychological behavior could play a key role in many decision-making situations, therefore introducing it into the decision-making process might lead to better decisions being made. In those decision-making problems with uncertainty that are modelled by probabilistic linguistic information, the semantics of such information could be interpreted in different ways by different decision makers (DMs). Consequently, this paper aims at developing a prospect theory (PT) based on the gained and lost dominance score (GLDS) method for probabilistic linguistic multi-attribute group decision-making (MAGDM) to consider both individual semantics and DMs’ psychological behavior. As such, a new similarity degree is introduced to determine DMs’ weights and two programming models are constructed to obtain the individual numerical scales of linguistic terms and optimal individual weights of attributes. Then, the DMs’ psychological behavior is reflected by defining the individual prospect gained and lost dominance scores of alternatives on the basis of PT. The individual prospect dominance scores of alternatives are aggregated into collective prospect dominance scores of alternatives and the collective ranking of alternatives is then generated. Finally, an emission reduction device supplier selection example is analyzed and comparison analyses show the superiority and effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007995",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Dominance (genetics)",
      "Gene",
      "Group decision-making",
      "Probabilistic logic",
      "Psychology",
      "Ranking (information retrieval)",
      "Social psychology"
    ],
    "authors": [
      {
        "surname": "Wan",
        "given_name": "Shu-Ping"
      },
      {
        "surname": "Zou",
        "given_name": "Wen-Chang"
      },
      {
        "surname": "Dong",
        "given_name": "Jiu-Ying"
      },
      {
        "surname": "Martínez",
        "given_name": "Luis"
      }
    ]
  },
  {
    "title": "Writer Identification using Deep Learning with FAST Keypoints and Harris corner detector",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115473",
    "abstract": "Writer identification from offline images of handwriting is an interesting pattern classification problem that has been investigated for many decades now. Despite significant research endeavors, the problem still remains challenging due to high intra-class variations and, at times, high similarity between writings of two individuals. This paper presents a writer identification system that relies on extraction of key points from handwriting and feeding small patches around these key points to a convolutional neural network for feature learning and classification. More specifically, we employ FAST key points and Harris corner detector to identify points of interest in the handwriting. A deep CNN is trained using small patches centered around these key points. Classification is carried out in an end-to-end manner as well as by encoding features extracted from one of the convolutional layers. Experimental study is carried out on three different datasets namely IAM, QUWI and IFN/ENIT and high identification rates are reported for each of these datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008848",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Computer security",
      "Convolutional neural network",
      "Corner detection",
      "Deep learning",
      "Detector",
      "Feature (linguistics)",
      "Feature extraction",
      "Handwriting",
      "Identification (biology)",
      "Image (mathematics)",
      "Key (lock)",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Similarity (geometry)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Semma",
        "given_name": "Abdelillah"
      },
      {
        "surname": "Hannad",
        "given_name": "Yaâcoub"
      },
      {
        "surname": "Siddiqi",
        "given_name": "Imran"
      },
      {
        "surname": "Djeddi",
        "given_name": "Chawki"
      },
      {
        "surname": "El Youssfi El Kettani",
        "given_name": "Mohamed"
      }
    ]
  },
  {
    "title": "Novel hybrid model based on echo state neural network applied to the prediction of stock price return volatility",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115490",
    "abstract": "The prediction of stock price return volatilities is important for financial companies and investors to help to measure and managing market risk and to support financial decision-making. The literature points out alternative prediction models - such as the widely used heterogeneous autoregressive (HAR) specification - which attempt to forecast realized volatilities accurately. However, recent variants of artificial neural networks, such as the echo state network (ESN), which is a recurrent neural network based on the reservoir computing paradigm, have the potential for improving time series prediction. This paper proposes a novel hybrid model that combines HAR specification, the ESN, and the particle swarm optimization (PSO) metaheuristic, named HAR-PSO-ESN, which combines the feature design of the HAR model with the prediction power of ESN, and the consistent PSO metaheuristic approach for hyperparameters tuning. The proposed model is benchmarked against existing specifications, such as autoregressive integrated moving average (ARIMA), HAR, multilayer perceptron (MLP), and ESN, in forecasting daily realized volatilities of three Nasdaq (National Association of Securities Dealers Automated Quotations) stocks, considering 1-day, 5-days, and 21-days ahead forecasting horizons. The predictions are evaluated in terms of r-squared and mean squared error performance metrics, and the statistical comparison is made through a Friedman test followed by a post-hoc Nemenyi test. Results show that the proposed HAR-PSO-ESN hybrid model produces more accurate predictions on most of the cases, with an average R 2 (coefficient of determination) of 0.635, 0.510, and 0.298, an average mean squared error of 5.78 × 10−8, 5.78 × 10−8, and 1.16 × 10−7, for 1, 5, and 21 days ahead on the test set, respectively. The improvement is statistically significant with an average rank of 1.44 considering the three different datasets and forecasting horizons.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009003",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Autoregressive integrated moving average",
      "Autoregressive model",
      "Biology",
      "Computer science",
      "Echo state network",
      "Econometrics",
      "Horse",
      "Machine learning",
      "Mathematics",
      "Mean squared error",
      "Multilayer perceptron",
      "Paleontology",
      "Particle swarm optimization",
      "Recurrent neural network",
      "Statistics",
      "Stock market",
      "Time series",
      "Volatility (finance)"
    ],
    "authors": [
      {
        "surname": "Trierweiler Ribeiro",
        "given_name": "Gabriel"
      },
      {
        "surname": "Alves Portela Santos",
        "given_name": "André"
      },
      {
        "surname": "Cocco Mariani",
        "given_name": "Viviana"
      },
      {
        "surname": "dos Santos Coelho",
        "given_name": "Leandro"
      }
    ]
  },
  {
    "title": "Predicting risk-adjusted returns using an asset independent regime-switching model",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115576",
    "abstract": "Financial markets tend to switch between various market regimes over time, making stationarity-based models unsustainable. We construct a regime-switching model independent of asset classes for risk-adjusted return predictions based on hidden Markov models. This framework can distinguish between market regimes in a wide range of financial markets such as the commodity, currency, stock, and fixed income market. The proposed method employs sticky features that directly affect the regime stickiness and thereby changing turnover levels. An investigation of our metric for risk-adjusted return predictions is conducted by analyzing daily financial market changes for almost twenty years. Empirical demonstrations of out-of-sample observations obtain an accurate detection of bull, bear, and high volatility periods, improving risk-adjusted returns while keeping a preferable turnover level.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009799",
    "keywords": [
      "Asset (computer security)",
      "Biology",
      "Computer science",
      "Computer security",
      "Currency",
      "Econometrics",
      "Economics",
      "Finance",
      "Financial economics",
      "Financial market",
      "Horse",
      "Machine learning",
      "Market risk",
      "Markov chain",
      "Metric (unit)",
      "Monetary economics",
      "Operations management",
      "Paleontology",
      "Stock market",
      "Volatility (finance)"
    ],
    "authors": [
      {
        "surname": "Werge",
        "given_name": "Nicklas"
      }
    ]
  },
  {
    "title": "Optimization of selective withdrawal systems in hydropower reservoir considering water quality and quantity aspects",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115474",
    "abstract": "The selective withdrawal system (SWS) has often been used to alleviate downstream temperature pollutions and in-reservoir unacceptable oxygen concentrations whilst increasing hydropower energy generation. The aim of this study is to a) design the optimal locations of intakes in SWS, and, b) derive optimal parameterized selective withdrawal operation rules. The integration of water quality and quantity aspects to meet long-term environmental services and economic benefits is challenging. The main challenge lies in coupling search-based optimization algorithms with a numerical hydrodynamic and water quality simulation model (i.e., CE-QUAL-W2). To cope with the computational burdens of CE-QUAL-W2, surrogate models have been developed to represent the dynamics of temperature and water quality according to various SWSs. The surrogate models and the hydropower energy computation module were coupled with a multi-objective particle swarm optimization algorithm in an adaptive surrogate-based simulation–optimization framework (ASBSOF). The ASBSOF is applied in Karkheh reservoir, Andimeshk, Khuzestan, Iran, to address problems a and b. The results indicate that the optimal SWSs provide the advantage of sustainable management to balance the energy generation whilst minimizing environmental damages.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100885X",
    "keywords": [
      "Algorithm",
      "Biology",
      "Computer science",
      "Ecology",
      "Electrical engineering",
      "Engineering",
      "Environmental science",
      "Hydropower",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Operations research",
      "Parameterized complexity",
      "Particle swarm optimization",
      "Process engineering",
      "Surrogate model",
      "Water quality"
    ],
    "authors": [
      {
        "surname": "Saadatpour",
        "given_name": "Motahareh"
      },
      {
        "surname": "Javaheri",
        "given_name": "Shima"
      },
      {
        "surname": "Afshar",
        "given_name": "Abbas"
      },
      {
        "surname": "Sandoval Solis",
        "given_name": "Samuel"
      }
    ]
  },
  {
    "title": "Multi-attribute group decision-making considering opinion dynamics",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115479",
    "abstract": "Multi-attribute group decision-making (MAGDM) is a complicated cognitive process that involves evaluation of opinion expression, information fusion, and analysis of multi-source uncertainties. In the incipient stage of MAGDM, decision makers (DMs) express their opinions on alternatives for each attribute and tend to interact with others. Despite the opinions of DMs are dynamically evolved, the traditional information fusion techniques are always relying on the data acquired at a static decision-making time point, which leads to the loss of information. In this study, we introduce a novel MAGDM method considering opinion dynamics, which employs the 2-tuple linguistic model for the representation of linguistic judgements and the technique for order preference by similarity to an ideal solution (TOPSIS) as the decision-making framework, to reduce the loss of information from the dimension of opinion formation. Moreover, a modified opinion dynamics model is as well as developed by extending the hypothesis of bounded confidence, where the opinion similarity, the credibility of DMs, and the human bounded rationality are collectively regarded as influential factors within the process of opinion evolution. Subsequently, three simulations are carried out to verify the feasibility of the extended bounded confidence model. And finally, a case study of supplier selection, as a typical MAGDM problem, is implemented and a comparison analysis is conducted to demonstrate the rationality of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008903",
    "keywords": [
      "Artificial intelligence",
      "Bounded function",
      "Bounded rationality",
      "Computer science",
      "Credibility",
      "Data mining",
      "Economics",
      "Group decision-making",
      "Image (mathematics)",
      "Law",
      "Mathematical analysis",
      "Mathematics",
      "Microeconomics",
      "Operations research",
      "Political science",
      "Politics",
      "Preference",
      "Prospect theory",
      "Representation (politics)",
      "Similarity (geometry)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yupeng"
      },
      {
        "surname": "Liu",
        "given_name": "Meng"
      },
      {
        "surname": "Cao",
        "given_name": "Jin"
      },
      {
        "surname": "Wang",
        "given_name": "Xiaolin"
      },
      {
        "surname": "Zhang",
        "given_name": "Na"
      }
    ]
  },
  {
    "title": "DeepYield: A combined convolutional neural network with long short-term memory for crop yield forecasting",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115511",
    "abstract": "Crop yield forecasting is of great importance to crop market planning, crop insurance, harvest management, and optimal nutrient management. Commonly used approaches for crop prediction include but are not limited to conducting extensive manual surveys or using data from remote sensing. Considering the increasing amount of data provided by remote sensing imagery, this approach is becoming increasingly important for the task of crop yield forecasting and there is a need for more sophisticated approaches to extract the inherent spatiotemporal patterns of these data. Although considerable progress has been made in this field by using Deep Learning (DL) methods such as Convolutional Neural Networks (CNN), no study before has investigated the use of Convolutional Long Short-Term Memory (ConvLSTM) for crop yield forecasting. Here, we propose DeepYield, a combined structure, that integrates the ConvLSTM layers with the 3-Dimensional CNN (3DCNN) for more accurate and reliable spatiotemporal feature extraction. The models are trained by using county-based historical yield data and MODIS Land Surface Temperature (LST), Surface Reflectance (SR), and Land Cover (LC) data over 1836 primary soybean growing counites in the Contiguous United States (CONUS). The forecasting performance of the developed models is compared against the competing approaches including Decision Trees, CNN + GP, and CNN-LSTM and results indicate that DeepYield significantly outperforms these techniques and also performs better than both ConvLSTM and 3DCNN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421009210",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Convolutional neural network",
      "Crop yield",
      "Deep learning",
      "Feature (linguistics)",
      "Field (mathematics)",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pure mathematics"
    ],
    "authors": [
      {
        "surname": "Gavahi",
        "given_name": "Keyhan"
      },
      {
        "surname": "Abbaszadeh",
        "given_name": "Peyman"
      },
      {
        "surname": "Moradkhani",
        "given_name": "Hamid"
      }
    ]
  },
  {
    "title": "Social media-based opinion retrieval for product analysis using multi-task deep neural networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115388",
    "abstract": "Social media platforms are considered one of the most effective intermediaries for companies to interact with consumers. Social media-based decision support systems for the marketing domain are highly developed, but product development and innovation-oriented studies remain limited. This study offers a novel approach which utilises opinion retrieval theme along with sentiment analysis to support the decision-making process for product analysis and development. To achieve this aim, we propose an end-to-end social media-based opinion retrieval system and utilise machine learning and natural language processing techniques. Google Glass is chosen as a use-case as this product was unable to achieve its commercial targets despite its superior technological offerings. We design a multi-task deep neural network architecture for the training of sentiment prediction and opinion detection tasks. We first divide the tweets containing certain useful opinions and suggestions into two categories based on their sentiment labels. The negative tweets are analysed to identify product-related concerns, whereas the positive and neutral tweets are used to extract innovative ideas and identify new use cases for product development. We visualise and interpret the clusters of keywords extracted from each sentiment label group. Apart from methodological contributions, this study offers practical contributions for the next generations of smart glasses.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008125",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Business",
      "Computer science",
      "Data science",
      "Domain (mathematical analysis)",
      "Economics",
      "Geometry",
      "Information retrieval",
      "Law",
      "Management",
      "Marketing",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "New product development",
      "Operating system",
      "Political science",
      "Process (computing)",
      "Product (mathematics)",
      "Reading (process)",
      "Sentiment analysis",
      "Social media",
      "Task (project management)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Gozuacik",
        "given_name": "Necip"
      },
      {
        "surname": "Sakar",
        "given_name": "C. Okan"
      },
      {
        "surname": "Ozcan",
        "given_name": "Sercan"
      }
    ]
  },
  {
    "title": "Physics-guided deep neural network to characterize non-Newtonian fluid flow for optimal use of energy resources",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115409",
    "abstract": "Numerical simulations of non-Newtonian fluids are indispensable for optimization and monitoring of several industrial processes such as crude oil transportation, nuclear cooling, geothermal and fossil fuel production. The governing equations derived for non-Newtonian fluid models result in nonlinear differential equations. Thus, increasing the complexity even for simple geometries. The cumbersome numerical computation and rudimentary empirical solutions hinder faster analysis over a wide range of parameters. However, machine and deep learning methods have higher accuracy but rely heavily on the quality and amount of training data, and the solution may become inconclusive if data is sparse. In this research, a novel algorithm (Herschel Bulkley Network) is introduced to simulate the non-Newtonian fluid flow in a pipe using data redundant deep neural network (DNN) for fully developed, laminar, and incompressible flow conditions. The objective of this investigation is to develop a physics dominated DNN solely driven by minimizing residuals from the Navier-Stokes based governing equations, establishing benchmark research. Herschel-Bulkley model is used to approximate the complex rheological behavior of a non-Newtonian fluid. The proposed DNN algorithm is structured to incorporate initial/boundary conditions in cylindrical coordinates and approximate the solution without the aid of any simulated or training data. The simulated results and analysis demonstrate an excellent agreement between the proposed algorithm and non-Newtonian fluids flow attributes. The detailed parametric analysis exhibits the competency of the proposed algorithm to explain the rheological features. Monte-Carlo simulation is performed by propagating uncertainty to investigate the dominant parameters affecting simulated results. The uncertainty in fluid consistency index is responsible for higher variance in the calculated flow rate, while the least variation is observed due to fluid behavior index uncertainty. The performance of the algorithm is validated with experimental datasets. The statistical error estimation exhibits a mean absolute error of 11.5%, and root mean squared error of 0.87. A comprehensive analysis on training unsupervised DNN and adjusted hyperparameters is also highlighted to achieve expedite convergence.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008307",
    "keywords": [
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Compressibility",
      "Computer science",
      "Flow (mathematics)",
      "Fluid dynamics",
      "Geodesy",
      "Geology",
      "Herschel–Bulkley fluid",
      "Laminar flow",
      "Mathematical optimization",
      "Mathematics",
      "Mechanics",
      "Newtonian fluid",
      "Non-Newtonian fluid",
      "Physics"
    ],
    "authors": [
      {
        "surname": "Kumar",
        "given_name": "Abhishek"
      },
      {
        "surname": "Ridha",
        "given_name": "Syahrir"
      },
      {
        "surname": "Narahari",
        "given_name": "Marneni"
      },
      {
        "surname": "Ilyas",
        "given_name": "Suhaib Umer"
      }
    ]
  },
  {
    "title": "Automatic method for classifying COVID-19 patients based on chest X-ray images, using deep features and PSO-optimized XGBoost",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115452",
    "abstract": "The COVID-19 pandemic, which originated in December 2019 in the city of Wuhan, China, continues to have a devastating effect on the health and well-being of the global population. Currently, approximately 8.8 million people have already been infected and more than 465,740 people have died worldwide. An important step in combating COVID-19 is the screening of infected patients using chest X-ray (CXR) images. However, this task is extremely time-consuming and prone to variability among specialists owing to its heterogeneity. Therefore, the present study aims to assist specialists in identifying COVID-19 patients from their chest radiographs, using automated computational techniques. The proposed method has four main steps: (1) the acquisition of the dataset, from two public databases; (2) the standardization of images through preprocessing; (3) the extraction of features using a deep features-based approach implemented through the networks VGG19, Inception-v3, and ResNet50; (4) the classifying of images into COVID-19 groups, using eXtreme Gradient Boosting (XGBoost) optimized by particle swarm optimization (PSO). In the best-case scenario, the proposed method achieved an accuracy of 98.71%, a precision of 98.89%, a recall of 99.63%, and an F1-score of 99.25%. In our study, we demonstrated that the problem of classifying CXR images of patients under COVID-19 and non-COVID-19 conditions can be solved efficiently by combining a deep features-based approach with a robust classifier (XGBoost) optimized by an evolutionary algorithm (PSO). The proposed method offers considerable advantages for clinicians seeking to tackle the current COVID-19 pandemic.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008654",
    "keywords": [
      "Artificial intelligence",
      "Boosting (machine learning)",
      "Classifier (UML)",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Deep learning",
      "Disease",
      "Environmental health",
      "Infectious disease (medical specialty)",
      "Machine learning",
      "Medicine",
      "Operating system",
      "Particle swarm optimization",
      "Pathology",
      "Pattern recognition (psychology)",
      "Population",
      "Preprocessor",
      "Standardization"
    ],
    "authors": [
      {
        "surname": "Dias Júnior",
        "given_name": "Domingos Alves"
      },
      {
        "surname": "da Cruz",
        "given_name": "Luana Batista"
      },
      {
        "surname": "Bandeira Diniz",
        "given_name": "João Otávio"
      },
      {
        "surname": "França da Silva",
        "given_name": "Giovanni Lucca"
      },
      {
        "surname": "Junior",
        "given_name": "Geraldo Braz"
      },
      {
        "surname": "Silva",
        "given_name": "Aristófanes Corrêa"
      },
      {
        "surname": "de Paiva",
        "given_name": "Anselmo Cardoso"
      },
      {
        "surname": "Nunes",
        "given_name": "Rodolfo Acatauassú"
      },
      {
        "surname": "Gattass",
        "given_name": "Marcelo"
      }
    ]
  },
  {
    "title": "A group decision making method to manage internal and external experts with an application to anti-lung cancer drug selection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115379",
    "abstract": "With the changes of lifestyle and environment of people, the incidence rate of lung cancer has increased year by year, and lung cancer has become one of the most malignant tumors that threaten the health of people. Within this context, choosing appropriate anti-lung cancer drugs is of great significance for the treatment of lung cancer patients. To improve the accuracy of anti-lung cancer drug selection, it is necessary to invite many experts to participate in the evaluation process, and such a selection process can be regarded as a large-scale group decision-making problem. In existing group decision-making models, there are two hypotheses: one assumed that all experts are independent, while the other assumed that experts have certain relationships. However, in practical decision-making problems involving both internal and external experts, it is common that only some experts have mutual relationships. To address this issue, this paper proposes a large-scale group decision-making model considering the trust relationship between a set of experts. We divide experts into internal experts and external experts. The internal experts are supposed to be not independent of each other due to trust relationships, and we analyze the relationships between internal experts through the DEMATEL method. The external experts are supposed to be independent of each other. Considering the non-cooperative behaviors of experts, we provide a confidence-based adaptive consensus reaching mechanism for internal experts and a delegation-based adaptive consensus reaching mechanism for external experts. The two expert panels reach consensus through their separate consensus reaching mechanisms, and the moderator determines the optimal alternative by combining the final opinions of the two expert panels. Finally, an illustrative example about the selection of anti-non-small cell lung cancer drugs is presented to show the validity and practicality of the proposed model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008058",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Delegation",
      "Engineering",
      "Group decision-making",
      "Internal medicine",
      "Knowledge management",
      "Law",
      "Lung cancer",
      "Management science",
      "Medicine",
      "Paleontology",
      "Political science",
      "Psychology",
      "Risk analysis (engineering)",
      "Selection (genetic algorithm)",
      "Social psychology"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xiaofang"
      },
      {
        "surname": "Liao",
        "given_name": "Huchang"
      }
    ]
  },
  {
    "title": "An exact routing optimization model for bio-waste collection in the Brussels Capital Region",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115392",
    "abstract": "This paper presents a routing optimization model for a wide range of waste collection problems which allows for multiple depots with homogeneous, capacitated vehicles, intermediate stops at multiple processing facilities, and multiple pick-ups per waste collection location. By minimizing collection routing and vehicle investment costs, the model estimates the total transportation costs for a given network design and waste volumes to be collected at different demand points. The number of feasible routes is severely reduced by restricting the number of subsequent pick-up location visits. The model’s use is illustrated through assessing four future, realistic bio-waste collection scenarios of the Brussels Capital Region (BCR). The scenarios differ with respect to assumptions on future collection rates, joint versus separate collection of food and green waste, and number and locations of processing facilities. The results show that the highest cost reduction can be achieved through joint collection of food and green waste combined with three composting locations. Moreover, we found that introducing multiple processing facilities significantly increases the complexity of the waste collection problem.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008150",
    "keywords": [
      "Business",
      "Capital investment",
      "Combinatorics",
      "Computer network",
      "Computer science",
      "Data collection",
      "Economics",
      "Engineering",
      "Environmental economics",
      "Environmental science",
      "Finance",
      "Food waste",
      "Homogeneous",
      "Investment (military)",
      "Law",
      "Mathematics",
      "Municipal solid waste",
      "Operations research",
      "Political science",
      "Politics",
      "Routing (electronic design automation)",
      "Statistics",
      "Waste collection",
      "Waste management"
    ],
    "authors": [
      {
        "surname": "Lavigne",
        "given_name": "Carolien"
      },
      {
        "surname": "Beliën",
        "given_name": "Jeroen"
      },
      {
        "surname": "Dewil",
        "given_name": "Reginald"
      }
    ]
  },
  {
    "title": "Modelling and solving profit-oriented U-shaped partial disassembly line balancing problem",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115431",
    "abstract": "Disassembly lines are utilized frequently to disassemble the end-of-life products completely or partially to retain the valuable components for remanufacturing or recycling. This research introduces and solves the profit-oriented U-shaped partial disassembly line balancing problem (PUPDLBP) for the first time. A 0–1 integer linear programming model is formulated to tackle the PUPDLBP with AND/OR precedence, which is capable of solving the small-size instances optimally. As the considered problem is NP-hard, a novel discrete cuckoo search (DCS) algorithm is implemented and improved to solve the considered problem. The proposed DCS employs a two-phase decoding procedure to handle the precedence constraint, and new population update and new method to select and replace the abandoned individuals to achieve the proper balance between exploitation and exploration. Case studies demonstrate that the U-shaped line might obtain the larger total profit than a straight line. The comparative study shows that the improvements enhance the performance of DCS by a significant margin. The proposed algorithm outperforms CPLEX solver when solving large-sized instances and produce competing performance in comparison with 11 other algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008484",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Engineering",
      "Integer programming",
      "Linear programming",
      "Mathematical optimization",
      "Mathematics",
      "Mechanical engineering",
      "Remanufacturing",
      "Solver"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Zixiang"
      },
      {
        "surname": "Janardhanan",
        "given_name": "Mukund Nilakantan"
      }
    ]
  },
  {
    "title": "A multi-stage data mining approach for liquid bulk cargo volume analysis based on bill of lading data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115304",
    "abstract": "Liquid bulk cargo (LBC) volume analysis has received considerably great attention recently since LBC is a valuable and high-demand cargo. Thus, it is important to establish an analysis system for LBC volume, as it can help inform strategies for port planning and management. Nevertheless, LBC volume analysis is a challenging task for researchers because trends in LBC volume are highly volatile and non-stationary. In this paper, a new framework for enabling informative LBC volume analysis based on bill of lading (BL) data is proposed, which consists of three parts: item segmentation, exploratory volume analysis, and volume prediction. Firstly, an innovative item segmentation system using item texts of BL data was developed, which can generate subcategory as well as category information of LBC items that existing system cannot provide. Next, exploratory volume analysis was performed to understand the volume characteristics of each categorized and subcategorized item in terms of geography and timeline. Lastly, manifold learning- and deep learning-based time series techniques were proposed to increase LBC volume prediction accuracy compared with existing statistical models. The experimental results for volume prediction show the accuracy increased by 34% and 18% in average at category and subcategory levels over baseline models. It is believed that our proposed method will be helpful for stakeholders in maritime logistics, giving them the insights that they need to make better decisions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007338",
    "keywords": [
      "Computer science",
      "Data mining",
      "Data science",
      "Exploratory analysis",
      "Exploratory data analysis",
      "Mathematics",
      "Operations research",
      "Physics",
      "Quantum mechanics",
      "Statistics",
      "Timeline",
      "Volume (thermodynamics)"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Suhyeon"
      },
      {
        "surname": "Sohn",
        "given_name": "Wonho"
      },
      {
        "surname": "Lim",
        "given_name": "Dongcheol"
      },
      {
        "surname": "Lee",
        "given_name": "Junghye"
      }
    ]
  },
  {
    "title": "Semi-supervised anomaly detection for visual quality inspection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115275",
    "abstract": "In this paper a semi-supervised method for the detection of anomalies in both texture- and object-based product images is presented. The method exploits a pre-trained Convolutional Neural Network (CNN) autoencoder that is blended with a statistical-based transformation of the neural network embedding layer in order to remove anomalies from the input image. The “cleaned” version of the input image is then compared with the input image itself in order to spatially localize the anomalies. The method does not require a specific training of the CNN to be applied to a new class of product, but it requires a very fast domain adaptation based on only “anomaly-free” examples. Experiments conducted on a publicly available dataset made of fifteen texture- and object-based classes show that overall performance is better than the state of the art of about 4%. In the case of texture-based classes the proposed method outperforms the state of the art of about 13%. In the case of object-based classes, the proposed method reaches overall the same performance of the state of the art. In this case, apart from 3 cases, that is “bottle”, “transistor” and “metal nut”, the proposed method performs better than the state of the art in 7 object classes out of 10.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007065",
    "keywords": [
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Condensed matter physics",
      "Data mining",
      "Epistemology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quality (philosophy)",
      "Visual inspection"
    ],
    "authors": [
      {
        "surname": "Napoletano",
        "given_name": "Paolo"
      },
      {
        "surname": "Piccoli",
        "given_name": "Flavio"
      },
      {
        "surname": "Schettini",
        "given_name": "Raimondo"
      }
    ]
  },
  {
    "title": "A cellular-based evolutionary approach for the extraction of emerging patterns in massive data streams",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115419",
    "abstract": "Today, the number of existing devices generates immense amounts of data on a continuous basis that must be processed by new distributed data stream mining approaches. In this paper we present a new approach for extracting descriptive emerging patterns in massive data streams from different sources through Apache Kafka and Apache Spark Streaming whose objective is to monitor the state of the system with respect to a variable of interest. For this purpose, the proposed algorithm is a cellular-based multi-objective evolutionary fuzzy system that uses an informed strategy for efficient data processing and a re-initialisation and filtering mechanism to eliminate redundant and low-reliable patterns. The experimental study carried out demonstrates an interpretability improvement of 25% in the extraction of high-interest knowledge by the proposed algorithm, which would make it easier for experts to analyse the problem. Finally, the proposed algorithm is up to five times faster than another proposal on the processing of the same amount of data. In this experimental study, up to 750,000 instances have been processed in approximately four seconds.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008381",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Data stream",
      "Data stream mining",
      "Distributed computing",
      "Evolutionary algorithm",
      "Fuzzy logic",
      "Interpretability",
      "Machine learning",
      "Programming language",
      "SPARK (programming language)",
      "Stream processing",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "García-Vico",
        "given_name": "Ángel M."
      },
      {
        "surname": "Carmona",
        "given_name": "Cristóbal"
      },
      {
        "surname": "González",
        "given_name": "Pedro"
      },
      {
        "surname": "del Jesus",
        "given_name": "María J."
      }
    ]
  },
  {
    "title": "A modified Marine Predator Algorithm based on opposition based learning for tracking the global MPP of shaded PV system",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115253",
    "abstract": "Under partial shading condition, the power-voltage curve of the photovoltaic (PV) system contains several maximum power points (MPPs). Among these points, there is only single global and some local points. Accordingly, modern optimization algorithms are highly required to tackle this problem. However, the methods are considered as time consuming. Therefore, finding a new algorithm that capable to solve the problem of tracking global maximum power point (GMPP) with minimum number of population is highly appreciated. Several new straightforward methods as well as meta-heuristic approaches are exist. Recently, the Marine Predator Algorithm (MPA) has been developed for engineering applications. In this study, an alternative method of MPA, integrating Opposition Based Learning (OBL) strategy with Grey Wolf Optimizer (GWO), named MPAOBL-GWO, is proposed to cope with the implied weaknesses of classical MPA. Firstly, Opposition Based Learning (OBL) strategy is adopted to prevent MPA method from searching deflation and to obtain faster convergence rate. Besides, the GWO is also implemented to further improve the swarm agents’ local search efficiency. Due to that, the MPA explores the search space well better than exploiting it; so, this combination improves the efficiency of the MPA and avoids it from falling in local points. To verify the effectiveness of the enhanced method, the well-known CEC’17 test suite and the maximum power point tracking (MPPT) of photovoltaic (PV) system problem are solved. The obtained results illustrate the ability of the proposed MPAOBL-GWO based method to achieve the optimum solution compared with the original MPA, GWO and Particle Swarm Optimization (PSO). The findings revealed that, the proposed method can be viewed as an efficient and effective strategy for more complex optimization scenarios and the MPPT as well.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006850",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Ecology",
      "Environmental science",
      "Law",
      "Opposition (politics)",
      "Pedagogy",
      "Photovoltaic system",
      "Political science",
      "Politics",
      "Predation",
      "Predator",
      "Sociology",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Houssein",
        "given_name": "Essam H."
      },
      {
        "surname": "Mahdy",
        "given_name": "Mohamed A."
      },
      {
        "surname": "Fathy",
        "given_name": "Ahmed"
      },
      {
        "surname": "Rezk",
        "given_name": "Hegazy"
      }
    ]
  },
  {
    "title": "Exploring the effects of different Clustering Methods on a News Recommender System",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115341",
    "abstract": "News recommendations distinguishes from general content recommendations as it takes in consideration news freshness, sparsity, monotony and time. Recent works approach these features using hybrid Collaborative-Content-based Filtering methods, adapting clustering techniques to handle sparsity and monotony without considering the effects that different clustering methods may have over recommendation results. Such studies often evaluate the results of varying different parameters individually, ignoring possible interaction effects between them. They also base their results on metrics such as accuracy and recall that are sensitive to bias. To investigate the importance of clustering method selection to News Recommender System results we evaluated the effects of different traditional techniques in recommending news articles. We implemented an algorithm that used a hybrid Collaborative-Content-based Filtering method to incorporate user behavior, user interest, article popularity and time effect. The system uses an article selection method that built the recommendation set based on content features. With this algorithm, we examined the existence of interaction effects between the input parameters. We used a Gaussian regression process to explore the response surface while sequentially optimizing parameters. To avoid being misled by underlying biases we used Informedness, an accuracy metric that captures both positive and negative information from prediction results. Our results demonstrated that different clustering methods had a significant influence on the recommendation results. It was also found that a traditional hierarchical method outperformed optimization methods with important performance improvement. In addition, we demonstrated that parameters may interact with each other and that analyzing them separately may mislead interpretation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007697",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Collaborative filtering",
      "Computer science",
      "Data mining",
      "Economics",
      "Information retrieval",
      "Machine learning",
      "Metric (unit)",
      "Operations management",
      "Popularity",
      "Precision and recall",
      "Programming language",
      "Psychology",
      "Recommender system",
      "Selection (genetic algorithm)",
      "Set (abstract data type)",
      "Social psychology"
    ],
    "authors": [
      {
        "surname": "Ulian",
        "given_name": "Douglas Zanatta"
      },
      {
        "surname": "Becker",
        "given_name": "João Luiz"
      },
      {
        "surname": "Marcolin",
        "given_name": "Carla Bonato"
      },
      {
        "surname": "Scornavacca",
        "given_name": "Eusebio"
      }
    ]
  },
  {
    "title": "Developing an ANN-based early warning model for airborne particulate matters in river banks areas",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115421",
    "abstract": "The adverse effects of dust storms such as air quality degradation, reduced visibility and property damage are particularly prominent in estuarian regions. To reduce significant property and health risks posed by such river dust storms, an early-warning system is a useful tool that is urgently needed. In this case study of an area near an estuary, an artificial neural network with various sampling strategies is proposed for building an early-warning model. Records of past river episodes involving high concentrations of airborne particulate matter were collected and used for modeling. Among the three models developed, Model III which considers the characteristics of both environmental media and pollution sources was most accurate among other Models in predicting the severity of an estuarian dust events. The results revealed that the proposed forecasting model can efficiently predict high-pollution events and thus provide useful information enabling administrators and the public to implement preemptive disaster mitigative measures to avoid the negative health consequences of dust storms within estuarian regions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100840X",
    "keywords": [
      "Air pollution",
      "Air quality index",
      "Biology",
      "Chemistry",
      "Computer science",
      "Dust storm",
      "Early warning system",
      "Ecology",
      "Environmental planning",
      "Environmental resource management",
      "Environmental science",
      "Estuary",
      "Geography",
      "Geology",
      "Meteorology",
      "Oceanography",
      "Organic chemistry",
      "Particulates",
      "Pollution",
      "Storm",
      "Telecommunications",
      "Visibility",
      "Warning system"
    ],
    "authors": [
      {
        "surname": "Lan Phuong Nguyen",
        "given_name": "Kieu"
      },
      {
        "surname": "Hsun Chuang",
        "given_name": "Yen"
      },
      {
        "surname": "Yu",
        "given_name": "Ruey-Fang"
      },
      {
        "surname": "Chen",
        "given_name": "Ho-Wen"
      }
    ]
  },
  {
    "title": "Detecting conflicts in collaborative learning through the valence change of atomic interactions",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115291",
    "abstract": "Naturally, every collaboration will bring conflicts that can affect the performance of a team. The earlier a conflict is detected and managed in a collaborative group, the better. Detecting and tracking conflicts in Computer-Supported Collaborative Learning (CSCL) is laborious work. If the teacher does it, the intervention may be out of time. Although written dialogues in groups having a conflict reveal the increment of negative emotions in comparison to non-conflict dialogues, a classifier that only uses statistics of the valence of consecutive messages in a window of the talk shows poor performance. This paper proposes to use features based on the valence change between a message and its response. In this way the algorithm focuses in the kind of interaction. We study different implementations of the bootstrap aggregating technique to detect conflicts. Results obtained show the viability of the proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007223",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Collaborative learning",
      "Computer science",
      "Implementation",
      "Knowledge management",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Valence (chemistry)"
    ],
    "authors": [
      {
        "surname": "Lescano",
        "given_name": "Germán"
      },
      {
        "surname": "Torres-Jimenez",
        "given_name": "Jose"
      },
      {
        "surname": "Costaguta",
        "given_name": "Rosanna"
      },
      {
        "surname": "Amandi",
        "given_name": "Analía"
      },
      {
        "surname": "Lara-Alvarez",
        "given_name": "Carlos"
      }
    ]
  },
  {
    "title": "Explainable artificial intelligence for manufacturing cost estimation and machining feature visualization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115430",
    "abstract": "Studies on manufacturing cost prediction based on deep learning have begun in recent years, but the cost prediction rationale cannot be explained because the models are still used as a black box. This study aims to propose a manufacturing cost prediction process for 3D computer-aided design (CAD) models using explainable artificial intelligence. The proposed process can visualize the machining features of the 3D CAD model that are influencing the increase in manufacturing costs. The proposed process consists of (1) data collection and pre-processing, (2) 3D deep learning architecture exploration, and (3) visualization to explain the prediction results. The proposed deep learning model shows high predictability of manufacturing cost for the computer numerical control (CNC) machined parts. In particular, using 3D gradient-weighted class activation mapping proves that the proposed model not only can detect the CNC machining features but also can differentiate the machining difficulty for the same feature. Using the proposed process, we can provide a design guidance to engineering designers in reducing manufacturing costs during the conceptual design phase. We can also provide real-time quotations and redesign proposals to online manufacturing platform customers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008472",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "CAD",
      "Computer science",
      "Computer-integrated manufacturing",
      "Engineering",
      "Engineering drawing",
      "Feature (linguistics)",
      "Feature recognition",
      "Industrial engineering",
      "Linguistics",
      "Machine learning",
      "Machining",
      "Manufacturing cost",
      "Manufacturing engineering",
      "Mechanical engineering",
      "Operating system",
      "Philosophy",
      "Physics",
      "Predictability",
      "Process (computing)",
      "Quantum mechanics",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Yoo",
        "given_name": "Soyoung"
      },
      {
        "surname": "Kang",
        "given_name": "Namwoo"
      }
    ]
  },
  {
    "title": "TransExplain: Using neural networks to find suitable explanations for Chinese phrases",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115440",
    "abstract": "When reading an article, especially a professional article, we often encounter words or phrases that we don't recognize. They may be specific domain terms or emerging entities. When we can't guess their meaning from the article, we generally refer to the terminology dictionary or search for related content on the Internet to understand them. Some researchers have used natural language generation (NLG) models to explain these unknown phrases in recent years automatically. Still, current NLG models have difficulties generating long sentences with good coherence, and they are difficult to generate multiple sentences that describe unknown phrases from different angles. Therefore, this paper proposes a model that can judge whether an existing sentence can explain a certain phrase, called TransExplain. TransExplain can use LSTM, convolutional neural network, and attention mechanism to extract multiple sentence features and map them to a fixed-dimensional semantic feature vector. By calculating the cosine similarity between the semantic feature vector and the unknown phrase vector, it is judged whether the sentence can explain the semantics of the unknown phrase. And a loss function called positive and negative means square error is introduced to improve the model's ability that distinguishes negative examples. For this task, we provided a Chinese dataset containing phrases and explanation pairs in 7 important domains. On this dataset, TransExplain can achieve better results than previous similar tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100854X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Cosine similarity",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Linguistics",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Phrase",
      "Programming language",
      "Semantics (computer science)",
      "Sentence",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Rongsheng"
      },
      {
        "surname": "Li",
        "given_name": "Zesong"
      },
      {
        "surname": "Huang",
        "given_name": "Shaobin"
      },
      {
        "surname": "Liu",
        "given_name": "Ye"
      },
      {
        "surname": "Qiu",
        "given_name": "Jiyu"
      }
    ]
  },
  {
    "title": "DeepShip: An underwater acoustic benchmark dataset and a separable convolution based autoencoder for classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115270",
    "abstract": "Underwater acoustic classification is a challenging problem because of presence of high background noise and complex sound propagation patterns in the sea environment. Various algorithms proposed in last few years used own privately collected datasets for design and validation. Such data is not publicly available. To conduct research in this field, there is a dire need of publicly available dataset. To bridge this gap, we construct and present an underwater acoustic dataset, named DeepShip, which consists of 47 h and 4 min of real world underwater recordings of 265 different ships belong to four classes. The proposed dataset includes recording from throughout the year with different sea states and noise levels. The presented dataset will not only help to evaluate the performance of existing algorithms but it shall also benefit the research community in future. Using the proposed dataset, we also conducted a comprehensive study of various machine learning and deep learning algorithms on six time–frequency based extracted features. In addition, we propose a novel separable convolution based autoencoder network for better classification accuracy. Experiments results, which are compared based on classification accuracy, precision, recall, f1-score, and analyzed by using paired sampled statistical t-test, show that the proposed network achieves classification accuracy of 77.53% using CQT feature, which is better than as achieved by other methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007016",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Benchmark (surveying)",
      "Computer science",
      "Convolution (computer science)",
      "Data mining",
      "Deep learning",
      "Feature (linguistics)",
      "Field (mathematics)",
      "Geodesy",
      "Geography",
      "Geology",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Noise (video)",
      "Oceanography",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pure mathematics",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Irfan",
        "given_name": "Muhammad"
      },
      {
        "surname": "Jiangbin",
        "given_name": "Zheng"
      },
      {
        "surname": "Ali",
        "given_name": "Shahid"
      },
      {
        "surname": "Iqbal",
        "given_name": "Muhammad"
      },
      {
        "surname": "Masood",
        "given_name": "Zafar"
      },
      {
        "surname": "Hamid",
        "given_name": "Umar"
      }
    ]
  },
  {
    "title": "Probabilistic modeling approach for interpretable inference and prediction with data for sepsis diagnosis",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115333",
    "abstract": "Sepsis is a serious disease that can cause death. It is important to predict sepsis within the early stages after the presence of sepsis symptoms. In this paper, a new probabilistic modeling approach is used to establish classifiers for sepsis diagnosis. This approach is characterized by unique strong interpretability, which is reflected in three aspects: (1) evidence acquisition based on likelihood analysis, (2) probabilistic rule-based inference, and (3) parameters optimization using machine learning algorithms. Four-fold cross-validation is used to train and validate classifiers established by the new approach and alternative ones. Results show that in terms of classification capability, the classifier established by the new approach generally performs better than the majority of alternative classifiers for sepsis diagnosis, and close to the best one. As the classifier also features an inherent interpretability, it can be used as a tool for supporting diagnostic decision-making in sepsis diagnosis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007624",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Immunology",
      "Inference",
      "Interpretability",
      "Machine learning",
      "Medicine",
      "Naive Bayes classifier",
      "Probabilistic classification",
      "Probabilistic logic",
      "Sepsis",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Yao",
        "given_name": "Shuaiyu"
      },
      {
        "surname": "Yang",
        "given_name": "Jian-Bo"
      },
      {
        "surname": "Xu",
        "given_name": "Dong-Ling"
      },
      {
        "surname": "Dark",
        "given_name": "Paul"
      }
    ]
  },
  {
    "title": "Image inpainting network for filling large missing regions using residual gather",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115381",
    "abstract": "Filling arbitrary missing regions with visually plausible contents is a challenging task. Existing methods have accomplished promising performance for inpainting regular or small defects. However, filling large continuous holes is still difficult due to out of step with the performance requirement for inferring large region contents. Considering that the model needs to predict more missing contents with less information when filling large holes, we first introduce a Residual Gather Module (RGM) for high prediction performance by fully taking advantage of residual learning and capturing feature information from a hierarchical distribution. To obtain both high-quality textures and deep model performance, we propose a novel image translation architecture. Different from U-Net, our skip method is semi-complete form. It eliminates losing details caused by downsampling and is capable of designing very deep generative models. Experiments demonstrate that the proposed method exhibits outstanding performance on filling large missing regions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008071",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Economics",
      "Feature (linguistics)",
      "Filling-in",
      "Gene",
      "Geometry",
      "Image (mathematics)",
      "Image translation",
      "Inpainting",
      "Linguistics",
      "Machine learning",
      "Management",
      "Mathematics",
      "Messenger RNA",
      "Missing data",
      "Net (polyhedron)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Residual",
      "Task (project management)",
      "Translation (biology)",
      "Upsampling"
    ],
    "authors": [
      {
        "surname": "Xiao",
        "given_name": "Qingguo"
      },
      {
        "surname": "Li",
        "given_name": "Guangyao"
      },
      {
        "surname": "Chen",
        "given_name": "Qiaochuan"
      }
    ]
  },
  {
    "title": "Time series classification via topological data analysis",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115326",
    "abstract": "In this paper, we develop topological data analysis methods for classification tasks on univariate time series. As an application, we perform binary and ternary classification tasks on two public datasets that consist of physiological signals collected under stress and non-stress conditions. We accomplish our goal by using persistent homology to engineer stable topological features after we use a time delay embedding of the signals and perform a subwindowing instead of using windows of fixed length. The combination of methods we use can be applied to any univariate time series and allows us to reduce noise and use long window sizes without incurring an extra computational cost. We then use machine learning models on the features we algorithmically engineered to obtain higher accuracies with fewer features.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007557",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Binary classification",
      "Binary number",
      "Biology",
      "Computer science",
      "Data mining",
      "Embedding",
      "Image (mathematics)",
      "Machine learning",
      "Mathematics",
      "Multivariate statistics",
      "Noise (video)",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Persistent homology",
      "Programming language",
      "Series (stratigraphy)",
      "Support vector machine",
      "Ternary operation",
      "Time series",
      "Topological data analysis",
      "Univariate"
    ],
    "authors": [
      {
        "surname": "Karan",
        "given_name": "Alperen"
      },
      {
        "surname": "Kaygun",
        "given_name": "Atabey"
      }
    ]
  },
  {
    "title": "Analyzing bank “black boxes”: A two-stage Nerlovian profit inefficiency model",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115405",
    "abstract": "The paper introduces a two-stage network Nerlovian profit inefficiency estimator, modelling bank’ “black box” production processes. The unique features of the model include the modelling of a risk factor measured by period non-performing loans (NPLs). Specifically, NPLs enter in the second stage of banks production processes, alongside a human capital factor. The model is applied to Japanese local Shinkin banks over the period 2008–2015. The findings highlight the importance of incorporating previous period NPLs and human capital factors when modelling bank production processes. Moreover, we provide evidence that previous period NPLs act as an endogenous risk factor having a negative effect on bank profit efficiency levels. The empirical findings suggest that the human capital factor has a positive effect on bank profit efficiency levels. Finally, both the effect of NPLs and human capital factor has an asymmetric effect on bank performance levels.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008277",
    "keywords": [
      "Business",
      "Computer science",
      "Econometrics",
      "Economic growth",
      "Economics",
      "Human capital",
      "Inefficiency",
      "Microeconomics",
      "Profit (economics)"
    ],
    "authors": [
      {
        "surname": "Fukuyama",
        "given_name": "Hirofumi"
      },
      {
        "surname": "Hashimoto",
        "given_name": "Atsuo"
      },
      {
        "surname": "Matousek",
        "given_name": "Roman"
      },
      {
        "surname": "Tzeremes",
        "given_name": "Nickolaos G."
      }
    ]
  },
  {
    "title": "Big data classification using heterogeneous ensemble classifiers in Apache Spark based on MapReduce paradigm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115369",
    "abstract": "In this era of big data, processing large scale data efficiently and accurately has become a challenging problem. Ensemble classification is a type of supervised learning that uses multiple experts to generate the final output. It provides a way to classify data more accurately. As a result of using multiple classifiers, they are often more complicated than single classifiers, especially for big data problems. Apache Spark is a unified analytics engine for big data processing which provides a scalable framework to analyze the data. In this paper, we first extend our previous work and design a distributed heterogeneous ensemble classifier inspired by the boosting approach, which is capable of dealing with big datasets. Using heterogeneous classifiers makes it possible to have more diverse classifiers, and consequently, a more accurate classifier is obtained. Then, we present the Spark version of the proposed approach to speed up our heterogeneous ensemble classifier using the MapReduce paradigm. In order to evaluate our approach, we have applied it to seven big datasets. Extensive experimental results indicate the superiority of the proposed method over the existing ensemble algorithms implemented by Spark MLlib in terms of the classification accuracy, performance, and scalability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100796X",
    "keywords": [
      "Artificial intelligence",
      "Big data",
      "Boosting (machine learning)",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Database",
      "Ensemble learning",
      "Machine learning",
      "Programming language",
      "SPARK (programming language)",
      "Scalability"
    ],
    "authors": [
      {
        "surname": "Kadkhodaei",
        "given_name": "Hamidreza"
      },
      {
        "surname": "Eftekhari Moghadam",
        "given_name": "Amir Masoud"
      },
      {
        "surname": "Dehghan",
        "given_name": "Mehdi"
      }
    ]
  },
  {
    "title": "Towards asymmetric uncertainty modeling in designing General Type-2 Fuzzy classifiers for medical diagnosis",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115370",
    "abstract": "One of the most studied application areas of intelligent systems is the classification area, and this is because classification covers a wide range of real-world problems. Some examples are fault-diagnosis, image segmentation, medical diagnosis, among others. In most cases, the intelligent systems designed for the solution of this kind of problems are based on supervised learning, which is based on learning how to classify with previous datasets for finding relations between the inputs and outputs. The main focus of the present paper is the supervised generation of general type-2 fuzzy classifiers with a new strategy for modeling data uncertainty. The proposed methodology includes a mix of concepts, such as the use of embedded type-1 membership functions, statistical concepts such as the quartiles, and nature inspired optimization methods. The classifiers generated with the proposed methodology are compared with respect to other general type-2 fuzzy classifiers based on symmetric uncertainty to evaluate their performance, in this way obtaining interesting results for medical diagnosis with benchmark data sets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007971",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Composite material",
      "Computer science",
      "Data mining",
      "Focus (optics)",
      "Fuzzy classification",
      "Fuzzy logic",
      "Fuzzy set",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Materials science",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Range (aeronautics)"
    ],
    "authors": [
      {
        "surname": "Ontiveros-Robles",
        "given_name": "Emanuel"
      },
      {
        "surname": "Castillo",
        "given_name": "Oscar"
      },
      {
        "surname": "Melin",
        "given_name": "Patricia"
      }
    ]
  },
  {
    "title": "Generalizing state-of-the-art object detectors for autonomous vehicles in unseen environments",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115417",
    "abstract": "In scene understanding for autonomous vehicles (AVs), models trained on the available datasets fail to generalize well to the complex, real-world scenarios with higher dynamics. In this work, we attempt to handle the distribution mismatch by employing the generative adversarial network (GAN) and weather modeling to strengthen the intra-domain data. We also alleviate the fragility of our trained models against natural distortions with state-of-the-art augmentation approaches. Finally, we assess our method for cross-domain object detection through CARLA simulation. Our experiments demonstrate that: (1) Augmenting training class with even limited intra-domain data captured from the adverse weather conditions boosts the generalization of the two kinds of object detectors; (2) Exploiting GANs and weather modeling to elaborately simulate the adverse, intra-domain weather conditions manages to surmount the adverse data scarcity issue for intra-domain object detection; (3) A combination of Augmix and style augmentations not only can promote the robustness of our trained models against different natural distortions but also can boost their performance in the cross-domain object detection; (4) Training GANs for unsupervised image-to-image translation by means of the existing, large-scale datasets outside of our training domain is found beneficial to alleviate image-based and instance-based domain shifts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008368",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Domain (mathematical analysis)",
      "Gene",
      "Generalization",
      "Generative grammar",
      "Image (mathematics)",
      "Image translation",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Khosravian",
        "given_name": "Amir"
      },
      {
        "surname": "Amirkhani",
        "given_name": "Abdollah"
      },
      {
        "surname": "Kashiani",
        "given_name": "Hossein"
      },
      {
        "surname": "Masih-Tehrani",
        "given_name": "Masoud"
      }
    ]
  },
  {
    "title": "Multi-obstacle path planning and optimization for mobile robot",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115445",
    "abstract": "In the past few decades, many results have been achieved in the research of mobile robot path planning, and they have been applied in simple scenarios, such as factory AGV, bank guide robot. However, path planning in highly dense and complex scenarios has become an important challenge for applications. Robots face dense map and complex obstacles and hardly find out an optimal path within a reasonable period, such as unmanned vehicles in freight ports and rescue robots in earthquake environment. Therefore, a multi-obstacle path planning and optimization method is proposed. In order to simplify complex environmental obstacles, the obstacles will be divided into basis obstacles and extension obstacles. Firstly, the basis obstacles and their contour point sets are determined according to the starting point and goal point. Furthermore, the basis obstacles are optimized by convex hulls, and then the corresponding basis point set is obtained. Secondly, the extension obstacles are determined by the basis point set, starting point and goal point, and then the corresponding extension point set is generated. After that, a path planner is designed by the multi-objective D* Lite algorithm for distance and smoothness in order to get reasonable and optimized path in a complex environment. Moreover, the path is smoothed by cubic bezier curves to fit the kinematic model of the robot. Finally, The proposed method conduct comparative experiments with other algorithms to verify its accuracy and computational efficiency of planning in complex environments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008599",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Basis (linear algebra)",
      "Classical mechanics",
      "Computer science",
      "Geography",
      "Geometry",
      "Kinematics",
      "Mathematical optimization",
      "Mathematics",
      "Mobile robot",
      "Motion planning",
      "Obstacle",
      "Obstacle avoidance",
      "Path (computing)",
      "Physics",
      "Point (geometry)",
      "Programming language",
      "Robot",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Deng",
        "given_name": "Xin"
      },
      {
        "surname": "Li",
        "given_name": "Ruifeng"
      },
      {
        "surname": "Zhao",
        "given_name": "Lijun"
      },
      {
        "surname": "Wang",
        "given_name": "Ke"
      },
      {
        "surname": "Gui",
        "given_name": "Xichun"
      }
    ]
  },
  {
    "title": "Learning attention embeddings based on memory networks for neural collaborative recommendation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115439",
    "abstract": "Recently, deep learning has dominated the recommender system, as it is able to effectively capture nonlinear and nontrivial user–item relationships, and perform complex nonlinear transformations. However, there are still some issues with respects to the existing methods. Firstly, they always treat user–item interactions independently, and may fail to cover more complex and hidden information that is inherently implicit in the local neighborhood surrounding an interaction sample. Secondly, by quantifying the dependence degree of user–item sequences, it demonstrates that both short-term and long-term dependent behavioral patterns co-exist. Unfortunately, typical deep learning methods might be problematic when coping with very long-term sequential dependencies. To address these issues, we propose a novel unified neural collaborative recommendation algorithm that capitalizes on memory networks for learning attention embedding from implicit interaction (NCRAE). Particularly, the attention is capable of learning the relative importance of different users and items from user–item interaction sequences, which provides a better solution for concentrating on inputs and helps to better memorize long-term sequential dependencies. Extensive experiments on three real-world datasets show significant improvements of our proposed NCRAE algorithm over the competitive methods. Empirical evidence shows that using memory networks for learning attention embeddings of users’ implicit interaction yields better recommendation performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008538",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Cognitive psychology",
      "Computer science",
      "Embedding",
      "Machine learning",
      "Memorization",
      "Physics",
      "Psychology",
      "Quantum mechanics",
      "Recommender system",
      "Term (time)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yihao"
      },
      {
        "surname": "Liu",
        "given_name": "Xiaoyang"
      }
    ]
  },
  {
    "title": "An uncertainty-induced axiomatic foundation of the analytic hierarchy process and its implication",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115427",
    "abstract": "Uncertainty is always encountered by decision makers (DMs) when addressing a complex decision-making problem. The existing axiomatic foundation of the analytic hierarchy process (AHP) cannot reflect the uncertainty experienced by DMs. In this paper, it is first to find that the uncertainty experienced by DMs can be characterized by the non-reciprocal property of pairwise comparisons. The novel concept of reciprocal symmetry breaking (RSB) is proposed to capture the situation without reciprocal property. Then an uncertainty-induced axiomatic foundation of the AHP model is reported, where the RSB is one of the proposed axioms. Some interesting results are derived from the new axioms involving the concept of approximate consistency and the method of eliciting priorities. An index is constructed to measure approximate consistency degree of pairwise comparison matrices (PCMs). Some comparisons with the typical AHP model are offered by carrying out some numerical examples. The obtained results reveal that the proposed axiomatic foundation and the derived facts form a novel operational basis of the AHP model under uncertainty.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008447",
    "keywords": [
      "Analytic hierarchy process",
      "Archaeology",
      "Artificial intelligence",
      "Axiom",
      "Axiomatic design",
      "Axiomatic system",
      "Computer science",
      "Consistency (knowledge bases)",
      "Economics",
      "Epistemology",
      "Foundation (evidence)",
      "Geometry",
      "Hierarchy",
      "History",
      "Lean manufacturing",
      "Linguistics",
      "Market economy",
      "Mathematical economics",
      "Mathematical optimization",
      "Mathematics",
      "Operations management",
      "Pairwise comparison",
      "Philosophy",
      "Property (philosophy)",
      "Reciprocal"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Fang"
      },
      {
        "surname": "Qiu",
        "given_name": "Mei-Yu"
      },
      {
        "surname": "Zhang",
        "given_name": "Wei-Guo"
      }
    ]
  },
  {
    "title": "A framework for 3D tracking of frontal dynamic objects in autonomous cars",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115343",
    "abstract": "Both recognition and 3D tracking of frontal dynamic objects are crucial problems in an autonomous vehicle, while depth estimation as an essential issue becomes a challenging problem using a monocular camera. Since both camera and objects are moving, the issue can be formed as a structure from motion (SFM) problem. In this paper, to elicit features from an image, the YOLOv3 approach is utilized beside an OpenCV tracker. Subsequently, to obtain the lateral and longitudinal distances, a nonlinear SFM model is considered alongside a state-dependent Riccati equation (SDRE) filter and a newly developed observation model. Additionally, a switching method in the form of switching estimation error covariance is proposed to enhance the robust performance of the SDRE filter. The stability analysis of the presented filter is conducted on a class of discrete nonlinear systems. Furthermore, the ultimate bound of estimation error caused by model uncertainties is analytically obtained to investigate the switching significance. Simulations are reported to validate the performance of the switched SDRE filter. Finally, real-time experiments are performed through a multi-thread framework implemented on a Jetson TX2 board, while radar data is used for the evaluation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007715",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Control (management)",
      "Control theory (sociology)",
      "Covariance",
      "Differential equation",
      "Filter (signal processing)",
      "Mathematical analysis",
      "Mathematics",
      "Nonlinear system",
      "Physics",
      "Quantum mechanics",
      "Riccati equation",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Lotfi",
        "given_name": "Faraz"
      },
      {
        "surname": "Taghirad",
        "given_name": "Hamid D."
      }
    ]
  },
  {
    "title": "New benchmark algorithm for hybrid flowshop scheduling with identical machines",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115422",
    "abstract": "With many industrial applications in the production sector, the hybrid flowshop scheduling problem (HFSP) has received wide recognition in the scheduling literature. Given the NP-hard nature of the HFSP, which is characterized by highly intractable solution spaces, effective solution approaches are of particular interest to facilitate its real-world use cases. This study proposes a new benchmark metaheuristic, the Chaos-enhanced Simulated Annealing (CSA) algorithm to minimize makespan in the HFSP with identical machines. A recently published testbed is used to evaluate the performance of CSA against that of the upper bounds/best-known solutions in the literature. In addition to improving the upper bounds, the computational results revealed that CSA performed very well in terms of computational efficiency and stability. The proposed CSA can serve as a strong benchmark algorithm for developing more algorithms to effectively and efficiently solve HFSPs and its extensions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008411",
    "keywords": [
      "Algorithm",
      "Benchmark (surveying)",
      "Computer network",
      "Computer science",
      "Geodesy",
      "Geography",
      "Job shop scheduling",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Operating system",
      "Schedule",
      "Scheduling (production processes)",
      "Simulated annealing",
      "Testbed",
      "Upper and lower bounds"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Shih-Wei"
      },
      {
        "surname": "Cheng",
        "given_name": "Chen-Yang"
      },
      {
        "surname": "Pourhejazy",
        "given_name": "Pourya"
      },
      {
        "surname": "Ying",
        "given_name": "Kuo-Ching"
      },
      {
        "surname": "Lee",
        "given_name": "Chia-Hui"
      }
    ]
  },
  {
    "title": "Assessment of the handcart pushing and pulling safety by using deep learning 3D pose estimation and IoT force sensors",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115371",
    "abstract": "Pushing and pulling (P&P) are common and repetitive tasks in industry, which non-ergonomic execution is among major causes of musculoskeletal disorders (MSD). The current safety management of P&P assumes restrictions of maximal weight, distance, height – while variable individual parameters (such as the P&P pose ergonomic) remain difficult to account for with the standardized guides. Since manual detection of unsafe P&P acts is subjective and inefficient, the aim of this study was to utilize IoT force sensors and IP cameras to detect unsafe P&P acts timely and objectively. Briefly, after the IoT module detects moments with increased P&P forces, the assessment of pose ergonomics was performed from the employee pose reconstructed with the VIBE algorithm. The experiments showed that turn-points correspond to the high torsion of torso, and that in such moments poses are commonly non ergonomic (although P&P forces are below values defined as critical in previous studies – their momentum cause serious load on the human body). Moreover, the analysis revealed that the loading/unloading of a cargo are also moments of frequent unsafe P&P acts – although they are commonly neglected when studying P&P. The experimental validation of the solution showed good agreement with motion sensors and high potential for monitoring and improving P&P workplace safety. Accordingly, future research will be directed towards: 1) acquisition of P&P data sets for direct recognition and classification of unsafe P&P acts; 2) incorporation of wearable sensors (EMG and EEG) for detecting fatigue and decrease of physical abilities.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007983",
    "keywords": [
      "Anatomy",
      "Artificial intelligence",
      "Bioinformatics",
      "Biology",
      "Computer science",
      "Computer security",
      "Embedded system",
      "Internet of Things",
      "Machine learning",
      "Medicine",
      "Physical medicine and rehabilitation",
      "Pose",
      "Safety monitoring",
      "Simulation",
      "Torso",
      "Wearable computer"
    ],
    "authors": [
      {
        "surname": "Vukicevic",
        "given_name": "Arso M."
      },
      {
        "surname": "Macuzic",
        "given_name": "Ivan"
      },
      {
        "surname": "Mijailovic",
        "given_name": "Nikola"
      },
      {
        "surname": "Peulic",
        "given_name": "Aleksandar"
      },
      {
        "surname": "Radovic",
        "given_name": "Milos"
      }
    ]
  },
  {
    "title": "G-SOMO: An oversampling approach based on self-organized maps and geometric SMOTE",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115230",
    "abstract": "Traditional supervised machine learning classifiers are challenged to learn highly skewed data distributions as they are designed to expect classes to equally contribute to the minimization of the classifiers cost function. Moreover, the classifiers design expects equal misclassification costs, causing a bias for overrepresented classes. Different strategies have been proposed to correct this issue. The modification of the data set has become a common practice since the procedure is generalizable to all classifiers. Various algorithms to rebalance the data distribution through the creation of synthetic instances were proposed in the past. In this paper, we propose a new oversampling algorithm named G-SOMO. The algorithm identifies optimal areas to create artificial data instances in an informed manner and utilizes a geometric region during the data generation process to increase their variability. Our empirical results on 69 datasets, validated with different classifiers and metrics against a benchmark of commonly used oversampling methods show that G-SOMO consistently outperforms competing oversampling methods. Additionally, the statistical significance of our results is established.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100662X",
    "keywords": [
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Benchmark (surveying)",
      "Biology",
      "Computer network",
      "Computer science",
      "Data mining",
      "Data set",
      "Evolutionary biology",
      "Function (biology)",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Minification",
      "Operating system",
      "Oversampling",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Douzas",
        "given_name": "Georgios"
      },
      {
        "surname": "Rauch",
        "given_name": "Rene"
      },
      {
        "surname": "Bacao",
        "given_name": "Fernando"
      }
    ]
  },
  {
    "title": "Assessment of the handcart pushing and pulling safety by using deep learning 3D pose estimation and IoT force sensors",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115371",
    "abstract": "Pushing and pulling (P&P) are common and repetitive tasks in industry, which non-ergonomic execution is among major causes of musculoskeletal disorders (MSD). The current safety management of P&P assumes restrictions of maximal weight, distance, height – while variable individual parameters (such as the P&P pose ergonomic) remain difficult to account for with the standardized guides. Since manual detection of unsafe P&P acts is subjective and inefficient, the aim of this study was to utilize IoT force sensors and IP cameras to detect unsafe P&P acts timely and objectively. Briefly, after the IoT module detects moments with increased P&P forces, the assessment of pose ergonomics was performed from the employee pose reconstructed with the VIBE algorithm. The experiments showed that turn-points correspond to the high torsion of torso, and that in such moments poses are commonly non ergonomic (although P&P forces are below values defined as critical in previous studies – their momentum cause serious load on the human body). Moreover, the analysis revealed that the loading/unloading of a cargo are also moments of frequent unsafe P&P acts – although they are commonly neglected when studying P&P. The experimental validation of the solution showed good agreement with motion sensors and high potential for monitoring and improving P&P workplace safety. Accordingly, future research will be directed towards: 1) acquisition of P&P data sets for direct recognition and classification of unsafe P&P acts; 2) incorporation of wearable sensors (EMG and EEG) for detecting fatigue and decrease of physical abilities.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007983",
    "keywords": [
      "Anatomy",
      "Artificial intelligence",
      "Bioinformatics",
      "Biology",
      "Computer science",
      "Computer security",
      "Embedded system",
      "Internet of Things",
      "Machine learning",
      "Medicine",
      "Physical medicine and rehabilitation",
      "Pose",
      "Safety monitoring",
      "Simulation",
      "Torso",
      "Wearable computer"
    ],
    "authors": [
      {
        "surname": "Vukicevic",
        "given_name": "Arso M."
      },
      {
        "surname": "Macuzic",
        "given_name": "Ivan"
      },
      {
        "surname": "Mijailovic",
        "given_name": "Nikola"
      },
      {
        "surname": "Peulic",
        "given_name": "Aleksandar"
      },
      {
        "surname": "Radovic",
        "given_name": "Milos"
      }
    ]
  },
  {
    "title": "G-SOMO: An oversampling approach based on self-organized maps and geometric SMOTE",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115230",
    "abstract": "Traditional supervised machine learning classifiers are challenged to learn highly skewed data distributions as they are designed to expect classes to equally contribute to the minimization of the classifiers cost function. Moreover, the classifiers design expects equal misclassification costs, causing a bias for overrepresented classes. Different strategies have been proposed to correct this issue. The modification of the data set has become a common practice since the procedure is generalizable to all classifiers. Various algorithms to rebalance the data distribution through the creation of synthetic instances were proposed in the past. In this paper, we propose a new oversampling algorithm named G-SOMO. The algorithm identifies optimal areas to create artificial data instances in an informed manner and utilizes a geometric region during the data generation process to increase their variability. Our empirical results on 69 datasets, validated with different classifiers and metrics against a benchmark of commonly used oversampling methods show that G-SOMO consistently outperforms competing oversampling methods. Additionally, the statistical significance of our results is established.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100662X",
    "keywords": [
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Benchmark (surveying)",
      "Biology",
      "Computer network",
      "Computer science",
      "Data mining",
      "Data set",
      "Evolutionary biology",
      "Function (biology)",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Minification",
      "Operating system",
      "Oversampling",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Douzas",
        "given_name": "Georgios"
      },
      {
        "surname": "Rauch",
        "given_name": "Rene"
      },
      {
        "surname": "Bacao",
        "given_name": "Fernando"
      }
    ]
  },
  {
    "title": "Limitation and optimization of inputs and outputs in the inverse data envelopment analysis under variable returns to scale",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115344",
    "abstract": "As an important component of data envelopment analysis (DEA), the inverse DEA method often has no feasible solution under variable returns to scale (VRS). By analyzing the reason of this problem, the limitation of inputs and outputs in the inverse DEA method under VRS is identified. Then the outputs possible set and the inputs possible set are defined for different inverse DEA models; and some models are thus developed to determine the change range of outputs/inputs. Sequentially, by determining the optimal given outputs/inputs, the inverse DEA model under VRS is used to estimate the optimal inputs increment or outputs diminution for the optimal given efficiency, and the problem that the inverse DEA method has no feasible solution is avoided. In addition, the effects of efficiency change and technological change on the optimization of inputs and outputs are further discussed in this paper. Finally, two examples are provided to illustrate the validity and effectiveness of our methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007727",
    "keywords": [
      "Component (thermodynamics)",
      "Composite material",
      "Computer science",
      "Data envelopment analysis",
      "Economics",
      "Geometry",
      "Inverse",
      "Macroeconomics",
      "Materials science",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Physics",
      "Production (economics)",
      "Programming language",
      "Quantum mechanics",
      "Range (aeronautics)",
      "Returns to scale",
      "Scale (ratio)",
      "Set (abstract data type)",
      "Thermodynamics",
      "Variable (mathematics)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Lei"
      },
      {
        "surname": "Wang",
        "given_name": "Ying-Ming"
      }
    ]
  },
  {
    "title": "Modified multi-objective evolutionary programming algorithm for solving project scheduling problems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115338",
    "abstract": "Though the Genetic Algorithm (GA) has received considerable attention recently in solving multi-objective optimization problems, inefficiency regarding performance has been reported in applications related to project scheduling. The degradation in efficiency was magnificent in applications of highly epistatic objective functions, including scheduling problems wherein the parameters being optimized are highly correlated. Furthermore, the crossover, being the dominant operator in GA, added significantly to the observed inefficiency for causing violations in the dependency between activities. Unlike GA, the Evolutionary Programming (EP) algorithm employs only a mutation operator which makes it less vulnerable to the dependency violation issue. This study proposes a modified Multi-Objective Evolutionary Programming (MOEP) algorithm to model and solve scheduling problems of multi-mode activities, including time–cost trade-off and finance-based scheduling with resource levelling. The modification involves the implementation of a new mutation operator to accommodate the scheduling problems in hand. Furthermore, the modified MOEP algorithm is benchmarked against the two multi-objective algorithms of SPEA-II and NSGA-II which have been used extensively in the literature to solve project scheduling problems. The results indicated that the modified MOEP algorithm outperformed SPEA-II and NSGA-II in terms of the diversity and quality of the Pareto optimal set.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007673",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Crossover",
      "Economics",
      "Evolutionary algorithm",
      "Flow shop scheduling",
      "Genetic algorithm scheduling",
      "Inefficiency",
      "Job shop scheduling",
      "Mathematical optimization",
      "Mathematics",
      "Microeconomics",
      "Operating system",
      "Schedule",
      "Scheduling (production processes)"
    ],
    "authors": [
      {
        "surname": "Abido",
        "given_name": "Mohammad A."
      },
      {
        "surname": "Elazouni",
        "given_name": "Ashraf"
      }
    ]
  },
  {
    "title": "Multiple Similarity-based Features Blending for Detecting Code Clones using Consensus-Driven Classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115364",
    "abstract": "Code clone detection helps to reduce the costs associated with software maintenance and bug prevention. Machine learning methods have previously suggested many ways by which to detect code clones. The majority of clone detectors are traditional in their approach, they can detect syntactic clones but are poor at detecting semantic clones. Researchers use machine learning to detect semantic clones and automatically scan the data to learn latent semantic features. In this study, we have introduced a new formal model of similarity which combines similarity measures so that method blocks can measure both the syntactic and semantic distances between method block pairs. The uniqueness of our study is in the use of different similarity measures, and similarity scores as features in machine learning, to detect code clones. We use a number of similarity measure computations to extract similarity score features, these features are then represented as vectors. Using ensemble classification models, we perform extensive comparisons and evaluations of the effectiveness of our proposed idea. The results indicate that our approach is significantly better at detecting clone types compared to contemporary code clone detectors. We achieved a 99% success rate in detecting cloned codes based on F-score, recall, and precision. Our approach achieves 98–100% accuracy in the majority of cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007922",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Block (permutation group theory)",
      "Code (set theory)",
      "Computer science",
      "DNA",
      "Data mining",
      "Genetics",
      "Geometry",
      "Image (mathematics)",
      "Latent semantic analysis",
      "Machine learning",
      "Mathematics",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Precision and recall",
      "Programming language",
      "Semantic similarity",
      "Set (abstract data type)",
      "Similarity (geometry)",
      "Source code",
      "clone (Java method)"
    ],
    "authors": [
      {
        "surname": "Sheneamer",
        "given_name": "Abdullah M."
      }
    ]
  },
  {
    "title": "Dynamic group optimization algorithm with a mean–variance search framework",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115434",
    "abstract": "Dynamic group optimization has recently appeared as a novel algorithm developed to mimic animal and human socialising behaviours. Although the algorithm strongly lends itself to exploration and exploitation, it has two main drawbacks. The first is that the greedy strategy, used in the dynamic group optimization algorithm, guarantees to evolve a generation of solutions without deteriorating than the previous generation but decreases population diversity and limit searching ability. The second is that most information for updating populations is obtained from companions within each group, which leads to premature convergence and deteriorated mutation operators. The dynamic group optimization with a mean–variance search framework is proposed to overcome these two drawbacks, an improved algorithm with a proportioned mean solution generator and a mean–variance Gaussian mutation. The new proportioned mean solution generator solutions do not only consider their group but also are affected by the current solution and global situation. The mean–variance Gaussian mutation takes advantage of information from all group heads, not solely concentrating on information from the best solution or one group. The experimental results on public benchmark test suites show that the proposed algorithm is effective and efficient. In addition, comparative results of engineering problems in welded beam design show the promise of our algorithms for real-world applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008502",
    "keywords": [
      "Accounting",
      "Algorithm",
      "Beam search",
      "Benchmark (surveying)",
      "Business",
      "Chemistry",
      "Computer science",
      "Convergence (economics)",
      "Demography",
      "Economic growth",
      "Economics",
      "Gaussian",
      "Geodesy",
      "Geography",
      "Greedy algorithm",
      "Group (periodic table)",
      "Mathematical optimization",
      "Mathematics",
      "Organic chemistry",
      "Particle swarm optimization",
      "Physics",
      "Population",
      "Premature convergence",
      "Quantum mechanics",
      "Search algorithm",
      "Sociology",
      "Variance (accounting)"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Rui"
      },
      {
        "surname": "Yang",
        "given_name": "Jie"
      },
      {
        "surname": "Fong",
        "given_name": "Simon"
      },
      {
        "surname": "Wong",
        "given_name": "Raymond"
      },
      {
        "surname": "Vasilakos",
        "given_name": "Athanasios V."
      },
      {
        "surname": "Chen",
        "given_name": "Yu"
      }
    ]
  },
  {
    "title": "Group recommender system based on genre preference focusing on reducing the clustering cost",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115396",
    "abstract": "The most significant advantage of the group recommender system over personalization is the low computational cost because the former analyzes the preferences of many users at once by integrating their preferences. The clustering step is the most time-consuming part of the entire process in a group recommender system. Existing studies either measured the similarities among all users or utilized a clustering algorithm based on the item preference vector to form the groups. However, these existing clustering methods overlooked the clustering cost, and the time complexity was not significantly better than that for personalized recommendations. Therefore, we propose a group recommender system based on the genre preferences of users to dramatically reduce the clustering cost. First, we define a genre preference vector and cluster the groups using this vector. Our group recommender system can reduce the time complexity more efficiently because the number of genres is significantly smaller than the number of items. In addition, we propose a new item preference along with genre weight to subdivide the preferences of users. The evaluation results show that the genre-based group recommender system significantly improves the time efficiency in terms of clustering. Clustering time was about five times faster when using k-means. In addition, for the Gaussian mixture model (GMM), it was about fifty times faster in MovieLens 100 k and about five hundred times faster in Last.fm. The normalized discounted cumulative gain (NDCG) (i.e., accuracy) is not much different from that of the item-based existing studies and is even higher when the number of users is low in a group in MovieLens 100 k.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008198",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Collaborative filtering",
      "Computer science",
      "Data mining",
      "Information retrieval",
      "Machine learning",
      "Mathematics",
      "MovieLens",
      "Personalization",
      "Preference",
      "Recommender system",
      "Statistics",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Seo",
        "given_name": "Young-Duk"
      },
      {
        "surname": "Kim",
        "given_name": "Young-Gab"
      },
      {
        "surname": "Lee",
        "given_name": "Euijong"
      },
      {
        "surname": "Kim",
        "given_name": "Hyungjin"
      }
    ]
  },
  {
    "title": "Application of small sample virtual expansion and spherical mapping model in wind turbine fault diagnosis",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115397",
    "abstract": "Due to the actual operation of the wind turbine, the collected fault data sets are limited, and it is difficult to realize fault diagnosis through the correlation between variables. To solve this problem, a fault diagnosis method based on virtual expansion and spherical mapping model is proposed in this paper. Firstly, Hermite interpolation is applied to discrete wind power data samples to obtain an interpolation curve about the characteristics of the sample, and a synchronous sampling method is adopted for the interpolation curve to construct a virtual sample. Then, the features of the virtual sample are mapped to a three-dimensional space. Define the spherical data model and perform spherical fitting in a three-dimensional coordinate system. Finally, feature extraction is performed on the fitted spherical surface for training and testing extreme learning machine (ELM). The distribution law of fault data in the spherical model is summarized. Using the data generated based on the Bootstrap method as a control group, comparative experiments were carried out in back-propagation neural network (BP), Probabilistic Neural Network (PNN), General Regression Neural Network (GRNN), and Support Vector Machine (SVM), which verified the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008204",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Data mining",
      "Fault (geology)",
      "Geology",
      "Hermite interpolation",
      "Hermite polynomials",
      "Interpolation (computer graphics)",
      "Mathematical analysis",
      "Mathematics",
      "Motion (physics)",
      "Pattern recognition (psychology)",
      "Probabilistic neural network",
      "Seismology",
      "Support vector machine",
      "Time delay neural network"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "WenXin"
      },
      {
        "surname": "Lu",
        "given_name": "Yang"
      },
      {
        "surname": "Wang",
        "given_name": "JunNian"
      }
    ]
  },
  {
    "title": "Improving stock market volatility forecasts with complete subset linear and quantile HAR models",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115416",
    "abstract": "Volatility forecasting plays an integral role in risk management, investments and security valuation for all assets with uncertain future payoffs. We enrich the literature by presenting computationally intensive variations of the heterogeneous autoregressive (HAR) volatility model: the complete subset linear/quantile regression HAR models, HAR-CSLR and HAR-CSQR. Predictions of 1- to 22-day-ahead volatility of four major market indices (NIKKEI 225, S&P 500, SSEC and STOXX 50) show that both models tend to outperform several benchmark HAR models. Forecasting accuracy improvements tend to stabilize for longer forecasting horizons: e.g., five-day-ahead improvements range from 6.57% (SSEC) to 35.62% (NIKKEI 225) and from 3.99% (STOXX) to 9.54% for mean square error (MSE) and QLIKE loss functions. In terms of MSE, the HAR-CSQR model outperforms several standard benchmark HAR models across all market indices and forecast horizons.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008356",
    "keywords": [
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Econometrics",
      "Economics",
      "Implied volatility",
      "Mathematics",
      "Mean squared error",
      "Paleontology",
      "Quantile",
      "Quantile regression",
      "Realized variance",
      "Statistics",
      "Stochastic volatility",
      "Stock market",
      "Stock market index",
      "Volatility (finance)"
    ],
    "authors": [
      {
        "surname": "Lyócsa",
        "given_name": "Štefan"
      },
      {
        "surname": "Stašek",
        "given_name": "Daniel"
      }
    ]
  },
  {
    "title": "Distributed representations of diseases based on co-occurrence relationship",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115418",
    "abstract": "The co-occurrence relationship among diseases facilitates the knowledge discovery in the medical field. However, due to limited data, previous researches are mainly based on clinician experience and simple statistics which make it difficult to discover deep associations among diseases. Treating the diagnoses in an electronic medical record (EMR) as interrelated random variables, we use Markov random fields to model the co-occurrence relationship among diseases and propose Di2Vec to learn distributed representations of diseases. The diseases having high co-occurrence frequency will be very close to each other in the embedding space. Considering the hierarchical structure in each diagnosis code, we introduce the subword embedding and explore its impact on the quality of embeddings, where the embedding of each diagnosis is expressed as the sum of its subword embedding. Qualitative and Quantitative experiments show that our Di2Vec can make the embeddings of diseases with high co-occurrence frequency close to each other, and can also outperform Skip-gram and CBOW when use these embeddings as the feature representations for medical expense prediction. Using subword embedding will make the disease embeddings to have better clustering property, but to a certain extent, it loss the co-occurrence information contained in the disease embeddings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100837X",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Embedding",
      "Epistemology",
      "Feature (linguistics)",
      "Field (mathematics)",
      "Linguistics",
      "Machine learning",
      "Markov chain",
      "Mathematics",
      "Medical diagnosis",
      "Medicine",
      "Operating system",
      "Pathology",
      "Philosophy",
      "Pure mathematics",
      "Simple (philosophy)",
      "Space (punctuation)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Haoqing"
      },
      {
        "surname": "Mai",
        "given_name": "Huiyu"
      },
      {
        "surname": "Deng",
        "given_name": "Zhi-hong"
      },
      {
        "surname": "Yang",
        "given_name": "Chao"
      },
      {
        "surname": "Zhang",
        "given_name": "Luxia"
      },
      {
        "surname": "Wang",
        "given_name": "Huai-yu"
      }
    ]
  },
  {
    "title": "Hybrid metaheuristics for solving a home health care routing and scheduling problem with time windows, synchronized visits and lunch breaks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115307",
    "abstract": "The home health care routing and scheduling problem (HHCRSP), a challenging operational problem in the field of home health care (HHC), consists of assigning suitable caregivers to serve patients at their homes and optimizing a set of caregiver’s visits according to certain criteria. Besides the time windows and qualifications of caregivers that are generally considered in HHCRSP, this study further concerns three practical constraints, which are (1): some patients require the services that should be performed by at least two caregivers simultaneously (synchronized visits); (2) caregivers should take lunch breaks when they working during the lunch period (lunch breaks); (3) caregivers can depart from either their homes or the HHC company (flexible departure modes of caregivers). In this study, the concerned problem is firstly modeled as a mixed-integer programming model, and four hybrid metaheuristics are developed. Numerical results obtained with the instances, adapted from a set of benchmark instances, as well as the statistical information, computed by Friedman test present that the hybrid genetic general variable neighborhood search (HGGVNS) shows the best performance among four algorithms. Furthermore, sensitivity analyses are conducted to evaluate the impact of synchronization scales, time window widths, break regulations, and departure strategies on the final solutions. The results of this study can offer HHC management a valuable scheme to construct a high-quality planning of HHC visits by taking into account various important real-life constraints, some of which were not yet been tackled in the literature.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007363",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer network",
      "Computer science",
      "Economic growth",
      "Economics",
      "Geodesy",
      "Geography",
      "Health care",
      "Home health",
      "Integer programming",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Operations research",
      "Programming language",
      "Routing (electronic design automation)",
      "Scheduling (production processes)",
      "Set (abstract data type)",
      "Variable neighborhood search",
      "Vehicle routing problem"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Wenheng"
      },
      {
        "surname": "Dridi",
        "given_name": "Mahjoub"
      },
      {
        "surname": "Fei",
        "given_name": "Hongying"
      },
      {
        "surname": "El Hassani",
        "given_name": "Amir Hajjam"
      }
    ]
  },
  {
    "title": "PLDLS: A novel parallel label diffusion and label Selection-based community detection algorithm based on Spark in social networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115377",
    "abstract": "Parallel and distributed community detection in large-scale complex networks, such as social networks, is a challenging task. Parallel and distributed algorithm with high accuracy and low computational complexity is one of the essential issues in the community detection field. In this paper, we propose a novel fast, and accurate Spark-based parallel label diffusion and label selection-based (PLDLS) community detection algorithm with two-step of label diffusion of core nodes along with a new label selection (propagation) method. We have used multi-factor criteria for computing node's importance and adopted a new method for selecting core nodes. In the first phase, utilizing the fact that nodes forming triangles, tend to be in the same community, parallel label diffusion of core nodes is performed to diffuse labels up to two levels. In the second phase, through an iterative and parallel process, the most appropriate labels are assigned to the remaining nodes. PLDLS proposes an improved robust version of LPA by putting aside randomness parameter tuning. Furthermore, we utilize a fast and parallel merge phase to get even more dense and accurate communities. Conducted experiments on real-world and artificial networks, indicates the better accuracy and low execution time of PLDLS in comparison with other examined methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008034",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Mathematics",
      "Merge (version control)",
      "Parallel computing",
      "Programming language",
      "Randomness",
      "SPARK (programming language)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Roghani",
        "given_name": "Hamid"
      },
      {
        "surname": "Bouyer",
        "given_name": "Asgarali"
      },
      {
        "surname": "Nourani",
        "given_name": "Esmaeil"
      }
    ]
  },
  {
    "title": "A preliminary analysis of AI based smartphone application for diagnosis of COVID-19 using chest X-ray images",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115401",
    "abstract": "The COVID-19 outbreak has catastrophically affected both public health system and world economy. Swift diagnosis of the positive cases will help in providing proper medical attention to the infected individuals and will also aid in effective tracing of their contacts to break the chain of transmission. Blending Artificial Intelligence (AI) with chest X-ray images and incorporating these models in a smartphone can be handy for the accelerated diagnosis of COVID-19. In this study, publicly available datasets of chest X-ray images have been utilized for training and testing of five pre-trained Convolutional Neural Network (CNN) models namely VGG16, MobileNetV2, Xception, NASNetMobile and InceptionResNetV2. Prior to the training of the selected models, the number of images in COVID-19 category has been increased employing traditional augmentation and Generative Adversarial Network (GAN). The performance of the five pre-trained CNN models utilizing the images generated with the two strategies has been compared. In the case of models trained using augmented images, Xception (98%) and MobileNetV2 (97.9%) turned out to be the ones with highest validation accuracy. Xception (98.1%) and VGG16 (98.6%) emerged as models with the highest validation accuracy in the models trained with synthetic GAN images. The best performing models have been further deployed in a smartphone and evaluated. The overall results suggest that VGG16 and Xception, trained with the synthetic images created using GAN, performed better compared to models trained with augmented images. Among these two models VGG16 produced an encouraging Diagnostic Odd Ratio (DOR) with higher positive likelihood and lower negative likelihood for the prediction of COVID-19.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008241",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Coronavirus disease 2019 (COVID-19)",
      "Deep learning",
      "Disease",
      "Generative adversarial network",
      "Infectious disease (medical specialty)",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Rangarajan",
        "given_name": "Aravind Krishnaswamy"
      },
      {
        "surname": "Ramachandran",
        "given_name": "Hari Krishnan"
      }
    ]
  },
  {
    "title": "Active cluster annotation for wafer map pattern classification in semiconductor manufacturing",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115429",
    "abstract": "In semiconductor manufacturing, wafer map pattern classification (WMPC) is important for ensuring good manufacturing quality because the defect type on a wafer map provides important information for determining defect causes. To construct a high-performance WMPC model, a large amount of labeled training data is required, which entails a high annotation cost for engineers. To reduce the annotation cost, an active learning framework has been investigated, in which annotation is conducted at the individual wafer map level. If wafer maps can be grouped into clusters based on the similarity of defect patterns, then annotation can be performed at the cluster level rather than at the wafer map level, thereby affording cost effectiveness. Based on the cluster-level annotation, we propose an active cluster annotation to obtain a high-performance WMPC model with reduced annotation cost. For a dataset annotated only for a small subset of wafer maps, clustering is first conducted for unlabeled wafer maps. In an active learning iteration, a convolutional neural network (CNN) is constructed with labeled wafer maps. Subsequently, the cluster-level classification uncertainties of the CNN for unlabeled wafer maps are calculated. With the uncertainties, the query clusters are selected and annotated by an engineer at the cluster level. The performance of the CNN is improved cost-effectively by repeating these iterations. We demonstrate the effectiveness of the proposed method through experiments on real-world data from a semiconductor manufacturer. In addition, we show that cluster-level annotation is a robust annotation method that can yield consistent labels.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008460",
    "keywords": [
      "Annotation",
      "Artificial intelligence",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Computer science",
      "Construct (python library)",
      "Convolutional neural network",
      "Data mining",
      "Materials science",
      "Optoelectronics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Semiconductor device fabrication",
      "Wafer"
    ],
    "authors": [
      {
        "surname": "Shim",
        "given_name": "Jaewoong"
      },
      {
        "surname": "Kang",
        "given_name": "Seokho"
      },
      {
        "surname": "Cho",
        "given_name": "Sungzoon"
      }
    ]
  },
  {
    "title": "An efficient deep Convolutional Neural Network based detection and classification of Acute Lymphoblastic Leukemia",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115311",
    "abstract": "Automated and accurate diagnosis of Acute Lymphoblastic Leukemia (ALL), blood cancer, is a challenging task. Nowadays, Convolutional Neural Networks (CNNs) have become a preferred approach for medical image analysis. However, for achieving excellent performance, classical CNNs usually require huge databases for proper training. This paper proposes an efficient deep CNNs framework to mitigate this issue and yield more accurate ALL detection. The salient features: depthwise separable convolutions, linear bottleneck architecture, inverted residual, and skip connections make it a faster and preferred approach. In this proposed method, a novel probability-based weight factor is suggested, which has a significant role in efficiently hybridizing MobilenetV2 and ResNet18 with preserving the benefits of both approaches. Its performance is validated using public benchmark datasets: ALLIDB1 and ALLIDB2. The experimental results display that the proposed approach yields the best accuracy (with 70% training and 30% testing) 99.39% and 97.18% in ALLIDB1 and ALLIDB2 datasets, respectively. Similarly, it also achieves the best accuracy (with 50% training and 50% testing) 97.92% and 96.00% in ALLIDB1 and ALLIDB2 datasets, respectively. Moreover, it also achieves the best performance compared to the recent transfer learning-based techniques in both the datasets, in terms of sensitivity, specificity, accuracy, precision, F1 score, and receiver operating characteristic (ROC) in most of the cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007405",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Bottleneck",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Economics",
      "Electronic engineering",
      "Embedded system",
      "Engineering",
      "Geodesy",
      "Geography",
      "Internal medicine",
      "Leukemia",
      "Lymphoblastic Leukemia",
      "Machine learning",
      "Management",
      "Medicine",
      "Pattern recognition (psychology)",
      "Receiver operating characteristic",
      "Residual",
      "Sensitivity (control systems)",
      "Task (project management)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Das",
        "given_name": "Pradeep Kumar"
      },
      {
        "surname": "Meher",
        "given_name": "Sukadev"
      }
    ]
  },
  {
    "title": "Vehicle classification using a real-time convolutional structure based on DWT pooling layer and SE blocks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115420",
    "abstract": "Real-time vehicle type classification has become more popular and significant in recent years owing to its potential applications. In this paper, a real-time Convolutional Neural Network is proposed for vehicle type classification. This novel CNN structure is a combination of classic CNN layers along with squeeze-and-excitation (SE) modules and Haar wavelet as the pooling layer. This structure improves the performance of the CNN classifier by emphasizing informative feature maps and decreasing the entropy of the network. A cross-entropy loss function is proposed for better performance. A new pooling method is also introduced based on the Haar transform. Network parameters and the number of layers are selected in such a way to be real-time. Experimental results on two vehicle datasets show that the overall performance of this model, including recognition time and accuracy, is better than the others. For example, using DWT instead of Max-pooling improved the recognition rate on the IRVD dataset from 97.12% to 99.06%. The number of training parameters of the proposed model is about 5 million which is much less than popular networks like VGG (128 m), ResNet152 (58 m), DarkNet (40 m), and Inception-V3 (24 m). This made it very faster so that its recognition time is only 42 ms on CPU which is suitable for real-time applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008393",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Convolutional neural network",
      "Discrete wavelet transform",
      "Entropy (arrow of time)",
      "Haar",
      "Haar wavelet",
      "Pattern recognition (psychology)",
      "Physics",
      "Pooling",
      "Quantum mechanics",
      "Wavelet",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Gholamalinejad",
        "given_name": "Hossein"
      },
      {
        "surname": "Khosravi",
        "given_name": "Hossein"
      }
    ]
  },
  {
    "title": "A feature selection method via analysis of relevance, redundancy, and interaction",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115365",
    "abstract": "Feature selection aims at selecting important features that can enhance learning performance in data mining, pattern recognition, and machine learning. Filter feature selection methods offer computational efficiency and feature evaluation criteria, while feature interaction information, which may greatly help increase classification accuracy, is often ignored. In this work, we instead propose a novel feature selection algorithm that uses the “maximum of the maximum” criterion to select highly relevant features and their maximally interactive features. Extensive experiments are performed to evaluate the performance of the proposed method with regard to the number of selected features and classification accuracy on thirty UCI datasets. The results demonstrate that the proposed algorithm not only efficiently selects the relevant features and the interactive features, but also enables classifiers to achieve classification accuracy that is better than, or comparably well to, ten representative competing feature selection algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007934",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Feature (linguistics)",
      "Feature selection",
      "Filter (signal processing)",
      "Law",
      "Linguistics",
      "Machine learning",
      "Minimum redundancy feature selection",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Redundancy (engineering)",
      "Relevance (law)",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Lianxi"
      },
      {
        "surname": "Jiang",
        "given_name": "Shengyi"
      },
      {
        "surname": "Jiang",
        "given_name": "Siyu"
      }
    ]
  },
  {
    "title": "Privacy preserving rule-based classifier using modified artificial bee colony algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115437",
    "abstract": "Privacy preserving data mining is a hot research field of data mining. The aim of privacy preserving data mining is to prevent the leakage of the sensitive information of individuals while performing data mining techniques. Classification task is one of the most studied fields in data mining hence in privacy preserving data mining as well. On the other hand, differential privacy is a powerful privacy guarantee that determines privacy leakage ratio by using ∊ parameter and enables researchers to mine data which includes sensitive information. Implementations of some well-known classification algorithms such as k-NN, Naïve Bayes, ID3, etc. with differential privacy have been developed. Although the success of the rule-based classifiers using meta-heuristics such as Ant-Miner, BeeMiner etc. in data mining has been demonstrated, any implementation of these classification algorithms with differential privacy has not been proposed in the literature until now to our best knowledge. Artificial bee colony (ABC) is a nature inspired algorithm which imitates foraging behavior of bees, and some approaches using ABC to discover classification rules have been proposed recently and the success of ABC algorithm for the discovery of classification rules has been demonstrated. Motivated by this shortcoming in the literature, we propose to develop a rule-based classifier using ABC algorithm with input perturbation technique of differential privacy to perform privacy preserving classification. According to our experimental results, the proposed ABC-based classifier performs better than the well-known algorithms that are SVM, C4.5, Holte’s One Rule, PART, and RIPPER over non-private and differentially private versions of the datasets in terms of classification performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008526",
    "keywords": [
      "Artificial bee colony algorithm",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Machine learning",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Zorarpacı",
        "given_name": "Ezgi"
      },
      {
        "surname": "Ayşe Özel",
        "given_name": "Selma"
      }
    ]
  },
  {
    "title": "A hierarchical guidance strategy assisted fruit fly optimization algorithm with cooperative learning mechanism",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115342",
    "abstract": "The fruit fly optimization algorithm (FOA) has drawn enormous attention from researchers and practitioners in the computation intelligence domain for the benefits of simple implementation mechanism and few parameters tuning requirement of FOA. However, FOA is hard to adapt directly to address complex continuous problems. A hierarchical guidance strategy assisted fruit fly optimization algorithm with cooperative learning mechanism (HGCLFOA) is proposed in this study. The population is divided into elitist and inferior subpopulations with the fitness of objective function. The population center is re-designed as an elitist subpopulation to maintain the diversity of the population. In the olfaction search stage, the hierarchical guidance strategy is introduced for local search according to the difference of solution qualities to assign inferior individuals to elitist individuals on different levels. Meanwhile, the inferior information is applied by the inferior solutions repairing strategy to deflect the prediction of the elitist subpopulation for preventing HGCLFOA from falling into the local optimum. In the vision search stage, a hybrid Gaussian distribution estimation strategy is adopted to extract the elitist information of previous generations to predict the distribution of potential elitist individuals in the next generation. The exploration and exploitation of the HGCLFOA are balanced by the cooperation between elitist subpopulation and inferior subpopulation. A random walk strategy is activated to assist the elitist solutions to jump out the local optimal. The parameters of the HGCLFOA are calibrated by DOE and ANOVA methods. The experimental results demonstrated that the HGCLFOA outperformed the classical FOA and state-of-arts variants of FOA.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007703",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Demography",
      "Estimation of distribution algorithm",
      "Local optimum",
      "Local search (optimization)",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Population",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Fuqing"
      },
      {
        "surname": "Ding",
        "given_name": "Ruiqing"
      },
      {
        "surname": "Wang",
        "given_name": "Ling"
      },
      {
        "surname": "Cao",
        "given_name": "Jie"
      },
      {
        "surname": "Tang",
        "given_name": "Jianxin"
      }
    ]
  },
  {
    "title": "A novel inertia moment estimation algorithm collaborated with Active Force Control scheme for wheeled mobile robot control in constrained environments",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115454",
    "abstract": "This paper presents a novel inertia moment estimation algorithm to enable the Active Force Control Scheme for tracking a wheeled mobile robot (WMR) effectively in a specific trajectory within constrained environments such as on roads or in factories. This algorithm, also known as laser simulator logic, has the capability to estimate the inertia moment of the AFC-controller when the robot is moving in a pre-planned path with the presence of noisy measurements. The estimation is accomplished by calculating the membership function based on the experts’ views in any form (symmetric or non-symmetric) with lowly or highly overlapped linguistic variables. A new Proportional-Derivative Active Force Controller (PD-AFC-LS-QC), employing the use of laser simulator logic and quick compensation loop, has been developed in this paper to robustly reject the noise and disturbances. This controller has three feedback control loops, namely, internal, external and quick compensation loops to compensate effectively the disturbances in the constrained environments. A simulation and experimental studies on WMR path control in two kinds of environments; namely, zigzag and highly curved terrains, were conducted to verify the proposed algorithm and controller which was then compared with other existed control schemes. The results of the simulation and experimental works show the capability of the proposed algorithms and the controller to robustly move the WMR in the constrained environments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008678",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Astronomy",
      "Biology",
      "Classical mechanics",
      "Compensation (psychology)",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Controller (irrigation)",
      "Image (mathematics)",
      "Inertia",
      "Mobile robot",
      "Moment (physics)",
      "Noise (video)",
      "Pedagogy",
      "Physics",
      "Psychoanalysis",
      "Psychology",
      "Robot",
      "Tracking (education)",
      "Trajectory"
    ],
    "authors": [
      {
        "surname": "Ali",
        "given_name": "Mohammed A.H."
      },
      {
        "surname": "Radzak",
        "given_name": "Muhammad S.A."
      },
      {
        "surname": "Mailah",
        "given_name": "Musa"
      },
      {
        "surname": "Yusoff",
        "given_name": "Nukman"
      },
      {
        "surname": "Razak",
        "given_name": "Bushroa Abd"
      },
      {
        "surname": "Karim",
        "given_name": "Mohd Sayuti Ab."
      },
      {
        "surname": "Ameen",
        "given_name": "Wadea"
      },
      {
        "surname": "Jabbar",
        "given_name": "Waheb A."
      },
      {
        "surname": "Alsewari",
        "given_name": "AbdulRahman A."
      },
      {
        "surname": "Rassem",
        "given_name": "Taha H."
      },
      {
        "surname": "Nasser",
        "given_name": "Abdullah B."
      },
      {
        "surname": "Abdulghafor",
        "given_name": "Rawad"
      }
    ]
  },
  {
    "title": "Detection and tracking of the trajectories of dynamic UAVs in restricted and cluttered environment",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115309",
    "abstract": "The unidentified number of unmanned aerial vehicles (UAVs) can execute aggressive maneuvers in the restricted and the cluttered environment. Therefore, it is difficult to detect and track the uncertain motion of the UAV target in such complex environment. In addition, multi-target tracking (MTT) algorithms such as joint data association approach faces various computational complexities that could exceeds the available computation resources. This paper develops a novel smoothing data association idea in a linear multi-target (LM) tracking based on integrated probabilistic data association (sLM-IPDA) algorithm that acts like a single target tracker in the MTT situation. The significant detection and tracking performance of a UAV are validated without a-prior information of the UAV’s initial position. The forward and backward tracks are initialized separately using sensor measurements received in each scan. The sLM-IPDA estimates the backward multi-tracks simultaneously associating backward tracks in a subsequent predicted forward track for fusion. Thus, a forward track state estimate is obtain using the smoothing (fusion) measurements. This significantly improves estimation accuracy for large number of cross-over targets in heavy clutter. Numerical assessments of the sLM-IPDA are verified using both simulation and experiment to demonstrate the application of the proposed algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007387",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Astronomy",
      "Clutter",
      "Computation",
      "Computer science",
      "Computer vision",
      "Data association",
      "Economics",
      "Finance",
      "Operating system",
      "Pedagogy",
      "Physics",
      "Position (finance)",
      "Probabilistic logic",
      "Psychology",
      "Radar",
      "Sensor fusion",
      "Smoothing",
      "Telecommunications",
      "Track (disk drive)",
      "Tracking (education)",
      "Trajectory"
    ],
    "authors": [
      {
        "surname": "Memon",
        "given_name": "Sufyan Ali"
      },
      {
        "surname": "Ullah",
        "given_name": "Ihsan"
      }
    ]
  },
  {
    "title": "A system for identifying an anti-counterfeiting pattern based on the statistical difference in key image regions",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115410",
    "abstract": "An important aspect of identifying whether items are likely to be forged or infringe upon a copyright is the use of an anti-counterfeiting pattern because it can help in determining and detecting forged anti-counterfeiting labels and patterns. At present, most anti-counterfeiting systems require special materials or large samples of images for training. Once they are geometrically attacked or forged by printing and scanning, they will be completely invalid. Therefore, this paper proposes an anti-counterfeiting system that uses a single corresponding feature difference sequence of the key regions for statistical analysis. The aim of our technology is to use the inks to generate random subtle texture patterns, and construct a supervised and guided segmentation algorithm and bone width transformation algorithm to locate the key regions of the sample images, which is used to identify the authenticity of the inspected product. The experiment shows that the system not only has high anti-counterfeiting performance and good robustness but also provides a convenient and practical idea for anti-counterfeiting technology.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008319",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Construct (python library)",
      "Data mining",
      "Gene",
      "Image (mathematics)",
      "Key (lock)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Robustness (evolution)",
      "Sample (material)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Zhaohui"
      },
      {
        "surname": "Zheng",
        "given_name": "Hong"
      },
      {
        "surname": "Ju",
        "given_name": "Jianping"
      },
      {
        "surname": "Chen",
        "given_name": "Deng"
      },
      {
        "surname": "Li",
        "given_name": "Xi"
      },
      {
        "surname": "Guo",
        "given_name": "Zhongyuan"
      },
      {
        "surname": "You",
        "given_name": "Changhui"
      },
      {
        "surname": "Lin",
        "given_name": "Mingyu"
      }
    ]
  },
  {
    "title": "Multi-objective distributed reentrant permutation flow shop scheduling with sequence-dependent setup time",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115339",
    "abstract": "The distributed reentrant permutation flow shop (DRPFS) is a combination of the reentrant flow shop problem and distributed scheduling. The DRPFS is a NP-hard problem that consists of two subproblems: (1) assigning a set of jobs to a set of available factories and (2) determining the operation sequence of jobs in each factory. This paper is the first study to consider the inclusion of sequence-dependent setup time in the DRPFS. The industrial applications of flow shop indicate that the machine setup time to process a job may depend on the previously processed jobs. Particularly, in DRPFS, the effect of sequence-dependent setup time is intensified due to its reentrant characteristic. An improved version of the multi-objective adaptive large neighborhood search (MOALNS) is proposed as a solution method for the sequence-dependent DRPFS with the aim to minimize the makespan, production cost, and tardiness. The proposed algorithm enhances the standard MOALNS by embedding an improved solution acceptance and non-dominated set updating criteria to assist the algorithm in finding the near-optimal Pareto front of the factory allocation and scheduling problems. To address the multiple objectives and the issue of non-uniform setup time, a new set of destroy and repair heuristics are developed. Further, the numerical experiments demonstrate the efficiency of IMOALNS in finding high-quality solutions in a relatively short time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007685",
    "keywords": [
      "Acoustics",
      "Algorithm",
      "Biology",
      "Computer science",
      "Distributed computing",
      "Flow shop scheduling",
      "Genetics",
      "Heuristics",
      "Job shop scheduling",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Permutation (music)",
      "Physics",
      "Programming language",
      "Reentrancy",
      "Schedule",
      "Scheduling (production processes)",
      "Sequence (biology)",
      "Tardiness"
    ],
    "authors": [
      {
        "surname": "Rifai",
        "given_name": "Achmad Pratama"
      },
      {
        "surname": "Mara",
        "given_name": "Setyo Tri Windras"
      },
      {
        "surname": "Sudiarso",
        "given_name": "Andi"
      }
    ]
  },
  {
    "title": "Neural identification of Type 1 Diabetes Mellitus for care and forecasting of risk events",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115367",
    "abstract": "Glucose–insulin models, testing glucose sensors and support systems for health care decisions play an important role in synthesis of glucose control algorithms. In this work we propose an online glucose–insulin identification using the Recurrent High Order Neural Network (RHONN). Then, the model obtained is used to predict n -steps forward of glucose levels, also by RHONN. The used data for identification is from a Type 1 Diabetes Mellitus (T1DM) patient, it was collected from the Continuous Monitoring Glucose System (CMGS) by MiniMed Inc ® and an insulin pump by Paradigm Real-time Insulin Pump ®. RHONN is trained online by Extended Kalman Filter (EKF). The results suggest that it is possible to make a prediction of up to 35 min in the future, which it would help to prevent risky events (hypoglycemia and hyperglycemia). Also shows that, it could be directly connected to a CGMS to help the patient improve the glucose control and even an automatic glucose control algorithm. The proposed Neural Network shows good performance compared to baseline methods in terms of evaluation criteria.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007946",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Botany",
      "Computer science",
      "Continuous glucose monitoring",
      "Control (management)",
      "Diabetes mellitus",
      "Endocrinology",
      "Extended Kalman filter",
      "Hypoglycemia",
      "Identification (biology)",
      "Insulin",
      "Insulin pump",
      "Internal medicine",
      "Kalman filter",
      "Machine learning",
      "Medicine",
      "Type 1 diabetes",
      "Type 2 Diabetes Mellitus"
    ],
    "authors": [
      {
        "surname": "Sanchez",
        "given_name": "Oscar D."
      },
      {
        "surname": "Alanis",
        "given_name": "Alma Y."
      },
      {
        "surname": "Ruiz Velázquez",
        "given_name": "E."
      },
      {
        "surname": "Valencia Murillo",
        "given_name": "Roberto"
      }
    ]
  },
  {
    "title": "SLSNet: Skin lesion segmentation using a lightweight generative adversarial network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115433",
    "abstract": "The determination of precise skin lesion boundaries in dermoscopic images using automated methods faces many challenges, most importantly, the presence of hair, inconspicuous lesion edges and low contrast in dermoscopic images, and variability in the color, texture and shapes of skin lesions. Existing deep learning-based skin lesion segmentation algorithms are expensive in terms of computational time and memory. Consequently, running such segmentation algorithms requires a powerful GPU and high bandwidth memory, which are not available in dermoscopy devices. Thus, this article aims to achieve precise skin lesion segmentation with minimum resources: a lightweight, efficient generative adversarial network (GAN) model called SLSNet, which combines 1-D kernel factorized networks, position and channel attention, and multiscale aggregation mechanisms with a GAN model. The 1-D kernel factorized network reduces the computational cost of 2D filtering. The position and channel attention modules enhance the discriminative ability between the lesion and non-lesion feature representations in spatial and channel dimensions, respectively. A multiscale block is also used to aggregate the coarse-to-fine features of input skin images and reduce the effect of the artifacts. SLSNet is evaluated on two publicly available datasets: ISBI 2017 and the ISIC 2018. Although SLSNet has only 2.35 million parameters, the experimental results demonstrate that it achieves segmentation results on a par with the state-of-the-art skin lesion segmentation methods with an accuracy of 97.61%, and Dice and Jaccard similarity coefficients of 90.63% and 81.98%, respectively. SLSNet can run at more than 110 frames per second (FPS) in a single GTX1080Ti GPU, which is faster than well-known deep learning-based image segmentation models, such as FCN. Therefore, SLSNet can be used for practical dermoscopic applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008496",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Channel (broadcasting)",
      "Combinatorics",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Dice",
      "Discriminative model",
      "Geometry",
      "Jaccard index",
      "Kernel (algebra)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pooling",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Sarker",
        "given_name": "Md. Mostafa Kamal"
      },
      {
        "surname": "Rashwan",
        "given_name": "Hatem A."
      },
      {
        "surname": "Akram",
        "given_name": "Farhan"
      },
      {
        "surname": "Singh",
        "given_name": "Vivek Kumar"
      },
      {
        "surname": "Banu",
        "given_name": "Syeda Furruka"
      },
      {
        "surname": "Chowdhury",
        "given_name": "Forhad U.H."
      },
      {
        "surname": "Choudhury",
        "given_name": "Kabir Ahmed"
      },
      {
        "surname": "Chambon",
        "given_name": "Sylvie"
      },
      {
        "surname": "Radeva",
        "given_name": "Petia"
      },
      {
        "surname": "Puig",
        "given_name": "Domenec"
      },
      {
        "surname": "Abdel-Nasser",
        "given_name": "Mohamed"
      }
    ]
  },
  {
    "title": "Distributed representations of diseases based on co-occurrence relationship",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115418",
    "abstract": "The co-occurrence relationship among diseases facilitates the knowledge discovery in the medical field. However, due to limited data, previous researches are mainly based on clinician experience and simple statistics which make it difficult to discover deep associations among diseases. Treating the diagnoses in an electronic medical record (EMR) as interrelated random variables, we use Markov random fields to model the co-occurrence relationship among diseases and propose Di2Vec to learn distributed representations of diseases. The diseases having high co-occurrence frequency will be very close to each other in the embedding space. Considering the hierarchical structure in each diagnosis code, we introduce the subword embedding and explore its impact on the quality of embeddings, where the embedding of each diagnosis is expressed as the sum of its subword embedding. Qualitative and Quantitative experiments show that our Di2Vec can make the embeddings of diseases with high co-occurrence frequency close to each other, and can also outperform Skip-gram and CBOW when use these embeddings as the feature representations for medical expense prediction. Using subword embedding will make the disease embeddings to have better clustering property, but to a certain extent, it loss the co-occurrence information contained in the disease embeddings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100837X",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Embedding",
      "Epistemology",
      "Feature (linguistics)",
      "Field (mathematics)",
      "Linguistics",
      "Machine learning",
      "Markov chain",
      "Mathematics",
      "Medical diagnosis",
      "Medicine",
      "Operating system",
      "Pathology",
      "Philosophy",
      "Pure mathematics",
      "Simple (philosophy)",
      "Space (punctuation)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Haoqing"
      },
      {
        "surname": "Mai",
        "given_name": "Huiyu"
      },
      {
        "surname": "Deng",
        "given_name": "Zhi-hong"
      },
      {
        "surname": "Yang",
        "given_name": "Chao"
      },
      {
        "surname": "Zhang",
        "given_name": "Luxia"
      },
      {
        "surname": "Wang",
        "given_name": "Huai-yu"
      }
    ]
  },
  {
    "title": "Multi-objective distributed reentrant permutation flow shop scheduling with sequence-dependent setup time",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115339",
    "abstract": "The distributed reentrant permutation flow shop (DRPFS) is a combination of the reentrant flow shop problem and distributed scheduling. The DRPFS is a NP-hard problem that consists of two subproblems: (1) assigning a set of jobs to a set of available factories and (2) determining the operation sequence of jobs in each factory. This paper is the first study to consider the inclusion of sequence-dependent setup time in the DRPFS. The industrial applications of flow shop indicate that the machine setup time to process a job may depend on the previously processed jobs. Particularly, in DRPFS, the effect of sequence-dependent setup time is intensified due to its reentrant characteristic. An improved version of the multi-objective adaptive large neighborhood search (MOALNS) is proposed as a solution method for the sequence-dependent DRPFS with the aim to minimize the makespan, production cost, and tardiness. The proposed algorithm enhances the standard MOALNS by embedding an improved solution acceptance and non-dominated set updating criteria to assist the algorithm in finding the near-optimal Pareto front of the factory allocation and scheduling problems. To address the multiple objectives and the issue of non-uniform setup time, a new set of destroy and repair heuristics are developed. Further, the numerical experiments demonstrate the efficiency of IMOALNS in finding high-quality solutions in a relatively short time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007685",
    "keywords": [
      "Acoustics",
      "Algorithm",
      "Biology",
      "Computer science",
      "Distributed computing",
      "Flow shop scheduling",
      "Genetics",
      "Heuristics",
      "Job shop scheduling",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Permutation (music)",
      "Physics",
      "Programming language",
      "Reentrancy",
      "Schedule",
      "Scheduling (production processes)",
      "Sequence (biology)",
      "Tardiness"
    ],
    "authors": [
      {
        "surname": "Rifai",
        "given_name": "Achmad Pratama"
      },
      {
        "surname": "Mara",
        "given_name": "Setyo Tri Windras"
      },
      {
        "surname": "Sudiarso",
        "given_name": "Andi"
      }
    ]
  },
  {
    "title": "Neural identification of Type 1 Diabetes Mellitus for care and forecasting of risk events",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115367",
    "abstract": "Glucose–insulin models, testing glucose sensors and support systems for health care decisions play an important role in synthesis of glucose control algorithms. In this work we propose an online glucose–insulin identification using the Recurrent High Order Neural Network (RHONN). Then, the model obtained is used to predict n -steps forward of glucose levels, also by RHONN. The used data for identification is from a Type 1 Diabetes Mellitus (T1DM) patient, it was collected from the Continuous Monitoring Glucose System (CMGS) by MiniMed Inc ® and an insulin pump by Paradigm Real-time Insulin Pump ®. RHONN is trained online by Extended Kalman Filter (EKF). The results suggest that it is possible to make a prediction of up to 35 min in the future, which it would help to prevent risky events (hypoglycemia and hyperglycemia). Also shows that, it could be directly connected to a CGMS to help the patient improve the glucose control and even an automatic glucose control algorithm. The proposed Neural Network shows good performance compared to baseline methods in terms of evaluation criteria.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007946",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Botany",
      "Computer science",
      "Continuous glucose monitoring",
      "Control (management)",
      "Diabetes mellitus",
      "Endocrinology",
      "Extended Kalman filter",
      "Hypoglycemia",
      "Identification (biology)",
      "Insulin",
      "Insulin pump",
      "Internal medicine",
      "Kalman filter",
      "Machine learning",
      "Medicine",
      "Type 1 diabetes",
      "Type 2 Diabetes Mellitus"
    ],
    "authors": [
      {
        "surname": "Sanchez",
        "given_name": "Oscar D."
      },
      {
        "surname": "Alanis",
        "given_name": "Alma Y."
      },
      {
        "surname": "Ruiz Velázquez",
        "given_name": "E."
      },
      {
        "surname": "Valencia Murillo",
        "given_name": "Roberto"
      }
    ]
  },
  {
    "title": "PLDLS: A novel parallel label diffusion and label Selection-based community detection algorithm based on Spark in social networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115377",
    "abstract": "Parallel and distributed community detection in large-scale complex networks, such as social networks, is a challenging task. Parallel and distributed algorithm with high accuracy and low computational complexity is one of the essential issues in the community detection field. In this paper, we propose a novel fast, and accurate Spark-based parallel label diffusion and label selection-based (PLDLS) community detection algorithm with two-step of label diffusion of core nodes along with a new label selection (propagation) method. We have used multi-factor criteria for computing node's importance and adopted a new method for selecting core nodes. In the first phase, utilizing the fact that nodes forming triangles, tend to be in the same community, parallel label diffusion of core nodes is performed to diffuse labels up to two levels. In the second phase, through an iterative and parallel process, the most appropriate labels are assigned to the remaining nodes. PLDLS proposes an improved robust version of LPA by putting aside randomness parameter tuning. Furthermore, we utilize a fast and parallel merge phase to get even more dense and accurate communities. Conducted experiments on real-world and artificial networks, indicates the better accuracy and low execution time of PLDLS in comparison with other examined methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008034",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Mathematics",
      "Merge (version control)",
      "Parallel computing",
      "Programming language",
      "Randomness",
      "SPARK (programming language)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Roghani",
        "given_name": "Hamid"
      },
      {
        "surname": "Bouyer",
        "given_name": "Asgarali"
      },
      {
        "surname": "Nourani",
        "given_name": "Esmaeil"
      }
    ]
  },
  {
    "title": "A preliminary analysis of AI based smartphone application for diagnosis of COVID-19 using chest X-ray images",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115401",
    "abstract": "The COVID-19 outbreak has catastrophically affected both public health system and world economy. Swift diagnosis of the positive cases will help in providing proper medical attention to the infected individuals and will also aid in effective tracing of their contacts to break the chain of transmission. Blending Artificial Intelligence (AI) with chest X-ray images and incorporating these models in a smartphone can be handy for the accelerated diagnosis of COVID-19. In this study, publicly available datasets of chest X-ray images have been utilized for training and testing of five pre-trained Convolutional Neural Network (CNN) models namely VGG16, MobileNetV2, Xception, NASNetMobile and InceptionResNetV2. Prior to the training of the selected models, the number of images in COVID-19 category has been increased employing traditional augmentation and Generative Adversarial Network (GAN). The performance of the five pre-trained CNN models utilizing the images generated with the two strategies has been compared. In the case of models trained using augmented images, Xception (98%) and MobileNetV2 (97.9%) turned out to be the ones with highest validation accuracy. Xception (98.1%) and VGG16 (98.6%) emerged as models with the highest validation accuracy in the models trained with synthetic GAN images. The best performing models have been further deployed in a smartphone and evaluated. The overall results suggest that VGG16 and Xception, trained with the synthetic images created using GAN, performed better compared to models trained with augmented images. Among these two models VGG16 produced an encouraging Diagnostic Odd Ratio (DOR) with higher positive likelihood and lower negative likelihood for the prediction of COVID-19.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008241",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Coronavirus disease 2019 (COVID-19)",
      "Deep learning",
      "Disease",
      "Generative adversarial network",
      "Infectious disease (medical specialty)",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Rangarajan",
        "given_name": "Aravind Krishnaswamy"
      },
      {
        "surname": "Ramachandran",
        "given_name": "Hari Krishnan"
      }
    ]
  },
  {
    "title": "Car crash detection using ensemble deep learning and multimodal data from dashboard cameras",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115400",
    "abstract": "Due to the increase in motor vehicle accidents, there is a growing need for high-performance car crash detection systems. The authors of this research propose a car crash detection system that uses both video data and audio data from dashboard cameras in order to improve car crash detection performance. While most existing car crash detection systems depend on single modal data (i.e., video data or audio data only), the proposed car crash detection system uses an ensemble deep learning model based on multimodal data (i.e., both video and audio data), because different types of data extracted from one information source (e.g., dashboard cameras) can be regarded as different views of the same source. These different views complement one another and improve detection performance, because one view may have information that the other view does not contain. In this research, deep learning techniques, gated recurrent unit (GRU) and convolutional neural network (CNN), are used to develop a car crash detection system. A weighted average ensemble is used as an ensemble technique. The proposed car crash detection system, which is based on multiple classifiers that use both video and audio data from dashboard cameras, is validated using a comparison with single classifiers that use video data or audio data only. Car accident YouTube clips are used to validate this research. The experimental results indicate that the proposed car crash detection system performs significantly better than single classifiers. It is expected that the proposed car crash detection system can be used as part of an emergency road call service that recognizes traffic accidents automatically and allows immediate rescue after transmission to emergency recovery agencies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100823X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Crash",
      "Dashboard",
      "Database",
      "Deep learning",
      "Machine learning",
      "Programming language",
      "Real-time computing"
    ],
    "authors": [
      {
        "surname": "Choi",
        "given_name": "Jae Gyeong"
      },
      {
        "surname": "Kong",
        "given_name": "Chan Woo"
      },
      {
        "surname": "Kim",
        "given_name": "Gyeongho"
      },
      {
        "surname": "Lim",
        "given_name": "Sunghoon"
      }
    ]
  },
  {
    "title": "Solving traveling salesman problem using hybridization of rider optimization and spotted hyena optimization algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115353",
    "abstract": "Traveling Salesman Problem (TSP) is the combinatorial optimization problem, where a salesman starting from a home city travels all the other cities and returns to the home city in the shortest possible path. TSP is a popular problem because the instances of TSP can be applied to solve real-world problems, the implication of which turns TSP into a typical test bench for performance evaluation of novel algorithms. In the current years, different optimization algorithms inspired by biological groups have become very familiar. A combined intelligence of diverse social insects like bees, ants, birds, termites, fish, etc. has been analyzed to introduce multiple meta-heuristic algorithms in the field of swarm intelligence. The main intent of this paper is to develop a hybrid algorithm for solving the TSP robustly and effectively. In order to attain this challenging point, the objective model considered in this research work is the minimization of the distance of the salesman traveling through entire cities. Here, the optimal solution pertains to solve the TSP is to minimize the distance travelled by the salesman, which is determined based on the new hybrid optimization algorithm. This proposal plans to integrate the two well-performing optimization algorithms like Rider Optimization Algorithm (ROA) and Spotted Hyena Optimizer algorithm (SHO) to frame the new algorithm, Spotted Hyena-based Rider Optimization (S-ROA). Finally, the experimental results obtained by the hybrid algorithm to solve these TSP cases are benchmarked against the results obtained by using state-of-the-art algorithms and prove the competitive performance of the proposed model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007818",
    "keywords": [
      "2-opt",
      "Algorithm",
      "Biology",
      "Computer science",
      "Ecology",
      "Hyena",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Nearest neighbour algorithm",
      "Optimization problem",
      "Particle swarm optimization",
      "Swarm intelligence",
      "Travelling salesman problem"
    ],
    "authors": [
      {
        "surname": "Krishna",
        "given_name": "Madugula Murali"
      },
      {
        "surname": "Panda",
        "given_name": "Nibedan"
      },
      {
        "surname": "Majhi",
        "given_name": "Santosh Kumar"
      }
    ]
  },
  {
    "title": "Neural network with fixed noise for index-tracking portfolio optimization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115298",
    "abstract": "Index tracking portfolio optimization is popular form of passive investment strategy, with a steady and profitable performance compared to an active investment strategy. Due to the revival of deep learning in recent years, several studies have been conducted to apply deep learning in the field of finance. However, most studies use deep learning exclusively to predict stock price movement, not to optimize the portfolio directly. We propose a deep learning framework to optimize the index-tracking portfolio and overcome this limitation. We use the output distribution of the softmax layer from the fixed noise as the portfolio weights and verify the tracking performance of the proposed method on the S&P 500 index. Furthermore, by performing the ablation studies on the training-validation dataset split ratio and data normalization, we demonstrate that these are critical parameters for applying deep learning to the portfolio optimization problem. We also verify the generalization performance of the proposed method through additional experiments with another index of a major stock market, the Hang Seng Index (HSI).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007284",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Deep learning",
      "Economics",
      "Finance",
      "Horse",
      "Index (typography)",
      "Machine learning",
      "Paleontology",
      "Portfolio",
      "Portfolio optimization",
      "Rate of return on a portfolio",
      "Softmax function",
      "Stock market",
      "Stock market index",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Kwak",
        "given_name": "Yuyeong"
      },
      {
        "surname": "Song",
        "given_name": "Junho"
      },
      {
        "surname": "Lee",
        "given_name": "Hongchul"
      }
    ]
  },
  {
    "title": "Mexican sign language segmentation using color based neuronal networks to detect the individual skin color",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115295",
    "abstract": "In recent years, the development of algorithms that assist in communicate with deaf people is an important challenge. The development of automatic systems to translate sign language is a current research topic. However, this involves several processes that range from video capture, pre-processing to identification or classification of the signal. The development of systems capable of extracting discriminative features that enhance the power of generalization of a classifier is even a very challenging problem. The meaning of a sign is the combination of the hand movement, hand shape, and the point of contact of the hand in the body. This paper presents a method to detect and translate hand gestures. First, we obtain 15 frames per word, obtaining 3 regions of interest (hands and face) from which we obtain geometric features. Finally we use several classifier techniques and present the experimental results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007260",
    "keywords": [
      "American Sign Language",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Gesture",
      "Gesture recognition",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Segmentation",
      "Sign language",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Espejel-Cabrera",
        "given_name": "Josué"
      },
      {
        "surname": "Cervantes",
        "given_name": "Jair"
      },
      {
        "surname": "García-Lamont",
        "given_name": "Farid"
      },
      {
        "surname": "Ruiz Castilla",
        "given_name": "José Sergio"
      },
      {
        "surname": "D. Jalili",
        "given_name": "Laura"
      }
    ]
  },
  {
    "title": "A neighborhood information utilization fireworks algorithm and its application to traffic flow prediction",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115189",
    "abstract": "A Fireworks Algorithm (FWA) is a novel intelligent optimization algorithm inspired by fireworks explosion, and it has been shown to be superior or competitive in various fields. Current FWA generates sparks without considering neighborhood information. This work improves sparks generation process by utilizing neighborhood information, and thus a neighborhood information utilization fireworks algorithm (NiFWA) is proposed. Specifically, fireworks are first divided into different subpopulations at different stages, and each subpopulation is considered as a whole, as well as it is also a neighborhood of other subpopulations. Then, a firework in a subpopulation generates its offspring by incorporating neighborhood information. Experimental results show that the proposed method is superior to FWA and its variants, competitive to state-of-the-art methods on CEC2013, CEC2017, three real world problems from CEC2010. Finally, NiFWA is used to assist kernel extreme learning machine in predicting the traffic flow.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006242",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Computer security",
      "Fireworks",
      "Information flow",
      "Kernel (algebra)",
      "Linguistics",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Organic chemistry",
      "Philosophy",
      "Process (computing)",
      "Traffic flow (computer networking)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Xiaojing"
      },
      {
        "surname": "Qin",
        "given_name": "Xiaolin"
      }
    ]
  },
  {
    "title": "Bhattacharyya distance based concept drift detection method for evolving data stream",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115303",
    "abstract": "The majority of online learners assume that the data distribution to be learned is established in advance. There are many real-world problems where the distribution of the data changes as a function of time. Variations in data streams data distributions can sufficiently reduce the skill of the learning algorithm on new data, if the learning algorithm is not equipped to track such changes. Therefore, the algorithm should initiate required actions to make sure that the new information is learned properly. In this article, we propose Bhattacharyya Distance-based Concept Drift Detection Method (BDDM) which uses Bhattacharyya distance to identify gradual or abrupt variations in the distribution. Experiments executed in the MOA framework using three artificial data generators and ten real-world datasets suggest that BDDM improves the detections and accuracies in many scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007326",
    "keywords": [
      "Artificial intelligence",
      "Bhattacharyya distance",
      "Biology",
      "Computer science",
      "Concept drift",
      "Data mining",
      "Data stream",
      "Data stream mining",
      "Evolutionary biology",
      "Function (biology)",
      "Machine learning",
      "Streaming data",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Baidari",
        "given_name": "Ishwar"
      },
      {
        "surname": "Honnikoll",
        "given_name": "Nagaraj"
      }
    ]
  },
  {
    "title": "Extended computational formulations for tolerance-based sensitivity analysis of uncertain transportation networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115252",
    "abstract": "Catastrophes may lead to disruption of human activities and extended damage to network infrastructures, which makes the transportation networks ’uncertain’. Therefore, crisis managers and transportation network computer-based systems (expert systems) need to use reliable facts and solid computational approaches to solve complex decision-making problems in pathfinding and designing transportation networks. In this context, the present work makes a step towards the stability analysis and reliability assessment of uncertain transportation networks (UTNs) and introduces an uncertainty theory-based mathematical model for post-crisis rescue work and on-time arrival of vehicles in disaster areas. This model incorporates uncertain reliability/risk variables associated with links to assess the maximum reliable transmission paths (MRTP) problem and studies the robustness of MRTPs in a post-crisis transportation network. Particularly, uncertainty theory is utilized to model and solve the problem of uncertain links’ tolerances. Unlike the well-known, straightforward frameworks for the tolerance evaluation of a single element, as studied in OR, a generalization of the tolerance-based stability analysis is articulated while perturbations in a set of links are considered, which characterizes the smallest and largest values between which a group of links may vary simultaneously while retaining the optimality of the MRTPs. Such tolerances are referred as to set tolerances, that are usually challenging to calculate. As far as we are concerned, this study is of the earliest ones investigating the uncertain set tolerance stability analysis problem. It proves that set tolerances are well defined and proposes computational formulations in the framework of uncertainty programming to show how such quantities can be systematically calculated or bounded, which are indeed far superior to successive re-optimizations. Finally, the practicality of the model and methods is demonstrated by adopting samples from our real case study and randomly generated networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006849",
    "keywords": [
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Context (archaeology)",
      "Distributed computing",
      "Electronic engineering",
      "Engineering",
      "Gene",
      "Generalization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Operations research",
      "Paleontology",
      "Physics",
      "Power (physics)",
      "Programming language",
      "Quantum mechanics",
      "Reliability (semiconductor)",
      "Robustness (evolution)",
      "Sensitivity (control systems)",
      "Set (abstract data type)",
      "Stability (learning theory)"
    ],
    "authors": [
      {
        "surname": "Hosseini",
        "given_name": "Ahmad"
      },
      {
        "surname": "Pishvaee",
        "given_name": "Mir Saman"
      }
    ]
  },
  {
    "title": "Multi-view content-context information bottleneck for image clustering",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115374",
    "abstract": "Image clustering is one of the most significant problems in computer vision and data mining. To mitigate the influence brought by appearance variation, many scholars attempt to cluster images with multiple features, a.k.a, multi-view image clustering. However, the majority of existing methods simply consider context information across images or content information within images, failing to combine both sides. We in this paper propose a novel auto-weighted multi-view content-context information bottleneck (AMC2IB) method for image clustering. The AMC2IB method can simultaneously utilize the content and context information for partitioning images. The “content” characterizes the intrinsic information within each image, e.g., appearance feature of color or shape; The “context” describes the close relationships among images of each view, e.g., inter-image similarity. The mechanism of maximum entropy is also introduced to automatically learn the view weight, and thus the importance of different views can be integrated for effective clustering. Additionally, to remove the extra weight regularization parameter in AMC2IB, we further propose an auto-weighted multi-view content-context information bottleneck without weight regularization (AMC2IBW) method. Afterwards, the above problems can be formulated as information loss functions by maintaining the context and content information maximally when the input images are compressed. Eventually, a new alternating iterative method is designed for the optimization of both proposed objective functions. Experimental results on five real-world multi-view image datasets show the superiority of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008009",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Bottleneck",
      "Cluster analysis",
      "Computer science",
      "Content (measure theory)",
      "Context (archaeology)",
      "Data mining",
      "Embedded system",
      "Image (mathematics)",
      "Information bottleneck method",
      "Information retrieval",
      "Mathematical analysis",
      "Mathematics",
      "Paleontology"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Shizhe"
      },
      {
        "surname": "Wang",
        "given_name": "Bo"
      },
      {
        "surname": "Lou",
        "given_name": "Zhengzheng"
      },
      {
        "surname": "Ye",
        "given_name": "Yangdong"
      }
    ]
  },
  {
    "title": "Automation of software test data generation using genetic algorithm and reinforcement learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115446",
    "abstract": "Software testing is one of the most important methods of analyzing software quality assurance. This process is very time consuming and expensive and accounts for almost 50% of the software production cost. In addition to the cost problem, the nature of the test, which seeks errors in the program, is such that software engineers are not interested in doing the process, so we are looking to use automated methods to reduce the cost and time of the test. In the last decade, various methods have been introduced for the automatic test data generation, the purpose of which is to maximize the detection of errors by generating minimum amount of test data. The main issue in the test data generation process is to determine the input data of the program in such a way that it meets the specified test criterion. In this research, a structural method has been used in order to automate the process of test data generation considering the criterion of covering all finite paths. In structural methods, the problem is converted into a search problem and meta-heuristic algorithms are used to solve it. The proposed method in this paper is a memetic algorithm in which reinforcement learning is used as a local search method within a genetic algorithm. Experimental results have shown that this method is faster for test data generation than many existing evolutionary or meta-heuristic algorithms and can provide better coverage with fewer evaluations. Compared algorithms include: conventional genetic algorithm, a variety of improvements to the genetic algorithm, random search, particle swarm optimization, bees algorithm, ant colony optimization, simulated annealing, hill climbing, and tabu search.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008605",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Automation",
      "Computer science",
      "Data mining",
      "Engineering",
      "Genetic algorithm",
      "Heuristic",
      "Machine learning",
      "Mechanical engineering",
      "Memetic algorithm",
      "Operating system",
      "Process (computing)",
      "Programming language",
      "Search-based software engineering",
      "Software",
      "Software construction",
      "Software development",
      "Software development process",
      "Software engineering",
      "Software system",
      "Test Management Approach",
      "Test data"
    ],
    "authors": [
      {
        "surname": "Esnaashari",
        "given_name": "Mehdi"
      },
      {
        "surname": "Damia",
        "given_name": "Amir Hossein"
      }
    ]
  },
  {
    "title": "Comparison of metaheuristic optimization algorithms for solving constrained mechanical design optimization problems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115351",
    "abstract": "Determining the solution for real mechanical design problems is a challenging task when using the newly developed and efficient swarm intelligence algorithms. There are so many difficulties to be addressed, including but not limited to mixed decision variables, diverse constraints, inherent errors, conflicting objectives, and numerous locally optimal solutions. This work analyzes the behavior of nine metaheuristic algorithms, namely, salp swarm algorithm (SSA), multi-verse optimizer (MVO), moth-flame optimizer (MFO), atom search optimization (ASO), ecogeography-based optimization (EBO), queuing search algorithm (QSA), equilibrium optimizer (EO), evolutionary strategy (ES) and hybrid self-adaptive orthogonal genetic algorithm (HSOGA). The efficiency of these algorithms is evaluated on eight mechanical design problems using the solution quality and convergence analysis, which verifies the wide applicability of these algorithms to real-world application problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100779X",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Evolutionary algorithm",
      "Genetic algorithm",
      "Mathematical optimization",
      "Mathematics",
      "Meta-optimization",
      "Metaheuristic",
      "Optimization problem",
      "Parallel metaheuristic",
      "Particle swarm optimization",
      "Swarm intelligence"
    ],
    "authors": [
      {
        "surname": "Gupta",
        "given_name": "Shubham"
      },
      {
        "surname": "Abderazek",
        "given_name": "Hammoudi"
      },
      {
        "surname": "Yıldız",
        "given_name": "Betül Sultan"
      },
      {
        "surname": "Yildiz",
        "given_name": "Ali Riza"
      },
      {
        "surname": "Mirjalili",
        "given_name": "Seyedali"
      },
      {
        "surname": "Sait",
        "given_name": "Sadiq M."
      }
    ]
  },
  {
    "title": "Integrating Elman recurrent neural network with particle swarm optimization algorithms for an improved hybrid training of multidisciplinary datasets",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115441",
    "abstract": "There are several types of neural networks (NNs) that are widely used for data classification tasks. The supervised learning NN is an advanced network with a training algorithm for setting the weights and biases of the network in its training phase. However, traditional training algorithms such as backpropagation have some drawbacks, such as slow convergence speed and falling into local minima, which reduces the performance of the classifier. Therefore, different nature-inspired metaheuristic algorithms are integrated with the NN training algorithms to provide derivative-free solutions for complex classification problems. Consequently, this paper proposes the integration of a particle swarm optimization (PSO) algorithm with an improved Elman recurrent neural network (ERNN) to form a PSO-ERNN metaheuristic model. The key contribution of this study is the development of a new dimensional equation for ERNN architecture and the integration of PSO in ERNN learning to produce the PSO-ERNN model. The PSO is constructed to train the NN and ERNN models to achieve a fast convergence rate and avoid local minima problems. The PSO-ERNN model is validated by comparing it against the standard PSO-NN metaheuristic model and similar models from the literature. The PSO-NN and PSO-ERNN models are tested and evaluated using ten benchmark classification problems of breast cancer, heart, hepatitis, liver, wine, iris, lung cancer, yeast, Pima Indians diabetes, and ionosphere datasets. In the training phase, the results show that the PSO-ERNN model performs better than the PSO-NN model when the training set has a bigger size of samples. In the testing phase, the PSO-ERNN model outperforms the PSO-NN model for all the tested datasets except the lung cancer and yeast datasets, in which the accuracy percentage slightly decreases. In the validation phase, the PSO-ERNN model shows better performance quality in terms of accuracy percentage in six of the tested datasets. The average percentage of the training, testing, and validation accumulation show that the PSO-NN performs better than the PSO-ERNN in the lung cancer (87.27, 83.32), and heart (73.56, 70.64) datasets. On the other hand, the PSO-ERNN performs better than the PSO-NN in the iris (88.18, 86.74), hepatitis (88.60, 87.93), wine (89.16, 86.08), liver (73.56, 70.64), ionosphere (83.98, 78.94), and breast cancer (94.84, 91.17). PSO-NN and PSO-ERNN produce the same average results in the Pima Indians diabetes (84.00, 84.00) and yeast (91.31, 91.30) dataset. These results show clearly that the PSO-ERNN generally outperforms the PSO-NN when considering the overall results of the ten datasets. Nevertheless, the combinations of the PSO-NN and PSO-ERNN are proven to represent consistent and robust classification methods. The computational efficiencies of the training processes in both the PSO-NN and PSO-ERNN models are highly improved when coupled with the PSO.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008551",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Backpropagation",
      "Computer science",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Maxima and minima",
      "Metaheuristic",
      "Particle swarm optimization"
    ],
    "authors": [
      {
        "surname": "Ab Aziz",
        "given_name": "Mohamad Firdaus"
      },
      {
        "surname": "Mostafa",
        "given_name": "Salama A"
      },
      {
        "surname": "Mohd. Foozy",
        "given_name": "Cik Feresa"
      },
      {
        "surname": "Mohammed",
        "given_name": "Mazin Abed"
      },
      {
        "surname": "Elhoseny",
        "given_name": "Mohamed"
      },
      {
        "surname": "Abualkishik",
        "given_name": "Abedallah Zaid"
      }
    ]
  },
  {
    "title": "Kernel-based framework to estimate deformations of pneumothorax lung using relative position of anatomical landmarks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115288",
    "abstract": "In video-assisted thoracoscopic surgeries, successful procedures of nodule resection are highly dependent on the precise estimation of lung deformation between the inflated lung in the computed tomography (CT) images during preoperative planning and the deflated lung in the treatment views during surgery. Lungs in the pneumothorax state during surgery have a large volume change from normal lungs, making it difficult to build a mechanical model.The purpose of this study is to develop a deformation estimation method of 3D surface of a deflated lung from a few partial observations. To estimate deformations for a largely deformed lung, a kernel regression-based solution was introduced. The proposed method used a few landmarks to capture the partial deformation between the 3D surface mesh obtained from preoperative CT and the intraoperative anatomical positions. The deformation for each vertex of the entire mesh model was estimated per-vertex as a relative position from the landmarks. The landmarks were placed in the anatomical position of the lung’s outer contour. The method was applied on nine datasets of the left lungs of live beagle dogs. Contrast-enhanced CT images of the lungs were acquired.The proposed method achieved a local positional error of vertices of 2.74 mm, Hausdorff distance of 6.11 mm, and Dice similarity coefficient of 0.94. Moreover, the proposed method achieved the estimation lung deformations from a small number of training cases and a small observation area.This study contributes to data-driven modeling of pneumothorax deformation of the lung.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007193",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Deformation (meteorology)",
      "Economics",
      "Finance",
      "Image (mathematics)",
      "Image segmentation",
      "Internal medicine",
      "Kernel (algebra)",
      "Lung",
      "Mathematics",
      "Medicine",
      "Meteorology",
      "Physics",
      "Position (finance)",
      "Radiology",
      "Sørensen–Dice coefficient"
    ],
    "authors": [
      {
        "surname": "Yamamoto",
        "given_name": "Utako"
      },
      {
        "surname": "Nakao",
        "given_name": "Megumi"
      },
      {
        "surname": "Ohzeki",
        "given_name": "Masayuki"
      },
      {
        "surname": "Tokuno",
        "given_name": "Junko"
      },
      {
        "surname": "Chen-Yoshikawa",
        "given_name": "Toyofumi Fengshi"
      },
      {
        "surname": "Matsuda",
        "given_name": "Tetsuya"
      }
    ]
  },
  {
    "title": "Budget optimized dynamic virtual machine provisioning in hybrid cloud using fuzzy analytic hierarchy process",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115398",
    "abstract": "Among various deployment models, Hybrid cloud is the preferred model allowing customers to maximize cost savings and performance by leveraging advantage of quick provisioning capabilities of public cloud. However, due to vast diversity of cloud services, today’s struggle for the customers is to evaluate and discover the best fit Cloud Service Provider (CSP) for an efficient Virtual Machine (VM) provisioning in various hybrid cloud contracts of multi cloud environment. Currently there is no framework that allows customers to evaluate CSPs based on application workload, application environment and employ an budget optimized VM provisioning with dynamically varying factors while adhering to Service Level Agreement (SLA). Hence, the proposed VM Provisioning framework is a Multi Criteria Decision-Making (MCDM) model with an objective to provision the VMs from the well suited CSP in hybrid cloud. The framework initially derives weights for each decision-making input criteria considered as Quality of Service (QoS) Service Measurement Index (SMI) attributes using extent analysis method of Fuzzy Analytical processing (FHP). It ranks the CSP by aggregating weights of decision makers among the alternatives. The framework then proposes a budget optimized algorithm called BOPVM to provision VMs in the hybrid cloud based on the order of assigned rank to CSPs. The framework is evaluated and implemented on real time workload among the CSPs – Amazon, Azure and Openstack. The evaluation results shows significant provisioning cost savings of 50% with reduced provisioning time by 65% when compared with existing algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008216",
    "keywords": [
      "Analytic hierarchy process",
      "Artificial intelligence",
      "Cloud computing",
      "Computer network",
      "Computer science",
      "Distributed computing",
      "Engineering",
      "Fuzzy logic",
      "Multiple-criteria decision analysis",
      "Operating system",
      "Operations research",
      "Provisioning",
      "Quality of service",
      "Virtual machine",
      "Workload"
    ],
    "authors": [
      {
        "surname": "Radhika",
        "given_name": "E.G."
      },
      {
        "surname": "Sudha Sadasivam",
        "given_name": "G."
      }
    ]
  },
  {
    "title": "Fake news detection based on explicit and implicit signals of a hybrid crowd: An approach inspired in meta-learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115414",
    "abstract": "The problem of automatic Fake News detection in digital media of news distribution (DMND - e.g., social networks, online newspaper, etc) has become even more relevant. Among the main detection approaches, the one based on crowd signals from DMND users has stood out by obtaining promising results. In essence, in order to classify a piece of news as fake or not fake, such approach explores the collective sense by combining opinions (signals, i.e., votes about the classification of some news) of a high number of users (crowd), considering the reputations of these users regarding their capacity of identifying Fake News. Although promising, the Crowd Signals approach has a significant limitation: it depends on the explicit user opinion (which is not always available) about the classification of the analyzed news. Such unavailability may be caused by the absence of a functionality in the DMND that collects user opinion about the news, or by the simple option of the users in not giving their opinion. Facing this limitation, the present work raises the hypothesis that it is possible to build models of Fake News detection with a performance comparable to the Crowd Signals based approach, avoiding the dependence on the explicit opinion of DMND users. To validate this hypothesis, the present work proposes HCS, an approach based on crowd signals that considers implicit user opinions instead of the explicit ones. The implicit opinions are inferred from the behavior of users concerning the dissemination of the news analyzed. Inspired in Meta-Learning, the HCS can also use the explicit opinions from machines (news classification models) to complement the implicit user opinions by means of hybrid Crowds. Experiments carried out in five datasets presented significant evidence that confirms the raised hypothesis. Even without considering DMND users’ explicit opinions, HCS was able to achieve results comparable to the ones produced by the Crowd Signals approach. Besides that, the results also revealed a performance improvement of HCS when the implicit opinions of the users were combined with the explicit opinions of machines.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008344",
    "keywords": [
      "Advertising",
      "Business",
      "Computer science",
      "Crowdsourcing",
      "Data science",
      "Engineering",
      "Information retrieval",
      "Newspaper",
      "Reliability engineering",
      "Social media",
      "Unavailability",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Souza Freire",
        "given_name": "Paulo Márcio"
      },
      {
        "surname": "Matias da Silva",
        "given_name": "Flávio Roberto"
      },
      {
        "surname": "Goldschmidt",
        "given_name": "Ronaldo Ribeiro"
      }
    ]
  },
  {
    "title": "Discrete-time adaptive neural network control for steer-by-wire systems with disturbance observer",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115395",
    "abstract": "This paper investigates the design and implementation of the discrete-time adaptive neural network control with disturbance observer (DO) on a steer-by-wire (SbW) system, to simultaneously realize accurate tracking and anti-interference performance. Specifically, to approximate the lumped system uncertainty including the friction torque and self-aligning torque, the neural network is employed. To improve the steering tracking performance, the discrete-time identification model is proposed so that the tracking error and modeling error can be utilized to adjust the neural network updating law. Then, the unknown compound disturbances caused by external disturbance, Euler approximation errors and neural network approximation error are restrained by two DOs. Finally, the Lyapunov stability theory shows that the system tracking error is uniformly ultimately bounded. Both numerical simulations and experiments are implemented to show the superiority of the proposed controller.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008186",
    "keywords": [
      "Adaptive control",
      "Agronomy",
      "Algorithm",
      "Approximation error",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Bounded function",
      "Computer science",
      "Control (management)",
      "Control engineering",
      "Control theory (sociology)",
      "Controller (irrigation)",
      "Discrete time and continuous time",
      "Engineering",
      "Lyapunov function",
      "Lyapunov stability",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Nonlinear system",
      "Observer (physics)",
      "Pedagogy",
      "Physics",
      "Psychology",
      "Quantum mechanics",
      "Stability (learning theory)",
      "Statistics",
      "Thermodynamics",
      "Torque",
      "Tracking (education)",
      "Tracking error"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yunlong"
      },
      {
        "surname": "Wang",
        "given_name": "Yongfu"
      }
    ]
  },
  {
    "title": "Multi-criteria decision analysis for non-conformance diagnosis: A priority-based strategy combining data and business rules",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115212",
    "abstract": "Business process analytics and verification have become a major challenge for companies, especially when process data is stored across different systems. It is important to ensure Business Process Compliance in both data-flow perspectives and business rules that govern the organisation. In the verification of data-flow accuracy, the conformance of data to business rules is a key element, since essential to fulfil policies and statements that govern corporate behaviour. The inclusion of business rules in an existing and already deployed process, which therefore already counts on stored data, requires the checking of business rules against data to guarantee compliance. If inconsistency is detected then the source of the problem should be determined, by discerning whether it is due to an erroneous rule or to erroneous data. To automate this, a diagnosis methodology following the incorporation of business rules is proposed, which simultaneously combines business rules and data produced during the execution of the company processes. Due to the high number of possible explanations of faults (data and/or business rules), the likelihood of faults has been included to propose an ordered list. In order to reduce these possibilities, we rely on the ranking calculated by means of an AHP (Analytic Hierarchy Process) and incorporate the experience described by users and/or experts. The methodology proposed is based on the Constraint Programming paradigm which is evaluated using a real example. .",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100645X",
    "keywords": [
      "Artifact-centric business process model",
      "Business",
      "Business analysis",
      "Business analytics",
      "Business domain",
      "Business model",
      "Business process",
      "Business process discovery",
      "Business process management",
      "Business process modeling",
      "Business rule",
      "Computer science",
      "Data mining",
      "Data science",
      "Machine learning",
      "Marketing",
      "Operating system",
      "Process (computing)",
      "Process management",
      "Ranking (information retrieval)",
      "Semantics of Business Vocabulary and Business Rules",
      "Work in process"
    ],
    "authors": [
      {
        "surname": "Ceballos",
        "given_name": "Rafael"
      },
      {
        "surname": "Borrego",
        "given_name": "Diana"
      },
      {
        "surname": "Gómez-López",
        "given_name": "María Teresa"
      },
      {
        "surname": "M. Gasca",
        "given_name": "Rafael"
      }
    ]
  },
  {
    "title": "A new biologically inspired global optimization algorithm based on firebug reproductive swarming behaviour",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115408",
    "abstract": "A new biologically inspired derivative-free global optimization algorithm called Firebug Swarm Optimization (FSO) inspired by reproductive swarming behaviour of Firebugs (Pyrrhocoris apterus) is proposed. The search for fit reproductive partners by individual bugs in a swarm of Firebugs can be viewed naturally as a search for optimal solutions in a search space. This work proposes a mathematical model for five different Firebug behaviours most relevant to optimization and uses these behaviours as the basis of a new global optimization algorithm. Performance of the FSO algorithm is compared with 17 popular heuristic algorithms on the Congress of Evolutionary Computation 2013 (CEC 2013) benchmark suite that contains high dimensional multimodal as well as shifted and rotated functions. Statistical analysis based on Wilcoxon Rank-Sum Test indicates that the proposed FSO algorithm outperforms 17 popular state-of-the-art heuristic global optimization algorithms like Guided Sparks Fireworks Algorithm (GFWA), Dynamic Learning PSO (DNLPSO), and Artificial Bee Colony Bollinger Bands (ABCBB) on the CEC 2013 benchmark.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008290",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bees algorithm",
      "Benchmark (surveying)",
      "Computer science",
      "Geodesy",
      "Geography",
      "Global optimization",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Optimization problem",
      "Particle swarm optimization",
      "Swarm behaviour",
      "Swarm intelligence"
    ],
    "authors": [
      {
        "surname": "Noel",
        "given_name": "Mathew Mithra"
      },
      {
        "surname": "Muthiah-Nakarajan",
        "given_name": "Venkataraman"
      },
      {
        "surname": "Amali",
        "given_name": "Geraldine Bessie"
      },
      {
        "surname": "Trivedi",
        "given_name": "Advait Sanjay"
      }
    ]
  },
  {
    "title": "Deep-learning-based recognition of symbols and texts at an industrially applicable level from images of high-density piping and instrumentation diagrams",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115337",
    "abstract": "Piping and instrumentation diagrams (P&IDs) are commonly used in the process industry as a transfer medium for the fundamental design of a plant and for detailed design, purchasing, procurement, construction, and commissioning decisions. The present study proposes a method for symbol and text recognition for P&ID images using deep-learning technology. Our proposed method consists of P&ID image pre-processing, symbol and text recognition, and the storage of the recognition results. We consider the recognition of symbols of different sizes and shape complexities in high-density P&ID images in a manner that is applicable to the process industry. We also standardize the training dataset structure and symbol taxonomy to optimize the developed deep neural network. A training dataset is created based on diagrams provided by a local Korean company. After training the model with this dataset, a recognition test produced relatively good results, with a precision and recall of 0.9718 and 0.9827 for symbols and 0.9386 and 0.9175 for text, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007661",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Engineering",
      "Environmental engineering",
      "Instrumentation (computer programming)",
      "Machine learning",
      "Operating system",
      "Operations management",
      "Pattern recognition (psychology)",
      "Piping",
      "Process (computing)",
      "Programming language",
      "Purchasing",
      "Symbol (formal)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Hyungki"
      },
      {
        "surname": "Lee",
        "given_name": "Wonyong"
      },
      {
        "surname": "Kim",
        "given_name": "Mijoo"
      },
      {
        "surname": "Moon",
        "given_name": "Yoochan"
      },
      {
        "surname": "Lee",
        "given_name": "Taekyong"
      },
      {
        "surname": "Cho",
        "given_name": "Mincheol"
      },
      {
        "surname": "Mun",
        "given_name": "Duhwan"
      }
    ]
  },
  {
    "title": "A novel extreme learning machine based kNN classification method for dealing with big data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115293",
    "abstract": "kNN algorithm, as an effective data mining technique, is always attended for supervised classification. On the other hand, the previously proposed kNN finding methods cannot be considered as efficient methods for dealing with big data. As there is daily generated and expanded big datasets on different online and offline servers, the efficient methods for such data must be introduced to find kNN. Moreover, massive amounts of data contain more noise and imperfection data samples that significantly increase the need for a robust kNN finding method. In this paper, a new fast and robust kNN finding framework is introduced to deal with the big datasets. In this method, a group of most relevant data samples to an input data sample are detected and the original kNN method is applied on them for finding the final nearest neighbors. The main goal of this method is dealing with the big datasets in an accurate, fast, and robust manner. Here, the training data samples of each label are grouped into some partitions based on the output of some mini-classifiers (i.e. ELM classifier). In fact, the behavior of the mini-classifiers is the basis of partitioning the training data samples. These mini-classifiers are trained using non-overlapping subsets of the training set in the form of each mini-classifier a subset. Here, an index is calculated for each partition to make the corresponding partition finding faster using a tree structure in which each partition index is fallen into a leaf. Then, outputs of the mini-classifiers for an input test sample are used to find the corresponding group of most relevant data samples to the input data sample on the tree. Experimental results indicate that the proposed method has better performance in most cases and comparable performance on other cases of original and noisy big data problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007247",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Big data",
      "Classifier (UML)",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Extreme learning machine",
      "Machine learning",
      "Mathematics",
      "Partition (number theory)",
      "Pattern recognition (psychology)",
      "Training set"
    ],
    "authors": [
      {
        "surname": "Shokrzade",
        "given_name": "Amin"
      },
      {
        "surname": "Ramezani",
        "given_name": "Mohsen"
      },
      {
        "surname": "Akhlaghian Tab",
        "given_name": "Fardin"
      },
      {
        "surname": "Abdulla Mohammad",
        "given_name": "Mahmud"
      }
    ]
  },
  {
    "title": "Human decision making through an entropic framework",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114926",
    "abstract": "In this paper, new entropy functions are formulated based on an agent’s perceived uncertainty that inevitably affects the agent’s choice. The role of the decision-maker’s (DM’s) attitude is emphasized as one of the key determinants of such an entropy function. More specifically, new attitude-based variants of Shannon’s, Pal & Pal, and Aggarwal’s probabilistic entropies are introduced. The extant fuzzy entropies are also extended to consider the agent’s specific attitude. The proposed entropy functions provide a wide range of entropy values with the conventional entropy functions as their special cases. The special cases of the proposed entropies are examined. The wide applicability of the proposed entropy functions in multi criteria decision making is highlighted. A case-study is included to showcase the usefulness of the proposed entropy functions in the real world.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003675",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Decision maker",
      "Entropy (arrow of time)",
      "Evolutionary biology",
      "Extant taxon",
      "Mathematics",
      "Operations research",
      "Physics",
      "Probabilistic logic",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Aggarwal",
        "given_name": "Manish"
      }
    ]
  },
  {
    "title": "A FMEA based novel intuitionistic fuzzy approach proposal: Intuitionistic fuzzy advance MCDM and mathematical modeling integration",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115413",
    "abstract": "This study proposes a novel three-stage intuitionistic fuzzy risk assessment (RA) approach based on Failure Modes and Effects Analysis (FMEA). In this study, it was paid attention for considering real constraints of firms such as capital, time etc. to prevent nan-fatal failure modes (FMs), interactions between FMs and risk level similarities created by risk factors (RFs). At the first stage of the proposed approach, RFs’ weights are computed by a new intuitionistic fuzzy weighting method considering similarities between RFs for risk levels that they can create. At the second stage, Modified Intuitionistic Fuzzy Multi Attribute Border Approximation Area (MIF-MABAC) including interactions between FMs is used to determine the rankings of FMs by using Extended Haussdorff distance function. At the third stage, two intuitionistic fuzzy mathematical models are established to show the effect of the real constraints of the firm to identify the risk types (RTs) that must be avoided primarily. It was seen that the first model gives the same ranking results with the MIF-MABAC. Additionally, when including the real constraints, the first model can give the more suitable results than the second model. The results obtained from the first model show that experts’ assessments and mathematical modeling identify the same FMs for preventing primarily. This study is the first one to suggest a new RA approach that reflects the real constraints of the firms to RA. Additionally, this is the first study that models’ interactions between FMs and risk level similarities created by RFs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008332",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Data mining",
      "Evolutionary biology",
      "Function (biology)",
      "Fuzzy logic",
      "Mathematical optimization",
      "Mathematics",
      "Medicine",
      "Multiple-criteria decision analysis",
      "Operations research",
      "Radiology",
      "Ranking (information retrieval)",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Yener",
        "given_name": "Yelda"
      },
      {
        "surname": "Can",
        "given_name": "Gülin Feryal"
      }
    ]
  },
  {
    "title": "Fake news detection based on explicit and implicit signals of a hybrid crowd: An approach inspired in meta-learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115414",
    "abstract": "The problem of automatic Fake News detection in digital media of news distribution (DMND - e.g., social networks, online newspaper, etc) has become even more relevant. Among the main detection approaches, the one based on crowd signals from DMND users has stood out by obtaining promising results. In essence, in order to classify a piece of news as fake or not fake, such approach explores the collective sense by combining opinions (signals, i.e., votes about the classification of some news) of a high number of users (crowd), considering the reputations of these users regarding their capacity of identifying Fake News. Although promising, the Crowd Signals approach has a significant limitation: it depends on the explicit user opinion (which is not always available) about the classification of the analyzed news. Such unavailability may be caused by the absence of a functionality in the DMND that collects user opinion about the news, or by the simple option of the users in not giving their opinion. Facing this limitation, the present work raises the hypothesis that it is possible to build models of Fake News detection with a performance comparable to the Crowd Signals based approach, avoiding the dependence on the explicit opinion of DMND users. To validate this hypothesis, the present work proposes HCS, an approach based on crowd signals that considers implicit user opinions instead of the explicit ones. The implicit opinions are inferred from the behavior of users concerning the dissemination of the news analyzed. Inspired in Meta-Learning, the HCS can also use the explicit opinions from machines (news classification models) to complement the implicit user opinions by means of hybrid Crowds. Experiments carried out in five datasets presented significant evidence that confirms the raised hypothesis. Even without considering DMND users’ explicit opinions, HCS was able to achieve results comparable to the ones produced by the Crowd Signals approach. Besides that, the results also revealed a performance improvement of HCS when the implicit opinions of the users were combined with the explicit opinions of machines.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008344",
    "keywords": [
      "Advertising",
      "Business",
      "Computer science",
      "Crowdsourcing",
      "Data science",
      "Engineering",
      "Information retrieval",
      "Newspaper",
      "Reliability engineering",
      "Social media",
      "Unavailability",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Souza Freire",
        "given_name": "Paulo Márcio"
      },
      {
        "surname": "Matias da Silva",
        "given_name": "Flávio Roberto"
      },
      {
        "surname": "Goldschmidt",
        "given_name": "Ronaldo Ribeiro"
      }
    ]
  },
  {
    "title": "Discrete-time adaptive neural network control for steer-by-wire systems with disturbance observer",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115395",
    "abstract": "This paper investigates the design and implementation of the discrete-time adaptive neural network control with disturbance observer (DO) on a steer-by-wire (SbW) system, to simultaneously realize accurate tracking and anti-interference performance. Specifically, to approximate the lumped system uncertainty including the friction torque and self-aligning torque, the neural network is employed. To improve the steering tracking performance, the discrete-time identification model is proposed so that the tracking error and modeling error can be utilized to adjust the neural network updating law. Then, the unknown compound disturbances caused by external disturbance, Euler approximation errors and neural network approximation error are restrained by two DOs. Finally, the Lyapunov stability theory shows that the system tracking error is uniformly ultimately bounded. Both numerical simulations and experiments are implemented to show the superiority of the proposed controller.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008186",
    "keywords": [
      "Adaptive control",
      "Agronomy",
      "Algorithm",
      "Approximation error",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Bounded function",
      "Computer science",
      "Control (management)",
      "Control engineering",
      "Control theory (sociology)",
      "Controller (irrigation)",
      "Discrete time and continuous time",
      "Engineering",
      "Lyapunov function",
      "Lyapunov stability",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Nonlinear system",
      "Observer (physics)",
      "Pedagogy",
      "Physics",
      "Psychology",
      "Quantum mechanics",
      "Stability (learning theory)",
      "Statistics",
      "Thermodynamics",
      "Torque",
      "Tracking (education)",
      "Tracking error"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yunlong"
      },
      {
        "surname": "Wang",
        "given_name": "Yongfu"
      }
    ]
  },
  {
    "title": "Multi-criteria decision analysis for non-conformance diagnosis: A priority-based strategy combining data and business rules",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115212",
    "abstract": "Business process analytics and verification have become a major challenge for companies, especially when process data is stored across different systems. It is important to ensure Business Process Compliance in both data-flow perspectives and business rules that govern the organisation. In the verification of data-flow accuracy, the conformance of data to business rules is a key element, since essential to fulfil policies and statements that govern corporate behaviour. The inclusion of business rules in an existing and already deployed process, which therefore already counts on stored data, requires the checking of business rules against data to guarantee compliance. If inconsistency is detected then the source of the problem should be determined, by discerning whether it is due to an erroneous rule or to erroneous data. To automate this, a diagnosis methodology following the incorporation of business rules is proposed, which simultaneously combines business rules and data produced during the execution of the company processes. Due to the high number of possible explanations of faults (data and/or business rules), the likelihood of faults has been included to propose an ordered list. In order to reduce these possibilities, we rely on the ranking calculated by means of an AHP (Analytic Hierarchy Process) and incorporate the experience described by users and/or experts. The methodology proposed is based on the Constraint Programming paradigm which is evaluated using a real example. .",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100645X",
    "keywords": [
      "Artifact-centric business process model",
      "Business",
      "Business analysis",
      "Business analytics",
      "Business domain",
      "Business model",
      "Business process",
      "Business process discovery",
      "Business process management",
      "Business process modeling",
      "Business rule",
      "Computer science",
      "Data mining",
      "Data science",
      "Machine learning",
      "Marketing",
      "Operating system",
      "Process (computing)",
      "Process management",
      "Ranking (information retrieval)",
      "Semantics of Business Vocabulary and Business Rules",
      "Work in process"
    ],
    "authors": [
      {
        "surname": "Ceballos",
        "given_name": "Rafael"
      },
      {
        "surname": "Borrego",
        "given_name": "Diana"
      },
      {
        "surname": "Gómez-López",
        "given_name": "María Teresa"
      },
      {
        "surname": "M. Gasca",
        "given_name": "Rafael"
      }
    ]
  },
  {
    "title": "Improving recommender systems via a Dual Training Error based Correction approach",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115386",
    "abstract": "We propose a method to improve the prediction performance of recommender systems via a Dual (user anditem) Training Error based Correction approach (DTEC). The proposed method is applied to the Synthetic Coordinate Recommendation system (SCoR) (Papadakis et al., 2017) and to other Ithree state-of-the-art systems. Initially, a recommender system is used Ito provide recommendations for users and items. Subsequently, we introduce a second stage, after initial execution of the recommender system, that improves its predictions taking into account the error in the training set between users and items and their similarity. These corrections can be performed from both user and item viewpoints, and finally a dual system is proposed that efficiently combines both corrections. DTEC computes a model that makes zero the recommendation error in the training set, and then applies it on the test set to improve the rating predictions. The proposed DTEC approach is applicable Ito any model-based recommender system with positive training error, potentially increasing the accuracy of the recommendations. The experimental results demonstrate the efficiency and high performance of DTEC on four well-known, real-world datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008101",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Dual (grammatical number)",
      "Image (mathematics)",
      "Information retrieval",
      "Literature",
      "Machine learning",
      "Programming language",
      "Recommender system",
      "Set (abstract data type)",
      "Similarity (geometry)",
      "Viewpoints",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Panagiotakis",
        "given_name": "Costas"
      },
      {
        "surname": "Papadakis",
        "given_name": "Harris"
      },
      {
        "surname": "Papagrigoriou",
        "given_name": "Antonis"
      },
      {
        "surname": "Fragopoulou",
        "given_name": "Paraskevi"
      }
    ]
  },
  {
    "title": "A new oversampling method in the string space",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115428",
    "abstract": "In syntactic and structural pattern recognition, data represented as strings appear in several supervised classification applications. In some situations, data collections show imbalanced class distributions, which typically results in the classifier biasing its performance to the class representing the majority of objects. To solve this problem, some oversampling methods have been proposed for data represented as strings. However, this type of method has been little studied in the literature. Therefore, in this paper, we present an oversampling method for working in string space that balances the minority class and gets better classification results than state-of-the-art oversampling methods, especially for highly imbalanced problems. Furthermore, according to our experiments, the proposed method is much faster than those reported in the literature.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008459",
    "keywords": [
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer network",
      "Computer science",
      "Machine learning",
      "Mathematical physics",
      "Mathematics",
      "Operating system",
      "Oversampling",
      "Pattern recognition (psychology)",
      "Space (punctuation)",
      "String (physics)"
    ],
    "authors": [
      {
        "surname": "Briones-Segovia",
        "given_name": "Víctor A."
      },
      {
        "surname": "Jiménez-Villar",
        "given_name": "Víctor"
      },
      {
        "surname": "Carrasco-Ochoa",
        "given_name": "Jesús Ariel"
      },
      {
        "surname": "Martínez-Trinidad",
        "given_name": "José Fco."
      }
    ]
  },
  {
    "title": "Combining max-pooling and wavelet pooling strategies for semantic image segmentation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115403",
    "abstract": "This paper presents a novel multi-pooling architecture generated by combining the advantages of wavelet and max-pooling operations in convolutional neural networks (CNNs), focusing on semantic segmentation tasks. CNNs often use pooling to reduce the number of parameters, improve invariance to certain distortions, and enlarge the receptive field. However, pooling can cause information loss and thus is detrimental to further operations such as feature extraction and analysis. This problem is particularly critical for semantic segmentation, where each pixel of an image is assigned to a specific class to divide the image into disjoint regions of interest. To address this problem, pooling strategies based on wavelets-operations have been proposed with the promise to achieve a better trade-off between receptive field size and computational efficiency. Previous works have confirmed the superiority of wavelet pooling over the traditional one in semantic segmentation tasks. However, we have observed in our computational experiments that the expressive gains reported from the use of wavelet pooling in other segmentation tasks were not observed in the scope of aerial imagery due to imprecision in the segmentation of image details. The combination of wavelet pooling and max-pooling, a solution not yet reported in the literature, can address that issue. Such gap observed in the pooling area motivated the two proposals that are the main contributions of this paper: (a) A new multi-pooling strategy combining wavelet and traditional pooling in a new network structure suitable for aerial image segmentation tasks; (b) Two-stream architectures using the traditional max-pooling and wavelet pooling as streams. These proposals were implemented using the Segnet, a known architecture for semantic segmentation. The computational experiments, based on the IRRG images from the Potsdam and Vaihingen data sets, demonstrated that the proposed architectures surpassed the original Segnet architecture’s performance with results comparable to state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008253",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Discrete wavelet transform",
      "Gabor wavelet",
      "Image segmentation",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Pooling",
      "Segmentation",
      "Wavelet",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "de Souza Brito",
        "given_name": "André"
      },
      {
        "surname": "Vieira",
        "given_name": "Marcelo Bernardes"
      },
      {
        "surname": "de Andrade",
        "given_name": "Mauren Louise Sguario Coelho"
      },
      {
        "surname": "Feitosa",
        "given_name": "Raul Queiroz"
      },
      {
        "surname": "Giraldi",
        "given_name": "Gilson Antonio"
      }
    ]
  },
  {
    "title": "TPCNN: Two-path convolutional neural network for tumor and liver segmentation in CT images using a novel encoding approach",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115406",
    "abstract": "Automatic liver and tumour segmentation in CT images are crucial in numerous clinical applications, such as postoperative assessment, surgical planning, and pathological diagnosis of hepatic diseases. However, there are still a considerable number of difficulties to overcome due to the fuzzy boundary, irregular shapes, and complex tissues of the liver. In this paper, for liver and tumor segmentation and to overcome the mentioned challenges a simple but powerful strategy is presented based on a cascade convolutional neural network. At the first, the input image is normalized using the Z-Score algorithm. This normalized image provides more information about the boundary of tumor and liver. Also, the Local Direction of Gradient (LDOG) which is a novel encoding algorithm is proposed to demonstrate some key features inside the image. The proposed encoding image is highly effective in recognizing the border of liver, even in the regions close to the touching organs. Then, a cascade CNN structure for extracting both local and semi-global features is used which utilized the original image and two other obtained images as the input data. Rather than using a complex deep CNN model with a lot of hyperparameters, we employ a simple but effective model to decrease the train and testing time. Our technique outperforms the state-of-the-art works in terms of segmentation accuracy and efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008289",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Boundary (topology)",
      "Cascade",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Encoding (memory)",
      "Hyperparameter",
      "Image (mathematics)",
      "Image segmentation",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Aghamohammadi",
        "given_name": "Amirhossein"
      },
      {
        "surname": "Ranjbarzadeh",
        "given_name": "Ramin"
      },
      {
        "surname": "Naiemi",
        "given_name": "Fatemeh"
      },
      {
        "surname": "Mogharrebi",
        "given_name": "Marzieh"
      },
      {
        "surname": "Dorosti",
        "given_name": "Shadi"
      },
      {
        "surname": "Bendechache",
        "given_name": "Malika"
      }
    ]
  },
  {
    "title": "Probability learning based tabu search for the budgeted maximum coverage problem",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115310",
    "abstract": "The Budgeted Maximum Coverage Problem (BMCP) is a general model with a number of real-world applications. Given n elements with nonnegative profits, m subsets of elements with nonnegative weights and a total budget, the BMCP aims to select some subsets such that the total weight of the selected subsets does not exceed the budget, while the total profit of the associated elements is maximized. BMCP is NP-hard and thus computationally challenging. We investigate for the first time an effective practical algorithm for solving this problem, which combines reinforcement learning and local search. The algorithm iterates through two distinct phases, namely a tabu search phase and a probability learning based perturbation phase. To assess the effectiveness of the proposed algorithm, we show computational results on a set of 30 benchmark instances introduced in this paper and present comparative studies with respect to the approximation algorithm, the genetic algorithm and the CPLEX solver.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007399",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Geodesy",
      "Geography",
      "Iterated function",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Programming language",
      "Reinforcement learning",
      "Set (abstract data type)",
      "Solver",
      "Tabu search"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Liwen"
      },
      {
        "surname": "Wei",
        "given_name": "Zequn"
      },
      {
        "surname": "Hao",
        "given_name": "Jin-Kao"
      },
      {
        "surname": "He",
        "given_name": "Kun"
      }
    ]
  },
  {
    "title": "Share price prediction of aerospace relevant companies with recurrent neural networks based on PCA",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115384",
    "abstract": "The capital market plays a vital role in marketing operations for the rapid development of the aerospace industry. However, due to the uncertainty and complexity of the stock market and many cyclical factors, the stock prices of listed aerospace companies fluctuate significantly. This makes the share price prediction challengeable. To improve the prediction of share price for aerospace industry sector and well understand the impact of various indicators on stock prices, we provided a hybrid prediction model by the combination of Principal Component Analysis (PCA) and Recurrent Neural Networks (RNN). We investigated two types of aerospace industries (manufacturer and operator). The experimental results show that PCA could improve both accuracy and efficiency of prediction. Various factors could influence the performance of prediction models, such as finance data, extracted features, optimisation algorithms, and parameters of the prediction model. The selection of features may depend on the stability of historical data: technical features could be the first option when the share price is stable, whereas fundamental features could be better when the share price has high fluctuation. The delays of RNN also depend on the stability of historical data for different types of companies. It would be more accurate through using short-term historical data for aerospace manufacturers, whereas using long-term historical data for aerospace operating airlines. The developed model could be an intelligent agent in an automatic stock prediction system, with which, the financial industry could make a prompt decision for their economic strategies and business activities in terms of predicted future share price, thus improving the return on investment. The study is for the prediction of aerospace industries at pre-COVID-19 time. Currently, COVID-19 severely influences aerospace industries. The developed approach can be used to predict the share price of aerospace industries at post COVID-19 time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008095",
    "keywords": [
      "Aerospace",
      "Aerospace engineering",
      "Artificial intelligence",
      "Artificial neural network",
      "Business",
      "Computer science",
      "Econometrics",
      "Economic forecasting",
      "Economics",
      "Engineering",
      "Finance",
      "Machine learning",
      "Market share",
      "Operations research",
      "Predictive modelling",
      "Principal component analysis",
      "Share price",
      "Stock exchange"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Linyu"
      },
      {
        "surname": "He",
        "given_name": "Hongmei"
      }
    ]
  },
  {
    "title": "Trading strategy of structured mutual fund based on deep learning network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115390",
    "abstract": "Considering the trading behavior and trading share numbers, we construct a deep learning network for trading strategies of structured mutual funds. Using data of the structured mutual funds in China’s financial markets from 2014 to 2020, we train and test the deep learning networks with single output of share numbers, dependent dual outputs of share numbers and trading behavior, and independent dual outputs of share numbers and trading behavior, by using transfer learning networks and two-stage training methods. The empirical results show that: (1) deep learning network can significantly improve the trading performance and enhance the generalization ability of the trading models; (2) both two-stage training and transfer learning can improve the model performance, and additionally, transfer learning can stabilize the portfolios’ returns; (3) incorporating trading behavior and trading share numbers simultaneously in the deep learning model can improve its performance, especially when the two outputs are independent; and (4) in the times of market uncertainty, the deep learning network with holding action outperforms the benchmark models with selling or buying actions, which suggests that the hold-and-wait strategy is useful in the Chinese financial markets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008149",
    "keywords": [
      "Algorithmic trading",
      "Art",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Business",
      "Computer science",
      "Deep learning",
      "Dual (grammatical number)",
      "Finance",
      "Financial market",
      "Generalization",
      "Geodesy",
      "Geography",
      "Literature",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Trading strategy",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Jiao"
      },
      {
        "surname": "Luo",
        "given_name": "Changqing"
      },
      {
        "surname": "Pan",
        "given_name": "Lurun"
      },
      {
        "surname": "Jia",
        "given_name": "Yun"
      }
    ]
  },
  {
    "title": "A novel self-learning feature selection approach based on feature attributions",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115219",
    "abstract": "Feature selection has shown its effectiveness in improving the accuracy and generalization of machine learning models, especially for those tasks with high-dimensional data. In this article, a novel self-learning feature selection (SLFS) approach based on feature attributions is proposed as a wrapper method, which has higher search efficiency for optimal feature subsets with three main improvements. First, we regard feature selection as a combinatorial optimization problem and propose a unified local search framework for wrapper methods by analyzing meta-heuristic algorithms in feature selection. Second, for the binary search space of feature selection, we propose two types of neighborhood structures, namely, ring-type and line-type structures, for the local search framework. Third, we focus on feature attribution methods, such as SHAP (SHapley Additive exPlanations) (Lundberg & Lee, 2017), which can interpret each feature’s importance to predictions. In each iteration, we adopt SHAP values and other attributes from previous subsets to guide the next selection of new subsets. To validate the performance of our SLFS approach, we collected 16 classification datasets from the UCI repository for comparison with other meta-heuristic wrapper approaches in terms of fitness, accuracy, F1 scores and selection ratios. The experimental results show that the SLFS approach can be used to obtain an optimal subset with fewer iterations and a small population, and SHAP values play a role in improving search efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006527",
    "keywords": [
      "Artificial intelligence",
      "Attribution",
      "Computer science",
      "Feature (linguistics)",
      "Feature learning",
      "Feature selection",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Psychology",
      "Selection (genetic algorithm)",
      "Social psychology"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Jianting"
      },
      {
        "surname": "Yuan",
        "given_name": "Shuhan"
      },
      {
        "surname": "Lv",
        "given_name": "Dongdong"
      },
      {
        "surname": "Xiang",
        "given_name": "Yang"
      }
    ]
  },
  {
    "title": "A new optimization method based on COOT bird natural life model",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115352",
    "abstract": "Recently, many intelligent algorithms have been proposed to find the best solution for complex engineering problems. These algorithms can search volatile and multi-dimensional solution spaces and find optimal answers timely. In this paper, a new meta-heuristic method is proposed that inspires the behavior of the swarm of birds called Coot. The Coot algorithm imitates two different modes of movement of birds on the water surface: in the first phase, the movement of birds is irregular, and in the second phase, the movements are regular. The swarm moves towards a group of leading leaders to reach a food supply; the movement of the end of the swarm is in the form of a chain of coots, each of coot which moves behind its front coots. The algorithm then runs on a number of test functions, and the results are compared with well-known optimization algorithms. In addition, to solve several real problems, such as Tension/Compression spring, Pressure vessel design, Welded Beam Design, Multi-plate disc clutch brake, Step-cone pulley problem, Cantilever beam design, reducer design problem, and Rolling element bearing problem this algorithm is used to confirm the applicability of this algorithm. The results show that this algorithm is capable to outperform most of the other optimization methods. The source code is currently available for public from: https://www.mathworks.com/matlabcentral/fileexchange/89102-coot-optimization-algorithm .",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007806",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Optimization problem",
      "Swarm behaviour"
    ],
    "authors": [
      {
        "surname": "Naruei",
        "given_name": "Iraj"
      },
      {
        "surname": "Keynia",
        "given_name": "Farshid"
      }
    ]
  },
  {
    "title": "Novel correlation coefficient between hesitant fuzzy sets with application to medical diagnosis",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115393",
    "abstract": "As an extension of the fuzzy set, the hesitant fuzzy set (HFS) is an effective tool for handling uncertainty and vagueness in decision making problems. Considering that the correlation coefficient (CC) has a strong ability to process and analyze data, we are developing a novel CC to measure the strength of the relationship between HFSs in this article. The CC presented between the HFSs has more desirable properties than the current ones. It relaxes limits on the length of the hesitant fuzzy elements (HFEs) and can be used to determine whether the HFSs are negatively or positively correlated. More importantly, it can ensure that the CC between two HFSs is equal to one (minus one) if and only if the two HFSs are the same (complement each other), and thus avoid the achievement of counter-intuitive decision results by inappropriate calculation approaches. The motivation of re-visiting the CC between HFSs is that a more effective CC between HFSs should be developed in order to significantly improve decision-making performance. To demonstrate the effectiveness of the proposed method, a case study on medical diagnosis is offered and the comparative analyses with other methods are also conducted.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008162",
    "keywords": [
      "Antenna (radio)",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Complement (music)",
      "Complementation",
      "Computer science",
      "Correlation coefficient",
      "Data mining",
      "Fuzzy logic",
      "Fuzzy set",
      "Gene",
      "HFSS",
      "Machine learning",
      "Mathematics",
      "Measure (data warehouse)",
      "Microstrip antenna",
      "Operating system",
      "Phenotype",
      "Process (computing)",
      "Programming language",
      "Set (abstract data type)",
      "Telecommunications",
      "Vagueness"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Xiaodi"
      },
      {
        "surname": "Wang",
        "given_name": "Zengwen"
      },
      {
        "surname": "Zhang",
        "given_name": "Shitao"
      },
      {
        "surname": "Garg",
        "given_name": "Harish"
      }
    ]
  },
  {
    "title": "An integrated contract manufacturer selection and product quality optimization methodology for the mechanical manufacturing industry",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115336",
    "abstract": "Outsourcing is a crucial strategy of the mechanical manufacturing industry. When outsourcing, it is necessary for companies to carefully inspect the performance of contract manufacturers (CMs) to ensure high-quality products and satisfactory services. This study presents an integrated methodology for the quality performance assessment and optimization of products with multiple quality characteristics for CM selection in the mechanical manufacturing industry. First, we present a novel process capability analysis chart (PCAC) using the 100(1–α)% upper confidence limits of Cpmh , Cpuh , and Cplh to gauge the process capabilities of products with multiple quality characteristics and reduce the influence of sampling errors. Second, we apply Pareto optimality to reexamine eligible CMs under the objectives of maximizing performance in individual quality characteristics and minimizing variance in overall performance. A feedback mechanism is included to advise ineligible CMs on means of improvement. Finally, we present a case study to demonstrate the feasibility and effectiveness of the proposed methodology.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100765X",
    "keywords": [
      "Accounting",
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Engineering",
      "Epistemology",
      "Geometry",
      "Industrial engineering",
      "Manufacturing",
      "Manufacturing engineering",
      "Marketing",
      "Mathematics",
      "Operating system",
      "Operations management",
      "Outsourcing",
      "Pareto chart",
      "Pareto principle",
      "Philosophy",
      "Process (computing)",
      "Product (mathematics)",
      "Quality (philosophy)",
      "Reliability engineering",
      "Risk analysis (engineering)",
      "Selection (genetic algorithm)",
      "Variance (accounting)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Chun-Ming"
      },
      {
        "surname": "Chen",
        "given_name": "Kuen-Suan"
      }
    ]
  },
  {
    "title": "Fuzzy Inference System Tree with Particle Swarm Optimization and Genetic Algorithm: A novel approach for PM10 forecasting",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115376",
    "abstract": "World health organization's estimates reveal that air pollution kills almost 6.5 million people in the world every year. As human beings, on average, spend 80–90% of their routine time in closed spaces, indoor air pollution has been a prime concern for their health and well-being. Primary and secondary pollutants are present in indoor environments, and they leave a considerable impact on human health. However, PM10 has attracted special scientific and legislative attention due to its close association with chronic health problems such as respiratory illness, lung cancer, and asthma attacks. Therefore, it is critical to develop a reliable model to analyze PM10 pollutants in the indoor environment so that building occupants can take relevant preventive measures. This paper focuses on the monitoring and predicting PM10 pollutant concentration in the indoor environments using the Fuzzy Inference System Tree (FIST) model. The forecasting model was trained using five different input parameters (PM2.5, CO2, VOC, temperature, and humidity) while considering PM10 as the target variable. The system performance was measured in terms of four performance indicators where MSE = 1.8126; MAE = 1.1821; MAPE = 4.4372%; RMSE = 1.3463 using normalized data. The model performance was further improved using Particle Swarm Optimization (PSO) and Genetic Algorithm (GA). Results show that the proposed aggregated FIS optimized with GA (RMSE = 1.2101) outperformed the PSO (RMSE = 1.2202) based model in terms of performance indicators. The proposed model can be installed in real-time environments to forecast PM10 concentration for improved public health and well-being.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008022",
    "keywords": [
      "Adaptive neuro fuzzy inference system",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Fuzzy control system",
      "Fuzzy logic",
      "Genetic algorithm",
      "Machine learning",
      "Mathematics",
      "Mean squared error",
      "Particle swarm optimization",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Saini",
        "given_name": "Jagriti"
      },
      {
        "surname": "Dutta",
        "given_name": "Maitreyee"
      },
      {
        "surname": "Marques",
        "given_name": "Gonçalo"
      }
    ]
  },
  {
    "title": "An interval valued neutrosophic decision-making structure for sustainable supplier selection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115354",
    "abstract": "Evolution of supply chain management (SCM) in recent years has transformed it beyond the simple logic of benefit and economic point of views. One of such key strategic elements in establishing a sustainable and socially responsive SCM is supplier selection and performance assessment. This study brings forward a sustainable supplier evaluation structure under multiple criteria and interval valued fuzzy neutrosophic (IVFN) model. The proposed structure uses CRiteria Importance Through Inter -criteriaCorrelation (CRITIC) and combined compromised solution (CoCoSo) under IVFN environment for evaluation and selection of suppliers for a dairy company in Iran. Alternative supplier 5 (S5) emerges as the best supplier with the highest overall score (1.168). Average Spearman rank correlation coefficient between the proposed model and other well established decision-making models methods is found as 0.9651 which establishes reliability of model outcomes. The estimated zero value of Gini Index indicates that supplier 5 has a constant ranking in all considered methods. This is a practical sustainable supplier selection platform which allows decision makers in procurement and SC industries to select the most suitable supplier in any pre-determined period.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100782X",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Combinatorics",
      "Computer science",
      "Economic inequality",
      "Fuzzy logic",
      "Gini coefficient",
      "Inequality",
      "Interval (graph theory)",
      "Machine learning",
      "Marketing",
      "Mathematical analysis",
      "Mathematics",
      "Operations research",
      "Procurement",
      "Rank (graph theory)",
      "Rank correlation",
      "Ranking (information retrieval)",
      "Selection (genetic algorithm)",
      "Spearman's rank correlation coefficient",
      "Supplier evaluation",
      "Supplier relationship management",
      "Supply chain",
      "Supply chain management"
    ],
    "authors": [
      {
        "surname": "Yazdani",
        "given_name": "Morteza"
      },
      {
        "surname": "Ebadi Torkayesh",
        "given_name": "Ali"
      },
      {
        "surname": "Stević",
        "given_name": "Željko"
      },
      {
        "surname": "Chatterjee",
        "given_name": "Prasenjit"
      },
      {
        "surname": "Asgharieh Ahari",
        "given_name": "Sahand"
      },
      {
        "surname": "Doval Hernandez",
        "given_name": "Violeta"
      }
    ]
  },
  {
    "title": "A cultural heritage framework using a Deep Learning based Chatbot for supporting tourist journey",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115277",
    "abstract": "The technology improvement has radically changed how tourists perform their journey offering new services to enhance their cultural experience and to easily retrieve required information. In this paper we propose a novel framework, that models intangible and tangible cultural objects into a unified data model, for supporting tourists journey. In particular, a Micro-service architecture has been designed to provide several services whose tourists can access through a conversational agent based on the Seq2Seq model. Furthermore, we propose also an Enterprise Service Bus to ingest events automatically from promotional website or manually from public and/or private organization. We have evaluated the proposed framework according to the following two experiments: (i) the efficiency and efficacy of the Chatbot engine, showing that the use of GRU cells allows to obtain better results in terms of loss and accuracy with respect to LSTM one and (ii) the efficacy of the proposed framework according to the NASA-TLX asking to 50 users to interact with the Chatbot, demonstrating that the proposed approach outperforms the state-of-the art ones combining natural processing language, user profile information and historical activities.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007089",
    "keywords": [
      "Archaeology",
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Chatbot",
      "Computer science",
      "Cultural heritage",
      "Data science",
      "Deep learning",
      "Economics",
      "Economy",
      "History",
      "Human–computer interaction",
      "Law",
      "Political science",
      "Service (business)",
      "Tourism",
      "Visual arts",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Sperlí",
        "given_name": "Giancarlo"
      }
    ]
  },
  {
    "title": "Forecasting cryptocurrency price using convolutional neural networks with weighted and attentive memory channels",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115378",
    "abstract": "After the invention of Bitcoin as well as other blockchain-based peer-to-peer payment systems, the cryptocurrency market has rapidly gained popularity. Consequently, the volatility of the various cryptocurrency prices attracts substantial attention from both investors and researchers. It is a challenging task to forecast the prices of cryptocurrencies due to the non-stationary prices and the stochastic effects in the market. Current cryptocurrency price forecasting models mainly focus on analyzing exogenous factors, such as macro-financial indicators, blockchain information, and social media data – with the aim of improving the prediction accuracy. However, the intrinsic systemic noise, caused by market and political conditions, is complex to interpret. Inspired by the strong correlations among cryptocurrencies and the powerful modelling capability displayed by deep learning techniques, we propose a Weighted & Attentive Memory Channels model to predict the daily close price and the fluctuation of cryptocurrencies. In particular, our proposed model consists of three modules: an Attentive Memory module combines a Gated Recurrent Unit with a self-attention component to establish attentive memory for each input sequence; a Channel-wise Weighting module receives the price of several heavyweight cryptocurrencies and learns their interdependencies by recalibrating the weights for each sequence; and a Convolution & Pooling module extracts local temporal features, thereby improving the generalization ability of the overall model. In order to validate the proposed model, we conduct a battery of experiments. The results show that our proposed scheme achieves state-of-the-art performance and outperforms the baseline models in prediction error, accuracy, and profitability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008046",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer security",
      "Cryptocurrency",
      "Deep learning",
      "Econometrics",
      "Economics",
      "Finance",
      "Futures contract",
      "Machine learning",
      "Overfitting",
      "Pooling"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zhuorui"
      },
      {
        "surname": "Dai",
        "given_name": "Hong-Ning"
      },
      {
        "surname": "Zhou",
        "given_name": "Junhao"
      },
      {
        "surname": "Mondal",
        "given_name": "Subrota Kumar"
      },
      {
        "surname": "García",
        "given_name": "Miguel Martínez"
      },
      {
        "surname": "Wang",
        "given_name": "Hao"
      }
    ]
  },
  {
    "title": "CAE-CNN: Predicting transcription factor binding site with convolutional autoencoder and convolutional neural network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115404",
    "abstract": "Transcription factor binding site (TFBS) is a DNA sequence that binds to transcription factor and regulates the transcription process of the gene. Although deep learning algorithms are superior to traditional methods in predicting transcription factor binding site, they often rely too much on negative sample data, which cannot be verified by experiment. In particular, a training model with such negative samples can generate a lot of noisy data and affect the classification performance. In order to cope with the aforementioned drawbacks, we propose a new architecture by combining a convolutional autoencoder with convolutional neural network, which is called CAE-CNN (Convolutional AutoEncoder and Convolutional Neural Network). Specifically, motivated by the image reconstruction, we use a convolutional autoencoder to extract useful features from the positive samples in DNA nucleotides. Consequently, the learned features will be used by the convolutional neural network in the phase of training. Furthermore, we employ a highway connection layer to better capture the features of DNA nucleotides through a gated unit. Extensive experiments based on human and mouse TFBS datasets evaluate the effectiveness of the proposed method for the motif discovery task, outperforming the state-of-the-art methods in accuracy, precision, recall, and AUC value. To the best of our knowledge, the original contribution of this work lies in integrating unsupervised and supervised learning methods to study the TFBS, thereby being able to build a more robust and generative TFBS prediction model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421008265",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Autoencoder",
      "Biology",
      "Computer science",
      "Convolutional code",
      "Convolutional neural network",
      "DNA binding site",
      "Decoding methods",
      "Deep learning",
      "Gene",
      "Gene expression",
      "Genetics",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Promoter",
      "Transcription (linguistics)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yongqing"
      },
      {
        "surname": "Qiao",
        "given_name": "Shaojie"
      },
      {
        "surname": "Zeng",
        "given_name": "Yuanqi"
      },
      {
        "surname": "Gao",
        "given_name": "Dongrui"
      },
      {
        "surname": "Han",
        "given_name": "Nan"
      },
      {
        "surname": "Zhou",
        "given_name": "Jiliu"
      }
    ]
  },
  {
    "title": "Evaluation of software development projects based on integrated Pythagorean fuzzy methodology",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115355",
    "abstract": "Implementing ERP (Enterprise Resource Planning) system successfully comes with a set of costly and complex challenges. The purpose of this paper is to offer an effective cloud-based ERP software evaluation framework to gain a better understanding of these complexities and address such challenges by identifying the selection criteria. The PFS (Pythagorean Fuzzy Set) is the new extension of Intuitionistic fuzzy sets, the Simple Additive Weighting (SAW) is an easy-to-use technique to determine the weights of the criteria, and the VIKOR technique is a compromise ranking methodology. In the recommended novel integrated MCDM (Multi-Criteria Decision-Making) methodology, Decision-Makers (DMs) must choose the best one among a set of candidate alternatives that satisfies the evaluation criteria. The GDM (Group Decision Making) is preferred here to deal with hesitancy in the evaluation of alternatives under group preferences. The sensitivity analysis resulted from various scenarios reveals that the proposed method is quite sensitive to criteria weights. The service and support, cost, and functionality are found as the essential evaluation criteria for the Turkish firm. These findings emphasize the significance of a qualified group of DMs for the evaluation process. The case study findings demonstrate that the developed approach is practical and validated for solving software development projects. This study presents a novel integrated methodology based on the PFS, SAW, and VIKOR to measure the performance of software development projects, to deal with the ambiguity and imprecision caused by the subjective judgments of DMs. The research contributes to the literature by delivering a combined decision methodology for the first time and by applying it to evaluate the software development projects.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007831",
    "keywords": [
      "Ambiguity",
      "Analytic hierarchy process",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Engineering",
      "Fuzzy logic",
      "Machine learning",
      "Management science",
      "Mathematics",
      "Medicine",
      "Multiple-criteria decision analysis",
      "Operations research",
      "Programming language",
      "Radiology",
      "Ranking (information retrieval)",
      "Set (abstract data type)",
      "Software",
      "VIKOR method",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Büyüközkan",
        "given_name": "Gülçin"
      },
      {
        "surname": "Göçer",
        "given_name": "Fethullah"
      }
    ]
  },
  {
    "title": "DSANet: Dilated spatial attention for real-time semantic segmentation in urban street scenes",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115090",
    "abstract": "Efficient and accurate semantic segmentation is particularly important in scene understanding for autonomous driving. Although Deep Convolutional Neural Networks(DCNNs) approaches have made a significant improvement for semantic segmentation. However, state-of-the-art models such as Deeplab and PSPNet have complex architectures and high computation complexity. Thus, it is inefficient for realtime applications. On the other hand, many works compromise the performance to obtain real-time inference speed which is critical for developing a light network model with high segmentation accuracy. In this paper, we present a computationally efficient network named DSANet, which follows a two-branch strategy to tackle the problem of real-time semantic segmentation in urban scenes. We first design a Semantic Encoding Branch, which employs channel split and shuffle to reduce the computation and maintain higher segmentation accuracy. Also, we propose a dual attention module consisting of dilated spatial attention and channel attention to make full use of the multi-level feature maps simultaneously, which helps predict the pixel-wise labels in each stage. Meanwhile, Spatial Encoding Network is used to enhance semantic information and preserve the spatial details. To better combine context information and spatial information, we introduce a Simple Feature Fusion Module. We evaluated our model with state-of-the-art semantic image semantic segmentation methods using two challenging datasets. The proposed method achieves an accuracy of 69.9% mean IoU and 71.3% mean IoU at speed of 75.3 fps and 34.08 fps on CamVid and Cityscapes test datasets respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005315",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Channel (broadcasting)",
      "Computation",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Convolutional neural network",
      "Encoding (memory)",
      "Feature (linguistics)",
      "Image segmentation",
      "Inference",
      "Linguistics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Elhassan",
        "given_name": "Mohammed A.M."
      },
      {
        "surname": "Huang",
        "given_name": "Chenxi"
      },
      {
        "surname": "Yang",
        "given_name": "Chenhui"
      },
      {
        "surname": "Munea",
        "given_name": "Tewodros Legesse"
      }
    ]
  },
  {
    "title": "A web-based decision support system for examination timetabling",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115363",
    "abstract": "We study exam timetabling problem (ETP) and supervisor assignment problem (SAP) of a vocational school offering associate level degrees in a university. The school has seven departments and plans almost 170 exams in each semester. We propose mixed integer programming (MIP) models for the ETP and the SAP. The optimal solution for ETP of the school is not attainable in two days with a commercial solver. We propose a decomposition method which is able to solve ETP using an open source solver in less than two minutes. The MIP models and the solution method are embedded into a web based decision support system (DSS). Using this DSS, a complete timetable can be prepared in less than two minutes by an average end-user.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007910",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Decision support system",
      "Decomposition",
      "Ecology",
      "Integer (computer science)",
      "Integer programming",
      "Law",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Operations research",
      "Pedagogy",
      "Political science",
      "Programming language",
      "Psychology",
      "Solver",
      "Supervisor",
      "Vocational education",
      "Web application",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Güler",
        "given_name": "Mehmet Güray"
      },
      {
        "surname": "Geçici",
        "given_name": "Ebru"
      },
      {
        "surname": "Köroğlu",
        "given_name": "Tuğçe"
      },
      {
        "surname": "Becit",
        "given_name": "Emre"
      }
    ]
  },
  {
    "title": "Deep neural networks for human’s fall-risk prediction using force-plate time series signal",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115220",
    "abstract": "Early and accurate identification of the balance deficits could reduce falls, in particular for older adults, a prone population. Our work investigates deep neural networks’ capacity to identify human balance patterns towards predicting fall-risk. Human balance ability can be characterized based on commonly-used balance metrics, such as those derived from the force-plate time series. We hypothesized that low, moderate, and high risk of falling can be characterized based on balance metrics, derived from the force-plate time series, in conjunction with deep learning algorithms. Further, we predicted that our proposed One-One-One Deep Neural Networks algorithm provides a considerable increase in performance compared to other algorithms. Here, an open source force-plate dataset, which quantified human balance from a wide demographic of human participants (163 females and males aged 18–86) for varied standing conditions (eyes-open firm surface, eyes-closed firm surface, eyes-open foam surface, eyes-closed foam surface) was used. Classification was based on one of the several indicators of fall-risk tied to the fear of falling: the clinically-used Falls Efficacy Scale (FES) assessment. For human fall-risk prediction, the deep learning architecture implemented comprised of: Recurrent Neural Network (RNN), Long-Short Time Memory (LSTM), One Dimensional Convolutional Neural Network (1D-CNN), and a proposed One-One-One Deep Neural Network. Results showed that our One-One-One Deep Neural Networks algorithm outperformed the other aforementioned algorithms and state-of-the-art models on the same dataset. With an accuracy, precision, and sensitivity of 99.9%, 100%, 100%, respectively at the 12th epoch, we found that our proposed One-One-One Deep Neural Network model is the most efficient neural network in predicting human’s fall-risk (based on the FES measure) using the force-plate time series signal. This is a novel methodology for an accurate prediction of human risk of fall.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006539",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Data mining",
      "Deep neural networks",
      "Machine learning",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Programming language",
      "SIGNAL (programming language)",
      "Series (stratigraphy)",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Savadkoohi",
        "given_name": "M."
      },
      {
        "surname": "Oladunni",
        "given_name": "T."
      },
      {
        "surname": "Thompson",
        "given_name": "L.A."
      }
    ]
  },
  {
    "title": "People re-identification using depth and intensity information from an overhead camera",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115287",
    "abstract": "This work presents a new people re-identification method, using depth and intensity images, both of them captured with a single static camera, located in an overhead position. The proposed solution arises from the need that exists in many areas of application to carry out identification and re-identification processes to determine, for example, the time that people remain in a certain space, while fulfilling the requirement of preserving people’s privacy. This work is a novelty compared to other previous solutions, since the use of top-view images of depth and intensity allows obtaining information to perform the functions of identification and re-identification of people, maintaining their privacy and reducing occlusions. In the procedure of people identification and re-identification, only three frames of intensity and depth are used, so that the first one is obtained when the person enters the scene (frontal view), the second when it is in the central area of the scene (overhead view) and the third one when it leaves the scene (back view). In the implemented method only information from the head and shoulders of people with these three different perspectives is used. From these views three feature vectors are obtained in a simple way, two of them related to depth information and the other one related to intensity data. This increases the robustness of the method against lighting changes. The proposal has been evaluated in two different datasets and compared to other state-of-the-art proposal. The obtained results show a 96,7% success rate in re-identification, with sensors that use different operating principles, all of them obtaining depth and intensity information. Furthermore, the implemented method can work in real time on a PC, without using a GPU.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007181",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Botany",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Gene",
      "Identification (biology)",
      "Operating system",
      "Overhead (engineering)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Luna",
        "given_name": "Carlos A."
      },
      {
        "surname": "Losada-Gutiérrez",
        "given_name": "Cristina"
      },
      {
        "surname": "Fuentes-Jimenez",
        "given_name": "David"
      },
      {
        "surname": "Mazo",
        "given_name": "Manuel"
      }
    ]
  },
  {
    "title": "Comparative study of hyperspectral image classification by multidimensional Convolutional Neural Network approaches to improve accuracy",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115280",
    "abstract": "This study presents multidimensional deep learning approaches on hyperspectral images. Storing, processing and interpreting hyperspectral data is very difficult due to its complexity and processing load. Consequently, conventional classifiers are not feasible to extract distinctive features. In order to present efficient classifiers, we utilize deep learning and present Convolutional Neural Network (CNN) approaches. In this study, we evaluate one-dimensional, two-dimensional and three-dimensional convolution model approaches that can present efficient classification performance. Within the scope of the study, samples of widely used hyperspectral data sets are classified by using one-dimensional, two-dimensional and three-dimensional convolutional neural networks by extracting spatial, spectral and spatial-spectral features. All the features provided by hyperspectral sensors are included in the classification by using both separate and joint spectral and spatial features. As a result, our studies have shown that our three-dimensional Convolutional Neural Networks have achieved higher classification rates compared to the state of art models. The accuracy performance of our three dimensional convolutional neural network is able to converge to 100 % during classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007119",
    "keywords": [
      "Architectural engineering",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Deep learning",
      "Engineering",
      "Hyperspectral imaging",
      "Joint (building)",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Ortac",
        "given_name": "Gizem"
      },
      {
        "surname": "Ozcan",
        "given_name": "Giyasettin"
      }
    ]
  },
  {
    "title": "A robust home health care routing-scheduling problem with temporal dependencies under uncertainty",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115209",
    "abstract": "Inspiring by real-world assumptions, this paper aims to present a robust optimization model for a home health care routing-scheduling problem with uncertain service and travel times. The constraints include temporal precedence and synchronization constraints, timing constraints for the transferring biological samples, and constraints associated with the multiple deployments of caregivers to provide the possibility of visiting one patient multiple times in the one-day planning horizon. Also, the existing uncertainty in service and travel times arising from an increase in customer-oriented service strategies is another crucial issue that should be appropriately addressed. Due to the complexity of this problem, three meta-heuristic algorithms (i.e., simulated annealing, genetic algorithm, and memetic algorithm) are proposed to solve the developed model. A series of experiments are implemented and shown that the memetic algorithm outperforms other proposed algorithms for large-sized problems. Furthermore, a detailed analysis of the results achieved by solving deterministic and robust models demonstrates the advantage of using the robust model. Finally, the dynamic version of this problem is proposed, and two dispatching policies are used to address this problem.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006424",
    "keywords": [
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Genetic algorithm",
      "Heuristic",
      "Job shop scheduling",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Memetic algorithm",
      "Operations research",
      "Routing (electronic design automation)",
      "Scheduling (production processes)",
      "Simulated annealing",
      "Time horizon",
      "Vehicle routing problem"
    ],
    "authors": [
      {
        "surname": "Shahnejat-Bushehri",
        "given_name": "Sina"
      },
      {
        "surname": "Tavakkoli-Moghaddam",
        "given_name": "Reza"
      },
      {
        "surname": "Boronoos",
        "given_name": "Mehdi"
      },
      {
        "surname": "Ghasemkhani",
        "given_name": "Ahmad"
      }
    ]
  },
  {
    "title": "Emergence-based self-advising in strong self-organizing systems: A case study in NASA ANTS mission",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115187",
    "abstract": "Self-organizing and self-adaptation are known as two necessary means for reducing costs and efforts required for the maintenance of complex software systems. In strong self-organizing systems, decision-making processes are distributed internally among system elements without any centralized control point (internal or external). In these systems, local communications of agents at the micro-level cause the emergent of macro-level behaviors, which can be seen from a global point of view. These global behaviors contain essential information that usually is ignored and is out of reach of agents. Via increasing context-awareness of agents, this information can be used by agents to improve the performance of the system. In this paper, inspired by the concept of consulting, a relation between the macro-level and micro-level is made. In summary, using information hidden in emergent behaviors, it is made an indirect and advice-based (non-compulsory) effect on agents at the micro-level. Therefore, first, the proposed self-advising property (as a property of the self-adaptive hierarchy) is defined. Then using the MAPE-K loop and stigmergic communication, the advising process is described. Besides, SASO-System is provided to describe the self-advising property and the advising process. This system is implemented on a case study of the NASA-ANTS mission. The comparison of results with other paper shows that using self-advising property reduces the time spent for self-protecting of leader agents in the scenario considered. Also, experimental results indicate that when the system is of self-advising property, the number of calls for the self-protecting function is reduced, and the decrease percentage of self-adaptive function calls is at the range of 1.86–13.56. Results also show that in 80% of radio ranges, the context-awareness of the self-advising system is better than or equal to the context-awareness of the non-self-advising system.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006229",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Biology",
      "Business",
      "Computer science",
      "Computer security",
      "Context (archaeology)",
      "Economics",
      "Engineering",
      "Epistemology",
      "Geometry",
      "Hierarchy",
      "Knowledge management",
      "Macro",
      "Market economy",
      "Mathematics",
      "Multi-agent system",
      "Neuroscience",
      "Operating system",
      "Operations research",
      "Paleontology",
      "Philosophy",
      "Point (geometry)",
      "Process (computing)",
      "Process management",
      "Programming language",
      "Property (philosophy)",
      "Psychology",
      "Self-organization"
    ],
    "authors": [
      {
        "surname": "Kalantari",
        "given_name": "Somayeh"
      },
      {
        "surname": "Nazemi",
        "given_name": "Eslam"
      },
      {
        "surname": "Masoumi",
        "given_name": "Behrooz"
      }
    ]
  },
  {
    "title": "A multi-objective multi-micro-swarm leadership hierarchy-based optimizer for uncertain flexible job shop scheduling problem with job precedence constraints",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115214",
    "abstract": "In realistic production scheduling, the processing time of operations and the due time of orders always fail to be precisely estimated as deterministic values due to fluctuating manufacturing environments and modest delay tolerance. When fabricating complex products that are assembled by multilevel parts, tree-structure dependencies between parts lead to hierarchical precedence constraints between corresponding jobs. Consequently, this paper studies an uncertain flexible job shop scheduling problem with job precedence constraints (U-FJSP-JPC). Uncertain processing time and due time are represented as interval grey number and trimmed triangular fuzzy number respectively. A tardiness index indicator is devised to assess the delay extent of grey completion time relative to fuzzy due time. To solve U-FJSP-JPC with minimizing three objectives simultaneously involving interval grey makespan, interval grey total machine workload and average tardiness index, this paper elaborately designs a novel multi-objective multiple-micro-swarm leadership hierarchy-based optimization algorithm (MOM2SLHO). This algorithm adopts a two-vector encoding scheme based on job and operation sequencing and a grey active decoding scheme based on heuristic machine assignment. In MOM2SLHO, the entire search agents are divided into multiple micro-swarms in which each one conducts an independent search based on leadership hierarchy and communicates with others by specific strategies. MOM2SLHO embodies an enhanced external grid archive to store and retrieve non-dominated Pareto optimal solutions. Extensive experiments and statistical analyses demonstrate that the proposed MOM2SLHO algorithm outperforms other well-known and state-of-art algorithms significantly for solving the studied U-FJSP-JPC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006473",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Fuzzy logic",
      "Interval (graph theory)",
      "Job scheduler",
      "Job shop scheduling",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Programming language",
      "Queue",
      "Schedule",
      "Scheduling (production processes)",
      "Tardiness"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Zhenwei"
      },
      {
        "surname": "Zhou",
        "given_name": "Xionghui"
      }
    ]
  },
  {
    "title": "Meta-heuristic algorithms to truss optimization: Literature mapping and application",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115197",
    "abstract": "Truss optimization is an important class of engineering problems, with better solutions allowing for reduced material use and reduced construction costs. Due to its complexity, meta-heuristic algorithms are often applied to this problem, since these algorithms require no prior knowledge of the search space, being built upon stochastic rules. The present work has two main objectives: the elaboration of a systematic literature mapping on meta-heuristic algorithms applied to truss optimization, and the application of EB-A-SHADE, a self-adaptive algorithm based on Differential Evolution, to truss size optimization with continuous variables. The systematic mapping was made based on 179 articles published between 2010 and 2020 and analyzed the categories of truss optimization, the algorithms used, and parameter self-adaptation techniques. This mapping provided a snapshot of the current state of truss optimization research, showing that it is an active field, with an increasing number of publications each year, but that also includes gaps for potential future works. For the truss size optimization application, seven benchmark structures were used to evaluate the EB-A-SHADE algorithm, and the results obtained were compared to those of state-of-the-art algorithms with the use of statistical analysis. Convergence, constraints, and diversity plots were also employed to analyze the algorithm behavior on the problems studied, showing a stable convergence. The experiments showed that EB-A-SHADE achieved competitive results, having the statistical best results in most structures, and with the lowest standard deviation values. These results show that the algorithm can be applied to more complex approaches to the problem at hand.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100631X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Engineering",
      "Heuristic",
      "Mathematical optimization",
      "Mathematics",
      "Meta heuristic",
      "Metaheuristic",
      "Optimization algorithm",
      "Structural engineering",
      "Truss"
    ],
    "authors": [
      {
        "surname": "Renkavieski",
        "given_name": "Christopher"
      },
      {
        "surname": "Parpinelli",
        "given_name": "Rafael Stubs"
      }
    ]
  },
  {
    "title": "Scene image representation by foreground, background and hybrid features",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115285",
    "abstract": "Previous methods for representing scene images based on deep learning primarily consider either the foreground or background information as the discriminating clues for the classification task. However, scene images also require additional information (hybrid) to cope with the inter-class similarity and intra-class variation problems. In this paper, we propose to use hybrid features in addition to foreground and background features to represent scene images. We suppose that these three types of information could jointly help to represent scene images more accurately. To this end, we adopt three VGG-16 architectures pre-trained on ImageNet, Places, and Hybrid (both ImageNet and Places) datasets for the corresponding extraction of foreground, background and hybrid information. All these three types of deep features are further aggregated to achieve our final features for the representation of scene images. Extensive experiments on two large benchmark scene datasets (MIT-67 and SUN-397) show that our method produces the state-of-the-art classification performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007168",
    "keywords": [
      "Artificial intelligence",
      "Background subtraction",
      "Computer science",
      "Computer vision",
      "Foreground detection",
      "Image (mathematics)",
      "Law",
      "Pattern recognition (psychology)",
      "Pixel",
      "Political science",
      "Politics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Sitaula",
        "given_name": "Chiranjibi"
      },
      {
        "surname": "Xiang",
        "given_name": "Yong"
      },
      {
        "surname": "Aryal",
        "given_name": "Sunil"
      },
      {
        "surname": "Lu",
        "given_name": "Xuequan"
      }
    ]
  },
  {
    "title": "A similarity measurement for time series and its application to the stock market",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115217",
    "abstract": "The stock market is a very important financial market, and the prediction of the stock has always been of great interest to many investors. Nowadays, many methods for predicting stocks have been developed and one of the most commonly adopted strategies is to seek similar stocks through historical data to make predictions. The key to this strategy is the construction of a reasonable similarity measurement. In this paper, for accurately describing the similarity between a pair of time series, a novel similarity measurement is proposed, which is named as the dynamic multi-perspective personalized similarity measurement (DMPSM). Specifically, the segmented stock series are weighted according to the principle that the closer to current data, the more weight will be given. Then, Canberra distance is embedded into the dynamic time warping (DTW) to measure the similarity between any pair of time series. By this way, the DMPSM can not only reflect the personalization of stock time series, but also eliminate the impact of singularities and apply to one-to-many matching. To validate the efficiency of DMPSM, experiments utilized 285 stocks from the Shanghai Stock Exchange and the results demonstrated the superiority of the proposed approach over similarity measurements, including Euclidean distance, Canberra distance and DTW.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006503",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Data mining",
      "Dynamic time warping",
      "Econometrics",
      "Economics",
      "Engineering",
      "Euclidean distance",
      "Finance",
      "Horse",
      "Image (mathematics)",
      "Machine learning",
      "Mathematics",
      "Mechanical engineering",
      "Paleontology",
      "Series (stratigraphy)",
      "Similarity (geometry)",
      "Similarity measure",
      "Stock (firearms)",
      "Stock exchange",
      "Stock market",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Feng"
      },
      {
        "surname": "Gao",
        "given_name": "Yating"
      },
      {
        "surname": "Li",
        "given_name": "Xinning"
      },
      {
        "surname": "An",
        "given_name": "Zhiyong"
      },
      {
        "surname": "Ge",
        "given_name": "Shiyu"
      },
      {
        "surname": "Zhang",
        "given_name": "Caiming"
      }
    ]
  },
  {
    "title": "Nuclear quadrupole resonance response detection using deep neural networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115227",
    "abstract": "Nuclear quadrupole resonance is a solid-state spectroscopy technique used for analyzing substances with quadrupolar nuclei. The technique is highly specific and used in multiple security applications, such as landmine detection and luggage or personnel screening for prohibited substances. However, the performance of existing detection systems needs improvement, especially taking into account the high risk associated with explosives, drugs and narcotics. This paper proposes the application of deep learning techniques to improve the detection’s performance. A new feature set is proposed for signal classification and multiple high-performance solutions are selected, adapted and implemented for quadrupole resonance detection. A comparative analysis is performed on a real data set composed of 20,000 acquisitions and AlexNet is shown to outperform the other algorithms. It ensures a detection accuracy of 99.9%, which is remarkable from an operational standpoint and surpasses the performance of existing signal processing and analysis solutions. The solution is deployed on a commercial computer integrated into the spectrometer and has an analysis time of approx. 1 s, which does not significantly affect the overall scan time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100659X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer hardware",
      "Computer science",
      "Digital signal processing",
      "Feature (linguistics)",
      "Linguistics",
      "Nuclear magnetic resonance",
      "Nuclear quadrupole resonance",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Programming language",
      "SIGNAL (programming language)",
      "Set (abstract data type)",
      "Signal processing"
    ],
    "authors": [
      {
        "surname": "Monea",
        "given_name": "Cristian"
      }
    ]
  },
  {
    "title": "Investigation of butterfly optimization and gases Brownian motion optimization algorithms for optimal multilevel image thresholding",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115286",
    "abstract": "Image processing is one of the major research regions in the most recent decades, in which, image segmentation has gained more popularity due to its enormous applications. Before segmenting an image, it has to be pre-processed using thresholding techniques. A properly thresholded image would give better segmentation results. Many researchers have contributed significant algorithms based on Otsu and Kapur based methods and reported outstanding results. These methods are suitable for bi-level as well as multilevel thresholding cases, however, the process of selecting the optimal thresholds in the case of multilevel thresholding is difficult and time consuming. To avoid this problem, this paper investigates the ability of two different algorithms namely: BOA (Butterfly Optimization Algorithm) and GBMO (Gases Brownian Motion Optimization) to determine the optimal threshold levels for image segmentation. The BOA is inspired from natural behaviour of butterflies which have special style of information propagation based on intensity of fragrance; whereas, GBMO is based on law of motion of gases which have greater ability to trade-off between exploration and exploitation. The candidate solutions were created using image histogram based on selected algorithms, and then they were updated according to the characteristics of each algorithm. The solutions are estimated using Otsu’s between class variance as fitness function during the process of optimization. The performance of the proposed algorithms has been evaluated using various benchmark images and has been compared with seven different algorithms such as MFO (Moth Flame Optimization), WOA (Whale Optimization Algorithm), SSO (Social Spider Optimization), FASSO (Firefly Assisted Social Spider Optimization), FA (Firefly Algorithm), SCA (Sine Cosine Algorithm), and HS (Harmony Search). The outcomes have been analysed based on various metrics like fitness value, PSNR and SSIM, as well as time complexity and the Wilcoxon statistical test. The experimental results showed that the proposed methods outperformed the other algorithms. Among BOA and GBMO, BOA yields better results than GBMO for all images at all threshold levels.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100717X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Firefly algorithm",
      "Geodesy",
      "Geography",
      "Histogram",
      "Image (mathematics)",
      "Image segmentation",
      "Optimization problem",
      "Otsu's method",
      "Particle swarm optimization",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Thresholding"
    ],
    "authors": [
      {
        "surname": "Sowjanya",
        "given_name": "Kotte"
      },
      {
        "surname": "Injeti",
        "given_name": "Satish Kumar"
      }
    ]
  },
  {
    "title": "An optimized Generative Adversarial Network based continuous sign language classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115276",
    "abstract": "Classifying manual and non-manual gestures in sign language recognition is a complex and challenging task. Sign language gestures are the combination of hand, face, and body postures, which often have self-occlusions and inter-object occlusions of both the hands, hands with face, or hands with upper body postures. This paper addresses the characterization of manual and non-manual gestures in recognizing the sign language gestures from continuous video sequences. This paper introduces a novel hyperparameter based optimized Generative Adversarial Networks (H-GANs) to classify the sign gestures, and it works in three phases. In phase-I, it adapts the stacked variational auto-encoders (SVAE) and Principal Component Analysis (PCA) to get the pre-tuned data with reduced feature dimensions. In Phase-II, the H-GANs employed Deep Long Short Term Memory (LSTM) as generator and LSTM with 3D Convolutional Neural Network (3D-CNN) as a discriminator. The generator generates random sequences with noise from the real sequence of frames, and the discriminator detects and classifies the real frames of sign gestures. In Phase-III, the proposed approach employs Deep Reinforcement Learning (DRL) for hyperparameter optimization and regularization. By getting the reward points, Proximal Policy Optimization (PPO) optimizes the hyperparameters, and Bayesian Optimization (BO) regularizes the hyperparameters. The proposed H-GANs used two different large vocabulary sign corpus of continuous sign videos to evaluate the performance and efficiency of the system. The experimental results on different dimensions reveal that the H-GANs improved the accuracy and recognition rate when compared with the state-of-the-art classification methods with reduced complexity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007077",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Detector",
      "Discriminator",
      "Generator (circuit theory)",
      "Gesture",
      "Gesture recognition",
      "Hidden Markov model",
      "Hyperparameter",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Sign language",
      "Speech recognition",
      "Telecommunications",
      "Vocabulary"
    ],
    "authors": [
      {
        "surname": "Elakkiya",
        "given_name": "R."
      },
      {
        "surname": "Vijayakumar",
        "given_name": "Pandi"
      },
      {
        "surname": "Kumar",
        "given_name": "Neeraj"
      }
    ]
  },
  {
    "title": "A decision support system for automating document retrieval and citation screening",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115261",
    "abstract": "The systematic literature review (SLR) process includes several steps to collect secondary data and analyze it to answer research questions. In this context, the document retrieval and primary study selection steps are heavily intertwined and known for their repetitiveness, high human workload, and difficulty identifying all relevant literature. This study aims to reduce human workload and error of the document retrieval and primary study selection processes using a decision support system (DSS). An open-source DSS is proposed that supports the document retrieval step, dataset preprocessing, and citation classification. The DSS is domain-independent, as it has proven to carefully select an article’s relevance based solely on the title and abstract. These features can be consistently retrieved from scientific database APIs. Additionally, the DSS is designed to run in the cloud without any required programming knowledge for reviewers. A Multi-Channel CNN architecture is implemented to support the citation screening process. With the provided DSS, reviewers can fill in their search strategy and manually label only a subset of the citations. The remaining unlabeled citations are automatically classified and sorted based on probability. It was shown that for four out of five review datasets, the DSS's use achieved significant workload savings of at least 10%. The cross-validation results show that the system provides consistent results up to 88.3% of work saved during citation screening. In two cases, our model yielded a better performance over the benchmark review datasets. As such, the proposed approach can assist the development of systematic literature reviews independent of the domain. The proposed DSS is effective and can substantially decrease the document retrieval and citation screening steps' workload and error rate.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100693X",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Citation",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Data science",
      "Decision support system",
      "Feature selection",
      "Geodesy",
      "Geography",
      "Information retrieval",
      "Law",
      "Machine learning",
      "Operating system",
      "Paleontology",
      "Political science",
      "Preprocessor",
      "Process (computing)",
      "Relevance (law)",
      "Selection (genetic algorithm)",
      "Workload",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "van Dinter",
        "given_name": "Raymon"
      },
      {
        "surname": "Catal",
        "given_name": "Cagatay"
      },
      {
        "surname": "Tekinerdogan",
        "given_name": "Bedir"
      }
    ]
  },
  {
    "title": "An efficient method to minimize cross-entropy for selecting multi-level threshold values using an improved human mental search algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115106",
    "abstract": "The minimum cross-entropy (MCIT) is introduced as a multi-level image thresholding approach, but it suffers from time complexity, in particular, when the number of thresholds is high. To address this issue, this paper proposes a novel MCIT-based image thresholding based on improved human mental search (HMS) algorithm, a recently proposed population-based metaheuristic algorithm to tackle complex optimisation problems. To further enhance the efficacy, we improve HMS algorithm, IHMSMLIT, with four improvements, including, adaptively selection of the number of mental searches instead of randomly selection, proposing one-step k-means clustering for region clustering, updating based on global and personal experiences, and proposing a random clustering strategy. To assess our proposed algorithm, we conduct an extensive set of experiments with several state-of-the-art and the most recent approaches on a benchmark set of images and in terms of several criteria including objective function, peak signal to noise ratio (PSNR), feature similarity index (FSIM), structural similarity index (SSIM), and stability analysis. The obtained results apparently demonstrate the competitive performance of our proposed algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005479",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Entropy (arrow of time)",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Thresholding"
    ],
    "authors": [
      {
        "surname": "Esmaeili",
        "given_name": "Leila"
      },
      {
        "surname": "Mousavirad",
        "given_name": "Seyed Jalaleddin"
      },
      {
        "surname": "Shahidinejad",
        "given_name": "Ali"
      }
    ]
  },
  {
    "title": "A path-based relation networks model for knowledge graph completion",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115273",
    "abstract": "We consider the problem of learning and inference in a large-scale knowledge graph containing incomplete knowledge. We show that a simple neural network module for relational reasoning through the path extracted from the knowledge base can be used to reliably infer new facts for the missing link. In our work, we used path ranking algorithm to extract the relation path from knowledge graph and use it to build train data. In order to learn the characteristics of relation, a detour path between nodes was created as training data using the extracted relation path. Using this, we trained a model that can predict whether a given triple (Head entity, relation, tail entity) is valid or not. Experiments show that our model obtains better link prediction, relation prediction and triple classification results than previous state-of-the-art models on benchmark datasets WN18RR, FB15k-237, WN11 and FB13.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007041",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Data mining",
      "Geodesy",
      "Geography",
      "Graph",
      "Inference",
      "Knowledge base",
      "Knowledge graph",
      "Machine learning",
      "Path (computing)",
      "Programming language",
      "Ranking (information retrieval)",
      "Relation (database)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Wan-Kon"
      },
      {
        "surname": "Shin",
        "given_name": "Won-Chul"
      },
      {
        "surname": "Jagvaral",
        "given_name": "Batselem"
      },
      {
        "surname": "Roh",
        "given_name": "Jae-Seung"
      },
      {
        "surname": "Kim",
        "given_name": "Min-Sung"
      },
      {
        "surname": "Lee",
        "given_name": "Min-Ho"
      },
      {
        "surname": "Park",
        "given_name": "Hyun-Kyu"
      },
      {
        "surname": "Park",
        "given_name": "Young-Tack"
      }
    ]
  },
  {
    "title": "A machine learning framework for predicting long-term graft survival after kidney transplantation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115235",
    "abstract": "Kidney transplantation (KT) is an optimal treatment for end-stage renal disease (ESRD). Currently, short-term KT outcomes are indeed excellent, but long-term successful outcomes are still difficult to achieve, and improving them is crucial for kidney recipients. An early and accurate prediction of long-term graft survival helps healthcare practitioners to create a more personalized treatment plans for patients and facilitates the performance of clinical trials. In this study, we propose a machine learning framework to early predict graft survival after five years of KT and determine the most influential parameters that affect the survival. Our dataset was collected from Charles Nicolle Hospital in Tunis in Tunisia and it included pre, peri, post KT aspects. We utilized four machine learning algorithms to select the most important features: the least absolute shrinkage and selection operator logistic regression (Lasso-LR), Random Forrest (RF), Decision Tree (DT), and Chi-square (Chi-sq). We utilized three Scikit-learn functions to implement those algorithms: SelectFromModel (SFM), Recursive Feature Elimination (RFE), and SelectKBest (SKB). Five algorithms were utilized to builds prediction models based on the data groups resulted from the feature selection step: logistic regression (LR), k-nearest neighbors (KNN), extreme gradient boosting (XGB), and artificial neural network (ANN). We evaluated the models using five performance measures: accuracy, sensitivity, specificity, F1 measure, and area under the curve (AUC). XGBoost resulted the best model with the highest AUC (89.7%). It was based ten features selected by RF algorithm and SFM function. The accuracy, sensitivity, specificity, and F1 of the best model were 91.5%, 91.9%, 87.5%, and 89.6%, respectively. This study proposes a novel approach for investigating long-term allograft survival while considering the complex relationship between all KT aspects and long-term outcomes. Our framework can be used as a decision support system for Nephrologists to early detect graft status, which helps in developing safer recommendations for kidney patients and consequently obtaining positive KT outcomes and mitigating the risks of graft failure.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006679",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Decision tree",
      "Feature selection",
      "Gradient boosting",
      "Lasso (programming language)",
      "Logistic regression",
      "Machine learning",
      "Mathematics",
      "Medicine",
      "Random forest",
      "Receiver operating characteristic",
      "Regression",
      "Statistics",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Badrouchi",
        "given_name": "Samarra"
      },
      {
        "surname": "Ahmed",
        "given_name": "Abdulaziz"
      },
      {
        "surname": "Mongi Bacha",
        "given_name": "Mohamed"
      },
      {
        "surname": "Abderrahim",
        "given_name": "Ezzedine"
      },
      {
        "surname": "Ben Abdallah",
        "given_name": "Taieb"
      }
    ]
  },
  {
    "title": "Condition-CNN: A hierarchical multi-label fashion image classification model",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115195",
    "abstract": "Current state of the art image classifiers predict a single class label of an image. However, in many industry settings such as online shopping, images belong to a class hierarchy where the first level represents the coarse grained or the most abstract class with subsequent levels representing the more specific classes. We propose a novel hierarchical image classification model, Condition-CNN, which addresses some of the shortcomings of the branching convolutional neural network in terms of training time and fine-grained accuracy. It applies the Teacher Forcing training algorithm, where the actual class labels of the higher level classes rather than the predicted labels are used to train the lower level branches. The technique also prevents error propagation, and thereby, reduces the training time. Besides learning the image features for each level of classes, Condition-CNN also learns the relationship between different levels of classes as conditional probabilities, which is used to estimate class predictions during scoring. By feeding the estimated higher-level class predictions as priors to the lower-level class prediction, Condition-CNN achieves a superior prediction accuracy while requiring fewer trainable parameters compared to the baseline CNN models. The validation results of Condition-CNN using the Kaggle Fashion Product Images data set demonstrate a prediction accuracy of 99.8%, 98.1%, and 91.0% for Level 1, 2 and 3 classes respectively, which are greater than that of B-CNN and other baseline CNN models. Moreover, Condition-CNN used only 77.58% of the total number of trainable parameters as that of B-CNN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006291",
    "keywords": [
      "Artificial intelligence",
      "Bayesian probability",
      "Class (philosophy)",
      "Class hierarchy",
      "Computer science",
      "Contextual image classification",
      "Convolutional neural network",
      "Forcing (mathematics)",
      "Image (mathematics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Object-oriented programming",
      "Pattern recognition (psychology)",
      "Prior probability",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Kolisnik",
        "given_name": "Brendan"
      },
      {
        "surname": "Hogan",
        "given_name": "Isaac"
      },
      {
        "surname": "Zulkernine",
        "given_name": "Farhana"
      }
    ]
  },
  {
    "title": "Pattern recognition in financial surveillance with the ARMA-GARCH time series model using support vector machine",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115334",
    "abstract": "As the intersection of finance and statistics, financial surveillance is a new interdisciplinary field of research. In this field, statistical process control methods are applied to monitor financial indices. The final aim is to detect out-of-control conditions and trigger a signal as soon as possible. These early signals can help practitioners in making on-time decisions. In this paper, a new method based on a support vector machine is proposed to detect upward and downward shifts with step and trend patterns in auto-correlated financial processes. These processes are modeled by the autoregressive moving average (ARMA) and generalized autoregressive conditional heteroskedasticity (GARCH) variance (ARMA-GARCH) time series model. Autocorrelation structure in data with changing volatility makes pattern recognition difficult. As such, some features are selected to extract different properties of the considered patterns. Moreover, a new feature named the maximum degree between the horizontal line and the consecutive observations line is used for the better distinction of the step and trend shift patterns. To evaluate the performance of the proposed method, we performed a comprehensive simulation study. Moreover, to illustrate the proposed method's application, it was applied to classify patterns in the OPEC crude oil basket return and the Tehran stock exchange index.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007636",
    "keywords": [
      "Artificial intelligence",
      "Autocorrelation",
      "Autoregressive conditional heteroskedasticity",
      "Autoregressive model",
      "Autoregressive–moving-average model",
      "Computer science",
      "Data mining",
      "Econometrics",
      "Economics",
      "Field (mathematics)",
      "Finance",
      "Heteroscedasticity",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Statistics",
      "Stock exchange",
      "Support vector machine",
      "Time series",
      "Volatility (finance)"
    ],
    "authors": [
      {
        "surname": "Doroudyan",
        "given_name": "Mohammad Hadi"
      },
      {
        "surname": "Niaki",
        "given_name": "Seyed Taghi Akhavan"
      }
    ]
  },
  {
    "title": "Deep graph convolutional reinforcement learning for financial portfolio management – DeepPocket",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115127",
    "abstract": "Portfolio management aims at maximizing the return on investment while minimizing risk by continuously reallocating the assets forming the portfolio. These assets are not independent but correlated during a short time period. A graph convolutional reinforcement learning framework called DeepPocket is proposed whose objective is to exploit the time-varying interrelations between financial instruments. These interrelations are represented by a graph whose nodes correspond to the financial instruments while the edges correspond to a pair-wise correlation function in between assets. DeepPocket consists of a restricted, stacked autoencoder for feature extraction, a convolutional network to collect underlying local information shared among financial instruments and an actor-critic reinforcement learning agent. The actor-critic structure contains two convolutional networks in which the actor learns and enforces an investment policy which is, in turn, evaluated by the critic in order to determine the best course of action by constantly reallocating the various portfolio assets to optimize the expected return on investment. The agent is initially trained offline with online stochastic batching on historical data. As new data become available, it is trained online with a passive concept drift approach to handle unexpected changes in their distributions. DeepPocket is evaluated against five real-life datasets over three distinct investment periods, including during the Covid-19 crisis, and clearly outperformed market indexes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005686",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Computer science",
      "Computer security",
      "Deep learning",
      "Economics",
      "Exploit",
      "Finance",
      "Financial market",
      "Graph",
      "Investment management",
      "Machine learning",
      "Management",
      "Market liquidity",
      "Portfolio",
      "Project management",
      "Project portfolio management",
      "Reinforcement learning",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Soleymani",
        "given_name": "Farzan"
      },
      {
        "surname": "Paquet",
        "given_name": "Eric"
      }
    ]
  },
  {
    "title": "Nested cross-validation when selecting classifiers is overzealous for most practical applications",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115222",
    "abstract": "When selecting a classification algorithm to be applied to a particular problem, one has to simultaneously select the best algorithm for that dataset and the best set of hyperparameters for the chosen model. The usual approach is to apply a nested cross-validation procedure: hyperparameter selection is performed in the inner cross-validation, while the outer cross-validation computes an unbiased estimate of the expected accuracy of the algorithm with cross-validation based hyperparameter tuning. The alternative approach, which we shall call “flat cross-validation”, uses a single cross-validation step both to select the optimal hyperparameter values and to provide an estimate of the expected accuracy of the algorithm that, while biased, may nevertheless still be used to select the best learning algorithm. We tested both procedures using 12 different algorithms on 115 real-life binary datasets and conclude that using the less computationally costly flat cross-validation procedure will generally result in the selection of an algorithm that is, for all practical purposes, of similar quality to that selected via nested cross-validation, provided the learning algorithms have relatively few hyperparameters to be optimised.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006540",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Binary classification",
      "Binary number",
      "Computer science",
      "Cross-validation",
      "Hyperparameter",
      "Hyperparameter optimization",
      "Machine learning",
      "Mathematics",
      "Model selection",
      "Programming language",
      "Selection (genetic algorithm)",
      "Set (abstract data type)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Wainer",
        "given_name": "Jacques"
      },
      {
        "surname": "Cawley",
        "given_name": "Gavin"
      }
    ]
  },
  {
    "title": "Enhancing store layout decision with agent-based simulations of consumers’ density",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115231",
    "abstract": "Customer concentration inside a store is of pivotal importance for retail management, acquiring controversial contributions about the best number of consumers in the floor space to ensure an enjoyable and pleasant experience. Indeed, the excessive concentration of people (crowd) might discourage from shopping in that location, while on the other hand, a certain traffic to the store generates profit for retailers. The aim of this paper is to support retailers’ informed decisions by refining our understanding of the extent to which store layouts influences consumer density. To this end, we conduct a large field study using a unique dataset covering customers in a real grocery store with agent-based simulations. Results clearly show the extent to which this kind of simulations help predicting the changes in store layout able to affect customer density in the areas, while ensuring the same number of individuals.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006631",
    "keywords": [
      "Business",
      "Computer science",
      "Economics",
      "Grocery store",
      "Marketing",
      "Microeconomics",
      "Operating system",
      "Profit (economics)",
      "Space (punctuation)"
    ],
    "authors": [
      {
        "surname": "Pantano",
        "given_name": "Eleonora"
      },
      {
        "surname": "Pizzi",
        "given_name": "Gabriele"
      },
      {
        "surname": "Bilotta",
        "given_name": "Eleonora"
      },
      {
        "surname": "Pantano",
        "given_name": "Pietro"
      }
    ]
  },
  {
    "title": "How to select and weight context dimensions conditions for context-aware recommendation?",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115176",
    "abstract": "Contextual information plays a key role in Context-Aware Recommender Systems (CARS). The rating prediction in CARS focuses on improving recommendation accuracy attempting to form a personalized information recommendation for users. Three key problems that affect the performances of recommender systems: (i) context condition’s selection; (ii) context condition’s weighting; and (iii) users’ context conditions matching. Context-aware approaches have the assumption that all context conditions have the same weight. These approaches ignore that users have different preferences in different contexts. To address these three problems, we introduce a novel approach for Selecting, Weighting Context Conditions (SWCC) and measuring semantic similarity between users’ situations. Evaluation experiments show that the proposed approach is outperforming the pioneering context-aware recommendation approaches of the literature.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100614X",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Paleontology"
    ],
    "authors": [
      {
        "surname": "Zammali",
        "given_name": "Saloua"
      },
      {
        "surname": "Ben Yahia",
        "given_name": "Sadok"
      }
    ]
  },
  {
    "title": "Meteorological and human mobility data on predicting COVID-19 cases by a novel hybrid decomposition method with anomaly detection analysis: A case study in the capitals of Brazil",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115190",
    "abstract": "In 2020, Brazil was the leading country in COVID-19 cases in Latin America, and capital cities were the most severely affected by the outbreak. Climates vary in Brazil due to the territorial extension of the country, its relief, geography, and other factors. Since the most common COVID-19 symptoms are related to the respiratory system, many researchers have studied the correlation between the number of COVID-19 cases with meteorological variables like temperature, humidity, rainfall, etc. Also, due to its high transmission rate, some researchers have analyzed the impact of human mobility on the dynamics of COVID-19 transmission. There is a dearth of literature that considers these two variables when predicting the spread of COVID-19 cases. In this paper, we analyzed the correlation between the number of COVID-19 cases and human mobility, and meteorological data in Brazilian capitals. We found that the correlation between such variables depends on the regions where the cities are located. We employed the variables with a significant correlation with COVID-19 cases to predict the number of COVID-19 infections in all Brazilian capitals and proposed a prediction method combining the Ensemble Empirical Mode Decomposition (EEMD) method with the Autoregressive Integrated Moving Average Exogenous inputs (ARIMAX) method, which we called EEMD-ARIMAX. After analyzing the results poor predictions were further investigated using a signal processing-based anomaly detection method. Computational tests showed that EEMD-ARIMAX achieved a forecast 26.73% better than ARIMAX. Moreover, an improvement of 30.69% in the average root mean squared error (RMSE) was noticed when applying the EEMD-ARIMAX method to the data normalized after the anomaly detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006254",
    "keywords": [
      "Anomaly (physics)",
      "Autoregressive integrated moving average",
      "Autoregressive model",
      "Computer science",
      "Condensed matter physics",
      "Coronavirus disease 2019 (COVID-19)",
      "Correlation",
      "Correlation coefficient",
      "Disease",
      "Econometrics",
      "Geography",
      "Geometry",
      "Hilbert–Huang transform",
      "Infectious disease (medical specialty)",
      "Mathematics",
      "Mean squared error",
      "Medicine",
      "Meteorology",
      "Pathology",
      "Pearson product-moment correlation coefficient",
      "Physics",
      "Statistics",
      "Telecommunications",
      "Time series",
      "Transmission (telecommunications)",
      "White noise"
    ],
    "authors": [
      {
        "surname": "da Silva",
        "given_name": "Tiago Tiburcio"
      },
      {
        "surname": "Francisquini",
        "given_name": "Rodrigo"
      },
      {
        "surname": "Nascimento",
        "given_name": "Mariá C.V."
      }
    ]
  },
  {
    "title": "Artificial intelligence in ophthalmopathy and ultra-wide field image: A survey",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115068",
    "abstract": "Fundus digital photography and optical coherence tomography (OCT) are currently the primary imaging approaches for early diagnosis and treatment of eye diseases. In recent years, the significant development in artificial intelligence (AI), particularly in machine learning (ML) and deep learning (DL) are new and vital technical-driven motivations impacting on the traditional diagnosis and treatment methods. At the same time, the ultra-wide field (UWF) imaging technology is getting widely accepted and prevalent by its obvious advantageous features of non-dilate pupils, express-track result and the vast pool of fundus viewing angles. As a result, numerous research have been done to explore AI in ultra-wide field fundus imaging ophthalmology for joint diagnosis and treatment. However, the current review of this method is still in least ink. We first outlines the application and impact of AI technology in ophthalmic diseases in the past ten years. With the following part exclusively summarizing the technical integration of ultra-wide field fundus images and AI technology in the past four years, which has brought innovations to clinical treatment methods for the diagnosis and treatment of ophthalmic diseases; finally, we analyzed the application and implementation of the novel technology as well as the potential limitations and challenges, to predict the possibility of the technology’s further principles role and values in clinical ophthalmology.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005091",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Digital image",
      "Digital imaging",
      "Field (mathematics)",
      "Fluorescein angiography",
      "Fundus (uterus)",
      "Fundus photography",
      "Image (mathematics)",
      "Image processing",
      "Imaging technology",
      "Mathematics",
      "Medical imaging",
      "Medical physics",
      "Medicine",
      "Ophthalmology",
      "Optical coherence tomography",
      "Optometry",
      "Pure mathematics",
      "Radiology",
      "Visual acuity"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Jie"
      },
      {
        "surname": "Fong",
        "given_name": "Simon"
      },
      {
        "surname": "Wang",
        "given_name": "Han"
      },
      {
        "surname": "Hu",
        "given_name": "Quanyi"
      },
      {
        "surname": "Lin",
        "given_name": "Chen"
      },
      {
        "surname": "Huang",
        "given_name": "Shigao"
      },
      {
        "surname": "Shi",
        "given_name": "Jian"
      },
      {
        "surname": "Lan",
        "given_name": "Kun"
      },
      {
        "surname": "Tang",
        "given_name": "Rui"
      },
      {
        "surname": "Wu",
        "given_name": "Yaoyang"
      },
      {
        "surname": "Zhao",
        "given_name": "Qi"
      }
    ]
  },
  {
    "title": "Can machine learning classification methods improve the prediction of leaf wetness in North-Western Europe compared to established empirical methods?",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115255",
    "abstract": "Leaf wetness is an important input parameter into disease prediction models. The use of machine learning algorithms for the classification of leaf wetness measurements from 30 meteorological stations in North Western Europe during the period of January 2014 to October 2018 was assessed in this study. The accuracy of the empirical models utilised within in this study was enhanced by increasing the relative humidity threshold from 90% to 92%. Increasing the relative humidity threshold led to an average increase in the classification accuracy of 1.12%. The use of machine learning classification algorithms consistently provided more accurate results for the prediction of leaf wetness when compared to the empirical models that were studied with an average increase in the classification accuracy of 4.85%. The sub-division of the data into regional subsets had a greater effect on the accuracy of the models than the temporal sub-division of the data. Machine learning classification techniques performed well compared with previously established empirical models for the prediction of leaf wetness. Further improvements in the algorithms are possible, making the techniques studied here a viable research tool.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006874",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Division (mathematics)",
      "Empirical modelling",
      "Geography",
      "Leaf wetness",
      "Machine learning",
      "Mathematics",
      "Meteorology",
      "Relative humidity",
      "Simulation"
    ],
    "authors": [
      {
        "surname": "Gillespie",
        "given_name": "Gary D."
      },
      {
        "surname": "McDonnell",
        "given_name": "Kevin P."
      },
      {
        "surname": "O'Hare",
        "given_name": "Gregory M.P."
      }
    ]
  },
  {
    "title": "A machine learning approach for forecasting hierarchical time series",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115102",
    "abstract": "In this paper, we propose a machine learning approach for forecasting hierarchical time series. When dealing with hierarchical time series, apart from generating accurate forecasts, one needs to select a suitable method for producing reconciled forecasts. Forecast reconciliation is the process of adjusting forecasts to make them coherent across the hierarchy. In literature, coherence is often enforced by using a post-processing technique on the base forecasts produced by suitable time series forecasting methods. On the contrary, our idea is to use a deep neural network to directly produce accurate and reconciled forecasts. We exploit the ability of a deep neural network to extract information capturing the structure of the hierarchy. We impose the reconciliation at training time by minimizing a customized loss function. In many practical applications, besides time series data, hierarchical time series include explanatory variables that are beneficial for increasing the forecasting accuracy. Exploiting this further information, our approach links the relationship between time series features extracted at any level of the hierarchy and the explanatory variables into an end-to-end neural network providing accurate and reconciled point forecasts. The effectiveness of the approach is validated on three real-world datasets, where our method outperforms state-of-the-art competitors in hierarchical forecasting.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005431",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Coherence (philosophical gambling strategy)",
      "Computer science",
      "Computer security",
      "Data mining",
      "Economics",
      "Exploit",
      "Hierarchy",
      "Machine learning",
      "Market economy",
      "Paleontology",
      "Physics",
      "Quantum mechanics",
      "Series (stratigraphy)",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Mancuso",
        "given_name": "Paolo"
      },
      {
        "surname": "Piccialli",
        "given_name": "Veronica"
      },
      {
        "surname": "Sudoso",
        "given_name": "Antonio M."
      }
    ]
  },
  {
    "title": "A Hybrid Approach for Parkinson’s Disease diagnosis with Resonance and Time-Frequency based features from Speech signals",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115283",
    "abstract": "Parkinson’s Disease (PD) is a progressive neurological disorder that affects the functioning of the brain. As PD progresses, there arises a problem of maintaining coordination between brain and different parts of the body. Subjects have the problem of performing daily activities. Its early diagnosis is important to improve the quality of life of Parkinson’s patients. Most of Parkinson’s patients are likely to have voice disorders in the early stages. Therefore, the analysis of voice recordings with machine learning-based models can improve the diagnosis process. Voice signals are non-linear, non-stationary signals which exhibit oscillatory behavior. In this paper, a hybrid approach is proposed to extract features from resonance-based and time-frequency based information. A combination of Resonance based Sparse Signal Decomposition (RSSD) + Time-Frequency (T-F) algorithm is proposed. Two datasets are collected for the study. The first dataset D1 is a public dataset consisting of 16 PD + 21 Healthy Controls (HC) and the second dataset D2 is collected from 20 HC to explore the effect of diversity in the diagnosis process. To explore the potential of deep learning techniques in diagnosing speech impairments in PD patients, the potential of Convolution Neural Network (CNN) is also explored. To evaluate the method, we conducted several experiments with state-of-the-art feature extraction and classification techniques. The proposed hybrid approach distinguished PD and HC with a validation accuracy of 99.37%. We have also explored the effect of diversity in the data collection process and it is observed that diversity in the socio-cultural group plays very important role to put the system in the clinical practice.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007144",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Disease",
      "Feature (linguistics)",
      "Feature extraction",
      "Linguistics",
      "Machine learning",
      "Medicine",
      "Parkinson's disease",
      "Pathology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "SIGNAL (programming language)",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Goyal",
        "given_name": "Jinee"
      },
      {
        "surname": "Khandnor",
        "given_name": "Padmavati"
      },
      {
        "surname": "Aseri",
        "given_name": "Trilok Chand"
      }
    ]
  },
  {
    "title": "An evaluation of recent neural sequence tagging models in Turkish named entity recognition",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115049",
    "abstract": "Named entity recognition (NER) is an extensively studied task that extracts and classifies named entities in a text. NER is crucial not only in downstream language processing applications such as relation extraction and question answering but also in large scale big data operations such as real-time analysis of online digital media content. Recent research efforts on Turkish, a less studied language with morphologically rich nature, have demonstrated the effectiveness of neural architectures on well-formed texts and yielded state-of-the art results by formulating the task as a sequence tagging problem. In this work, we empirically investigate the use of recent neural architectures (Bidirectional long short-term memory (BiLSTM) and Transformer-based networks) proposed for Turkish NER tagging in the same setting. Our results demonstrate that transformer-based networks which can model long-range context overcome the limitations of BiLSTM networks where different input features at the character, subword, and word levels are utilized. We also propose a transformer-based network with a conditional random field (CRF) layer that leads to the state-of-the-art result (95.95% f-measure) on a common dataset. Our study contributes to the literature that quantifies the impact of transfer learning on processing morphologically rich languages.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004905",
    "keywords": [],
    "authors": [
      {
        "surname": "Aras",
        "given_name": "Gizem"
      },
      {
        "surname": "Makaroğlu",
        "given_name": "Didem"
      },
      {
        "surname": "Demir",
        "given_name": "Seniz"
      },
      {
        "surname": "Cakir",
        "given_name": "Altan"
      }
    ]
  },
  {
    "title": "A fast technique to detect copy-move image forgery with reflection and non-affine transformation attacks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115282",
    "abstract": "Copy-move forgery is one of the most frequently utilized image tampering technique which uses the segment of the same image to produce manipulated image by duplicating or concealing image regions. To remove suspicious traces of forgery, various attacks are applied over the tampered image which make forgery detection process too complicated. We propose a forgery detection technique in which Center Surround Extrema (CenSurE) detector is applied for keypoint detection from images. To compute keypoint descriptors, Local Image Permutation Interval Descriptor (LIPID) is used. Keypoint matching is performed using k-Nearest Neighbor (k-NN) technique with utilization of k-d tree and Best-Bin-First (BBF) search method. Grouping over keypoints is performed using Fuzzy C-Means (FCM) clustering. We apply Random Sample Consensus (RANSAC) algorithm to remove outliers obtained during forgery detection process. Experimental results show that proposed technique can effectively detect forged images containing reflection and non-affine transformation with geometrical attacks. In addition, proposed approach also shows robustness against erosion, dilation, RGB color addition, zoom motion blur, JPEG compression, spread noise addition, and multiple copy-move attacks. Proposed scheme consumes least time in forgery detection as compared to state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007132",
    "keywords": [
      "Affine transformation",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "RANSAC"
    ],
    "authors": [
      {
        "surname": "Dixit",
        "given_name": "Anuja"
      },
      {
        "surname": "Bag",
        "given_name": "Soumen"
      }
    ]
  },
  {
    "title": "HiLAM-aligned kernel discriminant analysis for text-dependent speaker verification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115281",
    "abstract": "Probabilistic Linear Discriminant Analysis (PLDA) has been a commonly used backend classifier for many text-dependent speaker verification (TDSV) systems. Lately, PLDA projections have been integrated with the traditional Dynamic Time Warping (DTW) template matching framework, resulting in the DTW Online i-vector/PLDA system. The system is shown to achieve state-of-the-art performance for TDSV task. PLDA model serves to train a subspace that compensates for channel and session variabilities. It assumes linear separability between speaker-phrase information and other components. However, this relationship is known to be non-linear. The non-linearity is more prominent in case of short speech extracts, as in the case of the online i-vectors. This results in loss of vital speaker-phrase information at PLDA modeling. To this end, this work explores Kernel Discriminant Analysis (KDA) for TDSV task. It further proposes to use Hierarchical Multi-Layer Acoustic Model (HiLAM) to complement KDA with a more effective speaker-text class definition. The proposed system is hypothesized to benefit on three counts — non-linear modeling ability of KDA, speaker idiosyncrasy information associated with HiLAM-defined speaker-text units and modeling of the exact context of the pass-phrase, as offered by HiLAM. It shows a relative Equal Error Rate (EER) reduction of up to 50.63% on Part 1 of the RSR2015 database when compared to the baseline DTW Online i-vector/PLDA system.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007120",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Dynamic time warping",
      "Linear discriminant analysis",
      "Pattern recognition (psychology)",
      "Phrase",
      "Speaker diarisation",
      "Speaker recognition",
      "Speech recognition",
      "Subspace topology",
      "Support vector machine",
      "Word error rate"
    ],
    "authors": [
      {
        "surname": "Laskar",
        "given_name": "Mohammad Azharuddin"
      },
      {
        "surname": "Laskar",
        "given_name": "Rabul Hussain"
      }
    ]
  },
  {
    "title": "A mixed wholesale-option-contract to fix the demand imbalance between substitutable air cargo routes: A cooperative game approach",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115300",
    "abstract": "In this study, we consider a capacity allocation problem with an airline that sells cargo routes to multiple freight forwarders. The airline faces the problem whereby its fixed capacity from one route cannot cover the sum of freight forwarders’ orders (hot-selling routes), while the freight forwarders’ total orders from the substituting route are much less than its capacity (underutilized routes). To solve this imbalance problem, a sequential cooperative game is performed between the airline and the freight forwarders in which they agree that the airline assigns an amount in the underutilized routes proportional to the forwarder’s order from the hot-selling routes. In this game, the players’ payoffs are the expected profit from using a mixed-wholesale-option contract between the airline and the freight forwarders. The model solution shows that the demand in the underutilized routes follows self-replicating distributions. In addition, the mixed wholesale-option model is compared with the pure wholesale and pure option-contract models, and the results reveal that the mixed model provides the highest allocations in the underutilized routes, leading to a better demand balance among the substitutable routes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007302",
    "keywords": [
      "Air cargo",
      "Business",
      "Computer science",
      "Economics",
      "Engineering",
      "Finance",
      "Industrial organization",
      "Mathematics",
      "Microeconomics",
      "Operations research",
      "Order (exchange)",
      "Profit (economics)",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Shaban",
        "given_name": "I.A."
      },
      {
        "surname": "Chan",
        "given_name": "F.T.S."
      },
      {
        "surname": "Chung",
        "given_name": "S.H."
      },
      {
        "surname": "Qu",
        "given_name": "T."
      }
    ]
  },
  {
    "title": "A DEA evaluation of U.S. States’ healthcare systems in terms of their birth outcomes",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115278",
    "abstract": "States vary substantially in terms of infant mortality and preterm birth rates. Despite all the attention and effort at the national level in improving birth outcomes, many states’ healthcare systems are still suffering from high rates of unfavorable birth outcomes. Hence, the important question is what makes a healthcare system more successful when evaluating birth outcomes at the state level. To answer this question, we build a state–level database using data from the Centers for Disease Control and Prevention, Kaiser Family Foundation, and Area Health Resources File. Then, we implement a mixed-methods approach that includes Data Envelopment Analysis (DEA), robust principal component analysis, bootstrapping, and statistical characteristics methods such as truncated regression to systematically compare and contrast the performance of 50 state’s healthcare systems with respect to their birth outcomes (i.e., infant mortality, preterm birth, and low birthweight) and to provide benchmarks. Our findings reveal that socio-economic and demographic factors such as the poverty rate and the number of African-American women per 1000 population significantly explain the major variation in healthcare systems’ performance in terms of their birth outcomes. Therefore, disregarding these environmental factors leads to an overestimation of the system’s efficiency. Furthermore, our work contributes to the body of literature by introducing a unique application in the domain of efficiency performance measurement of states’ healthcare systems in the U.S., using a unique dataset that has not been analyzed before, identifying efficient and inefficient states, providing peers for each inefficient state and indicating how those inefficient states can make improvement in their performance, and providing unique policy insights.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007090",
    "keywords": [
      "Birth rate",
      "Bootstrapping (finance)",
      "Computer science",
      "Econometrics",
      "Economic growth",
      "Economics",
      "Environmental health",
      "Fertility",
      "Health care",
      "Healthcare system",
      "Medicine",
      "Population"
    ],
    "authors": [
      {
        "surname": "Darabi",
        "given_name": "Negar"
      },
      {
        "surname": "Ebrahimvandi",
        "given_name": "Alireza"
      },
      {
        "surname": "Hosseinichimeh",
        "given_name": "Niyousha"
      },
      {
        "surname": "Triantis",
        "given_name": "Konstantinos"
      }
    ]
  },
  {
    "title": "Transition-state replicator dynamics",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115254",
    "abstract": "Agent-based evolutionary game theory studies the dynamics of the autonomous agents. It is important for application that relies on the agents to perform the automated tasks. Since the agents make their own decision, therefore the stability of the interaction needs to be comprehended. The current state of the art in agent-based replicator dynamics are piecewise and state-coupled replicator dynamics which focus on joint-action single-state reward. This paper introduces additional reward parameter to the learning algorithm, extends the replicator dynamics to joint-action transition-state reward and shows that it can be changed to single-state reward and independent-action reward. The replicator equation is expressed based on the tree diagram approach and is verified with the numerical simulation in a two states battle of sexes coordination game for various types of rewards. The numerical results are consistent with the phase portraits generated by the replicator equation and are able to provide some general insights to the coordination game such as the number of convergence points, the rate of convergence and the effect of initial points on the convergence.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006862",
    "keywords": [
      "Action (physics)",
      "Algorithm",
      "Computer science",
      "Convergence (economics)",
      "Demography",
      "Economic growth",
      "Economics",
      "Evolutionary game theory",
      "Game theory",
      "Machine learning",
      "Mathematical economics",
      "Mathematical optimization",
      "Mathematics",
      "Physics",
      "Population",
      "Quantum mechanics",
      "Replicator equation",
      "Sociology",
      "Stability (learning theory)",
      "State (computer science)"
    ],
    "authors": [
      {
        "surname": "Khaw",
        "given_name": "Yan Ngee"
      },
      {
        "surname": "Kowalczyk",
        "given_name": "Ryszard"
      },
      {
        "surname": "Vo",
        "given_name": "Quoc Bao"
      },
      {
        "surname": "Abd Rahim",
        "given_name": "Nasrudin"
      },
      {
        "surname": "Che",
        "given_name": "Hang Seng"
      }
    ]
  },
  {
    "title": "Truck to door assignment in a shared cross-dock under uncertainty",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114889",
    "abstract": "This paper addresses the optimization of the truck to door assignment problem in cross-docks. It defines a new form of horizontal collaboration between suppliers by sharing the platform’s resources to enhance service level and reduce economical costs. Moreover, this study proposes to solve the problem by considering an uncertain transfer time, that is frequently observed in real-world cross-docks. Due to imprecise arrival time of trucks, equipment breakdown, or workload variation, etc, the actual transfer time tends to be shorter or longer than the prefixed one. This uncertainty is modeled as a triangular fuzzy number then a Fuzzy Chance Programming model has been proposed to solve the problem using possibilistic measures. The efficiency and robustness of both (deterministic and fuzzy) proposed models are tested empirically and obtained results confirm the positive effect of collaboration and uncertainty handling.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003304",
    "keywords": [
      "Automotive engineering",
      "Computer science",
      "DOCK",
      "Engineering",
      "Marine engineering",
      "Truck"
    ],
    "authors": [
      {
        "surname": "Essghaier",
        "given_name": "Fatma"
      },
      {
        "surname": "Allaoui",
        "given_name": "Hamid"
      },
      {
        "surname": "Goncalves",
        "given_name": "Gilles"
      }
    ]
  },
  {
    "title": "Fog computing based information classification in sensor cloud-agent approach",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115232",
    "abstract": "Sensor Cloud has emerged as a rising technology in removing the barriers of Wireless Sensor Network. It increases the lifetime of the sensor network by storing all the sensed information into cloud server instead of saving in node’s memory. Yet sensor cloud has some important issues like latency, information classification and accuracy which still need an attention to improve overall performance of the sensor cloud system. Fog computing is a problem solver for sensor cloud in removing latency and carrying out computational tasks in faster way. Fog computing is acting as a middleware between physical network and cloud, located at the edge of the network for fast computation and quick response. The proposed work utilizes the functionalities of sensor cloud and fog computing to classify and save the information in better way along with minimizing the latency issue. The Random forest classifier along with Genetic Algorithm is utilized for classifying the information and saving only required amount of information into cloud server with priority. Agent paradigm is included to save the energy of the physical sensor nodes and also to perform quick analyzing and classification of information at the fog server. The result shows that proposed work is working far better than conventional methods in terms of classification accuracy, latency, packet delivery ratio, energy consumption and network lifetime.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006643",
    "keywords": [
      "Biology",
      "Cloud computing",
      "Computer network",
      "Computer science",
      "Distributed computing",
      "Ecology",
      "Edge computing",
      "Energy consumption",
      "Latency (audio)",
      "Network packet",
      "Operating system",
      "Real-time computing",
      "Telecommunications",
      "Wireless sensor network"
    ],
    "authors": [
      {
        "surname": "Sutagundar",
        "given_name": "Ashok"
      },
      {
        "surname": "Sangulagi",
        "given_name": "Prashant"
      }
    ]
  },
  {
    "title": "An ordinal CNN approach for the assessment of neurological damage in Parkinson’s disease patients",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115271",
    "abstract": "3D image scans are an assessment tool for neurological damage in Parkinson’s disease (PD) patients. This diagnosis process can be automatized to help medical staff through Decision Support Systems (DSSs), and Convolutional Neural Networks (CNNs) are good candidates, because they are effective when applied to spatial data. This paper proposes a 3D CNN ordinal model for assessing the level or neurological damage in PD patients. Given that CNNs need large datasets to achieve acceptable performance, a data augmentation method is adapted to work with spatial data. We consider the Ordinal Graph-based Oversampling via Shortest Paths (OGO-SP) method, which applies a gamma probability distribution for inter-class data generation. A modification of OGO-SP is proposed, the OGO-SP- β algorithm, which applies the beta distribution for generating synthetic samples in the inter-class region, a better suited distribution when compared to gamma. The evaluation of the different methods is based on a novel 3D image dataset provided by the Hospital Universitario ‘Reina Sofía’ (Córdoba, Spain). We show how the ordinal methodology improves the performance with respect to the nominal one, and how OGO-SP- β yields better performance than OGO-SP.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007028",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Disease",
      "Machine learning",
      "Medicine",
      "Ordinal optimization",
      "Ordinal regression",
      "Parkinson's disease",
      "Pathology",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Barbero-Gómez",
        "given_name": "Javier"
      },
      {
        "surname": "Gutiérrez",
        "given_name": "Pedro-Antonio"
      },
      {
        "surname": "Vargas",
        "given_name": "Víctor-Manuel"
      },
      {
        "surname": "Vallejo-Casas",
        "given_name": "Juan-Antonio"
      },
      {
        "surname": "Hervás-Martínez",
        "given_name": "César"
      }
    ]
  },
  {
    "title": "Artificial datasets for hierarchical classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115218",
    "abstract": "Hierarchical classification (HC) is a especial type of multilabel classification, where an instance can be associated to multiple labels, but in HC the labels are arranged in a predefined structure, commonly a tree but in its general form a Directed Acyclic Graph (DAG). HC includes up to eight different problems, and when a method is proposed to solve one of them, the real world datasets for each problem is limited. Thus, a way to extend the evaluation of a method is to generate Artificial Datasets (ADs). ADs are useful to evaluate a method in different conditions that could not be present in the available datasets. Thus, in this work is proposed a method that is able to generate artificial datasets for up to four of the different hierarchical classification problems, which makes use of distributions to generate the instances. Furthermore, two groups of ADs were generated using the proposed method, Tree and DAG hierarchies, which are made available to the scientific community; also the source code is provided so that you can generate your own datasets. Finally, standard and state of the art methods were evaluated with the generated artificial datasets. The best performance in the datasets was obtained by a couple of methods of the state of the art which make use of Bayesian networks and chained classifiers. The proposed method for generating HC datasets provides a flexible and general alternative to evaluate different hierarchical classification methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006515",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bayesian probability",
      "Computer science",
      "Data mining",
      "Directed acyclic graph",
      "Graph",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Theoretical computer science",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Serrano-Pérez",
        "given_name": "Jonathan"
      },
      {
        "surname": "Sucar",
        "given_name": "L. Enrique"
      }
    ]
  },
  {
    "title": "A bi-objective reliable path-finding algorithm for battery electric vehicle routing",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115228",
    "abstract": "This paper proposes a bi-objective reliable path-finding algorithm for routing battery electric vehicles on a road network, with vehicles’ energy consumption uncertainty and travel time uncertainty. A bi-objective stochastic optimization problem is proposed and formulated to simultaneously maximize energy consumption reliability (ECR) and travel time reliability (TTR). ECR is defined as the probability of finishing a trip without exhausting a given battery energy budget, while TTR is the on-time arrival probability with the travel time budget. In this study, the proposed optimization problem is decomposed into two sub-problems: (1) finding K most reliable paths for maximizing the TTR objective and (2) finding the most reliable path for optimizing the ECR objective. Then, a novel ranking algorithm is proposed to exactly solve the formulated optimization problem. A case study is carried out on Hong Kong’s road network to demonstrate the efficacy and efficiency of the proposed algorithm for real-world applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006606",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Battery (electricity)",
      "Computer network",
      "Computer science",
      "Electric vehicle",
      "Electrical engineering",
      "Energy (signal processing)",
      "Energy consumption",
      "Engineering",
      "Mathematical optimization",
      "Mathematics",
      "Optimization problem",
      "Path (computing)",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Ranking (information retrieval)",
      "Reliability (semiconductor)",
      "Routing (electronic design automation)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Xiao-Wei"
      },
      {
        "surname": "Chen",
        "given_name": "Bi Yu"
      },
      {
        "surname": "Lam",
        "given_name": "William H.K."
      },
      {
        "surname": "Tam",
        "given_name": "Mei Lam"
      },
      {
        "surname": "Ma",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Multi-WRNN model for pricing the crude oil futures market",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115229",
    "abstract": "In this paper, we introduce a multi-factor wavelet-based deep recurrent neural network (Multi-WRNN) model for more accurate pricing of the crude oil future market. This model is capable of including several key factors (e.g. stock-change and refinery capacity utilization rate) flexibly. The Multi-WRNN model enables us to classify the time series of the key factors into stationary and non-stationary. Also, the model provides a dynamical system for predicting the variation mean, volatility, and value of non-stationary key factors which have high influences on market in crisis time, and consequently, can be applied in pricing of the crude oil market. The model uses a decomposition method to efficiently predict the volatilities time series. We compare our results with empirical mode decomposition (EMD) and the discrete wavelet transforms (DWT). Then, according to locality of support property of DWTs, including B-spline reverse subdivision, we use them to decompose the original high resolution volatility time series into a lowresolution time series and several details in various resolution levels. Then all decomposed time series from the DWT multiresolution process are fed to a deep recursive neural network (DRNN) model. Moreover, we analyze and compare different DWTs, including B-splines with various degrees, and two of Daubechies wavelets. Since the locality of the operations is essential, we consider only compact supported wavelets. Also, we analyze the effect of all given key factors in the performance of our pricing model. Finally, the results of Multi-WRNN model is compared with the conventional models, such as stochastic two-factor, ARIMA, FFNN-GARCH, and GARCH which show that the Multi-WRNN model with all seven key factors as inputs of the network outperforms the other models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006618",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Discrete wavelet transform",
      "Econometrics",
      "Economics",
      "Finance",
      "Futures contract",
      "Mathematical optimization",
      "Mathematics",
      "Paleontology",
      "Series (stratigraphy)",
      "Volatility (finance)",
      "Wavelet",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Hajiabotorabi",
        "given_name": "Zeinab"
      },
      {
        "surname": "Samavati",
        "given_name": "Faramarz F."
      },
      {
        "surname": "Maalek Ghaini",
        "given_name": "Farid Mohammad"
      },
      {
        "surname": "Shahmoradi",
        "given_name": "Akbar"
      }
    ]
  },
  {
    "title": "TRk-CNN: Transferable Ranking-CNN for image classification of glaucoma, glaucoma suspect, and normal eyes",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115211",
    "abstract": "In this paper, we proposed Transferable Ranking Convolutional Neural Network (TRk-CNN) that can be effectively applied when the classes of images to be classified show a high correlation with each other. The multi-class classification method based on the softmax function, which is generally used, is not effective in this case because the inter-class relationship is ignored. Although there is a Ranking-CNN that takes into account the ordinal classes, it cannot reflect the inter-class relationship to the final prediction. TRk-CNN, on the other hand, combines the weights of the primitive classification model to reflect the inter-class information to the final classification phase. We evaluated TRk-CNN in glaucoma image dataset that was labeled into three classes: normal, glaucoma suspect, and glaucoma eyes. Based on the literature we surveyed, this study is the first to classify three status of glaucoma fundus image dataset into three different classes. We compared the evaluation results of TRk-CNN with Ranking-CNN (Rk-CNN) and multi-class CNN (MC–CNN) using the DenseNet as the backbone CNN model. As a result, TRk-CNN achieved an average accuracy of 92.96%, specificity of 93.33%, sensitivity for glaucoma suspect of 95.12% and sensitivity for glaucoma of 93.98%. Based on average accuracy, TRk-CNN is 8.04% and 9.54% higher than Rk-CNN and MC–CNN and surprisingly 26.83% higher for sensitivity for suspicious than multi-class CNN. Our TRk-CNN is expected to be effectively applied to the medical image classification problem where the disease state is continuous and increases in the positive class direction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006448",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Electronic engineering",
      "Engineering",
      "Glaucoma",
      "Medicine",
      "Ophthalmology",
      "Pattern recognition (psychology)",
      "Ranking (information retrieval)",
      "Sensitivity (control systems)",
      "Softmax function"
    ],
    "authors": [
      {
        "surname": "Jun",
        "given_name": "Tae Joon"
      },
      {
        "surname": "Eom",
        "given_name": "Youngsub"
      },
      {
        "surname": "Kim",
        "given_name": "Dohyeun"
      },
      {
        "surname": "Kim",
        "given_name": "Cherry"
      },
      {
        "surname": "Park",
        "given_name": "Ji-Hye"
      },
      {
        "surname": "Nguyen",
        "given_name": "Hoang Minh"
      },
      {
        "surname": "Kim",
        "given_name": "Young-Hak"
      },
      {
        "surname": "Kim",
        "given_name": "Daeyoung"
      }
    ]
  },
  {
    "title": "Employee sentiment index: Predicting stock returns with online employee data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115294",
    "abstract": "We propose an aggregate measure of employee sentiment based on millions of employee online reviews and we test whether big employee data embedded in expert financial models can improve stock return predictability. In line with behavioral finance theory, our results document that the collective employee sentiment is a strong predictor of stock market returns with lower future returns following high employee sentiment. This predictive power is more pronounced when the employee sentiment index is constructed using the expectations of employees about the near-term business outlook of their employer. Our market-wide sentiment measure has superior performance compared to existing proxies of investor sentiment and commonly-studied macroeconomic variables. The forward-looking property of this data is also evident in predicting industry returns or portfolio returns sorted on characteristics, such as size, age, risk, profitability, dividend payout, tangibility, financial constraints and growth opportunities. Importantly, market-wide employee sentiment has relative power in predicting future asset returns after controlling for firm-level employee sentiment. The predictive power of aggregate employee online data is explained by investors’ biased beliefs about expected cash flows and volatility. These results indicate that financial models can be enriched with sentiment factors derived from various big data sources and stakeholders, providing insights into mispriced assets and assisting investment decisions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007259",
    "keywords": [
      "Business",
      "Cash flow",
      "Econometrics",
      "Economics",
      "Epistemology",
      "Finance",
      "Philosophy",
      "Physics",
      "Predictability",
      "Predictive power",
      "Profitability index",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Symitsi",
        "given_name": "Efthymia"
      },
      {
        "surname": "Stamolampros",
        "given_name": "Panagiotis"
      }
    ]
  },
  {
    "title": "How much do the central bank announcements matter on financial market? Application of the rule-based trading system approach",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115201",
    "abstract": "This paper proposes a rule-based trading system to investigate how much the central bank’s announcements matter on a financial market. We design a novel investment strategy and we simulate trades in order to quantify their profitability in the out–of–sample period using the data from a broad financial market in Poland spanning across 3 segments: stock market, foreign exchange market and bonds market. Our results show that the individual transactions delivered profits in 72.7% cases. The overall profitability across all events and all trading horizons was positive in as many as 63.6% cases. Although the financial market in Poland was only moderately sensitive to the NBP central bank’s communication, the identified types of the monetary policy announcements are economically significant and very useful for the investors, who can trade based on them and exploit them directly in the design of the rule-based trading strategies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006357",
    "keywords": [
      "Biology",
      "Business",
      "Computer science",
      "Computer security",
      "Electronic trading",
      "Exchange rate",
      "Exploit",
      "Finance",
      "Financial market",
      "Financial system",
      "Foreign exchange market",
      "Horse",
      "Market microstructure",
      "Order (exchange)",
      "Paleontology",
      "Profitability index",
      "Stock exchange",
      "Stock market",
      "Trading strategy"
    ],
    "authors": [
      {
        "surname": "Brzeszczyński",
        "given_name": "Janusz"
      },
      {
        "surname": "Gajdka",
        "given_name": "Jerzy"
      },
      {
        "surname": "Schabek",
        "given_name": "Tomasz"
      },
      {
        "surname": "Kutan",
        "given_name": "Ali M."
      }
    ]
  },
  {
    "title": "Using residual images with BSIF for iris liveness detection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115266",
    "abstract": "Deploying iris recognition systems in several security areas emphasized the importance of developing iris liveness methods. These methods verify if the iris sample acquired for authentication is fake or real. Recently, Binarized Statistical Image Features (BSIF) descriptor was successfully applied for that purpose. As BSIF is a powerful descriptor based on Local Binary Pattern (LBP) descriptor, we have supposed that enhancements that have worked before with LBP could work with BSIF as well. Widening a previous work, four public datasets representing printed, plastic, synthetic, and contact lens attacks were evaluated using 8-bit BSIF in both modes segmented and unsegmented eye images. Contact lens attack was the most challenging attack, especially in the unsegmented scenario. In this paper, a new method is proposed using residual images with BSIF to enhance the results of contact lens databases. Three high pass filters were applied separately before the feature extraction phase to improve the discrimination ability of BSIF. Clarkson contact lens database was used for evaluation in both modes segmented and unsegmented eye images. The results were promising in the unsegmented scenario and the three filters enhanced in the results with 8.6667%, 10%, and 18.3333%. The application of these filters with BSIF could be useful for other computer vision tasks like face liveness detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006989",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Computer vision",
      "Contact lens",
      "Feature (linguistics)",
      "Feature extraction",
      "Histogram",
      "IRIS (biosensor)",
      "Image (mathematics)",
      "Iris recognition",
      "Lens (geology)",
      "Linguistics",
      "Liveness",
      "Local binary patterns",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Programming language",
      "Residual"
    ],
    "authors": [
      {
        "surname": "Dronky",
        "given_name": "Manar Ramzy"
      },
      {
        "surname": "Khalifa",
        "given_name": "Wael"
      },
      {
        "surname": "Roushdy",
        "given_name": "Mohamed"
      }
    ]
  },
  {
    "title": "Fermatean fuzzy Einstein aggregation operators-based MULTIMOORA method for electric vehicle charging station selection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115267",
    "abstract": "The optimal location of electric vehicle charging station (EVCS) will promote the rapid development of the electric vehicle (EV) industry. Generally, EVCS location selection is treated as complex uncertain multi-criteria decision-making (MCDM) problem because of the existence of many quantitative and qualitative influencing factors. Moreover, uncertainty is usually occurred in EVCS location selection problem and Fermatean fuzzy set (FFS), as an expansion of Pythagorean fuzzy set, can effectively handle the ambiguity by reducing human intervention. Thus, the aim of the current study is to design an integrated decision making method for solving multi-criteria EVCS location selection problem under FFS context. This method is based on multi-objective optimization based on the ratio analysis with the full multiplicative form (MULTIMOORA) approach, maximizing deviation method and Einstein aggregation operators within Fermatean fuzzy environment. At the same time, the criteria weights are determined through the maximizing deviation method. For this, we introduce a divergence measure for FFSs. To aggregate the decision information, we propose some novel Einstein operations for FFS. In light of these operational laws, we further suggest some Fermatean fuzzy Einstein aggregation operators and their enviable characteristics. To illustrate the potentiality and usefulness of the present methodology, we carry out an illustrative study of EVCS location selection problem with FFS setting. Comparing the present MULTIMOORA framework with the extant methods confirms the strength of the obtained outcomes. The findings conclude that the introduced method is more useful and well-consistent with extant methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006990",
    "keywords": [
      "Ambiguity",
      "Artificial intelligence",
      "Biology",
      "Charging station",
      "Computer science",
      "Context (archaeology)",
      "Electric vehicle",
      "Engineering",
      "Fuzzy logic",
      "Fuzzy set",
      "Mathematical optimization",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Paleontology",
      "Physics",
      "Power (physics)",
      "Programming language",
      "Quantum mechanics",
      "Selection (genetic algorithm)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Rani",
        "given_name": "Pratibha"
      },
      {
        "surname": "Mishra",
        "given_name": "Arunodaya Raj"
      }
    ]
  },
  {
    "title": "A local search method based on edge age strategy for minimum vertex cover problem in massive graphs",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115185",
    "abstract": "Minimum vertex cover problem (MVC) is a classic combinatorial optimization problem, which has many critical real-life applications in scheduling, VLSI design, artificial intelligence, and network security. For MVC, researchers have proposed many heuristic algorithms, especially local search algorithms. And recently, researchers have increased their interest in solving large real-world graphs which require algorithms with faster searching performance. In this work, we propose a new edge weighting method called EABMS. EABMS has a time complexity of O(1). Based on EABMS, we propose our MVC solver framework called EAVC in solving MVC for massive graphs. We conducted experiments and compared the results of EAVC solvers with state of the art solvers. The results show that EABMS is effective in weighing edges for large sparse graphs and EAVC solvers outperform state of the art solvers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006217",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Cover (algebra)",
      "Edge cover",
      "Engineering",
      "Enhanced Data Rates for GSM Evolution",
      "Graph",
      "Heuristic",
      "Local search (optimization)",
      "Mathematical optimization",
      "Mathematics",
      "Mechanical engineering",
      "Medicine",
      "Programming language",
      "Radiology",
      "Scheduling (production processes)",
      "Solver",
      "Theoretical computer science",
      "Vertex (graph theory)",
      "Vertex cover",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Quan",
        "given_name": "Changsheng"
      },
      {
        "surname": "Guo",
        "given_name": "Ping"
      }
    ]
  },
  {
    "title": "Robust worst-practice interval DEA with non-discretionary factors",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115256",
    "abstract": "Traditionally, data envelopment analysis (DEA) evaluates the performance of decision-making units (DMUs) with the most favorable weights on the best practice frontier. In this regard, less emphasis is placed on non-performing or distressed DMUs. To identify the worst performers in risk-taking industries, the worst-practice frontier (WPF) DEA model has been proposed. However, the model does not assume evaluation in the condition that the environment is uncertain. In this paper, we examine the WPF-DEA from basics and further propose novel robust WPF-DEA models in the presence of interval data uncertainty and non-discretionary factors. The proposed approach is based on robust optimization where uncertain input and output data are constrained in an uncertainty set. We first discuss the applicability of worst-practice DEA models to a broad range of application domains and then consider the selection of worst-performing suppliers in supply chain decision analysis where some factors are unknown and not under varied discretion of management. Using the Monte-Carlo simulation, we compute the conformity of rankings in the interval efficiency as well as determine the price of robustness for selecting the worst-performing suppliers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006886",
    "keywords": [
      "Biochemistry",
      "Chemistry",
      "Combinatorics",
      "Composite material",
      "Computer science",
      "Data envelopment analysis",
      "Data mining",
      "Econometrics",
      "Gene",
      "Interval (graph theory)",
      "Interval data",
      "Materials science",
      "Mathematical optimization",
      "Mathematics",
      "Monte Carlo method",
      "Operations research",
      "Range (aeronautics)",
      "Robust optimization",
      "Robustness (evolution)",
      "Statistics",
      "Uncertain data"
    ],
    "authors": [
      {
        "surname": "Arabmaldar",
        "given_name": "Aliasghar"
      },
      {
        "surname": "Kwasi Mensah",
        "given_name": "Emmanuel"
      },
      {
        "surname": "Toloo",
        "given_name": "Mehdi"
      }
    ]
  },
  {
    "title": "On clustering categories of categorical predictors in generalized linear models",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115245",
    "abstract": "We propose a method to reduce the complexity of Generalized Linear Models in the presence of categorical predictors. The traditional one-hot encoding, where each category is represented by a dummy variable, can be wasteful, difficult to interpret, and prone to overfitting, especially when dealing with high-cardinality categorical predictors. This paper addresses these challenges by finding a reduced representation of the categorical predictors by clustering their categories. This is done through a numerical method which aims to preserve (or even, improve) accuracy, while reducing the number of coefficients to be estimated for the categorical predictors. Thanks to its design, we are able to derive a proximity measure between categories of a categorical predictor that can be easily visualized. We illustrate the performance of our approach in real-world classification and count-data datasets where we see that clustering the categorical predictors reduces complexity substantially without harming accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006771",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Cardinality (data modeling)",
      "Categorical variable",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Generalized linear model",
      "Law",
      "Machine learning",
      "Mathematics",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Carrizosa",
        "given_name": "Emilio"
      },
      {
        "surname": "Galvis Restrepo",
        "given_name": "Marcela"
      },
      {
        "surname": "Romero Morales",
        "given_name": "Dolores"
      }
    ]
  },
  {
    "title": "Knowledge discovery and visualisation framework using machine learning for music information retrieval from broadcast radio data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115236",
    "abstract": "Music radio data is currently underutilised in radio program management. Software tools that listen to and analyse music airplay are in many markets nonexistent, limited, or unaffordable. In this paper we present a novel knowledge discovery and visualisation framework for broadcast radio, ZeitMetric. The ZeitMetric framework uses machine learning and music information retrieval techniques to label radio audio automatically for knowledge discovery. The framework incorporates a novel music dataset collection technique (MusiGrab) to leverage online music services for ground-truth data, as well as a novel knowledge visualisation and presentation technique based on self-organizing maps (ZeitViz). The framework is compared to what little literature relating to this topic exists, and a set of requirements for a high-quality broadcast radio knowledge discovery is developed. MusiGrab specifically is compared to an existing static music information retrieval dataset and shown to offer superior results in this context. Future research directions using and extending the framework are also discussed. On acceptance of the paper, code for a use-case of the MusiGrab dataset collection technique will be released on GitHub.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006680",
    "keywords": [
      "Art",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Data science",
      "Information retrieval",
      "Knowledge extraction",
      "Leverage (statistics)",
      "Machine learning",
      "Music information retrieval",
      "Musical",
      "Paleontology",
      "Programming language",
      "Set (abstract data type)",
      "Visual arts",
      "Visualization",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Furner",
        "given_name": "Michael"
      },
      {
        "surname": "Islam",
        "given_name": "Md Zahidul"
      },
      {
        "surname": "Li",
        "given_name": "Chang-Tsun"
      }
    ]
  },
  {
    "title": "AutomaticAI – A hybrid approach for automatic artificial intelligence algorithm selection and hyperparameter tuning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115225",
    "abstract": "Recently, more and more real life problems are solved using artificial intelligence (AI) algorithms. One of the biggest challenges when working with AI is the selection and tuning of the best algorithm for solving the problem. The results generated by a given AI algorithm heavily depend on the way in which its hyperparameters are set. In most cases the process of algorithm selection and tuning is a manual, time consuming process in which the developer, based on experience and intuition tries to find the best solution from quality and execution time perspective. In this paper we present a method for solving the problem of AI algorithm selection and tuning, without human intervention, in a fully automated way. The method is a hybrid approach, a combination between particle swarm optimization and simulated annealing. We compare our approach with other similar tools like Auto-sklearn or Hyperopt-sklearn. We demonstrate the time efficiency and high accuracy of this method with some experiments on some known datasets. The paper also presents a platform for AI processing that include a set of procedures and services necessary in case of automatic processing of big datasets as well as the method for AI algorithm selection and tuning. This platform is useful for researchers and developers in an incipient phase of application development, when the best solution must be decided; it is also useful for specialists in different domains (physics, industry, economy) with less experience in using AI algorithms, but which has to process huge amount of data in an automated way.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006576",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Epistemology",
      "Hyperparameter",
      "Intuition",
      "Machine learning",
      "Operating system",
      "Particle swarm optimization",
      "Philosophy",
      "Process (computing)",
      "Programming language",
      "Selection (genetic algorithm)",
      "Set (abstract data type)",
      "Simulated annealing",
      "Swarm intelligence"
    ],
    "authors": [
      {
        "surname": "Czako",
        "given_name": "Zoltan"
      },
      {
        "surname": "Sebestyen",
        "given_name": "Gheorghe"
      },
      {
        "surname": "Hangan",
        "given_name": "Anca"
      }
    ]
  },
  {
    "title": "Divide and conquer: Ill-light image enhancement via hybrid deep network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115034",
    "abstract": "Intelligent system applications in computer vision suffer detection and identification problems in ill lighting conditions (i.e., non-uniform illumination), where under-exposed and over-exposed regions coexist in the captured images. Processing on these images results in over and under enhancement with colour and contrast distortions. The traditional methods design some handcrafted constraints and rely on image pairs and priors, whereas existing deep learning-based methods rely on large scale and even paired training data. But these method’s capacity is limited to specific scenes (i.e., lighting conditions). In this paper, we present a deep-hybrid ill-light image enhancement method and propose a contrast enhancement strategy based on the decomposition of the input images into reflection J and illumination T. A Divide to Glitter network (D2G-Net) is designed to learn from the few-shots of training samples and do not require paired and large quantity training data. D2G-Net is comprised of a multilayer Division-Net for image division and a Glitter-Net to amplify the illumination map. We propose to regularize learning using a correlation consistency of decomposition extracted from the input data itself. Extensive experiments are organized under ill-lighting conditions, where a new test dataset is also proposed with robust lighting variation to evaluate the performance of the proposed method. Experimental results prove that our method has superior performance for preserving structural and texture details compared to state-of-the-art approaches, which suggests that our method is more practical in interactive computer vision and intelligent expert system applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004759",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Contrast (vision)",
      "Deep learning",
      "Divide and conquer algorithms",
      "Division (mathematics)",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Khan",
        "given_name": "Rizwan"
      },
      {
        "surname": "Yang",
        "given_name": "You"
      },
      {
        "surname": "Liu",
        "given_name": "Qiong"
      },
      {
        "surname": "Qaisar",
        "given_name": "Zahid Hussain"
      }
    ]
  },
  {
    "title": "A review on image-based approaches for breast cancer detection, segmentation, and classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115204",
    "abstract": "The breast cancer as the most life-threatening disease among the woman has emerged in the worldwide. It is supposed that the early testing and treatment for breast cancer detection would be avoided the surgeries and increase the survival rate. A variety of research studies have motivated to improve the diagnostic methods for early diagnosis of breast cancer. This study investigates the automatic and semi-automatic image-based approaches for breast cancer diagnosis. The scope of this research has limited to theimages based diagnosisapplication journal that are published between 2016 and 2020 years. The principles and associated risk factors for diagnosis the breast cancer and existing imaging techniques are presented. The steps ofdiagnosis including preprocessing, segmentation, extracting tumor features, and tumor classification are investigated. The publicly available datasets for breast imaging are briefly introduced as well. The application issues, challenges of breast imaging technologies and future directions are discussed. Based on thedetailed study, most proposed methods use one type of imaging modalities, however, the doctor need to investigate the multiple imaging techniques to accurate diagnosis and effective treatment. Moreover, handling the multiple imaging require the processingof big data using acluster computing framework.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006370",
    "keywords": [
      "Artificial intelligence",
      "Breast cancer",
      "Breast imaging",
      "Cancer",
      "Computer science",
      "Image (mathematics)",
      "Image processing",
      "Image segmentation",
      "Internal medicine",
      "Machine learning",
      "Mammography",
      "Medical imaging",
      "Medicine",
      "Modalities",
      "Pattern recognition (psychology)",
      "Preprocessor",
      "Segmentation",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Rezaei",
        "given_name": "Zahra"
      }
    ]
  },
  {
    "title": "Development and application of slime mould algorithm for optimal economic emission dispatch",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115205",
    "abstract": "In this paper, an Improved version of the Slime Mould Algorithm (ISMA) is proposed and applied to efficiently solve the single-and bi-objective Economic and Emission Dispatch (EED) problems considering valve point effect. ISMA is developed to improve the performance of the conventional Slime Mould Algorithm (SMA). In ISMA, the solution positions are updated depending on two equations borrowed from the sine–cosine algorithm (SCA) to obtain the best solution. Multi-objective SMA (MOSMA) and Multi-objective ISMA (MOISMA) are developed based on the Pareto dominance concept and fuzzy decision-making. In the multi-objective EED problem, MOSMA and MOISMA are applied to minimize the total fuel costs and total emission with the valve point effect simultaneously. The proposed single-and bi-objective economic emission dispatch algorithms are validated using five test systems, 6-units, 10-units, 11-units, 40-units, and 110-units. The performance of the proposed algorithm is compared with Harris Hawk Optimizer (HHO), JellyfishSearch optimizer (JS), Tunicate Swarm Algorithm (TSA), Particle swarm optimization (PSO), and SMA algorithms. The results show that the proposed algorithms are more robust than other well-known algorithms. Feasible solutions using the proposed algorithms are also achieved, which adjust the schedule of generation without violation of the operating generation limits.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006382",
    "keywords": [
      "Algorithm",
      "Combinatorics",
      "Computer science",
      "Economic dispatch",
      "Electric power system",
      "Enumeration",
      "Geometry",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Particle swarm optimization",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Schedule",
      "Trigonometric functions"
    ],
    "authors": [
      {
        "surname": "Hassan",
        "given_name": "Mohamed H."
      },
      {
        "surname": "Kamel",
        "given_name": "Salah"
      },
      {
        "surname": "Abualigah",
        "given_name": "Laith"
      },
      {
        "surname": "Eid",
        "given_name": "Ahmad"
      }
    ]
  },
  {
    "title": "On clustering categories of categorical predictors in generalized linear models",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115245",
    "abstract": "We propose a method to reduce the complexity of Generalized Linear Models in the presence of categorical predictors. The traditional one-hot encoding, where each category is represented by a dummy variable, can be wasteful, difficult to interpret, and prone to overfitting, especially when dealing with high-cardinality categorical predictors. This paper addresses these challenges by finding a reduced representation of the categorical predictors by clustering their categories. This is done through a numerical method which aims to preserve (or even, improve) accuracy, while reducing the number of coefficients to be estimated for the categorical predictors. Thanks to its design, we are able to derive a proximity measure between categories of a categorical predictor that can be easily visualized. We illustrate the performance of our approach in real-world classification and count-data datasets where we see that clustering the categorical predictors reduces complexity substantially without harming accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006771",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Cardinality (data modeling)",
      "Categorical variable",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Generalized linear model",
      "Law",
      "Machine learning",
      "Mathematics",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Carrizosa",
        "given_name": "Emilio"
      },
      {
        "surname": "Galvis Restrepo",
        "given_name": "Marcela"
      },
      {
        "surname": "Romero Morales",
        "given_name": "Dolores"
      }
    ]
  },
  {
    "title": "Differential privacy trajectory data protection scheme based on R-tree",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115215",
    "abstract": "With the popularization of mobile devices with positioning functions, location-based service (LBS) plays a vital role in people's daily lives. The problem of privacy leakage becomes more and more critical. Although the application technology of LBS is developing rapidly, the corresponding privacy protection technology is growing slowly. At present, differential privacy technology has received attention from many researchers, but it is not easy to reasonably apply it to trajectory privacy protection. Most trajectory privacy protection models only focus on the spatial location of mobile users without considering the temporal characteristics of the trajectory, which destroys the spatial–temporal characteristics of the trajectory. Therefore, to beat this difficult problem, a differential privacy trajectory data protection scheme based on R-tree is suggested. Firstly, the trajectory similarity tree structure is proposed on the basis of R-tree index structure to realize the trajectory data's spatial storage and query processing. Secondly, the DPTS-tree (Differential Privacy Trajectory Similarity tree, DPTS-tree) is constructed with differential privacy technology, adding noise to the statistical values of mobile users in the nodes, which can greatly improve the ability to resist arbitrary background knowledge attacks and achieve the purpose of protecting data privacy. Then, to resist other information inference attacks in the trajectory data, when constructing the DPTS-tree, random noise is added to location data and other data in the trajectory. Finally, the algorithm is subjected to consistency constraints to address the problem that the added independent noise may cause data inconsistency. Experiments show that our algorithm can effectively protect sensitive and private information of users and ensure the usability of trajectory data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006485",
    "keywords": [
      "Astronomy",
      "Computer science",
      "Computer security",
      "Data mining",
      "Differential privacy",
      "Information leakage",
      "Mathematical analysis",
      "Mathematics",
      "Physics",
      "Privacy protection",
      "Trajectory",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Yuan",
        "given_name": "Shuilian"
      },
      {
        "surname": "Pi",
        "given_name": "Dechang"
      },
      {
        "surname": "Zhao",
        "given_name": "Xiaodong"
      },
      {
        "surname": "Xu",
        "given_name": "Meng"
      }
    ]
  },
  {
    "title": "A novel mathematical programming model for multi-mode project portfolio selection and scheduling with flexible resources and due dates under interval-valued fuzzy random uncertainty",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115207",
    "abstract": "Resource management plays a pivotal role in project implementation success. The profitability of project-based companies not only relies on the right selection of project portfolio but also depends on accurate scheduling and resource management. This issue is addressed by optimal project portfolio selection and scheduling using flexible resource availability and projects’ due date. The motivation of flexible resources is to maximize cash flow by resource management and adding more projects to a portfolio by reinvesting the gain to increase resource availability during planning horizon. Flexible due date leads to perform more projects besides increasing the robustness of multiple projects. This study proposes a new linear structure of mixed-integer programming (MIP) model of project portfolio selection and scheduling considering the resource management, cash flow, tardiness cost, and robustness of multiple projects. Moreover, to a better description of real-life project situations, a new solution approach is introduced based on triangular interval-valued fuzzy random variables to incorporate both fuzziness and randomness uncertainties into mathematical programming models. To solve the proposed interval-valued fuzzy random model, a generalized credibility-based chance constraint measure is applied. The application and performance of proposed model are evaluated through different small to large-sized test problems and a real case study to tackle uncertainties and complexities explicitly.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006400",
    "keywords": [
      "Accounting",
      "Artificial intelligence",
      "Biochemistry",
      "Cash flow",
      "Chemistry",
      "Computer science",
      "Economics",
      "Engineering",
      "Financial economics",
      "Fuzzy logic",
      "Gene",
      "Job shop scheduling",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Operations research",
      "Portfolio",
      "Project management",
      "Project portfolio management",
      "Randomness",
      "Robustness (evolution)",
      "Schedule",
      "Statistics",
      "Systems engineering",
      "Tardiness",
      "Time horizon"
    ],
    "authors": [
      {
        "surname": "Zolfaghari",
        "given_name": "Samaneh"
      },
      {
        "surname": "Mousavi",
        "given_name": "Seyed Meysam"
      }
    ]
  },
  {
    "title": "A hybrid approach of adaptive wavelet transform, long short-term memory and ARIMA-GARCH family models for the stock index prediction",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115149",
    "abstract": "Modelling and forecasting the stock price constitute an important area of financial research for both academics and practitioners. This study seeks to determine whether improvements can be achieved by forecasting the stock index and volatility with the use of a hybrid model and incorporating the financial variables. We extend the literature of stock market forecasting by applying a hybrid model which combines adaptive wavelet transform (AWT), long short-term memory (LSTM) and ARIMAX-GARCH family models to predict stock index and combines AWT, LSTM and heterogeneous autoregressive model of realized volatility (HAR-RV) model to predict stock volatility for two major indexes in the U.S. stock market including the Dow Jones Industrial Average (DJIA) and Nasdaq Composite (IXIC). The results indicate an overall improvement in forecasting of stock index using the AWT-LSTM-ARMAX-FIEGARCH model with student’s t distribution as compared to the benchmark models. The robust test proves that this model has a higher prediction accuracy in prediction of different time horizons (1-, 10-, 15-, 20-, 30-, and 60-days ahead) for both stock indexes. Also, AWT-LSTM improves ability of HAR (3) X-RV model in prediction of the stocks realized volatility for mentioned time horizons.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100590X",
    "keywords": [
      "Artificial intelligence",
      "Autoregressive conditional heteroskedasticity",
      "Autoregressive fractionally integrated moving average",
      "Autoregressive integrated moving average",
      "Autoregressive model",
      "Biology",
      "Composite index",
      "Composite indicator",
      "Computer science",
      "Econometrics",
      "Economics",
      "Engineering",
      "Horse",
      "Long memory",
      "Machine learning",
      "Mechanical engineering",
      "Paleontology",
      "Stock (firearms)",
      "Stock market",
      "Stock market index",
      "Stock market prediction",
      "Time series",
      "Volatility (finance)",
      "Wavelet"
    ],
    "authors": [
      {
        "surname": "Zolfaghari",
        "given_name": "Mehdi"
      },
      {
        "surname": "Gholami",
        "given_name": "Samad"
      }
    ]
  },
  {
    "title": "Automatic construction of RDF with web tables",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115200",
    "abstract": "Being a metadata model, the Resource Description Framework (RDF) has been widely used in the context of semantic extraction, unified organization and intelligent processing of a large number of data because RDF’s machine understandability. Knowledge graph based on RDF model, for example, is predominantly generated from massive data with various models and formats. While many efforts have been carried out in converting databases and XML into RDF, the generation of RDF from Web tables currently receives less attention due to their flexible and complex structure as well as difficult understanding. Given that billions of Web tables are widely used in real-world applications, in this paper, we devote to the automatic construction of RDF with diverse Web tables. We recognize total six representative types of Web tables that have been extensively applied in Web-based applications, including Web tables with simple structure and complex structure. For each type of Web tables, we propose our approach for converting it into RDF. Our approaches can well identify the semantics of Web tables with complex structure and then generate the corresponding RDF. We implement an RDF generation system and experimental results show the feasibility of our proposed approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006345",
    "keywords": [
      "Computer science",
      "Cwm",
      "Database",
      "Information retrieval",
      "Linked data",
      "RDF",
      "RDF Schema",
      "RDF/XML",
      "SPARQL",
      "Semantic Web",
      "Simple Knowledge Organization System",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Li"
      },
      {
        "surname": "Sheng",
        "given_name": "Jie"
      },
      {
        "surname": "Tu",
        "given_name": "Yaofeng"
      },
      {
        "surname": "Zhou",
        "given_name": "Xiangsheng"
      },
      {
        "surname": "Ma",
        "given_name": "Zongmin"
      }
    ]
  },
  {
    "title": "An interpretable data augmentation scheme for machine fault diagnosis based on a sparsity-constrained generative adversarial network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115234",
    "abstract": "Vibration signal-based methods have been widely utilized in machine fault diagnosis. Usually, a lack of sufficient training data can prevent these methods from achieving satisfactory performance. The generative adversarial network (GAN) is a feasible solution to this problem. However, existing GAN-based methods struggle to stably generate raw vibration signals. To achieve vibration signal generation, a novel sparsity-constrained GAN (SC-GAN) method containing a two-stage training process is developed, which can perform data augmentation for machine fault diagnosis with a simple structure. Autoencoder (AE)-based pretraining and sparsity regularization constraints are implemented in the proposed method. Furthermore, to understand the internal mechanisms of vibration signal generation, we propose a method for analyzing the network’s weight matrix to interpret the generation mechanism. In a case study on rolling element bearings, the SC-GAN is verified to be able to generate raw vibration signals under 10 different health conditions with a more stable training process than other models. In a fault diagnosis task, the data augmentation by SC-GAN significantly improves the diagnostic accuracy by 7.44%. An analysis of the well-trained SC-GAN shows that the model captures key frequency components, which provides a credible interpretation for the generation mechanism. Another case study on the gearbox illustrates the good generalization ability of SC-GAN to other machines and more complicated signals.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006667",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Fault (geology)",
      "Generative adversarial network",
      "Generative grammar",
      "Geology",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Scheme (mathematics)",
      "Seismology"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Liang"
      },
      {
        "surname": "Ding",
        "given_name": "Yu"
      },
      {
        "surname": "Wang",
        "given_name": "Zili"
      },
      {
        "surname": "Wang",
        "given_name": "Chao"
      },
      {
        "surname": "Ma",
        "given_name": "Jian"
      },
      {
        "surname": "Lu",
        "given_name": "Chen"
      }
    ]
  },
  {
    "title": "A heterogeneous GRA-CBR-based multi-attribute emergency decision-making model considering weight optimization with dual information correlation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115208",
    "abstract": "The emergency response in an engineering operating entails a heterogeneous multi-attribute emergency decision-making (HMAEDM) problem containing crisp, interval, linguistic variables, etc. which is a dynamic process due to the continuous changes of engineering environment. However, few of researches on engineering emergency decision involved with heterogeneous multi attributes, and the static and non-inferential analytical methods ignored the linkage of the dual information between objective characteristics and preference information, which failed to effectively quantify the attribute weights and the similarity between cases and thus are difficult to make a valid decision for the dynamic decision-making environment. Based on case-based reasoning (CBR), a novel emergency decision model embedding with the grey relational analysis (GRA) and grey wolf optimization (GWO) algorithms, is proposed to help engineering emergency decision. Specially, the global indicator is derived by grey incidence degree for the similarity measure between heterogeneous multi-attribute cases from the perspective of system space, along with the GWO-based relative entropy method considering dual information correlation is designed for more reasonable weight allocation. Hence, the proposed model can exploit previous experiences and retrieve the optimal alternative for the future engineering emergency with heterogeneous multi attributes by imitating human thinking process. Finally, a real case in a water transfer project and comparison with three popular methods are conducted to demonstrate the applicability and effectiveness of the proposed model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006412",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Data mining",
      "Dual (grammatical number)",
      "Embedding",
      "Exploit",
      "Grey relational analysis",
      "Ideal solution",
      "Image (mathematics)",
      "Literature",
      "Machine learning",
      "Mathematical economics",
      "Mathematics",
      "Operations research",
      "Physics",
      "Similarity (geometry)",
      "Similarity measure",
      "TOPSIS",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Wenlong"
      },
      {
        "surname": "Wang",
        "given_name": "Xiaoling"
      },
      {
        "surname": "Wang",
        "given_name": "Wei"
      },
      {
        "surname": "Zhu",
        "given_name": "Yushan"
      },
      {
        "surname": "Cai",
        "given_name": "Zhijian"
      },
      {
        "surname": "Yang",
        "given_name": "Shuai"
      }
    ]
  },
  {
    "title": "S-FPN: A shortcut feature pyramid network for sea cucumber detection in underwater images",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115306",
    "abstract": "Convolutional neural network is a prominent innovation in computer vision but is often troubled by problems such as dark light, turbidity, blur and high similarity to the background when applied to underwater object detection. Underwater object detection is one of the basic techniques of underwater grasping automation which plays a very important role in ocean detection and fishery of aquatic products. This paper presented an automatic detection method of underwater sea cucumber based on deep learning, which will provide effective technical support for the automated breeding and harvesting of sea cucumber. The Shortcut Feature Pyramid Network (S-FPN) proposed in this paper improves the existing multi-scale feature fusion strategy through shortcut connection. The ablation experimental results show that the mean average precision (mAP) of S-FPN reaches 91.5% which outperforms the baseline Feature Pyramid Network (88.6%), YOLO v3 (83.7%) and SVM-HOG (61.6%). To resolve the problem of complex environmental background interference of ocean floor, we proposed a Piecewise Focal Loss (PFL) function for balancing the positive and negative samples such that the algorithm can focus on the training difficulty of hard (i.e., positive) samples. And the ablation experimental results show that the mAP of PFL reaches 92.3% which outperforms the baseline Cross Entropy (91.5%) and Focal Loss (91.8%). Also, we chose Exponential Linear Unit as the optimization strategy, and Adaptive Moment Estimation as the activation function by ablation research, finally the mAP reached 94%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007351",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Colored dissolved organic matter",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Cross entropy",
      "Feature (linguistics)",
      "Geology",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Nutrient",
      "Object detection",
      "Oceanography",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Phytoplankton",
      "Pyramid (geometry)",
      "Remote sensing",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Fang"
      },
      {
        "surname": "Miao",
        "given_name": "Zheng"
      },
      {
        "surname": "Li",
        "given_name": "Fei"
      },
      {
        "surname": "Li",
        "given_name": "Zhenbo"
      }
    ]
  },
  {
    "title": "Detecting cryptocurrency pump-and-dump frauds using market and social signals",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115284",
    "abstract": "The cryptocurrency market has gained significant traction in the last decade, becoming an alternative finance platform to traditional stock market trading. Despite its rapid evolution, legal regulations have not yet caught up to the cryptocurrency market’s progress, attracting the attention of scammers looking to exploit legal loopholes for profits. Pump-and-dump schemes, a well-worn fraud device, has regained relevance in this new territory. In a typical pump-and-dump scheme, scammers organize and leverage media channels to artificially inflate the price of an alternative cryptocurrency, only to quickly sell them to profit off unsuspecting buyers. The disruptive nature of pump-and-dump schemes necessitates a system to reliably forecast pump targets and the magnitude of its success. In this paper, we propose an approach to predict the target cryptocurrency for each pump before its announcement using market and social media signals using Neural Network-based architectures while offering interpretable insights into their black-box nature. Additionally, we construct models that are capable of forecasting the highest price induced by the pump after the cryptocurrency’s identity is revealed within 6.1% error margin. We examine the optimal temporal windows and describe the limitations of social data to predict the manipulations in cryptocurrency trade. Our experimental results serve as proof of a feasible forecasting expert system for identifying cryptocurrency pump-and-dump frauds using publicly available data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007156",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer security",
      "Context (archaeology)",
      "Cryptocurrency",
      "Exploit",
      "Leverage (statistics)",
      "Paleontology",
      "Stock market"
    ],
    "authors": [
      {
        "surname": "Nghiem",
        "given_name": "Huy"
      },
      {
        "surname": "Muric",
        "given_name": "Goran"
      },
      {
        "surname": "Morstatter",
        "given_name": "Fred"
      },
      {
        "surname": "Ferrara",
        "given_name": "Emilio"
      }
    ]
  },
  {
    "title": "Stochastic Local Search for the Direct Aperture Optimisation Problem",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115206",
    "abstract": "Intensity modulated radiation therapy (IMRT) is a widely applied cancer treatment technique that aims at effectively deliver radiation to cancerous cells while sparing surrounding healthy tissues. To this end, the radiation coming from a linear accelerator is modulated using a physical device called multi-leaf collimator. Traditionally, a sequential approach is applied to generate treatment plans in IMRT. In this approach, the optimal intensities for each beam angle are computed (Fluence Map Optimisation, FMO), and then a sequencing problem is solved to obtain the collimator setup (aperture shapes) required to achieve the intensities computed for the FMO problem. The sequencing step is critical as the treatment plans computed to solve the FMO problem are not clinically acceptable. Unfortunately, treatment plans obtained by the sequencing step are severely impaired w.r.t. those obtained in the FMO step. One approach that aims to deal with the problem described above is the Direct Aperture Optimisation (DAO) approach. The DAO problem aims at simultaneously determining deliverable aperture shapes and a set of radiation intensities. This approach considers both physical and delivery time constraints, allowing to generate clinically acceptable treatment plans. In this paper, we propose a stochastic local search algorithm to solve the DAO problem. Our approach alternates the search between two neighbourhood definitions; the first focused on the fluence map (intensities) and the second on the aperture shapes. We analyse the algorithmic components, parameters, and overall results obtained by our algorithm on a set of clinical prostate cancer cases. We compare the obtained treatment plans to the ones obtained by the traditional two-step approach. Results show that our algorithm is able to find deliverable treatment plans without major impairments in treatment plans quality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006394",
    "keywords": [
      "Acoustics",
      "Algorithm",
      "Aperture (computer memory)",
      "Beam (structure)",
      "Collimator",
      "Computer science",
      "Linear particle accelerator",
      "Mathematical optimization",
      "Mathematics",
      "Multileaf collimator",
      "Optics",
      "Physics",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Pérez Cáceres",
        "given_name": "Leslie"
      },
      {
        "surname": "Araya",
        "given_name": "Ignacio"
      },
      {
        "surname": "Cabrera-Guerrero",
        "given_name": "Guillermo"
      }
    ]
  },
  {
    "title": "Automated EEG signal classification using chaotic local binary pattern",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115175",
    "abstract": "Background Electroencephalography (EEG) signals are the electrical signals which depicts the brain's neuronal activities. The EEG signals inherently have nondeterministic patterns. Hence, we have used a chaotic feature generation function to classify normal and abnormal EEG signals in this work. Method This research presents a new generation abnormal EEG detection model using a chaotic one-dimensional local binary pattern (CLBP) and wavelet packet decomposition (WPD) techniques. The Temple University Hospital (TUH) EEG dataset is used to develop and evaluate our chaotic feature generation model. In this work, the WPD is performed on EEG signals, and then CLBP is applied on the decomposed signals to extract the features. The iterative minimum redundancy maximum relevancy (ImRMR) is applied to select the clinically significant features. Finally, these features are classified into normal and abnormal EEG classes using a support vector machine (SVM) classifier. Results Our developed model yielded the detection accuracies of 93.84% to 98.19% for 24 channels using SVM classifier with ten-fold cross-validation strategy. Conclusion We have obtained the highest classification performance of 98.19% for the PZ channel that is the highest performance so far using this database. Our developed model is ready to be tested with more EEG data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006138",
    "keywords": [
      "Artificial intelligence",
      "Binary classification",
      "Chaotic",
      "Computer science",
      "Electroencephalography",
      "Feature extraction",
      "Pattern recognition (psychology)",
      "Psychiatry",
      "Psychology",
      "Speech recognition",
      "Support vector machine",
      "Wavelet"
    ],
    "authors": [
      {
        "surname": "Tuncer",
        "given_name": "Turker"
      },
      {
        "surname": "Dogan",
        "given_name": "Sengul"
      },
      {
        "surname": "Rajendra Acharya",
        "given_name": "U."
      }
    ]
  },
  {
    "title": "Stable first-arrival picking through adaptive threshold determination and spatial constraint clustering",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115216",
    "abstract": "First-arrival picking is a critical step of determining the onsets of signals in seismic exploration. Existing approaches are usually sensitive to background noise and user-specified parameters. In this paper, we present the first-arrival picking through adaptive threshold determination and spatial constraint clustering (FASC) algorithm with four steps. The mean filtering step steadily forms an extended signal region. The adaptive threshold determination step stably obtains the first-arrival range in a bottom-up manner. The spatial constraint clustering step clusters the data within the range. The adaptive cluster-selecting step selects the appropriate cluster as first arrivals. Experiments are undertaken on two field datasets. The results show that FASC is more accurate than the three classic and latest algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006497",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Composite material",
      "Computer science",
      "Constraint (computer-aided design)",
      "Data mining",
      "Geometry",
      "Image (mathematics)",
      "Materials science",
      "Mathematics",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Range (aeronautics)"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Lei"
      },
      {
        "surname": "Jiang",
        "given_name": "Haokun"
      },
      {
        "surname": "Min",
        "given_name": "Fan"
      }
    ]
  },
  {
    "title": "The AgriQ: A low-cost unmanned aerial system for precision agriculture",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115163",
    "abstract": "Precision agriculture currently presents significant growth thanks to the development of three main technologies: drones, vision-based systems, and embedded electronics. To take advantage of these technologies, services and products have launched cameras, drones, and algorithms in the cloud, from which the farmer can easily obtain valuable information for the optimization of their production. Notwithstanding the previous, in Latin America (and in general in the undeveloped countries), these technologies are costly and sometimes difficult to access for most farmers. This study proposes developing the low-cost Unmanned Aerial System (UAS) for precision agriculture tasks called AgriQ. It comprises three subsystems: (a) the drone; (b) the multispectral imaging system; and (c) the open-source software responsible for computing valuable information for the farmers. Intending to obtain a competitive UAS, we tackle the problem from four main points: (1) the construction of the drone; (2) the vision algorithms; (3) the autonomous trajectory considering all the parameters for properly recovering all the crops’ visual information; and (4) the construction of a low-cost multispectral imaging system with multiple bands. The multispectral imaging system is onboard the quadrotor and is composed of two low-cost cameras that have been modified to output multispectral imagery. With all this unmanned aerial system, we compute 8 vegetation indices that provide useful information to the farmers. To verify that the AgriQ is competitive versus commercial systems, we compared its performance to the professional multispectral camera RedEdge-M and popular Pix4D software. For this purpose, we have conducted experiments in two real environments, one of which is a real crop field. The experiments’ data show that our AgriQ UAS achieved prominent results at a fraction of the commercial system cost. Furthermore, to provide the user with a complete low-cost tool for precision agriculture, all the parts of the AgriQ are presented and explained in detail. The software is developed using open-source code where our contribution is available online on our GitHub site.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006047",
    "keywords": [
      "Agriculture",
      "Archaeology",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Drone",
      "Genetics",
      "Geography",
      "Machine vision",
      "Multispectral image",
      "Precision agriculture",
      "Programming language",
      "Real-time computing",
      "Software"
    ],
    "authors": [
      {
        "surname": "Montes de Oca",
        "given_name": "Andrés"
      },
      {
        "surname": "Flores",
        "given_name": "Gerardo"
      }
    ]
  },
  {
    "title": "Advancement of the search process of salp swarm algorithm for global optimization problems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115292",
    "abstract": "This paper propounds a modified version of the salp swarm algorithm (mSSA) for solving optimization problems more prolifically. This technique is refined from the base version with three simple but effective modifications. In the first one, the most important parameter in SSA responsible for balancing exploration and exploitation is chaotically changed by embedding a sinusoidal map in it to catch a better balance between exploration and exploitation from the first iteration until the last. As a short falling, SSA can’t exchange information amongst leaders of the chain. Therefore, a mutualistic relationship between two leader salps is included in mSSA to raise its search performance. Additionally, a random technique is systematically applied to the follower salps to introduce diversity in the chain. This can be since there may be some salps in the chain that do not necessarily follow the leader for exploring unvisited areas of the search space. Several test problems are solved by the advocated approach and results are presented in comparison with the relevant results in the available literature. It is ascertained that mSSA, despite its simplicity, significantly outperforms not only the basic SSA but also numerous recent algorithms in terms of fruitful solution precision and convergent trend line.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007235",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Base (topology)",
      "Computer science",
      "Embedding",
      "Epistemology",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Philosophy",
      "Process (computing)",
      "Simplicity",
      "Swarm behaviour"
    ],
    "authors": [
      {
        "surname": "Çelik",
        "given_name": "Emre"
      },
      {
        "surname": "Öztürk",
        "given_name": "Nihat"
      },
      {
        "surname": "Arya",
        "given_name": "Yogendra"
      }
    ]
  },
  {
    "title": "A parallel metaheuristic approach for ensemble feature selection based on multi-core architectures",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115290",
    "abstract": "Ensemble learning have emerged as a useful machine learning technique, which is based on the idea that combining the output of multiple models instead of using a single model. This practice, known as “diversity”, and it usually enhances the performance. On other hand, ensemble feature selection method is based on the same idea, where multiple feature subsets are combined to select an optimal subset of features. Learning methods have difficulties with the dimensionality curse that impact the performance and increase the time exponentially. To overcome this issue, we propose a parallel heterogeneous ensemble feature selection based on three well-regarded algorithms: genetic algorithm, particle swarm optimizer, and grey wolf optimizer. The proposed approach is based on four phases; namely, distribution phase, parallel ensemble feature selection phase, combining and aggregation phase, and testing phase. Three implementations of the proposed approach are presented: a sequential approach running on the central processing unit (CPU), a parallel approach running on multi-core CPU, and a parallel approach running on multi-core CPU with graphics processing units (GPU). To assess the performance of the proposed approach twenty-one large datasets were used. The results show that the proposed parallel approach improved the performance in terms of the prediction results and running time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007211",
    "keywords": [
      "Artificial intelligence",
      "Central processing unit",
      "Computer science",
      "Curse of dimensionality",
      "Ensemble learning",
      "Feature (linguistics)",
      "Feature selection",
      "Graphics processing unit",
      "Linguistics",
      "Machine learning",
      "Multi-core processor",
      "Operating system",
      "Parallel computing",
      "Philosophy",
      "Single-core"
    ],
    "authors": [
      {
        "surname": "Hijazi",
        "given_name": "Neveen Mohammed"
      },
      {
        "surname": "Faris",
        "given_name": "Hossam"
      },
      {
        "surname": "Aljarah",
        "given_name": "Ibrahim"
      }
    ]
  },
  {
    "title": "Modelling cloud service latency and availability using a deep learning strategy",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115121",
    "abstract": "Low latency and high availability in cloud services give users satisfactory response time and guarantee stability to request they make to services that are hosted in the cloud, thus increasing the usability and reliability of cloud services. On the other hand, high latencies and poor availability will cost businesses their customers due customers dissatisfaction, thus losing customers to competitors. This situation noticeable in e-commerce businesses where real-time response and decision making within seconds are critical for business service delivery. Therefore, latency and availability are important parameters in the Service Level Agreement for cloud users when choosing Cloud Services Providers (CSPs). But the challenge for businesses is that they do not have their in–house mechanism that can accurately predict the required latency and availability for their requirements. Companies only rely on the CSPs tools for estimating resource requirements, which is biased towards the CSPs business model. In this paper, we developed a deep learning algorithm that predicts the latency and availability of cloud services using real-time live data from three CSPs. We designed and implemented experiments on Amazon Web Services, Alibaba Cloud and Tencent Cloud in Beijing University of Posts and Telecommunications to run compute instances across the United States, Europe and Asia Pacific regions. In each cloud platform, five servers were used that resulted in 30,815,100 invocations of http and ping operations for 6 weeks. The algorithm used the data on hourly, daily and weekly basis as historical network data to predict latency and availability. We used MATLABs deep learning toolbox for the implementation of our algorithm and the results showed that the prediction is usually above 90% accurate as compared with the data obtained. The results also revealed that latency performance depends on the locations of users and the availability depends on number of availability zones used.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005625",
    "keywords": [
      "Business",
      "Cloud computing",
      "Computer network",
      "Computer science",
      "Latency (audio)",
      "Marketing",
      "Operating system",
      "Server",
      "Service (business)",
      "Service provider",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Peng"
      },
      {
        "surname": "Goteng",
        "given_name": "Gokop L."
      },
      {
        "surname": "He",
        "given_name": "Yu"
      }
    ]
  },
  {
    "title": "Vision-based hand gesture recognition using deep learning for the interpretation of sign language",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115657",
    "abstract": "Hand gestures have been the key component of communication since the beginning of an era. The hand gestures are the foundation of sign language, which is a visual form of communication. In this paper, a deep learning based convolutional neural network (CNN) model is specifically designed for the recognition of gesture-based sign language. This model has a compact representation that achieves better classification accuracy with a fewer number of model parameters over the other existing architectures of CNN. In order to evaluate the efficacy of this model, VGG-11 and VGG-16 have also been trained and tested in this work. To evaluate the performance, 2 datasets have been considered. First, in this work, a large collection of Indian sign language (ISL) gestures consisting of 2150 images is collected using RGB camera, and second, a publicly available American sign language (ASL) dataset is used. The highest accuracy of 99.96% and 100% is obtained by the proposed model for ISL and ASL datasets respectively. The performance of the proposed system, VGG-11, and VGG-16 are experimentally evaluated and compared with the existing state-of-art approaches. In addition to accuracy, other efficiency indices have been also used to ascertain the robustness of the proposed work. The findings indicate that the proposed model outperforms the existing techniques as it has the potential to classify maximum gestures with a minimal rate of error. The model is also tested with the augmented data and is found as invariant to rotation and scaling transformation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421010484",
    "keywords": [
      "American Sign Language",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Gene",
      "Gesture",
      "Gesture recognition",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "RGB color model",
      "Robustness (evolution)",
      "Sign language",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Sharma",
        "given_name": "Sakshi"
      },
      {
        "surname": "Singh",
        "given_name": "Sukhwinder"
      }
    ]
  },
  {
    "title": "Intelligent approach to automated star-schema construction using a knowledge base",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115226",
    "abstract": "Most data-warehouse construction processes are performed manually by experts, which is laborious, time-consuming, and prone to error. Furthermore, special knowledge is required to design complex multidimensional models, such as a star schema. This predicament has motivated computer scientists to propose automation techniques to generate such models. For this reason, we present a new strategy that incorporates knowledge-based models into a framework, named the Semantic-based Star-schema Designer, that assists the automation of star schema construction. Our models provide reasoning capabilities needed by star schema designs, including those that can disambiguate heterogeneous terms, detect appropriate data types and attribute sizes, and organize data hierarchies to support online analytical processes. We also propose strategies to overcome the uncertainty arising when attribute names are not available in the data source. The names of unknown attributes are thus predicted using an arithmetic coding technique to infer column names. Our system also generates star schema from semi-structured data (e.g., comma-separated-value files and spreadsheets), which do not provide primary keys, foreign keys, or relationship cardinalities between tables. Our framework facilitates star schema construction and their relationship information without human intervention using homegrown algorithms. Experiments demonstrate that our technique predicts column names and data types that enable the effective generation of star schema better than baseline approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006588",
    "keywords": [
      "Artificial intelligence",
      "Automation",
      "Computer science",
      "Data mining",
      "Database design",
      "Database schema",
      "Engineering",
      "Information retrieval",
      "Knowledge base",
      "Mechanical engineering",
      "Schema (genetic algorithms)",
      "Star schema"
    ],
    "authors": [
      {
        "surname": "Sanprasit",
        "given_name": "Non"
      },
      {
        "surname": "Jampachaisri",
        "given_name": "Katechan"
      },
      {
        "surname": "Titijaroonroj",
        "given_name": "Taravichet"
      },
      {
        "surname": "Kesorn",
        "given_name": "Kraisak"
      }
    ]
  },
  {
    "title": "Improved political optimizer for complex landscapes and engineering optimization problems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115178",
    "abstract": "Political Optimizer (PO) is a recently proposed meta-heuristic with excellent convergence speed and exploitation capability. However, it is found that PO prematurely converges for complex problems because of not giving enough time to the exploration. In this paper, the exploration capability and balance of PO are improved by making multiple modifications to propose an Improved Political Optimizer (IPO). To improve the exploration capability, the condition of an equal number of parties and constituencies is relaxed, and switching with a random member of a random party is incorporated in the party-switching phase. Moreover, the balance between exploration and exploitation is enhanced by modifying the position-updating strategy (RPPUS) in the election campaign phase and replacing the tunable party-switching rate with a self-adaptive parameter. The exploitation is further improved by utilizing the best solution of the population in the parliamentary affairs phase. In addition to improvement in PO, this paper also highlights a correction in the party-switching phase of the original PO. The performance of IPO is evaluated using 30 CEC-2014 benchmarks, 29 CEC-BC-2017 benchmarks, and 6 mechanical engineering problems. It is shown through non-parametric statistical Wilcoxon’s rank-sum test that IPO significantly outperforms PO. Moreover, IPO is also compared with 10 of the well-cited and 14 latest optimization algorithms published in 2020. It is shown by using the Friedman mean-rank test that IPO secures the first rank for both types of benchmark functions. Moreover, the comparison of IPO with PO and a few well-known algorithms for 6 of the engineering problems shows that IPO performs better or equivalently to the compared optimization algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006163",
    "keywords": [
      "Benchmark (surveying)",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Convergence (economics)",
      "Demography",
      "Economic growth",
      "Economics",
      "Finance",
      "Geodesy",
      "Geology",
      "Heuristic",
      "Initial public offering",
      "Mann–Whitney U test",
      "Mathematical optimization",
      "Mathematics",
      "Organic chemistry",
      "Parametric statistics",
      "Phase (matter)",
      "Population",
      "Position (finance)",
      "Rank (graph theory)",
      "Sociology",
      "Statistics",
      "Wilcoxon signed-rank test"
    ],
    "authors": [
      {
        "surname": "Askari",
        "given_name": "Qamar"
      },
      {
        "surname": "Younas",
        "given_name": "Irfan"
      }
    ]
  },
  {
    "title": "A solution method for the shared resource-constrained multi-shortest path problem",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115193",
    "abstract": "We tackle the problem of finding, for each network within a collection, the shortest path between two given nodes, while not exceeding the limits of a set of shared resources. We present an integer programming (IP) formulation of this problem and propose a parallelizable matheuristic consisting of three phases: (1) generation of feasible solutions, (2) combination of solutions, and (3) solution improvement. We show that the shortest paths found with our procedure correspond to the solution of some type of scheduling problems such as the Air Traffic Flow Management (ATFM) problem. Our computational results include finding optimal solutions to small and medium-size ATFM instances by applying Gurobi to the IP formulation. We use those solutions to assess the quality of the output produced by our proposed matheuristic. For the largest instances, which correspond to actual flight plans in ATFM, exact methods fail and we assess the quality of our solutions by means of Lagrangian bounds. Computational results suggest that the proposed procedure is an effective approach to the family of shortest path problems that we discuss here.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100628X",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Graph",
      "Integer programming",
      "K shortest path routing",
      "Longest path problem",
      "Mathematical optimization",
      "Mathematics",
      "Parallelizable manifold",
      "Path (computing)",
      "Programming language",
      "Scheduling (production processes)",
      "Set (abstract data type)",
      "Shortest path problem",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "García-Heredia",
        "given_name": "David"
      },
      {
        "surname": "Molina",
        "given_name": "Elisenda"
      },
      {
        "surname": "Laguna",
        "given_name": "Manuel"
      },
      {
        "surname": "Alonso-Ayuso",
        "given_name": "Antonio"
      }
    ]
  },
  {
    "title": "Evaluation of human resource information systems using grey ordinal pairwise comparison MCDM methods",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115151",
    "abstract": "This paper evaluates the human resource information systems provided by different vendors using two new hybrid multicriteria decision-making methods that require ordinal data as inputs. First, the grey-point-allocation full-consistency (Grey-PA-FUCOM) weighting method is proposed. The Grey-PA-FUCOM combines the simple point-allocation method widely used by human resource managers and the advanced FUCOM method widely accepted by scholars in grey system theory. Second, the grey-regime method, an extension of the classical regime method based on grey system theory, is used to account for the uncertainty in the evaluation. Next, the Grey-PA-FUCOM weights are applied in conjunction with the grey-regime scheme to evaluate the five vendors. Finally, to validate the results of this study, grey relational analysis with grey numbers, the grey weighted sum model, and a technique for order performance based on the similarity to the ideal solution with grey values are used.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005923",
    "keywords": [
      "Analytic hierarchy process",
      "Artificial intelligence",
      "Computer science",
      "Consistency (knowledge bases)",
      "Data mining",
      "Geometry",
      "Grey relational analysis",
      "Mathematics",
      "Medicine",
      "Multiple-criteria decision analysis",
      "Operations research",
      "Pairwise comparison",
      "Point (geometry)",
      "Radiology",
      "Statistics",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Esangbedo",
        "given_name": "Moses Olabhele"
      },
      {
        "surname": "Bai",
        "given_name": "Sijun"
      },
      {
        "surname": "Mirjalili",
        "given_name": "Seyedali"
      },
      {
        "surname": "Wang",
        "given_name": "Zonghan"
      }
    ]
  },
  {
    "title": "Orthogonal variance decomposition based feature selection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115191",
    "abstract": "Existing feature selection methods fail to properly account for interactions between features. In this paper, we attempt to remedy this issue by using orthogonal variance decomposition to evaluate features. The orthogonality of the decomposition allows us to directly calculate the total contribution of each feature to the output variance. As a result, we obtain an efficient and technically sound feature selection algorithm which takes into account feature interactions. The proposed algorithm has low computational complexity compared to other methods used in the literature. Numerical experiments demonstrate that our method accurately identifies relevant features and improves the accuracy of numerical models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006266",
    "keywords": [],
    "authors": [
      {
        "surname": "Kamalov",
        "given_name": "Firuz"
      }
    ]
  },
  {
    "title": "DeepSmoke: Deep learning model for smoke detection and segmentation in outdoor environments",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115125",
    "abstract": "Fire disaster throughout the globe causes social, environmental, and economical damage, making its early detection and instant reporting essential for saving human lives and properties. Smoke detection plays a key role in early fire detection but majority of the existing methods are limited to either indoor or outdoor surveillance environments, with poor performance for hazy scenarios. In this paper, we present a Convolutional Neural Network (CNN)-based smoke detection and segmentation framework for both clear and hazy environments. Unlike existing methods, we employ an efficient CNN architecture, termed EfficientNet, for smoke detection with better accuracy. We also segment the smoke regions using DeepLabv3+, which is supported by effective encoders and decoders along with a pixel-wise classifier for optimum localization. Our smoke detection results evince a noticeable gain up to 3% in accuracy and a decrease of 0.46% in False Alarm Rate (FAR), while segmentation reports a significant increase of 2% and 1% in global accuracy and mean Intersection over Union (IoU) scores, respectively. This makes our method a best fit for smoke detection and segmentation in real-world surveillance settings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005662",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "False alarm",
      "Fire detection",
      "Image segmentation",
      "Machine learning",
      "Meteorology",
      "Object detection",
      "Pattern recognition (psychology)",
      "Physics",
      "Segmentation",
      "Smoke",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Khan",
        "given_name": "Salman"
      },
      {
        "surname": "Muhammad",
        "given_name": "Khan"
      },
      {
        "surname": "Hussain",
        "given_name": "Tanveer"
      },
      {
        "surname": "Ser",
        "given_name": "Javier Del"
      },
      {
        "surname": "Cuzzolin",
        "given_name": "Fabio"
      },
      {
        "surname": "Bhattacharyya",
        "given_name": "Siddhartha"
      },
      {
        "surname": "Akhtar",
        "given_name": "Zahid"
      },
      {
        "surname": "de Albuquerque",
        "given_name": "Victor Hugo C."
      }
    ]
  },
  {
    "title": "Effective multinational trade forecasting using LSTM recurrent neural network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115199",
    "abstract": "Changes in foreign trade (export and import) constitute a crucial topic in international economics, international business management, and economic development. Numerous academics and industry leaders have sought effective means of forecasting foreign trade. However, with the uncertain nature of trade trends, obtaining accurate forecasts is a challenge. To analyze ten countries’ trade data, this study developed an effective foreign trade forecasting method that relies on a neural network with long short-term memory (LSTM); the results validated the effectiveness of the proposed method. This study is based on the economic theory that the two-way causal relationships present in trade data can improve trade forecasting. A multivariate LSTM-based method is proposed and exploited to extract temporal changes from trade data and provide effective trade forecasting. A comparison was conducted to understand the performance of the proposed method against time-series and economic structural models. The empirical results indicate that the method can appropriately model temporal information regarding uncertainty trends in foreign trade data. The method achieved almost perfect forecasting performance for data previously difficult to predict; in most cases, it had smaller values of root mean square error (RMSE) and mean absolute percentage error (MAPE) than did time-series models and economic structural models. On the export forecast, RMSE improved by 17.048% and MAPE by 1.463%, and for imports, RMSE improved by 40.939% and MAPE by 1.806%. This paper demonstrates the feasibility of the theoretical synthesis and provides a theoretical basis for interdisciplinary research in foreign trade forecasting.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006333",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Business",
      "Computer science",
      "Data mining",
      "Finance",
      "Machine learning",
      "Multinational corporation",
      "Recurrent neural network"
    ],
    "authors": [
      {
        "surname": "Shen",
        "given_name": "Mei-Li"
      },
      {
        "surname": "Lee",
        "given_name": "Cheng-Feng"
      },
      {
        "surname": "Liu",
        "given_name": "Hsiou-Hsiang"
      },
      {
        "surname": "Chang",
        "given_name": "Po-Yin"
      },
      {
        "surname": "Yang",
        "given_name": "Cheng-Hong"
      }
    ]
  },
  {
    "title": "Negative emotions detection on online mental-health related patients texts using the deep learning with MHA-BCNN model",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115265",
    "abstract": "Mining the emotions in the text related to mental health-care oriented is a challenging aspect, especially dealing with a long-text sequence of data. The extraction of emotions depends upon the various psychological depression factors like negative and ambiguity. Identifying these factors is the most perplexing task for every psychiatrist to treat their patients. Our study includes the deep learning (DL) models with global vector representations (GloVe) embeddings to capture the text sequence of data. We proposed a model multi-head attention with bidirectional long short-term memory and convolutional neural network (MHA-BCNN) is a pre-eminent mechanism that outperforms better than past research works for capturing the negative text-based emotions. In this paper, by using DL extracted the various negative mental-health emotions like addiction, anxiety, depression, insomnia, stress, and obsessive cleaning disorder (OCD). By using the GloVe embeddings and handled the ambiguity factors like multiple emotion words in a certain sequence. As we proposed a vigorous appliance in our research to capture and hoard the long-term dependencies. We extracted the questions related to mental health issues were posted by the patients in an online mental healthcare-oriented platform. We efficaciously handled both negative and ambiguity factors at the document level. Our suggested exemplary MHA-BCNN surmounts various aspects from preceding research works and ensued preeminent performance. Experimental results show that our proposed framework MHA-BCNN outperformed than the erstwhile research works.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006977",
    "keywords": [
      "Ambiguity",
      "Anxiety",
      "Artificial intelligence",
      "Biology",
      "Cognitive psychology",
      "Computer science",
      "Convolutional neural network",
      "Genetics",
      "Mental health",
      "Popularity",
      "Programming language",
      "Psychiatry",
      "Psychology",
      "Sequence (biology)",
      "Social psychology"
    ],
    "authors": [
      {
        "surname": "Dheeraj",
        "given_name": "Kodati"
      },
      {
        "surname": "Ramakrishnudu",
        "given_name": "Tene"
      }
    ]
  },
  {
    "title": "Scalable object detection pipeline for traffic cameras: Application to Tfl JamCams",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115154",
    "abstract": "With CCTV systems being installed in the transport infrastructures of many cities, there is an abundance of data to be extracted from the footage. This paper explores the application of the YOLOv3 object detection algorithm, trained on the COCO dataset, to the Transport for London’s (TfL) JamCam feed. The result, open-sourced and publicly available, is a series of easy to deploy Docker pipelines to create, store and serve (through a REST API) data on identified objects on that feed. The pipelines can be deployed to any Linux machine with an NVIDIA GPU to support accelerated computation. We studied how different confidence thresholds affect detections of relevant objects (cars, trucks and pedestrians) in London JamCam scenes. By running the system continuously for three weeks, we built a dataset of more than 2200 detection datapoints for each camera (~6 datapoints an hour). We further visualized the detections on an animated geospatial map, showcasing their effectiveness in identifying traffic patterns typical of an urban city like London, portraying the variation on different object population levels throughout the day.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005959",
    "keywords": [
      "Artificial intelligence",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Database",
      "Demography",
      "Engineering",
      "Environmental engineering",
      "Geography",
      "Geospatial analysis",
      "Object (grammar)",
      "Object detection",
      "Operating system",
      "Physics",
      "Pipeline (software)",
      "Pipeline transport",
      "Population",
      "Scalability",
      "Segmentation",
      "Sociology",
      "Thermodynamics",
      "Truck"
    ],
    "authors": [
      {
        "surname": "Gan",
        "given_name": "Huan Min"
      },
      {
        "surname": "Fernando",
        "given_name": "Senaka"
      },
      {
        "surname": "Molina-Solana",
        "given_name": "Miguel"
      }
    ]
  },
  {
    "title": "An empirical study of data intrinsic characteristics that make learning from imbalanced data difficult",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115297",
    "abstract": "Learning from data stemming from real-world problems is inherently challenging and difficult due to the numerous intrinsic characteristics present in datasets. The problem of class imbalance is known to significantly impair classification performance and has attracted increasing attention from researchers. On the other hand, some studies suggest that the detrimental effects of class imbalance occur only when the dataset encompasses other intrinsic characteristics such as small disjuncts, class overlapping, noise or data rarity. However, the literature is often ambiguous in terms of understanding and distinguishing the influence of these characteristics on the behaviour of standard classification algorithms. This paper provides a contemporary empirical study of the behaviour and performance of five well-known classifiers on a large number of imbalanced datasets exhibiting numerous combinations of the stated characteristics. The aim of the study is to identify and rank difficulty factors when learning from imbalanced data, depending on the type of classification algorithm used. In general, the obtained results suggest that if classifiers conceptually have no problem with class separation into sub-concepts, noise is the characteristic that most impairs their performance, closely followed by class overlapping and class imbalance. To alleviate these problems, oversampling and undersampling procedures were tested and directions are given for selecting appropriate techniques when dealing with the problem of class imbalance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007272",
    "keywords": [
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Class (philosophy)",
      "Combinatorics",
      "Computer network",
      "Computer science",
      "Data mining",
      "Empirical research",
      "Image (mathematics)",
      "Machine learning",
      "Mathematics",
      "Noise (video)",
      "Oversampling",
      "Rank (graph theory)",
      "Statistics",
      "Undersampling"
    ],
    "authors": [
      {
        "surname": "Dudjak",
        "given_name": "Mario"
      },
      {
        "surname": "Martinović",
        "given_name": "Goran"
      }
    ]
  },
  {
    "title": "ELM based two-handed dynamic Turkish Sign Language (TSL) word recognition",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115213",
    "abstract": "Hearing impaired individuals can easily overcome the barriers in communicating with other members of the society via computer technology. In this study, the recognition of dynamic words in Turkish Sign Language (TSL) with two hands was studied using the Leap Motion Controller (LMC) device. 50 dynamic words were determined considering the similarities and differences among themselves, and a dataset was created using 4 signers. For the system proposed in this paper, a comprehensive feature extraction process is executed. Applying feature selection algorithms and PCA, LDA and PCA+LDA dimension reduction methods to this dataset, new datasets with less dimension were obtained. For the first time, ELM architectures were operated as a classifier in a sign language recognition system. Recognition performance was tested with 5 different ELM networks and 2 classical classifiers and the results were compared. In addition, comparisons of classical and ELM based classifiers were presented. The 10-fold cross validation method was used to test the validity of the proposed system and the accuracy of the results obtained. Based on the results obtained by a comprehensive analysis, it was observed that the ML-KELM classifier maintains its performance rate and gives the highest performance rate. At the same time, it has been observed that ML-KELM classifier has a stable structure, which offers less user intervention.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006461",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Feature extraction",
      "Feature selection",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Sign language",
      "Speech recognition",
      "Turkish"
    ],
    "authors": [
      {
        "surname": "Katılmış",
        "given_name": "Zekeriya"
      },
      {
        "surname": "Karakuzu",
        "given_name": "Cihan"
      }
    ]
  },
  {
    "title": "Electoral forecasting using a novel temporal attenuation model: Predicting the US presidential elections",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115289",
    "abstract": "Electoral forecasting is an ongoing scientific challenge with high social impact, as current data-driven methods try to efficiently combine statistics with economic indices and machine learning. However, recent studies in network science pinpoint towards the importance of temporal characteristics in the diffusion of opinion. As such, we combine concepts of micro-scale opinion dynamics and temporal epidemics, and develop a novel macro-scale temporal attenuation (TA) model, which uses pre-election poll data to improve forecasting accuracy. Our hypothesis is that the timing of publicizing opinion polls plays a significant role in how opinion oscillates, especially right before elections. Thus, we define the momentum of opinion as a temporal function which bounces up when opinion is injected in a system of voters, and dampens down during states of relaxation. We validate TA on survey data from the US Presidential Elections between 1968 and 2016, and TA outperforms several statistical estimates, ARIMA models, as well as the best pollsters at their time, in 10 out of 13 presidential elections. We present two different implementations of the TA model, which accumulate an average forecasting error of 3.04 points over the 48-year period. Conversely, statistical methods accumulate 7.48 points error, and the best pollsters accumulate 3.64 points. Overall, TA offers increases of 23–37% in forecasting performance compared to the state of the art. We show that the effectiveness of TA does not drop when relatively few polls are available; moreover, with increasing availability of pre-election surveys, we believe that our TA model will become a reference alongside other modern election forecasting techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100720X",
    "keywords": [
      "Cartography",
      "Computer science",
      "Econometrics",
      "Economics",
      "Geography",
      "Law",
      "Mathematics",
      "Political science",
      "Politics",
      "Presidential election",
      "Presidential system",
      "Scale (ratio)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Topîrceanu",
        "given_name": "Alexandru"
      }
    ]
  },
  {
    "title": "A novel trilinear deep residual network with self-adaptive Dropout method for short-term load forecasting",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115272",
    "abstract": "Short-term load forecasting underlies the effective energy management. However, accurate short-term load forecasting is a big challenge owing to the significant uncertainty and volatility of load demand. Deep learning has successfully been applied for short-term load forecasting, but may encounter the problems such as vanishing gradient, exploding gradient, and overfitting of the neural networks, which affect the robustness of the forecasting results. Herein, a novel deep residual network with self-adaptive Dropout method is proposed for short-term load forecasting. First, a new trilinear deep residual network with three routes of stacked residual blocks is proposed to solve the problems of vanishing gradient and exploding gradient of neural networks. Second, a new self-adaptive Dropout method that considers the effects of both redundant features and interference terms of the neural networks is proposed for automatically setting the neuron drop ratio so as to improve the robustness of the model. Finally, an improved neural network ensemble method is applied to further enhance forecasting accuracy without additional training costs. The proposed model has been proven to achieve better forecasting accuracy and robustness than state-of-the-art baseline models on two public datasets. Therefore, the proposed model is beneficial in promoting the development of practical smart grids.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100703X",
    "keywords": [
      "Adaptive learning",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Deep learning",
      "Dropout (neural networks)",
      "Gene",
      "Machine learning",
      "Overfitting",
      "Physics",
      "Quantum mechanics",
      "Residual",
      "Robustness (evolution)",
      "Term (time)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Qian"
      },
      {
        "surname": "Zhang",
        "given_name": "Wenyu"
      },
      {
        "surname": "Zhu",
        "given_name": "Kun"
      },
      {
        "surname": "Zhou",
        "given_name": "Di"
      },
      {
        "surname": "Dai",
        "given_name": "Hua"
      },
      {
        "surname": "Wu",
        "given_name": "Quanquan"
      }
    ]
  },
  {
    "title": "A fuzzy weighted influence non-linear gauge system with application to advanced technology assessment at NASA",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115274",
    "abstract": "Evaluating the advanced technology projects at NASA is difficult due to multiple and intertwined evaluation criteria and uncertainties inherent in unproven new technologies. The conventional multi-criteria decision-making models often ignore the interdependencies and uncertainties in the evaluation process. We propose a fuzzy Weighted Influence Non-linear Gauge System (WINGS) to evaluate advanced technology projects at the Kennedy Space Center (KSC). The WINGS method uses ideographic causal maps to uncover the intertwined criteria and their causal relations in complex problems. Fuzzy set theory is an effective method that uses fuzzy logic to model uncertainties in ill-defined problems. The fuzzy WINGS method proposed in this study uncovers the interdependencies among the evaluation criteria by identifying the direction of the dependencies (influences) and their intensities, along with the strengths of the evaluation criteria. Fuzzy judgments are used to cope with uncertainties in untested new technologies. The conventional WINGS method does not consider a reference point in the solution space. For this reason, we introduce the concepts of ideal and nadir solutions, which are new to WINGS, to rank the alternative solutions according to their Euclidean distances from the ideal (or nadir) solutions. Finally, we present a case study to evaluate ten advanced technology projects based on six intertwined criteria and 38 sub-criteria at the KSC to demonstrate the applicability of the new fuzzy WINGS method proposed in this study.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007053",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Epistemology",
      "Fuzzy logic",
      "Ideal (ethics)",
      "Interdependence",
      "Law",
      "Mathematics",
      "Operations research",
      "Philosophy",
      "Political science"
    ],
    "authors": [
      {
        "surname": "Tavana",
        "given_name": "Madjid"
      },
      {
        "surname": "Mousavi",
        "given_name": "Hossein"
      },
      {
        "surname": "Khalili Nasr",
        "given_name": "Arash"
      },
      {
        "surname": "Mina",
        "given_name": "Hassan"
      }
    ]
  },
  {
    "title": "Multi-objective symbolic regression for physics-aware dynamic modeling",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115210",
    "abstract": "Virtually all dynamic system control methods benefit from the availability of an accurate mathematical model of the system. This includes also methods like reinforcement learning, which can be vastly sped up and made safer by using a dynamic system model. However, obtaining a sufficient amount of informative data for constructing dynamic models can be difficult. Consequently, standard data-driven model learning techniques using small data sets that do not cover all important properties of the system yield models that are partly incorrect, for instance, in terms of their steady-state characteristics or local behavior. However, often some knowledge about the desired physical properties of the model is available. Recently, several symbolic regression approaches making use of such knowledge to compensate for data insufficiency were proposed. Therefore, this knowledge should be incorporated into the model learning process to compensate for data insufficiency. In this paper, we consider a multi-objective symbolic regression method that optimizes models with respect to their training error and the measure of how well they comply with the desired physical properties. We propose an extension to the existing algorithm that helps generate a diverse set of high-quality models. Further, we propose a method for selecting a single final model out of the pool of candidate output models. We experimentally demonstrate the approach on three real systems: the TurtleBot 2 mobile robot, the Parrot Bebop 2 drone and the magnetic manipulation system. The results show that the proposed model-learning algorithm yields accurate models that are physically justified. The improvement in terms of the model’s compliance with prior knowledge over the models obtained when no prior knowledge was involved in the learning process is of several orders of magnitude.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006436",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Genetic programming",
      "Machine learning",
      "Mathematics",
      "Regression",
      "Regression analysis",
      "Statistics",
      "Symbolic regression"
    ],
    "authors": [
      {
        "surname": "Kubalík",
        "given_name": "Jiří"
      },
      {
        "surname": "Derner",
        "given_name": "Erik"
      },
      {
        "surname": "Babuška",
        "given_name": "Robert"
      }
    ]
  },
  {
    "title": "A hybrid algorithm on the vessel routing optimization for marine debris collection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115198",
    "abstract": "More and more marine debris not only damages the environment, but also endangers human health and marine life. Since the location of marine debris is not fixed, we first apply the GNOME software to predict the trajectory of marine debris and determine its location. Then, the logistics network is used to optimize the collection of marine debris. A mixed integer linear programming model for debris vessel routing is proposed. The objective function is to minimize the total cost, which includes the fuel cost consisting of is a piecewise linear function, labor cost, and fixed cost consisting of rent and insurance relative to the vessels used, taking into account the vessel weight capacity, fuel tank capacity, and time windows constraint at each debris location. We propose a hybrid algorithm of adaptive large-scale neighborhood search, combined with a wolf pack algorithm. This paper takes Yangtze River Estuary to the East China Sea as an example to verify the proposed model and algorithm. The results show that, compared with the worst-case, the most appropriate collection time can save up to 6.38% of the cost, which is about $75,590 per calendar year. The sensitivity analysis of debris weight and vessel capacity is also performed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006321",
    "keywords": [
      "Algorithm",
      "Computer network",
      "Computer science",
      "Debris",
      "Environmental science",
      "Genetic algorithm",
      "Geology",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Oceanography",
      "Routing (electronic design automation)"
    ],
    "authors": [
      {
        "surname": "Duan",
        "given_name": "Gang"
      },
      {
        "surname": "Fan",
        "given_name": "Tao"
      },
      {
        "surname": "Chen",
        "given_name": "Xiaohui"
      },
      {
        "surname": "Chen",
        "given_name": "Li"
      },
      {
        "surname": "Ma",
        "given_name": "Junfeng"
      }
    ]
  },
  {
    "title": "Optimizing S-box generation based on the Adaptive Agent Heroes and Cowards Algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115305",
    "abstract": "This paper introduces a new variant of a metaheuristic algorithm based on Agent Heroes and Cowards Algorithm (AHC), called Adaptive Agent Heroes and Cowards Algorithm (AAHC). The main feature of AAHC is the fact that the algorithm allows adaptive assignment of its population agents into cowards and heroes based on the exponential controlling functions. Furthermore, unlike its predecessor, AAHC also permits systematic manipulation of candidate solutions around the global best agent via the swap operator to boost its search intensification process. Meanwhile, to further enhance the diversification its solution, AHC also exploit the Tent map as the pseudo random generator replacement during its initial population initialization. Experimental results based on the generation of 8 × 8 substitution-box demonstrate that the proposed AAHC outperforms other competing metaheuristic algorithms in two main S-box criteria namely nonlinearity and strict avalanche criteria whilst maintaining commendable performances on bits independence criteria, differential approximation probability, linear approximation probability and transparency order.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100734X",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Demography",
      "Initialization",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Population",
      "Programming language",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Zamli",
        "given_name": "Kamal Z."
      }
    ]
  },
  {
    "title": "Soft sensor development based on kernel dynamic time warping and a relevant vector machine for unequal-length batch processes",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115223",
    "abstract": "The unequal-length problem in batch process data directly affects the performance of data-driven soft sensors. Meanwhile, the nonlinearity and high dimensionality of batch process data make the unequal-length problem more serious, and the development of effective soft sensors for unequal-length batch processes has become a challenge. To fully address this challenge, an effective soft sensor based on kernel dynamic time warping and a relevant vector machine is proposed in this paper. The proposed soft sensor consists of trajectory synchronization and online prediction modeling. First, combining the kernel trick, we design a kernel DTW (KerDTW) algorithm to effectively solve the synchronization of unequal-length trajectories with high dimensionality and strong nonlinearity characteristics. Meanwhile, a novel synchronization performance combination index (SPCI) is proposed to realize adaptive selection of the optimal parameter of the KerDTW algorithm. Then, based on the synchronized batch trajectories using the KerDTW algorithm, an online prediction model is established using an RVM to achieve online quality prediction of nonlinear process data. The effectiveness of the proposed soft sensor is illustrated through a penicillin fermentation process.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006552",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Astronomy",
      "Batch processing",
      "Channel (broadcasting)",
      "Combinatorics",
      "Computer network",
      "Computer science",
      "Curse of dimensionality",
      "Dynamic time warping",
      "Kernel (algebra)",
      "Mathematics",
      "Nonlinear system",
      "Operating system",
      "Physics",
      "Process (computing)",
      "Programming language",
      "Quantum mechanics",
      "Soft sensor",
      "Support vector machine",
      "Synchronization (alternating current)",
      "Trajectory"
    ],
    "authors": [
      {
        "surname": "Qiu",
        "given_name": "Kepeng"
      },
      {
        "surname": "Wang",
        "given_name": "Jianlin"
      },
      {
        "surname": "Wang",
        "given_name": "Rutong"
      },
      {
        "surname": "Guo",
        "given_name": "Yongqi"
      },
      {
        "surname": "Zhao",
        "given_name": "Liqiang"
      }
    ]
  },
  {
    "title": "Enhancing manufacturing intelligence through an unsupervised data-driven methodology for cyclic industrial processes",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115269",
    "abstract": "Recent trends in intelligent manufacturing are transforming shop floor environments into digital factories, thanks to a pervasive integration of information and communication technologies in production lines. Industrial processes become the source of high-volume heterogeneous data, paving the way to create manufacturing intelligence by means of machine learning and data-driven methodologies. In such settings, predictive diagnostics play a crucial role, as they promise to predict future critical conditions in the production process. Unfortunately, the diffusion of data-driven predictive maintenance methodologies is limited by (i) the absence of timely ground-truth knowledge (i.e., class labels), required in the learning phase of data-driven supervised approaches, and (ii) the limited availability of data-mining expertise among application-domain experts, required to harness the power of machine learning techniques. Innovative data-driven services are needed to support domain experts in (i) applying powerful self-learning intelligent techniques with limited technical expertise and (ii) easily understanding results and choices operated by such intelligent techniques, to increase trust by means of transparency. To this aim, this paper presents UDaMP, an integrated platform to support manufacturing intelligence by providing a transparent, self-tuning, unsupervised discovery and assisted data labelling service for predictive maintenance, specifically targeted at cyclic industrial processes. UDaMP includes (i) production-cycle-aware feature engineering, (ii) unsupervised discovery of production-cycle categories, (iii) self-tuning of the optimal number of categories, (iv) human-readable characterisation of production-cycle categories, and (v) assisted data labelling for domain experts. Scalable clustering algorithms automatically discover groups of production cycles sharing common time-independent properties. A self-tuning strategy is integrated to automatically configure the specific input parameter and select the best approach for the data under analysis. Each cluster is then locally characterised through the data distribution of the top 10 most relevant features to support domain experts in uncovering its meaning. Experimental evaluation of UDaMP has been performed on real-world data collected in two different industrial settings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421007004",
    "keywords": [
      "Artificial intelligence",
      "Big data",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Data science",
      "Database",
      "Domain (mathematical analysis)",
      "Engineering",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Predictive maintenance",
      "Reliability engineering",
      "Scalability",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Cerquitelli",
        "given_name": "Tania"
      },
      {
        "surname": "Ventura",
        "given_name": "Francesco"
      },
      {
        "surname": "Apiletti",
        "given_name": "Daniele"
      },
      {
        "surname": "Baralis",
        "given_name": "Elena"
      },
      {
        "surname": "Macii",
        "given_name": "Enrico"
      },
      {
        "surname": "Poncino",
        "given_name": "Massimo"
      }
    ]
  },
  {
    "title": "An improved GRASP method for the multiple row equal facility layout problem",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115184",
    "abstract": "As it is well documented in the literature, an effective facility layout design of a company significantly increases throughput, overall productivity, and efficiency. Symmetrically, a poor facility layout results in increased work-in process and manufacturing lead time. In this paper we focus on the Multiple Row Equal Facility Layout Problem (MREFLP) which consists in locating a given set of facilities in a layout where a maximum number of rows is fixed. We propose a Greedy Randomized Adaptive Search Procedure (GRASP), with an improved local search that relies on an efficient calculation of the objective function, and a probabilistic strategy to select those solutions that will be improved. We conduct a through preliminary experimentation to investigate the influence of the proposed strategies and to tune the corresponding search parameters. Finally, we compare our best variant with current state-of-the-art algorithms over a set of 552 diverse instances. Experimental results show that the proposed GRASP finds better results spending much less execution time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006205",
    "keywords": [
      "Advertising",
      "Algorithm",
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Database",
      "Focus (optics)",
      "GRASP",
      "Geometry",
      "Greedy algorithm",
      "Greedy randomized adaptive search procedure",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Optics",
      "Page layout",
      "Physics",
      "Probabilistic logic",
      "Process (computing)",
      "Programming language",
      "Reduction (mathematics)",
      "Row",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Uribe",
        "given_name": "Nicolás R."
      },
      {
        "surname": "Herrán",
        "given_name": "Alberto"
      },
      {
        "surname": "Colmenar",
        "given_name": "J. Manuel"
      },
      {
        "surname": "Duarte",
        "given_name": "Abraham"
      }
    ]
  },
  {
    "title": "Deep multi-scale separable convolutional network with triple attention mechanism: A novel multi-task domain adaptation method for intelligent fault diagnosis",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115087",
    "abstract": "Rotating components, as the core functional part of rotating machinery, their performance directly determines the stability, reliability, and safety of the equipment operation. Effective intelligent fault identification techniques are being developed as a promising tool for perceiving the state of rotating elements. However, the domain shift phenomenon caused by internal and external interference inevitably exists in practical application scenarios, which significantly deteriorates the performances of the intelligent diagnosis model. Besides, most of the existing intelligent fault diagnosis models are constructed mainly for the single task attribute, that is, the established model can only meet the requirements of a single task, such as the identification of different fault severities or the monitoring of different fault locations. To overcome these challenges, a novel multi-task domain adaptation framework, called deep multi-scale separable convolutional network with triple attention mechanism (MSSCN-TAM), is established in this paper. First, the condition monitoring data preprocessed based on Fast Fourier Transform (FFT) is fed into the improved separable convolution (ISC) module, in which depth-attention and point-attention are introduced to make it self-adjusting. Then, combined with the scale-attention mechanism, which determines the contribution of each branch, the output nodes of each ISC module are connected across scales and treated as the common input of the subsequent two task-specific discriminators. Finally, the weighted Multi-Kernel Maximum Mean Discrepancies (MK-MMD) is adopted to the proposed MSSCN-TAM model to align the distribution and extract domain-invariant features. A total of twenty transfer scenarios based on three rotating component datasets are employed for performance validation of the proposed MSSCN-TAM model, and the multi-task cross-domain transfer diagnosis results show that it has superior transferability and stability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005285",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Botany",
      "Computer science",
      "Convolution (computer science)",
      "Data mining",
      "Engineering",
      "Fast Fourier transform",
      "Fault (geology)",
      "Geology",
      "Identification (biology)",
      "Pattern recognition (psychology)",
      "Seismology",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Bo"
      },
      {
        "surname": "Zhang",
        "given_name": "Xianmin"
      },
      {
        "surname": "Zhan",
        "given_name": "Zhenhui"
      },
      {
        "surname": "Wu",
        "given_name": "Qiqiang"
      }
    ]
  },
  {
    "title": "A joint array resource allocation and transmit beampattern design approach for multiple targets tracking",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115188",
    "abstract": "In this paper, we consider a joint array resource allocation and transmit beamforming approach to track multiple targets using phased array radar. Firstly, array allocation is accomplished by solving a constrained optimization problem formulated to minimize total tracking Cramer-Rao Lower Bound with antenna number as a constraint. A procedure to solve this problem is developed by checking Karush-Kuhn-Tucker conditions. Based on the allocated array resource, we develop a novel transmit beampattern design with null steering capability. The emitted signal model and objective functions of optimization are presented with a given power budget and null steering constraint. The resulted problem is revealed to be hard to tackle, therefore, we propose a cyclic algorithm scheme for solving it. The scheme separates the optimization into two problems, where each problem is transformed into simpler convex problems. Lastly, the projected gradient decent method with a penalty function is applied for the optimal design of beamforming with null steering. Simulation results show that the proposed array allocation solution provides better tracking accuracy, and the proposed beampattern design method outperforms conventional optimization methods in the optimal beamforming by achieving much deeper null steering and more accurate peak power in the given directions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006230",
    "keywords": [
      "Algorithm",
      "Antenna (radio)",
      "Antenna array",
      "Artificial intelligence",
      "Beamforming",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Constraint (computer-aided design)",
      "Control (management)",
      "Control theory (sociology)",
      "Convex optimization",
      "Database",
      "Geometry",
      "Mathematical optimization",
      "Mathematics",
      "Null (SQL)",
      "Optimization problem",
      "Phased array",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Regular polygon",
      "Resource allocation",
      "Telecommunications",
      "Transmitter",
      "Transmitter power output"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zhenkai"
      },
      {
        "surname": "Esmaeili Najafabadi",
        "given_name": "Hamid"
      },
      {
        "surname": "Leung",
        "given_name": "Henry"
      },
      {
        "surname": "Jin",
        "given_name": "Biao"
      }
    ]
  },
  {
    "title": "A hybrid approach to classifying Wikipedia article quality flaws with feature fusion framework",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115089",
    "abstract": "Article quality has always been a major concern for Wikipedia. To improve article quality, it is critical to first identify defects. Thus, flaw classification has attracted considerable attention. To achieve this, several machine-learning-based approaches are available, including deep learning models based on either manually constructed or autoextracted features. However, adopting only features of either single type may not ensure a comprehensive description of articles. To improve flaw classification, we propose a feature fusion framework combining both handcrafted and autoextracted features. In this research, we first use a rule-based method from a previously proposed framework to extract handcrafted features. Additionally, we obtain autoextracted features using Bidirectional Encoder Representations from Transformers (BERT) and various deep learning models, including bidirectional long short-term memory (Bi LSTM), bidirectional gated recurrent unit (Bi GRU), bidirectional recurrent neural network (Bi RNN), and multihead self-attention models. Finally, the handcrafted features are standardized and concatenated with the autoextracted features. Then, the concatenated features are fed into a feedforward neural network for classification. A detailed comparison of different classifiers is conducted. We compare 12 different classifiers in terms of training performance, classification performance, and model training time. The experiments show that the proposed feature fusion framework can notably improve the effectiveness of quality flaw classification for Wikipedia articles. In particular, a Bi GRU model based on the proposed framework achieves excellent classification accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005303",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Encoder",
      "Feature (linguistics)",
      "Linguistics",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Recurrent neural network",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Ping"
      },
      {
        "surname": "Li",
        "given_name": "Muyan"
      },
      {
        "surname": "Li",
        "given_name": "Xiaodan"
      },
      {
        "surname": "Zhou",
        "given_name": "Heshen"
      },
      {
        "surname": "Hou",
        "given_name": "Jingrui"
      }
    ]
  },
  {
    "title": "Hyperspectral multi-level image thresholding using qutrit genetic algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115107",
    "abstract": "Hyperspectral images contain rich spectral information about the captured area. Exploiting the vast and redundant information, makes segmentation a difficult task. In this paper, a Qutrit Genetic Algorithm is proposed which exploits qutrit based chromosomes for optimization. Ternary quantum logic based selection and crossover operators are introduced in this paper. A new qutrit based mutation operator is also introduced to bring diversity in the off-springs. In the preprocessing stage two methods, called Interactive Information method and Band Selection Convolutional Neural Network are used for band selection. The modified Otsu Criterion and Masi entropy are employed as the fitness functions to obtain optimum thresholds. A quantum based disaster operation is applied to prevent the quantum population from getting stuck in local optima. The proposed algorithm is applied on the Salinas Dataset, the Pavia Centre Dataset and the Indian Pines dataset for experimental purpose. It is compared with classical Genetic Algorithm, Particle Swarm Optimization, Ant Colony Optimization, Gray Wolf Optimizer, Harris Hawk Optimization, Qubit Genetic Algorithm and Qubit Particle Swarm Optimization to establish its effectiveness. The peak signal-to-noise ratio and S ø rensen-Dice Similarity Index are applied to the thresholded images to determine the segmentation accuracy. The segmented images obtained from the proposed method are also compared with those obtained by two supervised methods, viz., U-Net and Hybrid Spectral Convolutional Neural Network. In addition to this, a statistical superiority test, called the one-way ANOVA test, is also conducted to judge the efficacy of the proposed algorithm. Finally, the proposed algorithm is also tested on various real life images to establish its diversity and efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005480",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Crossover",
      "Demography",
      "Genetic algorithm",
      "Hyperspectral imaging",
      "Image (mathematics)",
      "Machine learning",
      "Meta-optimization",
      "Particle swarm optimization",
      "Pattern recognition (psychology)",
      "Physics",
      "Population",
      "Quantum",
      "Quantum mechanics",
      "Qubit",
      "Qutrit",
      "Sociology",
      "Thresholding"
    ],
    "authors": [
      {
        "surname": "Dutta",
        "given_name": "Tulika"
      },
      {
        "surname": "Dey",
        "given_name": "Sandip"
      },
      {
        "surname": "Bhattacharyya",
        "given_name": "Siddhartha"
      },
      {
        "surname": "Mukhopadhyay",
        "given_name": "Somnath"
      },
      {
        "surname": "Chakrabarti",
        "given_name": "Prasun"
      }
    ]
  },
  {
    "title": "Boosting quantum rotation gate embedded slime mould algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115082",
    "abstract": "The slime mould algorithm is an interesting swarm-based algorithm proposed in 2020 based on this entity's trajectory finding abilities in nature. It simulates slime mould movement, foraging, and other behaviors to find the problem's optimal solution. Because of the complexity of the slime mould's trajectory, the SMA has strong randomness and makes the generated population diverse. However, in the late iteration of the algorithm, as the complexity of the problem to be dealt with increases, it tends to drop into the local best, and the convergence rate slows down. Therefore, in this study, an improved SMA, named WQSMA, is proposed to remedy the above imperfections. Specifically, the two strategies of quantum rotation gate and an operation from water cycle are used for the first time to improve the robustness of the original SMA. The purpose of adding both mechanisms is to keep the algorithm in equilibrium among exploration and exploitation inclinations. While expanding the search space of individual population, it also makes a more detailed exploration of the local area. The quantum rotation gate, which rotates by its small angle, can adequately exploit the algorithm and search in the local scope enough. Simultaneously, the water cycle mechanism can help the algorithm search thoroughly in the space to find the optimal solution. The improved algorithm was compared with 14 classical meta-heuristics and 14 advanced algorithms on the test set IEEE CEC 2014, and the results were obtained, with WQSMA ranking first in both comparisons. Also, to further illustrate the role of WQSMA in practical application, three engineering problems are used for verification. Experimental results show that WQSMA also performs well in solving such practical problems. A website at https://aliasgharheidari.com will support this research.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005236",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Demography",
      "Exploit",
      "Gene",
      "Heuristics",
      "Mathematical optimization",
      "Mathematics",
      "Particle swarm optimization",
      "Population",
      "Randomness",
      "Robustness (evolution)",
      "Rotation (mathematics)",
      "Sociology",
      "Statistics",
      "Swarm intelligence"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Caiyang"
      },
      {
        "surname": "Heidari",
        "given_name": "Ali Asghar"
      },
      {
        "surname": "Xue",
        "given_name": "Xiao"
      },
      {
        "surname": "Zhang",
        "given_name": "Lejun"
      },
      {
        "surname": "Chen",
        "given_name": "Huiling"
      },
      {
        "surname": "Chen",
        "given_name": "Weibin"
      }
    ]
  },
  {
    "title": "An experimental study on diversification in portfolio optimization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115203",
    "abstract": "New diversification strategies, along with other naive strategies as 1/N portfolios, have been proposed in the literature as a method for overcoming concentration limitations of the mean–variance model. However, it is not clear whether these strategies outperform the classical mean–variance model in all scenarios. Motivated by these points, this manuscript contributes an experimental study in which 11 diversification and mean–variance strategies are compiled and compared with a complete repository of 10 portfolio time series problems with three different estimation windows (composing a total of 30 datasets) and then evaluated using four performance metrics. Additionally, a novel purely data-driven method for determining the optimal value of the hyper-parameter associated with each approach is also proposed. Unlike results previously found in the literature, the empirical results obtained in this study show that equally weighed models obtain the worst ranking in all evaluation metrics except for the stability index, which is hypothetically due to the hyper-parameter optimization raising the transaction cost debate.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006369",
    "keywords": [
      "Accounting",
      "Business",
      "Computer science",
      "Data mining",
      "Diversification (marketing strategy)",
      "Econometrics",
      "Economics",
      "Finance",
      "Machine learning",
      "Marketing",
      "Mathematical optimization",
      "Mathematics",
      "Portfolio",
      "Portfolio optimization",
      "Ranking (information retrieval)",
      "Stability (learning theory)",
      "Transaction cost",
      "Variance (accounting)"
    ],
    "authors": [
      {
        "surname": "Martínez-Nieto",
        "given_name": "Luisa"
      },
      {
        "surname": "Fernández-Navarro",
        "given_name": "Francisco"
      },
      {
        "surname": "Carbonero-Ruz",
        "given_name": "Mariano"
      },
      {
        "surname": "Montero-Romero",
        "given_name": "Teresa"
      }
    ]
  },
  {
    "title": "Knowledge discovery from noisy imbalanced and incomplete binary class data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115179",
    "abstract": "Class imbalance creates a considerable impact on the classification of instances using traditional classifiers. Class imbalance, along with other difficulties, creates a significant impact on recognizing instances of minority class. Researchers work in various directions to mitigate class imbalance effect along with noise as well as missing values in datasets. However, combined studies of noisy class imbalance along with incomplete datasets have not been performed yet. This article contains a detailed analysis of 84 different machine learning models to deal with noisy binary class imbalanced and incomplete data using AUC, G-Mean, and F1-score as performance metrics. This article contains a detailed experiment considering missing value imputation and oversampling techniques. The article contains three comparisons: first missing value imputation techniques in incomplete and binary class imbalanced data, second, resampling techniques in noisy binary class imbalanced data, and third, combined techniques in noisy binary class imbalanced and incomplete data. We conclude that MICE and KNN techniques perform well with an increase in the imbalanced dataset's missing value from the first comparison. In second comparison, the SMOTE-ENN technique performs better than state-of-art in noisy binary class imbalanced datasets, and in the third comparison, we conclude that MICE with SMOTE-ENN technique perform well compared to the rest of the techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006175",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Binary classification",
      "Binary data",
      "Binary number",
      "Class (philosophy)",
      "Computer network",
      "Computer science",
      "Data mining",
      "Imputation (statistics)",
      "Machine learning",
      "Mathematics",
      "Missing data",
      "Oversampling",
      "Pattern recognition (psychology)",
      "Resampling",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Puri",
        "given_name": "Arjun"
      },
      {
        "surname": "Kumar Gupta",
        "given_name": "Manoj"
      }
    ]
  },
  {
    "title": "A knowledge-driven digital nudging approach to recommender systems built on a modified Onicescu method",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115170",
    "abstract": "Product recommendations are generally understood as data-driven – however, we argue that knowledge-driven management decisions may also play a role, especially in the cold start problem, which has been tackled with various degrees of success through a number of approaches. We hereby advocate an approach that captures managerial priorities in the act of recommendation building – i.e., the proposal is to complement the traditional customer-centric view (affected by uncertainty) with a machine-readable business-centric view. For this purpose, the paper reports on an engineered method for the “digital nudging” of recommendations - it starts by capturing a manager's priorities with diagrammatic means, which are further exposed as a Knowledge Graph to a recommender built on a modified version of the Onicescu method taking into consideration a business “utility” concept to influence decision-making. The research follows the Design Science methodology, resulting in a “method” artifact that tackles the cold start with the help of a (by-design) recommendation nudging mechanism. In terms of method engineering, the proposal orchestrates its ingredients into a coherent method with the help of (a) Agile Modeling Method Engineering, to setup up a diagrammatic tool for prioritization rules, (b) the Resource Description Framework, to capture the diagrammatic rules in knowledge graph form, and (c) the Onicescu multi-criteria decision method with modifications based on Zipf's Law. The evaluation was based on surveys with potential stakeholders, for the different steps of the method. The implications are that the notion of “digital nudging” can take a knowledge-driven form, engineered as an artifact that bridges the decision-makers' priorities (captured by diagrammatic means) with the customer-facing output (recommendations), instead of relying solely on the accumulated history of transactional data. This interpretation of digital nudging may be extended towards other “digital choice environments” where contextual decisions are called to influence the computational output.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006084",
    "keywords": [
      "Agile software development",
      "Artifact (error)",
      "Artificial intelligence",
      "Computer science",
      "Data science",
      "Design science",
      "Design science research",
      "Diagrammatic reasoning",
      "Electrical engineering",
      "Engineering",
      "Graph",
      "Information system",
      "Knowledge management",
      "Programming language",
      "Software engineering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Sitar-Tăut",
        "given_name": "Dan-Andrei"
      },
      {
        "surname": "Mican",
        "given_name": "Daniel"
      },
      {
        "surname": "Buchmann",
        "given_name": "Robert Andrei"
      }
    ]
  },
  {
    "title": "Best-Worst method and Hamacher aggregation operations for intuitionistic 2-tuple linguistic sets",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115088",
    "abstract": "Sometimes in the sense of intuitionist 2-tuple linguistic (I2TL) sets, experts may not be able to decide the most suitable criterion weight vector for multicriteria decision making (MCDM). To avoid this situation, the decision-makers (DMs) can use the Best-Worst method (BWM), in which DMs choose the best (most significant) criterion and the worst (least significant) criterion and then provide two preference vectors by comparing criteria best to other (BO) and other to worst (OW). In the Nonlinear Best-Worst Method (NBWM) it is more complicated to find the unique solution of the model. Therefore, the main goal of this study is to propose two approaches to BWM, namely, Linear Best-Worst Method (LBWM) and Euclidean Best-Worst method (EBWM) to achieve the best criteria priority vector for Multi-Criteria Group Decision Making (MCGDM) problems in the context of I2TL information. In the computational process of MCDM problems, we have to aggregate I2TL elements into a global one. Consequently, under certain critical properties, we are creating some operational laws for I2TL elements based on Hamacher operations. Also, the intuitionistic 2-tuple linguistic Hamacher weighted average (I2TLHWA), and the intuitionistic 2-tuple linguistic Hamacher weighted geometric (I2TLHWG) operators are introduced with the assistance of Hamacher operations and I2TL elements. Subsequently, we analyze some of the I2TLHWA operator’s related properties and we propose MCGDM framework under I2TL information. Finally, we demonstrate the validity and efficiency of our method and operations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005297",
    "keywords": [
      "Algorithm",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Context (archaeology)",
      "Discrete mathematics",
      "Gene",
      "Group decision-making",
      "Law",
      "Lie algebra",
      "Mathematical optimization",
      "Mathematics",
      "Multiple-criteria decision analysis",
      "Operator (biology)",
      "Paleontology",
      "Political science",
      "Pure mathematics",
      "Repressor",
      "Transcription factor",
      "Tuple",
      "Weight"
    ],
    "authors": [
      {
        "surname": "Faizi",
        "given_name": "Shahzad"
      },
      {
        "surname": "Sałabun",
        "given_name": "Wojciech"
      },
      {
        "surname": "Nawaz",
        "given_name": "Shoaib"
      },
      {
        "surname": "Rehman",
        "given_name": "Atiq ur"
      },
      {
        "surname": "Wątróbski",
        "given_name": "Jarosław"
      }
    ]
  },
  {
    "title": "RUN beyond the metaphor: An efficient optimization algorithm based on Runge Kutta method",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115079",
    "abstract": "The optimization field suffers from the metaphor-based “pseudo-novel” or “fancy” optimizers. Most of these cliché methods mimic animals' searching trends and possess a small contribution to the optimization process itself. Most of these cliché methods suffer from the locally efficient performance, biased verification methods on easy problems, and high similarity between their components' interactions. This study attempts to go beyond the traps of metaphors and introduce a novel metaphor-free population-based optimization method based on the mathematical foundations and ideas of the Runge Kutta (RK) method widely well-known in mathematics. The proposed RUNge Kutta optimizer (RUN) was developed to deal with various types of optimization problems in the future. The RUN utilizes the logic of slope variations computed by the RK method as a promising and logical searching mechanism for global optimization. This search mechanism benefits from two active exploration and exploitation phases for exploring the promising regions in the feature space and constructive movement toward the global best solution. Furthermore, an enhanced solution quality (ESQ) mechanism is employed to avoid the local optimal solutions and increase convergence speed. The RUN algorithm's efficiency was evaluated by comparing with other metaheuristic algorithms in 50 mathematical test functions and four real-world engineering problems. The RUN provided very promising and competitive results, showing superior exploration and exploitation tendencies, fast convergence rate, and local optima avoidance. In optimizing the constrained engineering problems, the metaphor-free RUN demonstrated its suitable performance as well. The authors invite the community for extensive evaluations of this deep-rooted optimizer as a promising tool for real-world optimization. The source codes, supplementary materials, and guidance for the developed method will be publicly available at different hubs at http://imanahmadianfar.com and http://aliasgharheidari.com/RUN.html.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005200",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Linguistics",
      "Local optimum",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Metaphor",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Ahmadianfar",
        "given_name": "Iman"
      },
      {
        "surname": "Heidari",
        "given_name": "Ali Asghar"
      },
      {
        "surname": "Gandomi",
        "given_name": "Amir H."
      },
      {
        "surname": "Chu",
        "given_name": "Xuefeng"
      },
      {
        "surname": "Chen",
        "given_name": "Huiling"
      }
    ]
  },
  {
    "title": "COMPASS: Unsupervised and online clustering of complex human activities from smartphone sensors",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115124",
    "abstract": "Modern mobile devices are able to provide context-aware and personalized services to the users, by leveraging on their sensing capabilities to infer the activity and situation in which a person is currently involved. Current solutions for context-recognition rely on annotated data and experts’ knowledge to predict the user context. In addition, their prediction ability is strongly limited to the set of situations considered during the model training or definition. However, in a mobile environment, the user context continuously evolves, and it cannot be merely restricted to a set of predefined classes. To overcome these limitations, we propose COMPASS, a novel unsupervised and online clustering algorithm aimed at identifying the user context in mobile environments based on the stream of high-dimensional data generated by smartphone sensors. COMPASS can distinguish an arbitrary number of user’s contexts from the sensors’ data, without defining a priori the collection of expected situations. This key feature makes it a general-purpose solution to provide context-aware features to mobile devices, supporting a broad set of applications. Experimental results on 18 synthetic and 2 real-world datasets show that COMPASS correctly identifies the user context from the sensors’ data stream, and outperforms the state-of-the-art solutions in terms of both clusters configuration and purity. Eventually, we evaluate its performances in terms of execution time and the results show that COMPASS can process 1000 high-dimensional samples in less than 20 s, while the reference solutions require about 60 min to evaluate the entire dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005650",
    "keywords": [
      "Artificial intelligence",
      "Cartography",
      "Cluster analysis",
      "Compass",
      "Computer science",
      "Geography",
      "Machine learning"
    ],
    "authors": [
      {
        "surname": "Campana",
        "given_name": "Mattia Giovanni"
      },
      {
        "surname": "Delmastro",
        "given_name": "Franca"
      }
    ]
  },
  {
    "title": "Tag-Enhanced Dynamic Compositional Neural Network over arbitrary tree structure for sentence representation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115182",
    "abstract": "Learning the distributed representation of a sentence is a fundamental operation for a variety of natural language processing tasks, such as text classification, machine translation, and text semantic matching. Tree-structured dynamic compositional networks have achieved promising performance in sentence representation due to its ability in capturing the richness of compositionality. However, existing dynamic compositional networks are mostly based on binarized constituency trees which cannot represent the inherent structural information of sentences effectively. Moreover, syntactic tag information, which is demonstrated to be useful in sentence representation, has been rarely exploited in existing dynamic compositional models. In this paper, a novel LSTM structure, ARTree-LSTM, is proposed to handle general constituency trees in which each non-leaf node can have any number of child nodes. Based on ARTree-LSTM, a novel network model, Tag-Enhanced Dynamic Compositional Neural Network (TE-DCNN), is proposed for sentence representation learning, which contains two ARTree-LSTMs, i.e. tag-level ARTree-LSTM and word-level ARTree-LSTM. The tag-level ARTree-LSTM guides the word-level ARTree-LSTM in conducting dynamic composition. Extensive experiments demonstrate that the proposed TE-DCNN achieves state-of-the-art performance on text classification and text semantic matching tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006199",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Data structure",
      "Law",
      "Linguistics",
      "Matching (statistics)",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Philosophy",
      "Political science",
      "Politics",
      "Principle of compositionality",
      "Programming language",
      "Representation (politics)",
      "Sentence",
      "Statistics",
      "Tree (set theory)",
      "Tree structure",
      "Trie",
      "Variety (cybernetics)",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Chunlin"
      },
      {
        "surname": "Wang",
        "given_name": "Hui"
      },
      {
        "surname": "Wu",
        "given_name": "Shengli"
      },
      {
        "surname": "Lin",
        "given_name": "Zhiwei"
      }
    ]
  },
  {
    "title": "A new point-of-interest approach based on multi-itinerary recommendation engine",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115026",
    "abstract": "The significance of tourism in the globe today is enormous since it is a major source of income and jobs for a nation. Tourists are facing a range of difficulties as they select suitable tours, consisting of several itineraries in terms of their interests and distinct constraints. An itinerary consists of many Points of Interest (POIs) and a POI can further be splitted into several attractions which are named as POI within POI. For selecting the itinerary, the existing techniques use the characteristics of POIs. However, a POI consists of many attractions. Out of these, one dominating attraction’s type is considered as POI type. This ignores the other type of attraction’s present in that POI. It may cause improper selection of itineraries. Therefore, selection of itineraries by considering POI within POI is of great benefit. But, it is very challenging. For this task, we suggest an algorithm called PWP. It recommends multiple itineraries that are based on the interest of visitors, popularity of itineraries and the cost of itineraries. If a tourist wants to visit unknown areas, the PWP algorithm can be expanded further. We have taken the similar user’s features to advise multiple itineraries using the Flickr dataset. The findings show that the proposed PWP algorithm out-performs the baseline algorithms in terms of real-life matrices and heuristic based metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100467X",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Composite material",
      "Computer science",
      "Economics",
      "Geography",
      "Geometry",
      "Globe",
      "Heuristic",
      "Management",
      "Materials science",
      "Mathematics",
      "Medicine",
      "Ophthalmology",
      "Point (geometry)",
      "Point of interest",
      "Popularity",
      "Psychology",
      "Range (aeronautics)",
      "Selection (genetic algorithm)",
      "Social psychology",
      "Task (project management)",
      "Tourism",
      "Tourist attraction"
    ],
    "authors": [
      {
        "surname": "Sarkar",
        "given_name": "Joy Lal"
      },
      {
        "surname": "Majumder",
        "given_name": "Abhishek"
      }
    ]
  },
  {
    "title": "Discrepancy detection between actual user reviews and numeric ratings of Google App store using deep learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115111",
    "abstract": "Nowadays online reviews play a significant role in influencing the decision of consumers. Consumers show their experience and information about product quality in their reviews. Product Reviews from Amazon to Restaurant Reviews from Yelp are facing problems with fake reviews and fake numeric ratings. Online reviews typically consist of qualitative (text format) and quantitative (rating) formats. In the case of Google Play store fake numeric ratings can play a big role in the success of apps. People tend to believe that a high-star rating may be significantly attached with a good review. However, user star level rating information does not usually match with text format of review. Despite many efforts to resolve this issue, Apple App Store and Google Play Store are still facing this problem. This study proposes a novel Google App numeric reviews & ratings contradiction prediction framework using Deep Learning approaches. The framework consists of two phases. In the first phase, the polarity of reviews are predicted using sentiment analysis tool to build ground truth. In the second phase, star ratings are predicted from text format of reviews after training deep learning models on ground truth obtained in the first phase. Experimental results demonstrate that based on actual user reviews the proposed framework significantly predicts unbiased star rating of app.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005522",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Contradiction",
      "Data science",
      "Epistemology",
      "Geometry",
      "Ground truth",
      "Information retrieval",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Philosophy",
      "Product (mathematics)",
      "Quality (philosophy)",
      "Sentiment analysis",
      "Star (game theory)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Sadiq",
        "given_name": "Saima"
      },
      {
        "surname": "Umer",
        "given_name": "Muhammad"
      },
      {
        "surname": "Ullah",
        "given_name": "Saleem"
      },
      {
        "surname": "Mirjalili",
        "given_name": "Seyedali"
      },
      {
        "surname": "Rupapara",
        "given_name": "Vaibhav"
      },
      {
        "surname": "Nappi",
        "given_name": "Michele"
      }
    ]
  },
  {
    "title": "An efficient ECG arrhythmia classification method based on Manta ray foraging optimization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115131",
    "abstract": "The Electrocardiogram (ECG) arrhythmia classification has become an interesting research area for researchers and developers as it plays a vital role in early prevention and diagnosis of cardiovascular diseases. In ECG signal classification, the feature extraction and selection processes are critical steps. Thus, in this paper, different ECG signal descriptors based on one-dimensional local binary pattern (LBP), wavelet, higher-order statistical (HOS), and morphological information are introduced for feature extraction. For feature selection and classification processes, a new hybrid ECG arrhythmia classification approach called MRFO-SVM that combines a metaheuristic algorithm termed Manta ray foraging optimization (MRFO) with support vector machine (SVM) is proposed to automatically determine the relevance features of LBP, HOS, wavelet and magnitude values. In MRFO-SVM approach, the MRFO is utilized to optimize the parameters of SVM and to select the significant features subset that provides the best classification performance, meanwhile SVM is used for classification purposes. The proposed MRFO-SVM approach is trained on the MIT-BIH Arrhythmia database containing four abnormal and one normal heartbeats. The experimental results of ECG arrhythmia classification using the proposed MRFO-SVM revealed with evidence its superiority with overall classification accuracy of 98.26% over seven well-known metaheuristic algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005728",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature selection",
      "Histogram",
      "Image (mathematics)",
      "Linguistics",
      "Local binary patterns",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Houssein",
        "given_name": "Essam H."
      },
      {
        "surname": "Ibrahim",
        "given_name": "Ibrahim E."
      },
      {
        "surname": "Neggaz",
        "given_name": "Nabil"
      },
      {
        "surname": "Hassaballah",
        "given_name": "M."
      },
      {
        "surname": "Wazery",
        "given_name": "Yaser M."
      }
    ]
  },
  {
    "title": "Solar radiation forecasting based on convolutional neural network and ensemble learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115167",
    "abstract": "Nowadays, we are moving forward to more sustainable energy production systems based on renewable sources. Among all Photovoltaic (PV) systems are spreading in our cities. In this view, new models are needed to forecast Global Horizontal Solar Irradiance (GHI), which strongly influences PV production. For example, this forecast is crucial to develop novel control strategies for smart grid management. In this paper, we present a novel methodology to forecast GHI in short- and long-term time-horizons, i.e. from next 15 min up to next 24 h. It implements machine learning techniques to achieve this purpose. We start from the analysis of a real-world dataset with different meteorological information including GHI, in the form of time-series. Then, we combined Variational Mode Decomposition (VMD) and two Convolutional Neural Networks (CNN) together with Random Forest (RF) or Long Short Term Memory (LSTM). Finally, we present the experimental results and discuss their accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006060",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Convolutional neural network",
      "Ecology",
      "Electrical engineering",
      "Engineering",
      "Machine learning",
      "Meteorology",
      "Mode (computer interface)",
      "Operating system",
      "Photovoltaic system",
      "Physics",
      "Random forest",
      "Renewable energy",
      "Solar irradiance"
    ],
    "authors": [
      {
        "surname": "Cannizzaro",
        "given_name": "Davide"
      },
      {
        "surname": "Aliberti",
        "given_name": "Alessandro"
      },
      {
        "surname": "Bottaccioli",
        "given_name": "Lorenzo"
      },
      {
        "surname": "Macii",
        "given_name": "Enrico"
      },
      {
        "surname": "Acquaviva",
        "given_name": "Andrea"
      },
      {
        "surname": "Patti",
        "given_name": "Edoardo"
      }
    ]
  },
  {
    "title": "Automated classification of five arrhythmias and normal sinus rhythm based on RR interval signals",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115031",
    "abstract": "Arrhythmias are abnormal heart rhythms that can be life-threatening. Atrial Fibrillation (AFIB), Atrial Flutter (AFL), Supraventricular Tachycardia (SVT), Sinus Tachycardia (ST), and Sinus Bradycardia (SB) are common arrhythmias that affect a growing number of patients. In this paper we describe a method to detect these arrhythmias in RR interval signals. We propose a deep learning algorithm to discriminate these fife arrhythmias and Normal Sinus Rhythm (NSR). The deep learning model was trained and tested with data from 10093 subjects. We used 10-fold cross-validation to establish the performance results. The overall accuracy for the six-class problem was 98.37%. When considering the binary problem of arrhythmia versus NSR, where the arrhythmia group is formed by combining the data from all fife arrythmias, the performance results are: Accuracy (ACC) = 98.55%, Sensitivity (SEN) = 99.40%, Specificity (SPE) = 94.30%. These results indicate that it is possible to discriminate RR interval sequences from SVT, ST, SB, AFIB, AFL, and NSR subjects with minimal error. Furthermore, the proposed model can provide a robust and independent second opinion when it comes to a decision if arrhythmia is present or not. Another positive aspect of the proposed arrhythmia detection algorithm is economic viability. RR interval signals are cost-effective to measure, communicate, and process. The discriminate powers of the proposed algorithm together with the advent of wearable technology and m-health infrastructure might lead to pervasive long-term arrhythmia monitoring. The detection results can support early diagnosis which helps to reduce the burden of the disease.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004723",
    "keywords": [
      "Atrial fibrillation",
      "Atrial flutter",
      "Blood pressure",
      "Cardiac arrhythmia",
      "Cardiology",
      "Combinatorics",
      "Computer science",
      "Electrocardiography",
      "Heart rate",
      "Heart rate variability",
      "Internal medicine",
      "Interval (graph theory)",
      "Mathematics",
      "Medicine",
      "Normal Sinus Rhythm",
      "RR interval",
      "Sinus rhythm",
      "Sinus tachycardia",
      "Supraventricular arrhythmia",
      "Supraventricular tachycardia",
      "Tachycardia"
    ],
    "authors": [
      {
        "surname": "Faust",
        "given_name": "Oliver"
      },
      {
        "surname": "Acharya",
        "given_name": "U. Rajendra"
      }
    ]
  },
  {
    "title": "High utility itemset mining using binary differential evolution: An application to customer segmentation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115122",
    "abstract": "In this paper, high utility itemset mining (HUIM) algorithms driven by Binary Differential Evolution (BDE) and an Adaptive Binary Differential Evolution (ABDE) are proposed separately. These are compared with the HUIM algorithms of (i) Binary Particle Swarm Optimization, (ii) Genetic Algorithm and (iii) a two-phase HUIM found in the literature. The proposed HUIM algorithms are applied on seven datasets, where OnlineRetail dataset is a real-life dataset and the objective there is to segment high value customers based on monetary value. From the results, it is clear that BDE based HUIM outperformed the extant algorithms in literature and also the ABDE HUIM algorithm on all datasets with respect to the maximum number of itemsets mined.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005637",
    "keywords": [
      "Aerospace engineering",
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Biology",
      "Computer science",
      "Data mining",
      "Differential (mechanical device)",
      "Differential evolution",
      "Engineering",
      "Evolutionary biology",
      "Extant taxon",
      "Machine learning",
      "Mathematics",
      "Particle swarm optimization",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Value (mathematics)"
    ],
    "authors": [
      {
        "surname": "Krishna",
        "given_name": "Gutha Jaya"
      },
      {
        "surname": "Ravi",
        "given_name": "Vadlamani"
      }
    ]
  },
  {
    "title": "Auto-adaptive multilayer perceptron for univariate time series classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115147",
    "abstract": "Time Series Classification (TSC) is an intricate problem that has encountered applications in various science fields. Accordingly, many researchers have presented interesting proposals to tackle the TSC problem. Nevertheless, most methods are hand-crafted to classify specific Time Series (TS) and are computationally expensive even for small data sets. In this paper, we propose a new approach to the Multilayer Perceptron (MLP) for TSC. The main novelty is that the hyperparameters related to batch size and the number of neurons in the hidden layers are auto-adapted according to the TS nature. We carried out an empirical study on 61 benchmark data sets from the University of California, Riverside (UCR). The experimental evaluation revealed that our proposal is competitive when we compare the accuracy versus 14 state-of-the-art methods. A non-parametric statistical test verifies that the proposed MLP ranked in fourth place and can be executed on standard computer equipment, making it simple, accessible, and competitive.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005881",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Data mining",
      "Geodesy",
      "Geography",
      "Hyperparameter",
      "Machine learning",
      "Mathematics",
      "Multilayer perceptron",
      "Multivariate statistics",
      "Novelty",
      "Paleontology",
      "Parametric statistics",
      "Pattern recognition (psychology)",
      "Perceptron",
      "Philosophy",
      "Preprocessor",
      "Series (stratigraphy)",
      "Statistics",
      "Theology",
      "Time series",
      "Univariate"
    ],
    "authors": [
      {
        "surname": "Arias del Campo",
        "given_name": "Felipe"
      },
      {
        "surname": "Guevara Neri",
        "given_name": "María Cristina"
      },
      {
        "surname": "Vergara Villegas",
        "given_name": "Osslan Osiris"
      },
      {
        "surname": "Cruz Sánchez",
        "given_name": "Vianey Guadalupe"
      },
      {
        "surname": "Ochoa Domínguez",
        "given_name": "Humberto de Jesús"
      },
      {
        "surname": "García Jiménez",
        "given_name": "Vicente"
      }
    ]
  },
  {
    "title": "Extended high dimensional indexing approach for reachability queries on very large graphs",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114962",
    "abstract": "Given a directed acyclic graph G = ( V , A ) and two vertices u , v ∈ V , the reachability problem is to answer if there is a path from u to v in the graph. In the context of very large graphs, with millions of vertices and a series of queries to be answered, it is not practical to search the graph for each query. On the other hand, the storage of the full transitive closure of the graph is also impractical due to its O ( | V | 2 ) size. Scalable approaches aim to create indices used to prune the search during its execution. Negative indices may be able to determine (in constant time) that a query has a negative answer while positive indices may determine (again in constant time) that a query has a positive answer. In this paper we propose a novel scalable approach called LYNX that uses a large number of topological sorts of G as a negative cut index without degrading the query time. A similar strategy is applied regarding a positive cut index. In addition, LYNX proposes a user-defined index size that enables the user to control the ratio between negative and positive cuts depending on the expected query pattern. We show by computational experiments that LYNX consistently outperforms the state-of-the-art approach in terms of query-time using the same index-size for graphs with high reachability ratio. In intelligent computer systems that rely on frequent tests of connectivity in graphs, LYNX can reduce the time delay experience by end users through a reduced query time. This comes at the expense of an increased setup time whenever the underlying graph is updated.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004036",
    "keywords": [
      "Biology",
      "Combinatorics",
      "Computer science",
      "Constant (computer programming)",
      "Context (archaeology)",
      "Database",
      "Graph",
      "Graph database",
      "Information retrieval",
      "Line graph",
      "Mathematics",
      "Paleontology",
      "Programming language",
      "Reachability",
      "Scalability",
      "Search engine indexing",
      "Theoretical computer science",
      "Transitive closure",
      "Transitive reduction",
      "Transitive relation",
      "Voltage graph"
    ],
    "authors": [
      {
        "surname": "da Silva",
        "given_name": "Rodrigo Ferreira"
      },
      {
        "surname": "Urrutia",
        "given_name": "Sebastián"
      },
      {
        "surname": "Hvattum",
        "given_name": "Lars Magnus"
      }
    ]
  },
  {
    "title": "Multilingual evaluation of pre-processing for BERT-based sentiment analysis of tweets",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115119",
    "abstract": "Social media offer a big amount of information, to exploit in many fields of research. However, while methods for Natural Language Processing are being developed with good results when applied to well-formed datasets made of written text with a clear syntax, these sources present text written in informal language, unstructured syntax, and with peculiar symbols; therefore, particular approaches are required for text processing in this case. In this paper, the task of sentiment analysis of tweets is regarded. In particular, in order to avoid noise constituted by some web constructs like URLs and mentions and by other text fragments, and to exploit information hidden in symbols like emoticons, emojis and hashtags, the pre-processing of tweets is analyzed. More in detail, a number of experiments, performed by a state-of-the-art classification model (BERT), are designed, to evaluate many currently available operations for pre-processing tweets, in terms of the statistical significance of their influence on sentiment analysis performances. Moreover, available data in two languages are considered, i.e., English and Italian, in order to also evaluate dependence on the language. Results allow to individuate the most convenient strategy to pre-process tweets, and thus to improve the state of the art in both languages for the considered task of sentiment analysis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005601",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Economics",
      "Exploit",
      "Information retrieval",
      "Management",
      "Natural language processing",
      "Process (computing)",
      "Programming language",
      "Sentiment analysis",
      "Social media",
      "State (computer science)",
      "Syntax",
      "Task (project management)",
      "Text processing",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Pota",
        "given_name": "Marco"
      },
      {
        "surname": "Ventura",
        "given_name": "Mirko"
      },
      {
        "surname": "Fujita",
        "given_name": "Hamido"
      },
      {
        "surname": "Esposito",
        "given_name": "Massimo"
      }
    ]
  },
  {
    "title": "Evaluating deep learned voice compression for use in video games",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115180",
    "abstract": "In recent years video games have become one of the most popular entertainment mediums. This can partly be attributed to advances in computer graphics, and the availability, affordability and performance of hardware which have made modern video games the most realistic and immersive they have ever been. These games have a rich story with large open worlds, and a diverse cast of fully voice acted characters which also means that they take up large amounts of disk space. While a large percentage of this audio is sound effects and music, modern, character-driven, open world games contain multiple hours and many gigabytes of spoken voice audio. This paper examines how audio compression in video games poses distinctly different challenges than in telecommunications or archiving, the primary motivating factor that inspired audio compression systems currently used in video games. By evaluating new, deep learning based, methods of voice compression with video games in mind, we determine the criteria needed to be met for a new method to succeed current methods in measures of compression factor and quality at an acceptable level of algorithmic performance and what directions new research is needed to meet this criteria.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006187",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Character (mathematics)",
      "Computer graphics (images)",
      "Computer science",
      "Data compression",
      "Entertainment",
      "Epistemology",
      "Factor (programming language)",
      "Geometry",
      "Graphics",
      "Mathematics",
      "Multimedia",
      "Operating system",
      "Philosophy",
      "Programming language",
      "Quality (philosophy)",
      "Sound quality",
      "Space (punctuation)",
      "Speech recognition",
      "Video game",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Possemiers",
        "given_name": "Aidan"
      },
      {
        "surname": "Lee",
        "given_name": "Ickjai"
      }
    ]
  },
  {
    "title": "Multi-outsourcing supply chain coordination under yield and demand uncertainties",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115177",
    "abstract": "As a popular production mode, outsourcing enables the cost saving for original equipment manufacturer (OEM) and the acquisition of advanced technology for contract manufacturer (CM). We consider a multi-outsourcing supply chain within the voluntary compliance regime. The game between one OEM facing uncertain demand and arbitrary CMs owning uncertain yields is modeled to study the interactions regarding lot-sizing decisions. A general method is developed to prove the concavity of profit functions, which is formidable to accomplish based on Hessian Matrix. The optimal ordering and production strategies are subsequently characterized. A revenue sharing with surplus purchase contract with great flexibility in parameter selection is proposed to coordinate the supply chain. We find that there exist threshold outsourcing prices beyond which the CMs are motivated to overproduce, otherwise they tend to produce in consistent with the orders. In a competitive market, price war is not a wise strategy for CM due to profit sacrifice. By contrast, if the selling price charged by the OEM is high, a smart CM can appropriately raise its outsourcing price because the order quantity difference among CMs is relatively small. Although supply stability improvement and outsourcing price reduction are both welcomed by the OEM, the latter is more important for it to consider in allocating orders.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006151",
    "keywords": [
      "Business",
      "Computer science",
      "Economics",
      "Finance",
      "Industrial organization",
      "Marketing",
      "Mathematics",
      "Microeconomics",
      "Operating system",
      "Operations research",
      "Original equipment manufacturer",
      "Outsourcing",
      "Profit (economics)",
      "Revenue",
      "Supply chain"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Zhiming"
      },
      {
        "surname": "Liu",
        "given_name": "Fanglong"
      }
    ]
  },
  {
    "title": "A novel modified fuzzy best-worst multi-criteria decision-making method",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115196",
    "abstract": "One of the latest multi-criteria decision-making methods is best-worst method (BWM). In the procedure of BWM, decision maker (DM) identifies the most and the least important criteria namely, best and worst. Thereafter, DM identifies the degrees which he believes the best criterion is better than the other criteria, and identifies the degrees which he believes the other criteria are better than the worst criterion. Because of uncertainties in comparisons due to using linguistic variables for pairwise comparisons and also the lack of complete information, the crisp values of pairwise comparisons cannot appropriately model the problems. This paper develops the BWM for considering fuzzy pairwise comparisons (FBWM) by proposing a new fuzzy mathematical model which yields crisp weights from a fuzzy pairwise comparison matrix. Unlike to some previous papers that obtains fuzzy weights from fuzzy pairwise comparison matrix, the crisp weights of the proposed method of this paper eliminates the supplementary aggregation of fuzzy weights and ranking procedures. Moreover, the proposed method avoids obtaining the different ranking results due to the different ranking procedures of fuzzy numbers. Another outstanding advantage of this paper is that the obtained weights of the proposed method better satisfy the initial judgments compared to previous methods, while as we know, the satisfaction of the initial judgments is essential for pairwise comparison judgments. This paper presents several numerical examples to prove the good performance and the merit of the proposed method. According to the provided numerical examples, the proposed method of this paper absolutely outperforms the two well-known previous methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006308",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Decision maker",
      "Fuzzy logic",
      "Fuzzy number",
      "Fuzzy set",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Operations research",
      "Pairwise comparison",
      "Ranking (information retrieval)"
    ],
    "authors": [
      {
        "surname": "Mohtashami",
        "given_name": "Ali"
      }
    ]
  },
  {
    "title": "Measuring the efficiency of the Portuguese public hospitals: A value modelled network data envelopment analysis with simulation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115169",
    "abstract": "In contrast to conventional data envelopment analysis (DEA), where a system is considered as a ”black-box”, network DEA acknowledges its internal structure to generate more enlightening results. It goes without saying that putting network DEA in practice is natural- and progressively rarer as the complexity of a system’s structure increases. In particular, its employment in healthcare is no exception. Thus, we designed a slacks-based model to measure the efficiency of the Portuguese secondary healthcare providers bearing in mind their internal services. However, the absence of data regarding the connections between those services called for the additional use of a simulation method – the well-known Monte Carlo method –, modelled with the judgements of a decision-maker. This unprecedented application of static systems with a matrix-type structure found that two-thirds of those providers were inefficient and allowed the identification of target areas for future policy reforms in the Portuguese National Health Service.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006072",
    "keywords": [
      "Biology",
      "Botany",
      "Business",
      "Computer science",
      "Data envelopment analysis",
      "Data mining",
      "Economic growth",
      "Economics",
      "Health care",
      "Identification (biology)",
      "Linguistics",
      "Marketing",
      "Mathematical optimization",
      "Mathematics",
      "Measure (data warehouse)",
      "Operations research",
      "Philosophy",
      "Portuguese",
      "Service (business)"
    ],
    "authors": [
      {
        "surname": "Pereira",
        "given_name": "Miguel Alves"
      },
      {
        "surname": "Ferreira",
        "given_name": "Diogo Cunha"
      },
      {
        "surname": "Figueira",
        "given_name": "José Rui"
      },
      {
        "surname": "Marques",
        "given_name": "Rui Cunha"
      }
    ]
  },
  {
    "title": "Anomalous object detection by active search with PTZ cameras",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115150",
    "abstract": "Due to the large amount of visual information generated daily, proposals that automatically analyze and process data are becoming increasingly necessary. This work focuses on the detection of anomalous objects in video sequences captured by PTZ (pan-tilt-zoom) cameras, considering as anomalies the objects which belong to categories that should not appear in a specific scene (e.g. pedestrians on a highway). There is a lack in the previous literature of a principled approach for the control of PTZ cameras that takes advantage of the recent developments in deep learning-based object detection. Our proposal aims to fill this gap by offering a probabilistic framework where the guidance of PTZ cameras is accommodated. The proposed methodology involves three different modules. An object detection stage, where deep learning networks are used to detect the objects that appear in the scene; an anomalous detection module, where a mixture of Dirichlet distributions is considered to detect automatically, and without supervised training, those detected objects which are likely to be anomalous; and finally, a PTZ camera controller which allows to follow and focus on the object considered as the most probably anomalous in the scene. The experimental results show the performance and viability of our proposal, which outperforms several competitors from qualitative and quantitative points of view.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005911",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Engineering",
      "Focus (optics)",
      "Lens (geology)",
      "Object (grammar)",
      "Object detection",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Petroleum engineering",
      "Physics",
      "Probabilistic logic",
      "Process (computing)",
      "Zoom"
    ],
    "authors": [
      {
        "surname": "López-Rubio",
        "given_name": "Ezequiel"
      },
      {
        "surname": "Molina-Cabello",
        "given_name": "Miguel A."
      },
      {
        "surname": "Castro",
        "given_name": "Francisco M."
      },
      {
        "surname": "Luque-Baena",
        "given_name": "Rafael M."
      },
      {
        "surname": "Marín-Jiménez",
        "given_name": "Manuel J."
      },
      {
        "surname": "Guil",
        "given_name": "Nicolás"
      }
    ]
  },
  {
    "title": "Embedding ranking-oriented recommender system graphs",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115108",
    "abstract": "Graph-based recommender systems (GRSs) analyze the structural information available in the graphical representation of data to make better recommendations, especially when direct user-item relation data is sparse. Ranking-oriented GRSs mostly use graphical representation of preference (or rank) data for measuring node similarities, from which they can infer recommendations using neighborhood-based methods. In this paper, we propose PGRec, a novel model-based ranking-oriented recommendation framework. Unlike many other graph-based methods, PGRec extracts vector representations for users and preferences from a novel graph structure called PrefGraph, which models entity relations, feedbacks, and content. A general graph-embedding process is improved and applied to extract vector representations for entities. The resulting embeddings are then used for predicting the target user’s unknown pairwise preferences by a neural network based on which a recommendation list is generated for the target user. We have evaluated the proposed method’s performance against the state of the art model-based and neighborhood-based recommendation algorithms. Our experiments show that PGRec outperforms the baseline algorithms in terms of the NDCG metric in several datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005492",
    "keywords": [],
    "authors": [
      {
        "surname": "Hekmatfar",
        "given_name": "Taher"
      },
      {
        "surname": "Haratizadeh",
        "given_name": "Saman"
      },
      {
        "surname": "Goliaei",
        "given_name": "Sama"
      }
    ]
  },
  {
    "title": "Online Subclass Knowledge Distillation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115132",
    "abstract": "Knowledge Distillation has been established as a highly promising approach for training compact and faster models by transferring knowledge from more heavyweight and powerful models, so as to satisfy the computation and storage requirements of deploying state-of-the-art deep neural models on embedded systems. However, conventional knowledge distillation requires multiple stages of training rendering it a computationally and memory demanding procedure. In this paper, a novel single-stage self knowledge distillation method is proposed, namely Online Subclass Knowledge Distillation (OSKD), that aims at revealing the similarities inside classes, improving the performance of any deep neural model in an online manner. Hence, as opposed to existing online distillation methods, we are able to acquire further knowledge from the model itself, without building multiple identical models or using multiple models to teach each other, rendering the OSKD approach more effective. The experimental evaluation on five datasets indicates that the proposed method enhances the classification performance, while comparison results against existing online distillation methods validate the superiority of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100573X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Computation",
      "Computer science",
      "Distillation",
      "Machine learning",
      "Organic chemistry",
      "Rendering (computer graphics)"
    ],
    "authors": [
      {
        "surname": "Tzelepi",
        "given_name": "Maria"
      },
      {
        "surname": "Passalis",
        "given_name": "Nikolaos"
      },
      {
        "surname": "Tefas",
        "given_name": "Anastasios"
      }
    ]
  },
  {
    "title": "Evolutionary algorithm hybridized with local search and intelligent seeding for solving multi-objective Euclidian TSP",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115192",
    "abstract": "Multi-objective Euclidian TSP (ETSP) has several practical applications such as mobile computing and maritime surveillance. This problem has complexity not only in terms of combinatorial constraints but also in terms of multiple objectives. The local heuristic-based algorithms are extremely efficient in solving the single-objective ETSPs. However, their efficacy is limited in solving the multi-objective ETSPs due to the presence of multiple objectives. To bridge this gap, a two-stage evolutionary algorithm (TSEA) is developed to solve multi-objective ETSPs. In this algorithm, the first stage involves the use of a hybrid local search evolutionary algorithm (HLS-EA) which incorporates the local heuristic of nearest neighbor and 2-opt in the framework of real-coded NSGA-II to solve the individual objectives of multi-objective ETSP. These individual single-objective solutions which represent the corner solutions of the Pareto optimal front are used in the second stage as seed solutions in seeded HLS-EA (SHLS-EA) for solving the corresponding multi-objective ETSP. The developed algorithm is tested on 17 two-objective, 6 three-objective, and 2 four-objective ETSPs to show the superior performance over multi-objective variants of the Lin-Kernighan algorithm. Also, the developed algorithm is compared with several variants of DE and GA to illustrate the superior performance over the variants of evolutionary algorithms. Further, the algorithm is extended to the multi-objective ETSPs up to 10,000 cities.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006278",
    "keywords": [
      "Aerospace engineering",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Engineering",
      "Euclidean distance",
      "Evolutionary algorithm",
      "Local search (optimization)",
      "Mathematical optimization",
      "Mathematics",
      "Seeding"
    ],
    "authors": [
      {
        "surname": "Agrawal",
        "given_name": "Anubha"
      },
      {
        "surname": "Ghune",
        "given_name": "Nitish"
      },
      {
        "surname": "Prakash",
        "given_name": "Shiv"
      },
      {
        "surname": "Ramteke",
        "given_name": "Manojkumar"
      }
    ]
  },
  {
    "title": "Discriminant Spatial Filtering Method (DSFM) for the identification and analysis of abnormal resting state brain activities",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115074",
    "abstract": "Advances in neuroimaging techniques have enabled early diagnosis and better understanding of neuro-psychological disorders using resting state - functional Magnetic Resonance Images (rs-fMRI). However, factors like the large intra-class variability, high homogeneity, limited sample size and varied acquisition methodologies in aggregated datasets have limited their classification performance. These issues especially are highly prevalent in the automatic diagnosis of Attention Deficit Hyperactivity Disorder (ADHD). In this paper, a Discriminative Spatial Filtering Method (DSFM) is developed to improve the separability between the two classes in the dataset by automatically identifying regions of the brain with abnormal activities. Further, DSFM uses an orthogonal projection to extract highly separable features from raw rs-fMRI data of the identified regions of the brain. Using a projection based learning classifier DSFM achieved a classification accuracy of 73.83 % on the publicly available ADHD200 dataset. The results show that, selective inclusion of data from the regions of the brain with discriminant activity identified using DSFM removes the redundant features and improved the classification performance (improvement of 3.5 % over baseline). Further, the brain activity maps derived by inverse mapping of the DSFM spatial filters describe the nature of the abnormal brain activities and identify the corresponding regions responsible for the cognitive symptoms in neuro-psychological disorders like ADHD. The results in this paper clearly show that DSFM is a reliable diagnostic tool with interpretable results critical for the accurate understanding the etiological factors of neuro-psychological disorders like ADHD using rs-fMRI.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005157",
    "keywords": [
      "Artificial intelligence",
      "Brain activity and meditation",
      "Classifier (UML)",
      "Computer science",
      "Discriminative model",
      "Electroencephalography",
      "Functional magnetic resonance imaging",
      "Linear discriminant analysis",
      "Machine learning",
      "Neuroimaging",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Psychology",
      "Resting state fMRI"
    ],
    "authors": [
      {
        "surname": "Aradhya",
        "given_name": "Abhay M.S."
      },
      {
        "surname": "Subbaraju",
        "given_name": "Vigneshwaran"
      },
      {
        "surname": "Sundaram",
        "given_name": "Suresh"
      },
      {
        "surname": "Sundararajan",
        "given_name": "Narasimhan"
      }
    ]
  },
  {
    "title": "Exploiting personalized calibration and metrics for fairness recommendation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115112",
    "abstract": "Recommendation systems are used to suggest items that users can be interested in. These systems are based on the user preference historic to create a recommendation list with items that have the higher similarity with the user’s interests, in order to achieve the best possible user’s satisfaction, which is usually measured as recommendation precision. However, the search for the best precision can cause some side effects such as overspecialization, few diversity and miscalibration of genres, classes and niches. Calibration provides fairer recommendations, which respect the genre proportionality on the user’s preferences, avoiding overspecialization. This article aims to explore ways to balance the trade-off weight between precision and calibration based on divergence measures, as well as to propose metrics to evaluate the calibration in the suggested list. The proposed system works in a post-processing step and does not depend on a specific recommender algorithm or workflow. For this purpose, we evaluate six recommender algorithms applied in the movie domain, analyzing variations of three fairness measures, two personalized trade-off weights and eleven constant weights. To understand the results we use the precision, the reciprocal rank and two proposed metrics. The results indicate that the trade-off formulation of personalized weights obtains better results when used to compare the recommendation lists using matrix factorization-based approaches on Movielens dataset. In addition, the calibration also impacts the precision and fairness of all considered algorithms used in evaluation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005534",
    "keywords": [
      "Calibration",
      "Collaborative filtering",
      "Computer science",
      "Data mining",
      "Database",
      "Eigenvalues and eigenvectors",
      "Information retrieval",
      "Machine learning",
      "Mathematics",
      "Matrix decomposition",
      "Mean reciprocal rank",
      "MovieLens",
      "Personalization",
      "Physics",
      "Quantum mechanics",
      "Recommender system",
      "Statistics",
      "Workflow",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "da Silva",
        "given_name": "Diego Corrêa"
      },
      {
        "surname": "Manzato",
        "given_name": "Marcelo Garcia"
      },
      {
        "surname": "Durão",
        "given_name": "Frederico Araújo"
      }
    ]
  },
  {
    "title": "Word sense induction using leader-follower clustering of automatically generated lexical substitutes",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115162",
    "abstract": "Word Sense Induction (WSI) concerns the automatic identification of the various senses of polysemous words. Any improvement in this process can directly affect the quality of the applications in which knowing the word’s senses is important. For example, word sense disambiguation, information retrieval, and clustering of web search result in lexically ambiguous queries. In this paper, we propose a novel WSI model that makes use of automatically generated lexical substitutes for a target word to construct a graph and data preparation for the next steps. Following the data preparation step, we make use of Leader–Follower graph clustering to find the basic senses of the target word. The senses of the target word inside the remaining or new upcoming instances will be decided according to their contextual embedding’s similarities with the basic sense. Besides, to make the number of found sense groups of a target word much closer to the reality, we apply post-processing at the end. The results of experiments on SemEval2010 dataset confirm that the proposed method outperforms all the state-of-the-art solutions in terms of both harmonic and geometric v-measure and f-score with a lower average number of sense groups.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006035",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Graph",
      "Linguistics",
      "Natural language processing",
      "Philosophy",
      "Theoretical computer science",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Akkasi",
        "given_name": "Abbas"
      },
      {
        "surname": "Snajder",
        "given_name": "Jan"
      }
    ]
  },
  {
    "title": "Intern retrieval from community question answering websites: A new variation of expert finding problem",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115044",
    "abstract": "In recent years, with the advent of new scientific and technical areas, hiring suitable interns has become very important for large companies. In internship programs, to reduce the risk of unsuccessful recruitment, companies are actively seeking those candidates who are ready to become skillful experts, and also incur the least financial costs. Therefore, generalists (i.e. Hyphen-shaped people) are the most suitable candidates for such positions. These candidates have general knowledge in the required skills of the position and do not have expertise in any other area. One of the environments that accurately reflect the knowledge of people is community question answering (CQA). This study is the first that focuses on retrieving interns from CQA websites. It uses the concepts of generalist and shape of expertise to identify suitable candidates for the internship programs of companies. Specifically, in this paper, we define the intern retrieval problem, in which given a set of required skills of an internship position, a ranking of candidates is generated so that the generalists who have general knowledge in those skills are retrieved in top ranks. We propose two retrieval models to address the problem. In order to evaluate the performance of these models, we introduce two specific measures (i.e. coverage and optimality). Our experiments on three test collections extracted from StackOverflow demonstrate the effectiveness of our models in comparison with several baselines.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004851",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Business",
      "Computer science",
      "Data science",
      "Finance",
      "Information retrieval",
      "Internship",
      "Medical education",
      "Medicine",
      "Paleontology",
      "Position (finance)",
      "Position paper",
      "Programming language",
      "Question answering",
      "Ranking (information retrieval)",
      "Set (abstract data type)",
      "Test (biology)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Rostami",
        "given_name": "Peyman"
      },
      {
        "surname": "Neshati",
        "given_name": "Mahmood"
      }
    ]
  },
  {
    "title": "An automatic method for segmentation of liver lesions in computed tomography images using deep neural networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115064",
    "abstract": "Liver cancer is one of the major causes of death by cancer. The early detection of lesions in the liver provides a better chance of treatment and cure of the disease. Computed tomography (CT) is one of the most used imaging techniques for the detection and diagnosis of liver lesions. However, the manual segmentation of liver and tumors, aside from being time-consuming, can still cause errors and may vary among specialists. Because of this hard work, computer-aided detection (CAD) and computer-aided diagnosis (CADx) systems have been developed to assist specialists in the detection and characterization of lesions in the liver and reduce the required time for diagnosis. The automatic segmentation of these lesions is a complex task since they present variability in contrast, shape, size, and location. In this work, a method to automatically segment liver lesions in CT images is proposed. The proposed method, which presents two deep convolutional neural networks (CNN) models, consists of five main steps: (1) image acquisition, (2) image pre-processing, (3) initial segmentation using RetinaNet, (4) lesion segmentation using U-Net, and (5) segmentation refinement. The proposed method was evaluated using a set of 131 CT images from the LiTS dataset, and the best result obtained a matthews correlation coefficient (MCC) of 83.62%, a sensitivity of 83.86%, a specificity of 99.96%, a Dice coefficient of 82.99%, a volumetric overlap error (VOE) of 27.89%, and a relative volume difference (RVD) of 1.69%. We show in our method that the problem of segmentation of liver lesions in CT images can be efficiently solved through the use of deep CNNs to define the scope of the problem and to precisely segment lesions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005054",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "CAD",
      "Computer science",
      "Computer vision",
      "Computer-aided diagnosis",
      "Convolutional neural network",
      "Deep learning",
      "Engineering",
      "Engineering drawing",
      "Image segmentation",
      "Medicine",
      "Pattern recognition (psychology)",
      "Radiology",
      "Segmentation",
      "Sørensen–Dice coefficient"
    ],
    "authors": [
      {
        "surname": "Araújo",
        "given_name": "José Denes Lima"
      },
      {
        "surname": "da Cruz",
        "given_name": "Luana Batista"
      },
      {
        "surname": "Ferreira",
        "given_name": "Jonnison Lima"
      },
      {
        "surname": "da Silva Neto",
        "given_name": "Otilio Paulo"
      },
      {
        "surname": "Silva",
        "given_name": "Aristófanes Corrêa"
      },
      {
        "surname": "de Paiva",
        "given_name": "Anselmo Cardoso"
      },
      {
        "surname": "Gattass",
        "given_name": "Marcelo"
      }
    ]
  },
  {
    "title": "ARQGAN: An evaluation of generative adversarial network approaches for automatic virtual inpainting restoration of Greek temples",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115092",
    "abstract": "Image reconstruction has received much attention and has advanced in recent years with the rise of deep learning. Deep neural models have been able to perform image-to-image translation by transferring pictorial styles, coloring old photographs or filling in missing parts. This last technique is known as image inpainting and enables restoration of damaged or missing parts of an image or photograph to obtain the complete picture. However, it is not always possible to properly define which parts are missing or to identify where they are missing, as in the case of superimposing new information on an already complete image. In this paper, we propose the use of generative adversarial networks (GANs), a well-known deep learning model, for virtual inpainting restoration of artificial landscape images containing archaeological remains of Greek temples. The network identifies key features determined by the internal logic of the architectural style denoted by the ruins and adds the missing architectural elements to obtain an image of the restored building. Unlike other studies, it does not receive any information on which elements should be added or where. Virtual inpainting restoration is capable of representing a building’s envelope but also integrates particular aspects of the building related to the architectural language used for its design. The restoration of the fundamental parts of the classical Greek order was consistent, and the results were evaluated with objective metrics and through a subjective survey between academics and architects. They showed that adding segmented images to the training dataset gives better results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005339",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Generative adversarial network",
      "Generative grammar",
      "Image (mathematics)",
      "Inpainting"
    ],
    "authors": [
      {
        "surname": "Nogales",
        "given_name": "Alberto"
      },
      {
        "surname": "Delgado-Martos",
        "given_name": "Emilio"
      },
      {
        "surname": "Melchor",
        "given_name": "Ángel"
      },
      {
        "surname": "García-Tejedor",
        "given_name": "Álvaro J."
      }
    ]
  },
  {
    "title": "Application of complex systems topologies in artificial neural networks optimization: An overview",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115073",
    "abstract": "Through the success of artificial neural networks (ANNs) in different domains, intense research has been recently centered on changing the networks architecture to optimize the performance. Due to the broad connectivity and complex structure of artificial neural networks (ANNs), designing diluted ANNs with less time, wiring costs and space while obtaining high performance have attracted much attention. Complex systems theory, in which the influence of structure on the overall behavior of the network is mainly considered, have been applied in ANNs to have more efficient and less complex structures. It has been shown that complex random topologies outperform the fully-connected ANNs with less connectivity. But according to neurobiological investigations, highly clustered neurons with short characteristic path length and scale-free distributions are more favored while spending less connectivity costs. Therefore, applying small-world and scale-free topologies on ANNs has been explored in recent years. In this paper, the methodology and results of recent studies are summarized and discussed in which the authors investigated the efficacy of small-world, scale-free and hybrid complex networks on the performance of Hopfield associative memory and layered ANNs compared with conventional fully-connected and random structures.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005145",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Associative property",
      "Combinatorics",
      "Complex network",
      "Complex system",
      "Computer science",
      "Content-addressable memory",
      "Machine learning",
      "Mathematics",
      "Network topology",
      "Operating system",
      "Physics",
      "Pure mathematics",
      "Quantum mechanics",
      "Scale (ratio)",
      "Topology (electrical circuits)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Kaviani",
        "given_name": "Sara"
      },
      {
        "surname": "Sohn",
        "given_name": "Insoo"
      }
    ]
  },
  {
    "title": "A fast graph modification method for social network anonymization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115148",
    "abstract": "Privacy on social networks is one of the most important and well-known issues. Various algorithms have been proposed to preserve the privacy of social network, all of which try to change the graph structure such that the utility of the graph is maintained. Although these algorithms have been successful in protecting the privacy and the utility of social networks, they are not suitable for anonymizing big data, because of the high cost of processing. Some of these algorithms have a high runtime. In addition, they should be improved from the aspect of preserving graph utility. In this paper, an effective algorithm has been introduced to increase the anonymization speed, as well as improving the graph utility. This algorithm uses number factorization to remove the best edges from the graph in the graph modification step of the algorithm. Since the appropriate edges are selected just through one scan of the edges, the runtime is reduced. In order to add edges to the graph, all the appropriate edges are selected simultaneously and added to the graph using NaFa algorithm. Some limitations are applied to the selection of edges to choose the correct ones. Taking into account the appropriate criteria to select edges in both the removal and addition steps improves the graph utility. The evaluation results of the proposed algorithm on real data sets show the efficiency of the algorithm to sharply reduce the runtime and increase the graph utility, simultaneously.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005893",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Graph",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Kiabod",
        "given_name": "Maryam"
      },
      {
        "surname": "Naderi Dehkordi",
        "given_name": "Mohammad"
      },
      {
        "surname": "Barekatain",
        "given_name": "Behrang"
      }
    ]
  },
  {
    "title": "Co-attentive representation learning for web services classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115070",
    "abstract": "The rapid adoption of services-related technologies, such as cloud computing, has lead to the explosive growth of web services. Automated service classification that groups web services by similar functionality is a widely used technique to facilitate the management and discovery of web services within a large-scale repository. The existing service classification approaches primarily focus on learning the isolated representations of service features but ignored their internal semantic correlations. To address the aforementioned issue, we propose a novel deep neural network with the Co-Attentive Representation Learning (CARL-Net) mechanism for effectively classifying services by learning interdependent characteristics of service without feature engineering. Specifically, we propose a service data augmentation mechanism by extracting informative words from the service description using information gain theory. Such a mechanism can learn a correlation matrix among embedded augmented data and description, thereby obtaining their interdependent semantic correlation representations for service classification. We evaluate the effectiveness of CARL-Net by comprehensive experiments based on a real-world dataset collected from ProgrammableWeb, which includes 10,943 web services. Compared with seven web service classification baselines based on CNN, LSTM, Recurrent-CNN, C-LSTM, BLSTM, ServeNet and ServeNet-BERT, the CARL-Net can achieve an improvement of 5.66%–172.21% in the F-measure of web service classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100511X",
    "keywords": [
      "Artificial intelligence",
      "Cloud computing",
      "Computer science",
      "Economics",
      "Economy",
      "Feature (linguistics)",
      "Information retrieval",
      "Interdependence",
      "Law",
      "Linguistics",
      "Machine learning",
      "Operating system",
      "Philosophy",
      "Political science",
      "Semantic Web",
      "Service (business)",
      "Social Semantic Web",
      "Web service",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Bin"
      },
      {
        "surname": "Yan",
        "given_name": "Meng"
      },
      {
        "surname": "Zhang",
        "given_name": "Neng"
      },
      {
        "surname": "Xu",
        "given_name": "Ling"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaohong"
      },
      {
        "surname": "Ren",
        "given_name": "Haijun"
      }
    ]
  },
  {
    "title": "Online disease risk monitoring using DEWMA control chart",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115059",
    "abstract": "Early detection of a disease risk plays a vital role in successful treatment of the disease. Many chronic diseases, e.g., stroke, can be treated satisfactorily if they can be detected early. Traditionally, people evaluate their health conditions by comparing the current readings of their medical risk factors with some threshold values, and if irregular readings are triggered out, further medical tests are conducted to find the causes of the disease. A limitation of the traditional disease detection methods is the usage of only current time point data while ignoring the historical data. To use the history of the process for early detection of the disease using statistical process control, we suggest the use of a double exponential weighted moving average control chart to monitor risk factors sequentially. For the estimation of disease risk factors, different kernel functions are used. In particular, we use Epanechnikov, triangular, tricube, biweight, Gaussian, triweight, and cosine kernels. To evaluate the performance of different kernel functions, we conducted simulations as well as a real data set is used. The real data set is about 1055 stroke patients, out of which 27 individuals have suffered from a stroke attack at least once during the study time and the remaining 1028 people did not experience stroke. Numerical results show that the suggested method performs well in detection of early disease risk. Furthermore, it is observed that the biweight kernel function performs better than the other kernel functions for online disease risk monitoring. It is also noticed that for small smoothing parameters of the DEWMA chart, the Epanechinkov and cosine kernels perform better whereas tricube and biweight for the large smoothing parameters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005005",
    "keywords": [
      "Artificial intelligence",
      "Chart",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Control chart",
      "Data set",
      "Disease",
      "Engineering",
      "Exponential smoothing",
      "Gaussian",
      "Gaussian function",
      "Gaussian process",
      "Kernel (algebra)",
      "Machine learning",
      "Mathematics",
      "Mechanical engineering",
      "Medicine",
      "Operating system",
      "Pathology",
      "Physics",
      "Process (computing)",
      "Programming language",
      "Quantum mechanics",
      "Set (abstract data type)",
      "Smoothing",
      "Statistics",
      "Stroke (engine)"
    ],
    "authors": [
      {
        "surname": "Ashraf",
        "given_name": "Amir"
      },
      {
        "surname": "Ali",
        "given_name": "Sajid"
      },
      {
        "surname": "Shah",
        "given_name": "Ismail"
      }
    ]
  },
  {
    "title": "News Sentiment Informed Time-series Analyzing AI (SITALA) to curb the spread of COVID-19 in Houston",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115104",
    "abstract": "Coronavirus disease (COVID-19) has evolved into a pandemic with many unknowns. Houston, located in the Harris County of Texas, is becoming the next hotspot of this pandemic. With a severe decline in international and inter-state travel, a model at the county level is needed as opposed to the state or country level. Existing approaches have a few drawbacks. Firstly, the data used is the number of COVID-19 positive cases instead of positivity. The former is a function of the number of tests carried out while the number of tests normalizes the latter. Positivity gives a better picture of the spread of this pandemic as, with time, more tests are being administered. Positivity under 5% has been desired for the reopening of businesses to almost 100% capacity. Secondly, the data used by models like SEIRD (Susceptible, Exposed, Infectious, Recovered, and Deceased) lacks information about the sentiment of people concerning coronavirus. Thirdly, models that make use of social media posts might have too much noise and misinformation. On the other hand, news sentiment can capture long-term effects of hidden variables like public policy, opinions of local doctors, and disobedience of state-wide mandates. The present study introduces a new artificial intelligence (i.e., AI) model, viz., Sentiment Informed Time-series Analyzing AI (SITALA), trained on COVID-19 test positivity data and news sentiment from over 2750 news articles for Harris county. The news sentiment was obtained using IBM Watson Discovery News. SITALA is inspired by Google-Wavenet architecture and makes use of TensorFlow. The mean absolute error for the training dataset of 66 consecutive days is 2.76, and that for the test dataset of 22 consecutive days is 9.6. A cone of uncertainty is provided within which future COVID-19 test positivity has been shown to fall with high accuracy. The model predictions fare better than a published Bayesian-based SEIRD model. The model forecasts that in order to curb the spread of coronavirus in Houston, a sustained negative news sentiment (e.g., death count for COVID-19 will grow at an alarming rate in Houston if mask orders are not followed) will be desirable. Public policymakers may use SITALA to set the tone of the local policies and mandates.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005455",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Coronavirus disease 2019 (COVID-19)",
      "Disease",
      "Infectious disease (medical specialty)",
      "Medicine",
      "Misinformation",
      "Pandemic",
      "Pathology",
      "Sentiment analysis",
      "Social media",
      "Watson",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Desai",
        "given_name": "Prathamesh S."
      }
    ]
  },
  {
    "title": "Multirobot coordination with deep reinforcement learning in complex environments",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115128",
    "abstract": "In the multiple autonomous robot system, it is very important to complete path planning coordinately and effectively in the processes of interference avoidance, resource allocation and information sharing. In traditional multirobot coordination algorithms, most of the solutions are in known environments, the target position that each robot needs to move to and the robot priority are set, which limits the autonomy of the robot. Only using visual information to solve the problem of multirobot coordination is still less. This paper proposes a multi-robot cooperative algorithm based on deep reinforcement learning to make the robot more autonomous in the process of selecting target positions and moving. We use the end-to-end approach, using only the top view, that is, a robot-centered top view, and the first-person view, that is, the image information collected from the first-person perspective of the robot, as input. The proposed algorithm, which includes a dueling neural network structure, can solve task allocation and path planning; we call the algorithm TFDueling. Through its perception and understanding of the environment, the robot can reach the target position without collision, and the robot can move to any target position. We compare the proposed algorithm, TFDueling, with different input structure algorithms, TDueling and FDueling, and with different neural network structures, TFDQN and TFDDQN. Experiments show that the proposed TFDueling algorithm has the highest accuracy and robustness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005698",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Gene",
      "Mobile robot",
      "Motion planning",
      "Reinforcement learning",
      "Robot",
      "Robot learning",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Di"
      },
      {
        "surname": "Deng",
        "given_name": "Hongbin"
      }
    ]
  },
  {
    "title": "ImageDataset2Vec: An image dataset embedding for algorithm selection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115053",
    "abstract": "Convolutional Neural Networks (CNNs) have become the main solution for image classification tasks in different applications. Although several CNN architectures are available, there is no best architecture regardless the problem at hand. The selection of the most suitable CNN architecture is usually performed by trial and error, which may take much time and computational resources. Meta-learning (MtL) is a framework developed in machine learning to perform algorithm selection based on the meta-features of each task being solved. Such meta-features are usually descriptive characteristics extracted from the training dataset available in the task at hand. Despite the increasing attention of MtL for algorithm selection, its success strongly depends on defining relevant meta-features to represent the classification tasks of interest. This paper proposes the ImageDataset2Vec method for extracting meta-features to describe image classification datasets. ImageDataset2Vec adopts a pre-trained deep neural network to extract features from images datasets, embedding them in a single feature vector. The derived meta-features are employed by MtL to select CNN architectures for image classification. The proposed approach was evaluated for selecting among six CNN algorithms in 45 two-classes image datasets. The results showed that MtL using ImageDataset2Vec overcame different baseline methods, selecting the best possible CNN algorithm in 84.45 % of the datasets. Furthermore, the proposal was statistically equivalent to the ground truth when the best CNN is recommended, i.e., when MtL does not select the best CNN, it selects a competitive algorithm. These results show that the proposal was able to extract representative features from image datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004942",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Contextual image classification",
      "Convolutional neural network",
      "Deep learning",
      "Economics",
      "Embedding",
      "Feature selection",
      "Image (mathematics)",
      "Machine learning",
      "Management",
      "Meta learning (computer science)",
      "Pattern recognition (psychology)",
      "Selection (genetic algorithm)",
      "Selection algorithm",
      "Support vector machine",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Dias",
        "given_name": "Lucas V."
      },
      {
        "surname": "Miranda",
        "given_name": "Péricles B.C."
      },
      {
        "surname": "Nascimento",
        "given_name": "André C.A."
      },
      {
        "surname": "Cordeiro",
        "given_name": "Filipe R."
      },
      {
        "surname": "Mello",
        "given_name": "Rafael Ferreira"
      },
      {
        "surname": "Prudêncio",
        "given_name": "Ricardo B.C."
      }
    ]
  },
  {
    "title": "An adaptive cluster-based sparse autoregressive model for large-scale multi-step traffic forecasting",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115093",
    "abstract": "Traffic forecasting has been extensively studied due to its importance for the design and development of Intelligent Transportation Systems (ITS). Most of the existing relevant literature focuses, almost exclusively, on the effectiveness of the traffic forecasting models, while neglecting the importance of computational efficiency. However, the need for faster models becomes increasingly urgent as the volume of available traffic data increases. In this paper, an effective and efficient model for large-scale multi-step traffic forecasting is presented. In particular, the classic autoregressive model is reformulated based on the idea that not all past traffic values are important for predicting future traffic values, and thus only some of them should be taken into account in the forecasting process. The selection of the appropriate past values is performed by the application of an eligibility criterion, controlled by a respective hyperparameter and its value is optimized using an efficient cluster-based method. The overall modelling approach leads to a sparse least squares problem, which is efficiently solved using a novel explicit preconditioned iterative method based on generic approximate sparse pseudoinverse. Large scale evaluation experiments were conducted using two real-world traffic datasets, and the experimental results indicate that the proposed model can achieve better balance between forecasting accuracy and computational efficiency compared to benchmark models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005340",
    "keywords": [
      "Artificial intelligence",
      "Autoregressive model",
      "Benchmark (surveying)",
      "Computer science",
      "Data mining",
      "Econometrics",
      "Geodesy",
      "Geography",
      "Hyperparameter",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Physics",
      "Process (computing)",
      "Quantum mechanics",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Salamanis",
        "given_name": "Athanasios I."
      },
      {
        "surname": "Lipitakis",
        "given_name": "Anastasia-Dimitra"
      },
      {
        "surname": "Gravvanis",
        "given_name": "George A."
      },
      {
        "surname": "Kotsiantis",
        "given_name": "Sotiris"
      },
      {
        "surname": "Anagnostopoulos",
        "given_name": "Dimosthenis"
      }
    ]
  },
  {
    "title": "Multi-objective model for supplier selection and order allocation problem with fuzzy parameters",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115129",
    "abstract": "Disasters, such as Coronavirus pandemic and Japan’s earthquake and tsunami, negatively hits firms and markets. It may drastically increase market demand for some products, or decrease suppliers’ ability to supply them at right quantity, quality and time. This uncertainty can be modeled with the fuzzy set theory that is less data-demanding than the probability theory. When a supplier selection problem (SSP) is formulated by fuzzy mathematical programming technique, we have to address two issues: (1) fuzzy constraints, due to the uncertainty in demand and supply capacity, and (2) fuzzy coefficients, due to the uncertainty in defective and late delivery rates, etc. In this study, we develop a fuzzy multi-objective model for a SSP to address these two issues. We first develop a weighted additive function to transform the fuzzy multi-objective model to a fuzzy single-objective model that can effectively consider the decision makers’ preferences. Then, a resolution method is applied to solve the single-objective model with fuzzy parameters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005704",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Economics",
      "Epistemology",
      "Finance",
      "Fuzzy logic",
      "Fuzzy number",
      "Fuzzy set",
      "Fuzzy set operations",
      "Mathematical optimization",
      "Mathematics",
      "Membership function",
      "Operations research",
      "Order (exchange)",
      "Philosophy",
      "Programming language",
      "Quality (philosophy)",
      "Selection (genetic algorithm)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Firouzi",
        "given_name": "Fatemeh"
      },
      {
        "surname": "Jadidi",
        "given_name": "Omid"
      }
    ]
  },
  {
    "title": "An EEG based hierarchical classification strategy to differentiate five intensities of pain",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115010",
    "abstract": "Past research emphasized on revealing the pain in different bands of electroencephalogram (EEG) including alpha band. In this study, we proposed an accurate and robust manner to differentiate pain intensities by deeply characterizing the alpha band in terms of distribution, spectrum and complexity changes in response to five different intensities of pain. Here, 44 subjects executed the Cold Pressor Task (CPT) and experienced five defined levels of pain while their EEGs were recorded via 34 silver channels. After de-noising and filtering the EEGs through the alpha band, 12 informative features were extracted from each channel in successive time frames. Since none of the features could discriminate the five classes, we applied the Kruskal-Wallis test to the features for observing their distribution in differentiating two or more classes. According to this result, we designed a decision tree classifier, where a Bayes optimized support vector machine (BSVM) was selected in each decision node. Sequential forward selection was applied in order to customize a subset of features for each BSVM. Our results provided 93.33% accuracy over the five classes and also generate 99.8% accuracy for separating pain and no-pain classes, which is statistically superior (P < 0.05) to state-of-the-art methods over our collected dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004516",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Decision tree",
      "Electroencephalography",
      "Naive Bayes classifier",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Psychology",
      "Random forest",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Afrasiabi",
        "given_name": "Somayeh"
      },
      {
        "surname": "Boostani",
        "given_name": "Reza"
      },
      {
        "surname": "Masnadi-Shirazi",
        "given_name": "Mohammad Ali"
      },
      {
        "surname": "Nezam",
        "given_name": "Tahereh"
      }
    ]
  },
  {
    "title": "Procedural generation of dungeons’ maps and locked-door missions through an evolutionary algorithm validated with players",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115009",
    "abstract": "The present research introduces an evolutionary algorithm able to procedurally generate dungeon maps containing locked door missions. The evolutionary algorithm evolves a tree structure, which encodes dungeons, aiming to generate levels close to the input configuration provided by a game designer. The tree structure holds information about the number of rooms, connections between them, and their position within a 2D map. The proposed encoding also allows evolving semantic information about the narrative of the game. This is done by feasibly setting keys and locks throughout the dungeons for locked door missions. The generated dungeons are then evaluated computationally and as a proof of concept using an adventure game prototype. A total of 70 players evaluated the contents and the results show that the procedurally generated levels are perceived as more human-made, fun, and difficult than their human-made counterparts for most cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004504",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Evolutionary algorithm"
    ],
    "authors": [
      {
        "surname": "Pereira",
        "given_name": "Leonardo Tortoro"
      },
      {
        "surname": "Prado",
        "given_name": "Paulo Victor de Souza"
      },
      {
        "surname": "Lopes",
        "given_name": "Rafael Miranda"
      },
      {
        "surname": "Toledo",
        "given_name": "Claudio Fabiano Motta"
      }
    ]
  },
  {
    "title": "Knowledge interoperability and re-use in Empathy Mapping: an ontological approach",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115065",
    "abstract": "Design Thinking is a human-centered approach extensively used across different domains that aims at problem solving, value creation for stakeholders and innovation by fostering creativity. The most characterising and critical step along the Design Thinking process is the empathy phase, in which stakeholder analysis is performed by looking at a given scenario from the perspective of different stakeholders. Such a methodology enables a systematic information gathering and organization that results in a deep understanding of actual problems, needs and expectations from the target stakeholders. The uniqueness of problems and the need for situation-specific data makes knowledge re-use not always practical, even within the most consolidated and experienced environments. In this paper we propose an ontological support to empathy mapping that aims to (i) establish an interoperable fine-grained data layer among the different data collected throughout the empathy mapping process, (ii) enable multi-scenario analysis underpinned by formal specifications and (iii) further empower the process through semantic enrichment and integration of insight from multiple sources and contexts. We believe this is the first step to design and properly integrate effective computational and AI-based functionalities along the creative design thinking process, as well as to enable in practice richer and more sophisticated approaches (e.g. through social networks).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005066",
    "keywords": [
      "Computer science",
      "Data science",
      "Design thinking",
      "Empathy",
      "Epistemology",
      "Human–computer interaction",
      "Interoperability",
      "Knowledge management",
      "Ontology",
      "Operating system",
      "Philosophy",
      "Political science",
      "Process (computing)",
      "Psychiatry",
      "Psychology",
      "Public relations",
      "Semantic interoperability",
      "Stakeholder",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Pileggi",
        "given_name": "Salvatore F."
      }
    ]
  },
  {
    "title": "Dissecting a data-driven prognostic pipeline: A powertrain use case",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115109",
    "abstract": "Nowadays, cars are instrumented with thousands of sensors continuously collecting data about its components. Thanks to the concept of connected cars, this data can be now transferred to the cloud for advanced analytics functionalities, such as prognostic or predictive maintenance. In this paper, we dissect a data-driven prognostic pipeline and apply it in the automotive scenario. Our pipeline is composed of three main steps: (i) selection of most important signals and features describing the scenario for the target problem, (ii) creation of machine learning models based on different classification algorithms, and (iii) selection of the model that works better for a deployment scenario. For the development of the pipeline, we exploit an extensive experimental campaign where an actual engine runs in a controlled test bench under different working conditions. We aim to predict failures of the High-Pressure Fuel System, a key part of the diesel engine responsible for delivering high-pressure fuel to the cylinders for combustion. Our results show the advantage of data-driven solutions to automatically discover the most important signals to predict failures of the High-Pressure Fuel System. We also highlight how an accurate model selection step is fundamental to identify a robust model suitable for deployment.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005509",
    "keywords": [
      "Aerospace engineering",
      "Artificial intelligence",
      "Automotive industry",
      "Computer science",
      "Computer security",
      "Data mining",
      "Embedded system",
      "Engineering",
      "Exploit",
      "Machine learning",
      "Model selection",
      "Physics",
      "Pipeline (software)",
      "Powertrain",
      "Programming language",
      "Selection (genetic algorithm)",
      "Software deployment",
      "Software engineering",
      "Test bench",
      "Thermodynamics",
      "Torque"
    ],
    "authors": [
      {
        "surname": "Giordano",
        "given_name": "Danilo"
      },
      {
        "surname": "Pastor",
        "given_name": "Eliana"
      },
      {
        "surname": "Giobergia",
        "given_name": "Flavio"
      },
      {
        "surname": "Cerquitelli",
        "given_name": "Tania"
      },
      {
        "surname": "Baralis",
        "given_name": "Elena"
      },
      {
        "surname": "Mellia",
        "given_name": "Marco"
      },
      {
        "surname": "Neri",
        "given_name": "Alessandra"
      },
      {
        "surname": "Tricarico",
        "given_name": "Davide"
      }
    ]
  },
  {
    "title": "A cross-modal fusion based approach with scale-aware deep representation for RGB-D crowd counting and density estimation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115071",
    "abstract": "To simultaneously estimate the crowd count and the density map from the crowd images, this paper presents a novel cross-modal fusion based approach for RGB-D crowd counting. For the RGB-D crowd counting task, the depth data is often utilized into the procedure of detecting heads of the crowd to enhance the counting performance so as to reduce the underestimation from the small heads of the crowd. Different from the traditional methods utilizing depth image for the target task, the proposed approach is essentially designed as a density estimation-based regression framework to learn the more abundant deep representation from the original images through cross-modal interactions in multiple locations of the framework, which is more beneficial to crowd counting in various scenes, especially the congested scenes. Meanwhile, modeling the global and local contexts is designed to facilitate the proposed approach to learn the more adequate scale-aware representation for the counting task. Extensive experiments on MICC and large-scale ShanghaiRGBD benchmarks demonstrate that the performance of the proposed approach is superior to the state-of-the-art methods for RGB-D crowd counting and density estimation. Further, the proposed approach could be extended to RGB crowd counting task and the experimental results show that it achieves the comparable performance with the existing crowd counting methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005121",
    "keywords": [
      "Artificial intelligence",
      "Cartography",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Density estimation",
      "Economics",
      "Estimator",
      "Geography",
      "Image (mathematics)",
      "Law",
      "Management",
      "Mathematics",
      "Modal",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Polymer chemistry",
      "RGB color model",
      "Representation (politics)",
      "Scale (ratio)",
      "Statistics",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Shihui"
      },
      {
        "surname": "Li",
        "given_name": "He"
      },
      {
        "surname": "Kong",
        "given_name": "Weihang"
      }
    ]
  },
  {
    "title": "A language-independent authorship attribution approach for author identification of text documents",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115139",
    "abstract": "In the Authorship Attribution (AA) task, the most likely author of textual documents, such as books, papers, news, and text messages and posts are identified using statistical and computational methods. In this paper, a new computational approach is presented for identifying the most likely author of text documents. The proposed solution emphasizes lazy profile-based classification and, by using the Term Frequency-Inverse Document Frequency (TF_IDF) scheme, introduces a new measure for identifying important terms of documents. The importance of the terms is then used to calculate the similarity between an anonymous document and known documents. The proposed solution works with raw text documents and does not require any NLP tools for preprocessing, which makes it language-independent. The efficiency of the proposed solution has been evaluated by conducting several experiments on two English and Persian datasets, each of which contains six corpora with different number of authors. The obtained results demonstrate that the proposed solution outperforms state-of-the-art stylometric features, employed by seven well-known classifiers, by obtaining 0.902 accuracy for the English dataset and 0.931 accuracy for the Persian dataset. In addition, supplementary experiments have been conducted to evaluate the effects of documents’ length on the accuracy of the proposed solution, to examine the computation time of the proposed solution and competitive classifiers, and to identify the most effective stylometric features and classifiers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005807",
    "keywords": [
      "Artificial intelligence",
      "Authorship attribution",
      "Biology",
      "Botany",
      "Computer science",
      "Document classification",
      "Economics",
      "Identification (biology)",
      "Image (mathematics)",
      "Information retrieval",
      "Management",
      "Natural language processing",
      "Physics",
      "Preprocessor",
      "Quantum mechanics",
      "Similarity (geometry)",
      "Task (project management)",
      "Term (time)",
      "tf–idf"
    ],
    "authors": [
      {
        "surname": "Ramezani",
        "given_name": "Reza"
      }
    ]
  },
  {
    "title": "Universal pooling – A new pooling method for convolutional neural networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115084",
    "abstract": "Pooling is one of the key elements in a convolutional neural network. It reduces the feature map size, thereby enabling training with a limited amount of computation. The most common pooling methods are average pooling, max pooling, and stride pooling. The common pooling methods, however, have the disadvantage that they can perform only specified and fixed pooling functions and thus have limited expressive power. In this paper, we propose a new pooling method named universal pooling (UP). UP performs different pooling functions depending on the training samples. UP is a general pooling and includes the previous common pooling methods as special cases. The structure of UP is inspired by attention methods. UP can actually be considered as a channel-wise local spatial attention module. It is quite different from attention-based feature reduction methods. We insert UP into a couple of popular networks and apply the networks to benchmark sets in two applications, namely, image recognition and semantic segmentation. The experiment results show that complex poolings are trained in the proposed UP and that UP achieves better performance than the previous pooling methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100525X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computation",
      "Computer science",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pooling",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Hyun",
        "given_name": "Junhyuk"
      },
      {
        "surname": "Seong",
        "given_name": "Hongje"
      },
      {
        "surname": "Kim",
        "given_name": "Euntai"
      }
    ]
  },
  {
    "title": "Characterization of Syrian refugees with work permit applications in Turkey: A data mining based methodology",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114846",
    "abstract": "With the technological advancements in data collection systems, data-driven approaches become a necessity for understanding and managing the socioeconomic systems. Motivated by this, we focus on the formal employment of Syrian refugees in Turkey, and propose a data mining based methodology in order to understand their profiles. In this context, Syrian refugees with work permit applications are examined between years 2010 and 2018. The dataset includes demographic properties of the applicants and characteristics of their workplaces. The proposed methodology aims to extract the hidden, interesting and useful characteristics of the Syrian refugees having formal employment potential. The proposed approach integrates several data mining tasks, i.e. clustering, classification, and association rule mining, and it has four phases. In the first phase, data pre-processing and visualization operations are performed. In the second phase, the profiles of the Syrian refugee workers are determined using clustering. Self-organizing map and hierarchical clustering are implemented for this purpose. In the third phase, decision tree is used to specify the distinguishing characteristics of the clusters. In the fourth phase, the association rules are generated to reveal the interesting and frequent properties of each cluster. The results reveal the profiles of Syrian refugees with work permit applications. The findings obtained from this study can be a basis for developing policies and strategies that facilitate the labor market integration of the immigrants. The proposed methodology can be used to analyze time-dependent patterns and other immigration data for different countries as well.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002876",
    "keywords": [
      "Archaeology",
      "Association rule learning",
      "Cluster analysis",
      "Computer science",
      "Context (archaeology)",
      "Data collection",
      "Data mining",
      "Data science",
      "Engineering",
      "Geography",
      "Hierarchical clustering",
      "Machine learning",
      "Mathematics",
      "Mechanical engineering",
      "Refugee",
      "Statistics",
      "Visualization",
      "Work (physics)"
    ],
    "authors": [
      {
        "surname": "Caglar Gencosman",
        "given_name": "Burcu"
      },
      {
        "surname": "İnkaya",
        "given_name": "Tülin"
      }
    ]
  },
  {
    "title": "A reduced variance unsupervised ensemble learning algorithm based on modern portfolio theory",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115085",
    "abstract": "Unsupervised ensemble learning or consensus clustering has gained popularity due to its ability to combine multiple clustering solutions into a single solution that is robust and often performs better than the individual ones. There have been several approaches to consensus clustering including voting and weighted voting algorithmic schemes. Although there have been several algorithms for adjusting the weights of a consensus clustering all of them are tuned based on some performance characteristic associated with clustering accuracy. In this paper, we propose a method for incorporating weights by taking into consideration the intra algorithmic variability i.e. algorithms that provide solutions with very different performance upon multiple runs. The methodology is inspired by modern portfolio theory and more specifically from Markowitz model for asset allocation where one is trying to identify the most efficient portfolio through the solution of a convex optimization problem. Here, efficiency is defined as the minimum amount of risk for an expected return. We apply this method to different datasets and compare with respect to performance and robustness. The proposed scheme appears to achieve competitive average performance with very low variability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005261",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Economics",
      "Ensemble learning",
      "Financial economics",
      "Gene",
      "Law",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Political science",
      "Politics",
      "Portfolio",
      "Robustness (evolution)",
      "Voting"
    ],
    "authors": [
      {
        "surname": "Ünlü",
        "given_name": "Ramazan"
      },
      {
        "surname": "Xanthopoulos",
        "given_name": "Petros"
      }
    ]
  },
  {
    "title": "Product processing prioritization in hybrid flow shop systems supported on Nash bargaining model and simulation-optimization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115066",
    "abstract": "Dynamic scheduling using real-time data in manufacturing systems enables quick response to unforeseen system events to reduce costs and makespan while increasing customer satisfaction. Since many production systems are multi-product, each product’s customers aim to receive the product in the shortest possible time, thus competing with each other. Extant research neglects to consider not only competition between customers, but also bargaining strategies. In this paper, a hybrid flow shop system with multi-product is regarded. The production system studied is the Alborz Tire Company (Iran), which uses multi-type machines subject to stochastic failure. The objective is to determine the product processing prioritization in workstations, based on the Nash bargaining model, to minimize makespan. To this end, a simulation–optimization approach based on discrete-event simulation and Simulated annealing is employed. The results of the case study show that makespan is reduced significantly for all players.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005078",
    "keywords": [
      "Algorithm",
      "Bargaining problem",
      "Computer science",
      "Economics",
      "Engineering",
      "Flow shop scheduling",
      "Geometry",
      "Job shop scheduling",
      "Mathematical optimization",
      "Mathematics",
      "Microeconomics",
      "Nash equilibrium",
      "Operating system",
      "Operations research",
      "Product (mathematics)",
      "Schedule",
      "Scheduling (production processes)",
      "Simulated annealing"
    ],
    "authors": [
      {
        "surname": "Malekpour",
        "given_name": "Hiva"
      },
      {
        "surname": "Hafezalkotob",
        "given_name": "Ashkan"
      },
      {
        "surname": "Khalili-Damghani",
        "given_name": "Kaveh"
      }
    ]
  },
  {
    "title": "Normalized nonconformity measures for automated valuation models",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115165",
    "abstract": "Automated valuation models are used by the real estate industry and financial institutions to provide estimated property values and automate the time-consuming process of property valuation. We apply conformal predictors to the problem of automated valuations to provide prediction intervals of estimated price that are guaranteed to be reliable at a confidence level set by the user, just assuming the data is independently and identically distributed. In this study, we use the rich Ames data set of house characteristics and sale prices over 2006 to 2010 and explore alternative nonconformity measures, used as part of the conformal predictors, based on normalization techniques and model averaging. We find that we can produce efficient prediction intervals using these methods that are competitive with existing commercial automated valuation models and have the additional benefit of guaranteed reliability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421006059",
    "keywords": [
      "Computer science",
      "Data mining",
      "Econometrics",
      "Economics",
      "Finance",
      "Independent and identically distributed random variables",
      "Mathematics",
      "Random variable",
      "Real estate",
      "Statistics",
      "Valuation (finance)"
    ],
    "authors": [
      {
        "surname": "Lim",
        "given_name": "Zhe"
      },
      {
        "surname": "Bellotti",
        "given_name": "Anthony"
      }
    ]
  },
  {
    "title": "Extracting control variables of casting processes with NMF and rule extraction",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115118",
    "abstract": "Process monitoring and optimization in an industrial setting is a complex topic, a large amount of data is captured and its measurements are affected by many complex relations. There is, therefore, a growing necessity for methods to reduce its complexity and dimension such that it can be interpreted by humans. In this work we study the extraction of interpretable rules derived from components of a non-negative matrix factorization using real data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005595",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Control (management)",
      "Data mining",
      "Dimension (graph theory)",
      "Eigenvalues and eigenvectors",
      "Extraction (chemistry)",
      "Machine learning",
      "Mathematics",
      "Matrix decomposition",
      "Non-negative matrix factorization",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Process (computing)",
      "Pure mathematics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Xarez",
        "given_name": "M."
      },
      {
        "surname": "Weiderer",
        "given_name": "P."
      },
      {
        "surname": "Tomé",
        "given_name": "A.M."
      },
      {
        "surname": "Lang",
        "given_name": "Elmar W."
      }
    ]
  },
  {
    "title": "Study on strategy of CT image sequence segmentation for liver and tumor based on U-Net and Bi-ConvLSTM",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115008",
    "abstract": "Accurate segmentation of the liver and tumors in computed tomography (CT) images is critical for intelligent computer-aided diagnosis (CAD). The commonly used segmentation methods based on fully convolutional networks (FCN) only take a single image into consideration but do not make good use of sequence information. In this paper, two more feasible sequence segmentation strategies than 3D U-net which can utilize inter-slice and intra-slice features simultaneously at the lower hardware and time cost are studied to improve the segmentation result. U-net serves as the backbone model of segmentation and Bi-directional convolutional long short-term memory (Bi-ConvLSTM) is chosen to extract and fuse the inter-slice feature. Strategy A corrects the pre-segmented results of U-net in the fusion of sequence information as a post-processing, where Mod-1, Mod-2 and Mod-3 models are built to compare the effects of width, depth, and residual structure on the modified model of sequence segmentation. Strategy B directly integrates the fusion of sequence information into the feature extraction of U-net, and then an end-to-end model called W-net is built based on it. The experiment results show that both strategies improve the liver and tumor segmentation performance in various aspects. The results based on strategy A are closer to the ground truth with less misdiagnose region: Mod-1 achieves better accuracy on liver contour segmentation because of the largest model width; Mod-2 can obtain more accurate tumor contour since the greatest depth of feature extraction process; and Mod-3 is at the average segmentation performance. Therefore, strategy A is recommended in the application of surgery planning of tumors. Strategy B achieves better space coincidence degree and less training time cost, which is more suitable for the early screening of liver cancer.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004498",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Genetics",
      "Ground truth",
      "Image segmentation",
      "Linguistics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Segmentation",
      "Sequence (biology)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Jing"
      },
      {
        "surname": "Ou",
        "given_name": "Xue"
      },
      {
        "surname": "Shen",
        "given_name": "Nanyan"
      },
      {
        "surname": "Sun",
        "given_name": "Jie"
      },
      {
        "surname": "Ding",
        "given_name": "Junli"
      },
      {
        "surname": "Zhang",
        "given_name": "Jiawen"
      },
      {
        "surname": "Yao",
        "given_name": "Jia"
      },
      {
        "surname": "Wang",
        "given_name": "Ziyan"
      }
    ]
  },
  {
    "title": "Bio-inspired optimization of weighted-feature machine learning for strength property prediction of fiber-reinforced soil",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115042",
    "abstract": "The fiber-reinforcement of soil is an effective and reliable ground improvement technique for increasing the strength and stability of soil for various purposes (including retaining structures, embankments, foundations, slopes and pavements). Numerous scholars have developed methods to identify factors that influence the shear strength and to predict the peak friction angle of fiber-reinforced soil (FRS). The accuracy of theoretical and empirical models for predicting the shear strength (peak friction angle) of FRS is questionable because of the difficulty of using these simplified models to describe the complex mechanism of soil-fiber interaction. Solutions to this problem require ever-increasing predictive accuracy, and ML-based methods have been confirmed to provide potential solutions to real-world engineering problems. Therefore, this study develops weighted-feature least squares support vector regression (WFLSSVR) that is optimized by a novel metaheuristic algorithm, jellyfish search (JS) algorithm, to predict the peak friction angle of FRS. Analytical results demonstrate that JS-WFLSSVR outperforms baseline, ensemble, and hybrid machine learning models as well as empirical methods in literature. Notably, analysis of the weight values that were obtained by JS-WFLSSVR enables the identification of new feature combinations that provide much higher accuracy than current models. Therefore, the JS-WFLSSVR model not only significantly provides better predictive accuracy than methods in the literature; it is also a good feature selection method, and can help geotechnical engineers in estimating the shear strength of FRS. Geotechnical engineers can use the proposed model to predict the shear strength and control the quality of FRS structures.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004838",
    "keywords": [
      "Artificial intelligence",
      "Composite material",
      "Computer science",
      "Epistemology",
      "Feature (linguistics)",
      "Fiber",
      "Linguistics",
      "Machine learning",
      "Materials science",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Property (philosophy)"
    ],
    "authors": [
      {
        "surname": "Chou",
        "given_name": "Jui-Sheng"
      },
      {
        "surname": "Truong",
        "given_name": "Dinh-Nhat"
      },
      {
        "surname": "Le",
        "given_name": "Thuy-Linh"
      },
      {
        "surname": "Thu Ha Truong",
        "given_name": "Thi"
      }
    ]
  },
  {
    "title": "Determination of COVID-19 pneumonia based on generalized convolutional neural network model from chest X-ray images",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115141",
    "abstract": "X-ray units have become one of the most advantageous candidates for triaging the new Coronavirus disease COVID-19 infected patients thanks to its relatively low radiation dose, ease of access, practical, reduced prices, and quick imaging process. This research intended to develop a reliable convolutional-neural-network (CNN) model for the classification of COVID-19 from chest X-ray views. Moreover, it is aimed to prevent bias issues due to the database. Transfer learning-based CNN model was developed by using a sum of 1,218 chest X-ray images (CXIs) consisting of 368 COVID-19 pneumonia and 850 other pneumonia cases by pre-trained architectures, including DenseNet-201, ResNet-18, and SqueezeNet. The chest X-ray images were acquired from publicly available databases, and each individual image was carefully selected to prevent any bias problem. A stratified 5-fold cross-validation approach was utilized with a ratio of 90% for training and 10% for the testing (unseen folds), in which 20% of training data was used as a validation set to prevent overfitting problems. The binary classification performances of the proposed CNN models were evaluated by the testing data. The activation mapping approach was implemented to improve the causality and visuality of the radiograph. The outcomes demonstrated that the proposed CNN model built on DenseNet-201 architecture outperformed amongst the others with the highest accuracy, precision, recall, and F1-scores of 94.96%, 89.74%, 94.59%, and 92.11%, respectively. The results indicated that the reliable diagnosis of COVID-19 pneumonia from CXIs based on the CNN model opens the door to accelerate triage, save critical time, and prioritize resources besides assisting the radiologists.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005820",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Chest radiograph",
      "Computer science",
      "Convolutional neural network",
      "Coronavirus disease 2019 (COVID-19)",
      "Data set",
      "Deep learning",
      "Disease",
      "Infectious disease (medical specialty)",
      "Internal medicine",
      "Machine learning",
      "Medicine",
      "Overfitting",
      "Pathology",
      "Pattern recognition (psychology)",
      "Pneumonia",
      "Radiography",
      "Radiology",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Alhudhaif",
        "given_name": "Adi"
      },
      {
        "surname": "Polat",
        "given_name": "Kemal"
      },
      {
        "surname": "Karaman",
        "given_name": "Onur"
      }
    ]
  },
  {
    "title": "Multi-step influenza outbreak forecasting using deep LSTM network and genetic algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115153",
    "abstract": "Influenza epidemic is a serious public health problem that has attracted worldwide attention due to cause cases of severe illness, an enormous economic burden, and even deaths worldwide each year. Forecasting influenza outbreak in advance has great significance on influenza-like illness (ILI) prevention and healthcare management. Existing research approaches based on traditional statistical and machine learning have failed to select superior features that detect sophisticated and non-linear characteristics of influenza epidemic sequential data. In this paper, it is introduced a hybrid method that combines long short-term memory (LSTM) neural network and genetic algorithm (GA) for multi-step influenza outbreak forecasting problems. LSTM model is employed to overcome the complexity and nonlinearity issues in an influenza prediction. In order to enhance the efficiency and performance of the neural network, the genetic algorithm is used to obtain the epoch size of the network, the number of LSTM layers, the size of units in each LSTM layer, and the time window size simultaneously. For comparison purposes, it is chosen the weekly data of influenza-like illness (ILI), also known as influenza or other similar illness showing flu-like symptoms, in the USA collected by the Centers for Disease Control and Prevention (CDC). The experimental results demonstrated that the presented hybrid model outperforms other highly developed machine learning approaches, a statistical model, and a fully-connected neural network considering different performance metrics during peak periods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005947",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Deep learning",
      "Extreme learning machine",
      "Genetic algorithm",
      "Machine learning",
      "Medicine",
      "Outbreak",
      "Recurrent neural network",
      "Virology"
    ],
    "authors": [
      {
        "surname": "Kara",
        "given_name": "Ahmet"
      }
    ]
  },
  {
    "title": "High-end equipment data desensitization method based on improved Stackelberg GAN",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114989",
    "abstract": "High-end equipment refers to a type of technical equipment with high technical content, large capital investment, and long development cycle. Therefore, high-end equipment data has extraordinary significance and its desensitization is an urgent problem in data analysis. Traditional data desensitization principles are processing original data such as substitution and adding noise. These methods may not only damage data correlation information, but also result in data disclosure and high computing cost. Given the aforementioned reasons, the study proposes a high-end equipment data desensitization method based on improved Stackelberg Generative Adversarial Networks (GAN). When compared with the normal GAN, the structure proposed in the study includes more generators and discriminators. By inputting the original data, the trained GAN can output indistinguishable data from the original data which helps data mining and also ensures the privacy of data. We experimented on two datasets: optimal improvement was determined by Gaussian dataset experiments, i.e. Stackelberg GAN with eight discriminators. The second experiment results on real-world datasets proved that the 8-discriminator Stackelberg GAN better fits the original data and significantly aids data desensitization.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004309",
    "keywords": [
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Desensitization (medicine)",
      "Detector",
      "Discriminator",
      "Mathematical economics",
      "Mathematics",
      "Receptor",
      "Stackelberg competition",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Xiang",
        "given_name": "Nan"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiongtao"
      },
      {
        "surname": "Dou",
        "given_name": "Yajie"
      },
      {
        "surname": "Xu",
        "given_name": "Xiangqian"
      },
      {
        "surname": "Yang",
        "given_name": "Kewei"
      },
      {
        "surname": "Tan",
        "given_name": "Yuejin"
      }
    ]
  },
  {
    "title": "A novel image-based convolutional neural network approach for traffic congestion estimation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115037",
    "abstract": "Traditional image-based traffic congestion estimation methods generally include two steps, which first extract the vehicles from the surveillance images, then calculate the congestion index using the vehicle counts. When working with vast amount of video frames, these approaches are time-consuming and hardly guarantee the real time detection of traffic congestion. In this study, firstly a specific and accurate definition of traffic congestion is proposed to quantify the level of traffic congestion. Then we construct an image-based traffic congestion estimation framework, in which a traffic parameter layer is integrated to the basic convolutional neural network (CNN) model. The proposed framework can directly perform traffic congestion calculation and estimation, which shortens the processing time and avoids the complicated postprocessing. A dataset of 1400 traffic images including 66,890 vehicles is collected for training the proposed CNN model. Another new dataset of 2400 traffic images including 113,516 vehicles is collected to test the proposed method on estimating traffic congestion. Experimental results show that our proposed approach has better efficiency and stability in both free flow and congested traffic conditions, as well as sunny and rainy scenes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004784",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer network",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Engineering",
      "Floating car data",
      "Image (mathematics)",
      "Network congestion",
      "Network packet",
      "Network traffic control",
      "Real-time computing",
      "Traffic congestion",
      "Traffic congestion reconstruction with Kerner's three-phase theory",
      "Traffic flow (computer networking)",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Ying"
      },
      {
        "surname": "Li",
        "given_name": "Jinlong"
      },
      {
        "surname": "Xu",
        "given_name": "Zhigang"
      },
      {
        "surname": "Liu",
        "given_name": "Zhangqi"
      },
      {
        "surname": "Zhao",
        "given_name": "Xiangmo"
      },
      {
        "surname": "Chen",
        "given_name": "Jianhua"
      }
    ]
  },
  {
    "title": "Global citation recommendation employing generative adversarial network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114888",
    "abstract": "The variety and plethora of research papers available on the Web motivated researchers to propose models that could assist users with personalized citation recommendations. In recent years, citation recommendation models using Network Representation Learning (NRL) methods have shown promising results. Nevertheless, existing NRL-based models are limited in terms of exploiting semantic relations and contextual information between the objects of bibliographic papers’ networks. Additionally, these models cannot adequately explore the structure of heterogeneous information networks, topical relevance, and relevant semantics. Consequently, they suffer from network sparsity and inadequate personalization problems. To overcome these shortcomings, we present a network embedding model termed as Global Citation Recommendation employing Generative Adversarial Network (GCR-GAN). The proposed model exploits the Heterogeneous Bibliographic Network (HBN) to generate personalized citation recommendations. In particular, the proposed model utilizes semantic relations corresponding to the objects of the heterogeneous bibliographic network and captures network structure proximity employing the Scientific Paper Embeddings using Citation-informed Transformers (SPECTER) and Denoising Auto-encoder networks to learn semantic-preserving graph representations. Compared to baseline models, the recommendations generated by our model over the DBLP and ACM datasets prove that it outperforms baseline methods by gaining almost 11% and 12% improvement in terms of Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (nDCG) metrics, respectively. Furthermore, we analyzed the effectiveness of the proposed model considering network sparsity issue, where our model gains almost 7% better recall@100 score against the second-best counterpart.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003298",
    "keywords": [
      "Artificial intelligence",
      "Baseline (sea)",
      "Citation",
      "Computer science",
      "Computer security",
      "Exploit",
      "Geology",
      "Information retrieval",
      "Learning to rank",
      "Oceanography",
      "Programming language",
      "Ranking (information retrieval)",
      "Recommender system",
      "Semantics (computer science)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Ali",
        "given_name": "Zafar"
      },
      {
        "surname": "Qi",
        "given_name": "Guilin"
      },
      {
        "surname": "Muhammad",
        "given_name": "Khan"
      },
      {
        "surname": "Kefalas",
        "given_name": "Pavlos"
      },
      {
        "surname": "Khusro",
        "given_name": "Shah"
      }
    ]
  },
  {
    "title": "An integrated framework with machine learning and radiomics for accurate and rapid early diagnosis of COVID-19 from Chest X-ray",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115152",
    "abstract": "The objective of the research article is to propose and validate a combination of machine learning and radiomics features to detect COVID-19 early and rapidly from chest X-ray (CXR) in presence of other viral/bacterial pneumonia and at different severity levels of diseases. It is vital to assess the performance of any diagnosis method on an independent data set and at very early stage of the disease when the disease severity of is very low. In such cases, most of the diagnosis methods fail. A total of 378 CXR images containing both normal lung and pneumonia (both COVID-19 and others lung conditions) were collected from publically available data set. 71 radiomics features for each lung segment were chosen from 100 extracted features based on Z-score heatmap and one way ANOVA test that can detect COVID-19. Three best performing classical machine learning algorithms during the training phase - 1) fine Gaussian support vector machine (SVM), 2) fine k-nearest neighbor (KNN) and 3) ensemble bagged model (EBM) trees were chosen for further evaluation on an independent test data set. The independent test data set consists of 115 COVID-19 CXR images collected from a local hospital and 100 CXR images collected from publically available data set containing normal lung and viral/bacterial pneumonia. Severity was scored between 0 to 4 by two experienced radiologists for each lung with pneumonia (both COVID-19 and non COVID-19) for the test data set. Ensemble Bagging Model Trees (EBM) with the selected radiomics features is the most suitable to distinguish between COVID-19 and other lung infections with an overall sensitivity of 87.8% and specificity of 97% (95.2% accuracy and 0.9228 area under curve) and is robust across severity levels. The method also can detect COVID-19 from CXR when two experienced radiologists were unable to detect any abnormality in the lung CXR (represented by severity score of 0). Once the CXR is acquired and lung is segmented, it takes less than two minutes for extracting radiomics features and providing diagnosis result. Since the proposed method does not require any manual intervention (e.g., sample collection etc.), it can be straightway integrated with standard X-ray reporting system to be used as an efficient, cost-effective and rapid early diagnosis device.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005935",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Data set",
      "Decision tree",
      "Disease",
      "Infectious disease (medical specialty)",
      "Internal medicine",
      "Machine learning",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)",
      "Pneumonia",
      "Programming language",
      "Radiomics",
      "Set (abstract data type)",
      "Support vector machine",
      "Test set"
    ],
    "authors": [
      {
        "surname": "Tamal",
        "given_name": "Mahbubunnabi"
      },
      {
        "surname": "Alshammari",
        "given_name": "Maha"
      },
      {
        "surname": "Alabdullah",
        "given_name": "Meernah"
      },
      {
        "surname": "Hourani",
        "given_name": "Rana"
      },
      {
        "surname": "Alola",
        "given_name": "Hossain Abu"
      },
      {
        "surname": "Hegazi",
        "given_name": "Tarek M."
      }
    ]
  },
  {
    "title": "A pareto-based ensemble of feature selection algorithms",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115130",
    "abstract": "In this paper, ensemble feature selection is modeled as a bi-objective optimization problem regarding features’ relevancy and redundancy degree. The proposed method, which is called PEFS, first uses the modeled bi-objective optimization problem to find the non-dominated features based on the decision matrix constructed by different feature selection algorithms. In the second step, the found non-dominated features are sorted using the crowding distance in the bi-objective space. These sorted features remove from the feature space, and the process of finding the non-dominated features will continue until all the features are sorted. To illustrate the optimality and efficiency of the proposed method, we have compared our approach with some ensemble feature selection methods and basic algorithms used in the ensemble process. The results show that our method in terms of accuracy and F-score is superior to other similar methods and performs in a short running-time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005716",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Ensemble learning",
      "Feature (linguistics)",
      "Feature selection",
      "Linguistics",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Multi-objective optimization",
      "Operating system",
      "Pareto principle",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Redundancy (engineering)",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Hashemi",
        "given_name": "Amin"
      },
      {
        "surname": "Bagher Dowlatshahi",
        "given_name": "Mohammad"
      },
      {
        "surname": "Nezamabadi-pour",
        "given_name": "Hossein"
      }
    ]
  },
  {
    "title": "A metaheuristic for the rural school bus routing problem with bell adjustment",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115086",
    "abstract": "This paper addresses the school bus routing problem with bell adjustments. This problem extends the traditional school bus routing problem by having the school working times as decision variables instead of input data. Adjusting schools working times (bell adjustment) increases managerial flexibility to lower transportation costs. On the other hand, this comes at the price of having to deal with more complex solution design and greater computational complexity, as underlaid by the scarce literature on the theme. Here we propose different bell adjustment strategies for a rural variant of the problem which has particular importance both economically and socially for developing countries that usually have schooling with multiple shifts and budget restrictions. The memetic algorithm combines an iterated local search with specialized neighborhood structures arranged in a variable neighborhood descent strategy and enriched with a diversification scheme that relies on an elite set to solve large scale real instances. Different bell adjustment strategies are richly explained, tested, and analyzed thoroughly. The results of statistical analysis show significant cost savings for both cases with or without multi-loading. The new strategy achieved up to 9% savings and 2.55% savings on the consolidated results. Instances with a lower number of vehicles and a higher number of schools presented higher savings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005273",
    "keywords": [
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Economics",
      "Flexibility (engineering)",
      "Iterated local search",
      "Management",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Operations research",
      "Routing (electronic design automation)",
      "Variable neighborhood search"
    ],
    "authors": [
      {
        "surname": "Miranda",
        "given_name": "Douglas M."
      },
      {
        "surname": "de Camargo",
        "given_name": "Ricardo S."
      },
      {
        "surname": "Conceição",
        "given_name": "Samuel V."
      },
      {
        "surname": "Porto",
        "given_name": "Marcelo F."
      },
      {
        "surname": "Nunes",
        "given_name": "Nilson T.R."
      }
    ]
  },
  {
    "title": "Global citation recommendation employing generative adversarial network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114888",
    "abstract": "The variety and plethora of research papers available on the Web motivated researchers to propose models that could assist users with personalized citation recommendations. In recent years, citation recommendation models using Network Representation Learning (NRL) methods have shown promising results. Nevertheless, existing NRL-based models are limited in terms of exploiting semantic relations and contextual information between the objects of bibliographic papers’ networks. Additionally, these models cannot adequately explore the structure of heterogeneous information networks, topical relevance, and relevant semantics. Consequently, they suffer from network sparsity and inadequate personalization problems. To overcome these shortcomings, we present a network embedding model termed as Global Citation Recommendation employing Generative Adversarial Network (GCR-GAN). The proposed model exploits the Heterogeneous Bibliographic Network (HBN) to generate personalized citation recommendations. In particular, the proposed model utilizes semantic relations corresponding to the objects of the heterogeneous bibliographic network and captures network structure proximity employing the Scientific Paper Embeddings using Citation-informed Transformers (SPECTER) and Denoising Auto-encoder networks to learn semantic-preserving graph representations. Compared to baseline models, the recommendations generated by our model over the DBLP and ACM datasets prove that it outperforms baseline methods by gaining almost 11% and 12% improvement in terms of Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (nDCG) metrics, respectively. Furthermore, we analyzed the effectiveness of the proposed model considering network sparsity issue, where our model gains almost 7% better recall@100 score against the second-best counterpart.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003298",
    "keywords": [
      "Artificial intelligence",
      "Baseline (sea)",
      "Citation",
      "Computer science",
      "Computer security",
      "Exploit",
      "Geology",
      "Information retrieval",
      "Learning to rank",
      "Oceanography",
      "Programming language",
      "Ranking (information retrieval)",
      "Recommender system",
      "Semantics (computer science)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Ali",
        "given_name": "Zafar"
      },
      {
        "surname": "Qi",
        "given_name": "Guilin"
      },
      {
        "surname": "Muhammad",
        "given_name": "Khan"
      },
      {
        "surname": "Kefalas",
        "given_name": "Pavlos"
      },
      {
        "surname": "Khusro",
        "given_name": "Shah"
      }
    ]
  },
  {
    "title": "Adaptive center pixel selection strategy in Local Binary Pattern for texture classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115123",
    "abstract": "Local Binary Pattern (LBP) is widely used in texture classification because of its powerful capability to extract texture features of a center pixel. However, LBP has three main drawbacks: (1) limited by the low resolution of imaging device, the quality of texture image is degraded, and some real existing pixels with more texture information are unavoidably lost. (2) Center pixel g c is the most important factor to extract correct LBP pattern. However, by far LBP and its variants do not show any solutions to enhance the robustness of center pixel g c . (3) Some LBP patterns include important texture microstructures, but they are ignored by uniform patterns. At the same time, some uniform patterns can be corrupted by noise and misclassified into non-uniform patterns. These LBP patterns therefore all lost their discrimination capability. In order to overcome these three disadvantages, in this paper, we propose a novel adaptive center pixel selection (ACPS) strategy. Inspired by image super-resolution techniques, ACPS firstly applies the interpolation method to recover the lost real existing pixels and generate the center pixel candidates with more texture information. Then, the gradient information is used to obtain the edge image aiming to find the non-uniform patterns at edge points which may contain complicated texture microstructures. After generating the center pixel candidates and edge image, we introduce ACPS strategy into the LBP framework. By adaptively selecting the optimal center pixel from all center pixel candidates, one non-uniform pattern at the edge point can be possibly recovered to the uniform pattern, and regain its discrimination power. It is worth noting that any other LBP variants can also employ the ACPS strategy to more effectively extract its texture features. By observing the experimental results on representative texture databases of Outex, UIUC, CUReT, XU_HR, ALOT and KTHTIPS2b after introducing the ACPS strategy into LBP and its variants of LTP, CLBP, BRINT, CRDP, FbLBP, and CJLBP, the texture classification performances can be significantly improved.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005649",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Enhanced Data Rates for GSM Evolution",
      "Gene",
      "Histogram",
      "Image (mathematics)",
      "Image processing",
      "Image texture",
      "Local binary patterns",
      "Pattern recognition (psychology)",
      "Pixel",
      "Robustness (evolution)",
      "Texture (cosmology)"
    ],
    "authors": [
      {
        "surname": "Pan",
        "given_name": "Zhibin"
      },
      {
        "surname": "Hu",
        "given_name": "Shiqi"
      },
      {
        "surname": "Wu",
        "given_name": "Xiuquan"
      },
      {
        "surname": "Wang",
        "given_name": "Ping"
      }
    ]
  },
  {
    "title": "A socially motivating and environmentally friendly tour recommendation framework for tourist groups",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115083",
    "abstract": "Traveling in a group brings various social and environmental benefits, yet members might have different (and sometime conflicting) preferences. In this study, a tour recommendation framework is proposed that receives a set of must-visit and preferred points of interest from each tourist and forms multi-day tours that cover all must-visit points. Furthermore, the framework attempts to maximize fairness among group members. This ensures all members are motivated to participate in the group tour. While generating the itinerary, the framework also guarantees that a threshold on the commuted distance, time, and monetary budget is met on each day. The benefits of this approach are the maximization of social wellbeing and minimization of energy consumption. The advantages of the proposed framework are confirmed via a test using a Foursquare dataset of two major cities of New York and Tokyo and a user study.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005248",
    "keywords": [
      "Computer science",
      "Consumption (sociology)",
      "Cover (algebra)",
      "Economics",
      "Engineering",
      "Environmental economics",
      "Law",
      "Mathematics",
      "Maximization",
      "Mechanical engineering",
      "Microeconomics",
      "Minification",
      "Operations research",
      "Political science",
      "Programming language",
      "Set (abstract data type)",
      "Social science",
      "Sociology",
      "Tourism",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Kargar",
        "given_name": "Mehdi"
      },
      {
        "surname": "Lin",
        "given_name": "Zhibin"
      }
    ]
  },
  {
    "title": "Spectral decision in cognitive radio networks based on deep learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115080",
    "abstract": "Cognitive radio networks (CRN) have gained great relevance in the efficient use of the radio spectrum, and one of the key aspects of this technology is the spectral decision. The performance of secondary user communication depends largely on the intelligent choice of an appropriate spectral opportunity. The purpose of this research is to propose and assess the performance of a spectral decision model for CRN based on the Deep Learning technique. To achieve this, a classifier was adapted through the feature extraction technique that identifies three levels of traffic (high, medium and low) in a spectral occupation experimental power matrix that models the primary user. The extraction of features is done by Deep Learning and the process of classifying the successful set of features is done by a Support Vector Machine (SVM). These were used along with five evaluation metrics—total handoffs, failed handoffs, bandwidth, delay and throughput—to measure the performance of the proposed spectral decision model based on the Deep Learning technique, and to compare the results with the Multi-Criteria Optimization and Compromise Solution (VIKOR), Technique for Order Preference by Similarity to Ideal Solution (TOPSIS), and Simple Additive Weighting (SAW). This work presents five contributions: incorporation of the real behavior of licensed users, implementation of performance metrics for spectral mobility, proposal of an RGB conversion algorithm based on the threshold level, feedback in the classifier and a methodology based on priorities and scores to establish the channels with the highest availability. The results of this evaluation show that the proposed model has a better performance in the five metrics compared to the other techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005212",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Cognitive radio",
      "Computer science",
      "Data mining",
      "Engineering",
      "Ideal solution",
      "Machine learning",
      "Medicine",
      "Operations research",
      "Physics",
      "Radiology",
      "Support vector machine",
      "TOPSIS",
      "Telecommunications",
      "Thermodynamics",
      "Weighting",
      "Wireless"
    ],
    "authors": [
      {
        "surname": "Giral",
        "given_name": "Diego"
      },
      {
        "surname": "Hernández",
        "given_name": "Cesar"
      },
      {
        "surname": "Salgado",
        "given_name": "Camila"
      }
    ]
  },
  {
    "title": "Differentially private and utility-aware publication of trajectory data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115120",
    "abstract": "Trajectory data is valuable for various applications, especially for intelligent transportation systems, which hunger for plenty of trajectories. However, publishing trajectory data while respecting users’ privacy has been a long-standing challenge. Currently, the prevailing releasing solutions usually merge trajectory locations based on k-means and add unbounded noise with Laplace distribution to the count of trajectory to achieve differential privacy protection. The trajectory merging methods based on k-means have a low efficiency and unbounded noise with Laplace distribution will leak user’ privacy and suffer from serious utility loss. To solve the above two problems, we devise two differentially private and utility-aware publication methods of trajectory data. More specifically, we first propose two trajectory merging schemes based on k-means || clustering. The first one is to use k-means || clustering algorithm to cluster the location area, and all the points in the cluster are replaced by the center of cluster. The other is to utilize Staircase mechanism to perturb the cluster centers in order to improve the level of privacy protection. Afterwards, we propose a bounded Staircase noise generation algorithm to perturb the true count of generalized trajectories. We prove our proposed methods preserve differential privacy theoretically. Experimental comparison show that our proposed publication methods significantly outperform existing approaches in terms of data utility and efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005613",
    "keywords": [
      "Astronomy",
      "Computer science",
      "Data mining",
      "Data science",
      "Information retrieval",
      "Physics",
      "Trajectory"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Qi"
      },
      {
        "surname": "Yu",
        "given_name": "Juan"
      },
      {
        "surname": "Han",
        "given_name": "Jianmin"
      },
      {
        "surname": "Yao",
        "given_name": "Xin"
      }
    ]
  },
  {
    "title": "Supervised discriminant Isomap with maximum margin graph regularization for dimensionality reduction",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115055",
    "abstract": "As one of the most popular nonlinear dimensionality reduction methods, Isomap has been widely used in pattern recognition and machine learning. However, Isomap has the following problems: (1) Isomap is an unsupervised dimensionality reduction method, it cannot use class label information to obtain discriminative low dimensional embedding for classification; (2) The embedding performance of Isomap is sensitive to neighborhood size parameter; (3) Isomap cannot deal with outside new data by direct embedding. In this paper, a novel dimensionality reduction method called supervised discriminant Isomap is proposed to solve the first two problems mentioned above. Specifically, first, raw data points are partitioned into different manifolds by using their class label information. Then, supervised discriminant Isomap aims at seeking an optimal nonlinear subspace to preserve the geometrical structure of each manifold according to the Isomap criterion, and to enhance the discriminating capability by maximizing the distances between data points of different classes and the maximum margin graph regularization term. Finally, the corresponding optimization problems are solved by using eigen-decomposition algorithm. Further, we extend supervised discriminant Isomap to a linear dimensionality reduction method called supervised discriminant Isomap projection for handling the above three problems. Moreover, our approaches have three important characteristics: (1) Proposed methods adaptively estimate the local neighborhood surrounding each sample based on data density and similarity; (2) The objective functions of proposed methods can maximize margins between the each classes in the dimension-reduced feature space; (3) The objective functions of proposed methods have closed-form solutions. Furthermore, our methods can capture more discriminative information from raw data than other Isomap based methods. Extensive experiments on nine data sets demonstrate that the proposed methods are superior to the related state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004966",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Computer science",
      "Data point",
      "Dimensionality reduction",
      "Isomap",
      "Linear discriminant analysis",
      "Mathematics",
      "Nonlinear dimensionality reduction",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Qu",
        "given_name": "Hongchun"
      },
      {
        "surname": "Li",
        "given_name": "Lin"
      },
      {
        "surname": "Li",
        "given_name": "Zhaoni"
      },
      {
        "surname": "Zheng",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "Dynamic few-view X-ray imaging for inspection of CAD-based objects",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115012",
    "abstract": "X-ray 3D Computed Tomography (3DCT) has great potential for inspection of industrial products. Unfortunately, conventional CT-based workflows to inspect objects created from a CAD model involve a time-consuming acquisition process, computationally expensive image reconstruction, and multiple postprocessing steps, preventing them from inline usage. In this paper, we propose DynaPose, a fast and yet accurate workflow for 3D X-ray inspection of objects created from a CAD model. By exploiting prior knowledge of the CAD model, and relying on only very few radiographs, inspection can be performed in real-time. The DynaPose method allows automated 3D pose estimation of the object to be inspected while dynamically acquiring radiographs that are optimal for the detection task. Through simulation and real experiments, we show that our approach paves the way for inline inspection of manufactured objects.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100453X",
    "keywords": [
      "Artificial intelligence",
      "CAD",
      "Computer graphics (images)",
      "Computer science",
      "Computer vision",
      "Database",
      "Engineering",
      "Engineering drawing",
      "Object (grammar)",
      "Operating system",
      "Process (computing)",
      "Systems engineering",
      "Task (project management)",
      "Workflow"
    ],
    "authors": [
      {
        "surname": "Presenti",
        "given_name": "Alice"
      },
      {
        "surname": "Sijbers",
        "given_name": "Jan"
      },
      {
        "surname": "De Beenhouwer",
        "given_name": "Jan"
      }
    ]
  },
  {
    "title": "The impact of artificial intelligence and big data on end-stage kidney disease treatments",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115076",
    "abstract": "In the field of medicine, decision-making has traditionally been carried out based on the best available scientific information and the experience of specialists using data found in analog formats such as radiographies, medical reports, and handwritten notes, among others. In this sense, the Big Data phenomenon is changing the world of medicine since the technologies that have been developed have made available to researchers and clinicians enormous amounts of data in digital formats that can be used to complement or help in complex tasks such as mentioned decision making. A key element in this process is data analysis techniques, since without them it is not possible to exploit the information. Currently the most used techniques are based on algorithms in the area of artificial intelligence and more specifically machine learning. This paper focuses on a specific domain of medicine, renal replacement therapies for end-stage renal disease, where machine learning is beginning to be used as a complementary tool to predict or make decisions. This paper provides a narrative review of the main machine learning methods that are being used to conduct end-stage renal disease treatment analyses.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005170",
    "keywords": [
      "Artificial intelligence",
      "Big data",
      "Computer science",
      "Computer security",
      "Data mining",
      "Data science",
      "Domain (mathematical analysis)",
      "Exploit",
      "Field (mathematics)",
      "Key (lock)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Process (computing)",
      "Pure mathematics"
    ],
    "authors": [
      {
        "surname": "Díez-Sanmartín",
        "given_name": "Covadonga"
      },
      {
        "surname": "Sarasa-Cabezuelo",
        "given_name": "Antonio"
      },
      {
        "surname": "Andrés Belmonte",
        "given_name": "Amado"
      }
    ]
  },
  {
    "title": "Analyzing the effects of the new labor law on outpatient nurse scheduling with law-fitting modeling and case studies",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115103",
    "abstract": "New labor laws that include additional legal requirements or amendments occasionally come into effect, and these new laws may also bring about challenges to common hospital operations such as outpatient nurse scheduling. This research thus examines changes in Taiwan’s new labor law that might affect this scheduling problem. A binary goal programming (BGP) model appropriate to this new law is established. In order to investigate how those changes (i.e., in rest time between working days, rest time between shifts, and total maximum weekly working hours) affect scheduling decisions, we apply the model to a case in a hospital currently utilizing a different model that fits the old law and find effective concessions made to the new legislation (e.g., the tension caused by the additional overtime evening shifts during scheduling can be mitigated). The comparisons indicate that the new law does seriously affect scheduling in terms of daily work patterns, number of working days, nursing workload, and related issues. Insights are gained, showing that associated changes in the law likewise affect the human resources (HR) setting of the hospital, with a critical trade-off between reductions in full-time nurses and reductions in part-time nurses. For further model applications, a larger empirical case facing similar migration problems and another anticipated virtual problem whose size is sufficiently large in practice are also solved efficiently. Given the lack of related topics analyzing these effects on nurse scheduling, this study offers several potentially worthwhile directions for future research (e.g., another different investigation under a common law system).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005443",
    "keywords": [
      "Affect (linguistics)",
      "Business",
      "Communication",
      "Computer science",
      "Economics",
      "Engineering",
      "Law",
      "Legislation",
      "Mathematics",
      "Mechanical engineering",
      "Operating system",
      "Operations management",
      "Operations research",
      "Overtime",
      "Political science",
      "Psychology",
      "Scheduling (production processes)",
      "Work (physics)",
      "Working time",
      "Workload"
    ],
    "authors": [
      {
        "surname": "Zhuang",
        "given_name": "Zheng-Yun"
      },
      {
        "surname": "Yu",
        "given_name": "Vincent F."
      }
    ]
  },
  {
    "title": "A novel embedding approach to learn word vectors by weighting semantic relations: SemSpace",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115146",
    "abstract": "In this study, we propose a novel embedding approach, called as SemSpace, to determine word vectors of synsets and to find the best weights for semantic relations. First, SemSpace finds the optimum weights to the semantic relations in WordNet by aligning them to values produced by human intelligence, and then, determines word vectors of synsets by adjusting euclidean distances among them. Proposed approach requires two inputs; first, a lexical-semantic network such as WordNet, second, a word-level similarity dataset generated by people. In the experiments, we used WordNet 3.0 data for the lexical-semantic network, and three (RG65, WS353, and MEN3K) benchmark testsets to align semantic weights. Using the aligned semantic weights and the determined word vectors, the obtained resultsresults on the benchmark testsets are compared with literature studies. According to the obtained results, it might be concluded that SemSpace is not only successful to find word level semantic similarity values and semantic weights, but also to discover new semantic relations with their semantic levels.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100587X",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Embedding",
      "Geodesy",
      "Geography",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Medicine",
      "Natural language processing",
      "Radiology",
      "Semantic Web",
      "Semantic computing",
      "Semantic equivalence",
      "Semantic network",
      "Semantic similarity",
      "Similarity (geometry)",
      "Weighting",
      "Word (group theory)",
      "Word embedding",
      "WordNet"
    ],
    "authors": [
      {
        "surname": "Orhan",
        "given_name": "Umut"
      },
      {
        "surname": "Tulu",
        "given_name": "Cagatay Neftali"
      }
    ]
  },
  {
    "title": "Multi-label graph node classification with label attentive neighborhood convolution",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115063",
    "abstract": "Learning with graph structured data is of great significance for many practical applications. A crucial and fundamental task in graph learning is node classification. In reality, graph nodes are often encoded with various attributes. In addition, the task is usually multi-labeled in nature. In this paper, we tackle the problem of multi-label graph node classification, by leveraging structure, attribute and label information simultaneously. Specifically, to obtain rational node feature representations, we propose an intuitive yet effective graph convolution module to aggregate local attribute information of a given node. Moreover, the homophily hypothesis motivates us to build a label attention module. By exploiting both input and output contextual representations, we utilize the additive attention mechanism and build a label-aware representation learning framework to measure the compatibility between pairs of node embeddings and label embeddings. The proposed novel neural network-based, multi-label classification method has been verified by extensive experiments conducted on five public-available benchmark datasets, including both attributed and non-attributed networks. The results demonstrate the effectiveness of the proposed model with respect to micro-F1, macro-F1 and Hamming loss, comparing with several state-of-the-art methods, including two relational neighbor classifiers and several popular graph neural network models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005042",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Feature learning",
      "Graph",
      "Homophily",
      "Machine learning",
      "Mathematics",
      "Multi-label classification",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Cangqi"
      },
      {
        "surname": "Chen",
        "given_name": "Hui"
      },
      {
        "surname": "Zhang",
        "given_name": "Jing"
      },
      {
        "surname": "Li",
        "given_name": "Qianmu"
      },
      {
        "surname": "Hu",
        "given_name": "Dianming"
      },
      {
        "surname": "Sheng",
        "given_name": "Victor S."
      }
    ]
  },
  {
    "title": "Sequential targeting: A continual learning approach for data imbalance in text classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115067",
    "abstract": "Text classification has numerous use cases including sentiment analysis, spam detection, document classification, hate speech detection, etc. In realistic settings, classification on text data confronts imbalanced data conditions where classes of interest usually compose a minor fraction. Deep neural networks used for text classification, such as recurrent neural networks and transformer networks, suffer from a lack of efficient methods addressing imbalanced data. Traditional data-level methods attempting to mitigate distributional skew include oversampling and undersampling. The oversampling methods destruct the quality of original language representation of the sparse data coming from minority classes whereas the undersampling methods fail to fully utilize the rich context of majority classes. We address such issues in data-driven approaches by enforcing continual learning on imbalanced data by partitioning the training data distribution into mutually exclusive subsets and performing continual learning, treating the individual subsets as distinct tasks. We demonstrate the effectiveness of our method through experiments on the IMDB dataset and constructed datasets from real-world data. The experimental results show that the proposed method improves by 56.38 %p on the IMDB dataset and by 16.89 %p and 34.76 %p on the constructed datasets compared to the baseline method in terms of the F1-score metric.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100508X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Bandwidth (computing)",
      "Classifier (UML)",
      "Computer network",
      "Computer science",
      "Data mining",
      "Labeled data",
      "Machine learning",
      "Oversampling",
      "Undersampling"
    ],
    "authors": [
      {
        "surname": "Jang",
        "given_name": "Joel"
      },
      {
        "surname": "Kim",
        "given_name": "Yoonjeon"
      },
      {
        "surname": "Choi",
        "given_name": "Kyoungho"
      },
      {
        "surname": "Suh",
        "given_name": "Sungho"
      }
    ]
  },
  {
    "title": "Robo-advisor using genetic algorithm and BERT sentiments from tweets for hybrid portfolio optimisation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115060",
    "abstract": "Robo-advisors are increasingly popular, with machine learning algorithms taking centre stage for researchers. However, classical financial theories and techniques, such as Constant Rebalancing (CRB) and Modern Portfolio Theory (MPT), can still be relevant by combining them with social media sentiments. In this study, we propose two novel models, namely Sentimental All-Weather (SAW) and Sentimental MPT (SMPT), which capture the up-to-date market conditions through Twitter sentiments via Google’s Bidirectional Transformer (BERT) model. Genetic Algorithm was used to optimise the models for different objectives including maximising cumulative returns and minimising volatility. Trained on tweets and the United States stock data from August 2018 to end December 2019, and tested on an out-of-sample period from January 2020 to April 2020, our proposed models achieved superior performance in terms of common measures of portfolio performance including Sharpe ratio, cumulative returns, and value-at-risk, compared to the following benchmarks: buy-and-hold SPY index, MPT model, and CRB model for an All-Weather Portfolio.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005017",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Econometrics",
      "Economics",
      "Engineering",
      "Finance",
      "Financial economics",
      "Financial market",
      "Genetic algorithm",
      "Horse",
      "Machine learning",
      "Mechanical engineering",
      "Paleontology",
      "Physics",
      "Portfolio",
      "Portfolio optimization",
      "Quantum mechanics",
      "Sharpe ratio",
      "Social media",
      "Stock (firearms)",
      "Stock market",
      "Stock market index",
      "Transformer",
      "Volatility (finance)",
      "Voltage",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Leow",
        "given_name": "Edmund Kwong Wei"
      },
      {
        "surname": "Nguyen",
        "given_name": "Binh P."
      },
      {
        "surname": "Chua",
        "given_name": "Matthew Chin Heng"
      }
    ]
  },
  {
    "title": "Multi-view gait recognition system using spatio-temporal features and deep learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115057",
    "abstract": "Systems based on physiological biometrics are ubiquitous but requires subject cooperation or high resolution to capture. Gait recognition is a great avenue for identification and authentication due to uniqueness of individual stride in an un-intrusive manner. Machine vision systems have been designed to capture the uniqueness of stride of a specific person but factors such as change in speed of stride, view point, clothes and carrying accessories make gait recognition challenging and open to innovation. Our proposed approach attempts to tackle these problems by capturing the spatio-temporal features of a gait sequence by training a 3D convolutional deep neural network (3D CNN). The proposed 3D CNN architecture tackles gait identification by employing holistic approach in the form of gait energy images (GEI) which is a condensed representation capturing the shape and motion characteristics of the the human gait. The network was evaluated on two of the largest publicly available datasets with substantial gender and age diversity; OULP and CASIA-B. Optimization strategies were explored to tune the hyper-parmeters and improve the performance of the 3D CNN network. The optimized 3D CNN and the GEI were effectively able to capture the unique characteristics of the gait cycle of an individual irrespective of the challenging covariates. State of the art results achieved on the multi-views and multiple carrying conditions of the subjects belonging to CASIA-B dataset demonstrating the efficacy of our proposed algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100498X",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Biometrics",
      "Botany",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Gait",
      "Identification (biology)",
      "Law",
      "Machine learning",
      "Medicine",
      "Pattern recognition (psychology)",
      "Physical medicine and rehabilitation",
      "Political science",
      "Politics",
      "Representation (politics)",
      "STRIDE"
    ],
    "authors": [
      {
        "surname": "Gul",
        "given_name": "Saba"
      },
      {
        "surname": "Malik",
        "given_name": "Muhammad Imran"
      },
      {
        "surname": "Khan",
        "given_name": "Gul Muhammad"
      },
      {
        "surname": "Shafait",
        "given_name": "Faisal"
      }
    ]
  },
  {
    "title": "GRBMC: An effective crowdsourcing recommendation for workers groups",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115039",
    "abstract": "Crowdsourcing as an important computing resource for the Crowd-based Cooperative, crowdsourcing workers, due to the complexity of their personnel composition and the ambiguity of group characteristics, make it difficult to accurately recommend suitable worker groups for the task. Recommending crowdsourcing tasks to suitable workers can greatly improve the efficiency and quality of the execution of crowdsourcing tasks. The current crowdsourcing recommendation algorithm is based on the single characteristics of the workers, ignoring the multi-dimensional characteristics of the workers and the community characteristics of the crowd. This paper proposes a worker group recommendation method based on multi-community collaboration (GRMBC) by utilizing the worker characteristics information extracted from crowdsourcing platform. Based on the characteristics of workers'reputation, preference and activity, the method divides the worker group into several characteristics communities with similar behavior to discover the potential multi-community structure in the crowdsourcing worker group, and then selects Top-N worker group for recommendation through the interaction among multi-communities. At the same time, we also proposed two mitigation strategies for the cold start problem of data in crowdsourcing. This paper uses the public data collected by AMT to do the experiments, and compares the aggregation results of the recommendations generated by different algorithms. The results show the recommendations generated by the GRBMC algorithm proposed in this paper performs the best comprehensively. When the GRBMC algorithm that considers both worker attributes and communi-ty characteristics has an accuracy improvement of 0.03 ~ 0.04 compared with not using the recommendation algorithm on each index, the accuracy of 0.01 ~ 0.02 is improved com-pared with the method considering single characteristic. Moreover, compared with the individual recommendation method of the workers, the recommendation method of the workers' group is more in line with the platform requirements, and can well reflect the wisdom of crowd wisdom.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004802",
    "keywords": [
      "Ambiguity",
      "Computer science",
      "Crowdsourcing",
      "Crowdsourcing software development",
      "Data science",
      "Engineering",
      "Epistemology",
      "Knowledge management",
      "Machine learning",
      "Philosophy",
      "Programming language",
      "Quality (philosophy)",
      "Recommender system",
      "Reputation",
      "Social science",
      "Sociology",
      "Software",
      "Software construction",
      "Software system",
      "Systems engineering",
      "Task (project management)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Liao",
        "given_name": "Zhifang"
      },
      {
        "surname": "Xu",
        "given_name": "Xin"
      },
      {
        "surname": "Fan",
        "given_name": "Xiaoping"
      },
      {
        "surname": "Zhang",
        "given_name": "Yan"
      },
      {
        "surname": "Yu",
        "given_name": "Song"
      }
    ]
  },
  {
    "title": "On ε -insensitive simplification of fuzzy rules for fetal distress assessment",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115052",
    "abstract": "Objective: The work introduces ε -insensitive distance based approach to simplification (by reducing the rules number) of the fuzzy classifier rule base. To obtain premises of the initial rules, a modified clustering with pairs of ε -hyperballs procedure is used. The goal of the presented solutions is to achieve high quality support for fetal distress assessment, based on cardiotocographic (CTG) signals classification with a reduced number of fuzzy rules. Methods: In the presented rule base simplification solution, two rules are considered similar (or contradictory) when the distances between their premises do not exceed the assumed value of ε . The proposed simplification process consists of two phases: the first with combining similar rules into representative rules, and the second (optional) with the removal of contradictory rules. In addition, two methods of determining conclusions for representative rules are considered: a representative rule retains the original (unchanged) conclusion, or from the conclusions of similar rules the one with the highest absolute value is chosen. In the introduced clustering with pairs of ε -hyperballs the sizes of object classes are taken into account, to reduce the potential adverse impact of the unbalanced classes in the considered research material. In experiments, two reference assessments for CTG signals based on the retrospective fetal state evaluation were considered. Results and conclusions: In the two-stage classification, the modified clustering outperformed the original procedure in terms of classification sensitivity and the QI index being the geometric mean of sensitivity and specificity. Among the examined rule base simplification methods, we consider the best to be the one based only on combining similar rules, with the unchanged value of conclusion for a representative rule. With the smallest number of rules (after simplification), an increased sensitivity, and in the case of pH-based reference assessment also increased QI value is obtained. Moreover, the achieved sensitivity and QI are higher in comparison to the reference methods and values reported in literature. Significance and main impact: The results confirmed the effectiveness of the ε -insensitive distance rule base simplification. The proposed methods can be applied to any fuzzy rules with premise membership functions with a defined center. Therefore, we believe that this work may have a positive impact on other studies concerning fuzzy rule-based systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004930",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Base (topology)",
      "Classifier (UML)",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Electronic engineering",
      "Engineering",
      "Fuzzy logic",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Sensitivity (control systems)"
    ],
    "authors": [
      {
        "surname": "Jezewski",
        "given_name": "Michal"
      },
      {
        "surname": "Czabanski",
        "given_name": "Robert"
      },
      {
        "surname": "Leski",
        "given_name": "Jacek M."
      },
      {
        "surname": "Matonia",
        "given_name": "Adam"
      },
      {
        "surname": "Kahankova",
        "given_name": "Radana"
      }
    ]
  },
  {
    "title": "Cyberbullying detection: Utilizing social media features",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115001",
    "abstract": "Cyberbullying has become a major problem around the world with the increasing usage of social networks. In this direction, many studies are conducted to detect cyberbullying content automatically. Most of the studies handle this problem using opinion mining approaches that focus on the text. In this study, it is aimed to present the importance of social media attributes in cyberbullying detection. Firstly, a balanced dataset consisting of 5000 labeled contents with many social media features were prepared. Then, the relationship between social media features and cyberbullying were analyzed using the chi-square test. It is seen that some features (e.g., sender followers) are strongly related to online bullying events according to the test results. For instance, users that have more followers on social networks are disinclined to post online bullying content. Then, machine learning algorithms experimented on two different variants of the prepared datasets. The first variant includes only textual features whereas the second variant consists of the determined social media features and textual features. It is observed that each experimented machine learning algorithm give more successful prediction performance on the variant containing social media features. The obtained results motivate doing further research about social media characteristics in cyberbullying.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004425",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Communication source",
      "Computer science",
      "Focus (optics)",
      "Information retrieval",
      "Machine learning",
      "Optics",
      "Paleontology",
      "Physics",
      "Social media",
      "Telecommunications",
      "Test (biology)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Bozyiğit",
        "given_name": "Alican"
      },
      {
        "surname": "Utku",
        "given_name": "Semih"
      },
      {
        "surname": "Nasibov",
        "given_name": "Efendi"
      }
    ]
  },
  {
    "title": "Fault tolerance in LWT-SVD based image watermarking systems using three module redundancy technique",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115058",
    "abstract": "Increasing the robustness, reliability and security of the digital image watermarking methods against image processing and geometrics attacks are a key issues especially for image copyright protection. Most of existing methods concentrate on increasing the robustness and/or security aspects. In this paper, a robust and reliable digital image watermarking method is proposed by combination of lifting wavelet transform and Singular Value Decomposition (SVD) along with Three Module Redundancy (TMR) technique. Also to increase the security of the proposed method, at first, the watermark image is encrypted using improved Arnold transform. Then, SVD is applied to the encrypted watermark image. Using the TMR technique, the singular values of the encrypted watermark image are embedded in three wavelet subbands of the host image using appropriate scaling factors. To obtain the balance between robustness and imperceptibility, scaling factor of each subband is determined by Artificial Bee Colony (ABC) algorithm. According to the evaluations, compared to existing works, the proposed method is robust, and also has a high imperceptibility. Also by employing TMR, as a fault tolerance mechanism, the proposed system is more reliable and can work properly in more situations compared to the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004991",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Digital watermarking",
      "Discrete wavelet transform",
      "Eigenvalues and eigenvectors",
      "Encryption",
      "Gene",
      "Image (mathematics)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Redundancy (engineering)",
      "Robustness (evolution)",
      "Singular value",
      "Singular value decomposition",
      "Watermark",
      "Wavelet",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Salehnia",
        "given_name": "Taybeh"
      },
      {
        "surname": "Fathi",
        "given_name": "Abdolhossein"
      }
    ]
  },
  {
    "title": "Modality adaptation in multimodal data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115126",
    "abstract": "Recently, multimodal data has received much attention. In classical machine learning, it is assumed that all data comes from one modality while in multimodal machine learning, the information comes from different modalities. In multimodal machine learning, transiting, or fusing knowledge from different modalities is an important step. Hence, in these steps, the different marginal distributions between different modalities should be taken into account. However, in recent years, modality adaptation has not gotten enough attention. The motivation of this work is to consider modality adaptation to effectively encode the shared common or complementary knowledge in multimodal data. To reduce the modality shift, we present a new perspective on the modality adaptation algorithm. In multimodal data, by applying the existing domain adaptation techniques to reduce the modality shift, a problem arises because of the insufficient capability of those techniques in preserving complementary knowledge. Our proposed modality adaptation is designed such that it simultaneously considers both the shared and complementary knowledge of each modality while preserving the discriminative ability of each modality in the label space. To evaluate the proposed approach, we have applied it to two different multimodal applications: multi-view object detection and RGBD image semantic segmentation. Our results show that the proposed modality adaptation technique is successful in transferring and fusing knowledge.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005674",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Discriminative model",
      "Domain adaptation",
      "Machine learning",
      "Modalities",
      "Modality (human–computer interaction)",
      "Multimodal learning",
      "Optics",
      "Physics",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Razzaghi",
        "given_name": "Parvin"
      },
      {
        "surname": "Abbasi",
        "given_name": "Karim"
      },
      {
        "surname": "Shirazi",
        "given_name": "Mahmoud"
      },
      {
        "surname": "Shabani",
        "given_name": "Niloofar"
      }
    ]
  },
  {
    "title": "Performance comparison of feature selection and extraction methods with random instance selection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115072",
    "abstract": "In pattern recognition, irrelevant and redundant features together with a large number of noisy instances in the underlying dataset decrease performance of trained models and make the training process considerably slower, if not practically infeasible. In order to combat this so-called curse of dimensionality, one option is to resort to feature selection (FS) methods designed to select the features that contribute the most to the performance of the model, and one other option is to utilize feature extraction (FE) methods that map the original feature space into a new space with lower dimensionality. These two methods together are called feature reduction (FR) methods. On the other hand, deploying an FR method on a dataset with massive number of instances can become a major challenge, from both memory and run time perspectives, due to the complex numerical computations involved in the process. The research question we consider in this study is rather a simple, yet novel one: do these FR methods really need the whole set of instances (WSI) available for the best performance, or can we achieve similar performance levels with selecting a much smaller random subset of WSI prior to deploying an FR method? In this work, we provide empirical evidence based on comprehensive computational experiments that the answer to this critical research question is in the affirmative. Specifically, with simple random instance selection followed by FR, the amount of data needed for training a classifier can be drastically reduced with minimal impact on classification performance. We also provide recommendations on which FS/ FE method to use in conjunction with which classifier.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005133",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computation",
      "Computer science",
      "Curse of dimensionality",
      "Data mining",
      "Dimensionality reduction",
      "Feature selection",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Malekipirbazari",
        "given_name": "Milad"
      },
      {
        "surname": "Aksakalli",
        "given_name": "Vural"
      },
      {
        "surname": "Shafqat",
        "given_name": "Waleed"
      },
      {
        "surname": "Eberhard",
        "given_name": "Andrew"
      }
    ]
  },
  {
    "title": "The inbound container space allocation in the automated container terminals",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115014",
    "abstract": "In this paper, we study an inbound container space allocation problem in the automated container terminals using simulation-embedded optimization. We aim to allocate the container terminal yard space for a batch of arrived inbound containers so as to minimize the total AGV (Automated Guided Vehicle) waiting time in the space allocation process and the external truck waiting time in the future container retrieval process. We propose an integer programming model to characterize the problem and prove the NP-hardness of the problem. A simulation module is employed to estimate the container rehandle number happened in the retrieval process. A simulation-embedded genetic algorithm is developed to solve the problem. Numerical experiment results show that (1) The proposed algorithm can significantly affect the performance of the allocation decision and achieve a trade-off between fast computation and good solution quality. (2) The space allocation schemes of the inbound containers are affected by different factors, such as initial block layout, quantity of arriving containers, and containers’ arriving information. (3) The automated container terminal operators who care about the external truck waiting time may consider the simulation-embedded approach when the block yard is congested.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004553",
    "keywords": [
      "Computer science",
      "Container (type theory)",
      "Engineering",
      "Mechanical engineering",
      "Operating system",
      "Space (punctuation)"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Mingzhu"
      },
      {
        "surname": "Liang",
        "given_name": "Zhuobin"
      },
      {
        "surname": "Teng",
        "given_name": "Yi"
      },
      {
        "surname": "Zhang",
        "given_name": "Zizhen"
      },
      {
        "surname": "Cong",
        "given_name": "Xuwen"
      }
    ]
  },
  {
    "title": "A cellular automata approach to local patterns for texture recognition",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115027",
    "abstract": "Texture recognition is one of the most important tasks in computer vision and, despite the recent success of learning-based approaches, there is still need for model-based solutions. This is especially the case when the amount of data available for training is not sufficiently large, a common situation in several applied areas, or when computational resources are limited. In this context, here we propose a method for texture descriptors that combines the representation power of complex objects by cellular automata with the known effectiveness of local descriptors in texture analysis. The method formulates a new transition function for the automaton inspired by local binary descriptors. It counterbalances the new state of each cell with the previous state, in this way introducing an idea of “controlled deterministic chaos”. The descriptors are obtained from the distribution of cell states. The proposed descriptors are applied to the classification of texture images both on benchmark data sets and a real-world problem, i.e., that of identifying plant species based on the texture of their leaf surfaces. Our proposal outperforms other classical and state-of-the-art approaches, especially in the real-world problem, thus revealing its potential to be applied in numerous practical tasks involving texture recognition at some stage.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004681",
    "keywords": [],
    "authors": [
      {
        "surname": "Florindo",
        "given_name": "Joao B."
      },
      {
        "surname": "Metze",
        "given_name": "Konradin"
      }
    ]
  },
  {
    "title": "A stock selection algorithm hybridizing grey wolf optimizer and support vector regression",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115078",
    "abstract": "Artificial intelligence remarkably facilitates quantitative investment. A latest intelligent search algorithm, grey wolf optimizer, is well integrated with support vector regression machine to obtain the optimal portfolio. The performance of the hybrid algorithm is empirically investigated through transactional and financial data from stock markets of America and China. The experimental results indicate that (i) the proposed algorithm is able to stably achieve excess returns; (ii) compared with genetic algorithm, particle swarm optimization, gravitational search algorithm and harmony search, the enhanced grey wolf optimizer significantly boots the predictive performance of support vector regression machine; (iii) the proposed algorithm can achieve the better profitability and the higher reliability in Chinese A-share market.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005194",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Data mining",
      "Economics",
      "Finance",
      "Genetic algorithm",
      "Harmony search",
      "Horse",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Paleontology",
      "Particle swarm optimization",
      "Profitability index",
      "Stock market",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Meng"
      },
      {
        "surname": "Luo",
        "given_name": "Kaiping"
      },
      {
        "surname": "Zhang",
        "given_name": "Junhuan"
      },
      {
        "surname": "Chen",
        "given_name": "Shengli"
      }
    ]
  },
  {
    "title": "Sentiment based multi-index integrated scoring method to improve the accuracy of recommender system",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115105",
    "abstract": "To the best of our knowledge, few studies have focused on the inconsistency between user ratings and reviews as well as natural noise management in recommender systems (RSs). To address these issues, this study introduces a sentiment based multi-index integrated scoring method to provide a reliable information input that reflects comprehensive user preferences for recommendation algorithms and facilitate improved performance. Initially, Bing Liu’s lexicon is expanded using a semi-supervised learning technique to obtain additional sentiment words and calculate the sentiment scores of reviews; then a normalized sentiment score method based on sigmoid function that considers the emotional tendencies of different users in reviews is designed to convert the scores into values corresponding to the rating scale of RS. Subsequently, a degree classification criteria approach is adopted to assign users and items to more fine-grained classes Further, a natural noise detection method is exploited to identify and correct noise ratings according to classification conditions. To effectively integrate normalized review and denoised rating information, two factors, user consistency and review feedback, are considered to obtain the importance of reviews and ratings; then, a weighted average method is used to generate a set of comprehensive ratings. The experimental results on two benchmark datasets indicate that the superiority of memory-based or model-based collaborative filtering methods (CFs) using comprehensive ratings over their respective methods using original ratings is determined by various accuracy metrics, which demonstrates that our scheme can enhance the reliability and accuracy of user information. Thus, the proposed scheme provides new insights for improving the accuracy of RSs from the perspective of multiple information sources. Additionally, this method exhibits good generalizability and practicality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005467",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Collaborative filtering",
      "Computer science",
      "Consistency (knowledge bases)",
      "Data mining",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Information retrieval",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Noise (video)",
      "Operating system",
      "Physics",
      "Power (physics)",
      "Programming language",
      "Quantum mechanics",
      "RSS",
      "Ranking (information retrieval)",
      "Recommender system",
      "Reliability (semiconductor)",
      "Scheme (mathematics)",
      "Sentiment analysis",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Wenhua"
      },
      {
        "surname": "Li",
        "given_name": "Xiaoguang"
      },
      {
        "surname": "Deng",
        "given_name": "Jiangzhou"
      },
      {
        "surname": "Wang",
        "given_name": "Yong"
      },
      {
        "surname": "Guo",
        "given_name": "Junpeng"
      }
    ]
  },
  {
    "title": "Utilizing 3D joints data extracted through depth camera to train classifiers for identifying suicide bomber",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115081",
    "abstract": "Safety and security of humans is an important concern in every aspect. With the advancement in engineering, sciences, and technology (unfortunately) new methods to harm humans have also been introduced. At the same time, scientists are paying attention to the security aspects by developing new software and hardware gadgets. In comparison to the system level security, the safety/security of human beings is more important. Suicide bombing is one such nuisance that is still an open challenge for the world to detect before it is triggered. This work deals with the identification of a suicide bomber using a 3D depth camera and machine learning techniques. This work utilizes the skeletal data provided by the 3D depth camera to identify a bomber wearing a suicide jacket. The prediction is based on real-time 3D posture data of the body joints obtained through the depth camera. Using a comprehensive experimental design, a dataset is created consisting of 20 joints information obtained from 120 participants. The dataset records this for each of the participants with and without wearing a suicide jacket. Experiments are performed with the suicide jacket bearing 10- to 20-kg weight. Simulations are performed using 3D spatial features of the participants' body in four ways: full body joints (20 joints), upper-half of the body (above the spine base of the skeleton), 20 joints with 15 frames, and 20 joints with 20 frames. It is observed that 15 to 20 frames are sufficient to identify a suspected suicide bomber. The proposed framework utilize four classifiers to identify vulnerability of a subject to be a suicide bomber. Results show that the proposed framework is capable of identifying a suicide bomber with an average accuracy of 92.30%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005224",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Engineering",
      "Harm",
      "Identification (biology)",
      "Mechanical engineering",
      "Programming language",
      "Psychology",
      "Social psychology",
      "Software",
      "Work (physics)"
    ],
    "authors": [
      {
        "surname": "Halim",
        "given_name": "Zahid"
      },
      {
        "surname": "Usman Ahmed Khan",
        "given_name": "Raja"
      },
      {
        "surname": "Waqas",
        "given_name": "Muhammad"
      },
      {
        "surname": "Tu",
        "given_name": "Shanshan"
      }
    ]
  },
  {
    "title": "RDF M : An alternative approach for representing, storing, and maintaining meta-knowledge in web of data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115043",
    "abstract": "The Web of Data needs additional information, i.e., meta-knowledge to ensure quality and build the trust in data. For representing meta-knowledge, there exist various approaches in the literature, for example, RDFt, tRDF, RDF Reification, Singleton Property and Named Graph. There are various issues associated with these approaches in representing the meta-knowledge, for example, the increasing graph size, additional statement generation, the representation of multi-dimensional and/or nested meta-knowledge, among others. In this work, we propose an approach called RDF M to represent, store, and manage meta-knowledge. RDF M integrates attributes to the predicate to represent nested and/or multi-dimensional meta-knowledge with lesser statements generation. A query language called SPARQL M is developed to extract RDF M data. The study analyzes the performance of RDF M in terms of the number of edges, number of statements generation, data redundancy, storage, query response time, required query length, representation of meta-knowledge in nested and different dimensions. The results show that the RDF M model performs significantly and gives advantage over storage management and graph data management.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100484X",
    "keywords": [
      "Computer science",
      "Data mining",
      "Domain knowledge",
      "Information retrieval",
      "Knowledge graph",
      "Knowledge management",
      "Law",
      "Linked data",
      "Political science",
      "Politics",
      "RDF",
      "Reification (Marxism)",
      "Semantic Web",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Sen",
        "given_name": "Sangeeta"
      },
      {
        "surname": "Katoriya",
        "given_name": "Devashish"
      },
      {
        "surname": "Dutta",
        "given_name": "Animesh"
      },
      {
        "surname": "Dutta",
        "given_name": "Biswanath"
      }
    ]
  },
  {
    "title": "Hybrid whale optimization algorithm with gathering strategies for high-dimensional problems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115032",
    "abstract": "In order to solve the problems, such as insufficient search ability and low search efficiency, of Whale Optimization Algorithm (WOA) in solving high-dimensional problems, a novel Hybrid WOA with Gathering strategies (HWOAG) is proposed in this paper. Firstly, an individual-based updating way is used in HWOAG instead of the dimension-based updating one of WOA to reduce the computational complexity and to be more suitable for high-dimensional problems. Secondly, a random opposition learning strategy is embedded into the individual-based WOA to form an opposition learning WOA (OWOA), and Grey Wolf Optimizer (GWO) is integrated into OWOA to form an OWOA with GWO (OWOAG) so as to improve the global search ability of WOA. Finally, two standalone OWOAGs are formulated to balance exploration and exploitation better. The two OWOAGs adopt strategies such as switching parameter tuning, random differential disturbance and global-best spiral operator to get stronger search ability. A lot of experimental results on high-dimensional (i.e. 1000-, 2000-, 4000- and 8000- dimensional) benchmark functions and clustering datasets for Fuzzy C-Means (FCM) optimization show that HWOAG has stronger search ability and higher search efficiency than WOA and quite a few state-of-the-art algorithms and that all the strategies gathered to WOA are effective. The source codes of the proposed algorithm HWOAG are available at https://github.com/kangzhai/HWOAG.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004735",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cluster analysis",
      "Computer science",
      "Differential evolution",
      "Geodesy",
      "Geography",
      "Mathematical optimization",
      "Mathematics"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Xinming"
      },
      {
        "surname": "Wen",
        "given_name": "Shaochen"
      }
    ]
  },
  {
    "title": "A blockchain-based system to enhance aircraft parts traceability and trackability for inventory management",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115101",
    "abstract": "Aircraft spare parts inventory management (ASPM) has played a critical role in tracing and tracking spare parts as any related maintenance or movement shall be recorded. Traceability and trackability of data ensure the compliance of airworthiness requirements. The International Air Transport Association (IATA) has strongly emphasised the significance of quality traceability data throughout the aircraft part’s life cycle, leading to enhanced inventory control accuracy, reduced maintenance error, and effective decision-making processes. However, with the rapid increase of spare parts types, the complexity of aircraft parts multi-stage supply chains leads to inefficient tracing and tracking operations with unsatisfactory traceability data quality and information security. This paper proposed a blockchain-based system that provided a managerial platform for accurate recording of spare parts traceability data with organisational consensus and validation using Hyperledger Fabric and Hyperledger Composer. A data model has been determined based on the existing ASPM, enabling information integrity during transaction operations. The channel mechanism has yielded a trustful data sharing platform between each contracting organisation for logistics and operational arrangements, which has enhanced information visibility and security. The blockchain-based system, executed under a decentralised ledger mechanism, shall improve the quality of traceability data and reliable information sharing within the spare parts supply chain. The enhanced blockchain-based inventory management system can establish the digital twin of aviation as part of Industry 4.0 in the future.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100542X",
    "keywords": [
      "Blockchain",
      "Computer science",
      "Computer security",
      "Engineering",
      "Inventory management",
      "Operations management",
      "Software engineering",
      "Traceability"
    ],
    "authors": [
      {
        "surname": "Ho",
        "given_name": "G.T.S."
      },
      {
        "surname": "Tang",
        "given_name": "Yuk Ming"
      },
      {
        "surname": "Tsang",
        "given_name": "Kun Yat"
      },
      {
        "surname": "Tang",
        "given_name": "Valerie"
      },
      {
        "surname": "Chau",
        "given_name": "Ka Yin"
      }
    ]
  },
  {
    "title": "A hybrid neural variational CF-NADE for collaborative filtering using abstraction and generation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115047",
    "abstract": "Generative models in some way reflect probability distributions over multiple variables. For latent variable generative models, the density function is intractable and these models thus make some independent assumptions to minimize the number of factors and, in turn, the number of parameters in the model. Autoregressive models such as Neural Autoregressive Distribution Estimator (NADE) have a tractable density with no latent variables and the goal is to learn a joint distribution over input variables. Autoregressive models have several advantages over latent variable models based on samples (ratings in our case) produced, independent assumptions, and tractable density functions. NADE models as such have seen less scrutiny in recommendation literature. Few papers have utilized NADE for collaborative filtering tasks but have limited capability as they could not perform abstraction, which restricts the model from learning accurate latent representations and the sampling rate was also slow. Further, the rating prediction process is calculative and not generative and model retraining is also required whenever a new user or new item gets added to the underlying dataset, limiting the model performance. Existing neural network based methods are deterministic and they can’t account for the uncertainty in user-item latent representations. To overcome these issues, we propose a hybrid neural Variational CF-NADE generative model that extends NADE for collaborative filtering tasks using variational Bayesian autoencoder to characterize the uncertainty in ratings. We do not make any independence assumption for the users or items rating vectors but use the default factorization to learn a joint distribution over ratings. Therefore, the proposed model uses a tractable density distribution to have good quality ratings generated. The model learns comprehensive user-item interaction function using abstraction and generation phases. The variational autoencoder performs abstraction on user-item metadata information which acts as a regularizer to the rating generated. By reusing the shared weights for fast sampling among different predicted ratings, we speed up the computation with an increased performance by decomposing the rating vector into conditionals by the chain rule and thus reducing the number of parameters. Experiments on three real-world datasets show that the proposed model beats recent state-of-the-art approaches for collaborative filtering tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004887",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Autoregressive model",
      "Collaborative filtering",
      "Computer science",
      "Econometrics",
      "Estimator",
      "Generative grammar",
      "Generative model",
      "Joint probability distribution",
      "Latent variable",
      "Latent variable model",
      "Machine learning",
      "Mathematics",
      "Recommender system",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Nahta",
        "given_name": "Ravi"
      },
      {
        "surname": "Meena",
        "given_name": "Yogesh Kumar"
      },
      {
        "surname": "Gopalani",
        "given_name": "Dinesh"
      },
      {
        "surname": "Chauhan",
        "given_name": "Ganpat Singh"
      }
    ]
  },
  {
    "title": "Analytic Continued Fractions for Regression: A Memetic Algorithm Approach",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115018",
    "abstract": "We present an approach for regression problems that employs analytic continued fractions as a novel representation. Comparative computational results using a memetic algorithm are reported in this work. Our experiments included fifteen other different machine learning approaches including five genetic programming methods for symbolic regression and ten machine learning methods. The comparison on training and test generalization was performed using 94 datasets of the Penn State Machine Learning Benchmark. The statistical tests showed that the generalization results using analytic continued fractions provide a powerful and interesting new alternative in the quest for compact and interpretable mathematical models for artificial intelligence.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004590",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Chemistry",
      "Computer science",
      "Fraction (chemistry)",
      "Generalization",
      "Genetic algorithm",
      "Genetic programming",
      "Geodesy",
      "Geography",
      "Law",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Memetic algorithm",
      "Organic chemistry",
      "Political science",
      "Politics",
      "Regression",
      "Representation (politics)",
      "Statistics",
      "Symbolic regression"
    ],
    "authors": [
      {
        "surname": "Moscato",
        "given_name": "Pablo"
      },
      {
        "surname": "Sun",
        "given_name": "Haoyuan"
      },
      {
        "surname": "Haque",
        "given_name": "Mohammad Nazmul"
      }
    ]
  },
  {
    "title": "CASTLE: Cluster-aided space transformation for local explanations",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115045",
    "abstract": "With Artificial Intelligence becoming part of a rapidly increasing number of industrial applications, more and more requirements about their transparency and trustworthiness are being demanded to AI systems, especially in military, medical and financial domains, where decisions have a huge impact on lives. In this paper, we propose a novel model-agnostic Explainable AI (XAI) technique, named Cluster-aided Space Transformation for Local Explanation (CASTLE), able to provide rule-based explanations based on both the local and global model’s workings, i.e. its detailed ”knowledge” in the neighborhood of the target instance and its general knowledge on the training dataset, respectively. The framework has been evaluated on six datasets in terms of temporal efficiency, cluster quality and model significance. Eventually, we asked 36 users to evaluate the explainability of the framework, getting as result an increase of interpretability of 6 % with respect to another state-of-the-art technique, named Anchors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004863",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Cluster (spacecraft)",
      "Computer science",
      "Computer security",
      "Data mining",
      "Epistemology",
      "Gene",
      "Interpretability",
      "Operating system",
      "Philosophy",
      "Programming language",
      "Quality (philosophy)",
      "Space (punctuation)",
      "Transformation (genetics)",
      "Transparency (behavior)",
      "Trustworthiness"
    ],
    "authors": [
      {
        "surname": "La Gatta",
        "given_name": "Valerio"
      },
      {
        "surname": "Moscato",
        "given_name": "Vincenzo"
      },
      {
        "surname": "Postiglione",
        "given_name": "Marco"
      },
      {
        "surname": "Sperlì",
        "given_name": "Giancarlo"
      }
    ]
  },
  {
    "title": "Identifying and ranking super spreaders in real world complex networks without influence overlap",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115061",
    "abstract": "In the present-days complex networks modeled on real-world data contain millions of nodes and billions of links. Identifying super spreaders in such an extensive network is a challenging task. Super spreaders are the most important or influential nodes in the network that play the central role during an infection spreading or information diffusion process. Depending on the application, either the most influential node needs to be identified, or a set of initial seed nodes are identified that can maximize the collective influence or the total spread in the network. Many centrality measures have been proposed to rank nodes in a complex network such as ‘degree’, ‘closeness’, ‘betweenness’, ‘coreness’ or ‘k-shell’ centrality, among others. All have some kind of inherent limitations. Mixed degree decomposition or m-shell is an improvement over k-shell that yields better ranking. Many researchers have employed single node identification heuristics to select multiple seed nodes by considering top-k nodes from the ranked list. This approach does not results in the optimal seed nodeset due to the considerable overlap in total spreading influence. Influence overlap occurs when multiple nodes from the seed nodeset influence a specific node, and it is counted multiple times during total collective influence computation. In this paper, we exploit the ‘node degree’, ‘closeness’ and ‘coreness’ among the nodes and propose novel heuristic template to rank the super spreaders in a network. We employ k-shell and m-shell as a coreness measure in two variants for a comparative evaluation. We use a geodesic-based constraint (enforcing a minimum distance between seed nodes) to select an initial seed nodeset from that ranked nodes for influence maximization instead of selecting the top-k nodes naively. All models and metrics are updated to avoid overlapping influence during total spread computation. Experimental simulation with the SIR (Susceptible-Infectious-Recovered) spreading model and an evaluation with performance metrics like spreadability, monotonicity of ranking, Kendall’s rank correlation on some benchmark real-world networks establish the superiority of the proposed methods and the improved seed node selection technique.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005029",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Betweenness centrality",
      "Centrality",
      "Combinatorics",
      "Complex network",
      "Computer science",
      "Data mining",
      "Degree (music)",
      "Engineering",
      "Mathematics",
      "Node (physics)",
      "Physics",
      "Programming language",
      "Rank (graph theory)",
      "Ranking (information retrieval)",
      "Set (abstract data type)",
      "Statistics",
      "Structural engineering",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Maji",
        "given_name": "Giridhar"
      },
      {
        "surname": "Dutta",
        "given_name": "Animesh"
      },
      {
        "surname": "Curado Malta",
        "given_name": "Mariana"
      },
      {
        "surname": "Sen",
        "given_name": "Soumya"
      }
    ]
  },
  {
    "title": "What is the best grid-map for self-driving cars localization? An evaluation under diverse types of illumination, traffic, and environment",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115077",
    "abstract": "The localization of self-driving cars is needed for several tasks such as keeping maps updated, tracking objects, and planning. Localization algorithms often take advantage of maps for estimating the car pose. Since maintaining and using several maps is computationally expensive, it is important to analyze which type of map is more adequate for each application. In order to contribute with this analysis, in this work, we compare the accuracy of a particle filter localization when using occupancy, reflectivity, color, or semantic grid maps. To the best of our knowledge, such evaluation is missing in the literature. For building semantic and color grid maps, point clouds from a Light Detection and Ranging (LiDAR) sensor are fused with images captured by a front-facing camera. Semantic information is extracted from images with the deep neural network DeepLabv3+. Experiments are performed in varied environments, under diverse conditions of illumination and traffic. Results show that occupancy grid maps lead to more accurate localization, followed by reflectivity grid maps. In most scenarios, the localization with semantic grid maps kept the position tracking without catastrophic losses, but with errors from 2 to 3 times bigger than the previous. Color grid maps led to inaccurate and unstable localization in most scenarios even using a robust metric, the entropy correlation coefficient, for comparing online data and the map.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005182",
    "keywords": [],
    "authors": [
      {
        "surname": "Mutz",
        "given_name": "Filipe"
      },
      {
        "surname": "Oliveira-Santos",
        "given_name": "Thiago"
      },
      {
        "surname": "Forechi",
        "given_name": "Avelino"
      },
      {
        "surname": "Komati",
        "given_name": "Karin S."
      },
      {
        "surname": "Badue",
        "given_name": "Claudine"
      },
      {
        "surname": "França",
        "given_name": "Felipe M.G."
      },
      {
        "surname": "De Souza",
        "given_name": "Alberto F."
      }
    ]
  },
  {
    "title": "A survey of safe landing zone detection techniques for autonomous unmanned aerial vehicles (UAVs)",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115091",
    "abstract": "The age of automation is upon us. Few decades earlier, nearly all the flying vehicles were human-controlled. Nowadays, almost every air vehicle is partially automated or getting closer to full automation. This race towards full automation has led to the introduction of features like autopilot. Unmanned aerial vehicles (UAVs) are the tiniest version of all types of air vehicles. The widespread usage of autonomous UAVs has spawned the need for safe landing zone (SLZ) detection techniques for UAV landing. A SLZ detection becomes an important face of a mission when the UAV needs emergency landing due to the technical difficulties or adverse weather conditions on the way of its operation. Before directly proceeding for landing, a UAV has to decide whether the landing zones are safe or not. On-board visual sensors provide potential information of the ground surface in the form of image or signal. Different image processing and safe landing area detection (SLAD) algorithms are then used to identify the best possible landing sites from the input data. In this survey, we discuss indoor and outdoor landing zone detection techniques. We further classify outdoor landing zones as either static or dynamic and discuss existing literature in the specific categories. We critique the shortcomings of existing SLZ detection techniques while also acknowledging their contributions. Further, we point to potential areas of improvement and future directions of the safe landing zone detection algorithms we surveyed. This survey paper may be a useful tutorial for understanding the types of landing zones and landing zone detection techniques for the UAVs, the strengths of zone detection algorithms, and the open areas for future improvement and research.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005327",
    "keywords": [
      "Adverse weather",
      "Aerial survey",
      "Aeronautics",
      "Aerospace engineering",
      "Automation",
      "Autopilot",
      "Climatology",
      "Computer science",
      "Engineering",
      "Geology",
      "Geometry",
      "Mathematics",
      "Mechanical engineering",
      "Point (geometry)",
      "Real-time computing",
      "Remote sensing"
    ],
    "authors": [
      {
        "surname": "Shah Alam",
        "given_name": "Md"
      },
      {
        "surname": "Oluoch",
        "given_name": "Jared"
      }
    ]
  },
  {
    "title": "Multi-objective trajectory planning of humanoid robot using hybrid controller for multi-target problem in complex terrain",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115110",
    "abstract": "Humanoid robotics is an emerging area of interest in the current engineering research scenario, owing to its ability to impersonate human deportment and emulate various jobs. The given article emphasizes the development and implementation of a hybrid navigational controller to optimize the path length, energy demand, and time spent for accomplishing assigned tasks. The proposed navigational controller is developed by hybridizing the metaheuristic Improved Spider Monkey Optimization (ISMO) approach and the Regression Analysis (RA) approach. Various input parameters like obstacle and target locations are fed to the RA approach that implements a proper navigational direction selection. And it forwards to the SMO approach that is improved using piecewise B-Spline path smoother, which exercises further refinement of the output turning angle and smoothness of path around obstacles. Simulations and real-time experiments are undertaken using different controllers involving single robot systems, which shows the proposed controller’s superiority. An average improvement of 13.72% and 13.94% in path length against RA in simulation and experiment, respectively, and an average improvement of 7.59% and 7.5% in path length against ISMO in simulation and experiment, respectively, is obtained. It is further evaluated for navigation by implementing in a single robot having a multi-target problem. Multiple robot navigation has to deal with the self-collision situations that are solved by prioritizing the specified robot using the dining philosopher controller. It is implemented in the proposed controller for navigation of multiple robots to solve the conflict. Both scenarios are tested in the simulation environment and ratified in the experimental environment. Average deviation under 5% for path length and time spent for single robot navigation and multiple robot navigation is obtained, which shows a good agreement with each other. Energy efficiency test has been performed in contrast to default controller of NAO for various joints, and an average improvement of 8.16%, 5.9% and 20.57%, has been recorded in torque for ankle, knee and hip, respectively. Comparison is carried with an established navigational controller in a similar environmental setup shows an improvement of 8.6% and 10.365%, respectively, in path length and time spent. The results obtained from these setups prove the proposed hybrid controller to be robust, efficient and superior while performing path planning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005510",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Controller (irrigation)",
      "Ecology",
      "Law",
      "Mathematical analysis",
      "Mathematics",
      "Motion planning",
      "Obstacle",
      "Path (computing)",
      "Political science",
      "Programming language",
      "Real-time computing",
      "Robot",
      "Simulation",
      "Smoothness",
      "Terrain"
    ],
    "authors": [
      {
        "surname": "Kumar Kashyap",
        "given_name": "Abhishek"
      },
      {
        "surname": "Parhi",
        "given_name": "Dayal R"
      }
    ]
  },
  {
    "title": "Driver distraction analysis using face pose cues",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115036",
    "abstract": "Vehicle driver distraction is one of the major reasons for road accidents. Involvement with a co-passenger, use of in-vehicle devices or phone leads to a situation where the driver head pose varies and the eye is off the road. A low cost early warning system should reduce the distracted driving instances, thus making our roads safer. Face pose information forms an important cue to determine driver distraction. The main objective of this work is to analyse the distractions of the driver based on his/her face pose cues. A straight pose or slight variation would indicate a non-distracted driver, while a large pose variation from the center would indicate a high probability for a distracted driver. Face pose database of vehicle drivers is developed and is bench-marked. A clustered two layer approach on Gabor features is proposed. A five layer convolutional network with three fully connected layers is also used to bench-mark the data. The proposed clustered two-layer approach with Gabor features and SVM classifier provides better results in driver distraction analysis when compared to the deep learning approach and other manifold approaches. The improved accuracy could be attributed to the improved modeling of manifold in our approach, better class discrimination of the Gabor features together with better classification provided by the SVM classifier.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004772",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Classifier (UML)",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Convolutional neural network",
      "Distracted driving",
      "Distraction",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "SAFER",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Hari",
        "given_name": "C.V."
      },
      {
        "surname": "Sankaran",
        "given_name": "Praveen"
      }
    ]
  },
  {
    "title": "Experimenting with big data computing for scaling data quality-aware query processing",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114858",
    "abstract": "Combining query processing techniques with data quality management approaches enables enforcement of quality constraints, such as timeliness, accuracy and completeness, as part of ad-hoc query specification and execution, improving the quality of query results. Despite the emergence of novel data quality processing tools, there is a dearth of studies assessing performance and scalability in the execution of data quality assessment tasks during query processing. This paper reports on an empirical study aiming to investigate the extent to which a big data computing framework (Spark) can offer significant gains in performance and scalability when executing data quality querying tasks over a range of computational platforms including a single commodity multi-core machine and a cluster-based platform for a wide range of workloads. Our results show that substantial performance and scalability gains can be obtained by using optimized data science libraries combined with the parallel and distributed capabilities of big data computing. We also provide guidelines on choosing the appropriate computational infrastructure for executing DQ-aware queries.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002992",
    "keywords": [
      "Big data",
      "Computer science",
      "Data mining",
      "Data quality",
      "Database",
      "Distributed computing",
      "Economics",
      "Information retrieval",
      "Metric (unit)",
      "Operations management",
      "Programming language",
      "Query expansion",
      "Query optimization",
      "Range query (database)",
      "SPARK (programming language)",
      "Sargable",
      "Scalability",
      "Search engine",
      "Web search query"
    ],
    "authors": [
      {
        "surname": "Cisneros-Cabrera",
        "given_name": "Sonia"
      },
      {
        "surname": "Michailidou",
        "given_name": "Anna-Valentini"
      },
      {
        "surname": "Sampaio",
        "given_name": "Sandra"
      },
      {
        "surname": "Sampaio",
        "given_name": "Pedro"
      },
      {
        "surname": "Gounaris",
        "given_name": "Anastasios"
      }
    ]
  },
  {
    "title": "In defense of group fuzzy AHP: A comparison of group fuzzy AHP and group AHP with confidence intervals",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114970",
    "abstract": "Group aggregation of the Analytic Hierarchy Process (AHP), both with crisp and fuzzy numbers, is a widely researched field in group multiple criteria decision-making. Typically, if uncertainty needs to be reflected in the outcome of an AHP, triangular fuzzy numbers are used, although this is a controversial method, as many argue that the traditional 1–9 scale used in crisp AHP introduces uncertainty to the decision-making process, leaving no need for fuzzification. This research introduces the use of confidence intervals around the aggregated group mean to estimate the score ranges generated through group fuzzy AHP (GFAHP), as confidence intervals are simpler to calculate and more familiar to many. A comparison of the score ranges resulting from GFAHP and group aggregation using crisp numbers with confidence intervals around the group mean score, aggregating on individual priorities using both the arithmetic mean and geometric mean, was completed. The concept of using confidence intervals on crisp scores to generate score ranges was implemented through a case study to select a new all-wheel drive crossover vehicle. This study shows that confidence intervals introduce excessive uncertainty to the decision-making problem.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004115",
    "keywords": [
      "Analytic hierarchy process",
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Confidence interval",
      "Fuzzy logic",
      "Group (periodic table)",
      "Mathematics",
      "Operations research",
      "Organic chemistry",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Coffey",
        "given_name": "Laura"
      },
      {
        "surname": "Claudio",
        "given_name": "David"
      }
    ]
  },
  {
    "title": "Performance analysis of all-optical logical gate using artificial neural network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115029",
    "abstract": "Photonic digital gates are the next generation of all optical digital devices. Exclusive OR gate (XOR) is one of the important and applicable components in the next generation of all-optical networks. Generally, the optical digital gates are simulated by complex and time- consuming numerical methods like FDTD. Prediction of the accurate output (the state of 0 or 1) is a key parameter in the digital gates. Photonic devices modeling by artificial neural networks (ANNs) will be introduced as a flexible, suitable and precise modeling alternative approach instead of numerical simulations. In this paper, performance of a 3-input all-optical exclusive OR gate (XOR) has been modeled using artificial neural networks. We discuss in detail that trained ANNs can be used as a fast modeling method for optical digital gates, with high accuracy. Here, we proposed to develop a modeling system for all-optical 3-input XOR gates based on multilayer perceptron (MLP) and radial basis function (RBF) neural networks. Therefore, the proposed neural networks are trained by data arising from simulations by numerical methods. The dataset is the power lasers of all possible logical conditions used to train neural networks. The full data set is split in 90/10 for the train and test data. To understand the ANN methods’ performance, the figures of the predicted results of all optical XOR gate are plotted and compared. Also, the error indices like mean square error (MSE) and Relative Square Error (RSE) are calculated for the test data to evaluate the performance of neural network models on the prediction of all optical XOR gate output. The correlation between the modeled data by neural networks and simulated data by numerical methods are established by Correlation Coefficient (R2) parameter too. According to the comparison of both neural networks algorithms, good results are reached which lead to an effective technique. The effect of training parameters of ANN’ models, like number of hidden layers, number neurons in the hidden layers, number of epochs, learning rate value, spread of gaussian functions on the prediction results and errors are compared and analyzed. The aim is to set the optimum parameters to prevent from complexity of the neutral network. The best results for RBF NN is the value of 1 for spread of Gaussian function and 90 hidden neurons that lead to MSE, RSE and R2 values of 4.0837 × 10−4, 0.0114, and 0.9888 respectively. The optimum structure of MLP NN is 2 hidden layers with 12 and 8 neurons (5 12 8 1) by training with 65 epochs. The activation function for hidden and output layers are chosen logsigmoid, logsigmoid and purelin respectively. The calculated MSE, RSE, and R2 for the best MLP NN structure has been 7.5 × 10−10, 0.000133, and 0.9999, respectively, which confirms the high accuracy of the mentioned neural network model. Even though the two mentioned neural networks models can be used to model all-optical 3-input XOR gates appropriately, results show that MLP NN using the most relevant features achieved the best results and better estimates. Finally, the implementation of the optical logic gates with neural network was illustrated for the future optical integrated circuits.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100470X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Logical analysis",
      "Mathematical statistics",
      "Mathematics",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Hamedi",
        "given_name": "Samaneh"
      },
      {
        "surname": "Dehdashti Jahromi",
        "given_name": "Hamed"
      }
    ]
  },
  {
    "title": "Wind turbine fault diagnosis based on ReliefF-PCA and DNN",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115016",
    "abstract": "A large amount of data would be generated during the operation of wind turbine (WT), which is easy to cause dimensional disaster, and if more than one WT fault occur, multiple sensors would alarm. To solve the problems of big data, inaccurate and untimely fault diagnosis and so on, a hybrid fault diagnosis method is developed based on ReliefF, principal component analysis (PCA) and deep neural network (DNN) in this paper. Firstly, the ReliefF method is used to select the fault features and reduce the data dimensions. Secondly, PCA algorithm is used to further reduce the data dimensions, which is mainly used to reduce the redundancy among the data and improve the accuracy of fault diagnosis. Finally, the ReliefF-PCA-DNN model is constructed, optimized and used for the fault case of a wind farm in Jilin Province. The experimental results show that, for the single fault, the accuracies of the proposed hybrid models are all more than 98.5% and for the multi faults, the accuracy of the proposed model is more than 96%, which both are all much higher than the comparison methods. So, the method could diagnose the WT faults well.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004577",
    "keywords": [
      "Actuator",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Data mining",
      "Engineering",
      "Fault (geology)",
      "Fault detection and isolation",
      "Geology",
      "Mechanical engineering",
      "Operating system",
      "Pattern recognition (psychology)",
      "Principal component analysis",
      "Redundancy (engineering)",
      "Seismology",
      "Turbine"
    ],
    "authors": [
      {
        "surname": "Wen",
        "given_name": "Xiaoqiang"
      },
      {
        "surname": "Xu",
        "given_name": "Ziang"
      }
    ]
  },
  {
    "title": "An integrated probabilistic graphic model and FMEA approach to identify product defects from social media data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115030",
    "abstract": "Recently, the explosive increase in social media data enables manufacturers to collect product defect information promptly. Extant literature gathers defect information like defective components or defect symptoms without distinguishing defect-related (DR) texts from defect-unrelated (DUR) texts and thus makes defects discussed by few texts buried in enormous DUR texts. Moreover, existing studies do not consider the defect severity which is valuable and important for manufacturers to make remedial decisions. To bridge these research gaps, we propose a novel approach that integrates the probabilistic graphic model named Product Defect Identification and Analysis Model (PDIAM) with Failure Mode and Effect Analysis (FMEA) to derive product defect information from social media data. Comparing to extant studies, PDIAM identifies DR texts and then extracts defect information from these texts. And PDIAM provides more defect information than previous researches. Besides, we further analyze defect severity with the combination of FMEA and PDIAM which alleviates the inherent subjectivity brought by expert evaluation in the traditional FMEA. A case study in the automobile industry proves the predominant performance of our approach and great potential in defect management.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004711",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Bridge (graph theory)",
      "Computer science",
      "Data mining",
      "Data science",
      "Evolutionary biology",
      "Extant taxon",
      "Geometry",
      "Internal medicine",
      "Mathematics",
      "Medicine",
      "Probabilistic logic",
      "Product (mathematics)",
      "Social media",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Lu"
      },
      {
        "surname": "He",
        "given_name": "Zhen"
      },
      {
        "surname": "He",
        "given_name": "Shuguang"
      }
    ]
  },
  {
    "title": "Classification of imbalanced hyperspectral images using SMOTE-based deep learning methods",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114986",
    "abstract": "Hyperspectral imaging (HSI) is one of the most advanced methods of digital imaging. This technique differs from RGB images with its wide range of the electromagnetic spectrum. Imbalanced data sets are frequently encountered in machine learning. As a result, the classifier performance may be poor. To avoid this problem, the data set must be balanced. The main motivation in this study is to reveal the difference and effects on the classifier performance between the original imbalanced dataset and the data set modified by balancing methods. In the proposed method, hyperspectral image classification study carried out on Xuzhou Hyspex dataset includes nine-classes including bareland-1, bareland-2, crops-1, crops-2, lake, coals, cement, trees, house-roofs of elements, by using the convolutional neural networks (CNN) and dataset balancing methods comprising the Smote, Adasyn, K-Means, and Cluster. This dataset has been taken from IEEE-Dataport Machine Learning Repository. To classify the hyperspectral image, the convolutional neural networks having different multiclass classification approaches like One-vs-All, One-vs-One. Dataset was splitted in two different ways: %50–%50 Hold-out and 5-Fold Cross-validation. In order to evaluate the performance of the proposed models, the confusion matrix, classification accuracy, precision, recall, and F-Measure have been used. Without the dataset balancing, the obtained classification accuracies are 93.63%, 92.33%, 88.36% for %50–%50 train-test split, and 94.46%, 94%, 92.24% for 5-Fold cross-validation using multi-class classification, One-vs-All, and One-vs-One respectively. After Smote balancing, the obtained classification accuracies are 96.41%, 95.6%, 92.53% for %50–%50 train-test split and 96.49%, 95.64%, 93.38% for 5-Fold cross-validation using multi-class classification, One-vs-All and One-vs-One respectively. After Adasyn balancing, the obtained classification accuracies are 95.86%, 93.62%, 87.05% for %50–%50 train-test split and 96.38%, 95.09%, 91.55% for 5-Fold cross-validation using multi-class classification, One-vs-All and One-vs-One respectively. After K-Means balancing, the obtained classification accuracies are 95.23%, 93.36%, 90.6% for %50–%50 train-test split and 95.74%, 94.72%, 91.94% for 5-Fold cross-validation using multi-class classification, One-vs-All and One-vs-One respectively. After Cluster balancing, the obtained classification accuracies are 94.83%, 94.1%, 90.07% for %50–%50 train-test split and 96.28%, 95.88%, 92.5% for 5-Fold cross-validation using multi-class classification, One-vs-All and One-vs-One respectively. The obtained results have shown that the best model is Smote Balanced 5-CV multiclass classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004279",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Classifier (UML)",
      "Computer science",
      "Confusion matrix",
      "Convolutional neural network",
      "Data mining",
      "Hyperspectral imaging",
      "Machine learning",
      "Multiclass classification",
      "Pattern recognition (psychology)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Özdemir",
        "given_name": "Akın"
      },
      {
        "surname": "Polat",
        "given_name": "Kemal"
      },
      {
        "surname": "Alhudhaif",
        "given_name": "Adi"
      }
    ]
  },
  {
    "title": "An attention enhanced sentence feature network for subtitle extraction and summarization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114946",
    "abstract": "An automatic subtitle summarization of videos not only aims to tackle the problem of content overloading but can also improve the performance of video retrieval, allowing viewers to efficiently access and understand the main content of a video. However, subtitle summarization is a challenging task due to documents being composed of incomplete sentences, meaningless phrases, and informal language. In this paper, we introduce a novel multiple attention mechanism for subtitle summarization to address such issues. We take advantage of both Convolutional Neural Networks (CNNs) and Bidirectional Long Short-Term Memory (Bi-LSTM) Networks to capture the critical information of the sentence that is used to identify the importance of the sentence. Based on the salient sentence score, we introduce the summary generation method to produce a summary of the video. The experiments are conducted on both subtitle documents from educational videos and text documents. To the best of our knowledge, no previous studies have applied multiple-attention mechanisms for summarizing educational videos. Besides, we experiment on two well-known text document datasets, DUC2002, and CNN/Daily Mail, to test the performance of our model. We utilize ROUGE measures for evaluating the generated summaries at 95% confidence intervals. The experimental results demonstrated that our model outperforms the baseline and state-of-the-art models on the ROUGE-1, ROUGE-2, and ROUGE-L scores.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003870",
    "keywords": [
      "Artificial intelligence",
      "Automatic summarization",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Economics",
      "Feature (linguistics)",
      "Information retrieval",
      "Linguistics",
      "Management",
      "Natural language processing",
      "Operating system",
      "Philosophy",
      "Sentence",
      "Subtitle",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Chootong",
        "given_name": "Chalothon"
      },
      {
        "surname": "Shih",
        "given_name": "Timothy K."
      },
      {
        "surname": "Ochirbat",
        "given_name": "Ankhtuya"
      },
      {
        "surname": "Sommool",
        "given_name": "Worapot"
      },
      {
        "surname": "Zhuang",
        "given_name": "Yung-Yu"
      }
    ]
  },
  {
    "title": "Disease profiling in pharmaceutical E-commerce",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115015",
    "abstract": "Pharmaceutical e-commerce platforms need to know users’ disease for product recommendation. However, symptom-based disease prediction is not applicable in this context due to the lack of symptom data. In this study, utilizing only purchase data, a prudent and iterative naïve Bayesian algorithm is proposed for disease prediction with the following advantages. First, it utilizes the freely available drug descriptions to identify some positive samples of a disease, hence converts the problem into a positive and unlabeled learning problem and avoids the costly training samples construction. Second, it proposes a prudent process to refine the quality of potentially positive samples in an iterative Bayesian learning process. This process also involves only one classifier while prior methods often involve multiple classifiers. This process enriches the scant strategies for positive sample selection in positive and unlabeled learning. The test on three liver diseases and three cardiovascular diseases indicate that the proposed algorithm could achieve a precision of 98.64% and recall of 90.90% across six diseases, which are superior to most of the benchmark algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004565",
    "keywords": [
      "Computer science",
      "Disease",
      "E-commerce",
      "Medicine",
      "Operating system",
      "Pathology",
      "Profiling (computer programming)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Xing"
      },
      {
        "surname": "Xu",
        "given_name": "Yunjie Calvin"
      },
      {
        "surname": "Yang",
        "given_name": "Xiaoyuan"
      }
    ]
  },
  {
    "title": "A robust SVM-based approach with feature selection and outliers detection for classification problems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115017",
    "abstract": "This paper proposes a robust classification model, based on support vector machine (SVM), which simultaneously deals with outliers detection and feature selection. The classifier is built considering the ramp loss margin error and it includes a budget constraint to limit the number of selected features. The search of this classifier is modeled using a mixed-integer formulation with big M parameters. Two different approaches (exact and heuristic) are proposed to solve the model. The heuristic approach is validated by comparing the quality of the solutions provided by this approach with the exact approach. In addition, the classifiers obtained with the heuristic method are tested and compared with existing SVM-based models to demonstrate their efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004589",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Feature selection",
      "Heuristic",
      "Machine learning",
      "Margin classifier",
      "Outlier",
      "Pattern recognition (psychology)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Baldomero-Naranjo",
        "given_name": "Marta"
      },
      {
        "surname": "Martínez-Merino",
        "given_name": "Luisa I."
      },
      {
        "surname": "Rodríguez-Chía",
        "given_name": "Antonio M."
      }
    ]
  },
  {
    "title": "A novel plume tracking method in partial 3D diffusive environments using multi-sensor fusion",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114993",
    "abstract": "In recent years, dangerous gas leakage has led to serious consequences in social security. Pervious strategies for locating the odor sources were carried out in 2D environments or the gas sensors were installed at a fixed height, ignoring the characteristics of plume distribution in 3D environments. In some cases, strategies developed in 2D environments may be completely ineffective when be applied to 3D environments. To enhance the success rate and rapidity, a novel method of odor source location in partial 3D diffusive environments based on multi-sensor fusion is proposed. Considering the suspected source existed and weak wind environments, machine olfactory and vision information are both employed to track the plumes and identify its source. To enhance the speed of plume searching and the reliability of plume tracking in the early stages, an autonomous mobile robot (AMR) simulates the social mechanism and hunting behaviors of the gray wolf population and tracks the plumes. A technical strategy is also designed to enable the AMR to complete the task of gas source location successfully and the subsumption architecture is adopted to define and arbitrate behavior priorities to coordinate different behaviors in this paper. The on-site test results show that the average positioning error is 0.13 m, the average running time is 147.7 s and the average distance traveled is 21.35 m. The results show that the proposed method is competent to accomplish the task of leakage gas source localization in partial 3D diffusive environments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004346",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Economics",
      "Engineering",
      "Environmental science",
      "Leakage (economics)",
      "Macroeconomics",
      "Meteorology",
      "Pedagogy",
      "Physics",
      "Plume",
      "Psychology",
      "Real-time computing",
      "Simulation",
      "Systems engineering",
      "Task (project management)",
      "Tracking (education)"
    ],
    "authors": [
      {
        "surname": "Shen",
        "given_name": "Xiangyuan"
      },
      {
        "surname": "Yuan",
        "given_name": "Jie"
      },
      {
        "surname": "Shan",
        "given_name": "Yugang"
      }
    ]
  },
  {
    "title": "Pareto-optimal equilibrium points in non-cooperative multi-objective optimization problems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114995",
    "abstract": "In this paper, we consider a class of multi-objective optimization (MOP) problems where the objective holders are independent humans or human-based entities. These problems are indeed game problems, which we call non-cooperative multi-objective optimization problems (NC-MOP). We discuss that for such problems, the Pareto-Optimal (PO) solutions are not necessarily valid as they primarily require Nash equilibrium (NE) solutions. Instead, we suggest that a new solution concept of the Pareto-optimal Equilibrium (POE) point could be adopted. Such a solution is, in particular, important in engineering design and articulation of new rules and protocols among independent entities. This paper reviews all relevant works that approach the POE concept and investigates the interplay between game problems and multi-objective optimization problems. We present illustrative examples to deepen our understanding of where a POE solution is achievable, as this is not always the case.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100436X",
    "keywords": [
      "Computer science",
      "Mathematical economics",
      "Mathematical optimization",
      "Mathematics",
      "Multi-objective optimization",
      "Pareto optimal",
      "Pareto principle"
    ],
    "authors": [
      {
        "surname": "Monfared",
        "given_name": "Mohammadali Saniee"
      },
      {
        "surname": "Monabbati",
        "given_name": "Sayyed Ehsan"
      },
      {
        "surname": "Kafshgar",
        "given_name": "Atefeh Rajabi"
      }
    ]
  },
  {
    "title": "DPP-VSE: Constructing a variable selection ensemble by determinantal point processes",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115025",
    "abstract": "As an effective tool to analyze high-dimensional data, variable selection is playing an increasingly important role in many fields. In recent years, variable selection ensembles (VSEs) have gained much interest of researchers due to their great potential to improve selection accuracy and to stabilize the results of traditional selection methods. Inspired by one common practice of Bayesian methods, we propose in this paper a novel technique named DPP-VSE to build a VSE by utilizing determinantal point processes (DPP) to infer a distribution of model size. By sampling from this distribution, DPP-VSE has the advantage that the number of variables for a base learner to select can be automatically determined. In contrast to other VSE strategies, it has fewer parameters for users to specify. The experiments conducted with both synthetic and real data illustrate that DPP-VSE performs best under most circumstances when being evaluated with several metrics. Hence, DPP-VSE can be seen as an effective and easy to use method to solve variable selection problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004668",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Feature selection",
      "Geometry",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Point (geometry)",
      "Selection (genetic algorithm)",
      "Variable (mathematics)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Chunxia"
      },
      {
        "surname": "Liu",
        "given_name": "Junmin"
      },
      {
        "surname": "Wang",
        "given_name": "Guanwei"
      },
      {
        "surname": "Li",
        "given_name": "Guanghai"
      }
    ]
  },
  {
    "title": "A knowledge-based automated design system for mechanical products based on a general knowledge framework",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114960",
    "abstract": "In the field of mechanical product design, there have been difficulties in building a general model to represent a design process since the type of mechanical products is diverse and the design process requires different complex domain knowledge. This results in the need for different domain experts to design special system development schemes for different kinds of mechanical products, which greatly increases the cost of the research and development. In addition, there have been difficulties in the automation, intellectualization and acceleration of mechanical product design process, as the process is based on experiential knowledge and relies extensively on the synthetic decision of experts (SDE), which is hard to be integrated into the design process without the participation of experts in person. To cope with these two issues, a knowledge-based automated design (KAD) system for mechanical products was developed in this study. Firstly, a general feature design flow (GFDF) framework was proposed to represent the design knowledge of various mechanical products from initial design stage, including requirement analysis, to the final automated generation of computer-aided design (CAD) model. Based on parametric technology and application programming interface (API) functions, the GFDF framework decomposes the explicit and implicit knowledge of a top-down design process into four rank-correlated features and two feature correlation matrices (FCMs) respectively. Based on the GFDF framework, a requirement analysis method using analysis hierarchy process (AHP) was proposed to transfer qualitative features to quantitative parameters. Secondly, by adopting the support vector regression (SVR) machine, a feature reuse case adaptation (FR-CA) method based on case-based reasoning (CBR) was proposed to achieve the automation and intellectualization of parameter solving integrating the SDE without the actual participation of experts. The FRCA method transforms the SDE into an FCM, which can be identified during the case adaptation process, and maximizes the utilization of solved features in each adaptation process. A comparison experiment between FR-CA and conventional method for case adaptation indicated that the adaptation performance of FR-CA method is better than that of the conventional one. Finally, the KAD system was applied to the design of corn huskers. The result showed that the KAD system could improve the automated, intelligent and rapid design of mechanical products.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004012",
    "keywords": [
      "Artificial intelligence",
      "Bottleneck",
      "Computer science",
      "Design knowledge",
      "Domain knowledge",
      "Embedded system",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Process (computing)",
      "Product (mathematics)",
      "Product design"
    ],
    "authors": [
      {
        "surname": "Long",
        "given_name": "Xinjiani"
      },
      {
        "surname": "Li",
        "given_name": "Haitao"
      },
      {
        "surname": "Du",
        "given_name": "Yuefeng"
      },
      {
        "surname": "Mao",
        "given_name": "Enrong"
      },
      {
        "surname": "Tai",
        "given_name": "Jianjian"
      }
    ]
  },
  {
    "title": "An autonomous vehicle interference-free scheduling approach on bidirectional paths in a robotic mobile fulfillment system",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114932",
    "abstract": "A robotic mobile fulfillment system (RMFS) is a “parts-to-picker” system employing a fleet of autonomous vehicles (AV), which transport pods between a storage area and picking stations. In this paper, an AV interference-free scheduling on bidirectional paths (IFSB) approach is studied and shows better working efficiency than unidirectional paths. In order to model AV scheduling on bidirectional path with interference-free constraints, a no-wait flexible process job shop scheduling problem (NWFPJSP) is employed to evaluate total completion time. A mathematical model aiming to minimize the total completion time is presented with as constraint interference-free operation. An A* algorithm is modified for path planning and a simulated annealing algorithm (SA) is employed for scheduling AVs and their paths. Based on a case study with a small RMFS, the IFSB approach results in a 40% increase of the storage capacity and a 22% decrease of the total completion time when compared to a unidirectional approach. Meanwhile, the number of AVs stop-starts are decreased by 40% and the paths length for all AV are decreased by 36%. This indicates the great potential of bidirectional paths in future logistic applications, which may increase the storage capacity, working efficiency, technical health of the AVs, and environmental sustainability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003730",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Distributed computing",
      "Mathematical optimization",
      "Mathematics",
      "Mobile robot",
      "Motion planning",
      "Real-time computing",
      "Robot",
      "Scheduling (production processes)",
      "Simulated annealing",
      "Simulation"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Yangjun"
      },
      {
        "surname": "Zhao",
        "given_name": "Ning"
      },
      {
        "surname": "Lodewijks",
        "given_name": "Gabriel"
      }
    ]
  },
  {
    "title": "Integrated decision support system for rich vehicle routing problems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114998",
    "abstract": "Recent economic and environmental constraints push supply chain management systems to adopt closed-loop supply chain operating modes that have to address very complex problems including the end-user quality of services, environmental considerations, and daily transportation time variations. Relevant and challenging research areas require a proper coordination between the data provider software (Transport Management Software) and the operational research tool in charge of trip definition. This paper proposes a decision support system applied to the Vehicle Routing Problem able to tackle very large instances with real-life constraints. Our contribution is to propose an architecture that handle both static resolution prior to the completion of routes and update them in a dynamical context during their completions. This is implemented through a REST based API using numerous state-of-the-art operational research methods. Moreover, this system in used in practice by the Mapotempo company.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004395",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer network",
      "Computer science",
      "Context (archaeology)",
      "Decision support system",
      "Engineering",
      "Epistemology",
      "Law",
      "Operations research",
      "Paleontology",
      "Philosophy",
      "Political science",
      "Programming language",
      "Quality (philosophy)",
      "Routing (electronic design automation)",
      "Software",
      "Supply chain",
      "Supply chain management",
      "Vehicle routing problem"
    ],
    "authors": [
      {
        "surname": "Lacomme",
        "given_name": "Philippe"
      },
      {
        "surname": "Rault",
        "given_name": "Gwénaël"
      },
      {
        "surname": "Sevaux",
        "given_name": "Marc"
      }
    ]
  },
  {
    "title": "Spatiotemporal trajectory clustering: A clustering algorithm for spatiotemporal data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115048",
    "abstract": "Spatial technologies generate large datasets quickly and continuously. The purpose of this study is to develop a clustering algorithm to mine spatiotemporal co-location events in trajectory datasets. We present a spatiotemporal algorithm for sub-trajectory clustering that divides a trajectory into line segments and groups theses sub-trajectories on the basis of both spatial and temporal aspects by extending DBSCAN (Density Based Spatial Clustering of Applications with Noise) algorithm. We adopt the concepts of entropy and silhouette index to validate the clusters. Experiments conducted on two different real datasets demonstrate that the proposed clustering algorithm effectively discovers optimal clusters. Furthermore, experimental results reveal hidden and useful clusters and demonstrate that the proposed algorithm outperforms the CorClustST (Correlation-based Clustering of Big Spatiotemporal Datasets), and the ST-OPTICS (Spatiotemporal-Ordering Points to Identify Clustering Structure) algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004899",
    "keywords": [
      "Affinity propagation",
      "Algorithm",
      "Artificial intelligence",
      "Astronomy",
      "CURE data clustering algorithm",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "DBSCAN",
      "Data mining",
      "Data stream clustering",
      "Fuzzy clustering",
      "Pattern recognition (psychology)",
      "Physics",
      "Silhouette",
      "Trajectory"
    ],
    "authors": [
      {
        "surname": "Ansari",
        "given_name": "Mohd Yousuf"
      },
      {
        "surname": "Mainuddin",
        "given_name": ""
      },
      {
        "surname": "Ahmad",
        "given_name": "Amir"
      },
      {
        "surname": "Bhushan",
        "given_name": "Gopal"
      }
    ]
  },
  {
    "title": "The promises and perils of Automatic Identification System data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114975",
    "abstract": "Automatic Identification System (AIS) is used to identify vessels in maritime navigation. Currently, it is used for various commercial purposes. However, the abundance and lack of quality of AIS data make it difficult to capitalize on its value. Therefore, an understanding of both the limitations of AIS data and the opportunities is important to maximize its value, but these have not been clearly stated in the existing literature. This study aims to help researchers and practitioners understand AIS data by identifying both the promises and perils of AIS data. We identify the different applications and limitations of AIS data in the literature and build upon them in a sequential mixed-design study. We first identify the promises and perils that exist in the literature. We then analyze AIS data from the port of Amsterdam quantitatively to detect noise and to find the perils researchers and practitioners could encounter. Our results incorporate quantitative findings with qualitative insights obtained from interviewing domain experts. This study extends the literature by considering multiple limitations of AIS data across different domains at the same time. Our results show that the amount of noise in AIS data depends on factors such as the equipment used, external factors, humans, dense traffic etc. The contribution that our paper makes is in combining and making a comprehensive list of both the promises and perils of AIS data. Consequently, this study helps researchers and practitioners to (i) identify the sources of noise, (ii) to reduce the noise in AIS data and (iii) use it for the benefits of their research or the optimization of their operations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004164",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Data collection",
      "Data mining",
      "Data quality",
      "Data science",
      "Domain (mathematical analysis)",
      "Economics",
      "Epistemology",
      "Identification (biology)",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Metric (unit)",
      "Noise (video)",
      "Operations management",
      "Philosophy",
      "Quality (philosophy)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Emmens",
        "given_name": "Ties"
      },
      {
        "surname": "Amrit",
        "given_name": "Chintan"
      },
      {
        "surname": "Abdi",
        "given_name": "Asad"
      },
      {
        "surname": "Ghosh",
        "given_name": "Mayukh"
      }
    ]
  },
  {
    "title": "Process mining technology selection with spherical fuzzy AHP and sensitivity analysis",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114999",
    "abstract": "Process mining (PM) supports organizations by improving their processes using event log data collected from information technology systems. Its primary purposes are discovering actual process models, monitoring and comparing actual and desired workflows, and enhancing processes by considering the discovered model and desired flow. Because process mining gains attraction day by day, various technology companies developed process mining tools to support organizations managing their business processes with data science. Technology selection is a complicated multi-criteria decision-making (MDCM) problem under several criteria and experts’ evaluation, including uncertainty and subjectivity. Spherical fuzzy set is a powerful concept to cope with uncertainty by presenting a wider decision-making area and identifying hesitancy. A fuzzy MDCM approach based on spherical fuzzy AHP is offered in this study to manage the problem of selecting process mining technology under uncertain and ambiguous conditions. Then, one-at-a-time sensitivity analysis is applied to reduce the decision-makers’ subjectivity. This study results in that Price, Process Discovery, Process Analysis&Analytics are the most relevant criteria to decide PM technology. It is interesting that even although Operational Support is one of the less important criteria, it may change the decision on selecting the best PM technology.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004401",
    "keywords": [
      "Analytic hierarchy process",
      "Analytics",
      "Artificial intelligence",
      "Business",
      "Business process",
      "Business process management",
      "Computer science",
      "Data mining",
      "Data science",
      "Database",
      "Decision support system",
      "Electronic engineering",
      "Engineering",
      "Event (particle physics)",
      "Fuzzy logic",
      "Information technology",
      "Mathematics",
      "Operating system",
      "Operations management",
      "Operations research",
      "Physics",
      "Process (computing)",
      "Process management",
      "Process mining",
      "Quantum mechanics",
      "Risk analysis (engineering)",
      "Selection (genetic algorithm)",
      "Sensitivity (control systems)",
      "Work in process",
      "Workflow"
    ],
    "authors": [
      {
        "surname": "Dogan",
        "given_name": "Onur"
      }
    ]
  },
  {
    "title": "Application of the novel harmony search optimization algorithm for DBSCAN clustering",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115054",
    "abstract": "At present, the DBSCAN clustering algorithm has been commonly used principally due to its ability in discovering clusters with arbitrary shapes. When the cluster number K is predefined, though the partitional clustering methods can perform efficiently, they cannot process the non-convex clustering and easily fall into local optimum. Thereby the concept of K-DBSCAN clustering is proposed in this paper. But the basic DBSCAN has a crucial defect, that is, difficult to predict the suitable clustering parameters. Here, the well-known harmony search (HS) optimization algorithm is considered to deal with this problem. By modifying the original HS, the novel harmony search (novel-HS) is put forward, which can improve the accuracy of results as well as enhance the robustness of optimization. In K-DBSCAN, the novel-HS is used to optimize the clustering parameters of DBSCAN to obtain better clustering effect with the number of K classifications. Experimental results show that the designed clustering method has superior performance to others and can be successfully considered as a new clustering scheme for further research.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004954",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "CURE data clustering algorithm",
      "Chemistry",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "DBSCAN",
      "Data mining",
      "Gene",
      "Harmony search",
      "Pattern recognition (psychology)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Qidan"
      },
      {
        "surname": "Tang",
        "given_name": "Xiangmeng"
      },
      {
        "surname": "Elahi",
        "given_name": "Ahsan"
      }
    ]
  },
  {
    "title": "Testing an instrument to measure the BPMS-KM Support model",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115005",
    "abstract": "BPMS (Business Process Management System) represents a type of software that automates the organizational processes looking for efficiency. Since the knowledge of organizations lies in their processes, it seems probable that a BPMS can be used to manage the knowledge applied in these processes. Through the BPMS-KM Support Model, this study aims to determine the reliability and validity of a 65-item instrument to measure the utility and the use of a BPMS for knowledge management (KM). A questionnaire was sent to 242 BPMS users and to determine its validity, a factorial analysis was conducted. The results showed that the measuring instrument is trustworthy and valid. It represents implications for research, since it provides an instrument validated for research on the success of a BPMS for KM. There would also be practical implications, since managers can evaluate the use of BPMS, in addition to automating processes to manage knowledge.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004462",
    "keywords": [
      "Computer science",
      "Computer security",
      "Data mining",
      "Knowledge management",
      "Measure (data warehouse)",
      "Operating system",
      "Physics",
      "Power (physics)",
      "Process (computing)",
      "Programming language",
      "Quantum mechanics",
      "Reliability (semiconductor)",
      "Software",
      "Trustworthiness"
    ],
    "authors": [
      {
        "surname": "Martín-Navarro",
        "given_name": "Alicia"
      },
      {
        "surname": "Lechuga Sancho",
        "given_name": "María Paula"
      },
      {
        "surname": "Medina-Garrido",
        "given_name": "José Aurelio"
      }
    ]
  },
  {
    "title": "Inductive Gaussian representation of user-specific information for personalized stress-level prediction",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114912",
    "abstract": "The accurate prediction of stress in a person’s life has a significant effect on improving personal health and the national economy. Since individuals have different historical circumstances and personality traits, stress symptoms and levels may vary from person to person. Thus, most studies on stress prediction pay attention to personalized models, which determine the personal stress level using user-specific information and heterogeneous stress-related data. However, these models cannot elaborately handle the uncertainty caused by the sparsity, data imbalance, irregularity, and high-dimensionality of user-specific information. In particular, out-of-sample users increase uncertainty. To cope with the problem, we propose a personalized stress-level prediction model with inductive Gaussian representation (PSP-IGR), which exploits heterogeneous inputs with a unified end-to-end approach. PSP-IGR extracts feature vectors from the heterogeneous inputs via Gaussian sampling, domain rules, and deep learning, depending on the characteristics of each input. Especially, PSP-IGR inductively generates a Gaussian feature vector called IGR by Gaussian sampling from the shared contents of user-specific information. Thus, PSP-IGR not only generalizes to both in-sample and out-of-sample users effectively but also deals with the uncertainty problem caused by limitations of healthcare datasets. Also, since we fuse the extracted feature vectors considering their characteristics (Gaussian and point vectors), we can preserve the expressiveness of each feature vector. Experiments on a real-world dataset, including survey results, wearable sensor signals, and contexts, demonstrate that PSP-IGR shows higher accuracy in predicting individual stress-level than previous models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003535",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Gaussian",
      "Human–computer interaction",
      "Law",
      "Linguistics",
      "Machine learning",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Representation (politics)",
      "Stress (linguistics)"
    ],
    "authors": [
      {
        "surname": "Oh",
        "given_name": "Byungkook"
      },
      {
        "surname": "Hwang",
        "given_name": "Jimin"
      },
      {
        "surname": "Seo",
        "given_name": "Seungmin"
      },
      {
        "surname": "Chun",
        "given_name": "Sejin"
      },
      {
        "surname": "Lee",
        "given_name": "Kyong-Ho"
      }
    ]
  },
  {
    "title": "Information security decisions of firms considering security risk interdependency",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114990",
    "abstract": "Information security management becomes more challenging nowadays due to the diverse security risk interdependency between firms. Prior researches rarely consider the impact of risk interdependency on security decisions. This paper comprehensively considers two types of security risk interdependency caused by the nature of information assets and the technical similarity. We find that it is necessary to distinguish the complementary and substitutable information assets since they have different effects on the firm’s investment incentive. As for the risk interdependency caused by the nature of the information assets, although both the high complementation degree and high substitution degree inhibit firms’ incentives to invest, the underlying reasons are different. Besides, for another risk interdependency, the technical similarity enhances the investment incentive of the complementary firms but suppresses that of the substitutable firms. Moreover, the free-riding problem is unavoidable when the firm makes security decisions independently. Thus, we propose two efficient mechanisms to coordinate the firm’s investment incentive: the effort-based mechanism and the liability-based mechanism. The effort-based mechanism demands the firm obtain a reward from its cooperative firm according to its security effort level. The liability-based mechanism demands the breached firm take the liability by compensating the non-breached firm. We find that both two mechanisms are efficient, and could guide firms to solve the problem of opportunism and shirking responsibility in practice. Finally, for generality, we extend our model to an asymmetric case and find that most of the results are robust.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004310",
    "keywords": [
      "Business",
      "Computer science",
      "Computer security",
      "Economics",
      "Finance",
      "Incentive",
      "Industrial organization",
      "Information security",
      "Interdependence",
      "Law",
      "Liability",
      "Microeconomics",
      "Political science",
      "Risk analysis (engineering)"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Yong"
      },
      {
        "surname": "Wang",
        "given_name": "Linping"
      },
      {
        "surname": "Cheng",
        "given_name": "Dong"
      },
      {
        "surname": "Dai",
        "given_name": "Tao"
      }
    ]
  },
  {
    "title": "Optimal insurance contract design with “No-claim Bonus and Coverage Upper Bound” under moral hazard",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115050",
    "abstract": "In this paper, an optimal insurance problem from the view of a risk-averse individual under moral hazard is considered. Based on the Principal-Agent theory, we introduce a combined incentive tool of “No-claim Bonus and Coverage Upper Bound” to encourage the insured to make a higher risk-reducing effort and obtain higher expected utility than in a basic contract. We confirm if marginal expected utility brought by the increase of risk-reducing effort decreases at the critical point of transition between two contracts, the combined incentive tool can restrain the moral hazard of the insured. Moreover, Pareto efficiency improvement of the tool is visually displayed by two case examples.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004917",
    "keywords": [
      "Actuarial science",
      "Auto insurance risk selection",
      "Business",
      "Casualty insurance",
      "Chemistry",
      "Computer science",
      "Economics",
      "Hazard",
      "Incentive",
      "Insurance policy",
      "Mathematical analysis",
      "Mathematics",
      "Microeconomics",
      "Moral hazard",
      "Morale hazard",
      "Organic chemistry",
      "Upper and lower bounds"
    ],
    "authors": [
      {
        "surname": "MA",
        "given_name": "Benjiang"
      },
      {
        "surname": "Zhang",
        "given_name": "Yechun"
      },
      {
        "surname": "Qin",
        "given_name": "Yifang"
      },
      {
        "surname": "Bashir",
        "given_name": "Muhammad Farhan"
      }
    ]
  },
  {
    "title": "A compound of feature selection techniques to improve solar radiation forecasting",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114979",
    "abstract": "The prediction of Global Horizontal Irradiance (GHI) allows to estimate in advance the future energy production of photovoltaic systems, thus ensuring their full integration into the electricity grids. This paper investigates the effectiveness of using exogenous inputs in performing short-term GHI forecasting. To this aim, we identified a subset of relevant input variables for predicting GHI by applying different feature selection techniques. The results revealed that the most significant input variables for predicting GHI are ultraviolet index, cloud cover, air temperature, relative humidity, dew point, wind bearing, sunshine duration and hour-of-the-day. The predictive performance of the selected features was evaluated by feeding them into five different machine learning models based on Feedforward, Echo State, 1D-Convolutional, Long Short-Term Memory neural networks and Random Forest, respectively. Our Long Short-Term Memory solution presents the best prediction performance among the five models, predicting GHI up to 4 h ahead with a Mean Absolute Deviation (MAD) of 24.51%. Then, to demonstrate the effectiveness of using exogenous inputs for short-term GHI forecasting, we compare the multivariate models against their univariate counterparts. The results show that exogenous inputs significantly improve the forecasting performance for prediction horizons greater than 15 min, reducing errors by more than 22% in 4 h ahead predictions, while for very short prediction horizons (i.e. 15 min) the improvements are negligible.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004206",
    "keywords": [
      "Artificial neural network",
      "Computer science",
      "Dew point",
      "Environmental science",
      "Feature selection",
      "Irradiance",
      "Machine learning",
      "Meteorology",
      "Multivariate statistics",
      "Physics",
      "Quantum mechanics",
      "Term (time)",
      "Univariate"
    ],
    "authors": [
      {
        "surname": "Castangia",
        "given_name": "Marco"
      },
      {
        "surname": "Aliberti",
        "given_name": "Alessandro"
      },
      {
        "surname": "Bottaccioli",
        "given_name": "Lorenzo"
      },
      {
        "surname": "Macii",
        "given_name": "Enrico"
      },
      {
        "surname": "Patti",
        "given_name": "Edoardo"
      }
    ]
  },
  {
    "title": "Fully adaptive dictionary for online correntropy kernel learning using proximal methods",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114976",
    "abstract": "We introduce a new sparse variant of the Correntropy Kernel Learning model, hereafter named Fully ADaptive Online Sparse CKL (FADOS-CKL), for online system identification in the presence of outliers. For this purpose, we develop a fully adaptive dictionary of support vectors (SVs) so that it can either grow (as most of the kernel models to date do) or shrink if some of the SVs become obsolete with time. For inclusion of SVs into the dictionary, existing strategies (ALD, Novelty, Surprise, and Coherence) have their performances compared in this paper, while for elimination of SVs we adopt a class of optimization techniques known as proximal methods. Dictionary updating in FADOS-CKL is carried out on-the-fly by the introduction of a recursive methodology based on the Sherman-Morrison-Woodbury formula to update the kernel matrix and its inverse with low computational complexity. Aiming at achieving the smallest predictive errors with the highest sparsity level, a comprehensive performance comparison involving the FADOS-CKL model and powerful alternatives is carried out using two large-scale benchmark datasets for different levels of outliers contamination. The results indicate an impressive balance between reduction in the dictionary size and the corresponding generalization capability of the proposed FADOS-CKL model over the existing alternatives.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004176",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Combinatorics",
      "Computer science",
      "Generalization",
      "Geodesy",
      "Geography",
      "Kernel (algebra)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Outlier"
    ],
    "authors": [
      {
        "surname": "Duarte",
        "given_name": "Michael S."
      },
      {
        "surname": "Barreto",
        "given_name": "Guilherme A."
      }
    ]
  },
  {
    "title": "Multi-level interpretable logic tree analysis: A data-driven approach for hierarchical causality analysis",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115035",
    "abstract": "This paper presents a data-driven approach for a hierarchical causality analysis of faults in a complex system, named a multi-level interpretable logic tree (MILTA). From a representative faults dataset, this approach constructs dependent trees that explain the relation structure between the root-causes, intermediate causes and faults with the minimum expert involvement. The MILTA model combines the discovered knowledge in dataset (KDD) in the form of feasible solutions and the fault tree analysis (FTA), level after level, as long as the root-causes are not completely uncovered. A burn-and-build algorithm is developed to maximize the representability of the feasible solutions with a minimum number of patterns. Using Bayes’ theorem, the hierarchical causality between the root-causes and the fault is captured through different causality rules that quantify the effects of the root-causes on the fault occurrence. An actuator system dataset that consists of complex fault and normal operation states is used as an illustrative example. The MILTA model finds the same documented root-cause and uncovers other root-causes with higher accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004760",
    "keywords": [
      "Artificial intelligence",
      "Causality (physics)",
      "Computer science",
      "Data mining",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Physics",
      "Quantum mechanics",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Waghen",
        "given_name": "Kerelous"
      },
      {
        "surname": "Ouali",
        "given_name": "Mohamed-Salah"
      }
    ]
  },
  {
    "title": "Learning audio sequence representations for acoustic event classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115007",
    "abstract": "Acoustic Event Classification (AEC) has become a significant task for machines to perceive the surrounding auditory scene. However, extracting effective representations that capture the underlying characteristics of the acoustic events is still challenging. Previous methods mainly focused on designing the audio features in a ‘hand-crafted’ manner. Interestingly, data-learnt features have been recently reported to show better performance. Up to now, these were only considered on the frame-level. In this article, we propose an unsupervised learning framework to learn a vector representation of an audio sequence for AEC. This framework consists of a Recurrent Neural Network (RNN) encoder and a RNN decoder, which respectively transforms the variable-length audio sequence into a fixed-length vector and reconstructs the input sequence on the generated vector. After training the encoder-decoder, we feed the audio sequences to the encoder and then take the learnt vectors as the audio sequence representations. Compared with previous methods, the proposed method can not only deal with the problem of arbitrary-lengths of audio streams, but also learn the salient information of the sequence. Extensive evaluation on a large-size acoustic event database is performed, and the empirical results demonstrate that the learnt audio sequence representation yields a significant performance improvement by a large margin compared with other state-of-the-art hand-crafted sequence features for AEC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004486",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Event (particle physics)",
      "Genetics",
      "Natural language processing",
      "Physics",
      "Quantum mechanics",
      "Sequence (biology)",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zixing"
      },
      {
        "surname": "Liu",
        "given_name": "Ding"
      },
      {
        "surname": "Han",
        "given_name": "Jing"
      },
      {
        "surname": "Qian",
        "given_name": "Kun"
      },
      {
        "surname": "Schuller",
        "given_name": "Björn W."
      }
    ]
  },
  {
    "title": "Effective multiple pedestrian tracking system in video surveillance with monocular stationary camera",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114992",
    "abstract": "Multiple pedestrian tracking in video surveillance is still a pressing challenge, especially under static and dynamic occlusions and target appearance variations. Considering these complex environments in video surveillance, a multiple pedestrian tracking system with special processing procedures is proposed in this paper. In the proposed tracking system, pedestrian candidates are detected on each frame and registered as the tracked targets or associated with existing targets when their situations are suitable. The registered pedestrian targets are tracked frame by frame and terminated when the termination criteria are satisfied. In order to distinguish these target individuals, multi-sample adaptive modeling (MSAM) is proposed, which is used to adapt to a new target’s unpredictable pose variation. Furthermore, static occlusions are annotated for each scene, which may occlude pedestrians in the annotated regions. These occluded targets are modified by the assigned rules and treated differently in the process of target association. Aiming to enhance the effect of target association, each target’s location on the current frame is predicted with information on the previous frames using a Kalman filter. The predicted location is regarded as the center of the search region of the corresponding target. The experimental results show that the proposed tracker achieves the best performance among the five state-of-the-art trackers on three publicly available databases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004334",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Geography",
      "Kalman filter",
      "Monocular vision",
      "Pedagogy",
      "Pedestrian",
      "Pedestrian detection",
      "Psychology",
      "Tracking (education)",
      "Tracking system",
      "Video processing",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zhihui"
      },
      {
        "surname": "Li",
        "given_name": "Ming"
      },
      {
        "surname": "Lu",
        "given_name": "Yu"
      },
      {
        "surname": "Bao",
        "given_name": "Yongtang"
      },
      {
        "surname": "Li",
        "given_name": "Zhe"
      },
      {
        "surname": "Zhao",
        "given_name": "Jianli"
      }
    ]
  },
  {
    "title": "Automatic modulation classification using different neural network and PCA combinations",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114931",
    "abstract": "This paper highlights one of the most promising research directions for automatic modulation recognition algorithms, although it does not provide a final solution. We study the design of a high-precision classifier for recognizing PSK, QAM and DVB-S2 APSK modulation signals. First, an efficient pattern recognition model that includes three main modules for feature extraction, feature optimization and classification is presented. The feature extraction module extracts the most useful combinations of up to six high-order cumulants that embed sixth-order moments and uses logarithmic function properties to improve the distribution curve of the six-order cumulants. To the best of our knowledge, this is the first time that these combinations and the improved feature criteria have been applied in this area. The optimizer module selects optimal features via principal component analysis (PCA). Then, in the classifier module, we study two important supervised neural network classifiers (i.e., multilayer perceptron (MLP)- and radial basis function (RBF)-based classifiers). Through an experiment, we determine the best classifier for recognizing the considered modulations. Then, we propose an RBF-PCA combined recognition system in which an optimization module is added to enhance the overall classifier performance. This module optimizes the classifier performance by searching for the best subset of features to use as the classifier input. The simulation results illustrate that the RBF-PCA classifier combination achieves high recognition accuracy even at a low signal-to-noise ratio (SNR) and with limited training samples.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003729",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Classifier (UML)",
      "Computer science",
      "Feature extraction",
      "Multilayer perceptron",
      "Pattern recognition (psychology)",
      "Principal component analysis"
    ],
    "authors": [
      {
        "surname": "Ali",
        "given_name": "Ahmed K."
      },
      {
        "surname": "Erçelebi",
        "given_name": "Ergun"
      }
    ]
  },
  {
    "title": "A fractional Black-Scholes model with stochastic volatility and European option pricing",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114983",
    "abstract": "In this paper, we introduce the stochastic volatility into the FMLS (finite moment log-stable) model to capture the effect of both jumps and stochastic volatility. However, this additional stochastic source adds another degree of complexity in seeking for analytical formula when pricing European options, as the involved FPDE (fractional partial differential equation) system governing option prices is now of three dimensions. Albeit difficult, we have still managed to present an analytical solution expressed in terms of Fourier cosine series, after a two-step solution procedure is developed for the target FPDE system. This solution is different from the most literature as it is truly explicit, involving no Fourier inversion. It is also shown through the numerical experiments that it converges very rapidly and has potential to be applied in practice.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004243",
    "keywords": [
      "Applied mathematics",
      "Black–Scholes model",
      "Computer science",
      "Econometrics",
      "Implied volatility",
      "Mathematical economics",
      "Mathematics",
      "Stochastic volatility",
      "Valuation of options",
      "Volatility (finance)",
      "Volatility smile"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Xin-Jiang"
      },
      {
        "surname": "Lin",
        "given_name": "Sha"
      }
    ]
  },
  {
    "title": "A novel integrating between tool path optimization using an ACO algorithm and interpreter for open architecture CNC system",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114988",
    "abstract": "This article proposes an effective methodology to reduce manufacturing time through the creation of quasi-optimal G command sequences, based on initial machining codes derived from CAD/CAM Service-Oriented Architecture (SOA) software. The optimization steps involved minimizing travel path time for CNC manufacturing machine milling and drilling. Also, identifying the optimal order of operation that enabled the shortest travel path for the cutting tool. These steps resulted in consistent enhancements of approximately 10.41% and 16.58% for milling and drilling machining, respectively. Furthermore, high-performing Ant Colony Optimization (ACO) algorithms were used to optimize the travel path time. In the context of automatic NC program production, the optimization of the cutting tool’s travel path could be achieved by integrating the ACO algorithm and the Open Architecture Control (OAC) into commercial CAD/CAM. In this paper, a new optimized generation of CNC systems, based on combining the Open Architecture Control (OAC) technology supported by the G-code data model and the optimization algorithm are presented.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004292",
    "keywords": [
      "Algorithm",
      "Ant colony optimization algorithms",
      "Architecture",
      "Art",
      "Biology",
      "CAD",
      "Computer science",
      "Context (archaeology)",
      "Engineering",
      "Engineering drawing",
      "Machining",
      "Mechanical engineering",
      "Numerical control",
      "Open architecture",
      "Operating system",
      "Paleontology",
      "Path (computing)",
      "Software",
      "Tool path",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Hatem",
        "given_name": "Noor"
      },
      {
        "surname": "Yusof",
        "given_name": "Yusri"
      },
      {
        "surname": "Kadir",
        "given_name": "Aini Zuhra A."
      },
      {
        "surname": "Latif",
        "given_name": "Kamran"
      },
      {
        "surname": "Mohammed",
        "given_name": "M.A"
      }
    ]
  },
  {
    "title": "Squirrel search algorithm for portfolio optimization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114968",
    "abstract": "Portfolio Optimization is a standard financial engineering problem. It aims for finding the best allocation of resources for a set of assets. This problem has been studied and different models have been proposed since the classical Mean-Variance model was introduced by Harry Markowitz in 1952 and the later modified version by William Sharpe. The inclusion of real-life constraints to the problem has led to the introduction of the extended Mean-Variance model. However, the successes of nature-inspired algorithms in hard computational optimization problems have encouraged researchers to design and apply these algorithms for a variety of optimization problems. In this paper, we design and adapt a Squirrel Search Algorithm (SSA) for the unconstrained and constrained portfolio optimization problems. SSA is a very recent swarm intelligence algorithm inspired by the dynamic foraging behavior of flying squirrels. The proposed SSA metaheuristic approach is compared with a variety of approaches presented in the literature such as classical single metaheuristics, hybrid metaheuristic approaches and multi-objective optimization approaches for portfolio optimization. Comparative analysis and computational results using different performance indicators show the superiority of the proposed approach for the unconstrained portfolio optimization using both extended Mean-Variance and Sharpe models. For the constrained version of the problem, the proposed approach has also achieved highly competitive results for the different models adopted.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004097",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Economics",
      "Financial economics",
      "Mathematical optimization",
      "Mathematics",
      "Meta-optimization",
      "Metaheuristic",
      "Modern portfolio theory",
      "Optimization problem",
      "Parallel metaheuristic",
      "Portfolio",
      "Portfolio optimization"
    ],
    "authors": [
      {
        "surname": "Dhaini",
        "given_name": "Mahdi"
      },
      {
        "surname": "Mansour",
        "given_name": "Nashat"
      }
    ]
  },
  {
    "title": "Real-time nondestructive fish behavior detecting in mixed polyculture system using deep-learning and low-cost devices",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115051",
    "abstract": "Fish behavior has attracted increasing attention in global aquaculture because it provides important information about productivity and fish quality. The use of images to detect fish behavior has shown potential in aquaculture behavioral studies by providing higher spatial resolution, efficiency, and accuracy than conventional approaches such as manual measurement. In addition, it allows for more quantitative data analysis than do other methods. To date, conventional image processing approaches to monitor fish behavior have been based primarily on appearance, morphology, and color information. This approach is complex and/or time-consuming and limits the practicality of such methods in aquaculture. To address these problems, we present herein a noninvasive, rapid, low-cost procedure based on an underwater imaging system and a deep learning framework to detect fish behavior with high accuracy in a mixed polyculture system. The specific objectives of this study are (1) to design a low-cost underwater imaging system that can describe and quantify fish behavior via visual images, and (2) to develop a lightweight deep learning structure to rapidly and accurately detect fish behavior under various conditions. Toward this end, images of fish are first captured via a low-cost imaging system, following which they are preprocessed to reduce noise and enhance data information. Finally, an improved You Only Look Once version 3 Lite (YOLOv3-Lite) network with a novel backbone structure is used to improve the pooling block and loss function and thereby better recognize fish behavior. The proposed method was tested on a real dataset and produced a Precision of 0.897, a Recall of 0.884, an intersection over union of 0.892, and 240 frames per second. Furthermore, when compared with faster region-convolutional neural network, YOLO, YOLOv2, YOLOv3, and single shot multi-Box detector, the performance of each evaluation metric of the proposed method was improved by 10%–20%. This comprehensive analysis indicates that the proposed method provides state-of-the-art performance and may be used in fish farms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004929",
    "keywords": [
      "Aquaculture",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Fish <Actinopterygii>",
      "Fishery",
      "Geology",
      "Image (mathematics)",
      "Machine learning",
      "Noise (video)",
      "Oceanography",
      "Pattern recognition (psychology)",
      "Polyculture",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Jun"
      },
      {
        "surname": "Zhao",
        "given_name": "Dandan"
      },
      {
        "surname": "Zhang",
        "given_name": "Yanfeng"
      },
      {
        "surname": "Zhou",
        "given_name": "Chengquan"
      },
      {
        "surname": "Chen",
        "given_name": "Wenxuan"
      }
    ]
  },
  {
    "title": "Overall efficiency of operational process with undesirable outputs containing both series and parallel processes: A SBM network DEA model",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115062",
    "abstract": "This paper proposes a new slacks-based measure network data envelopment analysis (SBM-NDEA) model with undesirable outputs to evaluate the performance of production processes that have complex structure containing both series and parallel processes. We demonstrate the proposed approach by evaluating Chinese commercial banks during 2012–2016. The operational process of these banks could be divided into deposit producing and deposit utilizing processes connected serially, while deposit utilizing process is further divided into profit generating and deposit reserve interest earning processes, which are parallel. The overall efficiency is decomposed into deposit producing and deposit utilizing efficiency. Deposit utilizing efficiency is further decomposed into profit generating and deposit reserve interest earning efficiency, respectively. Our empirical results suggest that the overall inefficiency is mainly from the profit generating process. The results also estimate the adjustment of variables for the network process of an inefficient bank.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005030",
    "keywords": [
      "Biology",
      "Computer science",
      "Operating system",
      "Paleontology",
      "Process (computing)",
      "Series (stratigraphy)"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Xiao"
      },
      {
        "surname": "Emrouznejad",
        "given_name": "Ali"
      },
      {
        "surname": "Yu",
        "given_name": "Wenqi"
      }
    ]
  },
  {
    "title": "Automatic mass spectra recognition for Ultra High Vacuum systems using multilabel classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114959",
    "abstract": "In Ultra High-Vacuum (UHV) systems it is common to find a mixture of many gases originating from surface outgassing, leaks and permeation that contaminate vacuum chambers and cause issues to reach ultimate pressures. The identification of these contaminants is, in general, done manually by trained technicians from the analysis of mass spectra. This task is time consuming and can lead to misinterpretation or partial understanding of issues. The challenge resides in the rapid identification of these contaminants by using some automatic gas identification technique. This paper explores the automatic and simultaneous identification of 80 molecules, including some of the most commonly present in this kind of environment by means of multilabel classification techniques. The best performance is drawn from a dependent binary relevance method trained by extreme gradient boosting. We obtain a Hamming loss of 0.0145 in the test set. The mean binary AUC for the test set was 0.986, and the minimum test AUC was higher than 0.89. A public interactive web app has been developed to allow vacuum users to test the model with their own data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004000",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Machine learning",
      "Mass spectrometry",
      "Mass spectrum",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Mateo",
        "given_name": "Fernando"
      },
      {
        "surname": "Garcés-Iniesta",
        "given_name": "Juan José"
      },
      {
        "surname": "Jenninger",
        "given_name": "Berthold"
      },
      {
        "surname": "Gómez-Sanchís",
        "given_name": "Juan"
      },
      {
        "surname": "Soria-Olivas",
        "given_name": "Emilio"
      },
      {
        "surname": "Chiggiato",
        "given_name": "Paolo"
      }
    ]
  },
  {
    "title": "An adaptive deep learning framework to classify unknown composite power quality event using known single power quality events",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115023",
    "abstract": "Distributed generation (DG) sources are preferred to meet today's energy needs effectively. The addition of many different types of renewable energy sources to the grid causes various problems in signal quality. Detection and classification of these problems increase efficiency by both the producer and the consumer. In the literature, incredibly singular and some composite power quality disturbance (PQD) detection is performed effectively. However, the multitude of composite PQD variations degrades the performance of existing algorithms. In this study, the classification of all PQD variations that may occur is performed by using singular PQD and some composite PQD signals. A different number of subcomponents representing the signal are created according to each signal characteristic. Instantaneous energies from these subcomponents are used as deep learning (DL) input. Deep learning cycles are created as much as the instantaneous energy number of each signal. Each cycle has specific features of defining a single event. Therefore, the proposed approach is able to classify composite PQD signals that it has not encountered before. The proposed method's performance is first evaluated with the known PQD events and compared with the current state-of-the-art methods in the literature. Then, a dataset containing the combinations of different events not encountered during the training is created, and the performance is evaluated on this dataset. In the experiments performed, it is revealed that the proposed framework produces higher performance than other state-of-the-art methods",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004644",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Epistemology",
      "Event (particle physics)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Power (physics)",
      "Power quality",
      "Quality (philosophy)",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Sindi",
        "given_name": "Hatem"
      },
      {
        "surname": "Nour",
        "given_name": "Majid"
      },
      {
        "surname": "Rawa",
        "given_name": "Muhyaddin"
      },
      {
        "surname": "Öztürk",
        "given_name": "Şaban"
      },
      {
        "surname": "Polat",
        "given_name": "Kemal"
      }
    ]
  },
  {
    "title": "Novel expert system to study human stress based on thermographic images",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115024",
    "abstract": "Human stress is a physical or emotional feeling. It can come from any situation or thought that makes one feel frustrated or nervous. Different biological manifestations take place in the presence of stress, such as tension, headache, and insomnia. Recent studies have reported that human stress can be related to facial expressions and fingertips due to temperature changes in the skin. Infrared thermography is a non-invasive technology that allows for the monitoring and analysis of human skin temperature. However, many works reported in the literature perform a manual analysis, depending on expert personnel, resulting in considerable human and economic efforts. In addition, the analyses reported to date with thermography are only based on the study of body parts. To reduce the limitations of these methodologies, expert systems have been proposed that simulate the thought process of a human expert to solve decision problems, which may be helpful for people focused on the health and psychology area who do not have expertise in the study of stress. Therefore, this paper proposes a novel expert system based on infrared thermography and thermal analysis of facial skin and fingertips, a rule-based method, and heuristic knowledge to classify and diagnose human stress in undergraduate university students. The system had the support of experts in the study of human stress and was validated in a stress study. The novel expert system was implemented in a local database that consisted of a group of 100 participants, undergraduate university students, of which 70 were stimulated by the Trier Social Stress Test (TSST) protocol and 30 were not induced to human stress, obtaining an accuracy in the expert system stress classification of 91.0%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004656",
    "keywords": [
      "Artificial intelligence",
      "Biomedical engineering",
      "Computer science",
      "Expert system",
      "Feeling",
      "Human–computer interaction",
      "Infrared",
      "Linguistics",
      "Machine learning",
      "Medicine",
      "Operating system",
      "Optics",
      "Philosophy",
      "Physics",
      "Process (computing)",
      "Psychology",
      "Skin temperature",
      "Social psychology",
      "Stress (linguistics)",
      "Thermography"
    ],
    "authors": [
      {
        "surname": "Resendiz-Ochoa",
        "given_name": "Emmanuel"
      },
      {
        "surname": "Cruz-Albarran",
        "given_name": "Irving A"
      },
      {
        "surname": "Garduño-Ramon",
        "given_name": "Marco A"
      },
      {
        "surname": "Rodriguez-Medina",
        "given_name": "David A"
      },
      {
        "surname": "Osornio-Rios",
        "given_name": "Roque A"
      },
      {
        "surname": "Morales-Hernández",
        "given_name": "Luis A."
      }
    ]
  },
  {
    "title": "An efficient multilevel color image thresholding based on modified whale optimization algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115003",
    "abstract": "Color image segmentation is a vital preprocessing stage in various image processing applications. In threshold-based segmentation, the success of the image segmentation depends mainly on the optimal selection of thresholds. The selection of threshold values for multilevel thresholding is indeed a time-consuming process compared to bi-level thresholding. The issue of optimal threshold selection is formulated as an optimization problem in the case of color image segmentation using multilevel thresholding. To optimize the threshold selection for a multilevel color image thresholding, a modified whale optimization algorithm (MWOA) is proposed in this paper. The Otsu’s and Kapur’s functions have been used in the proposed strategy as a fitness function that can be maximized by MWOA. In the MWOA, the position of the whales is controlled by adapting the cosine function during optimization process. Further, the movements of search agents are regulated during the search process by introducing the correction factors in position updation. These changes incorporated in the MWOA provide a proper balance between the phases of exploration and exploitation and also avoid local optima problem. The performance of the MWOA is evaluated quantitatively and qualitatively based on the best fitness values in terms of PSNR, SSIM, and FSIM, further CPU computing time and Wilcoxon test. The experimental outcomes show that the proposed multilevel optimal color image thresholding using MWOA algorithm yields better performance results in terms of image quality, feature conservation, and convergence rate with less CPU computing time than other state-of-the-art algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004449",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Fitness function",
      "Genetic algorithm",
      "Image (mathematics)",
      "Image segmentation",
      "Machine learning",
      "Otsu's method",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Thresholding"
    ],
    "authors": [
      {
        "surname": "Anitha",
        "given_name": "J."
      },
      {
        "surname": "Immanuel Alex Pandian",
        "given_name": "S."
      },
      {
        "surname": "Akila Agnes",
        "given_name": "S."
      }
    ]
  },
  {
    "title": "Minimum class variance class-specific extreme learning machine for imbalanced classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114994",
    "abstract": "Imbalanced problems occur in real-world applications when the number of majority instances far exceeds the number of minority instances. Traditional extreme learning machine (ELM) classifier becomes biased towards the majority class due to imbalanced learning. To handle this inherent drawback, several modifications of ELM have been proposed such as weighted ELM (WELM), variances-constrained WELM (VW-ELM) to tackle the class imbalance problem effectively. One of our recent works class-specific ELM (CSELM) employs class-specific regularization and has been shown to outperform WELM for imbalanced learning. Motivated by CSELM, this work proposes a minimum class variance class-specific extreme learning machine (MCVCSELM), a variant of CSELM for tackling binary class imbalance problems more effectively. MCVCSELM uses the advantages of both the minimum class variance and the class-specific regularization. The proposed work also has lower computational complexity compared to WELM and VW-ELM. In class-specific cost regulation ELM (CCR-ELM), the calculation of the regularization parameters does not consider class distribution and class overlap. However, the performance of the CCR-ELM is comparable to ELM. MCVCSELM utilizes a class-specific regularization parameter whose value is decided by using the class proportion. The experimental results on 38 binary class datasets with different imbalanced ratios demonstrate that the proposed algorithm outperforms several state-of-the-art methods for imbalanced learning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004358",
    "keywords": [
      "Accounting",
      "Artificial intelligence",
      "Artificial neural network",
      "Business",
      "Class (philosophy)",
      "Computer science",
      "Extreme learning machine",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Variance (accounting)"
    ],
    "authors": [
      {
        "surname": "Raghuwanshi",
        "given_name": "Bhagat Singh"
      },
      {
        "surname": "Shukla",
        "given_name": "Sanyam"
      }
    ]
  },
  {
    "title": "Meta-neuron learning based spiking neural classifier with time-varying weight model for credit scoring problem",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114985",
    "abstract": "This paper presents a meta-neuron learning-based spiking neural classifier with a time-varying weight model (MeST). MeST is developed to handle the class imbalance in classification problems without any data preprocessing methods. Meta-neuron based learning algorithm in MeST uses normalized postsynaptic potentials (global information) and weight of the connection (local information) to determine the sensitivity modulation factor. This modulation factor determines the proportion of the weight update for a given set of presynaptic spikes. The weight update is then embedded in a Gaussian function to determine the time-varying weight update. The centre of the time-varying Gaussian function is determined by the presynaptic spike times. MeST is demonstrated on 10 benchmark datasets from the University of California, Irvine California machine learning repository and then applied to solve credit scoring using three real-world datasets. Performance studies show that the generalization ability of MeST is better than other spiking neural networks with constant weight model, despite having a simple architecture. Furthermore, compared to other non-spiking shallow machine learning classifiers, MeST is a slightly better model for classification using highly imbalanced datasets. This indicates the learnability of a stand-alone classifier on an imbalanced dataset can be increased by using time-varying weights.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004267",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Classifier (UML)",
      "Computer science",
      "Machine learning",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Jeyasothy",
        "given_name": "Abeegithan"
      },
      {
        "surname": "Ramasamy",
        "given_name": "Savitha"
      },
      {
        "surname": "Sundaram",
        "given_name": "Suresh"
      }
    ]
  },
  {
    "title": "Stock market index prediction based on reservoir computing models",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115022",
    "abstract": "Prediction of the financial market price is critical for financial decision-making and market policy-making. Recently, various machine learning and deep learning methods have been adopted to predict financial markets’ movements using historical time series of prices. However, accurate prediction of financial prices is still a long-standing challenge that always calls for new approaches. In this study, a novel machine learning model of reservoir computing is developed to predict stock market indices. The performance of the proposed new model is systematically evaluated using the time series of daily closing prices of seven major international stock market indices including S&P500 Index, New York Stock Exchange Composite, Dow Jones Industrial Average, Nasdaq Composite Index, Financial Times Stock Exchange 100 Index, Nikkei 225 Index, and Shanghai Stock Exchange Index between January 4, 2010, and December 31, 2018 covering 2,272 trading days. The results show that our model outperforms the widely used deep learning methods of long short-term memory and recurrent neural network in most cases. To further evaluate the predictive capability of our model, we compare our model to the other two newly reported deep learning methods in recent studies. Comparative results also show that our model is competitive to those deep learning methods in predicting stock market indices. Our study contributes to the literature by developing novel reservoir computing models for financial market predictions. Meanwhile, our results also provide practical implications for financial practitioners of potential financial applications of reservoir computing in financial time series analysis and predictions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004632",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Composite index",
      "Computer science",
      "Deep learning",
      "Econometrics",
      "Economics",
      "Finance",
      "Financial market",
      "Horse",
      "Index (typography)",
      "Machine learning",
      "Paleontology",
      "Stock exchange",
      "Stock market",
      "Stock market index",
      "Time series",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Wei-Jia"
      },
      {
        "surname": "Tang",
        "given_name": "Yong"
      },
      {
        "surname": "Xiong",
        "given_name": "Jason"
      },
      {
        "surname": "Zhang",
        "given_name": "Yi-Cheng"
      }
    ]
  },
  {
    "title": "Dimensionality reduced robust ordinal regression applied to life cycle assessment",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115021",
    "abstract": "Life Cycle Assessment quantifies the multi-dimensional impact of goods and services and can be handled by Multi-Criteria Decision Analysis. In Multi-Criteria Decision Analysis, Robust Ordinal Regression manages all the compatible preference functions at once when assessing a set of alternatives and a group of preferences on reference alternatives. Robust Ordinal Regression is thus a versatile method of reducing the cognitive effort required by decision makers for eliciting their preference structures in Life Cycle Assessment, although it does not directly operate on noisy alternatives and requires Stochastic Multicriteria Acceptability Analysis to deal with such scenarios. We propose integrating a dimensionality reduction technique, Principal Component Analysis, and Robust Ordinal Regression methods, to reduce the problem dimensionality and ensure the actual problem features are considered. A generated dataset, a dataset from literature and a Life Cycle Assessment case study are used to test the effectiveness of the proposed methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004620",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Curse of dimensionality",
      "Data mining",
      "Dimensionality reduction",
      "Machine learning",
      "Mathematics",
      "Ordinal data",
      "Ordinal regression",
      "Preference",
      "Principal component analysis",
      "Programming language",
      "Regression",
      "Set (abstract data type)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Balugani",
        "given_name": "Elia"
      },
      {
        "surname": "Lolli",
        "given_name": "Francesco"
      },
      {
        "surname": "Pini",
        "given_name": "Martina"
      },
      {
        "surname": "Ferrari",
        "given_name": "Anna Maria"
      },
      {
        "surname": "Neri",
        "given_name": "Paolo"
      },
      {
        "surname": "Gamberini",
        "given_name": "Rita"
      },
      {
        "surname": "Rimini",
        "given_name": "Bianca"
      }
    ]
  },
  {
    "title": "Combination in the theory of evidence via a new measurement of the conflict between evidences",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114987",
    "abstract": "The Belief Function theory (BFT) has been frequently used to combine or aggregate different sources of information. In the BFT, a lot of rules to combine the available pieces of evidence have been proposed. The first one of them was Dempster’s rule of combination (DRC). It presents several drawbacks principally motivated by the normalization process used via a measure of conflict-based-combination between two pieces of evidence. This measure must be differentiated from conflict as an uncertainty-based-information measure of a piece of evidence in BFT. Many combination rules have been developed to solve the drawbacks that DRC presents, but each of them also has non-desirable behaviors in certain situations. In this work, we propose a set of mathematical properties that must be satisfied by every combination rule in BFT; we analyze some of the most known combination rules proposed in the literature, taking into account properties and behaviors. On the other hand, in this research, we present a new hybrid rule that verifies the required mathematical properties and does not present undesirable behaviors that other rules suffer. Finally, we show that the results of our proposal when it is used in practical applications are coherent.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004280",
    "keywords": [
      "Artificial intelligence",
      "Computer science"
    ],
    "authors": [
      {
        "surname": "Abellán",
        "given_name": "Joaquín"
      },
      {
        "surname": "Moral-García",
        "given_name": "Serafín"
      },
      {
        "surname": "Benítez",
        "given_name": "María D."
      }
    ]
  },
  {
    "title": "An improved memetic algebraic differential evolution for solving the multidimensional two-way number partitioning problem",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114938",
    "abstract": "In this article, we propose a novel and effective evolutionary algorithm for the challenging combinatorial optimization problem known as Multidimensional Two-Way Number Partitioning Problem (MDTWNPP). Since the MDTWNPP has been proven to be NP-hard, in the recent years, it has been increasingly addressed by means of meta-heuristic approaches. Nevertheless, previous proposals in literature do not make full use of critical problem information that may improve the effectiveness of the search. Here, we bridge this gap by designing an improved Memetic Algebraic Differential Evolution (iMADEB) algorithm that incorporates critical information about the problem. In particular, iMADEB evolves a population of candidate local optimal solutions by adopting three key design concepts: a novel non-redundant bit-string representation which maps population individuals one-to-one to MDTWNPP solutions, a smoother local search operator purposely designed for the MDTWNPP landscapes, and a self-adaptive algebraic differential mutation scheme built on the basis of the Lévy flight concept which automatically regulates the exploration-exploitation trade-off of the search. Computational experiments have been conducted on a widely accepted benchmark suite for the MDTWNPP with a twofold purpose: analyzing the robustness of iMADEB and compare its effectiveness with respect to the state-of-the-art approaches to date for the MDTWNPP. The experimental results provide important indications about iMADEB robustness and, most importantly, clearly show that iMADEB is the new state-of-the-art algorithm for the MDTWNPP.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003791",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Demography",
      "Differential evolution",
      "Evolutionary algorithm",
      "Gene",
      "Geodesy",
      "Geography",
      "History",
      "Local search (optimization)",
      "Mathematical optimization",
      "Mathematics",
      "Memetic algorithm",
      "Population",
      "Robustness (evolution)",
      "Sociology",
      "Suite",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Santucci",
        "given_name": "Valentino"
      },
      {
        "surname": "Baioletti",
        "given_name": "Marco"
      },
      {
        "surname": "Di Bari",
        "given_name": "Gabriele"
      }
    ]
  },
  {
    "title": "Explaining dimensionality reduction results using Shapley values",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115020",
    "abstract": "Dimensionality reduction (DR) techniques have been consistently supporting high-dimensional data analysis in various applications. Besides the patterns uncovered by these techniques, the interpretation of DR results based on each feature’s contribution to the low-dimensional representation supports new finds through exploratory analysis. Current literature approaches designed to interpret DR techniques do not explain the features’ contributions well since they focus only on the low-dimensional representation or do not consider the relationship among features. This paper presents ClusterShapley to address these problems, using Shapley values to generate explanations of dimensionality reduction techniques and interpret these algorithms using a cluster-oriented analysis. ClusterShapley explains the formation of clusters and the meaning of their relationship, which is useful for exploratory data analysis in various domains. We propose novel visualization techniques to guide the interpretation of features’ contributions on clustering formation and validate our methodology through case studies of publicly available datasets. The results demonstrate our approach’s interpretability and analysis power to generate insights about pathologies and patients in different conditions using DR results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004619",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Curse of dimensionality",
      "Data mining",
      "Data science",
      "Dimensionality reduction",
      "Exploratory analysis",
      "Exploratory data analysis",
      "Game theory",
      "Geometry",
      "Interpretability",
      "Interpretation (philosophy)",
      "Law",
      "Machine learning",
      "Mathematical economics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Programming language",
      "Reduction (mathematics)",
      "Representation (politics)",
      "Shapley value",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Marcílio-Jr",
        "given_name": "Wilson E."
      },
      {
        "surname": "Eler",
        "given_name": "Danilo M."
      }
    ]
  },
  {
    "title": "A hybrid approach for text document clustering using Jaya optimization algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115040",
    "abstract": "In this digital era, millions of Internet users are contributing vast amounts of data in the form of unstructured text documents. Organizing this material is a tedious task. The clustering of text document plays a vital role for organizing these unstructured text documents. In our paper, we make use of Hybrid Jaya Optimization algorithm (HJO) for text Document Clustering (DC), referred to as HJO-DC. We have used the Silhouette index as a metric to measure the quality of a solution. The proposed work is compared with partitioning techniques such as K-Means and K-Medoids and metaheuristic techniques such as Genetic algorithm, Cuckoo Search, Particle Swarm Optimizer, Firefly and Grey Wolf Optimizer. Remarkably, the proposed algorithm achieves the highest quality clustering in all benchmark examples.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004814",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Cuckoo search",
      "Data mining",
      "Document clustering",
      "Economics",
      "Firefly algorithm",
      "Genetic algorithm",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Metaheuristic",
      "Metric (unit)",
      "Operations management",
      "Particle swarm optimization",
      "k-medoids"
    ],
    "authors": [
      {
        "surname": "Thirumoorthy",
        "given_name": "Karpagalingam"
      },
      {
        "surname": "Muneeswaran",
        "given_name": "Karuppaiah"
      }
    ]
  },
  {
    "title": "A long short-term recurrent spatial-temporal fusion for myoelectric pattern recognition",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114977",
    "abstract": "Current state-of-the-art myoelectric interfaces employ traditional pattern recognition (PR) algorithms to decode the Electromyogram (EMG) signals into hand movements for controlling artificial limbs. Recently, deep learning (DL) models have also been exploited for EMG feature learning/extraction. Models like Convolutional Neural Networks (CNN), which capture the spatial correlations, and Long Short-Term Memory (LSTM), which capture the non-linear temporal dynamics of EMG time-series data, have been shown to outperform traditional EMG PR systems. Nevertheless, the large number of model parameters, long training times, and large amounts of data required to train these DL models remain limiting factors that may hinder their translation into clinically viable prostheses. Consequently, rather than applying DL directly, this paper leverages concepts derived from these models to build upon our proposed concept of a Fusion of Time Domain Descriptors (FTDD). The FTDD are augmented with Range Spatial Filtering (RSF) to capture the spatial correlations and combined into an LSTM-style framework. This process, denoted as Recurrent Spatial-Temporal Fusion (RSTF), can be applied in combination with any traditional feature extraction method to exploit temporal and spatial correlations, with the potential for bi-directional applications. The advantages of the proposed RSTF method include (1) the memory concept, capturing long- and short-term spatial and temporal dependencies of the EMG signals, (2) significantly improved performance outperforming other state-of-the-art models and (3) the simplicity and the fairly low computational costs for feature extraction. Results are bench-marked against several feature extraction methods, proving the power of the RSTF using data from 82 subjects from five EMG databases with varying recording characteristics. The proposed method significantly outperforms all other methods tested for EMG pattern recognition, including a deep LSTM and other CNN methods previously reported in the literature and at a fracture of the computational cost. On the most challenging dataset, improvements of as much as 15% were found.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004188",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Fusion",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Speech recognition",
      "Term (time)"
    ],
    "authors": [
      {
        "surname": "Khushaba",
        "given_name": "Rami N."
      },
      {
        "surname": "Scheme",
        "given_name": "Erik"
      },
      {
        "surname": "Al-Timemy",
        "given_name": "Ali H."
      },
      {
        "surname": "Phinyomark",
        "given_name": "Angkoon"
      },
      {
        "surname": "Taee",
        "given_name": "Ahmed Al-"
      },
      {
        "surname": "Al-Jumaily",
        "given_name": "Adel"
      }
    ]
  },
  {
    "title": "Automated imbalanced classification via meta-learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115011",
    "abstract": "Imbalanced learning is one of the most relevant problems in machine learning. However, it faces two crucial challenges. First, the amount of methods proposed to deal with such problem has grown immensely, making the validation of a large set of methods impractical. Second, it requires specialised knowledge, hindering its use by those without such level of experience. In this paper, we propose the Automated Imbalanced Classification method, ATOMIC. Such a method is the first automated machine learning approach for imbalanced classification tasks. It provides a ranking of solutions most likely to ensure an optimal approximation to a new domain, drastically reducing associated computational complexity and energy consumption. We carry this out by anticipating the loss of a large set of predictive solutions in new imbalanced learning tasks. We compare the predictive performance of ATOMIC against state-of-the-art methods using 101 imbalanced data sets. Results demonstrate that the proposed method provides a relevant approach to imbalanced learning while reducing learning and testing efforts of candidate solutions by approximately 95%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004528",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Database",
      "Domain (mathematical analysis)",
      "Economics",
      "Ensemble learning",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Meta learning (computer science)",
      "Programming language",
      "Ranking (information retrieval)",
      "Set (abstract data type)",
      "Task (project management)",
      "Workflow"
    ],
    "authors": [
      {
        "surname": "Moniz",
        "given_name": "Nuno"
      },
      {
        "surname": "Cerqueira",
        "given_name": "Vitor"
      }
    ]
  },
  {
    "title": "Cross domain-based ontology construction via Jaccard Semantic Similarity with hybrid optimization model",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115046",
    "abstract": "Semantic web technology seems to be in the infant stage as only little efforts have been taken on ontology construction with cross-domain application. This paper intends to take an effort on a new workspace, in which the ontology construction model under cross-domain application is performed. The core concern of this work is on two decision-making process namely data filtering and data annotation. Certain process is followed in this work: (i) Preprocessing (ii) Proposed Jaccard Similarity Evaluation (iii) Data filtering and Outlier Detection (iv) Semantic annotation and clustering. More particularly, data filtering is performed based on the evaluated similarity function. The outliers are identified and grouped separately. The data annotation is performed based on the semantics and thereby the clustering process takes place to form the ontology precisely. This clustering process obviously relies to the optimization crisis as the optimal centroid selection becomes the greatest issue. In order to solve this, this paper extends with the introduction of a hybrid algorithm named Circling Insisted-Rider Optimization Algorithm (CI-ROA), which hybrids the concept of Whale Optimization Algorithm (WOA) and Rider Optimization Algorithm (ROA), respectively. Finally, the performance of proposed work is compared and proved over other state-of-the-art models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004875",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Domain (mathematical analysis)",
      "Epistemology",
      "Image (mathematics)",
      "Information retrieval",
      "Jaccard index",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Ontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Semantic Web",
      "Semantic similarity",
      "Similarity (geometry)",
      "Upper ontology"
    ],
    "authors": [
      {
        "surname": "Kakad",
        "given_name": "Shital"
      },
      {
        "surname": "Dhage",
        "given_name": "Sudhir"
      }
    ]
  },
  {
    "title": "Deep monocular depth estimation leveraging a large-scale outdoor stereo dataset",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114877",
    "abstract": "Current self-supervised methods for monocular depth estimation are largely based on deeply nested convolutional networks that leverage stereo image pairs or monocular sequences during the training phase. However, they often exhibit inaccurate results around occluded regions and depth boundaries. In this paper, we present a simple yet effective approach for monocular depth estimation using stereo image pairs. The study aims to propose a student–teacher strategy in which a shallow student network is trained with the auxiliary information obtained from a deeper and more accurate teacher network. Specifically, we first train the stereo teacher network by fully utilizing the binocular perception of 3-D geometry, and then use the depth predictions of the teacher network to train the student network for monocular depth inference. This enables us to exploit all available depth data from massive unlabeled stereo pairs. We propose a strategy that involves the use of a data ensemble to merge the multiple depth predictions of the teacher network to improve the training samples by collecting non-trivial knowledge beyond a single prediction. To refine the inaccurate depth estimation that is used when training the student network, we further propose stereo confidence guided regression loss that handles the unreliable pseudo depth values in occlusion, texture-less region, and repetitive pattern. To complement the existing dataset comprising outdoor driving scenes, we built a novel large-scale dataset consisting of one million outdoor stereo images taken using hand-held stereo cameras. Finally, we demonstrate that the monocular depth estimation network provides feature representations that are suitable for high-level vision tasks. The experimental results for various outdoor scenarios demonstrate the effectiveness and flexibility of our approach, which outperforms state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003183",
    "keywords": [
      "Artificial intelligence",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Economics",
      "Estimation",
      "Geography",
      "Geology",
      "Image (mathematics)",
      "Management",
      "Monocular",
      "Remote sensing",
      "Scale (ratio)",
      "Stereo image",
      "Stereopsis"
    ],
    "authors": [
      {
        "surname": "Cho",
        "given_name": "Jaehoon"
      },
      {
        "surname": "Min",
        "given_name": "Dongbo"
      },
      {
        "surname": "Kim",
        "given_name": "Youngjung"
      },
      {
        "surname": "Sohn",
        "given_name": "Kwanghoon"
      }
    ]
  },
  {
    "title": "A hybrid model integrating deep learning with investor sentiment analysis for stock price prediction",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115019",
    "abstract": "Whether stock prices are predictable has been the center of debate in academia. In this paper, we propose a hybrid model that combines a deep learning approach with a sentiment analysis model for stock price prediction. We employ a Convolutional Neural Network model for classifying the investors’ hidden sentiments, which are extracted from a major stock forum. We then propose a hybrid research model by applying the Long Short-Term Memory (LSTM) Neural Network approach for analyzing the technical indicators from the stock market and the sentiment analysis results from the first step. Furthermore, this work has conducted real-life experiments from six key industries of three time intervals on the Shanghai Stock Exchange (SSE) to validate the effectiveness and applicability of the proposed model. The experiment results indicate that the proposed model has achieved better performance in classifying investor sentiments than the baseline classifiers, and this hybrid approach performs better in predicting stock prices compared to the single model and the models without sentiment analysis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004607",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Deep learning",
      "Engineering",
      "Machine learning",
      "Mechanical engineering",
      "Paleontology",
      "Sentiment analysis",
      "Series (stratigraphy)",
      "Stock (firearms)",
      "Stock price"
    ],
    "authors": [
      {
        "surname": "Jing",
        "given_name": "Nan"
      },
      {
        "surname": "Wu",
        "given_name": "Zhao"
      },
      {
        "surname": "Wang",
        "given_name": "Hefei"
      }
    ]
  },
  {
    "title": "SUFMACS: A machine learning-based robust image segmentation framework for COVID-19 radiological image interpretation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115069",
    "abstract": "The absence of dedicated vaccines or drugs makes the COVID-19 a global pandemic, and early diagnosis can be an effective prevention mechanism. RT-PCR test is considered as one of the gold standards worldwide to confirm the presence of COVID-19 infection reliably. Radiological images can also be used for the same purpose to some extent. Easy and no contact acquisition of the radiological images makes it a suitable alternative and this work can help to locate and interpret some prominent features for the screening purpose. One major challenge of this domain is the absence of appropriately annotated ground truth data. Motivated from this, a novel unsupervised machine learning-based method called SUFMACS (SUperpixel based Fuzzy Memetic Advanced Cuckoo Search) is proposed to efficiently interpret and segment the COVID-19 radiological images. This approach adapts the superpixel approach to reduce a large amount of spatial information. The original cuckoo search approach is modified and the Luus-Jaakola heuristic method is incorporated with McCulloch’s approach. This modified cuckoo search approach is used to optimize the fuzzy modified objective function. This objective function exploits the advantages of the superpixel. Both CT scan and X-ray images are investigated in detail. Both qualitative and quantitative outcomes are quite promising and prove the efficiency and the real-life applicability of the proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005108",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Coronavirus disease 2019 (COVID-19)",
      "Cuckoo search",
      "Data mining",
      "Disease",
      "Fuzzy logic",
      "Image segmentation",
      "Infectious disease (medical specialty)",
      "Machine learning",
      "Medicine",
      "Particle swarm optimization",
      "Pathology",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Chakraborty",
        "given_name": "Shouvik"
      },
      {
        "surname": "Mali",
        "given_name": "Kalyani"
      }
    ]
  },
  {
    "title": "One-shot learning for acoustic diagnosis of industrial machines",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114984",
    "abstract": "Automatic acoustic monitoring of machine health comprises a relevant field as, unfortunately, such equipment often suffers from faults, malfunctions, aging effects, etc. However, it is still an unexplored domain of research where the majority of existing works relies on traditional machine learning based approaches. After providing a critical survey of the available methods, this work highlights the most relevant limitations and designs a solution specifically addressing them. We introduce the one-shot learning paradigm into the specific domain and suitably extent it to (a) classify machine states, (b) detect novel ones, and (c) incorporate them in the class dictionary online. The backbone of the present system is a Siamese Neural Network (SNN) composed of convolutional layers. Conveniently, every processing stage depends on a standardized feature set free of domain knowledge, i.e. spectrograms. Interestingly, we enhance SNN’s classification ability by an appropriately designed data selection scheme. The proposed solution is applied on a publicly available dataset of vibration signals representing four states of a drill bit, i.e. healthy state, chisel wear, flank wear, and outer corner wear. After extensive experiments thoroughly examining every aspect of the proposed solution, it is shown to achieve state of the art results while using limited amount of training data. Importantly, at the same time it is able to operate under evolving environments. Last but not least, we show that the obtained predictions are interpretable, a property which is rapidly becoming a requirement in modern machine learning based technologies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004255",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Domain (mathematical analysis)",
      "Epistemology",
      "Feature (linguistics)",
      "Feature selection",
      "Field (mathematics)",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Philosophy",
      "Programming language",
      "Property (philosophy)",
      "Pure mathematics",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Ntalampiras",
        "given_name": "Stavros"
      }
    ]
  },
  {
    "title": "Fuzzy case-based reasoning approach for finding COVID-19 patients priority in hospitals at source shortage period",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114997",
    "abstract": "In this research article, we introduced an algorithm to evaluate COVID-19 patients admission in hospitals at source shortage period. Many researchers have expressed their conclusions from different perspectives on various factors such as spatial changes, climate risks, preparedness, blood type, age and comorbidities that may be contributing to COVID-19 mortality rate. However, as the number of people coming to the hospital for COVID-19 treatment increases, the mortality rate is likely to increase due to the lack of medical facilities. In order to provide medical assistance in this situation, we need to consider not only the extent of the disease impact, but also other important factors. No method has yet been proposed to calculate the priority of patients taking into account all the factors. We have provided a solution to this in this research article. Based on eight key factors, we provide a way to determine priorities. In order to achieve the effectiveness and practicability of the proposed method, we studied individuals with different results on all factors. The sigmoid function helps to easily construct factors at different levels. In addition, the cobweb solution model allows us to see the potential of our proposed algorithm very clearly. Using the method we introduced, it is easier to sort high-risk individuals to low-risk individuals. This will make it easier to deal with problems that arise when the number of patients in hospitals continues to increase. It can reduce the mortality of COVID-19 patients. Medical professionals can be very helpful in making the best decisions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004383",
    "keywords": [
      "Biology",
      "Business",
      "Computer science",
      "Construct (python library)",
      "Coronavirus disease 2019 (COVID-19)",
      "Disease",
      "Economic shortage",
      "Evolutionary biology",
      "Finance",
      "Function (biology)",
      "Government (linguistics)",
      "Infectious disease (medical specialty)",
      "Information retrieval",
      "Internal medicine",
      "Law",
      "Linguistics",
      "Medicine",
      "Mortality rate",
      "Order (exchange)",
      "Pathology",
      "Philosophy",
      "Political science",
      "Preparedness",
      "Programming language",
      "Risk analysis (engineering)",
      "sort"
    ],
    "authors": [
      {
        "surname": "Geetha",
        "given_name": "Selvaraj"
      },
      {
        "surname": "Narayanamoorthy",
        "given_name": "Samayan"
      },
      {
        "surname": "Manirathinam",
        "given_name": "Thangaraj"
      },
      {
        "surname": "Kang",
        "given_name": "Daekook"
      }
    ]
  },
  {
    "title": "Using back-and-forth translation to create artificial augmented textual data for sentiment analysis models",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115033",
    "abstract": "Sentiment analysis classification models trained using neural networks require large amounts of data, but collecting these datasets requires significant time and resources. Although artificial data has been used successfully in computer vision, there are few effective and generalizable methods for creating artificial augmented text data. In this paper, a text based data augmentation method is proposed called back-and-forth translation that can be used to artificially increase the size of any natural language dataset. By creating augmented text data and adding it to the original dataset, it is demonstrated by empirical experiments that back-and-forth translation data augmentation can reduce the error rate in binary sentiment classification models by up to 3.4%. These results are shown to be statistically significant.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004747",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Gene",
      "Machine learning",
      "Machine translation",
      "Messenger RNA",
      "Natural language processing",
      "Sentiment analysis",
      "Translation (biology)",
      "Word error rate"
    ],
    "authors": [
      {
        "surname": "Body",
        "given_name": "Thomas"
      },
      {
        "surname": "Tao",
        "given_name": "Xiaohui"
      },
      {
        "surname": "Li",
        "given_name": "Yuefeng"
      },
      {
        "surname": "Li",
        "given_name": "Lin"
      },
      {
        "surname": "Zhong",
        "given_name": "Ning"
      }
    ]
  },
  {
    "title": "Nine-Axis IMU-based Extended inertial odometry neural network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115075",
    "abstract": "With the development of mobile devices, such as smartphones, research on fast and accurate trajectory tracking is being actively conducted. This research requires a continuous integration of the acceleration and angular velocity data obtained from the low-cost microelectromechanical system-based inertial measurement unit (IMU) installed in a device to track the user’s trajectory. During this process, drift occurs over time due to the bias and intrinsic error of the IMU sensor. Hence, the 6-Axis IMU-based inertial odometry neural network (IONet) using deep learning, which is designed as a framework for velocity estimation, is used to reduce drift by dividing the acceleration data into independent windows. However, drift still occurs in estimating a pose containing both a position and an orientation because the integration of pose changes is also required. In this study, we proposed the Extended IONet that combines a 9-Axis IONet and Pose-TuningNet to improve the accuracy of trajectory tracking by compensating for the drift problem of the 6-Axis IONet. The proposed 9-Axis IONet uses the gravitational acceleration and geomagnetic data of the IMU in addition to the input structure of the existing 6-Axis IONet; thus, the estimation accuracy of pose changes improves by reducing the data dependence on the original input of the 6-Axis IONet. The proposed Pose-TuningNet is an auxiliary network that is capable of estimating pose changes more precisely using the higher-dimensional inclination-angle information obtained from the IMU to focus on the noise model of the IMU. Experiments were conducted using the Oxford Inertial Odometry Dataset, which is public dataset for deep learning based inertial navigation research to verify the performance of the proposed neural network. Compared with the existing 6-Axis IONet, the Extended IONet achieved superior performance in five out of seven cases, and its overall 39.8% RMSE improvement demonstrated its excellent performance. Additionally, the results showed that Pose-TuningNet improved the position estimation performance by correcting the drift problem in the 9-Axis IONet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421005169",
    "keywords": [
      "Acceleration",
      "Artificial intelligence",
      "Astronomy",
      "Classical mechanics",
      "Computer science",
      "Computer vision",
      "Control (management)",
      "Control theory (sociology)",
      "Economics",
      "Finance",
      "Geometry",
      "Gravitational acceleration",
      "Gravitational field",
      "Inertial measurement unit",
      "Mathematics",
      "Mobile robot",
      "Odometry",
      "Orientation (vector space)",
      "Physics",
      "Pose",
      "Position (finance)",
      "Robot",
      "Trajectory"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Won-Yeol"
      },
      {
        "surname": "Seo",
        "given_name": "Hong-Il"
      },
      {
        "surname": "Seo",
        "given_name": "Dong-Hoan"
      }
    ]
  },
  {
    "title": "Exploiting inter-frame regional correlation for efficient action recognition",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114829",
    "abstract": "Temporal feature extraction is an important issue in video-based action recognition. Optical flow is a popular method to extract temporal feature, which produces excellent performance thanks to its capacity of capturing pixel-level correlation information between consecutive frames. However, such a pixel-level correlation is extracted at the cost of high computational complexity and large storage resource. In this paper, we propose a novel temporal feature extraction method, Attentive Correlated Temporal Feature (ACTF), by exploring inter-frame correlation within a certain region. The proposed ACTF exploits both bilinear and linear correlations between successive frames on the regional level. Our method has the advantage of achieving performance comparable to or better than optical flow-based methods while avoiding the introduction of optical flow. Experimental results demonstrate our proposed method achieves the competitive performances of 96.3 % on UCF101 and 76.3 % on HMDB51 benchmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002700",
    "keywords": [
      "Action (physics)",
      "Action recognition",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Correlation",
      "Frame (networking)",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Yuecong"
      },
      {
        "surname": "Yang",
        "given_name": "Jianfei"
      },
      {
        "surname": "Mao",
        "given_name": "Kezhi"
      },
      {
        "surname": "Yin",
        "given_name": "Jianxiong"
      },
      {
        "surname": "See",
        "given_name": "Simon"
      }
    ]
  },
  {
    "title": "Cost-sensitive semi-supervised deep learning to assess driving risk by application of naturalistic vehicle trajectories",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115041",
    "abstract": "Most traffic accidents are caused by driver-related factors such as poor perception, aggressive decision-making, or improper maneuvering. Therefore, it is critical to evaluate and predict driving risks to provide drivers with timely feedback. However, risk assessment involves challenges related to a lack of labeled driving data and the presence of data imbalance in the description of different driving risk levels. To address these challenges, a cost-sensitive semi-supervised deep learning method is developed to obtain driving risk scores based on naturalistic vehicle trajectories. A convolutional neural network and a long short-term memory encoder/decoder network are embedded into a semi-supervised framework that uses only a small labeled dataset to label the remaining unlabeled data and produce a trained network model. As fixed weights cannot adapt to changes in the degree of class imbalance that occur over progressive semi-supervised learning iterations, an adaptive over-balanced cross-entropy loss function is developed to adaptively maintain an over-balanced state for the high-risk class to achieve cost-sensitive learning. The experimental results indicate that the accuracy of the proposed method in determining the current and future 2 s risk scores is 96.63% and 92.06%, respectively, thereby constituting the best comprehensive performance among existing machine learning methods. Moreover, the method is verified using a spatio-temporal diagram of driving risk-trajectory and a current–future risk score diagram. The findings demonstrate that the proposed method can be used to assess driving risks in a reliable and robust manner.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004826",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Deep learning",
      "Epistemology",
      "Machine learning",
      "Naturalism",
      "Philosophy",
      "Supervised learning"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Hongyu"
      },
      {
        "surname": "Wang",
        "given_name": "Qi"
      },
      {
        "surname": "Cheng",
        "given_name": "Ming"
      },
      {
        "surname": "Gao",
        "given_name": "Zhenhai"
      }
    ]
  },
  {
    "title": "Real-time dispatching of air taxis in metropolitan cities using a hybrid simulation goal programming algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115056",
    "abstract": "Expected to begin its operations in the coming years, air taxi aims to provide everyday transportation services to customers in metropolitan cities. These vehicles offer urban air mobility (UAM) services using the electric vertical takeoff and landing (eVTOL) technology. This research is the first to present a hybrid simulation goal programming (HSGP) approach to dispatch vehicles in a centralized air taxi network. After each customer drop-off, the model makes real-time decisions on (i) whether the air taxi must become idle or pick up customers, and (ii) the station to which the air taxi should be dispatched (if the air taxi is operational). The feasibility of the HSGP approach is tested using potential air taxi demands in New York City (NYC) provided by a prior study. The results of the experimentation suggest that the minimum number of air taxis required for efficient operation in NYC is 84, functioning with an average utilization rate of 66%. In addition, the impacts of commuter’s “willingness to fly” rate, percentage of demand fulfillment, on-road travel limit, maximum customer wait time, and arrival distribution on the optimal number of air taxis, utilization rate, number of customers served, and cost incurred per customer are examined. Analyses show that the “willingness to fly” rate appears to have a linear influence on the number of air taxis and the efficiency, while on-road travel distance has an exponential impact on the performance measures. The HSGP algorithm developed in this paper can be used by any company that is interested in venturing into the air taxi market.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004978",
    "keywords": [
      "Computer science",
      "Engineering",
      "Medicine",
      "Metropolitan area",
      "Operations research",
      "Pathology",
      "Taxis",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Rajendran",
        "given_name": "Suchithra"
      }
    ]
  },
  {
    "title": "On the feasibility of using physics-informed machine learning for underground reservoir pressure management",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115006",
    "abstract": "We evaluate the feasibility of using physics-informed machine learning (PIML) for underground energy-related pressure management. To this end, we develop a PIML framework to manage underground reservoir pressures by training neural networks to determine fluid extraction rates for dedicated extraction wells during fluid injection operations given a range of reservoir conditions (e.g., transmissivity and storativity). We implement an automatically-differentiable analytical physics model of fluid flow in porous media within the PIML framework as a proxy for more complicated models. This allows us to execute a sufficient number of training scenarios to fully evaluate the feasibility of using PIML to support pressure management activities. We quantify the number of physics-model parameters required for automatic differentiation to become more efficient than finite-difference gradient calculations. We use a simple scenario with a single injector, extractor, and critical location for our feasibility analysis. We evaluate the effect of the size of the training dataset (i.e., the number of reservoir condition samples) on the accuracy and efficiency of the PIML framework. For an equivalent number of model evaluations, the larger training dataset took less time to train and produced a neural network that was able to more accurately manage reservoir pressures. We also evaluate the effect of the training dataset batch size (i.e., number of reservoir condition samples used to update the neural network coefficients during training; i.e., how the training dataset is partitioned). While training ran faster with larger batch sizes, they produced neural networks that managed pressures less accurately. We demonstrate the approach on a more complex scenario involving 10 injectors, 10 extractors, and 4 critical locations (a relatively high well density of 20 wells/km2). We provide the number of forward and adjoint model evaluations required in each case as an indication of the feasibility of using PIML for pressure management when more complicated physics models with longer execution times are used.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004474",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Engineering",
      "Extractor",
      "Fluid dynamics",
      "Machine learning",
      "Mechanics",
      "Meteorology",
      "Physics",
      "Process engineering",
      "Training (meteorology)"
    ],
    "authors": [
      {
        "surname": "Harp",
        "given_name": "Dylan Robert"
      },
      {
        "surname": "O’Malley",
        "given_name": "Dan"
      },
      {
        "surname": "Yan",
        "given_name": "Bicheng"
      },
      {
        "surname": "Pawar",
        "given_name": "Rajesh"
      }
    ]
  },
  {
    "title": "Robust automated Parkinson disease detection based on voice signals with transfer learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115013",
    "abstract": "Parkinson's disease (PD) is a progressive-neurodegenerative disorder that affects more than 6 million people around the world. However, conventional techniques for PD detection are often hand-crafted, in which special expertise is needed. In this study, considering the importance of rapid diagnosis of the disease, it was aimed to develop deep convolutional neural networks (CNN) for automated PD identification based on biomarkers-derived voice signals. The developed CNN methods consisted of two main stages, including data pre-processing and fine-tunning-based transfer learning steps. To train and evaluate the performance of the developed model, datasets were collected from the mPower Voice database. SqueezeNet1_1, ResNet101, and DenseNet161 architectures were retrained and evaluated to determine which architecture can classify frequency-time information most accurately. The performance results revealed that the proposed model could successfully identify the PD with an accuracy of 89.75%, sensitivity of 91.50%, and precision of 88.40% for DenseNet-161 architecture identified as the most suitable fine-tuning architecture. The results revealed that the proposed model based on transfer learning with a fine-tuning approach provides an acceptable detection of PD with an accuracy of 89.75%. The outcomes of the study confirmed that by integrating the developed model into smart electronic devices, it will be able to develop alternative pre-diagnosis methods and will assist the physicians for PD detection during the in-clinic assessment. The success of the proposed model would imply an enhancement in the life quality of patients and a cost reduction for the national health system.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004541",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Electronic engineering",
      "Engineering",
      "Machine learning",
      "Parallel computing",
      "Pattern recognition (psychology)",
      "Sensitivity (control systems)",
      "Transfer (computing)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Karaman",
        "given_name": "Onur"
      },
      {
        "surname": "Çakın",
        "given_name": "Hakan"
      },
      {
        "surname": "Alhudhaif",
        "given_name": "Adi"
      },
      {
        "surname": "Polat",
        "given_name": "Kemal"
      }
    ]
  },
  {
    "title": "Robust automated Parkinson disease detection based on voice signals with transfer learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115013",
    "abstract": "Parkinson's disease (PD) is a progressive-neurodegenerative disorder that affects more than 6 million people around the world. However, conventional techniques for PD detection are often hand-crafted, in which special expertise is needed. In this study, considering the importance of rapid diagnosis of the disease, it was aimed to develop deep convolutional neural networks (CNN) for automated PD identification based on biomarkers-derived voice signals. The developed CNN methods consisted of two main stages, including data pre-processing and fine-tunning-based transfer learning steps. To train and evaluate the performance of the developed model, datasets were collected from the mPower Voice database. SqueezeNet1_1, ResNet101, and DenseNet161 architectures were retrained and evaluated to determine which architecture can classify frequency-time information most accurately. The performance results revealed that the proposed model could successfully identify the PD with an accuracy of 89.75%, sensitivity of 91.50%, and precision of 88.40% for DenseNet-161 architecture identified as the most suitable fine-tuning architecture. The results revealed that the proposed model based on transfer learning with a fine-tuning approach provides an acceptable detection of PD with an accuracy of 89.75%. The outcomes of the study confirmed that by integrating the developed model into smart electronic devices, it will be able to develop alternative pre-diagnosis methods and will assist the physicians for PD detection during the in-clinic assessment. The success of the proposed model would imply an enhancement in the life quality of patients and a cost reduction for the national health system.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004541",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Electronic engineering",
      "Engineering",
      "Machine learning",
      "Parallel computing",
      "Pattern recognition (psychology)",
      "Sensitivity (control systems)",
      "Transfer (computing)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Karaman",
        "given_name": "Onur"
      },
      {
        "surname": "Çakın",
        "given_name": "Hakan"
      },
      {
        "surname": "Alhudhaif",
        "given_name": "Adi"
      },
      {
        "surname": "Polat",
        "given_name": "Kemal"
      }
    ]
  },
  {
    "title": "Discovery of associated consumer demands: Construction of a co-demanded product network with community detection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115038",
    "abstract": "Some consumers have various product demands that are associated with each other. Although there are methods for discovering the associations among co-purchased products, they have limitations, including redundancy of the extracted association rules, the potential to miss novel and interesting associations among co-demanded products hidden in shopping behaviors, and neglect of several important influential factors. In order to provide effective product recommendations, it is necessary and beneficial to discover the associations among the products co-demanded by the same consumers in a short period based on the consumers’ shopping behaviors. Therefore, this paper proposes a novel model for discovering associated consumer demands based on a co-demanded product network. First, the model identifies each consumer’s product demands and calculates their intensity based on various online shopping behaviors. Second, the model constructs a co-demanded product network based on the products demanded by the same consumers within a short period. The model also considers several important factors, previous neglected in the literature, that can improve the detection of associations among co-demanded products, including the time interval between two product demands from the same consumers, the popularity of each demanded product, and the number of product demands from each consumer. Third, the model uses an algorithm for the detection of overlapping communities to identify the tightly connected co-demanded products within the network as communities of associated consumer demands, and ranks them based on their information density. We use a real-world dataset collected from a well-known e-commerce platform to validate the proposed model. The results show that the proposed model can detect more modular, diverse, practical, and reliable communities of associated products than the existing network analysis–based market basket analysis methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004796",
    "keywords": [
      "Business",
      "Computer science",
      "Geometry",
      "Mathematics",
      "Product (mathematics)"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Jiacong"
      },
      {
        "surname": "Wang",
        "given_name": "Yu"
      },
      {
        "surname": "Shafiee",
        "given_name": "Sara"
      },
      {
        "surname": "Zhang",
        "given_name": "Dongsong"
      }
    ]
  },
  {
    "title": "Rice diseases detection and classification using attention based neural network and bayesian optimization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114770",
    "abstract": "In this research, an attention-based depthwise separable neural network with Bayesian optimization (ADSNN-BO) is proposed to detect and classify rice disease from rice leaf images. Rice diseases frequently result in 20–40% corp production loss in yield and is highly related to the global economy. Rapid disease identification is critical to plan treatment promptly and reduce the corp losses. Rice disease diagnosis is still mainly performed manually. To achieve AI assisted rapid and accurate disease detection, we proposed the ADSNN-BO model based on MobileNet structure and augmented attention mechanism. Moreover, Bayesian optimization method is applied to tune hyper-parameters of the model. Cross-validated classification experiments are conducted based on a public rice disease dataset with four categories in total. The experimental results demonstrate that our mobile compatible ADSNN-BO model achieves a test accuracy of 94.65%, which outperforms all of the state-of-the-art models tested. To check the interpretability of our proposed model, feature analysis including activation map and filters visualization approach are also conducted. Results show that our proposed attention-based mechanism can more effectively guide the ADSNN-BO model to learn informative features. The outcome of this research will promote the implementation of artificial intelligence for fast plant disease diagnosis and control in the agricultural field.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002116",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Bayesian optimization",
      "Bayesian probability",
      "Biology",
      "Biotechnology",
      "Botany",
      "Computer science",
      "Data mining",
      "Field (mathematics)",
      "Identification (biology)",
      "Interpretability",
      "Machine learning",
      "Mathematics",
      "Naive Bayes classifier",
      "Pattern recognition (psychology)",
      "Plant disease",
      "Pure mathematics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yibin"
      },
      {
        "surname": "Wang",
        "given_name": "Haifeng"
      },
      {
        "surname": "Peng",
        "given_name": "Zhaohua"
      }
    ]
  },
  {
    "title": "Swarm intelligence based robotic search in unknown maze-like environments",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114907",
    "abstract": "This paper proposes a novel decentralize and asynchronous robotic search algorithm based on particle swarm optimization (PSO), which has focused on solving mazes and finding targets in unknown environments with minimal inter-swarm communication and without any synchronization or communication center. In the proposed method, robots are advanced particles of the PSO algorithm, enriched with a toolkit, including an angle of rotation to change the course when confronted with obstacles to avoid them (AoR tool), and a memory to remember and reuse their best personal experiences to turn back from dead-ends (Mem tool). This toolkit enables the swarm to avoid obstacles and solve mazes while moving toward the target. The performance of the proposed algorithm is tested in a specially designed framework. As a validation, the proposed algorithm is compared with some recently published methods, including Adaptive Robotic PSO (A-RPSO), Robotic Bat Algorithm (RBA), and Adaptive Robotic Bat Algorithm (ARBA), in simple search environments that they can solve. The results of this comparison show that the introduced search method has the highest success rate (100%) in environments of different sizes and reflects the nature of swarm intelligence better. The proposed method is also tested in various maze-like search environments. The results depict the algorithm’s high efficiency to solve mazes in varying complexity levels and locate the target in a reliable time. It is also shown that the performance of the proposed algorithm does not decrease and remains constant as the complexity of search environments increases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003481",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Machine learning",
      "Particle swarm optimization",
      "Swarm behaviour",
      "Swarm intelligence",
      "Swarm robotics"
    ],
    "authors": [
      {
        "surname": "Youssefi",
        "given_name": "Khalil Al-Rahman"
      },
      {
        "surname": "Rouhani",
        "given_name": "Modjtaba"
      }
    ]
  },
  {
    "title": "Search and rescue operation using UAVs: A case study",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114937",
    "abstract": "Many people go missing in the wild every year. In this paper, the Search and Rescue (SAR) mission is conducted using a novel system comprising an Unmanned Aerial Vehicle (UAV) coupled with real-time machine-learning-based object detection system embedded on a smartphone. Human detection from UAV in the wilderness is a challenging task, because of many constraints involved such as lack of computing and communication infrastructures. We proposed a novel combination of a robust architecture deployed on a smartphone and a novel Convolutional Neural Network (CNN) model to fulfil the goals of the project. Our approach achieved 94.73% of accuracy and 6.8 FPS on a smartphone. Our approach is highly portable, cost-effective, fast with high accuracy. This novel system is expected to contribute significantly to maximise chances of saving lives in the wild. This developed system has been recently launched by Police Scotland to facilitate the SAR teams to locate missing persons in Scotland wilderness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100378X",
    "keywords": [
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Ecology",
      "Embedded system",
      "Engineering",
      "Machine learning",
      "Object detection",
      "Pattern recognition (psychology)",
      "Real-time computing",
      "Robot",
      "Search and rescue",
      "Systems engineering",
      "Task (project management)",
      "Visual arts",
      "Wilderness"
    ],
    "authors": [
      {
        "surname": "Martinez-Alpiste",
        "given_name": "Ignacio"
      },
      {
        "surname": "Golcarenarenji",
        "given_name": "Gelayol"
      },
      {
        "surname": "Wang",
        "given_name": "Qi"
      },
      {
        "surname": "Alcaraz-Calero",
        "given_name": "Jose Maria"
      }
    ]
  },
  {
    "title": "Value-at-risk backtesting: Beyond the empirical failure rate",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114893",
    "abstract": "The quality of Value at Risk (VaR) forecasts is typically determined by the empirical assessment of the frequency of VaR misspecifications. Additionally, the risk of clustered VaR misspecification over time, especially in volatile market times, is usually assessed within a joint testing framework. In this paper, we exclusively focus on the identification of clustered VaR misspecficiations and discuss competing backtesting procedures with respect to their ability to detect inadequate VaR models that are characterized by risk clustering. We present a simulation analysis which comprises different VaR scenarios and we find that the quality of competing backtesting procedures depends on the underlying sample size. Moreover, if sample size is small, it is the parsimonious F-test which describes a sensible choice for applied VaR assessment.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003341",
    "keywords": [
      "Biology",
      "Botany",
      "Chemistry",
      "Chromatography",
      "Cluster analysis",
      "Computer science",
      "Econometrics",
      "Economics",
      "Identification (biology)",
      "Machine learning",
      "Management",
      "Market risk",
      "Mathematics",
      "Risk management",
      "Sample (material)",
      "Sample size determination",
      "Statistics",
      "Value (mathematics)",
      "Value at risk",
      "Vector autoregression"
    ],
    "authors": [
      {
        "surname": "Berger",
        "given_name": "Theo"
      },
      {
        "surname": "Moys",
        "given_name": "Gunnar"
      }
    ]
  },
  {
    "title": "Applications and Research avenues for drone-based models in logistics: A classification and review",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114854",
    "abstract": "The operational design and planning of drone-based logistics models is a rapidly growing area of scientific research. In this paper, we present a structured, comprehensive, and scalable framework for classifying drone-based delivery systems and their associated routing problems along with a comprehensive review and synthesis of the extant academic literature in this domain. While our proposed classification defines the boundaries and facilitates the comparison between a wide variety of possible drone-based logistics systems, our comprehensive literature review helps to identify and prioritize research gaps that need to be addressed by future work. Our review shows that the extant research reasonably considers some relevant real-world operational constraints. Although the multi-visit multi-drone Pure-play Drone-based (PD) delivery models are popular, the majority of the Synchronized Multi-modal (SM) delivery models focus on formulating and evaluating single-truck, single-drone models. Moreover, the Resupply Multi-modal (RM) models have not received the due attention for research compared to other drone-based delivery models. Our comprehensive review of use cases of drones for delivery indicates that most of the reviewed models are designed for applications in e-commerce and healthcare/emergency services. Other applications, such as food and mail deliveries are still underrepresented in the academic discussion.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002955",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Business",
      "Computer science",
      "Data science",
      "Database",
      "Drone",
      "Engineering",
      "Evolutionary biology",
      "Extant taxon",
      "Genetics",
      "Management science",
      "Operations research",
      "Process management",
      "Scalability",
      "Systems engineering",
      "Variety (cybernetics)"
    ],
    "authors": [
      {
        "surname": "Moshref-Javadi",
        "given_name": "Mohammad"
      },
      {
        "surname": "Winkenbach",
        "given_name": "Matthias"
      }
    ]
  },
  {
    "title": "Identifying meaningful clusters in malware data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114971",
    "abstract": "Finding meaningful clusters in drive-by-download malware data is a particularly difficult task. Malware data tends to contain overlapping clusters with wide variations of cardinality. This happens because there can be considerable similarity between malware samples (some are even said to belong to the same family), and these tend to appear in bursts. Clustering algorithms are usually applied to normalised data sets. However, the process of normalisation aims at setting features with different range values to have a similar contribution to the clustering. It does not favour more meaningful features over those that are less meaningful, an effect one should perhaps expect of the data pre-processing stage. In this paper we introduce a method to deal precisely with the problem above. This is an iterative data pre-processing method capable of aiding to increase the separation between clusters. It does so by calculating the within-cluster degree of relevance of each feature, and then it uses these as a data rescaling factor. By repeating this until convergence our malware data was separated in clear clusters, leading to a higher average Silhouette width.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004127",
    "keywords": [
      "Artificial intelligence",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Composite material",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Law",
      "Linguistics",
      "Malware",
      "Materials science",
      "Operating system",
      "Philosophy",
      "Political science",
      "Programming language",
      "Range (aeronautics)",
      "Relevance (law)",
      "Silhouette",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Cordeiro de Amorim",
        "given_name": "Renato"
      },
      {
        "surname": "Lopez Ruiz",
        "given_name": "Carlos David"
      }
    ]
  },
  {
    "title": "Text data augmentations: Permutation, antonyms and negation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114769",
    "abstract": "Text has traditionally been used to train automated classifiers for a multitude of purposes, such as: classification, topic modelling and sentiment analysis. State-of-the-art LSTM classifier require a large number of training examples to avoid biases and successfully generalise. Labelled data greatly improves classification results, but not all modern datasets include large numbers of labelled examples. Labelling is a complex task that can be expensive, time-consuming, and potentially introduces biases. Data augmentation methods create synthetic data based on existing labelled examples, with the goal of improving classification results. These methods have been successfully used in image classification tasks and recent research has extended them to text classification. We propose a method that uses sentence permutations to augment an initial dataset, while retaining key statistical properties of the dataset. We evaluate our method with eight different datasets and a baseline Deep Learning process. This permutation method significantly improves classification accuracy by an average of 4.1%. We also propose two more text augmentations that reverse the classification of each augmented example, antonym and negation. We test these two augmentations in three eligible datasets, and the results suggest an -averaged, across all datasets-improvement in classification accuracy of 0.35% for antonym and 0.4% for negation, when compared to our proposed permutation augmentation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002104",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer security",
      "Key (lock)",
      "Machine learning",
      "Natural language processing",
      "Negation",
      "Pattern recognition (psychology)",
      "Permutation (music)",
      "Physics",
      "Programming language",
      "Sentence"
    ],
    "authors": [
      {
        "surname": "Haralabopoulos",
        "given_name": "Giannis"
      },
      {
        "surname": "Torres",
        "given_name": "Mercedes Torres"
      },
      {
        "surname": "Anagnostopoulos",
        "given_name": "Ioannis"
      },
      {
        "surname": "McAuley",
        "given_name": "Derek"
      }
    ]
  },
  {
    "title": "A unified framework for effective team formation in social networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114886",
    "abstract": "Collaboration networks are social networks in which nodes represent experts, and edges represent the interactions between them. Team Formation Problem (TFP) in Social Networks (SN) is to construct a group of individuals to work on complex tasks. Teams should satisfy the skill set required by the tasks and can collaborate effectively under multiple constraints. Although many algorithms have been proposed to confront the TFP, most of them optimize different criteria and various parameters (e.g. communication cost or expertise level). There is no unified framework to incorporate the most significant parameters towards formulating effective teams of experts. We propose a unified framework for the TFP in SN based on a multi-objective cultural algorithm that involves the integration of essential cost functions such as communication cost, expertise level, collective trust score, and geological proximity. Since these are conflicting objectives, we return a set of Pareto front of teams that are not dominated by other feasible teams with regards to any of the objectives. Moreover, we examine the temporal nature of both communication costs and expertise levels in our model and introduce a new method to formulate them. We introduce a profile similarity formula to express the trust score. We then discuss the importance of emotional index in TFP. Our model is tested on a benchmark table, which is generated with various criteria of social networks. Our model is then compared with NSGA II, Graph-Based and Exhaustive search.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003274",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Geodesy",
      "Geography",
      "Graph",
      "Image (mathematics)",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Pareto principle",
      "Programming language",
      "Set (abstract data type)",
      "Similarity (geometry)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Selvarajah",
        "given_name": "Kalyani"
      },
      {
        "surname": "Zadeh",
        "given_name": "Pooya Moradian"
      },
      {
        "surname": "Kobti",
        "given_name": "Ziad"
      },
      {
        "surname": "Palanichamy",
        "given_name": "Yazwand"
      },
      {
        "surname": "Kargar",
        "given_name": "Mehdi"
      }
    ]
  },
  {
    "title": "Towards zero shot learning of geometry of motion streams and its application to anomaly recognition",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114916",
    "abstract": "Visual anomaly recognition (VAR) is the core part of many intelligent systems. However, vagueness in definitions and lack of a priori knowledge about the distribution of anomalies make VAR a challenging problem. Supervised solutions often fail to work in such scenarios due to a lack of ability to adapt with concept drifts. To this end, we have studied the effect of temporal derivatives over differential manifolds for designing a zero-shot (label agnostic) VAR solution. The rationale behind this work is leveraging the genericity and discriminative representation available in the geometric-structure of motion-tensors. Our approach proceeds by drawing segments of temporal-derivatives from raw image-sequences and projecting them over Grassmann product space before clustering. Suitability of the proposed approach is corroborated with extensive experiments and comparisons with other arts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003572",
    "keywords": [
      "A priori and a posteriori",
      "Anomaly (physics)",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Condensed matter physics",
      "Discriminative model",
      "Epistemology",
      "Fuzzy logic",
      "Law",
      "Linguistics",
      "Motion (physics)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Vagueness",
      "Zero (linguistics)"
    ],
    "authors": [
      {
        "surname": "Buckchash",
        "given_name": "Himanshu"
      },
      {
        "surname": "Raman",
        "given_name": "Balasubramanian"
      }
    ]
  },
  {
    "title": "A non-factoid question answering system for prior art search",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114910",
    "abstract": "A patent gives the owner of an invention the exclusive rights to make, use and sell their invention. Before a new patent application is filed, patent lawyers are required to engage in Prior Art Search to determine the likelihood that an invention is novel, valid or to make sense of the domain. To perform this search, existing platforms utilize keywords and Boolean Logic, which disregards the syntax and semantics of natural language and thus, making the search extremely difficult. Consequently, studies regarding semantics using neural embeddings exist, but these only consider a narrow number of unidirectional words. In this study, we propose an end-to-end framework to consider bidirectional semantics, syntax and the thematic nature of natural language for prior art search. The proposed framework goes beyond keywords as input queries and takes a patent as the input. The contributions of this paper is twofold; adapting pre-trained embedding models (e.g., BERT) to address the semantics and syntax of language, followed by the second component, which exploits topic modeling to build a diversified answer that covers all themes across domains of the input patent. We evaluate the performance of the proposed framework on the CLEF-IP 2011 benchmark dataset and a real-world dataset obtained from Google patent repository and show that the proposed framework outperforms existing methods and returns meaningful results for a given patent.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003511",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Domain (mathematical analysis)",
      "Embedding",
      "Geodesy",
      "Geography",
      "Information retrieval",
      "Mathematical analysis",
      "Mathematics",
      "Natural language",
      "Natural language processing",
      "Programming language",
      "Question answering",
      "Semantics (computer science)",
      "Syntax"
    ],
    "authors": [
      {
        "surname": "Zihayat",
        "given_name": "Morteza"
      },
      {
        "surname": "Etwaroo",
        "given_name": "Rochelle"
      }
    ]
  },
  {
    "title": "A continuous-state cellular automata algorithm for global optimization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114930",
    "abstract": "Cellular automata are capable of developing complex behaviors based on simple local interactions between their elements. Some of these characteristics have been used to propose and improve metaheuristics for global optimization; however, the properties offered by the evolution rules in cellular automata have not yet been used directly in optimization tasks. Inspired by the complexity that various evolution rules of cellular automata can offer, the continuous-state cellular automata algorithm is proposed. In this way, the algorithm takes advantage of different evolution rules to maintain a balance that maximizes the exploration and exploitation properties in each iteration. The efficiency of the algorithm is proven with 48 test problems widely used in the literature, 4 engineering applications that were also used in recent literature, and the design of adaptive infinite-impulse response filters, with the reference functions of 10 full-order filters being tested. The numerical results prove its competitiveness in comparison with state-of-the-art algorithms. The source codes of the proposed algorithm are publicly available at https://github.com/juanseck/CCAA.git.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003717",
    "keywords": [],
    "authors": [
      {
        "surname": "Seck-Tuoh-Mora",
        "given_name": "Juan Carlos"
      },
      {
        "surname": "Hernandez-Romero",
        "given_name": "Norberto"
      },
      {
        "surname": "Lagos-Eulogio",
        "given_name": "Pedro"
      },
      {
        "surname": "Medina-Marin",
        "given_name": "Joselito"
      },
      {
        "surname": "Zuñiga-Peña",
        "given_name": "Nadia Samantha"
      }
    ]
  },
  {
    "title": "Vehicle operating state anomaly detection and results virtual reality interpretation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114928",
    "abstract": "Nowadays, with the increasing complexity of vehicle operating environment, vehicle operating reliability becomes growing essential and has attracted significant attention in recent decades. The majority of vehicle operating activities are organized in the format of time series data. Many conventional approaches are proposed to deal with time-series data. Autoregressive Integrated Moving Average (ARIMA) approach is one of the most widely used methods to analyze time-series data. In this paper, an ARIMA-based anomaly detection framework is developed to identify abnormal states of the vehicles based on the multiple-channel operating time series data. The state anomaly is captured by the deviation of real-time values at different channels from the predictions. After abnormal operating states are identified, a novel immersive Virtual Reality (VR) tool is applied to visualize the difference between normal and abnormal status. The combination of ARIMA anomaly detection and VR visualization for vehicle multiple-channel operating time-series data would provide a robust way to present the results of vehicle operating status. A large scale data set of 14 days operational performance channels of a specific vehicle is utilized to validate the performance of the proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003699",
    "keywords": [
      "Algorithm",
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Computer science",
      "Condensed matter physics",
      "Data mining",
      "Interpretation (philosophy)",
      "Physics",
      "Programming language",
      "State (computer science)",
      "Virtual reality"
    ],
    "authors": [
      {
        "surname": "Alizadeh",
        "given_name": "Morteza"
      },
      {
        "surname": "Hamilton",
        "given_name": "Michael"
      },
      {
        "surname": "Jones",
        "given_name": "Parker"
      },
      {
        "surname": "Ma",
        "given_name": "Junfeng"
      },
      {
        "surname": "Jaradat",
        "given_name": "Raed"
      }
    ]
  },
  {
    "title": "Biometric keystroke barcoding: A next-gen authentication framework",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114980",
    "abstract": "Investigation of new intelligent solutions for user identification and authentication is and will be essential for enhancing the security of the alphanumeric passwords entered on touchscreen and traditional keyboards. Extraction of the keystrokes has been very beneficial given the intelligent authentication protocols operating in time-domain; while the time-domain solutions drastically lose their efficiency over time due to converging inter-key times. Realistically reflecting the habitual traits, the frequency-domain solutions, however, reveal unique biometric characteristics better, without any risk of convergence. On the contrary, the existing frequency-based frameworks don’t provide storable biometric data for further classification of the attempts. Therefore, we propose a novel barcoding framework converting habitual biometric information into storable barcodes as very low-size barcode images. The key-press times are extracted and turned into pseudo-signals exhibiting binary-train characteristics for continuous wavelet transformation (CWT). The transformed signals are primarily categorized with 4-scale scalograms by various complex frequency B-spline wavelets and subsequently superposed to create the unique barcodes. One-class support vector machines (SVM) is employed as the main classifier for training and testing the barcodes and very promising results are achieved given the lowest equal error rate (EER) of 1.83%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004218",
    "keywords": [
      "Artificial intelligence",
      "Authentication (law)",
      "Biology",
      "Biometrics",
      "Computer science",
      "Computer security",
      "DNA barcoding",
      "Ecology",
      "Keystroke dynamics",
      "Keystroke logging",
      "Natural language processing",
      "Password",
      "Pattern recognition (psychology)",
      "S/KEY"
    ],
    "authors": [
      {
        "surname": "Alpar",
        "given_name": "Orcan"
      }
    ]
  },
  {
    "title": "Deep convolutional features for image retrieval",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114940",
    "abstract": "Nowadays, the use of Convolutional Neural Networks (CNNs) has led to tremendous achievements in several computer vision challenges. CNN-based image retrieval methods vary in complexity, growing capacity, and execution time. This work presents a state-of-the-art review in Deep Convolutional Features for image retrieval, pointing out their scope, advantages, and limitations. Moreover, the paper presents a procedure that adopts the latest architectures of pre-trained CNNs that have been initially proposed for image classification to shape image retrieval features. It investigates their suitability on several image retrieval tasks, without any optimization procedure, exhaustive preparatory work, and tuning. Each network’s performance is evaluated in two different setups: one employing global and one using local representations. Extensive experiments on several well-known benchmark datasets demonstrate that a simple normalization on the pre-trained networks yields results comparable to state-of-the-art approaches. The global descriptor shapes a plug-and-play approach, which can be adopted for description and retrieval without any prior initialization or training. Moreover, the descriptor’s localized version outperforms significantly much more sophisticated and complex methods of the recent literature.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100381X",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Contextual image classification",
      "Convolutional neural network",
      "Deep learning",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Image retrieval",
      "Initialization",
      "Machine learning",
      "Normalization (sociology)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Gkelios",
        "given_name": "Socratis"
      },
      {
        "surname": "Sophokleous",
        "given_name": "Aphrodite"
      },
      {
        "surname": "Plakias",
        "given_name": "Spiros"
      },
      {
        "surname": "Boutalis",
        "given_name": "Yiannis"
      },
      {
        "surname": "Chatzichristofis",
        "given_name": "Savvas A."
      }
    ]
  },
  {
    "title": "A Hierarchical Memory Network for Knowledge Tracing",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114935",
    "abstract": "Knowledge Tracing (KT) is a task to acquire students’ mastery level of skills based on their performance in learning process. The existing KT models have gradually achieved improvements in prediction performance. However, they do not well simulate working memory and long-term memory in human memory mechanism, which is closely related to learning process. In our paper, we propose a Hierarchical Memory Network (HMN) to fit human memory mechanism better in KT. The hierarchical memory, an essential component of HMN, is achieved by an external memory matrix and two mechanisms (divide mechanism, decay mechanism). The matrix simulates working memory by working storage and long-term memory by long-term storage through divide mechanism. Furthermore, the working storage can be changed directly, while the long-term storage is changed according to decay rates obtained from decay mechanism. Experiments demonstrate that our model outperforms several classical models in four public datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003766",
    "keywords": [
      "Artificial intelligence",
      "Cognition",
      "Computer science",
      "Economics",
      "Epistemology",
      "Long-term memory",
      "Management",
      "Mechanism (biology)",
      "Memory model",
      "Neuroscience",
      "Operating system",
      "Parallel computing",
      "Philosophy",
      "Process (computing)",
      "Psychology",
      "Shared memory",
      "Short-term memory",
      "Task (project management)",
      "Tracing",
      "Working memory"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Sannyuya"
      },
      {
        "surname": "Zou",
        "given_name": "Rui"
      },
      {
        "surname": "Sun",
        "given_name": "Jianwen"
      },
      {
        "surname": "Zhang",
        "given_name": "Kai"
      },
      {
        "surname": "Jiang",
        "given_name": "Lulu"
      },
      {
        "surname": "Zhou",
        "given_name": "Dongbo"
      },
      {
        "surname": "Yang",
        "given_name": "Jing"
      }
    ]
  },
  {
    "title": "A brain-inspired information processing algorithm and its application in text classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114828",
    "abstract": "Cognitive scientists believe that the human brain is constantly predicting the information it is going to receive, and this prediction ability is acquired based on the experiences gained from previous information it received over its lifetime. Inspired by this brain-behavior, we propose a new information processing algorithm with the building blocks named as boxes and routes. The function of boxes is to store the learned information and the function of routes is to represent the relationship between information. The novel algorithm features generality and objectivity. It imitates the mechanism of the human brain and has functions such as information learning, comparison, prediction, and forgetting. It also has advantages in dealing with continuous time series data by using routes and high-order boxes. The algorithm is self-adaptive and unsupervised. It does not need manual intervention information to train the model. It can learn useful information from undefined data and subsequently construct a hierarchical network corresponding to the characteristics of the input information, which can be used for classification or prediction. To prove the validity of this new algorithm, a classifier is constructed based on the hierarchical network to do text classification. We select a collection of Chinese literature from 30 litterateurs as samples to train the classifier. For 10 classes situation, the optimal average accuracy of classification reaches 79.5%, which outperforms other approaches commonly used in literatures, verifying the effectiveness of the proposed algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002694",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Forgetting",
      "Generality",
      "Information processing",
      "Linguistics",
      "Machine learning",
      "Neuroscience",
      "Philosophy",
      "Psychology",
      "Psychotherapist"
    ],
    "authors": [
      {
        "surname": "Mou",
        "given_name": "Shenghong"
      },
      {
        "surname": "Du",
        "given_name": "Pengwei"
      },
      {
        "surname": "Cheng",
        "given_name": "Zhiyuan"
      }
    ]
  },
  {
    "title": "Expert systems: Definitions, advantages and issues in medical field applications",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114900",
    "abstract": "The aim of this review is to provide a broad overview of the state-of-the-art works mainly published in the last ten years on expert systems applied in different medical domains. Being able to support and sometimes substitute experts, an expert system may be a precious ally for medical diagnoses. Medical expert system applications provide physicians and patients with an immediate access to knowledge and advice, rooting their flexibility into their knowledge bases, rule sets and graphical interfaces. To be trusted by their users, medical expert systems should follow some criteria, which we investigate along with their different realization, from fuzzy logic to wearable solutions for out-of-clinical-environment care. We also consider the advantages of approaching diagnoses and alert systems through an artificial intelligence counterpart, without forgetting the importance of a good validation to assess the system functionality. Therefore, we show the heterogeneity of the solutions proposed by the literature, bounded to the specific needs a medical expert system is called to answer, the common lack of a system validation and the possible benefits deriving from these systems application.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003419",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data science",
      "Expert system",
      "Field (mathematics)",
      "Flexibility (engineering)",
      "Human–computer interaction",
      "Mathematics",
      "Medical diagnosis",
      "Medicine",
      "Pathology",
      "Pure mathematics",
      "Software engineering",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Saibene",
        "given_name": "Aurora"
      },
      {
        "surname": "Assale",
        "given_name": "Michela"
      },
      {
        "surname": "Giltri",
        "given_name": "Marta"
      }
    ]
  },
  {
    "title": "Feature discovery in NIR spectroscopy based Rocha pear classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114949",
    "abstract": "Non-invasive techniques for automatic fruit classification are gaining importance in the global agro-industry as they allow for optimizing harvesting, storage, management, and distribution decisions. Visible, near infra-red (NIR) diffuse reflectance spectroscopy is one of the most employed techniques in such fruit classification. Typically, after the acquisition of a fruit reflectance spectrum the wavelength domain signal is preprocessed and a classifier is designed. Up to now, little or no work considered the problem of feature generation and selection of the reflectance spectrum. This work aims at filling this gap, by exploiting a feature engineering phase before the classifier. The usual approach where the classifier is fed directly with the reflectances measured at each wavelength is contrasted with the proposed division of the spectra into bands and their characterization in wavelength, frequency, and wavelength-frequency domains. Feature selection is also applied for optimizing efficiency, predictive accuracy, and for mitigating over-training. A total of 3050 Rocha pear samples from different origins and harvest years are considered. Statistical tests of hypotheses on classification results of soluble solids content – a predictor of both fruit sweetness and ripeness – show that the proposed preliminary phase of feature engineering outperforms the usual direct approach both in terms of accuracy and in the number of necessary features. Moreover, the method allows for the identification of features that are physical chemistry meaningful.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003900",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Feature (linguistics)",
      "Linguistics",
      "PEAR",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Spectroscopy",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Daniel",
        "given_name": "Mariana"
      },
      {
        "surname": "Guerra",
        "given_name": "Rui"
      },
      {
        "surname": "Brázio",
        "given_name": "António"
      },
      {
        "surname": "Rodrigues",
        "given_name": "Daniela"
      },
      {
        "surname": "Cavaco",
        "given_name": "Ana Margarida"
      },
      {
        "surname": "Antunes",
        "given_name": "Maria Dulce"
      },
      {
        "surname": "Valente de Oliveira",
        "given_name": "José"
      }
    ]
  },
  {
    "title": "Deep Belief Network based audio classification for construction sites monitoring",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114839",
    "abstract": "In this paper, we propose a Deep Belief Network (DBN) based approach for the classification of audio signals to improve work activity identification and remote surveillance of construction projects. The aim of the work is to obtain an accurate and flexible tool for consistently executing and managing the unmanned monitoring of construction sites by using distributed acoustic sensors. In this paper, ten classes of multiple construction equipment and tools, frequently and broadly used in construction sites, have been collected and examined to conduct and validate the proposed approach. The input provided to the DBN consists in the concatenation of several statistics evaluated by a set of spectral features, like MFCCs and mel-scaled spectrogram. The proposed architecture, along with the preprocessing and the feature extraction steps, has been described in details while the effectiveness of the proposed idea has been demonstrated by some numerical results, evaluated by using real-world recordings. The final overall accuracy on the test set is up to 98% and is a significantly improved performance compared to other state-of-the-are approaches. A practical and real-time application of the presented method has been also proposed in order to apply the classification scheme to sound data recorded in different environmental scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002803",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Combinatorics",
      "Computer science",
      "Concatenation (mathematics)",
      "Data mining",
      "Deep belief network",
      "Deep learning",
      "Feature extraction",
      "Identification (biology)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Mel-frequency cepstrum",
      "Pattern recognition (psychology)",
      "Preprocessor",
      "Programming language",
      "Scheme (mathematics)",
      "Set (abstract data type)",
      "Spectrogram"
    ],
    "authors": [
      {
        "surname": "Scarpiniti",
        "given_name": "Michele"
      },
      {
        "surname": "Colasante",
        "given_name": "Francesco"
      },
      {
        "surname": "Di Tanna",
        "given_name": "Simone"
      },
      {
        "surname": "Ciancia",
        "given_name": "Marco"
      },
      {
        "surname": "Lee",
        "given_name": "Yong-Cheol"
      },
      {
        "surname": "Uncini",
        "given_name": "Aurelio"
      }
    ]
  },
  {
    "title": "Flexible runtime support of business processes under rolling planning horizons",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114857",
    "abstract": "This work has been motivated by the needs we discovered when analyzing real-world processes from the healthcare domain that have revealed high flexibility demands and complex temporal constraints. When trying to model these processes with existing languages, we learned that none of the latter was able to fully address these needs. This motivated us to design TConDec-R, a declarative process modeling language enabling the specification of complex temporal constraints. Enacting business processes based on declarative process models, however, introduces a high complexity due to the required optimization of objective functions, the handling of various temporal constraints, the concurrent execution of multiple process instances, the management of cross-instance constraints, and complex resource allocations. Consequently, advanced user support through optimized schedules is required when executing the instances of such models. In previous work, we suggested a method for generating an optimized enactment plan for a given set of process instances created from a TConDec-R model. However, this approach was not applicable to scenarios with uncertain demands in which the enactment of newly created process instances starts continuously over time, as in the considered healthcare scenarios. Here, the process instances to be planned within a specific timeframe cannot be considered in isolation from the ones planned for future timeframes. To be able to support such scenarios, this article significantly extends our previous work by generating optimized enactment plans under a rolling planning horizon. We evaluate the approach by applying it to a particularly challenging healthcare process scenario, i.e., the diagnostic procedures required for treating patients with ovarian carcinoma in a Woman Hospital. The application of the approach to this sophisticated scenario allows avoiding constraint violations and effectively managing shared resources, which contributes to reduce the length of patient stays in the hospital.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002980",
    "keywords": [
      "Aerospace engineering",
      "Business",
      "Computer science",
      "Engineering",
      "New horizons",
      "Process management",
      "Software engineering",
      "Spacecraft"
    ],
    "authors": [
      {
        "surname": "Barba",
        "given_name": "Irene"
      },
      {
        "surname": "Jiménez-Ramírez",
        "given_name": "Andrés"
      },
      {
        "surname": "Reichert",
        "given_name": "Manfred"
      },
      {
        "surname": "Del Valle",
        "given_name": "Carmelo"
      },
      {
        "surname": "Weber",
        "given_name": "Barbara"
      }
    ]
  },
  {
    "title": "Cross-efficiency evaluation and decomposition with directional distance function in series and parallel systems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114933",
    "abstract": "As an extension tool of data envelopment analysis (DEA), cross-efficiency evaluation has been widely applied in ranking decision making units (DMUs). Series and parallel are two basic network structures. Current cross-efficiency evaluation in both the structures has been developed with oriented DEA models that minimize inputs while producing at least the given outputs. The extension proposed here is aimed at providing a non-oriented cross-efficiency evaluation based on measures accounting for input excesses and output shortfalls simultaneously. This paper extends the cross-efficiency evaluation in series and parallel structures for use with the directional distance function (DDF). In the proposed DDF cross-efficiency evaluation, the system cross-efficiency score can be decomposed into a weighted average of process cross-efficiency scores, whether for the series or parallel system. The problem with the alternate optima for the multipliers is also addressed and models of avoiding zero multipliers are provided in this new context. Numerical examples from the literature and an empirical one illustrate the validity of the proposed network cross-efficiency evaluation and decomposition, and the superiority of the models in avoiding zero multipliers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003742",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Decomposition",
      "Ecology",
      "Evolutionary biology",
      "Function (biology)",
      "Paleontology",
      "Series (stratigraphy)"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Ruiyue"
      },
      {
        "surname": "Tu",
        "given_name": "Chong"
      }
    ]
  },
  {
    "title": "Hunger games search: Visions, conception, implementation, deep analysis, perspectives, and towards performance shifts",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114864",
    "abstract": "A recent set of overused population-based methods have been published in recent years. Despite their popularity, most of them have uncertain, immature performance, partially done verifications, similar overused metaphors, similar immature exploration and exploitation components and operations, and an insecure tradeoff between exploration and exploitation trends in most of the new real-world cases. Therefore, all users need to extensively modify and adjust their operations based on main evolutionary methods to reach faster convergence, more stable balance, and high-quality results. To move the optimization community one step ahead toward more focus on performance rather than change of metaphor, a general-purpose population-based optimization technique called Hunger Games Search (HGS) is proposed in this research with a simple structure, special stability features and very competitive performance to realize the solutions of both constrained and unconstrained problems more effectively. The proposed HGS is designed according to the hunger-driven activities and behavioural choice of animals. This dynamic, fitness-wise search method follows a simple concept of “Hunger” as the most crucial homeostatic motivation and reason for behaviours, decisions, and actions in the life of all animals to make the process of optimization more understandable and consistent for new users and decision-makers. The Hunger Games Search incorporates the concept of hunger into the feature process; in other words, an adaptive weight based on the concept of hunger is designed and employed to simulate the effect of hunger on each search step. It follows the computationally logical rules (games) utilized by almost all animals and these rival activities and games are often adaptive evolutionary by securing higher chances of survival and food acquisition. This method's main feature is its dynamic nature, simple structure, and high performance in terms of convergence and acceptable quality of solutions, proving to be more efficient than the current optimization methods. The effectiveness of HGS was verified by comparing HGS with a comprehensive set of popular and advanced algorithms on 23 well-known optimization functions and the IEEE CEC 2014 benchmark test suite. Also, the HGS was applied to several engineering problems to demonstrate its applicability. The results validate the effectiveness of the proposed optimizer compared to popular essential optimizers, several advanced variants of the existing methods, and several CEC winners and powerful differential evolution (DE)-based methods abbreviated as LSHADE, SPS_L_SHADE_EIG, LSHADE_cnEpSi, SHADE, SADE, MPEDE, and JDE methods in handling many single-objective problems. We designed this open-source population-based method to be a standard tool for optimization in different areas of artificial intelligence and machine learning with several new exploratory and exploitative features, high performance, and high optimization capacity. The method is very flexible and scalable to be extended to fit more form of optimization cases in both structural aspects and application sides. This paper's source codes, supplementary files, Latex and office source files, sources of plots, a brief version and pseudocode, and an open-source software toolkit for solving optimization problems with Hunger Games Search and online web service for any question, feedback, suggestion, and idea on HGS algorithm will be available to the public at https://aliasgharheidari.com/HGS.html.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003055",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convergence (economics)",
      "Demography",
      "Economic growth",
      "Economics",
      "Epistemology",
      "Machine learning",
      "Management science",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Philosophy",
      "Popularity",
      "Population",
      "Process (computing)",
      "Programming language",
      "Psychology",
      "Quality (philosophy)",
      "Set (abstract data type)",
      "Simple (philosophy)",
      "Social psychology",
      "Sociology",
      "Stability (learning theory)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Yutao"
      },
      {
        "surname": "Chen",
        "given_name": "Huiling"
      },
      {
        "surname": "Heidari",
        "given_name": "Ali Asghar"
      },
      {
        "surname": "Gandomi",
        "given_name": "Amir H"
      }
    ]
  },
  {
    "title": "A deep learning based hybrid method for hourly solar radiation forecasting",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114941",
    "abstract": "Solar radiation forecasting is a key technology to improve the control and scheduling performance of photovoltaic power plants. In this paper, a deep learning based hybrid method for 1-hour ahead Global Horizontal Irradiance (GHI) forecasting is proposed. Specifically, a deep learning based clustering method, deep time-series clustering, is adopted to group the GHI time series data into multiple clusters to better identify its irregular patterns and thus providing a better clustering performance. Then, the Feature Attention Deep Forecasting (FADF) deep neural network is built for each cluster to generate the GHI forecasts. The developed FADF dynamically allocates different importance to different features and utilizes the weighted features to forecast the next hour GHI. The solar forecasting performance of the proposed method is evaluated with the National Solar Radiation Database. Simulation results show that the proposed method yields the most accurate solar forecasting among the smart persistence and state-of-the-art models. The proposed method reduces the root mean square error as compared to the smart persistence by 11.88% and 12.65% for the Itupiranga and Ocala dataset, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003821",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Environmental science",
      "Machine learning",
      "Meteorology",
      "Physics"
    ],
    "authors": [
      {
        "surname": "Lai",
        "given_name": "Chun Sing"
      },
      {
        "surname": "Zhong",
        "given_name": "Cankun"
      },
      {
        "surname": "Pan",
        "given_name": "Keda"
      },
      {
        "surname": "Ng",
        "given_name": "Wing W.Y."
      },
      {
        "surname": "Lai",
        "given_name": "Loi Lei"
      }
    ]
  },
  {
    "title": "Joint extraction of entities and overlapping relations using source-target entity labeling",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114853",
    "abstract": "Joint extraction of entities and overlapping relations has attracted considerable attention in recent research. Existing relation extraction methods rely on a training set that is labeled by the distant supervision method for supervised relation extraction. However, the drawbacks of these methods are that large-scale unlabeled data cannot be used and the quality of labeled data cannot be guaranteed. Moreover, owing to the relatively complex overlapping relations, it is difficult to perform joint entity-relation extraction accurately. In this study, we propose an end-to-end neural network model (BERT-JEORE) for the joint extraction of entities and overlapping relations. First, we use the BERT-based parameter-sharing layer to capture the joint features of entities and overlapping relations. Then, we implement the source-target BERT model to assign entity labels to each token in a sentence, thereby expanding the amount of labeled data and improving their quality. Finally, we design a three-step overlapping relations extraction model and use it to predict the relations between all entity pairs. Experiments conducted on two public datasets show that BERT-JEORE achieves the best current performance and outperforms the baseline models by a significant margin. Further analysis shows that our model can effectively capture different types of overlapping relational triplets in a sentence.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002943",
    "keywords": [
      "Architectural engineering",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Data mining",
      "Economics",
      "Engineering",
      "Epistemology",
      "Joint (building)",
      "Labeled data",
      "Machine learning",
      "Management",
      "Margin (machine learning)",
      "Natural language processing",
      "Philosophy",
      "Programming language",
      "Quality (philosophy)",
      "Relation (database)",
      "Relationship extraction",
      "Security token",
      "Sentence",
      "Sequence labeling",
      "Set (abstract data type)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Hang",
        "given_name": "Tingting"
      },
      {
        "surname": "Feng",
        "given_name": "Jun"
      },
      {
        "surname": "Wu",
        "given_name": "Yirui"
      },
      {
        "surname": "Yan",
        "given_name": "Le"
      },
      {
        "surname": "Wang",
        "given_name": "Yunfeng"
      }
    ]
  },
  {
    "title": "A deep learning based hybrid method for hourly solar radiation forecasting",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114941",
    "abstract": "Solar radiation forecasting is a key technology to improve the control and scheduling performance of photovoltaic power plants. In this paper, a deep learning based hybrid method for 1-hour ahead Global Horizontal Irradiance (GHI) forecasting is proposed. Specifically, a deep learning based clustering method, deep time-series clustering, is adopted to group the GHI time series data into multiple clusters to better identify its irregular patterns and thus providing a better clustering performance. Then, the Feature Attention Deep Forecasting (FADF) deep neural network is built for each cluster to generate the GHI forecasts. The developed FADF dynamically allocates different importance to different features and utilizes the weighted features to forecast the next hour GHI. The solar forecasting performance of the proposed method is evaluated with the National Solar Radiation Database. Simulation results show that the proposed method yields the most accurate solar forecasting among the smart persistence and state-of-the-art models. The proposed method reduces the root mean square error as compared to the smart persistence by 11.88% and 12.65% for the Itupiranga and Ocala dataset, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003821",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Environmental science",
      "Machine learning",
      "Meteorology",
      "Physics"
    ],
    "authors": [
      {
        "surname": "Lai",
        "given_name": "Chun Sing"
      },
      {
        "surname": "Zhong",
        "given_name": "Cankun"
      },
      {
        "surname": "Pan",
        "given_name": "Keda"
      },
      {
        "surname": "Ng",
        "given_name": "Wing W.Y."
      },
      {
        "surname": "Lai",
        "given_name": "Loi Lei"
      }
    ]
  },
  {
    "title": "Soft target and functional complexity reduction: A hybrid regularization method for genetic programming",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114929",
    "abstract": "Regularization is frequently used in supervised machine learning to prevent models from overfitting. This paper tackles the problem of regularization in genetic programming. We apply, for the first time, soft target regularization, a method recently defined for artificial neural networks, to genetic programming. Also, we introduce a novel measure of functional complexity of the genetic programming individuals, aimed at quantifying their degree of curvature. We experimentally demonstrate that both the use of soft target regularization, and the minimization of the complexity during learning, are often able to reduce overfitting, but they are never able to eliminate it. On the other hand, we demonstrate that the integration of these two strategies into a novel hybrid genetic programming system can completely eliminate overfitting, for all the studied test cases. Last but not least, consistently with what found in the literature, we offer experimental evidence of the fact that the size of the genetic programming models has no correlation with their generalization ability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003705",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Genetic programming",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Overfitting",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "Vanneschi",
        "given_name": "Leonardo"
      },
      {
        "surname": "Castelli",
        "given_name": "Mauro"
      }
    ]
  },
  {
    "title": "WiFiNet: WiFi-based indoor localisation using CNNs",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114906",
    "abstract": "Different technologies have been proposed to provide indoor localisation: magnetic field, Bluetooth, WiFi, etc. Among them, WiFi is the one with the highest availability and highest accuracy. This fact allows for an ubiquitous accurate localisation available for almost any environment and any device. However, WiFi-based localisation is still an open problem. In this article, we propose a new WiFi-based indoor localisation system that takes advantage of the great ability of Convolutional Neural Networks in classification problems. Three different approaches were used to achieve this goal: a custom architecture called WiFiNet, designed and trained specifically to solve this problem, and the most popular pre-trained networks using both transfer learning and feature extraction. Results indicate that WiFiNet is as a great approach for indoor localisation in a medium-sized environment (30 positions and 113 access points) as it reduces the mean localisation error (33%) and the processing time when compared with state-of-the-art WiFi indoor localisation algorithms such as SVM.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100347X",
    "keywords": [
      "Artificial intelligence",
      "Computer network",
      "Computer science"
    ],
    "authors": [
      {
        "surname": "Hernández",
        "given_name": "Noelia"
      },
      {
        "surname": "Parra",
        "given_name": "Ignacio"
      },
      {
        "surname": "Corrales",
        "given_name": "Héctor"
      },
      {
        "surname": "Izquierdo",
        "given_name": "Rubén"
      },
      {
        "surname": "Ballardini",
        "given_name": "Augusto Luis"
      },
      {
        "surname": "Salinas",
        "given_name": "Carlota"
      },
      {
        "surname": "García",
        "given_name": "Iván"
      }
    ]
  },
  {
    "title": "On the spectrum of achievable targets in cross-efficiency evaluation and the associated secondary goal models",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114927",
    "abstract": "The cross-efficiency (CE) evaluation method was introduced to improve the discriminatory power of DEA and eliminate unrealistic DEA weighting schemes. One important issue in CE evaluation is the non-uniqueness of the CE scores. Several secondary goal models based on different targets for cross-efficiencies (CEs) of each DMU with respect to other DMUs were proposed to address this issue. However, the suggested targets, fixed value 1 and the CCR efficiency score, are not achievable for all CEs. Moreover, the proposed secondary goal models based on these targets are sensitive to outlier DMUs, and may generate unrealistic CE scores. In this manuscript, we prove that the spectrum of achievable targets of CEs can be obtained using the most resonated appreciative (MRA) model, proposed by Oral et al. (2015), and the least resonated appreciative (LRA) model that we introduce. To this end, we propose a general secondary goal model using multi-objective programming and show that the CEs generated using MRA (LRA) model for each DMU is greater (less) than the corresponding CEs obtained by any other model that can be derived from the proposed benevolent (aggressive) general model. Using this achievable spectrum, we then propose several benevolent, aggressive and neutral secondary goal, and a weighted average CE evaluation model. Using two real examples, we compare the results of the proposed CE methods with those obtained from several other CE methods. Our data analyses indicate that our proposed methods are less sensitive to outliers, less biased towards 1, has better discriminatory power and can identify pseudo-efficient DMUs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003687",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Medicine",
      "Outlier",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Radiology",
      "Spectrum (functional analysis)",
      "Uniqueness",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Davtalab-Olyaie",
        "given_name": "Mostafa"
      },
      {
        "surname": "Ghandi",
        "given_name": "Fatemeh"
      },
      {
        "surname": "Asgharian",
        "given_name": "Masoud"
      }
    ]
  },
  {
    "title": "Calculation of gamma-ray exposure buildup factor based on backpropagation neural network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115004",
    "abstract": "In this paper, a method based on a backpropagation neural network (BPNN) is proposed to calculate the exposure buildup factor ( B D ) of a point isotropic source in an infinite homogeneous medium under arbitrary energy and mean free path (mfp). The results obtained for aluminum, iron, lead, and concrete based on BPNN are compared to ANSI/ANS-6.4.3 standard data, the results calculated by MCNP 5 Monte Carlo code, and a geometric progression (G-P) fitting formula, and show that the B D calculated by the BPNN model is more consistent with the ANS standard data. This method improves the calculation and fitting effect of B D compared to other methods. This paper proposes a systematic process combining a Monte Carlo method and BPNN to calculate and predict the B D of new materials under different energy and mfp, thus replacing the G-P fitting formula and improving calculation accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004450",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Backpropagation",
      "Computer science",
      "Energy (signal processing)",
      "Homogeneous",
      "Isotropy",
      "Mathematics",
      "Monte Carlo method",
      "Optics",
      "Physics",
      "Statistical physics",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Runkai"
      },
      {
        "surname": "Cammi",
        "given_name": "Antonio"
      },
      {
        "surname": "Seidl",
        "given_name": "Marcus"
      },
      {
        "surname": "Macian-Juan",
        "given_name": "Rafael"
      },
      {
        "surname": "Wang",
        "given_name": "Xiang"
      }
    ]
  },
  {
    "title": "Towards online applications of EEG biometrics using visual evoked potentials",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114961",
    "abstract": "Electroencephalogram (EEG)-based biometrics have attracted increasing attention in recent years. A few studies have used visual evoked potentials (VEPs) in EEG biometrics due to their high signal-to-noise ratio (SNR) and good stability. However, a systematic comparison of different types of VEPs is still lacking. Therefore, this study proposes a system framework for VEP-based biometrics. We quantitatively compared the performance of three types of VEP signals in person identification. Flash VEPs (f-VEPs), steady-state VEPs (ss-VEPs), and code-modulated VEPs (c-VEPs) measured from a group of 21 subjects on two different days were used to estimate the correct recognition rate (CRR). We adopted a template-matching-based identification algorithm that was developed for VEP detection in brain-computer interfaces (BCIs) for person identification. Furthermore, this study demonstrates an online person identification system using c-VEPs with a group of 15 subjects. Among the three methods, c-VEPs achieved the highest CRRs of 100% using 3.15-s VEP data (a 5.25-s duration including 2.1-s intervals) in the intra-session condition and 99.48% using 10.5-s VEP data (a 17.5-s duration including 7-s intervals) in the cross-session condition. The online system achieved a cross-session CRR of 98.93% using 10.5-s VEP data (a 14-s duration including 3.5-s intervals). A systematic comparison of the performance of the three types of VEP signals in EEG-based person identification revealed that the c-VEP paradigm achieved the highest CRRs. The online system further demonstrated high performance in practical applications. The proposed VEP-based biometric system obtained promising identification performance, showing great potential for online person identification applications in real life.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004024",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Electroencephalography",
      "Human–computer interaction",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Psychology",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Hongze"
      },
      {
        "surname": "Chen",
        "given_name": "Yuanfang"
      },
      {
        "surname": "Pei",
        "given_name": "Weihua"
      },
      {
        "surname": "Chen",
        "given_name": "Hongda"
      },
      {
        "surname": "Wang",
        "given_name": "Yijun"
      }
    ]
  },
  {
    "title": "Ensemble forecasting system for short-term wind speed forecasting based on optimal sub-model selection and multi-objective version of mayfly optimization algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114974",
    "abstract": "Wind energy has attracted considerable attention in the past decades as a low-carbon, environmentally friendly, and efficient renewable energy. However, the irregularity of wind speed makes it difficult to integrate wind energy into smart grids. Thus, achieving credible and effective wind speed forecasting results is crucial for the operation and management of wind energy. In this study, we propose an ensemble forecasting system that integrates data decomposition technology, sub-model selection, a novel multi-objective version of the Mayfly algorithm, and different predictors to better demonstrate the stochasticity and fluctuation of wind speed data. After decomposition using the data decomposition technology, each decomposed wind speed series is considered as the input to multiple predictors, from which the optimal forecasting model for each sub-series is determined based on sub-model selection. To obtain reliable forecasting results, a novel multi-objective version of the Mayfly algorithm is proposed to estimate the optimal weight coefficients for integrating the forecasting values of the sub-series. Based on three experiments and four analyses, the proposed ensemble system is verified as effective for obtaining accurate and stable point forecasting and interval forecasting performances, thus aiding in the planning and dispatching of power grids.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004152",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Data mining",
      "Decomposition",
      "Ecology",
      "Electric power system",
      "Electrical engineering",
      "Engineering",
      "Ensemble forecasting",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Mayfly",
      "Meteorology",
      "Nymph",
      "Paleontology",
      "Physics",
      "Power (physics)",
      "Probabilistic forecasting",
      "Probabilistic logic",
      "Quantum mechanics",
      "Renewable energy",
      "Selection (genetic algorithm)",
      "Series (stratigraphy)",
      "Term (time)",
      "Time series",
      "Wind power",
      "Wind power forecasting",
      "Wind speed"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Zhenkun"
      },
      {
        "surname": "Jiang",
        "given_name": "Ping"
      },
      {
        "surname": "Wang",
        "given_name": "Jianzhou"
      },
      {
        "surname": "Zhang",
        "given_name": "Lifang"
      }
    ]
  },
  {
    "title": "Capturing dynamics of post-earnings-announcement drift using a genetic algorithm-optimized XGBoost",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114892",
    "abstract": "Post-Earnings-Announcement Drift (PEAD) is a stock market phenomenon when a stock’s cumulative abnormal return has a tendency to drift in the direction of an earnings surprise in the near term following an earnings announcement. Although it is one of the most studied stock market anomalies, the current literature is often limited in explaining this phenomenon by a small number of factors using simpler regression methods. In this paper, we use a machine learning based approach instead, and aim to capture the PEAD dynamics using data from a large group of stocks and a wide range of both fundamental and technical factors. Our model is built around the Extreme Gradient Boosting (XGBoost) and uses a long list of engineered input features based on quarterly financial announcement data from 1,106 companies in the Russell 1000 index between 1997 and 2018. We perform numerous experiments on PEAD predictions and analysis and have the following contributions to the literature. First, we show how Post-Earnings-Announcement Drift can be analysed using machine learning methods and demonstrate such methods’ prowess in credibly forecasting the drift direction. It is the first time PEAD dynamics are studied using XGBoost. We show that the drift direction is driven by different factors for stocks from different industrial sectors and in different quarters and XGBoost is effective in understanding the changing dynamics. Second, we show that an XGBoost well optimised by a Genetic Algorithm can help allocate out-of-sample stocks to form portfolios with higher positive returns to long and portfolios with lower negative returns to short, a finding that could be adopted in the process of developing market neutral strategies. Third, we show how theoretical event-driven stock strategies have to grapple with ever-changing market prices in reality, reducing their effectiveness. We present a tactic to remedy the difficulty of buying into a moving market when trading on PEAD signals.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100333X",
    "keywords": [
      "Acoustics",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Dynamics (music)",
      "Earnings",
      "Economics",
      "Finance",
      "Genetic algorithm",
      "Machine learning",
      "Physics"
    ],
    "authors": [
      {
        "surname": "Ye",
        "given_name": "Zhengxin Joseph"
      },
      {
        "surname": "Schuller",
        "given_name": "Björn W."
      }
    ]
  },
  {
    "title": "Dynamic sine cosine algorithm for large-scale global optimization problems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114950",
    "abstract": "The sine cosine algorithm (SCA) is a recently proposed swarm intelligence optimization based on sine and cosine mathematical functions. It has a novel principle to process global optimization, but when solving large-scale global optimization problems, the performance of this algorithm is greatly reduced. To tackle this problem, a dynamic sine cosine algorithm (DSCA) is proposed. DSCA includes a nonlinear random convergence parameter to update equation, dynamically balancing the exploration and exploitation of SCA. Moreover, in order to avoid fall into the local optimum, a dynamic inertia weight strategy is introduced to modify the position equation of this algorithm. To evaluate the performance in solving large-scale global optimization problems, DSCA is compared with state-of-art algorithms. The 15 standard high-dimensional functions ranging from 200 to 5000 and IEEE CEC2010 functions are selected. The results show that DSCA has better convergence precision, faster convergence speed and stronger robustness when solving large-scale optimization problems. Two practical engineering problems are also applied to this algorithm, the effectiveness of the dynamic sine cosine algorithm to ensure the efficient results in real-world optimization problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003912",
    "keywords": [
      "Algorithm",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Gene",
      "Geometry",
      "Global optimization",
      "Mathematical optimization",
      "Mathematics",
      "Optimization problem",
      "Physics",
      "Quantum mechanics",
      "Robustness (evolution)",
      "Scale (ratio)",
      "Sine",
      "Trigonometric functions"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yu"
      },
      {
        "surname": "Zhao",
        "given_name": "Yiran"
      },
      {
        "surname": "Liu",
        "given_name": "Jingsen"
      }
    ]
  },
  {
    "title": "Gradient-based grey wolf optimizer with Gaussian walk: Application in modelling and prediction of the COVID-19 pandemic",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114920",
    "abstract": "This research proposes a new type of Grey Wolf optimizer named Gradient-based Grey Wolf Optimizer (GGWO). Using gradient information, we accelerated the convergence of the algorithm that enables us to solve well-known complex benchmark functions optimally for the first time in this field. We also used the Gaussian walk and Lévy flight to improve the exploration and exploitation capabilities of the GGWO to avoid trapping in local optima. We apply the suggested method to several benchmark functions to show its efficiency. The outcomes reveal that our algorithm performs superior to most existing algorithms in the literature in most benchmarks. Moreover, we apply our algorithm for predicting the COVID-19 pandemic in the US. Since the prediction of the epidemic is a complicated task due to its stochastic nature, presenting efficient methods to solve the problem is vital. Since the healthcare system has a limited capacity, it is essential to predict the pandemic's future trend to avoid overload. Our results predict that the US will have almost 16 million cases by the end of November. The upcoming peak in the number of infected, ICU admitted cases would be mid-to-end November. In the end, we proposed several managerial insights that will help the policymakers have a clearer vision about the growth of COVID-19 and avoid equipment shortages in healthcare systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003614",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Convergence (economics)",
      "Coronavirus disease 2019 (COVID-19)",
      "Disease",
      "Economic growth",
      "Economic shortage",
      "Economics",
      "Field (mathematics)",
      "Gaussian",
      "Geodesy",
      "Geography",
      "Government (linguistics)",
      "Infectious disease (medical specialty)",
      "Linguistics",
      "Lévy flight",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Medicine",
      "Pandemic",
      "Pathology",
      "Philosophy",
      "Physics",
      "Pure mathematics",
      "Quantum mechanics",
      "Random walk",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Khalilpourazari",
        "given_name": "Soheyl"
      },
      {
        "surname": "Hashemi Doulabi",
        "given_name": "Hossein"
      },
      {
        "surname": "Özyüksel Çiftçioğlu",
        "given_name": "Aybike"
      },
      {
        "surname": "Weber",
        "given_name": "Gerhard-Wilhelm"
      }
    ]
  },
  {
    "title": "A comprehensive survey on deep neural networks for stock market: The need, challenges, and future directions",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114800",
    "abstract": "The stock market has been an attractive field for a large number of organizers and investors to derive useful predictions. Fundamental knowledge of stock market can be utilised with technical indicators to investigate different perspectives of the financial market; also, the influence of various events, financial news, and/or opinions on investors’ decisions and hence, market trends have been observed. Such information can be exploited to make reliable predictions and achieve higher profitability. Computational intelligence has emerged with various deep neural network (DNN) techniques to address complex stock market problems. In this article, we aim to review the significance and need of DNNs in the field of stock price and trend prediction; we discuss the applicability of DNN variations to the temporal stock market data and also extend our survey to include hybrid, as well as metaheuristic, approaches with DNNs. We observe the potential limitations for stock market prediction using various DNNs. To provide an experimental evaluation, we also conduct a series of experiments for stock market prediction using nine deep learning-based models; we analyse the impact of these models on forecasting the stock market data. We also evaluate the performance of individual models with different number of features. We discuss challenges, as well as potential future research directions, and conclude our survey with the experimental study. This survey can be referred for the recent perspectives of DNN-based stock market prediction, primarily covering research spanning over years 2017 - 2020 .",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002414",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Data science",
      "Econometrics",
      "Economics",
      "Engineering",
      "Finance",
      "Horse",
      "Mechanical engineering",
      "Paleontology",
      "Profitability index",
      "Stock (firearms)",
      "Stock market",
      "Stock market prediction"
    ],
    "authors": [
      {
        "surname": "Thakkar",
        "given_name": "Ankit"
      },
      {
        "surname": "Chaudhari",
        "given_name": "Kinjal"
      }
    ]
  },
  {
    "title": "Data-driven early fault diagnostic methodology of permanent magnet synchronous motor",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115000",
    "abstract": "Permanent magnet synchronous motor (PMSM) is one of the common core power components in modern industrial systems. Early fault diagnosis can avoid major accidents and plan maintenance in advance. However, the features of early faults are weak, and the symptoms are not obvious. Meanwhile, the fault signal is often overwhelmed by noise. Accordingly, fault diagnosis for early faults is difficult, and the diagnostic accuracy is generally low. A Bayesian-network-based data-driven early fault diagnostic methodology of PMSM is proposed with vibration and acoustic emission data. The wavelet threshold denoising and minimum entropy deconvolution methods are used to improve the signal-to-noise ratio. The complementary ensemble empirical mode decomposition method is used to extract signal eigenvalues, and Bayesian networks are applied to identify the early, middle, and permanent faults. Experimental data carried out with Tyco ST8N80P100V22E medium PMSM are used to train the fault diagnostic model and validate the proposed fault diagnostic methodology. Result shows that the accuracy for early faults is more than 90% when acoustic emission signal is used, and it is higher than the accuracy with vibration signal. The influence of load on diagnostic accuracy is also investigated, and it indicates that the accuracy with acoustic emission signal is higher than that with vibration signal under different loads.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004413",
    "keywords": [
      "Computer science",
      "Electrical engineering",
      "Engineering",
      "Fault (geology)",
      "Geology",
      "Magnet",
      "Permanent magnet synchronous generator",
      "Permanent magnet synchronous motor",
      "Seismology"
    ],
    "authors": [
      {
        "surname": "Cai",
        "given_name": "Baoping"
      },
      {
        "surname": "Hao",
        "given_name": "Keke"
      },
      {
        "surname": "Wang",
        "given_name": "Zhengda"
      },
      {
        "surname": "Yang",
        "given_name": "Chao"
      },
      {
        "surname": "Kong",
        "given_name": "Xiangdi"
      },
      {
        "surname": "Liu",
        "given_name": "Zengkai"
      },
      {
        "surname": "Ji",
        "given_name": "Renjie"
      },
      {
        "surname": "Liu",
        "given_name": "Yonghong"
      }
    ]
  },
  {
    "title": "Interpretable collaborative data analysis on distributed data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114891",
    "abstract": "This paper proposes an interpretable non-model sharing collaborative data analysis method as a federated learning system, which is an emerging technology for analyzing distributed data. Analyzing distributed data is essential in many applications, such as medicine, finance, and manufacturing, due to privacy and confidentiality concerns. In addition, interpretability of the obtained model plays an important role in the practical applications of federated learning systems. By centralizing intermediate representations, which are individually constructed by each party, the proposed method obtains an interpretable model, achieving collaborative analysis without revealing the individual data and learning models distributed between local parties. Numerical experiments indicate that the proposed method achieves better recognition performance than individual analysis and comparable performance to centralized analysis for both artificial and real-world problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003328",
    "keywords": [
      "Computer science",
      "Data mining",
      "Data science"
    ],
    "authors": [
      {
        "surname": "Imakura",
        "given_name": "Akira"
      },
      {
        "surname": "Inaba",
        "given_name": "Hiroaki"
      },
      {
        "surname": "Okada",
        "given_name": "Yukihiko"
      },
      {
        "surname": "Sakurai",
        "given_name": "Tetsuya"
      }
    ]
  },
  {
    "title": "Application of active learning in DNA microarray data for cancerous gene identification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114914",
    "abstract": "Microarray technology has an important role in evaluating gene expression data with unique patterns into existence. In gene-expression based experiments, the expression level of the gene is constantly monitored in order to classify a tissue sample. In microarray technology, the expressions of the genes are altered with respect to pathogenes. The altered expression values can be identified by analyzing the genes of the tissue/cell that are affected along with the tissues/cells that are unaffected are termed as biomarkers. In the current paper, we have developed an Active Learning (AL) model by using Support Vector Machine (SVM) in association with feature-selection (FS) algorithm; called Symmetrical Uncertainty (SU) for the prediction of cancer. The effectiveness of the proposed AL and SU combination is manifested and the biomarkers or cancerous genes identified by the proposed method on four gene-expression data sets are reported. In addition, the biological significance tests are performed for the cancer biomarkers obtained from the data sets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003559",
    "keywords": [
      "Artificial intelligence",
      "Bioinformatics",
      "Biology",
      "Botany",
      "Computational biology",
      "Computer science",
      "DNA microarray",
      "Data mining",
      "Feature selection",
      "Gene",
      "Gene chip analysis",
      "Gene expression",
      "Genetics",
      "Identification (biology)",
      "Microarray",
      "Microarray analysis techniques",
      "Microarray databases",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Begum",
        "given_name": "Shemim"
      },
      {
        "surname": "Sarkar",
        "given_name": "Ram"
      },
      {
        "surname": "Chakraborty",
        "given_name": "Debasis"
      },
      {
        "surname": "Sen",
        "given_name": "Sagnik"
      },
      {
        "surname": "Maulik",
        "given_name": "Ujjwal"
      }
    ]
  },
  {
    "title": "New robust method for image copyright protection using histogram features and Sine Cosine Algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114978",
    "abstract": "Zero-watermarking methods are widely used for efficient copyright protection of digital images. These methods have the ability to withstand both common image processing attacks and some geometric attacks. However, they cannot effectively resist the complex image attacks such as translation, cropping, combined geometric attacks, UnZign, etc. For this purpose, we propose in this work a novel robust zero-watermarking method that can effectively resist several complex image attacks. The proposed method involves the use of the histogram descriptor to generate a secret sequence of binary values from a user-selected Region of Interest (ROI). In order to check the intellectual property rights, the Sine Cosine Algorithm (SCA) is used for detecting the ROI in the attacked image. Then, the computed histogram of this ROI is binarized with a secret method. Next, the resulting sequence of binary values is compared to the original one generated from the original ROI. If there is a high similarity between these binary sequences, the grayscale image copyrights are validated. The simulation results show that the proposed method is not only resistant to geometric attacks (rotation, scaling) and to common image attacks (JPEG compression, filtering, etc.), but it is also robust against the most complex image attacks (cropping, translation, combined geometric attacks, etc.). Furthermore, the results of the comparisons carried out in terms of robustness against different types of attacks prove the superiority of our method over other similar recent zero-watermarking methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100419X",
    "keywords": [
      "Adaptive histogram equalization",
      "Algorithm",
      "Artificial intelligence",
      "Balanced histogram thresholding",
      "Binary image",
      "Computer science",
      "Computer vision",
      "Discrete cosine transform",
      "Geometry",
      "Histogram",
      "Histogram equalization",
      "Histogram matching",
      "Image (mathematics)",
      "Image histogram",
      "Image processing",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Sine",
      "Trigonometric functions"
    ],
    "authors": [
      {
        "surname": "Daoui",
        "given_name": "Achraf"
      },
      {
        "surname": "Karmouni",
        "given_name": "Hicham"
      },
      {
        "surname": "Sayyouri",
        "given_name": "Mhamed"
      },
      {
        "surname": "Qjidaa",
        "given_name": "Hassan"
      },
      {
        "surname": "Maaroufi",
        "given_name": "Mustapha"
      },
      {
        "surname": "Alami",
        "given_name": "Badreeddine"
      }
    ]
  },
  {
    "title": "MFSR: A novel multi-level fuzzy similarity measure for recommender systems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114969",
    "abstract": "There is nowadays explosive growth and diversity of information due to the development of the internet. Thus, decision making in various fields has faced different challenges. Recommender systems by identifying the interests of users, data filtering and data management, offer personalized services to users. This isbeneficialfor marketing and user satisfaction. Recommender system has always faced challenges such as cold start, sparsity, scalability, accuracy, and quality. Collaborative Filtering (CF) as one of the most successful methods used in recommender systems is based on the similarity between users. We argue that similarity is a fuzzy notion and we get more realistic results in recommender systems by using fuzzy logic. Fuzzy logic deals better with uncertainty and is an effective method to identify ambiguities and uncertainty in measuring the similarity of items and users. In this paper, we present a new multi-level fuzzy similarity measure for recommender systems, called MFSR, which is based on popularity and significance. In order to improve theaccuracy and quality of recommendations, we also propose a hierarchical structure for calculation of the similarity. To evaluate the contribution of this work, we use MAE, F1, recall, and precision. The MAE value based on the proposed similarity measure and the hierarchical structure is equal to 0.423 and outperforms the PIP and NHSM respectively by %4 and %13. Also, using the proposed similarity measure and the hierarchical structures, we obtain F1 value equal to 0.654, whichoutperforms the PIP and NHSM respectively by %17 and %20. We have also observed an improvement in recall and precision using the proposed approach. The results show that the proposed method (MFSR) performs better than similar methods in recent years such as PIP and NHSM.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004103",
    "keywords": [
      "Artificial intelligence",
      "Collaborative filtering",
      "Computer science",
      "Data mining",
      "Database",
      "Fuzzy logic",
      "Image (mathematics)",
      "Information retrieval",
      "Machine learning",
      "Measure (data warehouse)",
      "Recommender system",
      "Scalability",
      "Similarity (geometry)",
      "Similarity measure"
    ],
    "authors": [
      {
        "surname": "Shojaei",
        "given_name": "Mansoore"
      },
      {
        "surname": "Saneifar",
        "given_name": "Hassan"
      }
    ]
  },
  {
    "title": "ExEm: Expert embedding using dominating set theory with deep learning approaches",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114913",
    "abstract": "A collaborative network is a social network that is comprised of experts who cooperate with each other to fulfill a special goal. Analyzing this network yields meaningful information about the expertise of these experts and their subject areas. To perform the analysis, graph embedding techniques have emerged as an effective and promising tool. Graph embedding attempts to represent graph nodes as low-dimensional vectors. In this paper, we propose a graph embedding method, called ExEm, that uses dominating-set theory and deep learning approaches to capture node representations. ExEm finds dominating nodes of the collaborative network and constructs intelligent random walks that comprise of at least two dominating nodes. One dominating node should appear at the beginning of each path sampled to characterize the local neighborhoods. Moreover, the second dominating node reflects the global structure information. To learn the node embeddings, ExEm exploits three embedding methods including Word2vec, fastText and the concatenation of these two. The final result is the low-dimensional vectors of experts, called expert embeddings. The extracted expert embeddings can be applied to many applications. In order to extend these embeddings into the expert recommendation system, we introduce a novel strategy that uses expert vectors to calculate experts’ scores and recommend experts. At the end, we conduct extensive experiments to validate the effectiveness of ExEm through assessing its performance over multi-label classification, link prediction, and recommendation tasks on common datasets and our collected data formed by crawling the vast author Scopus profiles. The experiments show that ExEm outperforms the baselines especially in dense networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003547",
    "keywords": [],
    "authors": [
      {
        "surname": "Nikzad-Khasmakhi",
        "given_name": "Narjes"
      },
      {
        "surname": "Balafar",
        "given_name": "Mohammadali"
      },
      {
        "surname": "Feizi-Derakhshi",
        "given_name": "M. Reza"
      },
      {
        "surname": "Motamed",
        "given_name": "Cina"
      }
    ]
  },
  {
    "title": "Dynamic multi-period sparse portfolio selection model with asymmetric investors’ sentiments",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114945",
    "abstract": "Asymmetric investors’ sentiments on returns and risks play an important role in updating the portfolio strategies in multi-period portfolio selection problems. By introducing the Prospect Theory to measure the asymmetric investors’ sentiments, a dynamic sentiment-adjusted model (DSAM) is proposed to sparse portfolio selection problem over multiple periods, in which the objective is to minimize the risk of the portfolio. As we focus on the sparse portfolio, a l 0 constraint is added to our model. The l 0 constraint represents that we can only purchase at most k securities from N candidate securities, in which k is a small number compared to N. Since the objective function of the sparse portfolio with l 0 constraint is NP-hard, and could not be solved by the Deep Learning algorithms. The stochastic neural networks algorithm with re-parametrisation trick (SNNrP) is introduced to solve the DSAM. The back-testing framework of our paper includes a multi-period portfolio selection model, in which asymmetric investors’ sentiments are modeled to iterate investors’ expected return level each period. In the back-testing framework, we conduct the experiments for different investment periods with different investors’ sentiments. The experimental results for the Nasdaq and CSI 300 data sets show that, on average, compared with the traditional Mean–variance model, the terminal return and risk obtained by the DSAM model outperforms by 9% and 11.75%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003869",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Computer science",
      "Econometrics",
      "Economics",
      "Financial economics",
      "Period (music)",
      "Physics",
      "Portfolio",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Ju"
      },
      {
        "surname": "Yang",
        "given_name": "Yongxin"
      },
      {
        "surname": "Jiang",
        "given_name": "Mingzhu"
      },
      {
        "surname": "Liu",
        "given_name": "Jianguo"
      }
    ]
  },
  {
    "title": "Simulation-based optimisation of the timing of loan recovery across different portfolios",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114878",
    "abstract": "A novel procedure is presented for the objective comparison and evaluation of a bank’s decision rules in optimising the timing of loan recovery. This procedure is based on finding a delinquency threshold at which the financial loss of a loan portfolio (or segment therein) is minimised. Our procedure is an expert system that incorporates the time value of money, costs, and the fundamental trade-off between accumulating arrears versus forsaking future interest revenue. Moreover, the procedure can be used with different delinquency measures (other than payments in arrears), thereby allowing an indirect comparison of these measures. We demonstrate the system across a range of credit risk scenarios and portfolio compositions. The computational results show that threshold optima can exist across all reasonable values of both the payment probability (default risk) and the loss rate (loan collateral). In addition, the procedure reacts positively to portfolios afflicted by either systematic defaults (such as during an economic downturn) or episodic delinquency (i.e., cycles of curing and re-defaulting). In optimising a portfolio’s recovery decision, our procedure can better inform the quantitative aspects of a bank’s collection policy than relying on arbitrary discretion alone.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003195",
    "keywords": [
      "Business",
      "Computer science",
      "Econometrics",
      "Finance",
      "Loan",
      "Mathematics"
    ],
    "authors": [
      {
        "surname": "Botha",
        "given_name": "Arno"
      },
      {
        "surname": "Beyers",
        "given_name": "Conrad"
      },
      {
        "surname": "de Villiers",
        "given_name": "Pieter"
      }
    ]
  },
  {
    "title": "Model-free short-term fluid dynamics estimator with a deep 3D-convolutional neural network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114924",
    "abstract": "Deep learning models are not yet fully applied to fluid dynamics predictions, while they are the state-of-the-art solution in many other areas i.e. video and language processing, finance, robotics . Prediction problems on high-dimensional, complex dynamical systems require deep learning models devised to avoid overfitting while maintaining the required model complexity. In this work we present a deep learning prediction model based on a combination of 3D convolutional layers and a low-dimensional intermediate representation that is specifically designed to forecast the future states of this type of dynamical systems. The model predicts p future velocity-field time-slices (samples) based on k past samples from a training dataset consisting of a synthetic jet in transitional regime. The complexity of this flow is characterized by two topology patterns that are periodically changing, making this flow as a suitable example to test the performance of deep learning models to predict time states in complex flows. Moreover, the wide number of applications of synthetic jets (i.e.: fluid mixing, heat transfer enhancement, flow control), points out this example as a reference for future applications, where modeling synthetic jet flows with a reduced computational effort is needed. This work additionally opens up research opportunities for other areas that also operate with complex and high-dimensional time-series data: future frame video prediction, network traffic forecasting, network intrusion detection . The proposed model is presented in detail. A comprehensive analysis of the results is provided. The results are based on a strict validation strategy to ensure its generalization. The model offers an average symmetric mean absolute error (sMAPE) and a relative root mean square error (RRMSE) of 1.068 and 0.026 respectively (one order of magnitude improvement over low-rank approximation tools), using 10 past samples and predicting 6 future samples of a two-dimensional velocity field on a 70x50 point matrix associated to a synthetic jet dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003651",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Field (mathematics)",
      "Law",
      "Machine learning",
      "Mathematics",
      "Overfitting",
      "Political science",
      "Politics",
      "Pure mathematics",
      "Representation (politics)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Lopez-Martin",
        "given_name": "Manuel"
      },
      {
        "surname": "Le Clainche",
        "given_name": "Soledad"
      },
      {
        "surname": "Carro",
        "given_name": "Belen"
      }
    ]
  },
  {
    "title": "A corrected and improved symbiotic organisms search algorithm for continuous optimization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114981",
    "abstract": "The symbiotic organisms search (SOS) algorithm is investigated and corrected by removing the benefit factors due to their biased search around the original dot. However, the removal of these benefit factors results in performance that is far inferior to the outstanding performance of the basic SOS algorithm. Accordingly, this paper suggests adopting combination schemes for the mutualistic equations in order to prevent premature convergence, and further recommends adopting lower combination rates for parasitic equations. Combination schemes are found to be not applicable to the commensal equation, as this equation is not greedy. Therefore, this paper proposes two types of combination schemes to improve the corrected SOS version in terms of achieving high early convergence speed, attaining convergence precision at a lower cost, arriving at the convergence plateau at either a lower cost or a higher level of precision, handling tests of composition functions well, and achieving competitive performance on CEC2015 test problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100422X",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Computer security",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Finance",
      "Greedy algorithm",
      "Key (lock)",
      "Mathematical optimization",
      "Mathematics",
      "Order (exchange)",
      "Particle swarm optimization",
      "Premature convergence",
      "Rate of convergence"
    ],
    "authors": [
      {
        "surname": "Tsai",
        "given_name": "Hsing-Chih"
      }
    ]
  },
  {
    "title": "Providing music service in Ambient Intelligence: experiments with gym users",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114951",
    "abstract": "Ambient Intelligence (AmI) is an interdisciplinary research area of ICT which has evolved since the 90s, taking great advantage from the advent of the Internet of Things (IoT). AmI creates, by using Artificial Intelligence (AI), an intelligent ecosystem in which computers, sensors, lighting, music, personal devices, and distributed services, work together to improve the user experience through the support of natural and intuitive user interfaces. Nowadays, AmI is used in various contexts, e.g., for building smart homes and smart cities, providing healthcare, and creating an adequate atmosphere in retail and public environments. In this paper, we propose a novel AmI system for gym environments, named Gym Intelligence, able to provide adequate music atmosphere, according to the users’ physical effort during the training. The music is taken from Spotify and is classified according to some music features, as provided by Spotify itself. The system is based on a multi-agent computational intelligence model built on two main components: ( i ) machine learning methods that forecast appropriate values for the Spotify music features, and ( ii ) a multi-objective dynamic genetic algorithm that selects a specific Spotify music track, according to such values. Gym Intelligence is built by sensing the ambient with a minimal, low-cost, and non-intrusive set of sensors, and it has been designed considering the outcome of a preliminary analysis in real gyms, involving real users. We have considered well-known regression methods and we have validated them using a collected data ( i ) about the users’ physical effort, through the sensors, and ( ii ) about the users’ music preferences, through an Android app that the users have used during the training. Among the regression methods considered, the one that provided the best results is the Random Forest, which predicted Spotify music features with a mean absolute error of 0.02 and a root mean squared error of 0.05. We have implemented Gym Intelligence and deployed it in five real gyms. We have evaluated it conducting several experiments. The experiments show how, with the help of Gym Intelligence, the users’ satisfaction about the provided background music, rose from 3.05 to 4.91 (on a scale from 1 to 5, where 5 is the maximum score).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003924",
    "keywords": [
      "Ambient intelligence",
      "Artificial intelligence",
      "Atmosphere (unit)",
      "Cloud computing",
      "Computational intelligence",
      "Computer science",
      "Economics",
      "Economy",
      "Human–computer interaction",
      "Internet of Things",
      "Multimedia",
      "Operating system",
      "Physics",
      "Service (business)",
      "Smart environment",
      "Thermodynamics",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "De Prisco",
        "given_name": "Roberto"
      },
      {
        "surname": "Guarino",
        "given_name": "Alfonso"
      },
      {
        "surname": "Lettieri",
        "given_name": "Nicola"
      },
      {
        "surname": "Malandrino",
        "given_name": "Delfina"
      },
      {
        "surname": "Zaccagnino",
        "given_name": "Rocco"
      }
    ]
  },
  {
    "title": "Robust machine-learning workflow for subsurface geomechanical characterization and comparison against popular empirical correlations",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114942",
    "abstract": "Accurate subsurface geomechanical characterization is critical for fossil and geothermal energy recovery and extraction of earth resources. Compressional and shear travel time logs (DTC and DTS) acquired using sonic logging tools facilitate subsurface geomechanical characterization, such as brittleness, elastic moduli, and rock consolidation. In this study, 8 ‘easy-to-acquire’ conventional well logs were processed using 3 shallow regression-type supervised-learning models, namely ordinary least squares (OLS), multivariate adaptive regression splines (MARS), and artificial neural network (ANN), for depth-wise synthesis of compressional and shear travel times along the length of a well. Among the 6 models, MARS outperforms with R2 of 0.63 and 0.59 when synthesizing the compressional and shear travel times, respectively, in a new, unseen well. ANN models are not as stable as other shallow learning models. The 6 shallow learning models are trained and tested with 8481 data points acquired from a 4240-feet depth interval of a shale reservoir in Well 1, and the trained models are deployed in Well 2 for purposes of blind testing against 2920 data points from 1460-feet depth interval. Apart from the 6 shallow learning models, two widely used empirical models, Han Model (1987) and Castagna model (1985), are implemented and compared with the shallow learning models. Relative error of the MARS model in Well 2 is much smaller than the two empirical models and linear regression model, which indicates that the MARS model performs better than simple statistical and physics-based methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003833",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Astronomy",
      "Bayesian multivariate linear regression",
      "Computer science",
      "Database",
      "Geology",
      "Machine learning",
      "Mars Exploration Program",
      "Multivariate adaptive regression splines",
      "Physics",
      "Regression analysis",
      "Workflow"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Hao"
      },
      {
        "surname": "Misra",
        "given_name": "Siddharth"
      }
    ]
  },
  {
    "title": "Modeling train timetables as images: A cost-sensitive deep learning framework for delay propagation pattern recognition",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114996",
    "abstract": "As a vital component of train operational control, train delay propagation pattern discovery is critically important for both railway controllers and passengers. In this study, we present a carefully designed deep learning model, called FCF-Net, that comprises fully connected neural networks (FCNN) and convolutional neural networks (CNN) for train delay propagation pattern recognition in railway systems. FCF-Net first uses a CNN component that handles train timetables as images to capture interactions of train events and an FCNN component to capture the influence of non-operational features separately; then it uses another FCNN component to combinedly learn the dependencies between operational and non-operational features. In addition, considering the imbalance of train delay data, a cost-sensitive technique that assigns different misclassification costs for different class was used to better deal with the imbalanced data. The main goal of the FCF-Net is to realize efficient and accurate train delay propagation pattern recognition by mining potential knowledge from train operation data. The predictive and computational performance of the model was tested and evaluated on data from two high-speed railway lines with different operational features in China. The results show that FCF-Net, once trained with sufficient data, outperforms conventional deep learning with common loss and other state-of-the-art deep learning models for train delay propagation pattern recognition, indicating its capability in knowledge discovery from train operation data. In addition, the computational results show that FCF-Net exhibits more efficient training process than existing state-of-the-art deep learning models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004371",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Backpropagation",
      "Component (thermodynamics)",
      "Computer science",
      "Convolutional neural network",
      "Data modeling",
      "Database",
      "Deep learning",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Ping"
      },
      {
        "surname": "Li",
        "given_name": "Zhongcan"
      },
      {
        "surname": "Wen",
        "given_name": "Chao"
      },
      {
        "surname": "Lessan",
        "given_name": "Javad"
      },
      {
        "surname": "Corman",
        "given_name": "Francesco"
      },
      {
        "surname": "Fu",
        "given_name": "Liping"
      }
    ]
  },
  {
    "title": "A corrected and improved symbiotic organisms search algorithm for continuous optimization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114981",
    "abstract": "The symbiotic organisms search (SOS) algorithm is investigated and corrected by removing the benefit factors due to their biased search around the original dot. However, the removal of these benefit factors results in performance that is far inferior to the outstanding performance of the basic SOS algorithm. Accordingly, this paper suggests adopting combination schemes for the mutualistic equations in order to prevent premature convergence, and further recommends adopting lower combination rates for parasitic equations. Combination schemes are found to be not applicable to the commensal equation, as this equation is not greedy. Therefore, this paper proposes two types of combination schemes to improve the corrected SOS version in terms of achieving high early convergence speed, attaining convergence precision at a lower cost, arriving at the convergence plateau at either a lower cost or a higher level of precision, handling tests of composition functions well, and achieving competitive performance on CEC2015 test problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100422X",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Computer security",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Finance",
      "Greedy algorithm",
      "Key (lock)",
      "Mathematical optimization",
      "Mathematics",
      "Order (exchange)",
      "Particle swarm optimization",
      "Premature convergence",
      "Rate of convergence"
    ],
    "authors": [
      {
        "surname": "Tsai",
        "given_name": "Hsing-Chih"
      }
    ]
  },
  {
    "title": "Robust machine-learning workflow for subsurface geomechanical characterization and comparison against popular empirical correlations",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114942",
    "abstract": "Accurate subsurface geomechanical characterization is critical for fossil and geothermal energy recovery and extraction of earth resources. Compressional and shear travel time logs (DTC and DTS) acquired using sonic logging tools facilitate subsurface geomechanical characterization, such as brittleness, elastic moduli, and rock consolidation. In this study, 8 ‘easy-to-acquire’ conventional well logs were processed using 3 shallow regression-type supervised-learning models, namely ordinary least squares (OLS), multivariate adaptive regression splines (MARS), and artificial neural network (ANN), for depth-wise synthesis of compressional and shear travel times along the length of a well. Among the 6 models, MARS outperforms with R2 of 0.63 and 0.59 when synthesizing the compressional and shear travel times, respectively, in a new, unseen well. ANN models are not as stable as other shallow learning models. The 6 shallow learning models are trained and tested with 8481 data points acquired from a 4240-feet depth interval of a shale reservoir in Well 1, and the trained models are deployed in Well 2 for purposes of blind testing against 2920 data points from 1460-feet depth interval. Apart from the 6 shallow learning models, two widely used empirical models, Han Model (1987) and Castagna model (1985), are implemented and compared with the shallow learning models. Relative error of the MARS model in Well 2 is much smaller than the two empirical models and linear regression model, which indicates that the MARS model performs better than simple statistical and physics-based methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003833",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Astronomy",
      "Bayesian multivariate linear regression",
      "Computer science",
      "Database",
      "Geology",
      "Machine learning",
      "Mars Exploration Program",
      "Multivariate adaptive regression splines",
      "Physics",
      "Regression analysis",
      "Workflow"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Hao"
      },
      {
        "surname": "Misra",
        "given_name": "Siddharth"
      }
    ]
  },
  {
    "title": "Multi-disease prediction using LSTM recurrent neural networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114905",
    "abstract": "Prediction of future clinical events (e.g., disease diagnoses) is an important machine learning task in healthcare informatics research. In this work, we propose a deep learning approach to perform multi-disease prediction for intelligent clinical decision support. The proposed approach utilizes a long short-term memory network and extends it with two mechanisms (i.e., time-aware and attention-based) to conduct multi-label classification based on patients’ clinical visit records. The former mechanism (time-aware) is used to handle the temporal irregularity across clinical visits whereas the latter mechanism (attention-based) assists in determining the importance of each visit for the prediction task. Using a large clinical record data set (over 5 million records) collected from a hospital in Southeast China, we show that our proposed approach outperforms a variety of traditional and deep learning methods in predicting future disease diagnoses. We further study the impacts of different time interval choices for the time-aware mechanism and compare the performances of existing attention-based mechanisms with the one proposed in our study. Our work has implications for supporting physician diagnoses via the use of intelligent systems and more broadly for improving the quality of healthcare service.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003468",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Machine learning",
      "Recurrent neural network"
    ],
    "authors": [
      {
        "surname": "Men",
        "given_name": "Lu"
      },
      {
        "surname": "Ilk",
        "given_name": "Noyan"
      },
      {
        "surname": "Tang",
        "given_name": "Xinlin"
      },
      {
        "surname": "Liu",
        "given_name": "Yuan"
      }
    ]
  },
  {
    "title": "An overlapping clustering approach for precision, diversity and novelty-aware recommendations",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114917",
    "abstract": "Recommender systems aim to provide users with recommendations of quality. New evaluation metrics such as diversity, have taken an increasing interest in a wide spectrum of applications, including the ecommerce, due to their ability to improve online revenues. High recommendation diversity allows a higher chance to satisfy the users’ needs. However, in a large market of users and products, the scalability of the system is questionable because of the required computing resources. We present a scalable evolutionary clustering algorithm that allows to target two objectives. The proposed solution balances between the recommendation accuracy and coverage by making an overlapped clustering. In our approach, we use a Genetic Algorithm to assign each user to a main cluster from which he gets his recommendations and to secondary clusters as a candidate neighbor. The performance comparison of our algorithm against classic well-known approaches, such as k-NN based Collaborative Filtering, showed a significant improvement.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003584",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Cluster analysis",
      "Collaborative filtering",
      "Computer science",
      "Data mining",
      "Database",
      "Diversity (politics)",
      "Epistemology",
      "Machine learning",
      "Novelty",
      "Philosophy",
      "Quality (philosophy)",
      "Recommender system",
      "Scalability",
      "Sociology",
      "Theology"
    ],
    "authors": [
      {
        "surname": "Berbague",
        "given_name": "Chems Eddine"
      },
      {
        "surname": "Karabadji",
        "given_name": "Nour El-islem"
      },
      {
        "surname": "Seridi",
        "given_name": "Hassina"
      },
      {
        "surname": "Symeonidis",
        "given_name": "Panagiotis"
      },
      {
        "surname": "Manolopoulos",
        "given_name": "Yannis"
      },
      {
        "surname": "Dhifli",
        "given_name": "Wajdi"
      }
    ]
  },
  {
    "title": "The balanced dispatching problem in passengers transport services on demand",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114918",
    "abstract": "In the passengers’ transport services on demand, such as taxi services, a dispatching solution determines the service quality level and the incomes received by both drivers and the transport company. In the last years, related work has been mainly focused on improving the service quality. However, the drivers’ incomes balancing is still a challenge, especially for emerging economies. To address this situation, we introduce the balanced dispatching problem in passengers transport services (BDP-PTS) on demand, which seeks a dispatching solution that aims to minimize the variance of the incomes per unit of working time among the drivers. We propose five easy-to-implement online dispatching algorithms, which rely on dispatching rules obtained from the variance analysis and explore the performance measures for a dispatching solution provided by them. Those algorithms consider the BDP-PTS under an offline scenario, where all the information is revealed beforehand, and the maximum number of solicited transport services is performed. We are focused on a specific set of instances, called complete, which admit at least a feasible solution where all requested transport services are performed. Consequently, we prove the NP-completeness in the strong sense of the BDP-PTS under an offline scenario for these instances, and formulate a mixed integer quadratic programming (MIQP) model to solve it. This complexity computation status implies that no polynomial or pseudo time algorithms exist for solving it, unless P = NP, involving an important quantity of running time and memory resources to model and to resolve it in an empirical computation. Finally, computational experiments are carried out to compare the proposed online dispatching algorithms and the MIQP model on datasets of real complete instances from a Chilean transport company. The obtained results show that the proposed online dispatching algorithm based on the dispatching rule, called SRV, is able to reduce more efficiently the income dispersion among drivers within reduced running times, assigning over a 98% of total solicited transport service and allowing a practical implementation into an automated dispatching system on a basic hardware infrastructure, as it is the case of the transport companies in developing countries.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003596",
    "keywords": [
      "Accounting",
      "Algorithm",
      "Business",
      "Computation",
      "Computer network",
      "Computer science",
      "Economics",
      "Epistemology",
      "Integer programming",
      "Marketing",
      "Mathematical optimization",
      "Mathematics",
      "Operations research",
      "Philosophy",
      "Programming language",
      "Quality (philosophy)",
      "Quality of service",
      "Service (business)",
      "Set (abstract data type)",
      "Variance (accounting)"
    ],
    "authors": [
      {
        "surname": "Moraga-Correa",
        "given_name": "Javier"
      },
      {
        "surname": "Quezada",
        "given_name": "Franco"
      },
      {
        "surname": "Rojo-González",
        "given_name": "Luis"
      },
      {
        "surname": "Vásquez",
        "given_name": "Óscar C."
      }
    ]
  },
  {
    "title": "Improved grey system models for predicting traffic parameters",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114972",
    "abstract": "In transportation applications such as real-time route guidance, ramp metering, congestion pricing and special events traffic management, accurate short-term traffic flow prediction is needed. For this purpose, this paper proposes several novel online Grey system models (GM): GM(1,1 | cos ( ω t ) ), GM(1,1 | sin ( ω t ) , cos ( ω t ) ), and GM(1,1 | e - at , sin ( ω t ) , cos ( ω t ) ). To evaluate the performance of the proposed models, they are compared against a set of benchmark models: GM(1,1) model, Grey Verhulst models with and without Fourier error corrections, linear time series model, and nonlinear time series model. The evaluation is performed using loop detector and probe vehicle data from California, Virginia, and Oregon. Among the benchmark models, the error corrected Grey Verhulst model with Fourier outperformed the GM(1,1) model, linear time series, and non-linear time series models. In turn, the three proposed models, GM(1,1 | cos ( ω t ) ), GM(1,1 | sin ( ω t ) , cos ( ω t ) ), and GM(1,1 | e - at , sin ( ω t ) , cos ( ω t ) ), outperformed the Grey Verhulst model in prediction by at least 65%, 16% and 11%, in terms of Root Mean Squared Error, and by 82%, 58% and 42%, in terms of Mean Absolute Percentage Error, respectively. It is observed that the proposed Grey system models are more adaptive to location (e.g., perform well for all roadway types) and traffic parameters (e.g., speed, travel time, occupancy, and volume), and they do not require as many data points for training (4 observations are found to be sufficient).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004139",
    "keywords": [
      "Algorithm",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Geodesy",
      "Geography",
      "Mathematics",
      "Mean squared error",
      "Omega",
      "Paleontology",
      "Physics",
      "Quantum mechanics",
      "Series (stratigraphy)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Comert",
        "given_name": "Gurcan"
      },
      {
        "surname": "Begashaw",
        "given_name": "Negash"
      },
      {
        "surname": "Huynh",
        "given_name": "Nathan"
      }
    ]
  },
  {
    "title": "Multi-modal generative adversarial networks for traffic event detection in smart cities",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114939",
    "abstract": "Advances in the Internet of Things have enabled the development of many smart city applications and expert systems that help citizens and authorities better understand the dynamics of the cities, and make better planning and utilisation of city resources. Smart cities are composed of complex systems that usually process and analyse big data from the Cyber, Physical, and Social worlds. Traffic event detection is an important and complex task in smart transportation modelling and management. We address this problem using semi-supervised deep learning with data of different modalities, e.g., physical sensor observations and social media data. Unlike most existing studies focusing on data of single modality, the proposed method makes use of data of multiple modalities that appear to complement and reinforce each other. Meanwhile, as the amount of labelled data in big data applications is usually extremely limited, we extend the multi-modal Generative Adversarial Network model to a semi-supervised architecture to characterise traffic events. We evaluate the model with a large, real-world dataset consisting of traffic sensor observations and social media data collected from the San Francisco Bay Area over a period of four months. The evaluation results clearly demonstrate the advantages of the proposed model in extracting and classifying traffic events.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003808",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Big data",
      "Computer science",
      "Computer security",
      "Data mining",
      "Data science",
      "Deep learning",
      "Economics",
      "Event (particle physics)",
      "Generative grammar",
      "Internet of Things",
      "Machine learning",
      "Management",
      "Modalities",
      "Modality (human–computer interaction)",
      "Operating system",
      "Physics",
      "Process (computing)",
      "Quantum mechanics",
      "Smart city",
      "Social media",
      "Social science",
      "Sociology",
      "Task (project management)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Qi"
      },
      {
        "surname": "Wang",
        "given_name": "Wei"
      },
      {
        "surname": "Huang",
        "given_name": "Kaizhu"
      },
      {
        "surname": "De",
        "given_name": "Suparna"
      },
      {
        "surname": "Coenen",
        "given_name": "Frans"
      }
    ]
  },
  {
    "title": "Neural ordinary differential grey model and its applications",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114923",
    "abstract": "Due to the efficiency of grey models in predicting the time series of small samples, grey system theory has been well studied since it was first proposed and has now become an important method for small sample prediction. Inspired by the neural ordinary differential equations (NODE), this paper proposes a novel grey forecasting model called the neural ordinary differential grey model (NODGM). The NODGM model includes a novel whitening equation that allows the prediction model to be learned through a training procedure. Therefore, compared with other models whose structures and terms need to be artificially predefined based on the laws of real samples, the NODGM model has a wider application range and can learn the characteristics of different data samples. Then, to obtain a model with better prediction performance, we apply NODE to train the model. Finally, the predicting sequence is obtained by using the Runge–Kutta method to solve the model. In the experiments, we apply NODGM model to two energy samples and then contrast the experimental results with the results of some classical grey models to validate the effectiveness of NODGM model. The comparison results from predicting China’s annual crude oil consumption and forecasting oilfield production in northern China show that the prediction accuracies achieved by NODGM model are 28% and 8% higher, respectively, than those achieved by the state-of-the-art grey forecasting models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100364X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Composite material",
      "Computer science",
      "Data mining",
      "Differential equation",
      "Genetics",
      "Machine learning",
      "Materials science",
      "Mathematical analysis",
      "Mathematics",
      "Ordinary differential equation",
      "Range (aeronautics)",
      "Sequence (biology)"
    ],
    "authors": [
      {
        "surname": "Lei",
        "given_name": "Dajiang"
      },
      {
        "surname": "Wu",
        "given_name": "Kaili"
      },
      {
        "surname": "Zhang",
        "given_name": "Liping"
      },
      {
        "surname": "Li",
        "given_name": "Weisheng"
      },
      {
        "surname": "Liu",
        "given_name": "Qun"
      }
    ]
  },
  {
    "title": "Dynamic based trajectory estimation and tracking in an uncertain environment",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114919",
    "abstract": "This paper develops smoothing data association based on integrated probabilistic data association (FLIPDA-S) tracker to identify a multicopter UAV (MUAV) and estimate its trajectory in an uncertain and cluttered environment. Recently, a number of unidentified MUAVs have been flown in prohibited airspace and a radar system has been utilized to identify the MUAV and track the trajectory. However, target tracking methods could have difficulties to track the target MUAV through randomly distributed and moving objects. The vehicle state estimation (VSE) algorithm, which utilize FLIPDA-S in this paper, adopts UAV kinematics and dynamics for tracking a UAV in clutter and false targets with significant UAV identification performance. The performance of the method has been validated with and without an initial position of the target MUAV. Both numerical simulation and experiments are demonstrated to verify the effectiveness and accuracy of FLIPDA-S with VSE algorithm to track UAVs in an uncertain and clutter environment.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003602",
    "keywords": [
      "Artificial intelligence",
      "Astronomy",
      "Computer science",
      "Computer vision",
      "Economics",
      "Estimation",
      "Management",
      "Pedagogy",
      "Physics",
      "Psychology",
      "Tracking (education)",
      "Trajectory"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Myunggun"
      },
      {
        "surname": "Ali Memon",
        "given_name": "Sufyan"
      },
      {
        "surname": "Shin",
        "given_name": "Minho"
      },
      {
        "surname": "Son",
        "given_name": "Hungsun"
      }
    ]
  },
  {
    "title": "Trading support system for portfolio construction using wisdom of artificial crowds and evolutionary computation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114943",
    "abstract": "Effective portfolio management requires vast quantities of information and accurate forecasts to make decisions that generate a profitable strategy. In this study, we propose a framework that extracts useful information from Virtual Experts, which are generated using Strongly-Typed Genetic Programming. Specifically, we created a Virtual Expert pool that provides different recommendations on selling or purchasing of a particular stock, and then applied a Wisdom of Artificial Crowds post-processing algorithm to decide which action to take. We call this framework Community of Virtual Expert Investors, and it is evaluated on different metrics of risk. Results show that this approach manages to outperform a Buy and Hold strategy in a long-term scenario, both in return and in Conditional Sharpe Ratio measures. To test the robustness of these results, a bootstrapping test was performed, in which the general findings of the results were maintained.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003845",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Crowds",
      "Economics",
      "Engineering",
      "Finance",
      "Investment strategy",
      "Machine learning",
      "Microeconomics",
      "Operations management",
      "Operations research",
      "Portfolio",
      "Profit (economics)",
      "Purchasing",
      "Sharpe ratio",
      "Trading strategy"
    ],
    "authors": [
      {
        "surname": "Kristjanpoller",
        "given_name": "Werner"
      },
      {
        "surname": "Michell",
        "given_name": "Kevin"
      },
      {
        "surname": "Minutolo",
        "given_name": "Marcel C."
      },
      {
        "surname": "Dheeriya",
        "given_name": "Prakash"
      }
    ]
  },
  {
    "title": "Reliability prediction-based improved dynamic weight particle swarm optimization and back propagation neural network in engineering systems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114952",
    "abstract": "Aiming at the problem of low accuracy of reliability prediction, a back propagation neural network (BPNN) model is developed. In the process of reliability prediction, a dynamic weight particle swarm optimization-based sine map (SDWPSO) method including a novel inertial weight update strategy is developed. This new strategy introduced a linear decreasing parameter in the sine-map, which enables particles to perform a fine search at a very low speed in the later stage of the search and greatly improves the convergence speed of the algorithm. Furthermore, a hybrid model named SDWPSO-BPNN is created to improve the reliability prediction accuracy in engineering systems. The proposed SDWPSO approach is compared with four algorithms using fourteen benchmark functions to verify the effectiveness. The experimental results indicate that SDWPSO has a better search ability than the other algorithms. Then, the hybrid SDWPSO-BPNN is applied to predict the reliability of turbocharger and industrial robot systems, respectively. The obtained results manifest that the SDWPSO-BPNN is more powerful than that of SVM and ANN methods for reliability prediction in engineering.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003936",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Backpropagation",
      "Benchmark (surveying)",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Geodesy",
      "Geography",
      "Operating system",
      "Particle swarm optimization",
      "Physics",
      "Power (physics)",
      "Process (computing)",
      "Quantum mechanics",
      "Reliability (semiconductor)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Bai",
        "given_name": "Bin"
      },
      {
        "surname": "Zhang",
        "given_name": "Junyi"
      },
      {
        "surname": "Wu",
        "given_name": "Xuan"
      },
      {
        "surname": "wei Zhu",
        "given_name": "Guang"
      },
      {
        "surname": "Li",
        "given_name": "Xinye"
      }
    ]
  },
  {
    "title": "Effective link prediction in multiplex networks: A TOPSIS method",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114973",
    "abstract": "This paper investigates the link prediction in multiplex networks. Multiplex networks that represent multiple types of interaction between the same group of individuals are a special case of complex networks. Each type of interaction is modeled as a layer in a multiplex network. Usually, the topological structures between different layers of a multiplex network have a certain extent of correlation. As a result, the accuracy of link prediction in multiplex networks can be enhanced by combining the information of different layers. In this paper, link prediction in multiplex networks is regarded as a multiple-attribute decision-making problem, in which the potential links in the target layer are considered as alternatives, layers are viewed as attributes, and the similarity score of a potential link in each layer is an attribute value. In implementation, the TOPSIS method is employed to rank alternatives, and interlayer relevance is used to weight the attributes. The experimental results show that the proposed method is not sensitive to the parameter and the interlayer relevance measure, and achieves superior prediction accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004140",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bioinformatics",
      "Biology",
      "Combinatorics",
      "Complex network",
      "Composite material",
      "Computer network",
      "Computer science",
      "Data mining",
      "Image (mathematics)",
      "Law",
      "Layer (electronics)",
      "Link (geometry)",
      "Materials science",
      "Mathematics",
      "Measure (data warehouse)",
      "Multiplex",
      "Operations research",
      "Political science",
      "Rank (graph theory)",
      "Relevance (law)",
      "Similarity (geometry)",
      "TOPSIS",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Bai",
        "given_name": "Shenshen"
      },
      {
        "surname": "Zhang",
        "given_name": "Yakun"
      },
      {
        "surname": "Li",
        "given_name": "Longjie"
      },
      {
        "surname": "Shan",
        "given_name": "Na"
      },
      {
        "surname": "Chen",
        "given_name": "Xiaoyun"
      }
    ]
  },
  {
    "title": "Model checking agent-based communities against uncertain group commitments and knowledge",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114792",
    "abstract": "In recent years, the use of Multi-Agent Systems (MASs) to solve complex problems has grown rapidly. Social communicative commitments have been widely employed in such systems as a means of communication allowing heterogeneous agents to cooperate. However, to prevent undesirable outcomes, communicative commitments and their interactions with agents’ knowledge need to be verified. This paper aims at verifying MASs where agents have knowledge and communicate through manipulating uncertain social commitments, especially when the scope of commitments moves beyond the common agent-to-agent scheme. We introduce a model checking method for verifying those systems and capitalize on the interaction between not only individual but also group communicative commitments and knowledge in the presence of uncertainty. System’s properties are expressed using the Probabilistic Computation Tree Logic of Knowledge and Commitment ( PCTL kc + ). In the proposed approach, model checking PCTL kc + is reduced to model checking the probabilistic branching-time logic PCTL. This is achieved by transforming PCTL kc + model to a Markov Decision Process (MDP), and reducing PCTL kc + formulae into PCTL formulae compatible with PRISM, a reference model checking tool for probabilistic temporal systems. Thereafter, we provide the soundness and completeness proofs of the reduction technique, and compute its time complexity. The effectiveness of the proposed approach is evaluated by implementing it on top of PRISM using two concrete applications, namely Online Shopping System from the business domain, and Insurance Claim Processing from the industrial domain. The obtained results of the two case studies underscore the scientific value of our proposed framework and confirm that verifying commitment-based probabilistic epistemic MASs has become attainable by utilizing this approach. The presented work outperforms existing proposals because it considers the problem of modeling and verifying MASs where group social commitments are interacting with participating agents’ knowledge in the presence of uncertainty, which has not been addressed yet in the literature.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002335",
    "keywords": [
      "Artificial intelligence",
      "Completeness (order theory)",
      "Computer science",
      "Markov decision process",
      "Markov process",
      "Mathematical analysis",
      "Mathematics",
      "Model checking",
      "Probabilistic CTL",
      "Probabilistic analysis of algorithms",
      "Probabilistic logic",
      "Programming language",
      "Soundness",
      "Statistics",
      "Temporal logic",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Sultan",
        "given_name": "Khalid"
      },
      {
        "surname": "Bentahar",
        "given_name": "Jamal"
      },
      {
        "surname": "Yahyaoui",
        "given_name": "Hamdi"
      },
      {
        "surname": "Mizouni",
        "given_name": "Rabeb"
      }
    ]
  },
  {
    "title": "Active contour model driven by Self Organizing Maps for image segmentation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114948",
    "abstract": "Supervised active contour models can use information extracted from supervised samples to guide contour evolution. However, their applicability is limited by the accuracy of the probabilistic models they use, especially when processing images with intensity inhomogeneity. In this paper, an unsupervised activity contour model with Self Organizing Maps (SOM) is proposed. The proposed model employs the self-organizing neural network to perform clustering calculation and the clustering center is called local self-organizing clustering center. An adaptive sign function is used to control the direction of curve evolution. To improve the stability of curve evolution, an improved double-well potential function is proposed. The experiment results show that our model can effectively segment images with intensity inhomogeneity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003894",
    "keywords": [
      "Active contour model",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Image segmentation",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Dong",
        "given_name": "Bin"
      },
      {
        "surname": "Weng",
        "given_name": "Guirong"
      },
      {
        "surname": "Jin",
        "given_name": "Ri"
      }
    ]
  },
  {
    "title": "A temporal ensembling based semi-supervised ConvNet for the detection of fake news articles",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.115002",
    "abstract": "Internet-based information circulation has given rise to the proliferation of fake and misleading contents, which has extreme hostile effects on individuals and humanity. Supervised artificial intelligence techniques require a huge amount of annotated data which is a time-consuming, expensive and laborious task as the speed and volume of social media news generation is very high. To counter this situation, we propose an innovative Convolutional Neural Network semi-supervised framework built on the self-ensembling concept to take leverage of the linguistic and stylometric information of annotated news articles, at the same time explore the hidden patterns in unlabelled data as well. Self-ensembling provides consensus predictions of the labels of unannotated data using previous epochs outputs of network-in-training. These accumulated ensemble predictions are supposed to be a better predictor for the unknown labels than the output of most recent training epoch, thus suitable to be used as a proxy for the labels of unannotated data. The uniqueness of the framework is that it ensembles all the outputs of previous training epochs of the neural network to use them as an unsupervised target for comparing them with current output prediction of unlabelled articles. The framework is validated with extensive experiments on three datasets for different proportions of labelled and unlabelled data. It can achieve highest 97.45% fake news classification accuracy using 50% labelled articles on Fake News Data Kaggle dataset. Contemporary baseline methods are placed in juxtaposition with the proposed architecture which demonstrates the robustness of our work compared to the state-of-the-art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421004437",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Gene",
      "Labeled data",
      "Leverage (statistics)",
      "Machine learning",
      "Robustness (evolution)",
      "Sentiment analysis",
      "Social media",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Meel",
        "given_name": "Priyanka"
      },
      {
        "surname": "Vishwakarma",
        "given_name": "Dinesh Kumar"
      }
    ]
  },
  {
    "title": "Joint exploring of risky labeled and unlabeled samples for safe semi-supervised clustering",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114796",
    "abstract": "In the past few years, Safe Semi-Supervised Learning (S3L) has become an emerging research topic. A few studies have been investigated in the S3L field and obtained desired performance. However, these studies mainly focus on classification problems which cause less attention on clustering. Meanwhile, there is no study takes both risky labeled and unlabeled samples into consideration(e.g., mislabeled samples and outliers). Therefore, we propose a novel Safe Semi-Supervised clustering method to safely explore the labeled an unlabeled samples. Firstly, we apply an effective approach to compute Safe Degree (SD) by estimating local density and minimum distance of each labeled and unlabeled sample. If a sample has large local density and small minimum distance, it can be safe, and correspondingly SD should be high. Otherwise, the sample should be risky and SD is low. Then the SD is introduced into a model-based semi-supervised clustering method to reduce the negative influences of risky labeled and unlabeled samples. Additionally, we construct a graph-based regularization term to limit the outputs of risky labeled samples to be those of nearest unlabeled neighbors. In this case, it is expected to further reduce the harm of risky labeled samples. At the same time, an illustration on an artificial dataset is given to explain the usefulness of the defined SD. Finally, the results which conducted on ten UCI datasets show that our algorithm is effective enough to achieve good clustering performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002372",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Cluster analysis",
      "Computer science",
      "Graph",
      "Labeled data",
      "Machine learning",
      "Outlier",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)",
      "Sample (material)",
      "Semi-supervised learning",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Li"
      },
      {
        "surname": "Gan",
        "given_name": "Haitao"
      },
      {
        "surname": "Xia",
        "given_name": "Siyu"
      },
      {
        "surname": "Xu",
        "given_name": "Xiaobin"
      },
      {
        "surname": "Zhou",
        "given_name": "Tao"
      }
    ]
  },
  {
    "title": "Frog-GNN: Multi-perspective aggregation based graph neural network for few-shot text classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114795",
    "abstract": "Few-shot text classification aims to learn a classifier from very few labeled text instances per class. Previous few-shot research works in NLP are mainly based on Prototypical Networks, which encode support set samples of each class to prototype representations and compute distance between query and each class prototype. In the prototype aggregation progress, much useful information of support set and discrepancy between samples from different class are ignored. In contrast, our model focuses on all query-support pairs without information loss. In this paper, we propose a multi-perspective aggregation based graph neural network (Frog-GNN) that observes through eyes (support and query instance) and speaks by mouth (pair) for few-shot text classification. We construct a graph by pre-trained pair representations and aggregate information from neighborhoods by instance-level representations for message-passing. The final relational features of pairs imply intra-class similarity and inter-class dissimilarity after iteratively interactions among instances. In addition, our Frog-GNN with meta-learning strategy can well generalize to unseen class. Experimental results demonstrate that the proposed GNN model outperforms existing few-shot approaches in both few-shot text classification and relation classification on three benchmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002360",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Construct (python library)",
      "Contrast (vision)",
      "ENCODE",
      "Gene",
      "Graph",
      "Information retrieval",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Programming language",
      "Set (abstract data type)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Shiyao"
      },
      {
        "surname": "Xiang",
        "given_name": "Yang"
      }
    ]
  },
  {
    "title": "Change detection in textual classification with unexpected dynamics",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114831",
    "abstract": "Identifying changes in the dynamics of a classification scheme is an important task to solve using textual data streams. Changes in the volume of documents classified into one category could be a sign of a new emerging structure, which therefore gives clues on the need to update the classification scheme. In this paper, we present a method based on forecasting techniques, change detection and time series monitoring in order to raise alerts as soon as a change occurs in the volume of a given category. We build features only based on the textual content that enable us to accurately predict the expected temporal evolution of such category. Then, we use statistical process control to determine if the current volume is too far away from the one we might expect. We test our method on the New York Times Annotated Corpus and on an industrial data set from Electricité de France (EDF) and we observe that it raises alerts at the right time compared to other techniques from the literature.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002724",
    "keywords": [
      "Computer science",
      "Dynamics (music)",
      "Humanities",
      "Law",
      "Library science",
      "Pedagogy",
      "Philosophy",
      "Political science",
      "Politics",
      "Publics",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Christophe",
        "given_name": "Clément"
      },
      {
        "surname": "Velcin",
        "given_name": "Julien"
      },
      {
        "surname": "Cugliari",
        "given_name": "Jairo"
      },
      {
        "surname": "Suignard",
        "given_name": "Philippe"
      },
      {
        "surname": "Boumghar",
        "given_name": "Manel"
      }
    ]
  },
  {
    "title": "Adopting machine learning and condition monitoring P-F curves in determining and prioritizing high-value assets for life extension",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114897",
    "abstract": "Many machine learning algorithms and models have been proposed in the literature for predicting the remaining useful life (RUL) of systems and components that are subject to condition monitoring (CM). However, in cases where data is ubiquitous, identifying the most suitable equipment for life-extension based on CM data and RUL predictions is a rather challenging task. This paper proposes a technique for determining and prioritizing high-value assets for life-extension treatments when they reach the end of their useful life. The technique exploits the use of key concepts in machine learning (such as data mining and k-means clustering) in combination with an important tool from reliability-centered maintenance (RCM) called the potential-failure (P-F) curve. The RCM process identifies essential equipment within a plant which are worth monitoring, and then derives the P-F curves for equipment using CM and operational data. Afterwards, a new index called the potential failure interval factor (PFIF) is calculated for each equipment or unit, serving as a health indicator. Subsequently, the units are grouped in two ways: (i) a regression model in combination with suitably defined PFIF window boundaries, (ii) a k-means clustering algorithm based on equipment with similar data features. The most suitable equipment for life-extension are identified in groups in order to aid in planning, decision-making and deployment of maintenance resources. Finally, the technique is empirically tested on NASA’s Commercial Modular Aero-Propulsion System Simulation datasets and the results are discussed in detail.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003389",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Extension (predicate logic)",
      "Machine learning",
      "Programming language",
      "Value (mathematics)"
    ],
    "authors": [
      {
        "surname": "Ochella",
        "given_name": "Sunday"
      },
      {
        "surname": "Shafiee",
        "given_name": "Mahmood"
      },
      {
        "surname": "Sansom",
        "given_name": "Chris"
      }
    ]
  },
  {
    "title": "PAROT: Translating natural language to SPARQL",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114712",
    "abstract": "This paper provides a dependency based framework for converting natural language to SPARQL. We present a tool known as PAROT (which echos answers from ontologies) which is able to handle user’s queries that contain compound sentences, negation, scalar adjectives and numbered list. PAROT employs a number of dependency based heuristics to convert user’s queries to user’s triples. The user’s triples are then processed by the lexicon into ontology triples. It is these ontology triples that are used to construct SPARQL queries. From the experiments conducted, PAROT provides state of the art results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001536",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Construct (python library)",
      "Dependency (UML)",
      "Epistemology",
      "Heuristics",
      "Information retrieval",
      "Lexicon",
      "Named graph",
      "Natural language",
      "Natural language processing",
      "Negation",
      "Ontology",
      "Operating system",
      "Philosophy",
      "Programming language",
      "RDF",
      "SPARQL",
      "Semantic Web"
    ],
    "authors": [
      {
        "surname": "Ochieng",
        "given_name": "Peter"
      }
    ]
  },
  {
    "title": "A new technique for guided filter based image denoising using modified cuckoo search optimization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114884",
    "abstract": "In this work, a novel and efficient approach for image denoising is proposed. More often, noise affecting the pixels in image is Gaussian in nature and uniformly deters information pixels in image irrespective of their intensity values. This behaviour of noise can also be identified as Additive White Gaussian Noise (AWGN). For restoration of AWGN affected images, the proposed denoising approach is inspired by image adaptive guided image filtering using modified cuckoo search algorithm. The guidance image is itself derived from the noisy image for this purpose. Bilateral filtering smoothed noisy image is sharpened by unsharp masking and then employed as guidance image for the proposed optimal guided filtering approach. Optimal evaluation of parameters like guided filter smoothing parameter (regularization parameter or degree of smoothing (DoS)) and guided filter’s neighbourhood (kernel) size is done appropriately with the help of the modified cuckoo search algorithm. Two-dimensional search space is explored and exploited for deciding the behaviour of guided filtering adaptively as per the input image requirements. This guided image filter has a better behaviour at it acts as an edge preserving smoothing operator. It is considerably effective as its computational complexity is independent of filtering kernel size. A novel attempt is made by incorporating the Markov Random Field based Energy Minimization based objective/fitness function for imparting adaptive image denoising using metaheuristic intelligence. The proposed method is tested in terms of the performance metrics like peak signal to noise ratio, structural similarity index and mean square error. Performance of the proposed approach is compared with the already proposed image denoising techniques. For this comparison, only those methods are considered which were proposed for filtering of Gaussian Noise. Qualitative (visual) as well as quantitative (objective) results underlines the efficacy of the proposed method for filtering of Gaussian Noise.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003250",
    "keywords": [
      "Additive white Gaussian noise",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Cuckoo search",
      "Image denoising",
      "Mathematics",
      "Noise reduction",
      "Non-local means",
      "Particle swarm optimization",
      "Pattern recognition (psychology)",
      "Smoothing",
      "Statistics",
      "White noise"
    ],
    "authors": [
      {
        "surname": "Singh",
        "given_name": "Himanshu"
      },
      {
        "surname": "Kommuri",
        "given_name": "Sethu Venkata Raghavendra"
      },
      {
        "surname": "Kumar",
        "given_name": "Anil"
      },
      {
        "surname": "Bajaj",
        "given_name": "Varun"
      }
    ]
  },
  {
    "title": "Cooperative meta-heuristic algorithms for global optimization problems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114788",
    "abstract": "This paper presents an alternative global optimization meta-heuristics (MHs) approach, inspired by the natural selection theory. The proposed approach depends on the competition among six MHs that allows generating an offspring, which can breed the high characteristics of parents since they are unique and competitive. Therefore, this leads to improve the convergence of the solutions towards an optimal solution and also, to avoid the limitations of other methods that aim to balance between exploitation and exploration. The six algorithms are differential evolution, whale optimization algorithm, grey wolf optimization, symbiotic organisms search algorithm, sine–cosine algorithm, and salp swarm algorithm. According to these algorithms, three variants of the proposed method are developed, in the first variant, one of the six algorithms will be used to update the current individual based on a predefined order and the probability of the fitness function for each individual. Whereas, the second variant updates each individual by permuting the six algorithms, then using the algorithms in the current permutation to update individuals. The third variant is considered as an extension of the second variant, which updates all individuals using only one algorithm from the six algorithms. Three different experiments are carried out using CEC 2014 and CEC 2017 benchmark functions to evaluate the efficiency of the proposed approach. Moreover, the proposed approach is compared with well known MH methods, including the six methods used to build it. Comparison results confirmed the efficiency of the proposed approach compared to other approaches according to different performance measures.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002293",
    "keywords": [
      "Algorithm",
      "Benchmark (surveying)",
      "Computer science",
      "Convergence (economics)",
      "Differential evolution",
      "Economic growth",
      "Economics",
      "Geodesy",
      "Geography",
      "Mathematical optimization",
      "Mathematics"
    ],
    "authors": [
      {
        "surname": "Abd Elaziz",
        "given_name": "Mohamed"
      },
      {
        "surname": "Ewees",
        "given_name": "Ahmed A."
      },
      {
        "surname": "Neggaz",
        "given_name": "Nabil"
      },
      {
        "surname": "Ibrahim",
        "given_name": "Rehab Ali"
      },
      {
        "surname": "Al-qaness",
        "given_name": "Mohammed A.A."
      },
      {
        "surname": "Lu",
        "given_name": "Songfeng"
      }
    ]
  },
  {
    "title": "Meta-scalable discriminate analytics for Big hyperspectral data and applications",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114777",
    "abstract": "Recent technology developments in hyperspectral sensing has made it possible to acquire several hundred spectral bands that cover the electromagnetic spectrum of an observational scene in a single acquisition. The resulting hyperspectral data cube contains a large volume of spatial-spectral information. It has several concrete and special characteristics such as being multi-source, multi-scale, high dimensional and nonlinear. The hyperspectral video with temporal information further increases the data generation velocity and volume which lead to the Big data challenges especially in remote sensing applications. We term this type of Big data as Big hyperspectral data to differentiate it from the Big data generated from internet and multimedia-based sources. This paper presents a novel data computation framework for Big hyperspectral data discriminate analytics. This framework consists of some essential modules like tree-based divide-conquer (Tree-DC) mechanism, hierarchical spatial-spectral domain (HSSD) decomposition, global scalable and locally fast discriminative analytics (GSLF-DA), tree-based divide-conquer-merge (DCM), and temporal hyperspectral data decomposition. The challenge of the framework is to sustain the divide-conquer scalability for implementation on rapidly evolving parallel computing architectures i.e., transforming the divide-conquer mechanism to be meta-scalable. Moreover, the discriminate analytics in conjunction with the proposed mechanism can give the optimal solution in the final merging stage. Experiments are performed to validate the performance of the mechanisms in the framework.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002189",
    "keywords": [
      "Algorithm",
      "Analytics",
      "Artificial intelligence",
      "Big data",
      "Computer science",
      "Data mining",
      "Data science",
      "Database",
      "Divide and conquer algorithms",
      "Hyperspectral imaging",
      "Scalability"
    ],
    "authors": [
      {
        "surname": "Ang",
        "given_name": "Li-Minn"
      },
      {
        "surname": "Seng",
        "given_name": "Kah Phooi"
      }
    ]
  },
  {
    "title": "An agent-based system for modeling users’ acquisition and retention in startup apps",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114861",
    "abstract": "Startup companies boost the quality of everyday life in almost all dimensions, and their products and services are of relevance everywhere. One of the most important goals that startups pursue is to increase the number of their users quickly. Users are of two types, new and returning. The present study presents an agent-based model to simultaneously deal with these two types of users by placing them in a preferential attachment network to interact. In this model, new users can be added at each time step according to word-of-mouth (WOM) and marketing activities. To define the retention probability for an agent, a set of real users in the records with the same properties as the agent are looked for to check what they have done in the same situation. To validate and test the model, the agent-based system is first thoroughly verified and then applied to the real data of a startup in the game app industry. After the experiments on a real scenario, the best decisions are made about the users to focus on or incentivize, and the best combination of acquisition and retention policies is adopted. The results show that user retention on the early days of adoption is better than the acquisition of new users. In this regard, acquisition should be focused on when the retention is in an acceptable state. Furthermore, the highest increase in the number of users occurs when there is a good balance between acquisition and retention.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100302X",
    "keywords": [
      "Business",
      "Computer science",
      "Epistemology",
      "Everyday life",
      "Focus (optics)",
      "Human–computer interaction",
      "Knowledge acquisition",
      "Knowledge management",
      "Law",
      "Marketing",
      "Optics",
      "Philosophy",
      "Physics",
      "Political science",
      "Programming language",
      "Quality (philosophy)",
      "Relevance (law)",
      "Set (abstract data type)",
      "Word of mouth"
    ],
    "authors": [
      {
        "surname": "Sayyed-Alikhani",
        "given_name": "Amir"
      },
      {
        "surname": "Chica",
        "given_name": "Manuel"
      },
      {
        "surname": "Mohammadi",
        "given_name": "Ali"
      }
    ]
  },
  {
    "title": "A novel evolutionary algorithm based on even difference grey model",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114898",
    "abstract": "Evolutionary algorithm is an important subdomain of the meta-heuristics as a kind of the most popular optimization technology. Inspired by the grey prediction theory, this paper proposes a novel evolution algorithm based on the even difference grey model. Unlike population-based meta-heuristics with employing mutation and crossover operators to generate trial populations, the proposed algorithm develops a reproduction operator by using the even difference grey model. The novel reproduction operator regards population series as a time series. It firstly transforms an unorder data selected from the population series to a series data with approximate exponential law by using a grey operator. Based on the series data generated, an exponential model is constructed by using the even difference grey model. Finally, the reproduction operator obtains a trail population according to the prediction results of the exponential model. The effectiveness and superiority of the proposed algorithm are demonstrated on CEC2005 benchmark functions and six real-world engineering design problems. Philosophically speaking, the proposed algorithm towards a brand-new point of view to achieve the optimization process by forecasting the evolutionary direction at the macroscopic level, while the conventional evolutionary algorithms manipulate the chromosomes to realize their optimization at the microscopic level. It could be hoped from the algorithm to open a door for other prediction theories to construct new meta-heuristics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003390",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Crossover",
      "Demography",
      "Evolutionary algorithm",
      "Exponential function",
      "Gene",
      "Geodesy",
      "Geography",
      "Heuristics",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Operator (biology)",
      "Paleontology",
      "Population",
      "Repressor",
      "Series (stratigraphy)",
      "Sociology",
      "Transcription factor"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Zhongbo"
      },
      {
        "surname": "Gao",
        "given_name": "Cong"
      },
      {
        "surname": "Su",
        "given_name": "Qinghua"
      }
    ]
  },
  {
    "title": "SAEA: A security-aware and energy-aware task scheduling strategy by Parallel Squirrel Search Algorithm in cloud environment",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114915",
    "abstract": "The rapid growth of networking technologies resulted in the execution of an extensive data-centric task, which needs the critical quality of service by cloud data centers. The task scheduling problem is difficult to attain an optimal solution, so we use the Squirrel Search Algorithm to approximate the optimal solution. Traditional scheduling algorithms attempt to reduce execution time without taking into account the energetic cost and security issues. In this scheme, a fuzzy-based task scheduling (SAEA) algorithm is developed which closely combines energy cost, makespan, degree of imbalance, and security levels for multi-objective optimization scheduling problems. In addition, SAEA tries to find a high-quality knowledge base that accurately describes the fuzzy system by parallel squirrels search algorithm (PSSA). The automatic design of a fuzzy rule-based system is currently attracting the interest due to the inherently dynamic nature and the typical complex search spaces of cloud. Extensive experiments prove that SAEA algorithm obtains superior performances in energy cost around 45% compared with MGA and has a better result in terms of total execution time, makespan, degree of imbalance, and security value than other similar scheduling algorithms under high load condition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003560",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cloud computing",
      "Computer network",
      "Computer science",
      "Distributed computing",
      "Dynamic priority scheduling",
      "Embedded system",
      "Fair-share scheduling",
      "Fuzzy logic",
      "Job shop scheduling",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Quality of service",
      "Routing (electronic design automation)",
      "Scheduling (production processes)"
    ],
    "authors": [
      {
        "surname": "Mohammad Hasani Zade",
        "given_name": "Behnam"
      },
      {
        "surname": "Mansouri",
        "given_name": "Najme"
      },
      {
        "surname": "Javidi",
        "given_name": "Mohammad Masoud"
      }
    ]
  },
  {
    "title": "Quantum circuit representation of Bayesian networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114768",
    "abstract": "Probabilistic graphical models such as Bayesian networks are widely used to model stochastic systems to perform various types of analysis such as probabilistic prediction, risk analysis, and system health monitoring, which can become computationally expensive in large-scale systems. While demonstrations of true quantum supremacy remain rare, quantum computing applications managing to exploit the advantages of amplitude amplification have shown significant computational benefits when compared against their classical counterparts. We develop a systematic method for designing a quantum circuit to represent a generic discrete Bayesian network with nodes that may have two or more states, where nodes with more than two states are mapped to multiple qubits. The marginal probabilities associated with root nodes (nodes without any parent nodes) are represented using rotation gates, and the conditional probability tables associated with non-root nodes are represented using controlled rotation gates. The controlled rotation gates with more than one control qubit are represented using ancilla qubits. The proposed approach is demonstrated for three examples: a 4-node oil company stock prediction, a 10-node network for liquidity risk assessment, and a 9-node naive Bayes classifier for bankruptcy prediction. The circuits were designed and simulated using Qiskit, a quantum computing platform that enables simulations and also has the capability to run on real quantum hardware. The results were validated against those obtained from classical Bayesian network implementations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002098",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bayesian network",
      "Computer science",
      "Conditional probability",
      "Engineering",
      "Mathematics",
      "Node (physics)",
      "Physics",
      "Probabilistic logic",
      "Quantum",
      "Quantum computer",
      "Quantum gate",
      "Quantum mechanics",
      "Qubit",
      "Statistics",
      "Structural engineering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Borujeni",
        "given_name": "Sima E."
      },
      {
        "surname": "Nannapaneni",
        "given_name": "Saideep"
      },
      {
        "surname": "Nguyen",
        "given_name": "Nam H."
      },
      {
        "surname": "Behrman",
        "given_name": "Elizabeth C."
      },
      {
        "surname": "Steck",
        "given_name": "James E."
      }
    ]
  },
  {
    "title": "MK-Means: Detecting evolutionary communities in dynamic networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114807",
    "abstract": "K-Means algorithm is probably the most famous and popular clustering algorithm in the world. K-Means algorithm has the advantages of simple structure, easy implementation, high efficiency, fast convergence speed, and good results. It has been widely used in many applications, and many extensions of K-Means have been proposed. Basically, most K-Means variants deal with static data. Recently, the dynamic nature of data has received increasing attention from researchers. Therefore, some studies also use K-Means algorithm to deal with clustering problems in evolutionary data. In this article, we aim to improve past variants of K-Means used in evolutionary clustering. There are two ways to improve this problem. First, past research only considered how the previous clustering results affected the current clustering, but we also considered how the future clustering results affect the current clustering. Secondly, past research applied K-Means from one cycle to another in one pass, but we extended it to multiple passes. These two improvements make the proposed algorithm MK-Means provide more consistent, stable and smooth clustering results than previous models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002487",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Constrained clustering",
      "Convergence (economics)",
      "Correlation clustering",
      "Data mining",
      "Data stream clustering",
      "Economic growth",
      "Economics",
      "Evolutionary algorithm",
      "Machine learning"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yi-Cheng"
      },
      {
        "surname": "Chen",
        "given_name": "Yen-Liang"
      },
      {
        "surname": "Lu",
        "given_name": "Jyun-Yun"
      }
    ]
  },
  {
    "title": "A conservative approach for online credit scoring",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114835",
    "abstract": "This research is aimed at the case of credit scoring in risk management and presents a novel machine learning method to be used for the default prediction of high-risk branches or customers. This study uses the Kruskal-Wallis non-parametric statistic to form a conservative credit-scoring model and to study the impact on modeling performance on the benefit of the credit provider. The findings show that the new credit scoring methodology represents a reasonable coefficient of determination and a very low false-negative rate. It is computationally less expensive with high accuracy with around 18% improvement in Recall/Sensitivity. Because of the recent perspective of continued credit/behavior scoring, our study suggests using this credit score for non-traditional data sources for online loan providers to allow them to study and reveal changes in client behavior over time and choose the reliable unbanked customers, based on their application data. This is the first study that develops an online non-parametric credit scoring system, which is able to reselect effective features automatically for continued credit evaluation and weigh them out by their level of contribution with a good diagnostic ability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002761",
    "keywords": [
      "Actuarial science",
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Credit risk",
      "Credit score",
      "Data mining",
      "Finance",
      "Loan",
      "Machine learning",
      "Mathematics",
      "Statistic",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Ashofteh",
        "given_name": "Afshin"
      },
      {
        "surname": "Bravo",
        "given_name": "Jorge M."
      }
    ]
  },
  {
    "title": "A dynamic term discovery strategy for automatic speech recognizers with evolving dictionaries",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114860",
    "abstract": "We present a dynamic term discovery (TD) strategy that is capable of automatically adapting the dictionaries managed by ASR systems to the input speech, in terms of lexicon and language model (LM). The adaptation tries to solve the problem of out-of-vocabulary (OOV) words that are likely to appear in most realistic scenarios and uses external knowledge sources for extending the capabilities of the LMs present in the systems. The handling of the OOV words is made by existing TD strategies that are able to detect and solve OOVs, plus special word selection processes that decide which words are to be added or deleted, so as to update the vocabulary constantly. We also propose a mathematical model for controlling the vocabulary size of the ASR system as well as the word addition and deletion rates that are involved. Then, the update of the overall LM is based on an interpolation scheme with smaller LMs built with external language knowledge that depends on the current speech and the words to be added at each time. We designed a realistic experimental framework for evaluating the strategy, employing ASR systems with moderated vocabulary sizes and a couple of test speech corpora with very distinct features. The results show that the dynamic TD strategy is able to offer a general positive tendency in WER improvement over systems without it, being able indeed to reach a significant difference after few hours of speech processing.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003018",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Computer science",
      "Interpolation (computer graphics)",
      "Language model",
      "Lexicon",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Motion (physics)",
      "Natural language processing",
      "Optics",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Scheme (mathematics)",
      "Selection (genetic algorithm)",
      "Speech recognition",
      "Term (time)",
      "Vocabulary",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Coucheiro-Limeres",
        "given_name": "Alejandro"
      },
      {
        "surname": "Ferreiros-López",
        "given_name": "Javier"
      },
      {
        "surname": "Fernández-Martínez",
        "given_name": "Fernando"
      },
      {
        "surname": "Córdoba",
        "given_name": "Ricardo"
      }
    ]
  },
  {
    "title": "Symbiotic organisms search algorithm using random walk and adaptive Cauchy mutation on the feature selection of sleep staging",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114887",
    "abstract": "Sleep staging can objectively evaluate sleep quality to effectively assist in preventing and diagnosing sleep disorder. Because of the multi-channel and multi-model characteristics of physiological signals, high-dimensional features cannot be avoided when studying sleep staging. High-dimensional features are often mixed with redundant and irrelevant features, which may decrease the accuracy of classifiers and increase the computational cost. Feature selection can remove redundant and irrelevant features but is considered a challenging task in machine learning. Therefore, feature selection can be regarded as a multi-objective optimization problem. In this paper, the proposed symbiotic search algorithm (RCSOS), which is based on random walk and adaptive Cauchy mutation, can improve the optimization performance of the original algorithm. A binary version of RCSOS is proposed according to the twenty transformation functions. Then, the proposed algorithm is applied to feature selection in sleep staging. To validate the performance and generalization of the algorithm, seven groups of data from two different datasets were tested. Compared with the state-of-art algorithms, the proposed binary version of the RCSOS algorithm performs best on feature selection of sleep staging.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003286",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Feature (linguistics)",
      "Feature selection",
      "Generalization",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Miao",
        "given_name": "Fahui"
      },
      {
        "surname": "Yao",
        "given_name": "Li"
      },
      {
        "surname": "Zhao",
        "given_name": "Xiaojie"
      }
    ]
  },
  {
    "title": "Adaptive trading system integrating machine learning and back-testing: Korean bond market case",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114767",
    "abstract": "Although the bond market provides a basis for determining the capital cost of a corporation by forming the fair value of issued bonds, studies regarding the financial market has mainly focused on the stock market. Because the bond market is affected by several variables, it is a good candidate for machine learning applications. Specifically, traders create trading strategies that involves the difference between long- and short-term bond yields to minimize market risks; hence, if this spread can be predicted, it can serve as the data-driven long-term direction of the bond market and generate additional profits. Therefore, a prediction model that predicts the spreads between 10- and 3-year treasury bonds is proposed herein; subsequently, back-testing is applied to verify the performance of the prediction model. Consequently, the AdaBoost outperformed other prediction models. Moreover, when back-testing was applied based on the results of predictive models, we achieved up to 54.2% in return on investment over 6-month. This study establishes a novel adaptive trading system that integrates machine learning and back-testing for the bond market. In the future, this study will be extended using complex data or the reflection of real constraints based on its use as initial research in the bond market.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002086",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Bond",
      "Bond market",
      "Bond market index",
      "Business",
      "Computer science",
      "Econometrics",
      "Economics",
      "Finance",
      "Horse",
      "Machine learning",
      "Paleontology",
      "Stock market",
      "Trading strategy"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Misuk"
      }
    ]
  },
  {
    "title": "COVID-19: Automatic detection from X-ray images by utilizing deep learning methods",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114883",
    "abstract": "In recent months, a novel virus named Coronavirus has emerged to become a pandemic. The virus is spreading not only humans, but it is also affecting animals. First ever case of Coronavirus was registered in city of Wuhan, Hubei province of China on 31st of December in 2019. Coronavirus infected patients display very similar symptoms like pneumonia, and it attacks the respiratory organs of the body, causing difficulty in breathing. The disease is diagnosed using a Real-Time Reverse Transcriptase Polymerase Chain reaction (RT-PCR) kit and requires time in the laboratory to confirm the presence of the virus. Due to insufficient availability of the kits, the suspected patients cannot be treated in time, which in turn increases the chance of spreading the disease. To overcome this solution, radiologists observed the changes appearing in the radiological images such as X-ray and CT scans. Using deep learning algorithms, the suspected patients’ X-ray or Computed Tomography (CT) scan can differentiate between the healthy person and the patient affected by Coronavirus. In this paper, popular deep learning architectures are used to develop a Coronavirus diagnostic systems. The architectures used in this paper are VGG16, DenseNet121, Xception, NASNet, and EfficientNet. Multiclass classification is performed in this paper. The classes considered are COVID-19 positive patients, normal patients, and other class. In other class, chest X-ray images of pneumonia, influenza, and other illnesses related to the chest region are included. The accuracies obtained for VGG16, DenseNet121, Xception, NASNet, and EfficientNet are 79.01%, 89.96%, 88.03%, 85.03% and 93.48% respectively. The need for deep learning with radiologic images is necessary for this critical condition as this will provide a second opinion to the radiologists fast and accurately. These deep learning Coronavirus detection systems can also be useful in the regions where expert physicians and well-equipped clinics are not easily accessible.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003249",
    "keywords": [
      "2019-20 coronavirus outbreak",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Coronavirus disease 2019 (COVID-19)",
      "Deep learning",
      "Disease",
      "Infectious disease (medical specialty)",
      "Medicine",
      "Outbreak",
      "Pathology",
      "Pattern recognition (psychology)",
      "Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2)",
      "Virology"
    ],
    "authors": [
      {
        "surname": "Nigam",
        "given_name": "Bhawna"
      },
      {
        "surname": "Nigam",
        "given_name": "Ayan"
      },
      {
        "surname": "Jain",
        "given_name": "Rahul"
      },
      {
        "surname": "Dodia",
        "given_name": "Shubham"
      },
      {
        "surname": "Arora",
        "given_name": "Nidhi"
      },
      {
        "surname": "Annappa",
        "given_name": "B."
      }
    ]
  },
  {
    "title": "New formulations for the traveling repairman problem with time windows",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114863",
    "abstract": "The Traveling Repairman Problem (TRP) is one of the most important variants of the Traveling Salesman Problem (TSP). The objective function of TRP is to find a Hamiltonian path or tour starting from the origin while minimizing the total latency (waiting or delay time) for all customers. The latency of a customer is defined as the time passed from the beginning of a tour (or path) until a customer’s service is completed. TRP with time windows (TRPTW) is the case where the earliest and latest times for visiting each customer are restricted by prescribed time windows. The literature on TRPTW is scarce. We only found one formulation for TRPTW and one formulation for its variant. In this paper, we propose four new mathematical models for TRPTW with O(n2) binary variables and O(n2) constraints. We computationally analyze the performance of existing and new formulations by solving symmetric and asymmetric benchmark instances with CPLEX 12.5.0.1 and compare the results in terms of CPU times and optimality gap. We observed that our two formulations were extremely faster than existing formulations, and they could optimally solve symmetric instances up to 150 nodes and asymmetric instances up to 131 nodes within seconds.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003043",
    "keywords": [
      "Algorithm",
      "Benchmark (surveying)",
      "Bottleneck traveling salesman problem",
      "Computer science",
      "Geodesy",
      "Geography",
      "Graph",
      "Hamiltonian path",
      "Latency (audio)",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Path (computing)",
      "Telecommunications",
      "Theoretical computer science",
      "Time complexity",
      "Traveling purchaser problem",
      "Travelling salesman problem"
    ],
    "authors": [
      {
        "surname": "Önder Uzun",
        "given_name": "Gözde"
      },
      {
        "surname": "Kara",
        "given_name": "İmdat"
      }
    ]
  },
  {
    "title": "Identifying mortality factors from Machine Learning using Shapley values – a case of COVID19",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114832",
    "abstract": "In this paper we apply a series of Machine Learning models to a recently published unique dataset on the mortality of COVID19 patients. We use a dataset consisting of blood samples of 375 patients admitted to a hospital in the region of Wuhan, China. There are 201 patients who survived hospitalisation and 174 patients who died whilst in hospital. The focus of the paper is not only on seeing which Machine Learning model is able to obtain the absolute highest accuracy but more on the interpretation of what the Machine Learning models provides. We find that age, days in hospital, Lymphocyte and Neutrophils are important and robust predictors when predicting a patients mortality. Furthermore, the algorithms we use allows us to observe the marginal impact of each variable on a case-by-case patient level, which might help practicioneers to easily detect anomalous patterns. This paper analyses the global and local interpretation of the Machine Learning models on patients with COVID19.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002736",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Disease",
      "Infectious disease (medical specialty)",
      "Internal medicine",
      "Interpretation (philosophy)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Mortality rate",
      "Programming language",
      "Variable (mathematics)"
    ],
    "authors": [
      {
        "surname": "Smith",
        "given_name": "Matthew"
      },
      {
        "surname": "Alvarez",
        "given_name": "Francisco"
      }
    ]
  },
  {
    "title": "Big data analytics for default prediction using graph theory",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114840",
    "abstract": "With the unprecedented increase in data all over the world, financial sector such as companies and industries try to remain competitive by transforming themselves into data-driven organizations. By analyzing a huge amount of financial data, companies are able to obtain valuable information to determine their strategic plans such as risk control, crisis management, or growth management. However, as the amount of data increase dramatically, traditional data analytic platforms confront with storing, managing, and analyzing difficulties. Emerging Big Data Analytics (BDA) overcome these problems by providing decentralized and distributed processing. In this study, we propose two new models for default prediction. In the first model, called DPModel-1, statistical (logistic regression), and machine learning methods (decision tree, random forest, gradient boosting) are employed to predict company default. Derived from the first model, we propose DPModel-2 based on graph theory. DPModel-2 also comprises new variables obtained from the trading interactions of companies. In both models, grid search optimization and SHapley Additive exPlanations (SHAP) value are utilized in order to determine the best hyperparameters and make the models interpretable, respectively. By leveraging balance sheet, credit, and invoice datasets, default prediction is realized for about one million companies in Turkey between the years 2010–2018. The default rates of companies range between 3%-6% by year. The experimental results are conducted on a BDA platform. According to the DPModel-1 results, the highest AUC score is ensured by random forest with 0.87. In addition, the results are improved for each technique separately by adjusting new variables with graph theory. According to DPModel-2 results, the best AUC score is achieved by random forest with 0.89.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002815",
    "keywords": [
      "Analytics",
      "Big data",
      "Computer science",
      "Data mining",
      "Gradient boosting",
      "Hyperparameter",
      "Machine learning",
      "Random forest"
    ],
    "authors": [
      {
        "surname": "Yıldırım",
        "given_name": "Mustafa"
      },
      {
        "surname": "Okay",
        "given_name": "Feyza Yıldırım"
      },
      {
        "surname": "Özdemir",
        "given_name": "Suat"
      }
    ]
  },
  {
    "title": "Distributed aggregation-based attributed graph summarization for summary-based approximate attributed graph queries",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114921",
    "abstract": "With the drastically increasing size of graph data with more diversified and complex structures, it becomes more challenging to summarize and query large attributed graph data. In this paper, we propose a holistic approach for distributed aggregation-based attributed graph summarization for large-scale approximate attributed graph queries, which incorporates node attributes and relationships into topological structure for generating semantic understandable graph summary in a bottom-up way. First, we propose a holistic strategy of node aggregation to calculate the topological and attributed error increments of merging node pairs. Second, we propose a three-stage distributed implementation framework, where a novel heuristic measure for efficient parallelization is presented to reduce computation and communication costs across multiple machines. Third, a summary-based approximate graph query approach is introduced to accelerate graph query while maintaining high query accuracy. At last, extensive experiments were made over three real-world and synthetic attributed graphs. The results show that our approach has competitive performance in maintaining low error increment and computational costs in comparison with the state-of-the-art aggregation-based graph summarization approach, and that our summary-based approximate graph query can accelerate graph query while maintaining high query accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003626",
    "keywords": [
      "Automatic summarization",
      "Computer science",
      "Data mining",
      "Graph",
      "Graph database",
      "Information retrieval",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Shang"
      },
      {
        "surname": "Yang",
        "given_name": "Zhipeng"
      },
      {
        "surname": "Chen",
        "given_name": "Xiaona"
      },
      {
        "surname": "Zhao",
        "given_name": "Jingpeng"
      },
      {
        "surname": "Ma",
        "given_name": "Yinglong"
      }
    ]
  },
  {
    "title": "Named entity recognition for extracting concept in ontology building on Indonesian language using end-to-end bidirectional long short term memory",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114856",
    "abstract": "Information Extraction has been widely used to extract information from text. Named Entity Recognition (NER) is one of the primary tasks of Information Extraction to extract entities such as person, location, and organization. Extraction from text collection is essential to obtain information from unstructured text. Moreover, Named Entity Recognition is part of ontology building, which is the main objective of this research. Ontology can be built on the basis of a collection of concepts and relation between concepts. Concepts in ontology usually consist of a group of entities and are obtained using Noun Phrase Extraction or Named Entity Recognition. Our main focus in this research is to extract concepts in Ontology Building automatically using Named Entity Recognition. In this paper, Named Entity Recognition was chosen as our approach due to the lack of results from the previous Noun Phrase Extraction works, which is not all nouns obtained are entities. Our proposed methodology for Named Entity Recognition is applying an end-to-end model using Bidirectional Long Short Term Memory (Bi-LSTM). Bi-LSTM is able to perform a sequence classification task by understanding the context of the input. Named Entity Recognition approaches in the previous study uses Part-of-Speech (POS) Tagging in the preprocessing phase by using other tools or models. This Part-of Speech is also used as a feature to improve the performance of Named Entity Recognition. Our proposed methodology provides an end-to-end system that can be used for both POS Tagging and Named Entity Recognition. By using our proposed end-to-end model, no additional tool is needed for Part-of-Speech Tagging. This the advantage of our model compared to other models. Experiments were conducted on news documents that were labeled with four types of entity classes and 35 types of part-of-speech. The target entities that we have extracted in this study are person, location, organization, and miscellaneous. We evaluated the performance of our model using F1-Score. We have achieved the best F1-Score for Part-of-Speech Tagging of 91.79% and Named Entity Recognition of 83.18%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002979",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Economics",
      "Entity linking",
      "Epistemology",
      "Information extraction",
      "Information retrieval",
      "Knowledge base",
      "Management",
      "Named-entity recognition",
      "Natural language processing",
      "Noun",
      "Noun phrase",
      "Ontology",
      "Paleontology",
      "Philosophy",
      "Phrase",
      "Proper noun",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Santoso",
        "given_name": "Joan"
      },
      {
        "surname": "Setiawan",
        "given_name": "Esther Irawati"
      },
      {
        "surname": "Purwanto",
        "given_name": "Christian Nathaniel"
      },
      {
        "surname": "Yuniarno",
        "given_name": "Eko Mulyanto"
      },
      {
        "surname": "Hariadi",
        "given_name": "Mochamad"
      },
      {
        "surname": "Purnomo",
        "given_name": "Mauridhi Hery"
      }
    ]
  },
  {
    "title": "A likelihood-based preference ranking organization method using dual point operators for multiple criteria decision analysis in Pythagorean fuzzy uncertain contexts",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114881",
    "abstract": "Considering the new uncertainty format of Pythagorean fuzzy (PF) sets, this research aims to launch a point operator-based likelihood measure and establish a PF preference ranking organization method for enrichment evaluations (PROMETHEE) to manipulate multiple criteria decision analysis (MCDA) tasks within PF environments. Different from the previous extensions of PROMETHEE into PF circumstances, this research presents dual PF point operators to delineate an innovative likelihood measure as a means to ascertainment of preference relationships. As contrasted with the current probability distribution approach, this research takes advantage of the conception of scalar functions as well as the lower approximated estimations and upper approximated estimations via the dual operators to construct a creative point operator-based likelihood measure. This new likelihood measure has novelty value and possesses several desirable properties, such as boundedness, complementarity, and weak transitivity; thus, it can better reveal the possibility of the dominance relations between PF information. More useful concepts of a likelihood-based predominance index and predominance-based preference functions are proposed to facilitate intra-criteria and inter-criteria comparisons in the forms of PF performance ratings and PF characteristics, respectively. Furthermore, their beneficial and desirable properties are also investigated. On the grounds of these new concepts and measures, a likelihood-based PROMETHEE methodology is exploited to address MCDA problems in uncertain circumstances involving Pythagorean fuzziness. By simultaneously employing the positive and negative predominating flows, the likelihood-based PF PROMETHEE I yields a partial ranking of available alternatives and highlights any possible incomparability between alternatives. Based on the net predominating flow, the likelihood-based PROMETHEE II renders complete ranking orders of alternatives and preclude any incomparability among the competing alternatives. The reasonableness and effectuality of the initiated methodology are demonstrated with the assistance of a realistic case about evaluating financing policies for working capital management and a comparative analysis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003225",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Fuzzy logic",
      "Gene",
      "Geometry",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Measure (data warehouse)",
      "Multiple-criteria decision analysis",
      "Novelty",
      "Operator (biology)",
      "Philosophy",
      "Point estimation",
      "Preference",
      "Pythagorean theorem",
      "Ranking (information retrieval)",
      "Regret",
      "Repressor",
      "Statistics",
      "Theology",
      "Transcription factor"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Ting-Yu"
      }
    ]
  },
  {
    "title": "Class label altering fuzzy min-max network and its application to histopathology image database",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114880",
    "abstract": "Hyperbox classifier is efficiently implemented using fuzzy min max neural network, where the input patterns present in the training phase place a vital role. In the training phase, a set of hyperboxes are constructed which are used to classify a testing pattern. A better selection of input patterns of the training set will generate an efficient set of hyperboxes. Inappropriate selection of training samples leads to an erroneous set of hyperboxes, which will degrade the accuracy rate. Therefore, instead of using a single training set an extra training set, secondary training set, may be used to reshuffle the hyperboxes generated during the primary training set. In this paper, we have used a secondary training set to update the hyperboxes generated during the primary training phase. When a secondary training set is used, two cases are arised. First, change the class label of some inefficient hyperboxes created during the primary training phase. Second, fix the class label of efficient hyperboxes to the class allotted during the primary training set. By using the above secondary training set mechanism, a novel class label altering fuzzy min max (CLAFMM) network is proposed to alter the class labels depending on the secondary training set. Experimental results prove that the proposed approach provides more accuracy rate than the FMM, Enhanced FMM and K-nearest FMM. Simultaneously, the proposed approach reduces the complexity of the network by reducing the number of hyperboxes generated by the above said state-of-the-art methods. The proposed method is also applied to the breast cancer histopathological images to identify the best magnifying factor for classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003213",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Fuzzy logic",
      "Fuzzy set",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Training set"
    ],
    "authors": [
      {
        "surname": "Santhos Kumar",
        "given_name": "A."
      },
      {
        "surname": "Kumar",
        "given_name": "Anil"
      },
      {
        "surname": "Bajaj",
        "given_name": "Varun"
      },
      {
        "surname": "Singh",
        "given_name": "Girish Kumar"
      }
    ]
  },
  {
    "title": "Context-aware item attraction model for session-based recommendation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114834",
    "abstract": "Session-based recommendation uses existing items in users’ interaction sessions to predict the next items with which users will interact. The existing items in sessions usually have different degrees of relevance with each other, and this item relevance also reflects users’ interests. Moreover, when sessions are represented in different structural forms, there will be different types of relevance between items, an aspect typically neglected by previous work. In this paper, we propose a novel Context-aware Item Attraction Model (CIAM) for session-based recommendation, which is capable of capturing different types of relevance between items in order to obtain users’ general and temporal interests and predict the next items in sessions. First, we convert sessions into local and global undirected graphs to mine the item adjacency relevance within and across sessions in order to better determine users’ general interests. Second, we retain the natural sequence structure of sessions, and model the transition relevance between items in sessions to get users’ temporal interests. Third, we design a context-aware item embedding method to obtain the embedding of each item; this method utilizes superposition and a weighted graph convolutional network to aggregate the context information from both the item’s features and the item’s neighborhood. Finally, based on users’ general and temporal interests, as well as the context-aware embeddings of items, we predict the next items with which users will interact during a session. The proposed model is then extensively evaluated on two real-world datasets. Experimental results show that our model outperforms the state-of-the-art baseline methods. Through the analysis of the experiments, we prove that our model can effectively capture the different types of relevance between items within and across sessions for accurately modeling user interests, therefore improving recommendation performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100275X",
    "keywords": [
      "Artificial intelligence",
      "Attraction",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Linguistics",
      "Machine learning",
      "Paleontology",
      "Philosophy",
      "Session (web analytics)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Feng",
        "given_name": "Chaoqun"
      },
      {
        "surname": "Shi",
        "given_name": "Chongyang"
      },
      {
        "surname": "Liu",
        "given_name": "Chuanming"
      },
      {
        "surname": "Zhang",
        "given_name": "Qi"
      },
      {
        "surname": "Hao",
        "given_name": "Shufeng"
      },
      {
        "surname": "Jiang",
        "given_name": "Xinyu"
      }
    ]
  },
  {
    "title": "Integrated technique of segmentation and classification methods with connected components analysis for road extraction from orthophoto images",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114908",
    "abstract": "Road networks are one of the main urban features. Therefore, road parts extraction from high-resolution remotely sensed imagery and updated road database are beneficial for many GIS applications. However, owing to the presence of various types of obstacles in the images, such as shadows, cars, and trees, with similar transparency and spectral values as road class, achieving accurate road extraction using different classification and segmentation methods is still difficult. This paper proposes an integrated method combining segmentation and classification methods with connected components analysis to extract road class from orthophoto images. The proposed technique is threefold. First, multiresolution segmentation method was applied to segment images. Then, the main classification methods, namely, decision trees (DT), k-nearest neighbors (KNN), and support vector machines (SVM), were implemented based on spectral, geometric, and textural information to classify the obtained results into two classes: road and non-road. Three main accuracy evaluation measures, such as recall, precision, and F1-score, were evaluated to determine the performance of the proposed method, with respective average values of 87.62%, 89.71%, and 88.61%, respectively, for DT; 86.61%, 88.17%, and 87.30%, respectively, for KNN; and 89.83%, 89.52%, and 89.67%, respectively, for SVM. Finally, connected components labelling was used to extract road component parts, and morphological operation was employed to delete non-road parts and noises and improve the performance. These results were also compared with other prior works, which confirmed that the integrated method is an effective road extraction technique.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003493",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Connected component",
      "Connected-component labeling",
      "Image segmentation",
      "Orthophoto",
      "Pattern recognition (psychology)",
      "Scale-space segmentation",
      "Segmentation",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Abdollahi",
        "given_name": "Arnick"
      },
      {
        "surname": "Pradhan",
        "given_name": "Biswajeet"
      }
    ]
  },
  {
    "title": "Towards graph-based class-imbalance learning for hospital readmission",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114791",
    "abstract": "Predicting hospital readmission with effective machine learning techniques has attracted a great attention in recent years. The fundamental challenge of this task stems from characteristics of the data extracted from electronic health records (EHR), which are imbalanced class distributions. This challenge further leads to the failure of most existing models that only provide a partial understanding for the learning problem and result in a biased and inaccurate prediction. To address this challenge, we propose a new graph-based class-imbalance learning method by fully making use of the data from different classes. First, we conduct graph construction for learning the pattern discrimination from between-class and within-class data samples. Then we design an optimization framework to incorporate the constructed graphs to obtain a class-imbalance aware graph embedding and further alleviate performance degeneration. Finally, we design a neural network model as the classifier to conduct imbalanced classification, i.e., hospital readmission prediction. Comprehensive experiments on six real-world readmission datasets show that the proposed method outperforms state-of-the-art approaches in readmission prediction task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002323",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Economic growth",
      "Economics",
      "Embedding",
      "Graph",
      "Graph embedding",
      "Health care",
      "Health records",
      "Machine learning",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Du",
        "given_name": "Guodong"
      },
      {
        "surname": "Zhang",
        "given_name": "Jia"
      },
      {
        "surname": "Ma",
        "given_name": "Fenglong"
      },
      {
        "surname": "Zhao",
        "given_name": "Min"
      },
      {
        "surname": "Lin",
        "given_name": "Yaojin"
      },
      {
        "surname": "Li",
        "given_name": "Shaozi"
      }
    ]
  },
  {
    "title": "Automatic ocular version evaluation in images using random forest",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114847",
    "abstract": "The version exam is a test performed by specialists in ophthalmology to detect restrictions, paralysis, and disproportionate actions on the eye muscles in binocular movements, which is the simultaneous movement of the eyes. This test is commonly used in the detection, follow-up, and surgical planning of patients with strabismus. Strabismus is a condition in which the eyes do not have adequate alignment, generating several issues not only related to vision but also in social relations. This condition affects approximately 4% of the world’s population. In practice, the version exam is a subjective test, and the creation of a method that aims to automate the examination can help obtain a more objective result. Thus, this work presents an innovative computational method to perform the version examination automatically through face images. The proposed method is organized in seven main steps: (1) image acquisition; (2) preprocessing through a mean filter and Color Badger; (3) eye localization, based on skin segmentation and using Histograms of Oriented Gradients and Random Forest; (4) sclera segmentation, using statistical characteristics and the Random Forest classifier; (5) limbus localization, using as aid the segmented sclera; (6) eye corner localization, based on eye shape, analyzing the inner and outer eyes corners; and (7) version exam, using circle markers for each of the nine eye positions exams. The automatic measurement presented by the method was evaluated through the mean of the difference between the results provided by the method and the original versions measured by the specialist. When considering version, the proposed method obtained a mean accuracy and error of, respectively, 85.18% and 0.29 for the Medial Rectus muscle, 100% and 0 for the Lateral Rectus muscle, 86.9% and 0.47 for the Inferior Oblique muscle, 87.5% and 0.16 for the Superior Rectus muscle, 100% and 0 for the Superior Oblique muscle, and 95.23% and 0.28 for the Inferior Rectus muscle.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002888",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Environmental health",
      "Histogram",
      "Image (mathematics)",
      "Medicine",
      "Ophthalmology",
      "Population",
      "Preprocessor",
      "Random forest",
      "Sclera",
      "Segmentation",
      "Strabismus"
    ],
    "authors": [
      {
        "surname": "Pinheiro",
        "given_name": "Jullyana Fialho"
      },
      {
        "surname": "de Almeida",
        "given_name": "João Dallyson Sousa"
      },
      {
        "surname": "Teixeira",
        "given_name": "Jorge Antonio Meireles"
      },
      {
        "surname": "Junior",
        "given_name": "Geraldo Braz"
      },
      {
        "surname": "de Paiva",
        "given_name": "Anselmo Cardoso"
      },
      {
        "surname": "Silva",
        "given_name": "Aristófanes Correa"
      },
      {
        "surname": "Veras",
        "given_name": "Rodrigo de Melo Souza"
      }
    ]
  },
  {
    "title": "On using the modularity of recurrence network communities to detect change-point behaviour",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114837",
    "abstract": "The behaviour of a dynamical system can be examined from time series using the technique of recurrence plots through visualization and quantification methods. One such method is the quadrant scan in which a recurrence plot of a time series is converted into a second time series whose local maxima can be identified and interpreted as possible transitions in dynamic behaviour. A recurrence plot can also be represented as a complex network. A quadrant scan can similarly be thought of as a partition of the network vertices into two communities. Here, we argue that different dynamic behavioural regimes can be realized as network communities and the quality of such partitions can be assessed using modularity. Thereby, community modularity can be used as an alternative to the quadrant scan. We investigate this correspondence with respect to two promising arenas for quadrant scan uptake, namely, concept drift detection from machine learning and tipping point or failure and damage monitoring in system maintenance. We also examine two additional data sets to highlight the potential of the methods. These are a geophysical time series of earth tremor data and a physiological time series of an electrocardiogram.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002785",
    "keywords": [
      "Aesthetics",
      "Art",
      "Art history",
      "Artificial intelligence",
      "Biology",
      "Combinatorics",
      "Complex network",
      "Computer science",
      "Data mining",
      "Electrical engineering",
      "Engineering",
      "Genetics",
      "Geology",
      "Mathematical analysis",
      "Mathematics",
      "Maxima",
      "Maxima and minima",
      "Medicine",
      "Modularity (biology)",
      "Nonlinear system",
      "Paleontology",
      "Partition (number theory)",
      "Pathology",
      "Performance art",
      "Philosophy",
      "Physics",
      "Plot (graphics)",
      "Quadrant (abdomen)",
      "Quantum mechanics",
      "Recurrence plot",
      "Series (stratigraphy)",
      "Statistics",
      "Time point",
      "Tipping point (physics)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Walker",
        "given_name": "David M."
      },
      {
        "surname": "Zaitouny",
        "given_name": "Ayham"
      },
      {
        "surname": "Corrêa",
        "given_name": "Débora C."
      }
    ]
  },
  {
    "title": "A deep reinforcement learning-based method applied for solving multi-agent defense and attack problems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114896",
    "abstract": "Learning to cooperate among agents has always been an important research topic in artificial intelligence. Multi-agent defense and attack, one of the important issues in multi-agent cooperation, requires multiple agents in the environment to learn effective strategies to achieve their goals. Deep reinforcement learning (DRL) algorithms have natural advantages dealing with continuous control problems especially under situations with dynamic interactions, and have provided new solutions for those long-studied multi-agent cooperation problems. In this paper, we start from deep deterministic policy gradient (DDPG) algorithm and then introduce multi-agent DDPG (MADDPG) to solve the multi-agent defense and attack problem under different situations. We reconstruct the considered environment, redefine the continuous state space, continuous action space, reward functions accordingly, and then apply deep reinforcement learning algorithms to obtain effective decision strategies. Several experiments considering different confrontation scenarios are conducted to validate the feasibility and effectiveness of the DRL-based methods. Experimental results show that through learning the agents can make better decisions, and learning with MADDPG achieves superior performance than learning with other DRL-based models, which also explains the importance and necessity of mastering other agents’ information.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003377",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Machine learning",
      "Psychology",
      "Reinforcement",
      "Reinforcement learning",
      "Social psychology"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Liwei"
      },
      {
        "surname": "Fu",
        "given_name": "Mingsheng"
      },
      {
        "surname": "Qu",
        "given_name": "Hong"
      },
      {
        "surname": "Wang",
        "given_name": "Siying"
      },
      {
        "surname": "Hu",
        "given_name": "Shangqian"
      }
    ]
  },
  {
    "title": "IWOSSA: An improved whale optimization salp swarm algorithm for solving optimization problems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114901",
    "abstract": "In this paper, a hybrid improved whale optimization salp swarm algorithm (IWOSSA) is proposed. The main idea behind IWOSSA is to combine improved Whale Optimization Algorithm (IWOA) and Salp Swarm Algorithm (SSA). First, WOA algorithm is improved by applying exponential relationships instead of linear relationships. Then, the algorithm chooses between either IWOA or SSA depending on a specific condition. To validate the efficiency of the proposed algorithm, IWOSSA is applied to 23 different benchmark functions of different dimensions and results are compared with 8 optimization algorithms including WOA and SSA. As an application to an industrial process and to confirm the good performance of the suggested algorithm, they are applied to tune an adaptive PID controller. The PID controller is used in controlling a divided wall column. Different disturbances are applied. From the simulation results, the enhancement made by the IWOSSA is proved by means of the different performance indexes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003420",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Control (management)",
      "Control engineering",
      "Control theory (sociology)",
      "Engineering",
      "Fishery",
      "Geodesy",
      "Geography",
      "Mathematical optimization",
      "Mathematics",
      "Multi-swarm optimization",
      "Operating system",
      "Optimization algorithm",
      "PID controller",
      "Particle swarm optimization",
      "Process (computing)",
      "Swarm behaviour",
      "Swarm intelligence",
      "Temperature control",
      "Whale"
    ],
    "authors": [
      {
        "surname": "Saafan",
        "given_name": "Mahmoud M."
      },
      {
        "surname": "El-Gendy",
        "given_name": "Eman M."
      }
    ]
  },
  {
    "title": "A multibiometric system based on the fusion of fingerprint, finger-vein, and finger-knuckle-print",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114687",
    "abstract": "Authentication systems play an essential role in our lives today. Human biological, behavioral, and morphological characteristics are usually used in an authentication system in a vast scope of applications ranging from unlocking consumer devices to surveillance and forensic analysis and offered an alternative to credit cards, ID cards, passports, driving licenses, etc. Unibiometric systems, which use a single biometric modality, suffer from some drawbacks such as low protection of user’s privacy against attacks. Multibiometric systems, which fuse features of biometric characteristics, can cope with unibiometric systems’ drawbacks and improve security and recognition accuracy. However, some main questions such as ‘what is the optimal number of biometric modalities?’, ‘how much accuracy do we need for our system?’, and ‘how much money do we want to invest for our system?’ have to be considered and answered before designing and implementing a multibiometric system. Unfortunately, the existing multibiometric systems have not considered and responded to all questions. Furthermore, identification mode for multibiometric systems is a challenging task and almost all of them have focused on verification mode. In this paper, we only consider a finger and employ the maximum possible modalities of the finger, i.e., fingerprint, finger-vein, and finger-knuckle-print, to increase user-friendliness and reducing the cost of implementation of the system. The proposed system only uses three cameras to capture all contactless fingerprint, finger-vein, and finger-knuckle images. Also, we propose an algorithm that makes enable the system to work in identification mode. The experiments on established databases exhibit that our proposed algorithm significantly increases recognition accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001287",
    "keywords": [
      "Artificial intelligence",
      "Authentication (law)",
      "Biology",
      "Biometrics",
      "Botany",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Engineering",
      "Finger print",
      "Fingerprint (computing)",
      "Hand geometry",
      "Human–computer interaction",
      "Identification (biology)",
      "Knuckle",
      "Mechanical engineering",
      "Modalities",
      "Modality (human–computer interaction)",
      "Mode (computer interface)",
      "Palm print",
      "Pattern recognition (psychology)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Khodadoust",
        "given_name": "Javad"
      },
      {
        "surname": "Medina-Pérez",
        "given_name": "Miguel Angel"
      },
      {
        "surname": "Monroy",
        "given_name": "Raúl"
      },
      {
        "surname": "Khodadoust",
        "given_name": "Ali Mohammad"
      },
      {
        "surname": "Mirkamali",
        "given_name": "Seyed Saeid"
      }
    ]
  },
  {
    "title": "An efficient hybrid sine-cosine Harris hawks optimization for low and high-dimensional feature selection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114778",
    "abstract": "Feature selection, an optimization problem, becomes an important pre-process tool in data mining, which simultaneously aims at minimizing feature-size and maximizing model generalization. Because of large search space, conventional optimization methods often fail to generate global optimum solution. A variety of hybrid techniques merging different search strategies have been proposed in feature selection literature, but mostly deal with low dimensional datasets. In this paper, a hybrid optimization method is proposed for numerical optimization and feature selection, which integrates sine-cosine algorithm (SCA) in Harris hawks optimization (HHO). The goal of SCA integration is to cater ineffective exploration in HHO, moreover exploitation is enhanced by dynamically adjusting candidate solutions for avoiding solution stagnancy in HHO. The proposed method, namely SCHHO, is evaluated by employing CEC’17 test suite for numerical optimization and sixteen datasets with low and high-dimensions exceeding 15000 attributes, and compared with original SCA and HHO, as well as, other well-known optimization methods like dragonfly algorithm (DA), whale optimization algorithm (WOA), grasshopper optimization algorithm (GOA), Grey wolf optimization (GWO), and salp swarm algorithm (SSA); in addition to state-of-the-art methods. Performance of the proposed method is also validated against hybrid methods proposed in recent related literature. The extensive experimental and statistical analyses suggest that the proposed hybrid variant of HHO is able to produce efficient search results without additional computational cost. With increased convergence speed, SCHHO reduced feature-size up to 87% and achieved accuracy up to 92%. Motivated from the findings of this study, various potential future directions are also highlighted.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002190",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Feature (linguistics)",
      "Feature selection",
      "Linguistics",
      "Mathematical optimization",
      "Mathematics",
      "Optimization problem",
      "Particle swarm optimization",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Hussain",
        "given_name": "Kashif"
      },
      {
        "surname": "Neggaz",
        "given_name": "Nabil"
      },
      {
        "surname": "Zhu",
        "given_name": "William"
      },
      {
        "surname": "Houssein",
        "given_name": "Essam H."
      }
    ]
  },
  {
    "title": "Multi-criteria recommender system based on social relationships and criteria preferences",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114868",
    "abstract": "Multi-criteria recommender systems have garnered considerable interests from researchers and practitioners. In this paper, we study the optimization of the accuracy and scalability of multi-criteria recommendation systems using social relationships and criteria preferences information. We firstly construct a hybrid social recommendation algorithm to investigate the advantages of social relationships, and extend the application scope of the algorithm by an implicit social relationship inference technique. Then the nonlinear aggregate functions are adopted to uncover the relationship between criteria and the overall rating. Besides, we cluster users and train the aggregate function for each user group with a much smaller sample size, which is useful for improving the training efficiency. Finally, we validate the proposed approaches on TripAdvisor multi-criteria rating data sets with different sparsity. The proposed social recommendation model outperforms traditional approaches for both active and cold start users in predicting criteria ratings. Multi-criteria ratings enhance accuracy on the condition that criteria ratings can be accurately predicted. Our results also confirm the benefits from nonlinear aggregate functions and cluster analysis, especially when the data set is extremely sparse.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003092",
    "keywords": [
      "Aggregate (composite)",
      "Artificial intelligence",
      "Collaborative filtering",
      "Composite material",
      "Computer science",
      "Construct (python library)",
      "Data mining",
      "Database",
      "Inference",
      "Machine learning",
      "Materials science",
      "Programming language",
      "Recommender system",
      "Scalability",
      "Scope (computer science)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Kun"
      },
      {
        "surname": "Liu",
        "given_name": "Xinwang"
      },
      {
        "surname": "Wang",
        "given_name": "Weizhong"
      },
      {
        "surname": "Li",
        "given_name": "Jing"
      }
    ]
  },
  {
    "title": "Prediction of protein–protein interactions based on elastic net and deep forest",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114876",
    "abstract": "Prediction of protein–protein interactions (PPIs) helps to grasp molecular roots of disease. However, web-lab experiments to predict PPIs are limited and costly. Using machine-learning-based frameworks can not only automatically identify PPIs, but also provide new ideas for drug research and development from a promising alternative. We present a novel deep-forest-based method for PPIs prediction. Firstly, pseudo amino acid composition (PAAC), autocorrelation descriptor (Auto), multivariate mutual information (MMI), composition-transition-distribution (CTD), amino acid composition position-specific scoring matrix (AAC-PSSM), and dipeptide composition PSSM (DPC-PSSM) are adopted to extract and construct the pattern of PPIs. Secondly, elastic net is utilized to optimize the initial feature vectors and boost the predictive performance. Finally, we ensemble XGBoost, random forest, and extremely randomized trees to construct deep forest model via cascade architecture for PPIs prediction (GcForest-PPI). Benchmark experiments reveal that the proposed approach outperforms other state-of-the-art predictors on Saccharomyces cerevisiae and Helicobacter pylori. We also apply GcForest-PPI on independent test sets, CD9-core network, crossover network, and cancer-specific network. The evaluation shows that GcForest-PPI can boost the prediction accuracy, complement experiments and improve drug discovery.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003171",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Construct (python library)",
      "Data mining",
      "Elastic net regularization",
      "Feature selection",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Partial least squares regression",
      "Pattern recognition (psychology)",
      "Programming language",
      "Random forest"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Bin"
      },
      {
        "surname": "Chen",
        "given_name": "Cheng"
      },
      {
        "surname": "Wang",
        "given_name": "Xiaolin"
      },
      {
        "surname": "Yu",
        "given_name": "Zhaomin"
      },
      {
        "surname": "Ma",
        "given_name": "Anjun"
      },
      {
        "surname": "Liu",
        "given_name": "Bingqiang"
      }
    ]
  },
  {
    "title": "Geographic Named Entity Recognition and Disambiguation in Mexican News using word embeddings",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114855",
    "abstract": "In recent years, dense word embeddings for text representation have been widely used since they can model complex semantic and morphological characteristics of language, such as meaning in specific contexts and applications. Contrary to sparse representations, such as one-hot encoding or frequencies, word embeddings provide computational advantages and improvements on the results in many natural language processing tasks, similar to the automatic extraction of geospatial information. Computer systems capable of discovering geographic information from natural language involve a complex process called geoparsing. In this work, we explore the use of word embeddings for two NLP tasks: Geographic Named Entity Recognition and Geographic Entity Disambiguation, both as an effort to develop the first Mexican Geoparser. Our study shows that relationships between geographic and semantic spaces arise when we apply word embedding models over a corpus of documents in Mexican Spanish. Our models achieved high accuracy for geographic named entity recognition in Spanish.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002967",
    "keywords": [
      "Artificial intelligence",
      "Cartography",
      "Computer science",
      "Economics",
      "Embedding",
      "Entity linking",
      "Geography",
      "Geospatial analysis",
      "Information extraction",
      "Information retrieval",
      "Knowledge base",
      "Law",
      "Linguistics",
      "Management",
      "Named-entity recognition",
      "Natural language",
      "Natural language processing",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Task (project management)",
      "Word (group theory)",
      "Word embedding"
    ],
    "authors": [
      {
        "surname": "Molina-Villegas",
        "given_name": "Alejandro"
      },
      {
        "surname": "Muñiz-Sanchez",
        "given_name": "Victor"
      },
      {
        "surname": "Arreola-Trapala",
        "given_name": "Jean"
      },
      {
        "surname": "Alcántara",
        "given_name": "Filomeno"
      }
    ]
  },
  {
    "title": "Multi-criteria decision making with interval type 2 fuzzy Bonferroni mean",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114789",
    "abstract": "In this paper, the new aggregation models for the interrelated multi-criteria decision making (MCDM) problems based on the quantifier guided ordered weighted averaging (QGOWA) and Bonferroni mean (BM) operator with interval type 2 fuzzy sets (IT2FS) are developed. In most MCDM problems, the decision criteria might not be totally independent. For some MCDM methodologies that are not considering the interrelationship of the criteria, the decision results suggested are meaningless. The BM operator can express the interrelationship of the input arguments, which serves as a mean type aggregator. Yager introduced the OWA operator which is associated with the orness level (attitudinal character) by means of the quantifiers. Different quantifier functions are associated with the respective different orness levels. This is referred to as the QGOWA operator. Besides, the real world MCDM problems are mostly under uncertain environments. To address such MCDM problems, the linguistic criteria weights and the alternative rates are better characterized by IT2FS. The major contributions of this paper are to propose the interrelation MCDM aggregation models with various extensions, to construct the mixed integer linear programming models for obtaining the optimal QGOWA BM IT2FS weights, and to formulate a new interrelation MCDM paradigm. The developed aggregation models are: (1). Ordinary MCDM aggregation; (2). BM with OWA weights; (3). BM with OWA weights and personal importance; (4). BM with QGOWA weights; (5). BM with QGOWA weights and personal importance; (6). BM with QGOWA weights and attitudinal characters; (7). BM with QGOWA weights, attitudinal characters and personal importance. A new MCDM aggregation methodology with application based on the developed models is introduced. The application results from the MCDM aggregation methodology demonstrate that the final decision prioritization are actually affected by the various orness levels predetermined by the decision experts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100230X",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Fuzzy logic",
      "Fuzzy set",
      "Gene",
      "Mathematical optimization",
      "Mathematics",
      "Multiple-criteria decision analysis",
      "Operator (biology)",
      "Quantifier (linguistics)",
      "Repressor",
      "Transcription factor"
    ],
    "authors": [
      {
        "surname": "Chiao",
        "given_name": "Kuo-Ping"
      }
    ]
  },
  {
    "title": "A case study of batch and incremental recommender systems in supermarket data under concept drifts and cold start",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114890",
    "abstract": "Recommender systems uncover relationships between users and items, thus allowing personalized recommendations. Nonetheless, users’ preferences may change over time, the so-called concept drifts; or new users and items may appear, making the recommender system unable to accurately map the relationship between users and items due to the cold start problem. Consequently, concept drift and cold start are challenges that downgrade the recommender system’s predictive performance. This paper assesses existing approaches for collaborative-filtering recommender systems over a real supermarket dataset that exhibits both of the issues mentioned above. For this purpose, our comparative analysis encompasses batch and streaming learning approaches. As a result, we can observe that streaming-based models achieve better recommendation rates since these are tailored to fit the concept drift. More specifically, the predictive performance of streaming-based recommendations increases by up to 21% over those provided by batch methods. The supermarket dataset used in experimentation is also made publicly available for future studies and recommender systems comparisons.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003316",
    "keywords": [
      "Aerospace engineering",
      "Cold start (automotive)",
      "Collaborative filtering",
      "Computer science",
      "Computer security",
      "Concept drift",
      "Data mining",
      "Data stream mining",
      "Downgrade",
      "Engineering",
      "Information retrieval",
      "Recommender system",
      "Streaming data"
    ],
    "authors": [
      {
        "surname": "Viniski",
        "given_name": "Antônio David"
      },
      {
        "surname": "Barddal",
        "given_name": "Jean Paul"
      },
      {
        "surname": "Britto Jr.",
        "given_name": "Alceu de Souza"
      },
      {
        "surname": "Enembreck",
        "given_name": "Fabrício"
      },
      {
        "surname": "Campos",
        "given_name": "Humberto Vinicius Aparecido de"
      }
    ]
  },
  {
    "title": "Silas: A high-performance machine learning foundation for logical reasoning and verification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114806",
    "abstract": "This paper introduces a new high-performance machine learning tool named Silas, which is built to provide a more transparent, dependable and efficient data analytics service. We discuss the machine learning aspects of Silas and demonstrate the advantage of Silas in its predictive and computational performance. We show that several customised algorithms in Silas yield better predictions in a significantly shorter time compared to the state-of-the-art. Another focus of Silas is on providing a formal foundation of decision trees to support logical analysis and verification of learned prediction models. We illustrate the potential capabilities of the fusion of machine learning and logical reasoning by showcasing applications in three directions: formal verification of the prediction model against user specifications, training correct-by-construction models, and explaining the decision-making of predictions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002475",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Machine learning"
    ],
    "authors": [
      {
        "surname": "Bride",
        "given_name": "Hadrien"
      },
      {
        "surname": "Cai",
        "given_name": "Cheng-Hao"
      },
      {
        "surname": "Dong",
        "given_name": "Jie"
      },
      {
        "surname": "Dong",
        "given_name": "Jin Song"
      },
      {
        "surname": "Hóu",
        "given_name": "Zhé"
      },
      {
        "surname": "Mirjalili",
        "given_name": "Seyedali"
      },
      {
        "surname": "Sun",
        "given_name": "Jing"
      }
    ]
  },
  {
    "title": "Domain adaptation based self-correction model for COVID-19 infection segmentation in CT images",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114848",
    "abstract": "The capability of generalization to unseen domains is crucial for deep learning models when considering real-world scenarios. However, current available medical image datasets, such as those for COVID-19 CT images, have large variations of infections and domain shift problems. To address this issue, we propose a prior knowledge driven domain adaptation and a dual-domain enhanced self-correction learning scheme. Based on the novel learning scheme, a domain adaptation based self-correction model (DASC-Net) is proposed for COVID-19 infection segmentation on CT images. DASC-Net consists of a novel attention and feature domain enhanced domain adaptation model (AFD-DA) to solve the domain shifts and a self-correction learning process to refine segmentation results. The innovations in AFD-DA include an image-level activation feature extractor with attention to lung abnormalities and a multi-level discrimination module for hierarchical feature domain alignment. The proposed self-correction learning process adaptively aggregates the learned model and corresponding pseudo labels for the propagation of aligned source and target domain information to alleviate the overfitting to noises caused by pseudo labels. Extensive experiments over three publicly available COVID-19 CT datasets demonstrate that DASC-Net consistently outperforms state-of-the-art segmentation, domain shift, and coronavirus infection segmentation methods. Ablation analysis further shows the effectiveness of the major components in our model. The DASC-Net enriches the theory of domain adaptation and self-correction learning in medical imaging and can be generalized to multi-site COVID-19 infection segmentation on CT images for clinical deployment.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100289X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Feature (linguistics)",
      "Image segmentation",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Jin",
        "given_name": "Qiangguo"
      },
      {
        "surname": "Cui",
        "given_name": "Hui"
      },
      {
        "surname": "Sun",
        "given_name": "Changming"
      },
      {
        "surname": "Meng",
        "given_name": "Zhaopeng"
      },
      {
        "surname": "Wei",
        "given_name": "Leyi"
      },
      {
        "surname": "Su",
        "given_name": "Ran"
      }
    ]
  },
  {
    "title": "The capacitated minimum spanning tree problem with arc time windows",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114859",
    "abstract": "We consider a new variant of the minimum spanning tree problem, where time windows are associated with the arcs of the underlying graph and capacities relate to the maximum number of vertices that subtrees may incorporate. The problem is referred to as the Capacitated Minimum Spanning Tree with Arc Time Windows (CMSTP_ATW) and emerges in routing situations with flow disruptions across road segments. We devise a Mixed Integer Programming (MIP) formulation to model the problem, which can be solved using CPLEX. To examine the quality of the solutions obtained, we convert the data sets of Solomon (1987) to appropriately capture CMSTP_ATW instances and provide results for the problems with 25, 50 and 100 vertices. Furthermore, we compare the CPLEX built-in heuristic that determines the initial integer solution for the CMSPT_ATW, vis-a-vis a greedy heuristic we have developed that offers high quality solutions in short computational times for the large size test problems. Experimental results show that there is a strong negative correlation between the GAP of CPLEX and the total number of iterations in one-hour performance time, against the no or positive correlation between the GAP of CPLEX and the initial iterations. Finally, we modify the MIP by adding parts of the solution derived, using the greedy heuristic, in the set of problem constraints, and observe that the CPLEX results for the CMSTP_ATW are in general improved, offering evidence that it is a promising solution approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003006",
    "keywords": [
      "Algorithm",
      "Arc routing",
      "Combinatorics",
      "Computer network",
      "Computer science",
      "Greedy algorithm",
      "Heuristic",
      "Integer (computer science)",
      "Integer programming",
      "Kruskal's algorithm",
      "Mathematical optimization",
      "Mathematics",
      "Minimum spanning tree",
      "Programming language",
      "Routing (electronic design automation)",
      "Spanning tree",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Kritikos",
        "given_name": "Manolis N."
      },
      {
        "surname": "Ioannou",
        "given_name": "George"
      }
    ]
  },
  {
    "title": "Query expansion based on term distribution and DBpedia features",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114909",
    "abstract": "Query Expansion (QE) approaches that involve the reformulation of queries by adding new terms to the initial user query, are intended to ameliorate the vocabulary mismatch between the query keywords and the documents’ in Information Retrieval Systems (IRS). One big issue in QE is the selection of the right candidate terms for expansion. For this purpose Linked Data can be used, as a valuable resource, for providing additional expansion features such as the values of sub- and super classes of resources. The underlying research question is whether interlinked data and vocabulary items provide features which can be taken into account for query expansion. In this paper, we introduced a new QE approach that aimed at improving IRS by using the well-known distribution based method Bose-Einstein statistics (Bo1) as well as Linked Data from the knowledge base DBpedia using different numbers of expansion terms. We evaluated the effectiveness of each method individually as well as their combinations using two Text REtrieval Conference (TREC) test collections. Our approach has lead to significant improvement in terms of precision, recall, Mean Average Precision (MAP) at rank 10, and normalized Discounted Cumulative Gain (nDCG) at different ranks compared to Pseudo Relevance Feedback (PRF) that we used as a baseline. The results show that the inclusion of semantic annotations clearly improves the retrieval performance over the baseline method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100350X",
    "keywords": [
      "Artificial intelligence",
      "Baseline (sea)",
      "Combinatorics",
      "Computer science",
      "Geology",
      "Information retrieval",
      "Law",
      "Linguistics",
      "Mathematics",
      "Oceanography",
      "Philosophy",
      "Physics",
      "Political science",
      "Precision and recall",
      "Quantum mechanics",
      "Query expansion",
      "Rank (graph theory)",
      "Relevance (law)",
      "Selection (genetic algorithm)",
      "Term (time)",
      "Vocabulary"
    ],
    "authors": [
      {
        "surname": "Dahir",
        "given_name": "Sarah"
      },
      {
        "surname": "El Qadi",
        "given_name": "Abderrahim"
      },
      {
        "surname": "Bennis",
        "given_name": "Hamid"
      }
    ]
  },
  {
    "title": "A Camera to LiDAR calibration approach through the optimization of atomic transformations",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114894",
    "abstract": "This paper proposes a camera-to-3D Light Detection And Ranging calibration framework through the optimization of atomic transformations. The system is able to simultaneously calibrate multiple cameras with Light Detection And Ranging sensors, solving the problem of Bundle. In comparison with the state-of-the-art, this work presents several novelties: the ability to simultaneously calibrate multiple cameras and LiDARs; the support for multiple sensor modalities; the calibration through the optimization of atomic transformations, without changing the topology of the input transformation tree; and the integration of the calibration framework within the Robot Operating System (ROS) framework. The software pipeline allows the user to interactively position the sensors for providing an initial estimate, to label and collect data, and visualize the calibration procedure. To test this framework, an agricultural robot with a stereo camera and a 3D Light Detection And Ranging sensor was used. Pairwise calibrations and a single calibration of the three sensors were tested and evaluated. Results show that the proposed approach produces accurate calibrations when compared to the state-of-the-art, and is robust to harsh conditions such as inaccurate initial guesses or small amount of data used in calibration. Experiments have shown that our optimization process can handle an angular error of approximately 20 degrees and a translation error of 0.5 meters, for each sensor. Moreover, the proposed approach is able to achieve state-of-the-art results even when calibrating the entire system simultaneously.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003353",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Bundle adjustment",
      "Calibration",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Gene",
      "Geology",
      "Lidar",
      "Mathematics",
      "Photogrammetry",
      "Pipeline (software)",
      "Point cloud",
      "Programming language",
      "Ranging",
      "Remote sensing",
      "Robot",
      "Statistics",
      "Telecommunications",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Pinto de Aguiar",
        "given_name": "André Silva"
      },
      {
        "surname": "Riem de Oliveira",
        "given_name": "Miguel Armando"
      },
      {
        "surname": "Pedrosa",
        "given_name": "Eurico Farinha"
      },
      {
        "surname": "Neves dos Santos",
        "given_name": "Filipe Baptista"
      }
    ]
  },
  {
    "title": "Optimizing electric vehicle routing problems with mixed backhauls and recharging strategies in multi-dimensional representation network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114804",
    "abstract": "Electric vehicles are environmental transportation modes that are widely applied in green logistics systems. To guarantee the energy efficiency, the impacts of customer service modes and recharging strategies need to be integrated into the optimization of electric logistics resource. This paper proposes an electric vehicle routing problem with mixed backhauls, time windows, and recharging strategies (EVRPMBTW-RS), minimizing the total travel cost with sophisticated constraints on the time-dependent pickup and delivery requests, limited recharging station capacity, and battery remaining capacity of electric vehicles. Mixed service sequences of linehaul and backhaul customers is allocated for the routing planning, with the synchronous optimization of recharging strategies including the selection of recharging stations and determination of recharging time. A time-discretized multi-commodity network flow model is constructed based on an extended space–time-state modeling framework, which is formulated as a quadratic 0–1 programming model by using the augmented Lagrangian relaxation technique. After the dualization and linearized transformation, we decompose the model into a sequence of least-cost path subproblems based on the alternating direction multiplier method (ADMM). The subproblems are alternately minimized and solved using the time-dependent forward dynamic programming algorithm. The solution quality can be guaranteed through calculating the optimality gap between the best lower bound and upper bound for each iteration. The proposed solution approach is examined on examples of a simple 7-node network and real-world Yizhuang road network. This paper provides a theoretical foundation for the route optimization method of electric logistics vehicles, and contributes to improve the operational efficiency of electric logistics systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002451",
    "keywords": [
      "Benchmark (surveying)",
      "Computer network",
      "Computer science",
      "Electric vehicle",
      "Flow network",
      "Geodesy",
      "Geography",
      "Lagrangian relaxation",
      "Mathematical optimization",
      "Mathematics",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Routing (electronic design automation)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Senyan"
      },
      {
        "surname": "Ning",
        "given_name": "Lianju"
      },
      {
        "surname": "Tong",
        "given_name": "Lu Carol"
      },
      {
        "surname": "Shang",
        "given_name": "Pan"
      }
    ]
  },
  {
    "title": "Non-destructive internal disorder detection of Conference pears by semantic segmentation of X-ray CT scans using deep learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114925",
    "abstract": "Long term storage is required to deliver high quality pear fruit year-round. Under suboptimal storage conditions, internal disorders, such as internal browning and cavity formation, can develop and are often invisible from the outside. We present a non-destructive inspection method to quantify internal disorders in X-ray CT scans of pear fruit using a deep neural network for semantic segmentation. Herein, a U-net based model was trained to automatically indicate healthy tissue, core and regions affected by internal disorders, i.e., cavity formation and internal browning. The quantitative data resulting from the segmentations was used to measure the severity of internal disorders. Excellent classification accuracies of 99.4 and 92.2% were obtained for the classification of “consumable” vs “non-consumable” fruit on the one hand and “healthy” vs “defect but consumable” vs “non-consumable” fruit on the other hand. The identification of “defect but consumable” fruit showed to be the most difficult.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003663",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Browning",
      "Computer science",
      "Deep learning",
      "Food science",
      "PEAR",
      "Pattern recognition (psychology)",
      "Segmentation",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Van De Looverbosch",
        "given_name": "Tim"
      },
      {
        "surname": "Raeymaekers",
        "given_name": "Ellen"
      },
      {
        "surname": "Verboven",
        "given_name": "Pieter"
      },
      {
        "surname": "Sijbers",
        "given_name": "Jan"
      },
      {
        "surname": "Nicolaï",
        "given_name": "Bart"
      }
    ]
  },
  {
    "title": "Measuring dynamic inefficiency in the presence of corporate social responsibility and input indivisibilities",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114849",
    "abstract": "This article proposes a model for evaluating inefficiency accounting for firms’ corporate social responsibility engagement as part of their broader output production activities. The model combines the production of marketable outputs, socially responsible outputs, and undesirable outputs into overall measures of firm performance using data envelopment approaches. Methodologically, the article builds on the dynamic by-production model, which accounts for adjustment costs related with investments, allowing for non-convexities of the production set and input indivisibility, as well as firm corporate social responsibility activities. This study compares dynamic technical inefficiency scores for each input, output, and investment, estimated assuming the presence of input indivisibility (non-convexity) and its absence (convexity). The empirical application focuses on European firms in three industries (offering capital, consumption, and other goods) for the period 2010–2017. The results show significant differences between inefficiencies with and without the convexity assumption and find evidence for non-convexity of firms’ production set and inputs’ indivisibility. Overall, among all outputs, the results reveal the highest inefficiency in the production of socially responsible outputs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002906",
    "keywords": [
      "Archaeology",
      "Biology",
      "Capital (architecture)",
      "Computer science",
      "Consumption (sociology)",
      "Convexity",
      "Corporate social responsibility",
      "Data envelopment analysis",
      "Ecology",
      "Econometrics",
      "Economics",
      "Financial economics",
      "History",
      "Industrial organization",
      "Inefficiency",
      "Investment (military)",
      "Law",
      "Mathematical optimization",
      "Mathematics",
      "Microeconomics",
      "Political science",
      "Politics",
      "Production (economics)",
      "Programming language",
      "Set (abstract data type)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Kapelko",
        "given_name": "Magdalena"
      },
      {
        "surname": "Oude Lansink",
        "given_name": "Alfons"
      },
      {
        "surname": "Stefanou",
        "given_name": "Spiro E."
      }
    ]
  },
  {
    "title": "An approach to generate the bug report summaries using two-level feature extraction",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114816",
    "abstract": "Bug report is one of the major software artifact which is generated during the software development process. Changing requirements in the software development process leads to the continuous evolution of bugs which give challenges to the project management task. Bug Reports are the most consulted artifact by the software community. A Bug Report not only contains the information about the bug but also includes information like the resolution process, the enhancements by other persons, and the suggestions from the users if there are any. During the software evolution and maintenance phase, a developer spends a lot of effort and time searching for the appropriate bug report for resolving the bug quickly. Automatic Bug Report Summarization is one approach to solve the issue of time and effort. Bug report summarization helps developers not only find the appropriate bug report quickly but also assists in managing many tasks related to Bug Report Maintenance. In this paper, we have developed a two-level approach to generate the Bug Report summaries where the title, the description and the comments of a resolved bug report are considered for the summary. We find the entities in the title to create a template-based sentence for describing what the bug report is about. We use the PageRank algorithm along with the cosine similarity measure to find the summary of the Description field of bug report. The two level feature-based approach is used to find the relevant comments and the sentences from the comments. We have used the BRC dataset which has been used by most of the research community in this field. Finally the summaries from title, description and comments are merged together to create a final summary. The approach uses the features which have been successfully used by other researchers for the text summarization and especially to the domain of the meeting conversation like data. Empirical results shows that our approach works equally well with the other supervised and unsupervised approaches in terms of ROUGE Scores.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002578",
    "keywords": [
      "Artifact (error)",
      "Artificial intelligence",
      "Automatic summarization",
      "Cluster analysis",
      "Computer science",
      "Cosine similarity",
      "Data mining",
      "Economics",
      "Feature (linguistics)",
      "Field (mathematics)",
      "Information retrieval",
      "Linguistics",
      "Management",
      "Mathematics",
      "Philosophy",
      "Process (computing)",
      "Programming language",
      "Pure mathematics",
      "Software",
      "Software bug",
      "Software development",
      "Software engineering",
      "Software maintenance",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Gupta",
        "given_name": "Som"
      },
      {
        "surname": "Gupta",
        "given_name": "Sanjai Kumar"
      }
    ]
  },
  {
    "title": "RweetMiner: Automatic identification and categorization of help requests on twitter during disasters",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114787",
    "abstract": "Catastrophic events create uncertain situations for humanitarian organizations locating and providing aid to affected people. Many people turn to social media during disasters for requesting help and/or providing relief to others. However, the majority of social media posts seeking help could not properly be detected and remained concealed because often they are noisy and ill-formed. Existing systems lack in planning an effective strategy for tweet preprocessing and grasping the contexts of tweets. This research, first of all, formally defines request tweets in the context of social networking sites, hereafter rweets, along with their different primary types and sub-types. Our main contributions are the identification and categorization of rweets. For rweet identification, we employ two approaches, namely a rule-based and logistic regression, and show their high precision and F1 scores. The rweets classification into sub-types such as medical, food, shelter, using logistic regression shows promising results and outperforms exiting works. Finally, we introduce an architecture to store intermediate data to accelerate the development process of the machine learning classifiers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002281",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Categorization",
      "Computer science",
      "Computer security",
      "Data mining",
      "Data science",
      "Identification (biology)"
    ],
    "authors": [
      {
        "surname": "Ullah",
        "given_name": "Irfan"
      },
      {
        "surname": "Khan",
        "given_name": "Sharifullah"
      },
      {
        "surname": "Imran",
        "given_name": "Muhammad"
      },
      {
        "surname": "Lee",
        "given_name": "Young-Koo"
      }
    ]
  },
  {
    "title": "Stacked authorship attribution of digital texts",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114866",
    "abstract": "In computational authorship attribution (AA) – the task of identifying the author of a given text based on a set of possible candidates – existing differences across domains, languages or input settings may require using knowledge from multiple sources, ranging from surface character patterns to deeper semantics and others. Moreover, since increasing the model complexity may easily lead to overfitting, sources of this kind have to be selected judiciously according to each particular input. Based on these observations, this article introduces a novel approach to AA consisting of stacked classifiers built from multiple knowledge sources - words, characters, part-of-speech n-grams, syntactic dependencies, word embeddings and more - that are dynamically included in the AA model according to the relevant input. In doing so, we would like to show that a stacking approach not only outperforms previous work in the field, but also that dynamic model selection outperforms the use of any of the individual components alone. The current model - called DynAA - is evaluated in a number of AA scenarios covering multiple languages, domains and input sizes, and is shown to generally outperform a number of baseline alternatives, including convolutional neural networks, BERT and others.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003079",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Character (mathematics)",
      "Computer science",
      "Convolutional neural network",
      "Economics",
      "Field (mathematics)",
      "Geometry",
      "Linguistics",
      "Management",
      "Mathematics",
      "Natural language processing",
      "Overfitting",
      "Philosophy",
      "Programming language",
      "Pure mathematics",
      "Set (abstract data type)",
      "Task (project management)",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Custódio",
        "given_name": "José Eleandro"
      },
      {
        "surname": "Paraboni",
        "given_name": "Ivandré"
      }
    ]
  },
  {
    "title": "Large-scale analysis of grooming in modern social networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114808",
    "abstract": "Social networks are evolving to engage their users more by providing them with more functionalities. One of the most attracting ones is streaming. Users may broadcast part of their daily lives to thousands of others world-wide and interact with them in real-time. Unfortunately, this feature is reportedly exploited for grooming. In this work, we provide the first in-depth analysis of this problem for social live streaming services. More precisely, using a dataset that we collected, we identify predatory behaviours and grooming on chats that bypassed the moderation mechanisms of the LiveMe, the service under investigation. Beyond the traditional text approaches, we also investigate the relevance of emojis in this context, as well as the user interactions through the gift mechanisms of LiveMe. Finally, our analysis indicates the possibility of grooming towards minors, showing the extent of the problem in such platforms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002499",
    "keywords": [
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Law",
      "Machine learning",
      "Moderation",
      "Paleontology",
      "Physics",
      "Political science",
      "Quantum mechanics",
      "Relevance (law)",
      "Scale (ratio)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Lykousas",
        "given_name": "Nikolaos"
      },
      {
        "surname": "Patsakis",
        "given_name": "Constantinos"
      }
    ]
  },
  {
    "title": "Machine Learning Techniques to Identify Unsafe Driving Behavior by Means of In-Vehicle Sensor Data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114818",
    "abstract": "Traffic crashes are one of the biggest causes of accidental death in the way where, every year, more than 1.35 million of people die. In most of them, the main cause is related to the driver’s behavior. The driver performs a set of actions on the vehicle commands, such as steering, braking, accelerating or changing gear, which generate a direct response of the vehicle, or other tasks, such as visual, auditory, or haptic related tasks (e.g. looking for items, listening to radio, and using a smartphone), which can still impact on the driving safety. In this work we propose a methodology based on machine learning techniques aimed at recognizing safe and unsafe driving behaviors by means of in-vehicle sensor data. Starting from these signals we compute a set of descriptive features capable to accurately describe the behavior of the driver. Two different classification tools, namely Support Vector Machines and feed-forward neural networks, have been trained and tested on a publicly available dataset containing more than 26 hours of total driving time. The classification results report an average accuracy above 90% for both classifiers and the McNemar test shows no performance difference between the models at the 0.05 significance level, demonstrating a concrete possibility of identifying unsafe driving using in-vehicle sensor data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002591",
    "keywords": [
      "Active listening",
      "Artificial intelligence",
      "Artificial neural network",
      "Communication",
      "Computer science",
      "Control (management)",
      "Machine learning",
      "Mathematics",
      "McNemar's test",
      "Platoon",
      "Programming language",
      "Set (abstract data type)",
      "Sociology",
      "Statistics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Lattanzi",
        "given_name": "Emanuele"
      },
      {
        "surname": "Freschi",
        "given_name": "Valerio"
      }
    ]
  },
  {
    "title": "A cloud theory-based multi-objective portfolio selection model with variable risk appetite",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114911",
    "abstract": "This study proposes a cloud theory-based multi-objective portfolio selection model with variable risk appetite, which incorporates four objectives of mean, variance, skewness, and liquidity constrained by several realistic constraints. Cloud model theory is employed to characterize the return rates and liquidity of assets due to the superiority of simultaneously capturing the ambiguity and randomness of information. The crisp numerical characteristics of the cloud model are defined to obtain the crisp form of the proposed model. To highlight and portray the investors’ risk (averse-neutral-seeking) appetites, the generalized acceptance and rejection functions are modeled by using the extreme values of each objective and introducing a variable risk appetite parameter. Thus the corresponding model is transformed with the objective functions of maximizing acceptance and minimizing rejection, which is solved through the compromise programming approach. The extended model provides investors with an opportunity to adjust risk parameters according to current market status. Moreover, the preference ratio vector is introduced when optimizing, which provides investors with overall control over the preferences regarding all objectives, so that investors can derive optimal portfolios well compatible with their expectations through customized weighting schemes. A real-world empirical application is presented to demonstrate the effectiveness of the proposed model",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003523",
    "keywords": [
      "Ambiguity",
      "Artificial intelligence",
      "Cloud computing",
      "Computer science",
      "Econometrics",
      "Economics",
      "Expected utility hypothesis",
      "Feature selection",
      "Finance",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Medicine",
      "Modern portfolio theory",
      "Operating system",
      "Portfolio",
      "Portfolio optimization",
      "Programming language",
      "Radiology",
      "Risk aversion (psychology)",
      "Skewness",
      "Statistics",
      "Variable (mathematics)",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Gong",
        "given_name": "Xiaomin"
      },
      {
        "surname": "Yu",
        "given_name": "Changrui"
      },
      {
        "surname": "Min",
        "given_name": "Liangyu"
      }
    ]
  },
  {
    "title": "Hiding sensitive association rules using the optimal electromagnetic optimization method and a dynamic bit vector data structure",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114879",
    "abstract": "Hiding the association rules is one of the methods used to protect sensitive information in data-mining processes. Its goal is to transform the original dataset so that the support for, or the reliability of, sensitive rules is reduced below the minimum threshold. Then these sensitive rules cannot be exploited, while the rules that are non-sensitive can still be exploited normally. Many methods have been proposed for hiding the association rules. However, most of these methods are very slow and consume a large amount of storage space. Consequently, they are not suitable when mining large datasets. Recently, the electromagnetic field optimization (EFO4ARH) method was proposed, and it was found to hide the sensitive association rules better than the other methods. To increase mining efficiency further, this paper proposes a new workaround called EFODBV4ARH. This technique applies a dynamic bit vector data structure in combination with the electromagnetic field optimization method. Experimental results indicate that EFODBV4ARH is significantly more efficient than EFO4ARH.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003201",
    "keywords": [
      "Algorithm",
      "Association rule learning",
      "Biology",
      "Bit array",
      "Computer science",
      "Data mining",
      "Ecology",
      "Electronic engineering",
      "Engineering",
      "Field (mathematics)",
      "Mathematics",
      "Physics",
      "Power (physics)",
      "Programming language",
      "Pure mathematics",
      "Quantum mechanics",
      "Reliability (semiconductor)",
      "Sensitivity (control systems)",
      "Type (biology)",
      "Workaround"
    ],
    "authors": [
      {
        "surname": "Le",
        "given_name": "Bac"
      },
      {
        "surname": "Phuong Le",
        "given_name": "Dong"
      },
      {
        "surname": "Tran",
        "given_name": "Minh-Thai"
      }
    ]
  },
  {
    "title": "NSGA-II with objective-specific variation operators for multiobjective vehicle routing problem with time windows",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114779",
    "abstract": "Vehicle routing problem with time windows (VRPTW) is a pivotal problem in logistics domain as it possesses multiobjective characteristics in real-world applications. Literature contains a general multiobjective VRPTW (MOVRPTW) with five objectives along with MOVRPTW benchmark instances that are derived from real-world data. In this paper, we have proposed a nondominated sorting genetic algorithm II (NSGA-II) based approach with objective-specific variation operators to address the MOVRPTW. In the proposed NSGA-II approach, the crossover and mutation operators are designed by exploiting the problem characteristics as well as the attributes of each objective. The performance of the proposed approach is evaluated on the standard benchmark instances of the problem and compared with the state-of-the-art approach available in literature. The computational results demonstrate the superiority of our approach over the state-of-the-art approach for the MOVRPTW.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002207",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Astrophysics",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Computer network",
      "Computer science",
      "Crossover",
      "Domain (mathematical analysis)",
      "Gene",
      "Genetic algorithm",
      "Geodesy",
      "Geography",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Multi-objective optimization",
      "Mutation",
      "Physics",
      "Routing (electronic design automation)",
      "Sorting",
      "State (computer science)",
      "Variation (astronomy)",
      "Vehicle routing problem"
    ],
    "authors": [
      {
        "surname": "Srivastava",
        "given_name": "Gaurav"
      },
      {
        "surname": "Singh",
        "given_name": "Alok"
      },
      {
        "surname": "Mallipeddi",
        "given_name": "Rammohan"
      }
    ]
  },
  {
    "title": "A novel hybrid ensemble model based on tree-based method and deep learning method for default prediction",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114899",
    "abstract": "Default prediction plays an important role in emerging financial market, so it has attracted extensive attention from financial industry and academic community. A slight improvement in default prediction performance can avoid huge economic losses. Many existing studies have used feature selection to improve the performance of default prediction models but paid limited attention to feature generation. Additionally, deep learning methods have been gradually explored for classification problems. In this study, a novel hybrid ensemble model is proposed to improve the performance of default prediction. First, a tree-based method (i.e., LightGBM) is used to learn new feature interactions and enhance the representation of original features. Second, a deep learning method (i.e., Convolutional Neural Network) is used as feature generation method to generate deeper feature interactions. Moreover, the structure of Inner Product-based Neural Network (IPNN) is used as deep learning classifier to learn feature interactions and reach a good trade-off between predictive accuracy and complexity. Third, ensemble learning method is used to combine the deep learning classifier with tree-based classifiers to obtain superior predictive results. Finally, two default datasets and four evaluation metrics are used to measure the predictive performance. The experimental results show that each component of the proposed model has significant improvement on overall performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003407",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Classifier (UML)",
      "Computer science",
      "Convolutional neural network",
      "Decision tree",
      "Deep learning",
      "Ensemble forecasting",
      "Ensemble learning",
      "Feature (linguistics)",
      "Feature learning",
      "Feature selection",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Hongliang"
      },
      {
        "surname": "Fan",
        "given_name": "Yanli"
      }
    ]
  },
  {
    "title": "A novel deep auto-encoder considering energy and label constraints for categorization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114936",
    "abstract": "Deep auto-encoder (DAE) is one of the representative deep learning algorithms for feature extraction. However, it often shows relatively poor generalization performance to express data without considering the probability distribution of data. Additionally, it cannot be directly applied to classification, because label information is ignored in DAE to judge the given categories. To tackle these issues, in this paper, we propose an energy and label constrained DAE (ELDAE) by integrating energy and label constraints to improve the feature extraction ability of network for classification. Specifically, as the probability distribution for fitting data can be reflected by energy of a network, the energy constraint is designed in this study to improve the probability of ELDAE for fitting data, and make a better expression to data. Moreover, the label constraint is integrated in ELDAE using label information to describe categorization rule, contributing to enhancing the accuracy of classification. We first give the complexity analysis of ELDAE, which is crucial to the property of speed. To exhibit the performance of the proposed ELDAE, we perform comprehensive experiments on benchmark USPS and MNIST datasets, and parameter sensitivity analysis is then provided to investigate the effects of three key parameters including the balance coefficients of weight decay, energy constraint and label constraint. In addition, we compare ELDAE with six state-of-the-art algorithms including Auto-Encoder (AE), Sparse AE (SAE), Deep AE (DAE), Deep Belief Network (DBN), Noisy AE (NAE) and Semi-supervised AE (SSAE). The comparative experimental results demonstrate that ELDAE performs better than the other six competitors in terms of classification accuracy, and keeps the same order of magnitude in terms of training time and testing time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003778",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Benchmark (surveying)",
      "Computer science",
      "Computer security",
      "Constraint (computer-aided design)",
      "Data mining",
      "Deep belief network",
      "Deep learning",
      "Encoder",
      "Energy (signal processing)",
      "Feature (linguistics)",
      "Feature extraction",
      "Generalization",
      "Geodesy",
      "Geography",
      "Geometry",
      "Key (lock)",
      "Linguistics",
      "MNIST database",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Wei"
      },
      {
        "surname": "Zhang",
        "given_name": "Yuxuan"
      },
      {
        "surname": "Park",
        "given_name": "Soon Cheol"
      }
    ]
  },
  {
    "title": "A framework for inventor collaboration recommendation system based on network approach",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114833",
    "abstract": "Precise and timely information about opportunities for potential collaborations is very vital for the collaboration-intense research environment prevailing in innovation ecosystems. As the identification of suitable inventors for collaboration will be decisive for inventors in different phases of their careers, inventor collaboration recommendation systems are of great importance. Existing recommendation system frameworks for collaboration recommendations for academic authors and inventors are slightly intensive on the usage of link semantics. Like academic collaboration through co-authorship, collaborations of inventors through co-inventorship of patents can be found in almost all industrial areas in various degrees. Network representation of co-inventorship can be used to retrieve many insights that can even be vital for policymaking. In this work, for inventor collaboration recommendations, a minimal link semantics (MLS) approach based framework is built to overcome these major drawbacks and to improve usability. The case of inventors in the area ‘Wireless power transmission’ is analyzed using patent data for the demonstration of the MLS framework and on evaluation, the framework is found to be capable of retrieving novel and diverse recommendations to and from inventors that belong to different phases of a career.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002748",
    "keywords": [
      "Biology",
      "Botany",
      "Computer science",
      "Data science",
      "Engineering",
      "Human–computer interaction",
      "Identification (biology)",
      "Knowledge management",
      "Law",
      "Mechanical engineering",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Semantics (computer science)",
      "Usability",
      "Work (physics)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "George",
        "given_name": "Susan"
      },
      {
        "surname": "Lathabai",
        "given_name": "Hiran H."
      },
      {
        "surname": "Prabhakaran",
        "given_name": "Thara"
      },
      {
        "surname": "Changat",
        "given_name": "Manoj"
      }
    ]
  },
  {
    "title": "Hierarchical distributed model predictive control based on fuzzy negotiation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114836",
    "abstract": "This work presents a hierarchical distributed model predictive control approach for multiple agents with cooperative negotiations based on fuzzy inference. Specifically, a fuzzy-based two-layer control architecture is proposed. In the lower control layer, there are pairwise negotiations between agents according to the couplings and the communication network. The resulting pairwise control sequences are sent to a coordinator in the upper control layer, which merges them to compute the final ones. Furthermore, conditions to guarantee feasibility and stability in the closed-loop system are provided. The proposed control algorithm has been tested on an eight-coupled tank plant via simulation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002773",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Distributed computing",
      "Fuzzy control system",
      "Fuzzy logic",
      "Hierarchical control system",
      "Inference",
      "Law",
      "Layer (electronics)",
      "Machine learning",
      "Model predictive control",
      "Negotiation",
      "Organic chemistry",
      "Pairwise comparison",
      "Political science",
      "Stability (learning theory)"
    ],
    "authors": [
      {
        "surname": "Masero",
        "given_name": "Eva"
      },
      {
        "surname": "Francisco",
        "given_name": "Mario"
      },
      {
        "surname": "Maestre",
        "given_name": "José M."
      },
      {
        "surname": "Revollar",
        "given_name": "Silvana"
      },
      {
        "surname": "Vega",
        "given_name": "Pastora"
      }
    ]
  },
  {
    "title": "Input data selection for daily traffic flow forecasting through contextual mining and intra-day pattern recognition",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114902",
    "abstract": "There is a large amount of literature about the traffic flow forecasting and most existing studies focus on prediction algorithm itself. However, how to select the appropriate historical data as input is also vital for the prediction task, while such studies are limited. This paper aims to cover this gap and proposes a method to select the appropriate historical data for daily traffic flow forecasting. The main idea is that some contextual factors including season, day of the week, weather, and holiday, influence the daily traffic flow pattern, and we select historical days with the similar pattern to the target day as the training data for prediction algorithm. The method consists of three steps: first, the similarities for traffic flow series between any two days are measured by Dynamic Time Warping, and then historical days are divided into different groups using a density-peak clustering algorithm; Second, the contextual factors are sorted by Elitist Non-dominated Sorting Genetic Algorithm (NSGA-II) using the clustering results, and their degrees of importance are transformed into weights in order to better measure the degrees of similarity between the clustered groups of days and the target day; third, one clustered group of historical data is selected based on the weighted degree of similarity and this group is used as the input for the prediction algorithm. At last, the benefits of the new method are discussed based on a Seattle case study, which illustrates that the proposed approach has higher prediction accuracy and stability across various prediction algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003432",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Computer security",
      "Data mining",
      "Dynamic time warping",
      "Economics",
      "Genetic algorithm",
      "Image (mathematics)",
      "Machine learning",
      "Management",
      "Pattern recognition (psychology)",
      "Selection (genetic algorithm)",
      "Similarity (geometry)",
      "Sorting",
      "Stability (learning theory)",
      "Task (project management)",
      "Traffic flow (computer networking)"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Dongfang"
      },
      {
        "surname": "Song",
        "given_name": "Xiang Ben"
      },
      {
        "surname": "Zhu",
        "given_name": "Jiacheng"
      },
      {
        "surname": "Ma",
        "given_name": "Weihao"
      }
    ]
  },
  {
    "title": "Dynamic network embedding via structural attention",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114895",
    "abstract": "Network embedding aims to learn low-dimensional vector representations for each node in a network, which facilitates various learning tasks such as node classification, link prediction and so on. The majority of existing embedding methods mainly focus on static networks. However, many real-world networks are dynamic and change over time. Although a small number of very recent literatures have been developed for dynamic network embedding, they either need to be retrained without closed-form expression, or suffer high-time complexity. Additionally, a large number of real-world networks may be both large and noisy, presenting great challenges to effective network representation learning. In this paper, we propose a novel method named Dynamic Network Embedding via Structural Attention (DNESA). Specifically, we incorporate the attention mechanism into network embedding, which facilitates our method mainly concentrating on task-related parts of the given graph while avoiding or ignoring noisy parts of the network. Furthermore, we can capture the evolving characteristic of dynamic networks and learn embedding vectors of each node at different time steps by modeling the process of developing an open triad into a closed triad under the attention mechanism. Meanwhile, we carefully design an optimization function for preserving both the first-order and second-order proximities. Empirical experiments conducted on six real-world networks illustrate the efficiency of the proposed method, which outperforms state-of-the-art network embedding methods in applications including link prediction and node classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003365",
    "keywords": [
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Dynamic network analysis",
      "Embedding",
      "Engineering",
      "Graph",
      "Law",
      "Machine learning",
      "Node (physics)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Structural engineering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Chen"
      },
      {
        "surname": "Fan",
        "given_name": "Yiming"
      },
      {
        "surname": "Xie",
        "given_name": "Yu"
      },
      {
        "surname": "Yu",
        "given_name": "Bin"
      },
      {
        "surname": "Li",
        "given_name": "Chunyi"
      },
      {
        "surname": "Pan",
        "given_name": "Ke"
      }
    ]
  },
  {
    "title": "Multi class SVM algorithm with active learning for network traffic classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114885",
    "abstract": "With the current massive amount of traffic that is going through the internet, internet service providers (ISPs) and networking service providers (NSPs) are looking for various ways to accurately predict the application type of flow that is going through the internet. Such prediction is critical for security and network monitoring applications as they require application type to be known in prior. Traditional ways using port-based or payload-based analysis are not sufficient anymore as many applications start using dynamic unknown port numbers, masquerading, and encryption techniques to avoid being detected. Recently, machine learning has gained significant attention in many prediction applications including traffic classification from flow features or characteristics. However, such algorithms suffer from an imbalanced data problem where some applications have fewer flow data and hence difficult to predict. In this paper, we employ network flow-level characteristics to identify the application type of traffic. Furthermore, we propose the use of an improved support vector machine (SVM) algorithm, named cost-sensitive SVM (CMSVM), to solve the imbalance problem in network traffic identification. CMSVM adopts a multi-class SVM algorithm with active learning which dynamically assigns a weight for applications. We examine the classification accuracy and performance of the CMSVM algorithm using two different datasets, namely MOORE_SET and NOC_SET datasets. Our results show that the CMSVM algorithm can reduce computation cost, improve classification accuracy and solve the imbalance problem when compared to other machine learning techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003262",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer network",
      "Computer science",
      "Data mining",
      "Identification (biology)",
      "Machine learning",
      "Network packet",
      "Payload (computing)",
      "Support vector machine",
      "The Internet",
      "Traffic classification",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Dong",
        "given_name": "Shi"
      }
    ]
  },
  {
    "title": "A proposed customer relationship framework based on information retrieval for effective Firms’ competitiveness",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114882",
    "abstract": "Nowadays, firms are strongly racing to raise their competitive level in the international market. As this market has a natural connection, therefore, one of the vital roads for competing is exploring the users’ behaviour which continuously changes over time. This research proposes an intelligent information retrieval-based framework which applies a set of techniques to monitor the customers’ behaviour and determine the behaviour similarity. These techniques followed a determined opinion mining, knowledge discovery, weight measurement, and text analysis approaches. The aim of the proposed framework is to explore the suitable recommendations for the current customers and acquire new customers who could be selected from the customers’ social friends, which leads to the increase of the market share, a raise in the loyal customers’ segment, and finally in the firm’s competitiveness level. The framework has been successfully verified in two successful companies, the evaluation included different measures such as responding to change rate, and the customers’ segment share percentage. The evaluation presented an increase in the customers’ satisfaction level to be 97.91% and 97.31% for the two companies respectively while the willingness of new customers to join the customers’ segment has been raised by 83.15%. while However, the study could be further expanded in many directions such as the discovery of the customers’ opinion based on different sentiment levels.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003237",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Customer satisfaction",
      "Image (mathematics)",
      "Market share",
      "Marketing",
      "Programming language",
      "Sentiment analysis",
      "Set (abstract data type)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Almazroi",
        "given_name": "Abdulwahab Ali"
      },
      {
        "surname": "Khedr",
        "given_name": "Ayman E."
      },
      {
        "surname": "Idrees",
        "given_name": "Amira M."
      }
    ]
  },
  {
    "title": "“Taps”: A trading approach based on deterministic sign patterns",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114761",
    "abstract": "We propose a new methodology for trading financial instruments based on deterministic sign patterns. These patterns are obtained from the m-dimensional elementary sample space consisting of - 1 , 1 m , the two possible signs for trading and with m varying. The collection of all possible sign combinations coming from this sample space creates a zero-cost trading strategy and we consider strategies that are selected from rotations among the possible sign sequences using several statistical criteria. Performing simulations, based on a geometric Brownian motion, we find that – on average – our strategies can outperform the buy & hold benchmark about 30% of the time in terms of total return and around 60% of the time in terms of maximum drawdown. We then illustrate the practical efficacy of the proposed strategies using daily returns from the S&P500 index, two of the largest Chinese stock market indices, the CSI300 and the SSE50, and three exchange traded funds (ETFs). Our results strongly suggest performance improvements over the corresponding buy & hold benchmarks and, furthermore, that these performance differences can be attributed to the entropy of the US and Chinese markets: we find that the two Chinese indices, which have larger entropy than the US index, provide considerable performance enhancements when traded based on our suggested methodology.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002025",
    "keywords": [
      "Aquifer",
      "Biology",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Diffusion process",
      "Drawdown (hydrology)",
      "Econometrics",
      "Economics",
      "Engineering",
      "Entropy (arrow of time)",
      "Finance",
      "Financial economics",
      "Geometric Brownian motion",
      "Geotechnical engineering",
      "Groundwater",
      "Horse",
      "Index (typography)",
      "Innovation diffusion",
      "Knowledge management",
      "Mathematical analysis",
      "Mathematics",
      "Paleontology",
      "Physics",
      "Quantum mechanics",
      "Sample (material)",
      "Sign (mathematics)",
      "Stock exchange",
      "Stock market",
      "Stock market index",
      "Technical analysis",
      "Trading strategy",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Xi"
      },
      {
        "surname": "Thomakos",
        "given_name": "Dimitrios D."
      }
    ]
  },
  {
    "title": "A new integrated similarity measure for enhancing instance-based credit assessment in P2P lending",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114798",
    "abstract": "Instance-based learning has been proved to be effective for credit assessment in Peer-to-peer (P2P) lending. A key challenge of this application is how to measure the similarity of loans, which usually have multiple features gained from different data sources and models. In this paper, a new similarity measure is introduced to effectively integrate the information from different sources and models for credit assessment in P2P lending. Specifically, we firstly deconstructed the characteristics of P2P lending and presented four heterogeneous distance functions, which were generated by different models and information sources, to measure the loans’ similarity. Then, we proposed an integrated similarity measure that combined the above similarities by minimizing their conflicts, which could overcome the bias of the single model and single information source. Finally, we employed the portfolio selection model to develop our investment strategy. Experimental results using real datasets from Prosper demonstrated that our integrated similarity measure improves the performance of the instance-based credit assessment in P2P lending.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002396",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Image (mathematics)",
      "Machine learning",
      "Measure (data warehouse)",
      "Similarity (geometry)",
      "Similarity measure"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Yanhong"
      },
      {
        "surname": "Jiang",
        "given_name": "Shuai"
      },
      {
        "surname": "Qiao",
        "given_name": "Han"
      },
      {
        "surname": "Chen",
        "given_name": "Feiting"
      },
      {
        "surname": "Li",
        "given_name": "Yaocong"
      }
    ]
  },
  {
    "title": "Novel local feature extraction for age invariant face recognition",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114786",
    "abstract": "Age variation is a major problem in the area of face recognition under uncontrolled environments such as pose variation, lighting effects, expression etc. Most of the works of this area have used discriminative feature descriptors. These discriminative feature descriptors are based on their fixed encoding which considers pixels of different radial widths for feature extraction and ignores some radii pixels which hold important discriminative information for age variation. Therefore, consideration of all the pixels of the local region is necessary for important feature extraction in the case of age invariant face recognition. This paper introduces a novel local feature descriptor to find difference pattern and dual directional relation pattern for age invariant face recognition. The proposed descriptor is applied over the preprocessed face images and its parts-periocular region i.e. left and right eye, mouth and nose region of a face image. The proposed difference pattern and dual directional relation pattern descriptors extract the texture features on the local region of a specified dimension. Chi-square metric has been used for finding the similarity between probe and gallery images. Evaluation of the proposed feature descriptor has been performed on two standard challenging datasets FGNET and MORPH for age invariant face recognition. The proposed descriptor performed well and outperformed to the existing age invariant face recognition state-of-the-art methods on FGNET dataset and also performed well on MORPH dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100227X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Face (sociological concept)",
      "Facial recognition system",
      "Feature (linguistics)",
      "Feature extraction",
      "Histogram",
      "Image (mathematics)",
      "Invariant (physics)",
      "Linguistics",
      "Local binary patterns",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Tripathi",
        "given_name": "Rajesh Kumar"
      },
      {
        "surname": "Jalal",
        "given_name": "Anand Singh"
      }
    ]
  },
  {
    "title": "A binary artificial bee colony algorithm and its performance assessment",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114817",
    "abstract": "Artificial bee colony algorithm, ABC for short, is a swarm-based optimization algorithm proposed for solving continuous optimization problems. Due to its simple but effective structure, some binary versions of the algorithm have been developed. In this study, we focus on modification of its xor-based binary version, called as binABC. The solution update rule of basic ABC is replaced with a xor logic gate in binABC algorithm, and binABC works on discretely-structured solution space. The rest of components in binABC are the same as with the basic ABC algorithm. In order to improve local search capability and convergence characteristics of binABC, a stigmergic behavior-based update rule for onlooker bees of binABC and extended version of xor-based update rule are proposed in the present study. The developed version of binABC is applied to solve a modern benchmark problem set (CEC2015). To validate the performance of proposed algorithm, a series of comparisons are conducted on this problem set. The proposed algorithm is first compared with the basic ABC and binABC on CEC2015 set. After its performance validation, six binary versions of ABC algorithm are considered for comparison of the algorithms, and a comprehensive comparison among the state-of-art variants of swarm intelligence or evolutionary computation algorithms is conducted on this set of functions. Finally, an uncapacitated facility location problem set, a pure binary optimization problem, is considered for the comparison of the proposed algorithm and binary variants of ABC algorithm. The experimental results and comparisons show that the proposed algorithm is successful and effective in solving binary optimization problems as its basic version in solving continuous optimization problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100258X",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial bee colony algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Binary number",
      "Binary search algorithm",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Evolutionary computation",
      "Geodesy",
      "Geography",
      "Mathematical optimization",
      "Mathematics",
      "Particle swarm optimization",
      "Programming language",
      "Search algorithm",
      "Set (abstract data type)",
      "Swarm intelligence"
    ],
    "authors": [
      {
        "surname": "Kiran",
        "given_name": "Mustafa Servet"
      }
    ]
  },
  {
    "title": "Visual analytic based ship collision probability modeling for ship navigation safety",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114755",
    "abstract": "This study presents a tangible visual analytic tool to analyse maritime traffic in spatio-temporal basis using AIS data. This novel approach helps in understanding the macroscopic safety structure of both fairways and individual ships with evidences in microscopic level. Qualification of our system is demonstrated with 7-days AIS trajectory collected from Mexican Gulf. We find out that spatio-temporal position pattern of encountered ships in Port Houston varies over time, significantly. In addition, the spatial distribution of ship accidents coincide with proposed near-miss density areas. Furthermore, proposed tool is capable of capturing real accident cases. Field experiments with domain experts have demonstrated that our approach helps in making realistic inferences about navigational safety behaviour of both individual vessel and water area.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001962",
    "keywords": [
      "Aerospace engineering",
      "Astronomy",
      "Collision",
      "Collision avoidance",
      "Computer science",
      "Computer security",
      "Course (navigation)",
      "Data mining",
      "Domain (mathematical analysis)",
      "Economics",
      "Electrical engineering",
      "Engineering",
      "Field (mathematics)",
      "Finance",
      "Marine engineering",
      "Mathematical analysis",
      "Mathematics",
      "Physics",
      "Port (circuit theory)",
      "Position (finance)",
      "Pure mathematics",
      "Trajectory"
    ],
    "authors": [
      {
        "surname": "Öztürk",
        "given_name": "Ülkü"
      },
      {
        "surname": "Boz",
        "given_name": "Hasan Alp"
      },
      {
        "surname": "Balcisoy",
        "given_name": "Selim"
      }
    ]
  },
  {
    "title": "A nanosatellite task scheduling framework to improve mission value using fuzzy constraints",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114784",
    "abstract": "Task scheduling is an effective approach to increase the value of a satellite mission, which leads to improved resource management and quality of service. This work improves the energy prediction model and a task scheduling formulation, expressed in integer programming to maximize the number of tasks performed in nanosatellite missions. A realistic battery model is introduced in the formulation to extend battery lifetime. This is achieved by a disjunctive program that makes battery charge and discharge more efficient. Furthermore, fuzzy constraints are designed to limit the current rates (for charge and discharge) and the depth of discharge for battery lifetime preservation. Each battery access is penalized in the objective function, thereby stimulating energy consumption to match energy input. For simulation purposes, the varying power input was based on two-line element data of the CubeSat FloripaSat-I, operating in an orbit with J2 perturbation and an attitude that keeps one face of the nanosatellite towards the Earth for the entire orbit, similar to a remote sensing mission. The effectiveness of the task-scheduling methodology was shown by means of simulated experiments of representative scenarios. With the improvements proposed here, a robust and realistic framework for optimal offline scheduling of nanosatellite missions is achieved.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002256",
    "keywords": [
      "Aerospace engineering",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "CubeSat",
      "Engineering",
      "Fuzzy logic",
      "Integer programming",
      "Mathematical optimization",
      "Mathematics",
      "Real-time computing",
      "Reliability engineering",
      "Satellite",
      "Scheduling (production processes)"
    ],
    "authors": [
      {
        "surname": "Rigo",
        "given_name": "Cezar Antônio"
      },
      {
        "surname": "Seman",
        "given_name": "Laio Oriel"
      },
      {
        "surname": "Camponogara",
        "given_name": "Eduardo"
      },
      {
        "surname": "Morsch Filho",
        "given_name": "Edemar"
      },
      {
        "surname": "Bezerra",
        "given_name": "Eduardo Augusto"
      }
    ]
  },
  {
    "title": "Texture defect classification with multiple pooling and filter ensemble based on deep neural network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114838",
    "abstract": "Fabric quality control is one of the most important phases of production in order to ensure high-quality standards in the fabric production sector. For this reason, the development of successful automatic quality control systems has been a very important research subject. In this study, we propose a Multiple Pooling and Filter approach based on a Deep Neural Network (MPF-DNN) for the classification of texture defects. This model consists of three basic stages: preprocessing, feature extraction, and classification. In the preprocessing stage, the texture images were first divided into n × n equal parts. Then, median filtering and pooling processes were applied to each piece prior to performing image merging. In the proposed pre-treatment stage, it is aimed to clarify fabric errors and increase performance. For the feature extraction stage, deep features were extracted from the texture images using the pretrained ResNet101 model based on the transfer learning approach. Finally, classification and testing procedures were conducted on the obtained deep-effective properties using the SVM method. The multiclass TILDA dataset was used in order to test the proposed model. In experimental work, the MPF-DNN model for all four classes achieved a significant overall accuracy score of 95.82%. In the results obtained from extensive experimental studies, it was observed that the proposed MPF-DNN model was more successful than previous studies that used pretrained deep architectures.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002797",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Feature (linguistics)",
      "Feature extraction",
      "Filter (signal processing)",
      "Geometry",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pooling",
      "Preprocessor",
      "Pyramid (geometry)",
      "Support vector machine",
      "Texture (cosmology)"
    ],
    "authors": [
      {
        "surname": "Uzen",
        "given_name": "Huseyin"
      },
      {
        "surname": "Turkoglu",
        "given_name": "Muammer"
      },
      {
        "surname": "Hanbay",
        "given_name": "Davut"
      }
    ]
  },
  {
    "title": "BBO-BPNN and AMPSO-BPNN for multiple-criteria inventory classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114842",
    "abstract": "Item classification is an issue among inventory managers who want to achieve key-point management of items with different emphases, thus prompting managers and researchers to pursue efficient and effective classification algorithms. In processing multiple-criteria inventory classifications, back-propagation neural network (BPNN) shows its superiority in balancing items’ multiple competing attributes. However, because the training processes and the final results of BPNN rely on the initial connection weights and thresholds, finding reasonable values of the two parameters is a challenge. This paper introduces biogeography-based optimization (BBO) and an adaptive mutation particle swarm optimizer (AMPSO) to BPNN to optimize the training parameters, i.e., the global initial connection weights and thresholds, of BPNN. On the basis of this, two hybrid classification algorithms—BBO-BPNN and AMPSO-BPNN—are presented. Real-life data from three cases are adopted to verify the effectiveness and feasibility of the two proposed hybrid algorithms. Experimental results demonstrate that BBO-BPNN and AMPSO-BPNN show higher classification accuracy than other hybrid models, i.e., BPNN, PSO-BPNN, DE-BPNN, and GA-BPNN, in the multiple-criteria inventory classification problem.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002839",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Data mining",
      "Machine learning",
      "Particle swarm optimization",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Cui",
        "given_name": "Ligang"
      },
      {
        "surname": "Tao",
        "given_name": "Yongqiang"
      },
      {
        "surname": "Deng",
        "given_name": "Jie"
      },
      {
        "surname": "Liu",
        "given_name": "Xiaolin"
      },
      {
        "surname": "Xu",
        "given_name": "Dongyang"
      },
      {
        "surname": "Tang",
        "given_name": "Guofeng"
      }
    ]
  },
  {
    "title": "Closest target setting for two-stage network system: An application to the commercial banks in China",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114799",
    "abstract": "Traditional data envelopment analysis (DEA) models mainly set the furthest targets as the frontier projection for inefficient decision-making units (DMUs). To achieve the efficient status with less effort, the closest target models are introduced which projected the least input/output improvement for inefficient DMUs. However, these works typically consider each DMU as a “black box” in the closest target setting, and thus these models cannot be directly extended to a system with network structure because there may be no efficient DMU for reference. This paper fills the gap by developing a closest target model for a two-stage system. Instead of constructing the efficient frontier only by the system efficient DMUs in the “black box” system, all the extreme efficient stages of the DMUs are considered to form the closest target for an inefficient DMU. Using our network closest target (NCT) model, a case of the 16 leading commercial banks in China is analyzed. The results show that these commercial banks performed steadily in both efficiency and input/output required improvement during the study period. Moreover, compared with the network furthest target (NFT) model, NCT requires each DMU less improvement on inputs/outputs, and usually obtain the higher efficiency. That is, NCT is more feasible, economical and optimistic. Lastly, we discuss the proportion of each dominating peer referred by the inefficient banks and the importance of these peers is ranked accordingly.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002402",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Data envelopment analysis",
      "Economics",
      "Efficient frontier",
      "Finance",
      "Mathematical optimization",
      "Mathematics",
      "Portfolio",
      "Programming language",
      "Projection (relational algebra)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "An",
        "given_name": "Qingxian"
      },
      {
        "surname": "Wu",
        "given_name": "Qifan"
      },
      {
        "surname": "Zhou",
        "given_name": "Xiaoyang"
      },
      {
        "surname": "Chen",
        "given_name": "Xiaohong"
      }
    ]
  },
  {
    "title": "Human vital sign determination using tactile sensing and fuzzy triage system",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114781",
    "abstract": "The ability to quickly and accurately triage a person’s medical condition in an emergency situation or other critical scenarios could mean the difference between life and death. Endowing a robotic system with vision and tactile capabilities, similar to those of medical professionals, and thus enabling robots to assess a patient’s status in an emergency is a highly sought after characteristic in healthcare robotics. This paper presents a novel fuzzy triage system exploiting visual and tactile sensing, to equip a robot with the skills to accurately determine key vital signs in humans. There are three key signs of human health: respiratory rate, pulse rate (Beats Per Minute (BPM)) and capillary refill time. Using ground truth from a medical professional, the fuzzy triage system is trained and validated initially with informed synthetic data and then further evaluated using vital signs data collected from subjects in a pilot study. Results from this pilot study indicate that the fuzzy triage system is capable of classifying a patient’s health using the novel approaches for collecting BPM, Respiratory Rate (RR) and Capillary Refill Time (CRT) which replicate, to some extent, the approaches used by medical professionals for measuring vital signs. Furthermore, the intelligent system proved capable of determining whether a pulse was regular or arrhythmic, whether respiratory rate was regular or irregular, and determining the subject’s capillary refill time. Such results imply that this system could ultimately be used, for example, in a home assistance robot for elderly or disabled persons, or as a first responder robot. Ultimately the aim would be that these methods could be utilised by robotic systems in emergency scenarios or disaster zones.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002220",
    "keywords": [
      "Artificial intelligence",
      "Blood pressure",
      "Capillary refill",
      "Computer science",
      "Computer security",
      "Engineering",
      "Fuzzy logic",
      "Heart rate",
      "Human–computer interaction",
      "Key (lock)",
      "Medical emergency",
      "Medicine",
      "Radiology",
      "Respiratory rate",
      "Robot",
      "Robotics",
      "Simulation",
      "Surgery",
      "Systems engineering",
      "Task (project management)",
      "Triage",
      "Vital signs"
    ],
    "authors": [
      {
        "surname": "Kerr",
        "given_name": "Emmett"
      },
      {
        "surname": "McGinnity",
        "given_name": "T.M."
      },
      {
        "surname": "Coleman",
        "given_name": "Sonya"
      },
      {
        "surname": "Shepherd",
        "given_name": "Andrea"
      }
    ]
  },
  {
    "title": "Effective and diverse POI recommendations through complementary diversification models",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114775",
    "abstract": "Nowadays, recommender systems play an important role in several Location-Based Social Networks (LBSNs). The current advances have considered the trade-off between accuracy and diversity to help users to discover and explore new points-of-interest (POI). However, differently from traditional recommendation scenarios, other equally relevant dimensions (e.g., social and geographical user information) have to be considered to understand how the characteristics of services offered by each POI fit the user needs. Specifically, this work sheds light upon naive failures introduced by traditional recommendation methods while they handle this trade-off between diversity and accuracy in POI recommendations. We hypothesize that some efforts on POI recommendations somehow are deviating from basic learnings from the area. In this context, this work addresses four characteristics inherent to the POI domain that previous efforts have failed to recognize: (1) POI categories and locations are complementary dimensions of diversification that should be simultaneously addressed; (2) Diversity is a complex concept that should be modeled by distinct and non-orthogonal models; (3) Distinct users have different biases and willingness to move to fulfill their needs; (4) POI recommendation is a multi-objective task. In order to demonstrate the gains of properly addressing these aspects, we also propose DisCovER, a straightforward re-ordering method that linearly combines geographical and categorical diversification. DisCovER results demonstrate that even simple strategies to exploit simultaneously these complementary dimensions can increase diversification while keeping accuracy high. Differently from state-of-the-art diversification methods, DisCovER does not penalize any quality dimension in favor of others. It allows us to discuss future directions towards more robust user modeling and preference elicitation in POI domains.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002165",
    "keywords": [
      "Anthropology",
      "Archaeology",
      "Artificial intelligence",
      "Business",
      "Categorical variable",
      "Computer science",
      "Computer security",
      "Context (archaeology)",
      "Data mining",
      "Data science",
      "Dimension (graph theory)",
      "Diversification (marketing strategy)",
      "Diversity (politics)",
      "Exploit",
      "Geography",
      "Machine learning",
      "Marketing",
      "Mathematics",
      "Point of interest",
      "Pure mathematics",
      "Recommender system",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Werneck",
        "given_name": "Heitor"
      },
      {
        "surname": "Santos",
        "given_name": "Rodrigo"
      },
      {
        "surname": "Silva",
        "given_name": "Nícollas"
      },
      {
        "surname": "Pereira",
        "given_name": "Adriano C.M."
      },
      {
        "surname": "Mourão",
        "given_name": "Fernando"
      },
      {
        "surname": "Rocha",
        "given_name": "Leonardo"
      }
    ]
  },
  {
    "title": "Hybrid ensemble approaches to online harassment detection in highly imbalanced data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114751",
    "abstract": "Online harassment is a major threat to users of social media platforms, especially young adults and women. It can cause mental illnesses and impacts deeply and negatively economic institutions experiencing cyberbully attacks by losing their credibility and business. This makes automatic detection of online harassment extremely important. Most of current studies within this context apply machine-learning algorithms that assume balanced class distribution. However, this assumption does not hold for most real datasets. This research provides a comprehensive investigation of various approaches that combine diverse techniques under three dimensions: feature representation, imbalanced data handling, and supervised learning. For the first dimension, three word-embedding models have been considered, namely: word2vec, Glove, and SSWE. For the other two dimensions, nine techniques for balancing skewed class distributions have been employed to feed several learning models. In particular, resampling methods, cost-sensitive learning, and Weight-Selection strategy-based methods have been used with deep neural networks. The ultimate goal of this study is to evaluate the potential of using such hybrid approaches to handle the online harassment detection task efficiently using highly-imbalanced Twitter data and to select the best combination concerning the intended purpose. An extensive comparative study has been conducted, and the results have been discussed in terms of three evaluation metrics widely used for imbalanced classification. As main findings, Glove has been found as the best feature representation and some combinations as the best performing most notably LSTM and BLSTM with cost-sensitive learning and VL strategy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001925",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Credibility",
      "Data mining",
      "Embedding",
      "Ensemble learning",
      "Feature (linguistics)",
      "Feature learning",
      "Feature selection",
      "Harassment",
      "Law",
      "Linguistics",
      "Machine learning",
      "Paleontology",
      "Philosophy",
      "Political science",
      "Word2vec"
    ],
    "authors": [
      {
        "surname": "Tolba",
        "given_name": "Marwa"
      },
      {
        "surname": "Ouadfel",
        "given_name": "Salima"
      },
      {
        "surname": "Meshoul",
        "given_name": "Souham"
      }
    ]
  },
  {
    "title": "A multi-leader whale optimization algorithm for global optimization and image segmentation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114841",
    "abstract": "In this paper, a multilevel thresholding image segmentation method base on the enhancement of the performance of the whale optimization algorithm (WOA). The developed method, called the multi-leader whale optimization algorithm (MLWOA), aims to avoid the limitations of traditional WOA during the searching process, such as stagnation at the local optimum. This was achieved by integrating the different tools with WOA, such as memory mechanism, multi-leader method, self-learning strategy, and levy flight method. Each of these techniques has its own task, for example, the memory structure of traditional WOA and add a multi-leader mechanism to enhance the ability of exploration. The superiority of leaders will make more influence in MLWOA by adding a self-learning strategy. Also, it used levy flight trajectory to make the algorithm more robust and avoid premature convergence. To evaluate the performance of the developed MLWOA, a set of experiments are conducted using the CEC2017 benchmark. In addition, it is applied to determine the optimal threshold values to segment a set of images using the Otsu method, fuzzy entropy, and Kapur's entropy as a fitness function. The results of MLWOA are compared with well-known meta-heuristic algorithms inside the experiments. The comparison results indicated that MLWOA provides better performance in CEC2017 benchmark functions and shows high superiority in image segmentation in terms of performance measures. In addition, the MLWOA provides better results using Otsu, followed by the Fuzzy entropy and Kapur in terms of PSNR. In terms of SSIM, fuzzy entropy and Otsu have nearly the same SSIM value, but the fuzzy entropy provides better results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002827",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Entropy (arrow of time)",
      "Fuzzy logic",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Image segmentation",
      "Mathematical optimization",
      "Mathematics",
      "Otsu's method",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Segmentation",
      "Thresholding"
    ],
    "authors": [
      {
        "surname": "Abd Elaziz",
        "given_name": "Mohamed"
      },
      {
        "surname": "Lu",
        "given_name": "Songfeng"
      },
      {
        "surname": "He",
        "given_name": "Sibo"
      }
    ]
  },
  {
    "title": "Data-free knowledge distillation in neural networks for regression",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114813",
    "abstract": "Knowledge distillation has been used successfully to compress a large neural network (teacher) into a smaller neural network (student) by transferring the knowledge of the teacher network with its original training dataset. However, the original training dataset is not reusable in many real-world applications. To address this issue, data-free knowledge distillation, which is knowledge distillation in the absence of the original training datasets, has been studied. However, existing methods are limited to classification problems and cannot be directly applied to regression problems. In this study, we propose a novel data-free knowledge distillation method that is applicable to regression problems. Given a teacher network, we adopt a generator network to transfer the knowledge in the teacher network to a student network. We simultaneously train the generator and student networks in an adversarial manner. The generator network is trained to create synthetic data on which the teacher and student networks make different predictions, with the student network being trained to mimic the teacher network’s predictions. We demonstrate the effectiveness of the proposed method on benchmark datasets. Our results show that the student network emulates the prediction ability of the teacher network with little performance loss.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002542",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Data mining",
      "Distillation",
      "Machine learning",
      "Mathematics",
      "Regression",
      "Regression analysis",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Kang",
        "given_name": "Myeonginn"
      },
      {
        "surname": "Kang",
        "given_name": "Seokho"
      }
    ]
  },
  {
    "title": "A robust optimization model for location-transportation problem of disaster casualties with triage and uncertainty",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114867",
    "abstract": "Emergency medical services (EMS) are essential components for post-disaster rescue activities. Considering injury heterogeneity and deterioration over time of the casualties, the uncertainty in the number of the casualties can effectively improve the medical service performance. This paper develops a robust optimization model for combined facility location and casualty transportation under uncertainty in the number of casualties. We divide casualties into two types: mild casualties who are transported to on-site clinics with rescue vehicles and serious casualties who are transported to general hospitals with helicopters. Meanwhile, we consider the Injury Severity Score (ISS) increment to describe the injury deterioration of the casualties over time. The objective is to minimize the total weighted ISS increment of mild casualties and serious casualties. Then, we employ the robust optimization method to deal with the uncertainty and derive the robust counterpart of the proposed model in this paper. Case studies based on Yushu Earthquake show that the model can get the optimal emergency facility location and casualty transportation scheme to minimize the total weighted ISS increment. The total weighted ISS increment increases as the constraint violation probability decreases, which reflects the trade-off between performance and robustness. Sensitivity analyses show that the greater uncertainty of the casualty number is, the greater the impact on the total weighted ISS increment is, the more conservative the decision scheme is. The capacity of general hospitals has a greater effect on the objective value compared with the capacity of on-site clinics and the decision scheme of the robust optimization model has a greater optimality compared with the deterministic model when the problem size is magnified.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003080",
    "keywords": [
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Emergency medical services",
      "Engineering",
      "Facility location problem",
      "Gene",
      "Injury prevention",
      "Mass-casualty incident",
      "Mathematical optimization",
      "Mathematics",
      "Medical emergency",
      "Medicine",
      "Operations research",
      "Poison control",
      "Robust optimization",
      "Robustness (evolution)",
      "Triage"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Huali"
      },
      {
        "surname": "Wang",
        "given_name": "Yang"
      },
      {
        "surname": "Zhang",
        "given_name": "Jianghua"
      },
      {
        "surname": "Cao",
        "given_name": "Wenqian"
      }
    ]
  },
  {
    "title": "Machine Learning for industrial applications: A comprehensive literature review",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114820",
    "abstract": "Machine Learning (ML) is a branch of artificial intelligence that studies algorithms able to learn autonomously, directly from the input data. Over the last decade, ML techniques have made a huge leap forward, as demonstrated by Deep Learning (DL) algorithms implemented by autonomous driving cars, or by electronic strategy games. Hence, researchers have started to consider ML also for applications within the industrial field, and many works indicate ML as one the main enablers to evolve a traditional manufacturing system up to the Industry 4.0 level. Nonetheless, industrial applications are still few and limited to a small cluster of international companies. This paper deals with these topics, intending to clarify the real potentialities, as well as potential flaws, of ML algorithms applied to operation management. A comprehensive review is presented and organized in a way that should facilitate the orientation of practitioners in this field. To this aim, papers from 2000 to date are categorized in terms of the applied algorithm and application domain, and a keyword analysis is also performed, to details the most promising topics in the field. What emerges is a consistent upward trend in the number of publications, with a spike of interest for unsupervised and especially deep learning techniques, which recorded a very high number of publications in the last five years. Concerning trends, along with consolidated research areas, recent topics that are growing in popularity were also discovered. Among these, the main ones are production planning and control and defect analysis, thus suggesting that in the years to come ML will become pervasive in many fields of operation management.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100261X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data science",
      "Deep learning",
      "Domain (mathematical analysis)",
      "Engineering",
      "Field (mathematics)",
      "Industrial engineering",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Operations research",
      "Popularity",
      "Psychology",
      "Pure mathematics",
      "Social psychology",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Bertolini",
        "given_name": "Massimo"
      },
      {
        "surname": "Mezzogori",
        "given_name": "Davide"
      },
      {
        "surname": "Neroni",
        "given_name": "Mattia"
      },
      {
        "surname": "Zammori",
        "given_name": "Francesco"
      }
    ]
  },
  {
    "title": "The student scheduling problem at Université de Technologie de Compiègne",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114735",
    "abstract": "At Université de Technologie de Compiègne (UTC), complete course timetables are made available to students for the forthcoming semester, then students select their courses. A student cannot attend two events at the same time without violating a hard conflict constraint. Ideally, a conflict-free individual timetable will be created for each student, but it may be the case that there is no feasible solution for all students simultaneously. With thousands of students and hundreds of courses, UTC’s Student Scheduling Problem (SSP) cannot be processed manually. A heuristic designed decades ago can no longer be used, because too many students are left non-assigned and because the heuristic was not designed to deal with the current requirements. In this study we propose a preprocessing that reduces the size of instances, together with lower bounds that allow the quality of computed solutions to be assessed. Where most of the graphs relating to students’ hard conflict constraints are interval graphs, we show that a clique formulation of the hard conflict constraints may substantially reduce the number of equations in relation to other formulations. We propose integer linear programming (ILP) formulations to address the current requirements. We test an ILP with a global objective function and other ILPs within a lexicographic scheme. We investigate valid inequalities for reducing computation times. We report our computational experiments and results obtained on real instances from UTC. The solution method has proved its effectiveness and is now the tool used for student scheduling at UTC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001767",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Clique",
      "Combinatorics",
      "Computer science",
      "Heuristic",
      "Integer programming",
      "Lexicographical order",
      "Linear programming",
      "Mathematical optimization",
      "Mathematics",
      "Preprocessor",
      "Scheduling (production processes)"
    ],
    "authors": [
      {
        "surname": "Boufflet",
        "given_name": "Jean-Paul"
      },
      {
        "surname": "Arbaoui",
        "given_name": "Taha"
      },
      {
        "surname": "Moukrim",
        "given_name": "Aziz"
      }
    ]
  },
  {
    "title": "Polygonal Coordinate System: Visualizing high-dimensional data using geometric DR, and a deterministic version of t-SNE",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114741",
    "abstract": "Dimensionality Reduction (DR) is useful to understand high-dimensional data. It attracts wide attention from industry and academia and is employed in areas such as machine learning, data mining, and pattern recognition. This work presents a geometric approach to DR termed Polygonal Coordinate System (PCS), capable of representing multidimensional data in two or three dimensions while preserving their inherent overall structure by taking advantage of a polygonal interface bridging high- and low-dimensional spaces. PCS can handle Big Data by adopting an incremental, geometric DR with linear-time complexity. A new version of t-Distributed Stochastic Neighbor Embedding (t-SNE), a state-of-the-art algorithm for DR, is also provided. It employs a PCS-based deterministic strategy and is named t-Distributed Deterministic Neighbor Embedding (t-DNE). Several synthetic and real data sets were used as well-known real-world problem archetypes in our benchmark, providing a means to evaluate PCS and t-DNE against four embedding-based DR algorithms: two linear-transformation ones (Principal Component Analysis and Non-negative Matrix Factorization) and two nonlinear ones (t-SNE and Sammon’s Mapping). Statistical comparisons of the execution times of these algorithms, by the Friedman’s significance test, highlight the efficiency of PCS in data embedding. PCS tends to surpass its counterparts in several aspects explored in this work, including asymptotic time and space complexity, preservation of global data-inherent structures, number of hyperparameters, and applicability to unobserved data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001822",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Big data",
      "Computer science",
      "Data mining",
      "Data structure",
      "Dimensionality reduction",
      "Embedding",
      "Geometric data analysis",
      "Principal component analysis",
      "Programming language",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Flexa",
        "given_name": "Caio"
      },
      {
        "surname": "Gomes",
        "given_name": "Walisson"
      },
      {
        "surname": "Moreira",
        "given_name": "Igor"
      },
      {
        "surname": "Alves",
        "given_name": "Ronnie"
      },
      {
        "surname": "Sales",
        "given_name": "Claudomiro"
      }
    ]
  },
  {
    "title": "Hand gesture recognition via enhanced densely connected convolutional neural network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114797",
    "abstract": "Hand gesture recognition (HGR) serves as a fundamental way of communication and interaction for human being. While HGR can be applied in human computer interaction (HCI) to facilitate user interaction, it can also be utilized for bridging the language barrier. For instance, HGR can be utilized to recognize sign language, which is a visual language represented by hand gestures and used by the deaf and mute all over the world as a primary way of communication. Hand-crafted approach for vision-based HGR typically involves multiple stages of specialized processing, such as hand-crafted feature extraction methods, which are usually designed to deal with particular challenges specifically. Hence, the effectiveness of the system and its ability to deal with varied challenges across multiple datasets are heavily reliant on the methods being utilized. In contrast, deep learning approach such as convolutional neural network (CNN), adapts to varied challenges via supervised learning. However, attaining satisfactory generalization on unseen data is not only dependent on the architecture of the CNN, but also dependent on the quantity and variety of the training data. Therefore, a customized network architecture dubbed as enhanced densely connected convolutional neural network (EDenseNet) is proposed for vision-based hand gesture recognition. The modified transition layer in EDenseNet further strengthens feature propagation, by utilizing bottleneck layer to propagate the features being reused to all the feature maps in a bottleneck manner, and the following Conv layer smooths out the unwanted features. Differences between EDenseNet and DenseNet are discerned, and its performance gains are scrutinized in the ablation study. Furthermore, numerous data augmentation techniques are utilized to attenuate the effect of data scarcity, by increasing the quantity of training data, and enriching its variety to further improve generalization. Experiments have been carried out on multiple datasets, namely one NUS hand gesture dataset and two American Sign Language (ASL) datasets. The proposed EDenseNet obtains 98.50% average accuracy without augmented data, and 99.64% average accuracy with augmented data, outperforming other deep learning driven instances in both settings, with and without augmented data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002384",
    "keywords": [
      "Artificial intelligence",
      "Bottleneck",
      "Bridging (networking)",
      "Computer network",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Embedded system",
      "Feature (linguistics)",
      "Feature extraction",
      "Generalization",
      "Gesture",
      "Gesture recognition",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Sign language",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Tan",
        "given_name": "Yong Soon"
      },
      {
        "surname": "Lim",
        "given_name": "Kian Ming"
      },
      {
        "surname": "Lee",
        "given_name": "Chin Poo"
      }
    ]
  },
  {
    "title": "Risk-averse supplier selection and order allocation in the centralized supply chains under disruption risks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114691",
    "abstract": "This paper proposes a mixed-integer non-linear programming (MINLP) model for the integrated supplier selection and order allocation in a centralized supply chain considering the disruption risks and a risk-averse decision-maker. In order to capture a realistic scenario of considering the geographical characteristics of the suppliers, we assume that the suppliers belong to two regions: the buyer’s region (domestic suppliers) and outside of the buyer’s region (foreign suppliers). Considering this realistic feature, the supply chain might face two types of disruption risk: first, local disruption risks which might uniquely occur inside each supplier such as equipment breakdowns, and second, regional disruption risks that might occur in the region of the suppliers located in the same geographical region such as natural hazards. We formulate the problem considering a risk-neutral decision-maker as a benchmark, and then a risk-averse model is presented. In the latter case, we apply two types of risk assessment tools introduced in the finance literature to analyze the decision maker’s behavior: value-at-risk (VaR) and conditional value-at-risk (CVaR). We show that developed models are non-convex programming, and therefore, we apply the particle swarm optimization (PSO) algorithm as the solution approach. We also compare the developed PSO algorithm with the Genetic algorithm (GA) and the commercial GAMS solver to verify the efficiency of the solution method. The computational experiments indicate the impact of the decision maker’s attitude on the supplier selection and the order quantity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001329",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Business",
      "CVAR",
      "Computer science",
      "Decision maker",
      "Expected shortfall",
      "Finance",
      "Geodesy",
      "Geography",
      "Marketing",
      "Mathematical optimization",
      "Mathematics",
      "Newsvendor model",
      "Operations research",
      "Order (exchange)",
      "Particle swarm optimization",
      "Risk management",
      "Selection (genetic algorithm)",
      "Solver",
      "Supply chain"
    ],
    "authors": [
      {
        "surname": "Esmaeili-Najafabadi",
        "given_name": "Elham"
      },
      {
        "surname": "Azad",
        "given_name": "Nader"
      },
      {
        "surname": "Saber Fallah Nezhad",
        "given_name": "Mohammad"
      }
    ]
  },
  {
    "title": "A gradient-based method for multilevel thresholding",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114845",
    "abstract": "At present, the research about fast algorithm for multi-threshold image segmentation is mostly focused on bionic algorithm. Many of bionic algorithms, such as particle swarm optimization algorithm (PSO), artificial bee colony algorithm (ABC), bat algorithm (BA), firefly algorithm (FA) and grey wolf optimization algorithm (GWO), have achieved good results and are being further improved. In this paper, a gradient-based method different from bionic algorithm is proposed to search optimal multilevel thresholds. For most realistic images, the objective functions in mostly-used multi-threshold methods, such as Kapur’s entropy and Otsu function, have good convexity. On this basis, gradient descent method can be used to solve the optimization problem more simply and directly. Based on gradient descent method, our method is tailored for discrete objection function, and the process of searching optimal threshold is divided into two stages, which makes algorithm convergence efficient and accurate. The multi-thresholding experiments of above mentioned algorithms have been conducted, whose results show that the gradient-based method is efficient in computation and has equal or even better performance of segmentation than other algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002864",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Firefly algorithm",
      "Gradient descent",
      "Gradient method",
      "Image (mathematics)",
      "Image segmentation",
      "Mathematical optimization",
      "Mathematics",
      "Particle swarm optimization",
      "Thresholding"
    ],
    "authors": [
      {
        "surname": "Shang",
        "given_name": "Caijie"
      },
      {
        "surname": "Zhang",
        "given_name": "Dong"
      },
      {
        "surname": "Yang",
        "given_name": "Yan"
      }
    ]
  },
  {
    "title": "Improved distance estimation with node selection localization and particle swarm optimization for obstacle-aware wireless sensor networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114773",
    "abstract": "Sensor-node localization is among the greatest concerns in the field of wireless sensor networks. Range-based localization techniques generally outperform range-free techniques, particularly in terms of their accuracy. Range-based localization techniques depend on a popular distance estimation method, which requires conversion from a received signal strength indicator to distances. In a case where sensor nodes are in an area with obstacles, direct communication between certain pairs of nodes is impracticable; the data must be relayed over multihop (or detour) routes. One promising approach to improve the accuracy of sensor-node distance estimation is to segment (or cluster) sensor nodes to a restricted set of anchor nodes whose estimated distances to unknown nodes are not on a detour route. Some certain topologies can decrease the localization precision; e.g., when each group’s node density is low, large empty spaces (or gaps) might affect the localization precision. If an unknown node is close to another group, using only anchor nodes within its own group could reduce the estimation precision. When anchor nodes within the same group lie along a straight line, the approximation of the unknown-node location could be misinterpreted. Thus, to enhance the localization precision, we make use of anchor nodes in other nearby groups to estimate the locations of unknown nodes. We also apply particle swarm optimization (PSO) with an improved fitness function to estimate the locations of unknown nodes. The localization performance is intensively evaluated in obstacle-prone scenarios. The simulation results show that the proposed scheme achieves higher accuracy than recent state-of-the-art PSO-based methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002141",
    "keywords": [
      "Aerospace engineering",
      "Algorithm",
      "Combinatorics",
      "Computer network",
      "Computer science",
      "Engineering",
      "Key distribution in wireless sensor networks",
      "Law",
      "Mathematics",
      "Node (physics)",
      "Obstacle",
      "Particle swarm optimization",
      "Political science",
      "Programming language",
      "Range (aeronautics)",
      "Real-time computing",
      "Sensor node",
      "Set (abstract data type)",
      "Structural engineering",
      "Telecommunications",
      "Topology (electrical circuits)",
      "Wireless",
      "Wireless network",
      "Wireless sensor network"
    ],
    "authors": [
      {
        "surname": "Phoemphon",
        "given_name": "Songyut"
      },
      {
        "surname": "So-In",
        "given_name": "Chakchai"
      },
      {
        "surname": "Leelathakul",
        "given_name": "Nutthanon"
      }
    ]
  },
  {
    "title": "Efficient deep feature extraction and classification for identifying defective photovoltaic module cells in Electroluminescence images",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114810",
    "abstract": "Electroluminescence (EL) imaging has become the standard test procedure for defect detection throughout the production, installation and operation stages of solar modules. Using this test, defects such as micro cracks, broken cells, and finger interruptions on photovoltaic modules could be easily detected and potential power loss issues could be effectively addressed. Although EL test is a very powerful inspection method, interpreting the EL images could be quite challenging due to the inhomogeneous background and complex defect patterns. Therefore, evaluating the damaged cells and determining the defect severity require expertise, and could be time consuming to apply these processes manually for each cell. Hence, the automatic visual inspection of photovoltaic cells is very important. In this study, a novel automatic defect detection and classification framework for solar cells is proposed. In the proposed Deep Feature-Based (DFB) method, the image features extracted through deep neural networks are classified with machine learning methods such as support vector machines, K-Nearest Neighborhood, Decision Tree, Random Forest and Naive Bayes. Thus, classical machine learning and deep learning techniques are used together. In order to combine the features taken from different deep network architectures in various combinations, the minimum Redundancy Maximum Relevance (mRMR) algorithm is employed for the feature selection. In this way, the dimensions of the feature vectors are reduced and the classification performance is increased with fewer features. With the determination of the best features extracted from different layers of deep neural networks, state-of-the-art results were obtained for both 4-class and 2-class datasets. Moreover, a Lightweight Convolutional Neural Network (L-CNN) architecture has been proposed and trained from scratch, and the results are compared with previous works. As a result, the highest scores are obtained using DFB method with Support Vector Machines (SVM) and classification scores of 90.57% and 94.52% were obtained for the dataset with 4 - class and 2 - class, respectively. The proposed DFB-SVM models outperformed other studies using the same dataset. The results showed that the proposed framework can detect PV cell defects with high accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002517",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Decision tree",
      "Deep learning",
      "Electrical engineering",
      "Engineering",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature selection",
      "Linguistics",
      "Machine learning",
      "Naive Bayes classifier",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Photovoltaic system",
      "Random forest",
      "Redundancy (engineering)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Demirci",
        "given_name": "Mustafa Yusuf"
      },
      {
        "surname": "Beşli",
        "given_name": "Nurettin"
      },
      {
        "surname": "Gümüşçü",
        "given_name": "Abdülkadir"
      }
    ]
  },
  {
    "title": "A new intelligent MCDM model for HCW management: The integrated BWM–MABAC model based on D numbers",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114862",
    "abstract": "Healthcare waste (HCW) management is a complex and challenging problem. It is one of the priorities in health. An increase in the number of the health services provided leads to an increase in the amount of HCW, which has particularly been noticeable in recent years. Since this is the waste that may pose a risk to humans and the environment, it is necessary to ensure an adequate treatment of the same. HCW management is particularly important in developing countries, due to inappropriate disposal methods, underfunding and a lack of the infrastructure. In order to achieve the cost-effectiveness and sustainability of this area, HCW should be minimized through an adequate treatment of the same. The Public Enterprise Zdravstvo Brčko (Brčko Health System) has intensively been addressing the HCW management issue. They have decided to upgrade the HCW system by purchasing a new infectious waste treatment facility. The paper is aimed at creating a new original integrated multicriteria decision-making model based on D numbers for processing fuzzy linguistic information. This model will serve to support management in the procurement of the mentioned facility. The model integrates the benefits of different approaches and theories. An initial model was formed, consisting of the six potential solutions evaluated based on the 18 criteria classified into the following four groups: social, environmental, economic and technological. Four experts in this field evaluated the criteria and potential solutions. Then, a new Best-Worst Method based on D numbers (BWM-D) was applied in order to determine the significance of the criteria. After that, a Multi-Attributive Border Approximation Area Comparison Based on D numbers (MABAC-D) was developed and applied so as to evaluate and select an infectious waste treatment facility. The results have shown that the alternative A1 gives the best results, whereas the alternative A5 shows the worst results. Finally, a sensitivity analysis was performed to validate the obtained results. In this part of the paper, the Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS-D) and VIšekriterijumska Optimizacija Kompromisnog Rešenja (VIKOR-D) were developed in order to validate the results. When procuring a new Contagious Waste Treatment System, the characteristics of the available devices need be perceived and all the criteria need be taken into account in order to provide a device which will solve the HCW problem in the best way. This paper has shown how D numbers can be used when making a selection of an HCW management device, and also how all the characteristics of such a device can be perceived and how the device demonstrating the best characteristics can be selected.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003031",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Business",
      "Computer science",
      "Decision support system",
      "Ecology",
      "Engineering",
      "Finance",
      "Marketing",
      "Multiple-criteria decision analysis",
      "Operating system",
      "Operations management",
      "Operations research",
      "Order (exchange)",
      "Procurement",
      "Purchasing",
      "Risk analysis (engineering)",
      "Sustainability",
      "Upgrade"
    ],
    "authors": [
      {
        "surname": "Pamučar",
        "given_name": "Dragan"
      },
      {
        "surname": "Puška",
        "given_name": "Adis"
      },
      {
        "surname": "Stević",
        "given_name": "Željko"
      },
      {
        "surname": "Ćirović",
        "given_name": "Goran"
      }
    ]
  },
  {
    "title": "Anomaly detection in multivariate streaming PMU data using density estimation technique in wide area monitoring system",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114865",
    "abstract": "In recent years, the advancement in technology and the availability of a large number of sensors has led to a huge amount of real-time streaming data.The real-time data measured by Phasor Measurement Units (PMU), are of streaming data type and their analysis can identify the status of the power system failure which leads to system instability.Hence online monitoring of PMU data is extremely important which includes preprocessing, filtering, and analysis before taking any corrective actions in the power system.One of the crucial analyses that can be performed over PMU data is the identification of anomalies that arise due to several factors. Early detection of anomalies in the PMU streaming data is highly useful though in real-time it is difficult to detect anomalies consistently.This paper proposes a novel framework for detecting anomalies in multivariate streaming PMU data based on the Density Estimation technique. The main novelty of this paper is that the proposed model is suitable for any kind of anomaly detection in multivariate streaming PMU data which is not addressed in literatures. The proposed system implements Principal Component Analysis in streaming real-time data to select the relevant features. The K-Means algorithm is performed over the selected feature vectors to find the number of clusters in the streaming PMU data. The Gaussian Mixture Model (GMM) is developed to represent the normal PMU data of the system. The minimum and maximum value of the probability of the training samples being generated by the model is selected as a minimum and maximum threshold. The samples with probability less than the minimum threshold or greater than the maximum threshold are considered as anomalies. The main advantage of this proposed model is that it can perform anomaly detection in multivariate streaming PMU data even online within the specified window. The proposed system can identify real time anomalies of multivariate dataset even by offline training model of GMM. The proposed model is developed in MATLAB 2018b version and is tested in streaming PMU data both for offline and online datasets. The performance of the proposed model is validated using performance metrics and the results show that F1 score is 0.95 for 2 features, 0.91 for 3 features, and 0.99 for 8, 12 and 16 features and 1 for 10 features in offline mode. The performance evaluation of testing data in online mode for 2, 3, 8, 10, 12 and 16 features are obtained which proves that the system is adapting to the streaming PMU data with reduced false positive rate and F1 score of 1 for 8, 10, 12 and 16 features. The experimental results prove that the proposed framework for anomaly detection in streaming data is more effective and accurate in identifying the anomalies in real time PMU streamed dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003067",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Data pre-processing",
      "Electric power system",
      "Machine learning",
      "Mixture model",
      "Multivariate statistics",
      "Novelty",
      "Novelty detection",
      "Phasor",
      "Phasor measurement unit",
      "Philosophy",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Real-time computing",
      "Streaming data",
      "Theology"
    ],
    "authors": [
      {
        "surname": "Amutha",
        "given_name": "A L"
      },
      {
        "surname": "Annie Uthra",
        "given_name": "R"
      },
      {
        "surname": "Preetha Roselyn",
        "given_name": "J"
      },
      {
        "surname": "Golda Brunet",
        "given_name": "R"
      }
    ]
  },
  {
    "title": "The matching scarcity problem: When recommenders do not connect the edges in recruitment services",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114764",
    "abstract": "Connecting candidates and jobs to promote real placement opportunities is one of the most impacting and challenging scenarios for Recommender Systems (RSs). A major concern when building RSs for recruitment services is ensuring placement opportunities for all candidates and jobs as soon as possible, avoiding financial losses for both sides. We refer to these scenarios where candidates or jobs suffer from the absence of matching in the system as the Matching Scarcity Problem (MaSP). This paper introduces the MaSP, discussing the reasons we consider it as a recurring threat to recruitment services and presenting novel strategies to identify, characterize, and mitigate the MaSP on real scenarios. Experimental assessments on real data from Catho, the leading recruitment company in Latin America, evinced that curricula tend to be more poorly written than job descriptions, exhibiting several irrelevant pieces of information. Replacing these pieces properly by useful ones reduces up to 50% the number of curricula and jobs that suffer from MaSP.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002050",
    "keywords": [
      "Computer science",
      "Curriculum",
      "Data science",
      "Economic growth",
      "Economics",
      "Matching (statistics)",
      "Mathematics",
      "Microeconomics",
      "RSS",
      "Recommender system",
      "Scarcity",
      "Statistics",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Cardoso",
        "given_name": "Alan"
      },
      {
        "surname": "Mourão",
        "given_name": "Fernando"
      },
      {
        "surname": "Rocha",
        "given_name": "Leonardo"
      }
    ]
  },
  {
    "title": "A league-winner algorithm for defect classification in an industrial web inspection system",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114753",
    "abstract": "This paper presents a modification to be added to multiclass classifiers, that improves their performance when classifying, in this case, defects appearing in polyethylene films. It aims to classify a new defect by confronting every defect type against each of the other types. In a simplified way, the type that results winner in more matches is the type that the defect belongs to. Different ways of implementing neural networks have been tested, using Gradient Descent and techniques for backpropagation. These techniques have been formally and understandably explained. In addition, a method based on decision trees has been included for comparison. Different issues related to the practical implementation of the detection and identification system within an installed production chain are addressed. The resulting system has been incorporated as a real inspection automatism in a polyethylene manufacturing line, and trained with defects previously obtained from the same line.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001949",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Backpropagation",
      "Biology",
      "Botany",
      "Computer science",
      "Engineering",
      "Geometry",
      "Gradient descent",
      "Identification (biology)",
      "Line (geometry)",
      "Machine learning",
      "Mathematics",
      "Mechanical engineering",
      "Pattern recognition (psychology)",
      "Production line"
    ],
    "authors": [
      {
        "surname": "Gonzalez-Rodriguez",
        "given_name": "Angel Gaspar"
      },
      {
        "surname": "Gonzalez-Rodriguez",
        "given_name": "Antonio"
      },
      {
        "surname": "Castillo-Garcia",
        "given_name": "Fernando Jose"
      }
    ]
  },
  {
    "title": "Efficient influence spread management via budget allocation at community scale",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114814",
    "abstract": "Given a social network, the classical influence maximization (IM) and misinformation prevention (MP) problems both adopt similar seed triggering models, i.e., convincing k specific users to become seed nodes by material incentive (e.g. free products). However, in real life, those chosen seeds may not be willing to spread information as expected, which will affect the final diffusion. Instead of convincing one single user, we can target a user community, in the hope that some of them may turn into propagation seeds voluntarily. This community-based seed (Com-seed) triggering model can be used in real-world applications such as distributing flyers or offering discounts in local communities, where the objective is to maximize the promotion effect with the given budget constraints. In this paper, we aim to maximize the influence or minimize the misinformation spread by finding an optimal community-based budget allocation under the Com-seed triggering models. We present new formulations of the influence maximization and misinformation prevention problems from the community perspective and design effective and scalable algorithms to solve these new problems. With intricately designed community-based sampling schemes and approximation guarantee of the greedy approach over integer lattice, our algorithms can achieve ( 1 - 1 / e - ∊ ) -approximation results. Experiments show our methods outperform all baselines and run faster than the state-of-the-art methods in both influence maximization and misinformation prevention problems, which demonstrate the effectiveness and scalability of the proposed algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002554",
    "keywords": [
      "Algorithm",
      "Budget constraint",
      "Computer science",
      "Computer security",
      "Database",
      "Economics",
      "Greedy algorithm",
      "Incentive",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Microeconomics",
      "Misinformation",
      "Scalability"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Can"
      },
      {
        "surname": "Zhang",
        "given_name": "Yangguang"
      },
      {
        "surname": "Shi",
        "given_name": "Qihao"
      },
      {
        "surname": "Feng",
        "given_name": "Yan"
      },
      {
        "surname": "Chen",
        "given_name": "Chun"
      }
    ]
  },
  {
    "title": "A sequence-based and context modelling framework for recommendation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114665",
    "abstract": "Since the last decade, data collection is becoming more pervasive, passive and easier to perform. This is resulting in the rise of data wherein a user performs some activities in a sequence, such as locations visited, physical activities performed, and modes of transport taken. In such cases, activities are often performed in a particular order, and each activity in turn may influence the subsequent activities to be performed. Moreover, such activities may be associated with multiple features or contexts, such as location, time, weather, etc. The order encoded in such data, along with the context, capture important information when it comes to modelling the preferences and personal habits of users. Traditional recommender systems, however, typically do not consider the order in which users perform activities and there is little work which considers both sequence and context simultaneously. In this work, a generic recommendation framework is proposed which leverages both sequences and context in user activity data for activity recommendation. To model user activities, a semantic view of the user’s past activities as a timeline of activity objects is presented. An essential step in the recommendation process is finding patterns in past activities performed which are closely aligned to the recent activities undertaken by the user. To calculate the distance between timelines, a novel two-level distance metric is presented which calculates distance with respect to the order of the activities as well as the context features associated with each activity occurrence. The efficacy of the proposed activity recommendation framework in various recommendation scenarios, is demonstrated using real-world datasets from multiple domains.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001068",
    "keywords": [
      "Activity recognition",
      "Archaeology",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Data science",
      "Economics",
      "Finance",
      "Genetics",
      "History",
      "Human–computer interaction",
      "Image (mathematics)",
      "Information retrieval",
      "Metric (unit)",
      "Operating system",
      "Operations management",
      "Order (exchange)",
      "Paleontology",
      "Process (computing)",
      "Recommender system",
      "Sequence (biology)",
      "Similarity (geometry)",
      "Timeline",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Kumar",
        "given_name": "Gunjan"
      },
      {
        "surname": "Jerbi",
        "given_name": "Houssem"
      },
      {
        "surname": "O’Mahony",
        "given_name": "Michael P."
      }
    ]
  },
  {
    "title": "Dynamic ticket pricing of airlines using variant batch size interpretable multi-variable long short-term memory",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114794",
    "abstract": "Research of airlines shows that seat inventory control and therefore, revenue management is based not on a systematic analysis but more on human judgement. Machine learning models have been developed and applied to support decisions for ticket pricing dynamically. However, conventional models and approaches yield low statistical evaluation scores. In this study, the features used in other studies were explored and the cost available seat kilometer (CASK) value and target revenue features were included for the first time to the best of our knowledge which are essential components of ticket price decision. Real data from a low-cost carrier airline in Turkey were collected and the observation data were splitted into two to study with the highest profit sale data. Then the outliers were filtered to let the models learn from and generate better price offerings businesswise. Observation datasets obtained in each step were recorded to be tested. 7 different model techniques were simulated and tested with 4 different datasets according to 6 different statistical evaluation criteria. A new approach to Interpretable Multi-Variable Long Short-Term Memory (IMV-LSTM) model was proposed by taking every flight and its sales as an independent series, that is to assign a dynamic batch size. Extensive experiments on real datasets reveal enhanced statistical evaluation scores by using the proposed approach and model. The proposed model can be used by the airlines to mitigate human judgement on ticket pricing, to manage their price offerings to reach their target revenues and to increase their profits. The model can be used by other business cases that have similar historical data as overlapping windows structure.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002359",
    "keywords": [
      "Business",
      "Computer science",
      "Computer security",
      "Data mining",
      "Dynamic pricing",
      "Econometrics",
      "Economics",
      "Finance",
      "Judgement",
      "Law",
      "Marketing",
      "Mathematical analysis",
      "Mathematics",
      "Microeconomics",
      "Operations research",
      "Physics",
      "Political science",
      "Profit (economics)",
      "Quantum mechanics",
      "Revenue",
      "Revenue management",
      "Term (time)",
      "Ticket",
      "Variable (mathematics)"
    ],
    "authors": [
      {
        "surname": "Koc",
        "given_name": "Ismail"
      },
      {
        "surname": "Arslan",
        "given_name": "Emel"
      }
    ]
  },
  {
    "title": "An improved bat algorithm hybridized with extremal optimization and Boltzmann selection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114812",
    "abstract": "As a meta-heuristic algorithm, bat algorithm (BA) is based on the characteristics of bat-based echolocation and has been widely used in various aspects of optimization problems since it appeared. However, the original BA still has many shortcomings, such as insufficient local search ability, lack of diversity and poor performance on high-dimensional optimization problems. To overcome these weaknesses, this paper proposes an improved BA with extremal optimization (EO) algorithm (IBA-EO) to improve the performance of BA. In IBA-EO, an improved update strategy is proposed to obtain the solutions generating from the random selected bats to enhance the global search capability. The exploitation ability is improved by EO algorithm with excellent local search capability. Furthermore, Boltzmann selection and a monitor mechanism are employed to keep suitable balance between exploration ability and exploitation ability. To testify the performance of IBA-EO in handling various optimization problems, this study considers four groups of contrast experiments. Extensive simulation results demonstrate that IBA-EO can achieve a strong competitive performance by comparing with other fifteen well-established algorithms in terms of accuracy, reliability and statistical tests.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002530",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bat algorithm",
      "Computer science",
      "Extremal optimization",
      "Heuristic",
      "Local search (optimization)",
      "Mathematical optimization",
      "Mathematics",
      "Meta-optimization",
      "Optimization algorithm",
      "Optimization problem",
      "Particle swarm optimization",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Reliability (semiconductor)",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Min-Rong"
      },
      {
        "surname": "Huang",
        "given_name": "Yi-Yuan"
      },
      {
        "surname": "Zeng",
        "given_name": "Guo-Qiang"
      },
      {
        "surname": "Lu",
        "given_name": "Kang-Di"
      },
      {
        "surname": "Yang",
        "given_name": "Liu-Qing"
      }
    ]
  },
  {
    "title": "Picture fuzzy extension of the CODAS method for multi-criteria vehicle shredding facility location",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114644",
    "abstract": "An emerging question for waste managers is how to determine the best vehicle shredding facility location from a finite set of available alternatives under numerous conflicting criteria as well as high levels of imprecise, vague, and uncertain information. For the first time, we investigate the vehicle shredding facility location problem via the picture fuzzy sets (PFSs), which show a great power in capturing ambiguous, uncertain, and vague information, and mitigating information loss. This paper aims to exploit PFSs and develop a novel picture fuzzy COmbinative Distance-Based ASsessment (CODAS) method for multi-criteria vehicle shredding facility location. The developed method is applied to a real-life case study for locating a new vehicle shredding facility in the Republic of Serbia. The results show that “Bor” is the best alternative among six possible alternative locations. In the decision-making process, four main criteria, such as economical, environmental, social, and technical, and 23 sub-criteria are considered. The robustness of the proposed method is validated by comparing its results with the outcomes of the PFS based TOPSIS, EDAS, TODIM, VIKOR, MABAC, Cross-entropy, Projection, Grey relational projection, and Grey relational analysis methods. The ranking similarity between the proposed picture fuzzy CODAS method and the available state-of-the-art PFS based methods is checked by applying the Spearman's rank correlation coefficient, in which 90% of rankings are matched. The results of the comparative and sensitivity analyses showed that the proposed method generates highly robust outcomes. The formulated picture fuzzy CODAS method can help waste managers to more naturally express their preferences by voting and identify the best facility location. Besides, it can be used to solve any other MCDM problem under the picture fuzzy environment.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000853",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Data mining",
      "Entropy (arrow of time)",
      "Exploit",
      "Fuzzy logic",
      "Fuzzy set",
      "Gene",
      "Grey relational analysis",
      "Mathematics",
      "Operations research",
      "Physics",
      "Quantum mechanics",
      "Ranking (information retrieval)",
      "Robustness (evolution)",
      "Statistics",
      "TOPSIS"
    ],
    "authors": [
      {
        "surname": "Simic",
        "given_name": "Vladimir"
      },
      {
        "surname": "Karagoz",
        "given_name": "Selman"
      },
      {
        "surname": "Deveci",
        "given_name": "Muhammet"
      },
      {
        "surname": "Aydin",
        "given_name": "Nezir"
      }
    ]
  },
  {
    "title": "MCHT: A maximal clique and hash table-based maximal prevalent co-location pattern mining algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114830",
    "abstract": "Co-location patterns refer to subsets of Boolean spatial features with instances of these features frequently appear in nearby geographic space. Maximal co-location patterns are a compact representation of these patterns that lead users more easily to absorb results and make meaningful inferences. The current algorithms for maximal co-location pattern mining are based on a generate-test candidate model. The main execution time of this model is occupied by collecting co-location instances of candidates, which makes discovering maximal co-location patterns is still very challenging when data is big and/or dense. To take up the challenge, a novel maximal co-location pattern mining framework based on maximal cliques and hash tables (MCHT) is developed in this study. First, all maximal cliques that can compactly represent neighbor relationships between instances of a spatial data set are enumerated. The advantages of bit string operations are fully utilized to speed up the process of enumerating maximal cliques. Next, a participating instance hash table structure is constructed based on these maximal cliques. Then information about the co-location instances of maximal patterns can be queried and collected efficiently from the hash table. After that, by calculating participation indexes of these patterns to measure their prevalence, maximal prevalent co-location patterns can be filtered efficiently. Finally, a series of experiments is conducted on both synthetic and real-facility data sets to demonstrate that the proposed algorithm can efficiently reduce both the computational time and the memory consumption compared with the existing algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002712",
    "keywords": [
      "Algorithm",
      "Clique",
      "Combinatorics",
      "Computer science",
      "Computer security",
      "Data mining",
      "Data structure",
      "Hash function",
      "Hash table",
      "Mathematics",
      "Programming language",
      "Set (abstract data type)",
      "Table (database)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Tran",
        "given_name": "Vanha"
      },
      {
        "surname": "Wang",
        "given_name": "Lizhen"
      },
      {
        "surname": "Chen",
        "given_name": "Hongmei"
      },
      {
        "surname": "Xiao",
        "given_name": "Qing"
      }
    ]
  },
  {
    "title": "A combinatorial evolutionary algorithm for unrelated parallel machine scheduling problem with sequence and machine-dependent setup times, limited worker resources and learning effect",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114843",
    "abstract": "The existing papers on unrelated parallel machine scheduling problem with sequence and machine-dependent setup times (UPMSP-SMDST) ignore the worker resources and learning effect. Given the influence and potential of human factors and learning effect in real production systems to improve production efficiency and decrease production cost, we propose a UPMSP-SMDST with limited worker resources and learning effect (NUPMSP). In the NUPMSP, the workers have learning ability and are categorized to different skill levels, i.e., a worker’s skill level for a machine is changing with his accumulating operation times on the same machine. A combinatorial evolutionary algorithm (CEA) which integrates a list scheduling (LS) heuristic, the shortest setup time first (SST) rule and an earliest completion time first (ECT) rule is presented to solve the NUPMSP. In the experimental phase, 72 benchmark instances of NUPMSP are constructed to test the performance of the CEA and facilitate future study. The Taguchi method is used to obtain the best combination of key parameters of the CEA. The effectiveness of the LS, SST and ECT is verified based on 15 benchmark instances. Extensive experiments conducted to compare the CEA with some well-known algorithms confirm that the proposed CEA is superior to these algorithms in terms of solving accuracy and efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002840",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Economics",
      "Evolutionary algorithm",
      "Genetics",
      "Job shop scheduling",
      "Learning effect",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Microeconomics",
      "Operating system",
      "Schedule",
      "Scheduling (production processes)",
      "Sequence (biology)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Like"
      },
      {
        "surname": "Deng",
        "given_name": "Qianwang"
      },
      {
        "surname": "Lin",
        "given_name": "Ruihang"
      },
      {
        "surname": "Gong",
        "given_name": "Guiliang"
      },
      {
        "surname": "Han",
        "given_name": "Wenwu"
      }
    ]
  },
  {
    "title": "Discriminant analysis based on reliability of local neighborhood",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114790",
    "abstract": "To obtain a compact and effective low-dimensional representation, recently, most existing discriminant manifold learning methods have integrated manifold learning into discriminant analysis (DA) for extracting the intrinsic structure of data. These methods learn two kinds of adjacency graphs, such as intrinsic graph and penalty graph, to characterize the similarity between samples from intraclass and the pseudo similarity of interclass. However, they treat every sample equally, which results in the following defects: (1) These methods cannot accurately characterize the marginal region among different classes only through penalty graphs. (2) They can not identify the noisy and outlier samples which reduce the robustness of these methods. To address these problems, we introduce an adaptive adjacency factor to perform the discriminative based reliability analysis for each sample. By integrating the adjacency factor into discriminant manifold learning methods, we propose a novel method for DA namely discriminant analysis based on reliability of local neighborhood (DA-RoLN). We mainly have three contributions in this paper: (1) By the introduction of adjacency factor, sample points can be divided into three parts: intraclass samples, marginal samples, and outliers. Therefore, DA-RoLN emphasizes the effect of valid samples and filters the influence of outliers. (2) We adaptively calculate the adjacency factor in low-dimensional space, thus, the margin between different classes in low-dimensional space is emphasized. (3) An iterative algorithm is developed to solve the objective function of DA-RoLN, and it is easy to solve with a low computational cost. Extensive experimental results show the effectiveness of DA-RoLN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002311",
    "keywords": [
      "Adjacency list",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Dimensionality reduction",
      "Discriminant",
      "Discriminative model",
      "Linear discriminant analysis",
      "Machine learning",
      "Mathematics",
      "Nonlinear dimensionality reduction",
      "Outlier",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Yunlong"
      },
      {
        "surname": "Zhang",
        "given_name": "Yisong"
      },
      {
        "surname": "Pan",
        "given_name": "Jinyan"
      },
      {
        "surname": "Luo",
        "given_name": "Sizhe"
      },
      {
        "surname": "Yang",
        "given_name": "Chengyu"
      }
    ]
  },
  {
    "title": "Neural network modeling of consumer satisfaction in mobile commerce: An empirical analysis",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114803",
    "abstract": "The mobile commerce (m-commerce) industry has rapidly grown in value in recent years, as has the number of m-commerce service providers and interest in it from consumers and academia alike. In order to ensure customer loyalty, providers must determine which factors influence consumer satisfaction in m-commerce. Therefore, the objective of this study is to determine and rank the significant predictors of satisfaction in m-commerce. The paper also develops a procedure for artificial neural network model design and parameter setting in technology acceptance studies. Data was collected from 224 users of m-commerce services. The results presented are based on a combination of structural equation modeling (SEM) and artificial neural network (ANN) analyses. A multi-layer perceptron was used for ANN modeling. The results show that the optimal ANN model has one hidden layer and a sigmoid as an activation function in both layers, while the number of hidden nodes should be determined using a recommended rule-of-thumb. In addition, mobility and trust were found to be the most significant determinants of consumer satisfaction in m-commerce. The results of the study are significant as they have important implications for both academia and companies, due to the fact that some of the factors investigated in the study, such as mobility, have rarely been explored in previous consumer satisfaction studies, but were proved to be very significant. Another important result of the study is the proposal of a detailed procedure of ANN model design and the recommendations made for the selection of ANN model architecture and parameter settings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100244X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Business",
      "Computer science",
      "Customer satisfaction",
      "Data mining",
      "Finance",
      "Loyalty",
      "Loyalty business model",
      "Machine learning",
      "Marketing",
      "Mobile commerce",
      "Multilayer perceptron",
      "Order (exchange)",
      "Service (business)",
      "Service quality",
      "Value (mathematics)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Kalinić",
        "given_name": "Zoran"
      },
      {
        "surname": "Marinković",
        "given_name": "Veljko"
      },
      {
        "surname": "Kalinić",
        "given_name": "Ljubina"
      },
      {
        "surname": "Liébana-Cabanillas",
        "given_name": "Francisco"
      }
    ]
  },
  {
    "title": "A hybrid method with dynamic weighted entropy for handling the problem of class imbalance with overlap in credit card fraud detection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114750",
    "abstract": "Class imbalance with overlap is a very challenging problem in electronic fraud transaction detection. Fraudsters have racked their brains to make a fraud transaction as similar as a genuine one in order to avoid being found. Therefore, lots of data of fraud transactions overlap with genuine transactions so that it is hard to distinguish them. However, most attention has been focused on class imbalance rather than overlapping issues for machine-learning-based methods of fraud transaction detection. This paper proposes a novel hybrid method to handle the problem of class imbalance with overlap based on a divide-and-conquer idea. Firstly, an anomaly detection model is trained on the minority samples for excluding both a few outliers of minority class and lots of majority samples from the original dataset. Then the remaining samples form an overlapping subset that has a low imbalance ratio and a reduced learning interference from both minority class and majority class than the original dataset. After that, this difficult overlapping subset is dealt with a non-linear classifier in order to distinguish them well. To achieve good properties of the overlapping subset, we propose a novel assessment criterion, Dynamic Weighted Entropy (DWE), to evaluate its quality. It is a specially designed trade-off between the number of excluded outliers of minority class and the ratio of class imbalance of overlapping subset. With the help of DWE, time consumption on searching good hyper-parameters is dramatically declined. Extensive experiments on Kaggle fraud detection dataset and a large real electronic transaction dataset demonstrate that our method significantly outperforms state-of-the-art ones.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001913",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Credit card",
      "Credit card fraud",
      "Data mining",
      "Database",
      "Database transaction",
      "Entropy (arrow of time)",
      "Machine learning",
      "Outlier",
      "Payment",
      "Physics",
      "Quantum mechanics",
      "Transaction data",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Zhenchuan"
      },
      {
        "surname": "Huang",
        "given_name": "Mian"
      },
      {
        "surname": "Liu",
        "given_name": "Guanjun"
      },
      {
        "surname": "Jiang",
        "given_name": "Changjun"
      }
    ]
  },
  {
    "title": "Highly shared Convolutional Neural Networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114782",
    "abstract": "In order to deploy deep Convolutional Neural Networks (CNNs) on the mobile devices, many mobile CNNs are introduced. Currently, some online applications are usually re-trained because of the constantly-increasing data. However, compared with the regular models, it is not very efficient to train the present mobile models. Therefore, the purpose of this paper is to propose efficient mobile models both in the training and test processes through exploring the main causes of the current mobile CNNs’ inefficiency and the parameters’ properties. Finally, this paper introduces Highly Shared Convolutional Neural Networks (HSC-Nets). The HSC-Nets employ two shared mechanisms to reuse the filters comprehensively. Experimental results showed that, compared with the regular networks and the latest state-of-the-art group-conv mobile networks, the HSC-Nets can achieve promising performances and effectively decrease the model size. Furthermore, it is also more efficient in both the training and test processes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002232",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Cellular network",
      "Computer network",
      "Computer science",
      "Convolutional neural network",
      "Distributed computing",
      "Ecology",
      "Economics",
      "Inefficiency",
      "Machine learning",
      "Microeconomics",
      "Reuse"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Yao"
      },
      {
        "surname": "Lu",
        "given_name": "Guangming"
      },
      {
        "surname": "Zhou",
        "given_name": "Yicong"
      },
      {
        "surname": "Li",
        "given_name": "Jinxing"
      },
      {
        "surname": "Xu",
        "given_name": "Yuanrong"
      },
      {
        "surname": "Zhang",
        "given_name": "David"
      }
    ]
  },
  {
    "title": "An evolutionary approach for the p-next center problem",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114728",
    "abstract": "The p-next center problem is an extension of the classical p-center problem, in which a backup center must be assigned to welcome users from a suddenly unavailable center. Usually, users tend to seek help in the closest facility they can find. However, during a significant event or crisis, one only realizes that the closest facility has been disrupted upon his/her arrival. Therefore, the user seeks help in the next closest center from the one that has failed to provide service. Therefore, the objective of the p-next center problem is to minimize the path of any user, which is made by the distance from this origin to its closest installed facility, plus the distance from this facility to its backup. We propose an evolutionary approach for the p-next center problem and an extension for the current benchmark instances. The proposed methods are built on the Multi-Parent Biased Random-Key Genetic Algorithm with Implicit Path-Relinking. Computational experiments carried out on 416 test instances show, experimentally, the outstanding performance of the developed algorithms and their flexibility to reach a good quality-speed trade-off.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100169X",
    "keywords": [
      "Artificial intelligence",
      "Center (category theory)",
      "Chemistry",
      "Computer science",
      "Crystallography",
      "Evolutionary algorithm"
    ],
    "authors": [
      {
        "surname": "Londe",
        "given_name": "Mariana A."
      },
      {
        "surname": "Andrade",
        "given_name": "Carlos E."
      },
      {
        "surname": "Pessoa",
        "given_name": "Luciana S."
      }
    ]
  },
  {
    "title": "Efficiency measurement in multi-period network DEA model with feedback",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114815",
    "abstract": "Decision-making unit (DMU) internal structure simulation is the basis for network Data Envelopment Analysis (DEA) to open “black box” and evaluate system efficiency with complex internal structure. Based upon summarizing and analyzing the existing model assumptions in network DEA, this paper proposes a hybrid multi-period DEA model with feedback to open the internal structure of the DMU system, as well as to provide horizontal comparison of the efficiency change of a same DMU at different time periods. In the model construction, the global production frontier is used for multi-period evaluation, Chebyshev distance is used to construct an unbiased two-stage model. Under the cooperation hypothesis, it is considered that the two stages are equally important, which solves the defect that the current two-stage method is not unique in its optimal solution and has two-stage contribution bias. A binary heuristic algorithm is proposed to reduce the time complexity of model solving while maintaining relatively high accuracy. The correctness and feasibility of the algorithm are demonstrated through the investigation of the relevant properties. Finally, the 5-year ecological data of China is used for illustrative application, providing suggestions for future environmental governance. Several comparative experiments are conducted to demonstrate the advantages of our proposed model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002566",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Basis (linear algebra)",
      "Computer science",
      "Construct (python library)",
      "Correctness",
      "Data envelopment analysis",
      "Data mining",
      "Geometry",
      "Heuristic",
      "Mathematical optimization",
      "Mathematics",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "You-wei"
      },
      {
        "surname": "Zhang",
        "given_name": "Hong-jun"
      },
      {
        "surname": "Cheng",
        "given_name": "Kai"
      },
      {
        "surname": "Zhang",
        "given_name": "Zi-xuan"
      },
      {
        "surname": "Chen",
        "given_name": "Yu-tian"
      }
    ]
  },
  {
    "title": "A fuzzy penalized regression model with variable selection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114696",
    "abstract": "In the classical multiple regression modeling, there might be some insignificant input variables. These variables can be eliminated by automatic selectors, known as penalized methods. We propose a penalized estimation method for the coefficients of a linear regression model for studying the dependence of a LR fuzzy response (output) variable on a set of crisp explanatory (input) variables. To show the performances of the proposed model a simulation study was utilized under three scenarios of multicollinear and sparse data. The model demonstrates better performances in comparison to another three models on the basis of specified goodness of fit measures, under three variants of the penalty function. Evaluation of the method has been conducted on real data. The results demonstrate superior performances in terms of the goodness of fit measures in comparison to the other models. To take into account the imprecision due to the lack of knowledge about the data generation process, in the applications to real data bootstrap-t confidence intervals were also utilized.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001378",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Feature selection",
      "Fuzzy logic",
      "Goodness of fit",
      "Linear regression",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Model selection",
      "Regression",
      "Regression analysis",
      "Statistics",
      "Variable (mathematics)"
    ],
    "authors": [
      {
        "surname": "Kashani",
        "given_name": "M."
      },
      {
        "surname": "Arashi",
        "given_name": "M."
      },
      {
        "surname": "Rabiei",
        "given_name": "M.R."
      },
      {
        "surname": "D’Urso",
        "given_name": "P."
      },
      {
        "surname": "De Giovanni",
        "given_name": "L."
      }
    ]
  },
  {
    "title": "A clinical decision support system for predicting cirrhosis stages via high frequency ultrasound images",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114680",
    "abstract": "Many visceral organs contain two important structures: capsule and parenchyma. As a non-invasive examination method, ultrasound is widely used. In this study, we develop a computational framework that consists of capsule localization and parenchyma assessment for disease diagnosis. Clinical decision support system helps providing standard, objective and timely diagnosis. However, although current cirrhosis diagnosis methods are mainly based on the images produced from medical imaging technique and experienced clinicians subjective analysis, many imaging techniques are invasive, and experienced clinicians are in short supply, especially in underdeveloped regions. The proposed system employs an incremental classification model for grading parenchyma patch stages in predicting cirrhosis stages from high frequency ultrasound images. The incremental classification model is based on auto-extracted membrane structures along with patch-ensemble model, using a severe-first strategy. The framework firstly applies multi-scale capsule extraction automatically. The lesions of the capsule and parenchyma show increasing changes in the first three stages. In the final stage, due to the liver ascites, partial lesions restore to normal. To handle the inconsecutive change, the proposed patch-ensemble model applies two layers. We firstly recognize images belong to the severe stage via capsule-specific CIFAR in the first layer. Parenchyma-aimed Resnet is then applied to classify rest images into rest stages. In each layer a data split and aggregation scheme is proposed to evaluate the cirrhosis stage for liver images. The experimental results demonstrate that the proposed method achieves high precision and effectiveness and can be effectively applied to the auxiliary diagnosis of cirrhosis. Some integral parts of the system are also available for visceral organs with similar structure.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001214",
    "keywords": [
      "Artificial intelligence",
      "Cirrhosis",
      "Civil engineering",
      "Computer science",
      "Engineering",
      "Gastroenterology",
      "Grading (engineering)",
      "Medicine",
      "Parenchyma",
      "Pathology",
      "Pattern recognition (psychology)",
      "Radiology",
      "Ultrasound"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Xiang"
      },
      {
        "surname": "Ma",
        "given_name": "Rui Lin"
      },
      {
        "surname": "Zhao",
        "given_name": "Jingwen"
      },
      {
        "surname": "Song",
        "given_name": "Jia Ling"
      },
      {
        "surname": "Zhang",
        "given_name": "Jian Quan"
      },
      {
        "surname": "Wang",
        "given_name": "Shuo Hong"
      }
    ]
  },
  {
    "title": "Human opinion dynamics optimization for ICI mitigation in MC-CDMA systems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114903",
    "abstract": "This paper presents a novel social impact based approach named as Human Opinion Dynamics Optimization (HODO) to detect signals of Multi-Carrier Code Division Multiple Access (MC-CDMA) systems in the presence of Inter Subcarrier Interference (ICI). The ICI occurs because of various factors, such as Carrier Frequency Offset (CFO), Sampling Frequency Offset (SFO) and channel mobility. The ICI degrades orthogonality between different subcarriers in MC-CDMA system due to which Multiple Access Interference (MAI) occurs. The HODO is based on the socio-psychological behavior of humans in the society. Opinion dynamics leads to human consensus and consensus builds up due to human interactions in a society. The proposed approach could be used to introduce human opinion based intelligent communication networks. Our method is novel in the sense that with this algorithm, social sciences could be made compatible with wireless communication technology, which will go a long way to bridge the gap between humans and the communication technology. This new method works with very few number of control parameters and reduces the bit error rate (BER) better as compared with other conventional stochastic methods like Genetic Algorithm (GA) and Particle Swarm Optimization (PSO). Numerical simulation results show that the proposed detector also reduces the computational complexity manifold.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003444",
    "keywords": [
      "Algorithm",
      "Bit error rate",
      "Carrier frequency offset",
      "Channel (broadcasting)",
      "Code division multiple access",
      "Computer science",
      "Electronic engineering",
      "Engineering",
      "Frequency offset",
      "Geometry",
      "Mathematics",
      "Orthogonal frequency-division multiplexing",
      "Orthogonality",
      "Particle swarm optimization",
      "Subcarrier",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Kumar Goyal",
        "given_name": "Anmol"
      },
      {
        "surname": "Gao",
        "given_name": "Xiao-Zhi"
      }
    ]
  },
  {
    "title": "Active contour model with adaptive weighted function for robust image segmentation under biased conditions",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114811",
    "abstract": "The segmentation of images under biased conditions such as low contrast, high-intensity inhomogeneity, and noise is a challenge for any image segmentation model. The ideal image segmentation model must be capable of segmenting images with maximum accuracy and a minimum false-positive rate under biased conditions. In this paper, we propose a region-based active contour model (ACM), called global signed pressure and K-means clustering based on local correntropy with the exponential family (GSLCE), to address segmentation challenges under biased conditions. An adaptive weighted function is formulated based on the global and local image differences such that a single weighted function can drive both the global and local intensities. Further, the Riemannian steepest descent method is used for convergence of the proposed GSLCE energy function, and a Gaussian kernel is applied for spatial smoothing to obviate the computationally expensive level-set re-initialization. The experimental results show that, compared with state-of-the-art ACMs, the proposed GSLCE model obtained the best visual image segmentation results for synthetic and real images under biased conditions. Further, the qualitative and quantitative experimental results validate that the proposed model outperforms the state-of-the-art ACMs by yielding higher values of performance metrics. Moreover, the proposed GSLCE model requires substantially less processing time compared to the state-of-the-art ACMs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002529",
    "keywords": [
      "Active contour model",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Image segmentation",
      "Initialization",
      "Kernel (algebra)",
      "Level set (data structures)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Scale-space segmentation",
      "Segmentation",
      "Smoothing"
    ],
    "authors": [
      {
        "surname": "Joshi",
        "given_name": "Aditi"
      },
      {
        "surname": "Khan",
        "given_name": "Mohammed Saquib"
      },
      {
        "surname": "Niaz",
        "given_name": "Asim"
      },
      {
        "surname": "Akram",
        "given_name": "Farhan"
      },
      {
        "surname": "Song",
        "given_name": "Hyun Chul"
      },
      {
        "surname": "Choi",
        "given_name": "Kwang Nam"
      }
    ]
  },
  {
    "title": "Note on “Tolerance-based intuitionistic fuzzy-rough set approach for attribute reduction”",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114869",
    "abstract": "In this note, we show by counterexamples that Theorem 5.7 and Theorem 5.9 of Tiwari et al. (2018) are incorrect. Further their corrected versions are presented.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421003109",
    "keywords": [
      "Computer science",
      "Counterexample",
      "Data mining",
      "Discrete mathematics",
      "Geometry",
      "Mathematics",
      "Programming language",
      "Reduction (mathematics)",
      "Rough set",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Rehman",
        "given_name": "Noor"
      },
      {
        "surname": "Ali",
        "given_name": "Abbas"
      },
      {
        "surname": "Hila",
        "given_name": "Kostaq"
      }
    ]
  },
  {
    "title": "Deep sequence to sequence Bi-LSTM neural networks for day-ahead peak load forecasting",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114844",
    "abstract": "The power industry is currently facing the problem of an electricity supply–demand imbalance. The most inexpensive and efficient solution to alleviate this imbalance is to decrease electricity demand. Local electrical utilities should deploy demand response programs (DRP), and short-term peak demand forecasting (STPDF) plays a crucial role in their successful deployment. In residential sectors, peak demand forecasting is also critical because the energy policies, technological growth, and changing climate are further increasing the peak demand. Therefore, an accurate peak demand forecasting will help utility companies in avoiding blackouts and secure a continuous power supply by implementing subsidy schemes such as DRP. However, daily peak load is volatile, nonstationary, and nonlinear in nature, and hence it is hard to predict it accurately. This research work for the first time has attempted to design, implement, and test deep bidirectional long short-term memory based sequence to sequence (Bi-LSTM S2S) regression approach for “day-ahead” peak demand forecasting and has accomplished preliminary success. The day-ahead peak electricity demand forecasting model is designed and tested using the MATLAB software. For performance comparison, shallow Bi-LSTM S2S, shallow LSTM S2S, deep LSTM S2S, Levenberg-Marquardt backpropagation artificial neural networks (LMBP-ANN), and medium Gaussian support vector regression (MG-SVR) forecasting models are also developed and tested. Mean absolute percentage error (MAPE) and Root Mean Squared Error (RMSE) are used as performance metrics. It has been found out that in terms of both performance metrics, the proposed deep Bi-LSTM S2S day-ahead “peak” forecasting model has outperformed all the other models on both public holidays and normal days. The load pattern on public holidays is always different than on normal days, and there is always less data available in contrast to the normal days. Therefore, it is hard to accurately forecast their load.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002852",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Backpropagation",
      "Computer science",
      "Demand forecasting",
      "Electrical engineering",
      "Electricity",
      "Engineering",
      "Levenberg–Marquardt algorithm",
      "Machine learning",
      "Mathematics",
      "Mean absolute percentage error",
      "Mean squared error",
      "Operations research",
      "Peak demand",
      "Recurrent neural network",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Mughees",
        "given_name": "Neelam"
      },
      {
        "surname": "Mohsin",
        "given_name": "Syed Ali"
      },
      {
        "surname": "Mughees",
        "given_name": "Abdullah"
      },
      {
        "surname": "Mughees",
        "given_name": "Anam"
      }
    ]
  },
  {
    "title": "Macroeconomic forecasting through news, emotions and narrative",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114760",
    "abstract": "This study proposes a new method of incorporating emotions from newspaper articles into macroeconomic forecasts, attempting to forecast industrial production and consumer prices leveraging narrative and sentiment from global newspapers. For the most part, existing research includes positive and negative tone only to improve macroeconomic forecasts, focusing predominantly on large economies such as the US. These works use mainly anglophone sources of narrative, thus not capturing the entire complexity of the multitude of emotions contained in global news articles. This study expands the existing body of research by incorporating a wide array of emotions from newspapers around the world – extracted from the Global Database of Events, Language and Tone (GDELT) – into macroeconomic forecasts. We present a thematic data filtering methodology based on a bi-directional long short term memory neural network (Bi-LSTM) for extracting emotion scores from GDELT and demonstrate its effectiveness by comparing results for filtered and unfiltered data. We model industrial production and consumer prices across a diverse range of economies using an autoregressive framework, and find that including emotions from global newspapers significantly improves forecasts compared to three autoregressive benchmark models. We complement our forecasts with an interpretability analysis on distinct groups of emotions and find that emotions associated with happiness and anger have the strongest predictive power for the variables we predict.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002013",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Linguistics",
      "Narrative",
      "Philosophy",
      "Sentiment analysis"
    ],
    "authors": [
      {
        "surname": "Tilly",
        "given_name": "Sonja"
      },
      {
        "surname": "Ebner",
        "given_name": "Markus"
      },
      {
        "surname": "Livan",
        "given_name": "Giacomo"
      }
    ]
  },
  {
    "title": "Revisiting the performance of evolutionary algorithms",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114819",
    "abstract": "The advent of numerical computational approaches permits evolutionary algorithms (EAs) to solve complex, real-world engineering problems. The additional modification or hybridization of such EAs in academic research and application demonstrates improved performance for domain-specific challenges. However, developing a new algorithm or comparison and selection of existing EAs for challenges in the field of optimization is relatively unexplored. The performance of different well-established algorithms is, therefore, investigated in this work. The selection of algorithms using nonparametric tests encompasses different categories to include- Genetic Algorithm, Particle Swarm Optimization, Harmony Search Algorithm, Cuckoo Search Algorithm, Bat Algorithm, Firefly algorithm, Differential Evolution, and Artificial Bee Colony. These algorithms are applied to solve test functions, including unconstrained, constrained, industry specific problems, CEC 2011 real world optimization problems and selected CEC 2013 benchmark test functions. The three distinct performance metrics, namely, efficiency, reliability, and quality of solution derived using the quantitative attributes are provided to evaluate the performance of the employed EAs. The categorical assignment of performance attributes helps to compare different algorithms for a specific optimization problem while the performance metrics are useful to provide the common platform for new or hybrid EA development.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002608",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Cuckoo search",
      "Differential evolution",
      "Evolutionary algorithm",
      "Evolutionary computation",
      "Firefly algorithm",
      "Geodesy",
      "Geography",
      "Harmony search",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Particle swarm optimization"
    ],
    "authors": [
      {
        "surname": "Vala",
        "given_name": "Tejas M."
      },
      {
        "surname": "Rajput",
        "given_name": "Vipul N."
      },
      {
        "surname": "Geem",
        "given_name": "Zong Woo"
      },
      {
        "surname": "Pandya",
        "given_name": "Kartik S."
      },
      {
        "surname": "Vora",
        "given_name": "Santosh C."
      }
    ]
  },
  {
    "title": "Multi-analysis surveillance and dynamic distribution of computational resources: Towards extensible, robust, and efficient monitoring of environments",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114692",
    "abstract": "Intelligent surveillance has been a hot area of research for the past two decades. However, the integral security of environments remains a challenge due to the presence of new types of threats and the very dynamism and complexity of these environments. To make further progress, new proposals are needed to facilitate the design and deployment of multi-analysis surveillance systems where different types of analysis are simultaneously performed and the available computation resources are limited. Such systems need to provide three main characteristics: extensibility, robustness and efficiency. Extensibility to add new analysis components when new events of interest must be monitored. Robustness to avoid that failures in one analysis component affect the rest. Efficiency to analyze environments in real-time and to support decision-making processes that address the detected anomalies. This paper proposes a formal model for the multi-analysis surveillance of environments by means of the named components of normality, designed to deploy surveillance systems that satisfies the three previously mentioned characteristics. Extensibility thanks to the activation and deactivation of components of normality based on the monitoring needs of the analyzed environment. Robustness as a result of the isolation of these components such that a failure in one of them does not spread to the rest. Finally, efficiency due to the dynamic allocation of resources that benefits the components that detect anomalies at a given time. The experimental results prove that such dynamic allocation improves execution times and reduces waiting times.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001330",
    "keywords": [
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Distributed computing",
      "Dynamism",
      "Extensibility",
      "Gene",
      "Normality",
      "Operating system",
      "Physics",
      "Psychiatry",
      "Psychology",
      "Quantum mechanics",
      "Real-time computing",
      "Robustness (evolution)",
      "Software deployment",
      "Software engineering"
    ],
    "authors": [
      {
        "surname": "Albusac",
        "given_name": "Javier"
      },
      {
        "surname": "Vallejo",
        "given_name": "David"
      },
      {
        "surname": "Castro-Schez",
        "given_name": "Jose J."
      },
      {
        "surname": "Sanchez-Sobrino",
        "given_name": "Santiago"
      },
      {
        "surname": "Gomez-Portes",
        "given_name": "Cristian"
      }
    ]
  },
  {
    "title": "A novel multi-objective forest optimization algorithm for wrapper feature selection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114737",
    "abstract": "Feature selection is one of the important techniques of dimensionality reduction in data preprocessing because datasets generally have redundant and irrelevant features that adversely affect the performance and complexity of classification models. Feature selection has two main objectives, i.e., reducing the number of features and increasing classification performance due to its inherent nature. In this paper, we propose a multi-objective feature selection algorithm based on forest optimization algorithm (FOA) using the archive, grid, and region-based selection concepts. For this purpose, two versions of the proposed algorithm are developed using continuous and binary representations. The performance of the proposed algorithms is investigated on nine UCI datasets and two microarray datasets. Next, the obtained results are compared with seven traditional single-objective and five multi-objective methods. Based on the results, both proposed algorithms have reached the same performance or even outperformed the single-objective methods. Compared with other multi-objective algorithms, MOFOA with continuous representation has managed to reduce the classification error in most cases by selecting less number of features than other methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001780",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Curse of dimensionality",
      "Data mining",
      "Data pre-processing",
      "Dimensionality reduction",
      "Feature (linguistics)",
      "Feature selection",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Preprocessor",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Nouri-Moghaddam",
        "given_name": "Babak"
      },
      {
        "surname": "Ghazanfari",
        "given_name": "Mehdi"
      },
      {
        "surname": "Fathian",
        "given_name": "Mohammad"
      }
    ]
  },
  {
    "title": "Fast automatic step size selection for zeroth-order nonconvex stochastic optimization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114749",
    "abstract": "The efficacy and simplicity of using only function evaluations in zeroth-order stochastic optimization (ZOSO) makes it achieve great attention in solving large scale learning tasks. However, the question of how to choose an appropriate step size sequence timely for ZOSO has been less researched. To fill this defect, this paper provides a fast automatic step size selection approach by using an improved Barzilai-Borwein (IBB) technique with the zeroth-order (ZO) gradient estimator for ZOSO. In detail, this work shows the efficacy of the IBB technique by applying it into the advanced ZOSO method, i.e., zeroth-order stochastic variance reduced gradient (ZO-SVRG) method, which leads to a method: ZO-SVRG-IBB. We theoretically analyze the convergence of the ZO-SVRG-IBB method under the random gradient estimator and the coordinate-wise gradient estimator settings, respectively, and show that the convergence rate of ZO-SVRG-IBB matches the best-known convergence rate of advanced ZOSO methods on nonconvex functions. We further show that the query complexity of ZO-SVRG-IBB is comparable to advanced ZOSO methods. Extensive numerical experiments performed on different datasets show that the proposed method outperforms or matches state-of-the-art ZOSO methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001901",
    "keywords": [
      "Accounting",
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Business",
      "Computer science",
      "Computer security",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Estimator",
      "Evolutionary biology",
      "Function (biology)",
      "Key (lock)",
      "Mathematical optimization",
      "Mathematics",
      "Rate of convergence",
      "Selection (genetic algorithm)",
      "Statistics",
      "Variance (accounting)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Zhuang"
      }
    ]
  },
  {
    "title": "Reliability-driven Automotive Software Deployment based on a Parametrizable Probabilistic Model Checking",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114572",
    "abstract": "Embedded systems span a wide range from a small platform of sensors and actuators to distributed systems combining several interacting nodes. Designing such systems includes hardware parts and software parts. The software part acquires in importance since it handles the resources and services to interact with the hardware part. The paper introduces a novel deployment-decision making based on PRISM probabilistic model checker that takes software components and the physical platform to produce a set of deployment candidates. Starting from System Modeling Language (SysML), the process includes mechanisms to extract hardware and software features and carry out a set of deployment candidates. Each candidate should satisfy the reliability property written in Probabilistic Computation Tree Logic. Formally, we capture the underlying semantics of software blocks behaviour expressed as an activity diagram and their generated PRISM code to prove the approach soundness. Illustration relies on the automotive control system to show the applicability of the proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000130",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Distributed computing",
      "Embedded system",
      "Probabilistic logic",
      "Programming language",
      "Software",
      "Software deployment",
      "Software engineering",
      "Software system",
      "Soundness",
      "Systems Modeling Language",
      "Unified Modeling Language"
    ],
    "authors": [
      {
        "surname": "Baouya",
        "given_name": "Abdelhakim"
      },
      {
        "surname": "Mohamed",
        "given_name": "Otmane Ait"
      },
      {
        "surname": "Ouchani",
        "given_name": "Samir"
      },
      {
        "surname": "Bennouar",
        "given_name": "Djamal"
      }
    ]
  },
  {
    "title": "Hybrid slime mould algorithm with adaptive guided differential evolution algorithm for combinatorial and global optimization problems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114689",
    "abstract": "The Slime Mould Algorithm (SMA) is a recent metaheuristic inspired by the oscillation of slime mould. Similar to other original metaheuristic algorithms (MAs), SMA may suffer from drawbacks, such as being trapped in minimum local regions and improper balance between exploitation and exploration phases. To overcome these weaknesses, this paper proposes a hybrid algorithm: SMA combined to Adaptive Guided Differential Evolution Algorithm (AGDE) (SMA-AGDE). The AGDE mutation method is employed to enhance the swarm agents’ local search, increase the population’s diversity, and help avoid premature convergence. The SMA-AGDE’s performance is evaluated on the CEC’17 test suite, three engineering design problems – tension/compression spring, pressure vessel, and rolling element bearing – and two combinatorial optimization problems – bin packing and quadratic assignment. The SMA-AGDE is compared with three categories of optimization methods: (1) The well-studied MAs, i.e., Biogeography-Based Optimizer (BBO), Gravitational Search Algorithm (GSA), and Teaching Learning-Based Optimization (TLBO), (2) Recently developed MAs, i.e., Harris Hawks Optimization (HHO), Manta Ray Foraging optimization (MRFO), and the original SMA, and (3) High-performance MAs, i.e., Evolution Strategy with Covariance Matrix Adaptation (CMA-ES), and AGDE. The overall simulation results reveal that the SMA-AGDE ranked first among the compared algorithms, and so, over different function landscapes. Thus, the proposed SMA-AGDE is a promising optimization tool for global and combinatorial optimization problems and engineering design problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001305",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "CMA-ES",
      "Computer science",
      "Differential evolution",
      "Evolution strategy",
      "Evolutionary algorithm",
      "Local search (optimization)",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Optimization problem",
      "SMA*",
      "Swarm behaviour"
    ],
    "authors": [
      {
        "surname": "Houssein",
        "given_name": "Essam H."
      },
      {
        "surname": "Mahdy",
        "given_name": "Mohamed A."
      },
      {
        "surname": "Blondin",
        "given_name": "Maude J."
      },
      {
        "surname": "Shebl",
        "given_name": "Doaa"
      },
      {
        "surname": "Mohamed",
        "given_name": "Waleed M."
      }
    ]
  },
  {
    "title": "On the propagation of quality requirements for mechanical assemblies in industrial manufacturing",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114608",
    "abstract": "A frequent challenge encountered by manufacturers of mechanical assemblies consists of the definition of quality criteria for the assembly lines of the subcomponents which are mounted into the final product. The rollout of Industry 4.0 standards paves the way for the usage of data-driven, intelligent approaches towards this goal. In this work, we investigate such a scenario originating in the daily operations of thyssenkrupp Presta AG, where new vibroacoustic quality specifications must be derived for the assembly line producing ball nut assemblies, based on the feedback offered by the vibroacoustic quality test of the steering gear, the final mechanical assembly they are mounted into. We first present a Mixed Integer Linear Programming (MILP) formulation for the problem and show that small instances of the corresponding available dataset can be solved to optimality. Upon ascertainment of the unsuitability of the MILP approach for industrial daily operations due to its long computation time, we propose a heuristic solving approach based on genetic algorithms and measure the performance gap between them in terms of achieved solution quality and computation time. Finally, we additionally propose a greedy heuristic designed to outperform the genetic algorithm in terms of computation time while still featuring a comparable solution quality. The practical relevance of the results is guaranteed, since the best solution reached by the genetic algorithm reduces the scrap costs with respect to the method currently employed by the company by 49.91%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100049X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computation",
      "Computer science",
      "Engineering",
      "Epistemology",
      "Genetic algorithm",
      "Heuristic",
      "Industrial engineering",
      "Integer programming",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Mechanical engineering",
      "Philosophy",
      "Quality (philosophy)",
      "Scrap"
    ],
    "authors": [
      {
        "surname": "Bucur",
        "given_name": "Paul Alexandru"
      },
      {
        "surname": "Armbrust",
        "given_name": "Philipp"
      },
      {
        "surname": "Hungerländer",
        "given_name": "Philipp"
      }
    ]
  },
  {
    "title": "A new hybrid ensemble model with voting-based outlier detection and balanced sampling for credit scoring",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114744",
    "abstract": "The credit scoring system has been revolutionized with the development of the financial system and has received increasing attention from the academia and industry. Artificial intelligence technology has reshaped credit scoring through predictive classification. In this study, a new hybrid ensemble model with voting-based outlier detection and balanced sampling is proposed to achieve superior predictive power for credit scoring. To avoid noise-filled data from misleading the classifier training, a new voting-based outlier detection method is proposed to enhance the classic outlier detection algorithms with the weighted voting mechanism and boost the outlier scores into the training set to form an outlier-adapted training set. To reduce the information loss caused by under-sampling when dealing with imbalanced data, a new bagging-based balanced sampling method is proposed to enhance the traditional under-sampling methods with the bagging strategy to obtain a balanced training set. To further improve the performance of the proposed model, a stacking-based ensemble modeling method is proposed to first perform parametrical optimization and then construct the stacking-based multi-stage ensemble model. Five datasets from the UC Irvine machine learning repository and five evaluation indicators were adopted to evaluate the model performance. The experimental results indicate the superior performance of the proposed model and prove its robustness and effectiveness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001858",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Ensemble forecasting",
      "Ensemble learning",
      "Gene",
      "Law",
      "Machine learning",
      "Outlier",
      "Political science",
      "Politics",
      "Robustness (evolution)",
      "Voting"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Wenyu"
      },
      {
        "surname": "Yang",
        "given_name": "Dongqi"
      },
      {
        "surname": "Zhang",
        "given_name": "Shuai"
      }
    ]
  },
  {
    "title": "Stacking hybrid GARCH models for forecasting Bitcoin volatility",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114747",
    "abstract": "Machine learning techniques have been used frequently for volatility forecasting. However, previous studies have built these hybrid models in a form of a first-order GARCH(1,1) process by following general use for GARCH models. But the way of estimating parameters for GARCH and machine learning models differs considerably. Hence, we have investigated the effect of different model orders of the GARCH process on the volatility forecasts of Bitcoin obtained by the four most used machine learning models. Furthermore, we have proposed a stacking ensemble methodology based on GARCH hybrid models to improve the results further. The proposed stacking ensemble methodology utilizes the techniques of feature selection and feature extraction to reduce the dimension of the predictors before meta-learning. The results show that using higher model orders increases the accuracy of volatility forecasts for hybrid GARCH models. Also, the proposed stacking ensemble with LASSO produces forecasts superior to almost all hybrid models and better than the ordinary stacking ensemble.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001883",
    "keywords": [
      "Artificial intelligence",
      "Autoregressive conditional heteroskedasticity",
      "Computer science",
      "Econometrics",
      "Ensemble forecasting",
      "Ensemble learning",
      "Lasso (programming language)",
      "Machine learning",
      "Mathematics",
      "Nuclear magnetic resonance",
      "Physics",
      "Stacking",
      "Volatility (finance)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Aras",
        "given_name": "Serkan"
      }
    ]
  },
  {
    "title": "A factorization scheme for observability analysis in transportation networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114727",
    "abstract": "Traffic flow data are a significant component of most intelligent transportation systems (ITS). Complete traffic flow data are required for most ITS applications, but providing traffic sensors in all network streets is not practical. Some flow types are difficult to observe directly, such as node pair demand (O/D flow). This study provides a mathematical analysis approach using a factorization scheme to convert conventional traffic assignment mapping into a useful format. The new mapping structure helps in identifying the amount of traffic-counting data (link flows) necessary to solve either the full observability problem for the network or a partial one. Once the required data are provided, the observability problem can be easily solved using backward substitution. In addition, the new format provides the dependencies of the different flow measures in the network. The proposed approach can track the change in the network observability state with the route choice uncertainty. Two fully reported illustrative examples in addition to a real case network are presented to demonstrate the generality of the proposed method and its potential contribution to the observability problem.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001688",
    "keywords": [
      "Applied mathematics",
      "Civil engineering",
      "Computer network",
      "Computer science",
      "Engineering",
      "Flow network",
      "Generality",
      "Intelligent transportation system",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Observability",
      "Psychology",
      "Psychotherapist",
      "Scheme (mathematics)",
      "Traffic flow (computer networking)"
    ],
    "authors": [
      {
        "surname": "Owais",
        "given_name": "Mahmoud"
      },
      {
        "surname": "Matouk",
        "given_name": "Ahmed E."
      }
    ]
  },
  {
    "title": "Expert-novice classification of mobile game player using smartphone inertial sensors",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114700",
    "abstract": "The gaming industry has seen a tremendous growth in the last decade due to an exponential increase in the number of smartphone users. Embedded smartphone sensors provide solutions for automatic game controls during game-play. In this paper, we present an experimental study for the expertise classification of a game (mobile-based) player using smartphone inertial sensors, while they are simultaneously used for game controls. The game expertise level of participants is either labeled as expert or novice using game scores. Towards this end, data from 38 participants are curated during Traffic Racer game-play (in three different trials) using the embedded gyroscope and accelerometer sensors of the smartphone. These signals are pre-processed using Savitzky-Golay smoothing filter to remove noise. Twenty time domain features are extracted from the pre-processed data and are subjected to the wrapper-based feature selection method to select an optimum subset of features. Three classifiers, including k-nearest neighbor (k-NN), random forest, and the Naive Bayes, are evaluated towards the classification of player’s expertise level, i.e., expert and novice. The best average accuracy of 92.1 % is achieved with k-NN classifier using the fusion of gyroscope and accelerometer data, which outperforms the existing state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100141X",
    "keywords": [
      "Accelerometer",
      "Aerospace engineering",
      "Android (operating system)",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Engineering",
      "Feature selection",
      "Gyroscope",
      "Inertial measurement unit",
      "Machine learning",
      "Naive Bayes classifier",
      "Operating system",
      "Pattern recognition (psychology)",
      "Random forest",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Ehatisham-ul-Haq",
        "given_name": "Muhammad"
      },
      {
        "surname": "Arsalan",
        "given_name": "Aamir"
      },
      {
        "surname": "Raheel",
        "given_name": "Aasim"
      },
      {
        "surname": "Anwar",
        "given_name": "Syed Muhammad"
      }
    ]
  },
  {
    "title": "On deep neural network for trust aware cross domain recommendations in E-commerce",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114757",
    "abstract": "Over the years, cross-domain and trust-based recommendation systems are proven to be very helpful in solving issues pertaining to data sparsity and cold start. Many e-commerce sites used recommender systems as business tools for increasing their sale productivity and help their customers in finding suitable products. However, due to sparse rating and lack of historical information, such systems cannot generate effective recommendations. Matrix factorization and deep learning techniques have been the focus of research community for the last few years to solve the problem of data sparsity and cold start. In this paper, we have proposed a model called Trust Aware Cross-Domain Deep Neural Matrix Factorization (TCrossDNMF) that predicts rating of an item for an active user and solves user cold start problem in the cross-domain scenario of ‘User Overlap’ in e-commerce system. TCrossDNMF model is divided into four main steps: i) Features learning that learns the users’ features using a latent factor model and then finds the similarity between users of source and target domains. As the users are shared between two domains, the proposed model learns the common information and transfers the knowledge from a source to target domain. ii) Ranking that finds set of similar users (neighbors), and then filters out the dissimilar users based on similarity threshold θ , and then generates a bipartite trust graph from these reduced set of users and executes Ant Colony Optimization, to find trustworthy neighbors for an active user. iii) Weighting computes the trust degree between an active user and his or her top-k neighbors. iv) Prediction that trains the TCrossDNMF model using multilayer perceptron (MLP) and generalized matrix factorization (GMF) by representing user-item interactions in higher dimensions and ensembles the GMF and MLP with trust information for rating prediction. We evaluated proposed model on a real dataset collected from a popular e-commerce retail service ‘AliExpress’. We used categories available in ‘AliExpress’ as source domain and a target domain. For observing the performance of proposed model, we took six domains that have a higher ratio of sparsity. The proposed model is evaluated by using MAE, RMSE and F-measure metrics and compared it with baselines. The experiments show that the proposed model is a viable solution for the mentioned problem with significant improvements in results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001986",
    "keywords": [
      "Aerospace engineering",
      "Artificial intelligence",
      "Bipartite graph",
      "Cold start (automotive)",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Domain (mathematical analysis)",
      "Eigenvalues and eigenvectors",
      "Engineering",
      "Graph",
      "Image (mathematics)",
      "Information retrieval",
      "Learning to rank",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Matrix decomposition",
      "Medicine",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Radiology",
      "Ranking (information retrieval)",
      "Recommender system",
      "Set (abstract data type)",
      "Similarity (geometry)",
      "Theoretical computer science",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Ahmed",
        "given_name": "Adeel"
      },
      {
        "surname": "Saleem",
        "given_name": "Khalid"
      },
      {
        "surname": "Khalid",
        "given_name": "Osman"
      },
      {
        "surname": "Rashid",
        "given_name": "Umer"
      }
    ]
  },
  {
    "title": "Stamantic clustering: Combining statistical and semantic features for clustering of large text datasets",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114710",
    "abstract": "Document clustering in text mining is a problem that is heavily researched upon. It is observed that individual approaches based on statistical features and semantic features have been extensively used to solve this problem. However, techniques combining the advantages of both types of features have not been frequently researched upon. Specifically, when the growth in the size of textual data is immense, there is a need for such an approach that combines the advantages of both types of features to give more accurate results within an acceptable range of time. In this paper, a document clustering technique is proposed that combines the effectiveness of the statistical features (using TF-IDF) and semantic features (using lexical chains). It is designed to use a fewer number of features while maintaining a comparable and even better accuracy for the task of document clustering.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001512",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Composite material",
      "Computer science",
      "Data mining",
      "Document clustering",
      "Economics",
      "Information retrieval",
      "Management",
      "Materials science",
      "Range (aeronautics)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Mehta",
        "given_name": "Vivek"
      },
      {
        "surname": "Bawa",
        "given_name": "Seema"
      },
      {
        "surname": "Singh",
        "given_name": "Jasmeet"
      }
    ]
  },
  {
    "title": "Joint aspect terms extraction and aspect categories detection via multi-task learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114688",
    "abstract": "Aspect Terms Extraction (ATE) and Aspect Categories Detection (ACD) are two fundamental sub-tasks for aspect-based sentiment analysis. Most of the existing works mainly focus on the ATE task or the co-extraction of aspect terms and opinion words, while few attention are paid to the ACD task. In this work, we propose a joint model to seamlessly integrate the ATE and ACD tasks into a multi-task learning framework. Each of the tasks is based on multi-layer Convolutional Neural Networks (CNNs) for computing high-level word representations, and produces a task-specific and a task-share vector. The task-share vector of one task is used to propagate information to the other, and guides the counterpart task to align the informative textual features to produce the task-specific vectors. Finally, a fully-connected layer with a softmax/sigmoid function is applied to the task-specific vectors for the specific information extraction. The rationale underlying the proposed joint model is that, aspect terms and aspect categories are semantically related, and the information propagated between the two tasks can help to capture the semantic alignments between the aspect terms and categories, and produce informative task-specific vectors. Moreover, the ATE task models local semantics at each position of a sentence, while the ACD task extracts global features of the whole sentence. The mutual interactions between local and global features, therefore, can reciprocally capture informative textual features for the information extraction tasks. We validate the effectiveness of the proposed model on two widely used datasets, and show its advantage over the state-of-the-art baselines. We also investigate the effectiveness of the multi-task framework by comparing the proposed model with its variants. Further, we study the robustness of the proposed model by presenting the model performance with respect to different hyperparameters. Finally, we provide visualization examples to gain a better understanding of the advantages the multi-task learning scheme.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001299",
    "keywords": [
      "Architectural engineering",
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Economics",
      "Engineering",
      "Information extraction",
      "Joint (building)",
      "Machine learning",
      "Management",
      "Multi-task learning",
      "Natural language processing",
      "Sentence",
      "Softmax function",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Youcai"
      },
      {
        "surname": "Zhang",
        "given_name": "Hongyun"
      },
      {
        "surname": "Fang",
        "given_name": "Jian"
      },
      {
        "surname": "Wen",
        "given_name": "Jiahui"
      },
      {
        "surname": "Ma",
        "given_name": "Jingwei"
      },
      {
        "surname": "Zhang",
        "given_name": "Guangda"
      }
    ]
  },
  {
    "title": "Conditional Wasserstein GAN-based oversampling of tabular data for imbalanced learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114582",
    "abstract": "Class imbalance impedes the predictive performance of classification models. Popular countermeasures include oversampling minority class cases by creating synthetic examples. The paper examines the potential of Generative Adversarial Networks (GANs) for oversampling. A few prior studies have used GANs for this purpose but do not reflect recent methodological advancements for generating tabular data using GANs. The paper proposes an approach based on a conditional Wasserstein GAN that can effectively model tabular datasets with numerical and categorical variables and pays special attention to the down-stream classification task through an auxiliary classifier loss. We focus on a credit scoring context in which binary classifiers predict the default risk of loan applications. Empirical comparisons in this context evidence the competitiveness of GAN-based oversampling compared to several standard oversampling regimes. We also clarify the conditions under which oversampling in general and the proposed GAN-based approach in particular raise predictive performance. In sum, our findings suggest that GAN architectures for tabular data and our extensions deserve a place in data scientists’ modelling toolbox.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000233",
    "keywords": [],
    "authors": [
      {
        "surname": "Engelmann",
        "given_name": "Justin"
      },
      {
        "surname": "Lessmann",
        "given_name": "Stefan"
      }
    ]
  },
  {
    "title": "Learning style detection in E-learning systems using machine learning techniques",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114774",
    "abstract": "Learning style plays a vital role in helping students retain learned concepts for a longer time and also improves the understanding of the concepts. Learning styles in offline and online scenarios are recognized using questionnaires. The recent trend is to identify and use attributes to detect the learning style of the learner automatically without disturbing the learner. The paper is an extension of the authors' earlier work with some changes to the methodology. In this paper, the authors have identified new attributes and scaled-down the attributes identified earlier, which would help identify the learner's learning style. The authors implemented classification algorithms and compared the accuracy of the different algorithms on the dataset. Various interesting patterns are observed in learner's behaviour while learning different types of concepts in different situations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002153",
    "keywords": [
      "Active learning (machine learning)",
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Economics",
      "Extension (predicate logic)",
      "History",
      "Learning styles",
      "Machine learning",
      "Management",
      "Mathematics education",
      "Multi-task learning",
      "Programming language",
      "Psychology",
      "Style (visual arts)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Rasheed",
        "given_name": "Fareeha"
      },
      {
        "surname": "Wahid",
        "given_name": "Abdul"
      }
    ]
  },
  {
    "title": "APFA: Automated product feature alignment for duplicate detection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114759",
    "abstract": "To keep up with the growing interest of using Web shops for product comparison, we have developed a method that targets the problem of product duplicate detection. If duplicates can be discovered correctly and quickly, customers can compare products in an efficient manner. We build upon the state-of-the-art Multi-component Similarity Method (MSM) for product duplicate detection by developing an automated pre-processing phase that occurs before the similarities between products are calculated. Specifically, in this prior phase the features of products are aligned between Web shops, using metrics such as the data type, coverage, and diversity of each key, as well as the distribution and used measurement units of their corresponding values. With this information, the values of these keys can be more meaningfully and efficiently employed in the process of comparing products. Applying our method to a real-world dataset of 1629 TV’s across 4 Web shops, we find that we increase the speed of the product similarity phase by roughly a factor 3 due to fewer meaningless comparisons, an improved brand analyzer, and a renewed title analyzer. Moreover, in terms of quality of duplicate detection, we significantly outperform MSM with an F 1 -measure of 0.746 versus 0.525.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002001",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Data mining",
      "Database",
      "Epistemology",
      "Feature (linguistics)",
      "Geometry",
      "Image (mathematics)",
      "Key (lock)",
      "Linguistics",
      "Mathematics",
      "Measure (data warehouse)",
      "Operating system",
      "Philosophy",
      "Process (computing)",
      "Product (mathematics)",
      "Quality (philosophy)",
      "Similarity (geometry)",
      "Spectrum analyzer",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Valstar",
        "given_name": "Nick"
      },
      {
        "surname": "Frasincar",
        "given_name": "Flavius"
      },
      {
        "surname": "Brauwers",
        "given_name": "Gianni"
      }
    ]
  },
  {
    "title": "Harnessing heterogeneous social networks for better recommendations: A grey relational analysis approach",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114771",
    "abstract": "Most of the extant studies in social recommender system are based on explicit social relationships, while the potential of implicit relationships in the heterogeneous social networks remains largely unexplored. This study proposes a new approach to designing a recommender system by employing grey relational analysis on the heterogeneous social networks. It starts with the establishment of heterogeneous social networks through the user-item bipartite graph, user social network graph and user-attribute bipartite graph; and then uses grey relational analysis to identify implicit social relationships, which are then incorporated into the matrix factorization model. Five experiments were conducted to test the performance of our approach against four state-of-the-art baseline methods. The results show that compared with the baseline methods, our approach can effectively alleviate the sparsity problem, because the heterogeneous social network provides richer information. In addition, the grey relational analysis method has the advantage of low requirements for data size and efficiently relieves the cold start problem. Furthermore, our approach saves processing time, thus increases recommendation efficiency. Overall, the proposed approach can effectively improve the accuracy of rating prediction in social recommendations and provide accurate and efficient recommendation service for users.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002128",
    "keywords": [
      "Artificial intelligence",
      "Baseline (sea)",
      "Biology",
      "Bipartite graph",
      "Computer science",
      "Data mining",
      "Eigenvalues and eigenvectors",
      "Evolutionary biology",
      "Extant taxon",
      "Geology",
      "Graph",
      "Heterogeneous network",
      "Machine learning",
      "Matrix decomposition",
      "Oceanography",
      "Physics",
      "Quantum mechanics",
      "Recommender system",
      "Social graph",
      "Social media",
      "Social network (sociolinguistics)",
      "Social network analysis",
      "Telecommunications",
      "Theoretical computer science",
      "Wireless",
      "Wireless network",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Weng",
        "given_name": "Lijuan"
      },
      {
        "surname": "Zhang",
        "given_name": "Qishan"
      },
      {
        "surname": "Lin",
        "given_name": "Zhibin"
      },
      {
        "surname": "Wu",
        "given_name": "Ling"
      }
    ]
  },
  {
    "title": "P300 event-related potential detection using one-dimensional convolutional capsule networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114701",
    "abstract": "The main challenge in creating a brain-computer interface (BCI) is establishing an effective brain signal recognition model suitable for achieving direct communication between humans and computers. Recently, various deep learning-based methods have been proposed to improve the performance of P300 event-related potentials (ERPs) for BCI. However, during the detection of P300 ERP signals, even electroencephalogram (EEG) signals from the same person are inconsistent and may be significantly distorted, resulting in impaired classification accuracy in many deep learning methods. Here, we propose a machine learning model based on a one-dimensional convolutional capsule network (1D-CapsNet). This network topology can effectively detect P300 ERP signals in the time domain, thereby achieving a better detection performance than can the current convolutional neural network (CNN)-based methods. Two classifiers based on the 1D-CapsNet model are proposed, namely, 1D-CapsNet-64 and 1D-CapsNet-8, which are used for classifying EEG data with 64 and 8 electrodes, respectively. These two classifiers are tested and compared on dataset II of the third BCI competition. The results show that the 1D-CapsNet-64 classifier obtains the best character recognition rate result (96%). The proposed method is superior to both state-of-the-art CNN-based methods and various traditional machine learning methods. The experimental results reveal the feasibility of our proposed method for detecting P300 ERP signals. The proposed method is expected to expand the concept of EEG signal recognition pattern and improve BCI design and applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001421",
    "keywords": [
      "Artificial intelligence",
      "Brain–computer interface",
      "Classifier (UML)",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Electroencephalography",
      "Feature extraction",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Psychiatry",
      "Psychology"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Xiang"
      },
      {
        "surname": "Xie",
        "given_name": "Qingsheng"
      },
      {
        "surname": "Lv",
        "given_name": "Jian"
      },
      {
        "surname": "Huang",
        "given_name": "Haisong"
      },
      {
        "surname": "Wang",
        "given_name": "Weixing"
      }
    ]
  },
  {
    "title": "Algorithm selection for solving educational timetabling problems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114694",
    "abstract": "In this paper, we present the construction process of a per-instance algorithm selection model to improve the initial solutions of Curriculum-Based Course Timetabling (CB-CTT) instances. Following the meta-learning framework, we apply a hybrid approach that integrates the predictions of a classifier and linear regression models to estimate and compare the performance of four meta-heuristics across different problem sub-spaces described by seven types of features. Rather than reporting the average accuracy, we evaluate the model using the closed SBS-VBS gap, a performance measure used at international algorithm selection competitions. The experimental results show that our model obtains a performance of 0.386, within the range obtained by per-instance algorithm selection models in other combinatorial problems. As a result of the process, we conclude that the performance variation between the meta-heuristics has a significant role in the effectiveness of the model. Therefore, we introduce statistical analyses to evaluate this factor within per-instance algorithm portfolios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001354",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Composite material",
      "Computer science",
      "Heuristics",
      "Machine learning",
      "Materials science",
      "Mathematical optimization",
      "Mathematics",
      "Model selection",
      "Operating system",
      "Process (computing)",
      "Range (aeronautics)",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "de la Rosa-Rivera",
        "given_name": "Felipe"
      },
      {
        "surname": "Nunez-Varela",
        "given_name": "Jose I."
      },
      {
        "surname": "Ortiz-Bayliss",
        "given_name": "José C."
      },
      {
        "surname": "Terashima-Marín",
        "given_name": "Hugo"
      }
    ]
  },
  {
    "title": "Detecting abusive Instagram comments in Turkish using convolutional Neural network and machine learning methods",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114802",
    "abstract": "Instagram is a free photo-sharing platform where each user has a profile and can upload photos for followers to view, like, and comment. Abusive comments on images can be humiliating and harmful to those who share photos. Developing a comment filter in languages other than English is difficult and time-consuming. This paper proposes a dataset called Abusive Turkish Comments (ATC) to detect abusive Instagram comments in Turkish. It is composed of a large number of Instagram comments posted to tabloid and sports accounts (i.e., 10,528 abusive and 19,826 not-abusive). It is the first public dataset dedicated to detecting abusive Turkish messages, as far as we know. The sentiment annotation has been done in sentence-level by assigning polarity to each comment. The performance of the abusive message detection models was evaluated using several performance metrics: Convolutional Neural Network (CNN), five well-known classifiers (i.e., Naive Bayes, Support Vector Machine, Decision Tree, Random Forest, and Logistic Regression), and two reweighted classifiers (i.e., Adaptive Boosting (AdaBoost), eXtreme Gradient Boosting (XGBoost)) were compared in terms of F1-score, precision, and recall. The results showed that the best performance (i.e., Micro-averaged F1-score: 0.974, Macro-averaged F1-score: 0.973, Kappa-value: 0.946) was yielded by the CNN model on the oversampled ATC dataset. The abusive message detection model proposed in this study can contribute to the development of Turkish comment filters on Instagram. Different model combinations are considered to select the best model that gives better recognition accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002438",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Decision tree",
      "F1 score",
      "Linguistics",
      "Machine learning",
      "Naive Bayes classifier",
      "Philosophy",
      "Random forest",
      "Sentiment analysis",
      "Support vector machine",
      "Turkish"
    ],
    "authors": [
      {
        "surname": "Karayiğit",
        "given_name": "Habibe"
      },
      {
        "surname": "İnan Acı",
        "given_name": "Çiğdem"
      },
      {
        "surname": "Akdağlı",
        "given_name": "Ali"
      }
    ]
  },
  {
    "title": "An improved multi-objective whale optimization algorithm for the hybrid flow shop scheduling problem considering device dynamic reconfiguration processes",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114793",
    "abstract": "Manufacturing industries frequently encounter production scheduling problems containing device dynamic reconfiguration processes (DRP). DRP refers to dynamic device adjustments (such as replacement of tools), leading to changes in the devices’ actual processing time. It has a severe impact on the production schedule. Nevertheless, there is scarcely research upon hybrid flow shop scheduling problem (HFSP) with DRP. Besides, it is necessary to consider multiple conflict objectives in the HFSP. Thus, the multi-objective HFSP with DRP (MOHFSP-DRP) is significant in both theoretical research and application. This paper first proposes a multi-objective mathematical model (MOHFSP-DRP) that simultaneously considers the DRP and devices’ adjustable processing modes. The bi-objective of this model is to minimize both the makespan and the whole device’s energy consumption. This study then proposes an improved multi-objective whale optimization algorithm (IMOWOA) to solve the MOHFSP-DRP and obtain the Pareto-based optimal solution set. After that, to verify the proposed method’s effectiveness, numerical experiments are implemented based on the real-world cases in a Chinese company’s digital hot-rolling workshop. Results denote that the presented IMOWOA is superior to SPEA2 and NSGA-II. Finally, the MOHFSP-DRP model and IMOWOA are applied to a real-world hot-rolling shop successfully. The real-world cases verify the proposed IMOWOA can tackle the presented MOHFSP-DRP very well.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002347",
    "keywords": [
      "Algorithm",
      "Biology",
      "Computer science",
      "Control reconfiguration",
      "Ecology",
      "Embedded system",
      "Energy consumption",
      "Flow shop scheduling",
      "Job shop scheduling",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Multi-objective optimization",
      "Operating system",
      "Pareto principle",
      "Schedule",
      "Scheduling (production processes)"
    ],
    "authors": [
      {
        "surname": "Yankai",
        "given_name": "Wang"
      },
      {
        "surname": "Shilong",
        "given_name": "Wang"
      },
      {
        "surname": "Dong",
        "given_name": "Li"
      },
      {
        "surname": "Chunfeng",
        "given_name": "Shen"
      },
      {
        "surname": "Bo",
        "given_name": "Yang"
      }
    ]
  },
  {
    "title": "An efficient multilevel thresholding based satellite image segmentation approach using a new adaptive cuckoo search algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114633",
    "abstract": "It is the most challenging and difficult task to segment a satellite image because of its complete randomness, multiple regions of interest, weak correlation with pixels, and regions of ambiguity. There are several Nature-inspired algorithms available, which are used to overcome these difficulties and those are more efficient to generate the best threshold value for the segmentation of satellite images. Though various modern methodologies opt for better results but methods have some drawbacks too like techniques are computationally expensive and time-consuming. In this paper, we have proposed a more effective satellite image segmentation approach using a new adaptive cuckoo search (ACS) algorithm. The result obtained from the projected technique is compared with CSMcCulloch incorporating McCulloch’s method for levy flight generation in Cuckoo Search (CS) algorithm by using two different objective functions namely Otsu’s method and Tsallis entropy function. The measurement techniques such as PSNR, MSE, FSIM, SSIM, UIQI, and computational time in term of CPU running time have been considered for validating and evaluating the proposed method. This proposed algorithm technique has resulted in improve segmentation quality of satellite images and reduced computational time. The analysis of the convergence rate proves that ACS is superior to the CSMcCulloch algorithm for reaching the global convergence rate. These experimental outcomes help to encourage researchers in different domains such as computer vision, application of medical image analysis, machine learning as well as deep learning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000749",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Cuckoo search",
      "Image (mathematics)",
      "Image segmentation",
      "Particle swarm optimization",
      "Pixel",
      "Segmentation",
      "Thresholding"
    ],
    "authors": [
      {
        "surname": "Rahaman",
        "given_name": "Jarjish"
      },
      {
        "surname": "Sing",
        "given_name": "Mihir"
      }
    ]
  },
  {
    "title": "PODCD: Probabilistic overlapping dynamic community detection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114650",
    "abstract": "Community detection is an important task to reveal hidden structures of real-world complex networks which are vary over time. Most of the existing works on the dynamic community detection assumes the sparse connectivity between communities and supposes that the number of nodes and communities in different snapshots is constant. In this work, a probabilistic overlapping community detection method called PODCD is proposed that considers the task of detecting communities as a non-negative matrix factorization problem. The proposed method considers the more likely assumption of dense connections between communities and utilizes a probabilistic model to control the dynamics of community structure. The proposed method uses the block coordinate decent method to solve the objective function of the matrix factorization model. This solver estimates non-negative latent factor to speeds up the computation of gradients. We demonstrate the efficiency of the proposed method by performing experiments on several synthetic and real-world dynamic networks. The obtained results reveal that the proposed method outperforms the earlier algorithms on evolving networks in terms of well-known evaluation criteria.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000919",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Block (permutation group theory)",
      "Computation",
      "Computer science",
      "Data mining",
      "Economics",
      "Eigenvalues and eigenvectors",
      "Evolutionary biology",
      "Factorization",
      "Function (biology)",
      "Geometry",
      "Machine learning",
      "Management",
      "Mathematics",
      "Matrix decomposition",
      "Non-negative matrix factorization",
      "Physics",
      "Probabilistic logic",
      "Programming language",
      "Quantum mechanics",
      "Solver",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Bahadori",
        "given_name": "Sondos"
      },
      {
        "surname": "Zare",
        "given_name": "Hadi"
      },
      {
        "surname": "Moradi",
        "given_name": "Parham"
      }
    ]
  },
  {
    "title": "Detecting anomalies in X-ray diffraction images using convolutional neural networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114740",
    "abstract": "Our understanding of life is based upon the interpretation of macromolecular structures and their dynamics. Almost 90% of currently known macromolecular models originated from electron density maps constructed using X-ray diffraction images. Even though diffraction images are critical for structure determination, due to their vast amounts and noisy, non-intuitive nature, their quality is rarely inspected. In this paper, we use recent advances in machine learning to automatically detect seven types of anomalies in X-ray diffraction images. For this purpose, we utilize a novel X-ray beam center detection algorithm, propose three different image representations, and compare the predictive performance of general-purpose classifiers and deep convolutional neural networks (CNNs). In benchmark tests on a set of 6,311 X-ray diffraction images, the proposed CNN achieved between 87% and 99% accuracy depending on the type of anomaly. Experimental results show that the proposed anomaly detection system can be considered suitable for early detection of sub-optimal data collection conditions and malfunctions at X-ray experimental stations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001810",
    "keywords": [
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Condensed matter physics",
      "Convolutional neural network",
      "Diffraction",
      "Geodesy",
      "Geology",
      "Image (mathematics)",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Czyzewski",
        "given_name": "Adam"
      },
      {
        "surname": "Krawiec",
        "given_name": "Faustyna"
      },
      {
        "surname": "Brzezinski",
        "given_name": "Dariusz"
      },
      {
        "surname": "Porebski",
        "given_name": "Przemyslaw Jerzy"
      },
      {
        "surname": "Minor",
        "given_name": "Wladek"
      }
    ]
  },
  {
    "title": "Latent factor recommendation models for integrating explicit and implicit preferences in a multi-step decision-making process",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114772",
    "abstract": "Recommendation models are vital to the success of recommender systems. The latent factor model is one of the outstanding collaborative recommendation models. It faces many difficulties due to the lack of quality and quantity of preferences observed from users. An effective approach is to use both types of user preferences, which are implicit and explicit, in the latent factor models. In this paper, we aim to propose two different latent factor models, namely SC1 and SC2, for integrating these two types of user preferences in a multi-step decision-making process, instead of just a single step as in previous studies. SC1 works well for experienced users, who have a lot of field knowledge, while SC2 is suitable for users with less domain knowledge, who do not exactly know what they want in the system.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100213X",
    "keywords": [
      "Artificial intelligence",
      "Collaborative filtering",
      "Computer science",
      "Decision-making",
      "Domain (mathematical analysis)",
      "Economics",
      "Epistemology",
      "Factor (programming language)",
      "Factor analysis",
      "Field (mathematics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Operations management",
      "Philosophy",
      "Process (computing)",
      "Programming language",
      "Purchasing",
      "Pure mathematics",
      "Quality (philosophy)",
      "Recommender system",
      "User interface",
      "User modeling"
    ],
    "authors": [
      {
        "surname": "Nguyen Hoai Nam",
        "given_name": "Le"
      }
    ]
  },
  {
    "title": "An evolutionary approach to implement logic circuits on three dimensional FPGAs",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114780",
    "abstract": "Three Dimensional Field Programmable Gate Arrays (3D FPGAs) recently are presented as the next generation of the FPGA family to continue the integration of more transistors on a single chip seamlessly. The 3D FPGA are fabricated by stacking several layers of semiconductor substrates and the interconnection among layers are realized using Through Silicon Vias (TSVs). Despite their benefits regarding less area and higher speed, 3D FPGAs encounter two major problems; huge size of single TSV and trapping generated heat in inner layers. To handle these problems, we propose a complete Computer Aided Design (CAD) flow to implement an arbitrary logic circuit on 3D FPGA. Prtitioning, Placement, and Routing are primary stages of the proposed CAD flow. The partitioning and placement stages of the flow are based on Simulated Annealing algorithm. Furthermore, the routing stage is a modified version of the Pathfinder algorithm. Unbalanced SA based partitioning tremendously reduces the required TSVs along with distribution of highly active circuit’s modules on the bottom layers and constructing thermal channels facilitate transferring the generated heat in intermediate layers. Simulation results show more than 60%, 65%, and 23% reduction in TSV count, heat transfer performance, and area respectively, along with 4% increase in critical path delay. In addition, comparison between 2D FPGA and 3D FPGA with our proposed architecture (including 2 tier), shows that the circuit speed increases by 28.61%, and minimum channel width decreases by 30.47%. Finally, the results of comparison between 2-tier and 4-tier in 3D FPGA show that circuit speed and minimum channel width increase by 15.95% and 15.92% in 4-tier, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002219",
    "keywords": [
      "Algorithm",
      "Computer network",
      "Computer science",
      "Embedded system",
      "Field-programmable gate array",
      "Integrated circuit",
      "Interconnection",
      "Operating system",
      "Parallel computing",
      "Routing (electronic design automation)",
      "Simulated annealing",
      "Three-dimensional integrated circuit"
    ],
    "authors": [
      {
        "surname": "Rahimi",
        "given_name": "H."
      },
      {
        "surname": "Jahanirad",
        "given_name": "H."
      }
    ]
  },
  {
    "title": "Detection of counterfeit coins based on 3D height-map image analysis",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114801",
    "abstract": "Detecting a counterfeit coin using 2D image processing is nearly impossible in some cases, especially when the coin is damaged, corroded or worn out. Edge detection is one of the most widely used techniques to extract features from 2D images. However, in 2D images, the height information is missing, losing the hidden characteristics. In this paper, we propose a 3D approach to detect and analyze the precipice borders from the coin surface and extract significant features to train an ensemble classification system. To extract the features, we also propose Binned Borders in Spherical Coordinates (BBSC) to analyze different parts of precipice borders at different polar and azimuthal angles. The proposed method is robust even against degradation which appears on shiny coins after 3D scanning. Therefore, there is no need to restore the degraded images before the feature extraction process. Here, the system has been trained and tested with four types of Danish and two types of Chinese coins. We take advantage of stack generalization to classify the coins and add the reject option to increase the reliability of the system. The results illustrate that the proposed method outperforms other counterfeit coin detectors. The accuracy obtained by testing Danish 1990, 1991, 1996, and 2008 datasets are 98.6%, 98.0%, 99.8%, and 99.9% respectively. In addition, results for half Yuan Chinese 1942 and one Yuan Chinese 1997 were 95.5% and 92.2% respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002426",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Counterfeit",
      "Enhanced Data Rates for GSM Evolution",
      "Generalization",
      "Geography",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Khazaee",
        "given_name": "Saeed"
      },
      {
        "surname": "Sharifi Rad",
        "given_name": "Maryam"
      },
      {
        "surname": "Suen",
        "given_name": "Ching Y."
      }
    ]
  },
  {
    "title": "Feature Selection for Classification using Principal Component Analysis and Information Gain",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114765",
    "abstract": "Feature Selection and classification have previously been widely applied in various areas like business, medical and media fields. High dimensionality in datasets is one of the main challenges that has been experienced in classifying data, data mining and sentiment analysis. Irrelevant and redundant attributes have also had a negative impact on the complexity and operation of algorithms for classifying data. Consequently, the algorithms record poor results or performance. Some existing work use all attributes for classification, some of which are insignificant for the task, thereby leading to poor performance. This paper therefore develops a hybrid filter model for feature selection based on principal component analysis and information gain. The hybrid model is then applied to support classification using machine learning techniques e.g. the Naïve Bayes technique. Experimental results demonstrate that the hybrid filter model reduces data dimensions, selects appropriate feature sets, and reduces training time, hence providing better classification performance as measured by accuracy, precision and recall..",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002062",
    "keywords": [
      "Artificial intelligence",
      "Component (thermodynamics)",
      "Computer science",
      "Computer vision",
      "Curse of dimensionality",
      "Data mining",
      "Dimensionality reduction",
      "Feature (linguistics)",
      "Feature selection",
      "Filter (signal processing)",
      "Information gain",
      "Information gain ratio",
      "Linguistics",
      "Machine learning",
      "Naive Bayes classifier",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Principal component analysis",
      "Support vector machine",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Odhiambo Omuya",
        "given_name": "Erick"
      },
      {
        "surname": "Onyango Okeyo",
        "given_name": "George"
      },
      {
        "surname": "Waema Kimwele",
        "given_name": "Michael"
      }
    ]
  },
  {
    "title": "Detecting anomalies in X-ray diffraction images using convolutional neural networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114740",
    "abstract": "Our understanding of life is based upon the interpretation of macromolecular structures and their dynamics. Almost 90% of currently known macromolecular models originated from electron density maps constructed using X-ray diffraction images. Even though diffraction images are critical for structure determination, due to their vast amounts and noisy, non-intuitive nature, their quality is rarely inspected. In this paper, we use recent advances in machine learning to automatically detect seven types of anomalies in X-ray diffraction images. For this purpose, we utilize a novel X-ray beam center detection algorithm, propose three different image representations, and compare the predictive performance of general-purpose classifiers and deep convolutional neural networks (CNNs). In benchmark tests on a set of 6,311 X-ray diffraction images, the proposed CNN achieved between 87% and 99% accuracy depending on the type of anomaly. Experimental results show that the proposed anomaly detection system can be considered suitable for early detection of sub-optimal data collection conditions and malfunctions at X-ray experimental stations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001810",
    "keywords": [
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Condensed matter physics",
      "Convolutional neural network",
      "Diffraction",
      "Geodesy",
      "Geology",
      "Image (mathematics)",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Czyzewski",
        "given_name": "Adam"
      },
      {
        "surname": "Krawiec",
        "given_name": "Faustyna"
      },
      {
        "surname": "Brzezinski",
        "given_name": "Dariusz"
      },
      {
        "surname": "Porebski",
        "given_name": "Przemyslaw Jerzy"
      },
      {
        "surname": "Minor",
        "given_name": "Wladek"
      }
    ]
  },
  {
    "title": "Chameleon Swarm Algorithm: A bio-inspired optimizer for solving engineering design problems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114685",
    "abstract": "This paper presents a novel meta-heuristic algorithm named Chameleon Swarm Algorithm (CSA) for solving global numerical optimization problems. The base inspiration for CSA is the dynamic behavior of chameleons when navigating and hunting for food sources on trees, deserts and near swamps. This algorithm mathematically models and implements the behavioral steps of chameleons in their search for food, including their behavior in rotating their eyes to a nearly 360°scope of vision to locate prey and grab prey using their sticky tongues that launch at high speed. These foraging mechanisms practiced by chameleons eventually lead to feasible solutions when applied to address optimization problems. The stability of the proposed algorithm was assessed on sixty-seven benchmark test functions and the performance was examined using several evaluation measures. These test functions involve unimodal, multimodal, hybrid and composition functions with different levels of complexity. An extensive comparative study was conducted to demonstrate the efficacy of CSA over other meta-heuristic algorithms in terms of optimization accuracy. The applicability of the proposed algorithm in reliably addressing real-world problems was demonstrated in solving five constrained and computationally expensive engineering design problems. The overall results of CSA show that it offered a favorable global or near global solution and better performance compared to other meta-heuristics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001263",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Geodesy",
      "Geography",
      "Global optimization",
      "Heuristic",
      "Heuristics",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Programming language",
      "Scope (computer science)",
      "Stability (learning theory)",
      "Swarm behaviour"
    ],
    "authors": [
      {
        "surname": "Braik",
        "given_name": "Malik Shehadeh"
      }
    ]
  },
  {
    "title": "Node classification using kernel propagation in graph neural networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114655",
    "abstract": "In this work, we introduce a kernel propagation method that enables graph neural networks (GNNs) to leverage higher-order network structural information without increasing the complexity of the networks. Recent studies have introduced GNNs that include higher-order neighborhood features containing global network information by propagating node features using a higher-order feature propagation rule. Though these GNNs have shown to improve node classification performance, they fail to include local connectivity information. Alternatively, GNNs also concatenate increasing orders of adjacency matrix in deeper layers in order to include higher-order structural information. In addition to global network information, GNNs also make use of node features which are network and node dependent features that serve to distinguish structurally isomorphic sub-structures within graphs. However, such node features may not always be available or depending on the network, may lead to deteriorating classification performance. Hence, to resolve these limitations, we propose a kernel propagation method that introduces a pre-processing step for GNNs to leverage higher-order structural features. The higher-order structural features are computed using a weighted random walk matrix which is node independent while using the first-order spectral propagation rule which explicitly considers local connectivity. Through our benchmark experiments, we find that the computed higher-order structural features are capable of replacing node dependent features while performing node classification task with performance on par with the state of the art approaches. Further, we also find that including both node features and higher-order structural features increases the performance of GNNs on large scale benchmark networks considered in this study. Our results show that considering local and global structural information as input to GNNs lead to an improvement in node classification performance in the absence/presence of node features without loss of performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000968",
    "keywords": [
      "Adjacency matrix",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Data mining",
      "Engineering",
      "Geodesy",
      "Geography",
      "Graph",
      "Leverage (statistics)",
      "Node (physics)",
      "Structural engineering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Arul Prakash",
        "given_name": "Sakthi Kumar"
      },
      {
        "surname": "Tucker",
        "given_name": "Conrad S."
      }
    ]
  },
  {
    "title": "Parameterized fuzzy-logic controllers for the attitude control of nanosatellites in low earth orbits. A comparative studio with PID controllers",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114679",
    "abstract": "The pointing requirements for a satellite (e.g. for its payload or for its subsystems) determine the different attitude control system operational modes that are necessary along its mission. Two examples are: a low cost mode for nominal attitude stabilization; and a faster mode for antenna pointing during communication phases. In this work, an attitude control algorithm based on fuzzy logic is presented to achieve an optimal performance of the controller for different operational requirements of the satellite mission. After analysing the design process and the response of previous fuzzy controllers, several improvements have been implemented concerning the number of membership functions, their shape, and the contents of the fuzzy rules, obtaining a parameterized final fuzzy inference system whose performance depends on eighteen parameters, six per each of the three control axes. The design proposed in this work will be tested soon on board the European Space Agency OPS-SAT mission and it is easily extensible to most nanosatellite missions in Low Earth Orbits. Different sets of the eighteen tuning parameters have been used to analyse the design and the performance of this new fuzzy controller. The values have been selected through multi-objective genetic optimizations to deal with two conflicting objectives: low cost and accuracy. A numerical performance comparison shows that the new design provides more efficient controllers than those used in previous works. In addition, the performance of the new fuzzy controller has been evaluated against traditional Proportional Integrative Derivative (PID) controllers for different satellite operational requirements. The values of the PID gains have also been selected through the same multi-objective genetic optimization. The comparisons show that the new fuzzy design provides great advantages over the PID strategy in most of the operational modes considered, and how the PID strategy has difficulties to produce a low cost control law profile. Furthermore, the new fuzzy design is able to improve the Pareto front to optimal points that are unreachable with the PID schema. These results open new possibilities for the application of intelligent controllers in the space industry.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001202",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Attitude control",
      "Biology",
      "Computer network",
      "Computer science",
      "Control (management)",
      "Control engineering",
      "Control theory (sociology)",
      "Controller (irrigation)",
      "Engineering",
      "Fuzzy control system",
      "Fuzzy logic",
      "Network packet",
      "PID controller",
      "Parameterized complexity",
      "Payload (computing)",
      "Temperature control"
    ],
    "authors": [
      {
        "surname": "Bello",
        "given_name": "Álvaro"
      },
      {
        "surname": "del Castañedo",
        "given_name": "Ástor"
      },
      {
        "surname": "Olfe",
        "given_name": "Karl Stephan"
      },
      {
        "surname": "Rodríguez",
        "given_name": "Jacobo"
      },
      {
        "surname": "Lapuerta",
        "given_name": "Victoria"
      }
    ]
  },
  {
    "title": "Novel automatic group identification approaches for group recommendation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114709",
    "abstract": "Group recommender systems are specialized in suggesting preferable products or services to a group of users rather than an individual by aggregating personal preferences of group members. In such expert systems, the initial task is to identify groups of similar users via clustering approaches as user groups are usually not predefined. However, clustering users into groups commonly suffer from sparsity, scalability, and complexity problems as the content in the domain proliferate. Moreover, group homogeneity and size are the critical parameters for organizing group members and enhancing their satisfaction. In this study, we propose novel automatic user grouping approaches by constructing a binary decision tree via bisecting k -means clustering for enhanced group formation and group size restriction. Furthermore, we propose adopting a genre-based mapping of user ratings into a tiny and dense vector to represent users, which both improves computation time for constructing the binary decision tree and enables eliminating adverse effects of sparsity. Finally, since the quality of group formation is not only dependent on conforming preferences but also to the demographic harmony among members, we further introduce utilizing similarities based on demographic characteristics along with the genre-based similarities. We propose applying two distinct strategies for small and large groups by decorating the genre-based similarities with demographic properties, which leads to a more homogeneous automatic group formation. Experiments performed on real-world benchmark datasets demonstrate that each proposed method outperforms its traditional rival significantly, and the final proposed method achieves significantly more qualified ranked recommendation lists than the state-of-the-art algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001500",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Database",
      "Decision tree",
      "Group (periodic table)",
      "Information retrieval",
      "Machine learning",
      "Organic chemistry",
      "Recommender system",
      "Scalability"
    ],
    "authors": [
      {
        "surname": "Yalcin",
        "given_name": "Emre"
      },
      {
        "surname": "Bilge",
        "given_name": "Alper"
      }
    ]
  },
  {
    "title": "Towards missing electric power data imputation for energy management systems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114743",
    "abstract": "Demand for electricity is gradually increasing in many countries. Efforts in related studies have been made for the application of data mining techniques over related electric power data for the development of more effective energy management systems. However, one major challenge is how to compensate for parts of the collected dataset, such as power consumption, voltage, or electric current that may be missing for a specific period of time. In the literature, several methods have been employed for imputation of the missing data, especially single feature value imputation. However, the performance of the different types of imputation methods, i.e. statistical and machine learning methods, for multiple missing features of electric power data has not been fully explored. Moreover, variations in their imputation performance during the summer/non-summer seasons and in the peak/off-peak/semi-peak times have not been investigated. In this paper, the performance of five well-known imputation methods for processing electric power data, two statistical methods, autoregressive integrated moving average (ARIMA) and linear interpolation (LI) models, and three machine learning methods, k-nearest neighbor (K-NN), multilayer perceptron (MLP), and support vector regression (SVR) is compared. The experimental results, based on electric power data for a two-year period in Taiwan, show that the machine learning methods generally perform better than the statistical ones, with K-NN and SVR performing the best. In particular, all of the imputation methods produced higher error rates during the summer season than the non-summer seasons. Moreover, the machine learning methods (especially K-NN) are better choices for the imputation of missing data during peak times, whereas the statistical methods (especially LI) are better for off-peak and semi-peak times.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001846",
    "keywords": [
      "Artificial intelligence",
      "Autoregressive integrated moving average",
      "Computer science",
      "Data mining",
      "Electric power",
      "Imputation (statistics)",
      "Machine learning",
      "Missing data",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Support vector machine",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Ming-Chang"
      },
      {
        "surname": "Tsai",
        "given_name": "Chih-Fong"
      },
      {
        "surname": "Lin",
        "given_name": "Wei-Chao"
      }
    ]
  },
  {
    "title": "Modeling the received signal strength intensity of Wi-Fi signal using Hidden Markov Models",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114726",
    "abstract": "Wi-Fi fingerprinting is one of the methods that are widely used to provide Location Based Services (LBS). Gaussian, or a mixture of Gaussians, is the preferred model used by Wi-Fi fingerprinting for LBS. Nevertheless, Received Signal Strength Intensity (RSSI) Wi-Fi histograms are skewed, and a Gaussian model is not well suited for modeling data when their histogram is skewed. In addition, another important characteristic present in the RSSI Wi-Fi temporal series is autocorrelation, which cannot be modeled using a Gaussian model. In this paper, we explore the feasibility of using Hidden Markov Models (HMM) to model RSSI Wi-Fi signals. The mathematical derivation of formulas to calculate autocorrelation based on the HMM parameters is presented. Exhaustive experimentation, using data sampled in a real scenario, was performed to test the dependency of the autocorrelation coefficients on the number of hidden states, and the number of iterations used when creating the HMM. The results are compared with autocorrelation coefficients calculated using the real data. Kullback–Leibler (KL) divergence was used to compare the similarity of the real histograms and those provided by a mixture of Gaussians and by an HMM. HMM models reported more accurate results than a mixture of Gaussians model in both cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001676",
    "keywords": [
      "Artificial intelligence",
      "Autocorrelation",
      "Computer science",
      "Divergence (linguistics)",
      "Gaussian",
      "Hidden Markov model",
      "Histogram",
      "Image (mathematics)",
      "Linguistics",
      "Mathematics",
      "Mixture model",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Belmonte-Fernández",
        "given_name": "Óscar"
      }
    ]
  },
  {
    "title": "An end-to-end approach and tool for BPMN process discovery",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114662",
    "abstract": "Information systems daily register a large amount of data in event logs. All these data can be used for the organizations to automatically discover their process models. However, the automatic construction of simple process models with consistently high and balanced fitness and precision remains a challenging task, which has attracted the attention in the scientific and organizational communities to develop suitable methods for creating high-quality business process models. In this paper, we present an approach to discover the model of a business process based on the Business Process Model and Notation (BPMN) standard from its behavior observed in event logs. Particularly, we propose: (1) a method to detect outlier behavior in a given event log; (2) a set of heuristic rules to discover join gateways associated with the closing of each of the split gateways in conformance with the rules of BPMN models; and (3) P-Miner, a tool that realizes our proposed approach to automatically discover and visualize process models. We use a state of the art process discovery technique as a basis for discovering XOR/AND gateways in the models. A set of experiments was carried out on real and artificial event logs for evaluating our proposal, considering the fitness and precision metrics to determine the quality of the built models. The amount of gateways in the resulting BPMN model and the time required to build the process models were estimated. The results reveal that the process models derived by our proposed approach exhibited competitive and more balanced results in fitness and precision than those derived with other discovery techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001032",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Business Process Model and Notation",
      "Business process",
      "Business process discovery",
      "Business process management",
      "Business process modeling",
      "Computer science",
      "Conformance checking",
      "Data mining",
      "Event (particle physics)",
      "Marketing",
      "Physics",
      "Process (computing)",
      "Process mining",
      "Process modeling",
      "Programming language",
      "Quantum mechanics",
      "Set (abstract data type)",
      "Work in process"
    ],
    "authors": [
      {
        "surname": "Marin-Castro",
        "given_name": "Heidy Marisol"
      },
      {
        "surname": "Tello-Leal",
        "given_name": "Edgar"
      }
    ]
  },
  {
    "title": "Occlusion-robust method for RGB-D 6-DOF object tracking with particle swarm optimization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114736",
    "abstract": "Visual trackers can be used to determine the trajectories of rigid 3D objects with 6-DOF. This functionality is required for many types of applications, such as augmented reality or robotics, and its efficiency is commonly related to the accuracy, robustness and execution time of these trackers. Several works contributed to the improvement of these techniques, among which stand out optimization-based, learning-based, and hybrid methods, that is, that use both cooperatively. However, despite the variety of 3D tracking object techniques, in general they have difficulty to perform tracking efficiently in challenging environments as those with occlusion or quick rotation and translation movements of the object. In this context, we propose improvements in the 6-DOF tracking of arbitrary 3D objects that uses particle swarm optimization and try to overcome those limitations. In this type of tracker, a top-down approach is used in which, for each video frame, a pose hypothesis set is created and optimized in order to find the pose as close as possible to the tracked object real pose. To perform this task, a fitness function uses 3D scene information obtained from RGB-D sensors to evaluate each hypothesis. The proposed approach tries to solve problems that in practice reduce the efficiency of this optimization, and are associated with the dynamism of the search space, the definition of the limits of the search subspace at each iteration, the exclusion of global optimum of search subspace, the premature convergence to local optima, the different types of object occlusion, the definition of a fitness function independent of the tracked object class and environment characteristics, and the execution time for processing all steps of the particle swarm optimization. Therefore, in this scenario we present a tracker algorithm with a novel fitness function based on the harmonic mean of 3D coordinate, color and normal vector distances. We also introduce a new approach for computing the boundaries of solutions subspace at runtime, observing the inertia of the target object. We present a robust method for filtering the visible model points. Finally, in order to improve execution time, we employ a dynamic region of interest selection in the scene point cloud and implement the particle swarm optimization algorithm completely in GPU. Experiments have shown that such changes led to improvements in accuracy, robustness and execution time. When compared to a state-of-the-art learning-based technique, our tracker was, on average, 19.3% and 16.3% more accurate with respect to translation and rotation errors, respectively, and presented 43.2% less tracking fails. Our tracker was also 5–17 times faster than an existing particle swarm optimization-based technique.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001779",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "BitTorrent tracker",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Eye tracking",
      "Gene",
      "Machine learning",
      "Object (grammar)",
      "Particle swarm optimization",
      "Robustness (evolution)",
      "Subspace topology",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "dos Santos Júnior",
        "given_name": "José Guedes"
      },
      {
        "surname": "Silva do Monte Lima",
        "given_name": "João Paulo"
      },
      {
        "surname": "Teichrieb",
        "given_name": "Veronica"
      }
    ]
  },
  {
    "title": "Learning to recommend via random walk with profile of loan and lender in P2P lending",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114763",
    "abstract": "P2P Lending recommender systems are embracing portraying schemes to obtain profiles of both loan and lender, and thus to overcome inherent limitations of general recommendation models. A successful recommendation method requires proper handling the interactions between loans and lenders. We argue that three fundamental problems need to be addressed: 1) how to fully utilize different properties of loan for establishing its profile, 2) how to adapt social and psychological factors for enhancing lender’s profile, and 3) how to exploit the interactions between loan and lender. To the best of our knowledge, there lacks a unified framework that addresses these problems. In this work, we contribute a new solution named RRWP (Recommendation via Random Walk with Profile of loan and lender), for learning recommender systems for P2P Lending. We develop a hybrid graph random walk-based model to capture the complicated interactions between loans and lenders. In particular, the algorithm consists of three stages for better P2P lending recommendation. (1) Loan profile is built by utilizing attributes of both loan and borrower; (2) Lender profile is established via his social and psychological factors together with interactions between loan and lender; (3) A hybrid graph is constructed based on which random walk is performed to recommend for both loan and lender. Extensive experiments on real-world dataset demonstrate the effectiveness of RRWP. Further analysis reveals that profile modelling is consistent with the basic investment theory in finance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002049",
    "keywords": [
      "Actuarial science",
      "Business",
      "Computer science",
      "Computer security",
      "Exploit",
      "Finance",
      "Graph",
      "Loan",
      "Machine learning",
      "Mathematics",
      "Random walk",
      "Recommender system",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yuhang"
      },
      {
        "surname": "Ma",
        "given_name": "Huifang"
      },
      {
        "surname": "Jiang",
        "given_name": "Yanbin"
      },
      {
        "surname": "Li",
        "given_name": "Zhixin"
      }
    ]
  },
  {
    "title": "A network-based sparse and multi-manifold regularized multiple non-negative matrix factorization for multi-view clustering",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114783",
    "abstract": "Multi-view clustering has attracted increasing attention in recent years since many real data sets are usually gathered from different sources or described by different feature types. Amongst various existing multi-view clustering algorithms, those that are based on non-negative matrix factorization (NMF) have exhibited superior performance. However, NMF decomposing original data directly fails to exploit global relationships between data samples and cannot be applied to datasets that are not strictly non-negative. In this paper, a network-based sparse and multi-manifold regularized multiple NMF (NSM_MNMF) for multi-view clustering is proposed, where multi-view data is transformed into multiple networks, and NMF is used to jointly factorize transformed multiple networks for capturing the shared cluster structure embedded in different views. Furthermore, sparse and multi-manifold regularization are incorporated into NMF to keep the intrinsic geometrical information of the multi-view network manifold space. Networks characterize intra-view similarity, and joint factorization reveals inter-view similarity across distinct views, while using NMF to decompose the networks instead of the original data means NSM_MNMF can be applied to datasets that are not strictly non-negative and the clustering results are interpretable. Extensive experiments are conducted on nine real data sets to assess the method proposed, and the results illustrate that NSM_MNMF outperforms other baseline approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002244",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Computer science",
      "Data mining",
      "Dimensionality reduction",
      "Eigenvalues and eigenvectors",
      "Engineering",
      "Image (mathematics)",
      "Manifold (fluid mechanics)",
      "Matrix decomposition",
      "Mechanical engineering",
      "Non-negative matrix factorization",
      "Nonlinear dimensionality reduction",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Lihua"
      },
      {
        "surname": "Du",
        "given_name": "Guowang"
      },
      {
        "surname": "Lü",
        "given_name": "Kevin"
      },
      {
        "surname": "Wang",
        "given_name": "Lizhen"
      }
    ]
  },
  {
    "title": "Towards a self-sufficient face verification system",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114734",
    "abstract": "The absence of a previous collaborative manual enrolment represents a significant handicap towards designing a face verification system for face re-identification purposes. In this scenario, the system must learn the target identity incrementally, using data from the video stream during the operational authentication phase. So, manual labelling cannot be assumed apart from the first few frames. On the other hand, even the most advanced methods trained on large-scale and unconstrained datasets suffer performance degradation when no adaptation to specific contexts is performed. This work proposes an adaptive face verification system, for the continuous re-identification of target identity, within the framework of incremental unsupervised learning. Our Dynamic Ensemble of SVM is capable of incorporating non-labelled information to improve the performance of any model, even when its initial performance is modest. The proposal uses the self-training approach and is compared against other classification techniques within this same approach. Results show promising behaviour in terms of both knowledge acquisition and impostor robustness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001755",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Botany",
      "Chemistry",
      "Computer science",
      "Face (sociological concept)",
      "Facial recognition system",
      "Gene",
      "Identification (biology)",
      "Identity (music)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Robustness (evolution)",
      "Social science",
      "Sociology",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Lopez-Lopez",
        "given_name": "Eric"
      },
      {
        "surname": "Regueiro",
        "given_name": "Carlos V."
      },
      {
        "surname": "Pardo",
        "given_name": "Xosé M."
      },
      {
        "surname": "Franco",
        "given_name": "Annalisa"
      },
      {
        "surname": "Lumini",
        "given_name": "Alessandra"
      }
    ]
  },
  {
    "title": "A Machine Learning-based DSS for mid and long-term company crisis prediction",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114758",
    "abstract": "In the field of detection and prediction of company defaults and bankruptcy, significant effort has been devoted to evaluating financial ratios as predictors using statistical models and machine learning techniques. This problem becomes crucially important when financial decision-makers are provided with predictions on which to act, based on the output of prediction models. However, research has shown that such predictors are sufficiently accurate in the short-term, with the focus mainly directed towards large and medium-large companies. In contrast, in this paper, we focus on mid- and long-term bankruptcy prediction (up to 60 months) targeting small and/or medium enterprises. The key contribution of this study is a substantial improvement of the prediction accuracy in the short-term (12 months) using machine learning techniques, compared to the state-of-the-art, while also making accurate mid- and long-term predictions (measure of the area under the ROC curve of 0.88 with a 60 month prediction horizon). Extensive computational tests on the entire set of companies in Italy highlight the efficiency and accuracy of the developed method, as well as demonstrating the possibility of using it as a tool for the development of strategies and policies for entire economic systems. Considering the recent COVID-19 pandemic, we show how our method can be used as a viable tool for large-scale policy-making.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001998",
    "keywords": [
      "Artificial intelligence",
      "Bankruptcy",
      "Bankruptcy prediction",
      "Business",
      "Computer science",
      "Computer security",
      "Default",
      "Econometrics",
      "Economics",
      "Field (mathematics)",
      "Finance",
      "Financial crisis",
      "Focus (optics)",
      "Key (lock)",
      "Machine learning",
      "Macroeconomics",
      "Mathematics",
      "Optics",
      "Physics",
      "Predictive modelling",
      "Pure mathematics",
      "Quantum mechanics",
      "Support vector machine",
      "Telecommunications",
      "Term (time)",
      "Warning system"
    ],
    "authors": [
      {
        "surname": "Perboli",
        "given_name": "Guido"
      },
      {
        "surname": "Arabnezhad",
        "given_name": "Ehsan"
      }
    ]
  },
  {
    "title": "Opposition-based Laplacian Equilibrium Optimizer with application in Image Segmentation using Multilevel Thresholding",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114766",
    "abstract": "This paper proposes a modified version of freshly developed Equilibrium Optimizer (EO) for segmentation of gray-scale images using multi-level thresholding. Laplace distribution based random walk is utilized to update the concentration of search agents around equilibrium candidates (best solution) towards to attain optimal position (equilibrium state) for achieving better diversification of search space. An opposition based learning (OBL) mechanism is then applied with hybridization of the varying acceleration coefficient to the best solution for accelerating exploitation at a later phase of each iteration. The performance of proposed Opposition-based Laplacian Equilibrium Optimizer (OB-L-EO) is validated using test suites containing benchmark problems of wide varieties of complexities. Various analyses are conducted including Wilcoxon ranksum test for statistical significance, convergence curves and distance between solution before and after applying modification strategies. Finally, the proposed OB-L-EO is employed for image segmentation by utilizing Otsu’s interclass variance function to obtain optimum threshold values for image segmentation. The performance of the proposed algorithm is verified by determining mean value of interclass variance and peak signal to noise ratio (PSNR). The obtained results are then compared and analysed with other metaheuristics algorithms to show superiority of proposed OB-L-EO.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002074",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Friedman test",
      "Image (mathematics)",
      "Image segmentation",
      "Laplace operator",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Statistical hypothesis testing",
      "Statistics",
      "Thresholding"
    ],
    "authors": [
      {
        "surname": "Dinkar",
        "given_name": "Shail Kumar"
      },
      {
        "surname": "Deep",
        "given_name": "Kusum"
      },
      {
        "surname": "Mirjalili",
        "given_name": "Seyedali"
      },
      {
        "surname": "Thapliyal",
        "given_name": "Shivankur"
      }
    ]
  },
  {
    "title": "Use of a domain-specific ontology to support automated document categorization at the concept level: Method development and evaluation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114681",
    "abstract": "Voluminous, conveniently accessible textual documents, created and disseminated by modern information technology, makes automated document organization increasingly important for both individuals and organizations. Many existing techniques rely on document content analysis that classifies new, unlabeled documents by examining the similarity based on the overlap between their important features and the representative features of each document category. However, the performance of feature-based techniques can be significantly hindered by word mismatch and ambiguity problems. As a remedy, this study takes a concept-based approach and propose a text categorization method that incorporates a domain-specific ontology to support automated document categorization more effectively. The proposed method classifies documents according to their respective range of relevant concepts. We empirically evaluate our method versus several prevalent benchmarks that include feature-based k-nearest neighbors (kNN) and semantic-based techniques. The results show the proposed method more effective than the benchmark techniques; it achieves better performances when using a complete concept hierarchy without considering the hierarchical relationships among concepts. The proposed method illustrates how to incorporate a domain-specific ontology to improve document classification. Our method is computationally efficient because it produces a concept space of relatively few dimensionalities and does not require semantic space reconstruction as new documents arrive. Moreover, the relationships and patterns for classifying documents, generated by our method, are explicit and comprehensible.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001226",
    "keywords": [
      "Ambiguity",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Categorization",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Document classification",
      "Document clustering",
      "Domain (mathematical analysis)",
      "Economics",
      "Epistemology",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Hierarchy",
      "Image (mathematics)",
      "Information retrieval",
      "Linguistics",
      "Market economy",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Ontology",
      "Philosophy",
      "Programming language",
      "Similarity (geometry)",
      "Text categorization"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Yen-Hsien"
      },
      {
        "surname": "Hu",
        "given_name": "Paul Jen-Hwa"
      },
      {
        "surname": "Tsao",
        "given_name": "Wan-Jung"
      },
      {
        "surname": "Li",
        "given_name": "Liang"
      }
    ]
  },
  {
    "title": "Graph convolutional neural networks with node transition probability-based message passing and DropNode regularization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114711",
    "abstract": "Graph convolutional neural networks (GCNNs) have received much attention recently, owing to their capability in handling graph-structured data. Among the existing GCNNs, many methods can be viewed as instances of a neural message passing motif; features of nodes are passed around their neighbors, aggregated and transformed to produce better nodes’ representations. Nevertheless, these methods seldom use node transition probabilities, a measure that has been found useful in exploring graphs. Furthermore, when the transition probabilities are used, their transition direction is often improperly considered in the feature aggregation step, resulting in an inefficient weighting scheme. In addition, although a great number of GCNN models with increasing level of complexity have been introduced, the GCNNs often suffer from over-fitting when being trained on small graphs. Another issue of the GCNNs is over-smoothing, which tends to make nodes’ representations indistinguishable. This work presents a new method to improve the message passing process based on node transition probabilities by properly considering the transition direction, leading to a better weighting scheme in nodes’ features aggregation compared to the existing counterpart. Moreover, we propose a novel regularization method termed DropNode to address the over-fitting and over-smoothing issues simultaneously. DropNode randomly discards part of a graph, thus it creates multiple deformed versions of the graph, leading to data augmentation regularization effect. Additionally, DropNode lessens the connectivity of the graph, mitigating the effect of over-smoothing in deep GCNNs. Extensive experiments on eight benchmark datasets for node and graph classification tasks demonstrate the effectiveness of the proposed methods in comparison with the state of the art.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001524",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Distributed computing",
      "Graph",
      "Medicine",
      "Message passing",
      "Radiology",
      "Regularization (linguistics)",
      "Smoothing",
      "Theoretical computer science",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Do",
        "given_name": "Tien Huu"
      },
      {
        "surname": "Nguyen",
        "given_name": "Duc Minh"
      },
      {
        "surname": "Bekoulis",
        "given_name": "Giannis"
      },
      {
        "surname": "Munteanu",
        "given_name": "Adrian"
      },
      {
        "surname": "Deligiannis",
        "given_name": "Nikos"
      }
    ]
  },
  {
    "title": "Fully automatic electrocardiogram classification system based on generative adversarial network with auxiliary classifier",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114809",
    "abstract": "A generative adversarial network (GAN) based fully automatic electrocardiogram (ECG) arrhythmia classification system with high performance is presented in this paper. The generator (G) in our GAN is designed to generate various coupling matrix inputs conditioned on different arrhythmia classes for data augmentation. Our designed discriminator (D) is trained on both real and generated ECG coupling matrix inputs, and is extracted as an arrhythmia classifier upon completion of training for our GAN. After fine-tuning the D by including patient-specific normal beats estimated using an unsupervised algorithm, and generated abnormal beats by G that are usually rare to obtain, our fully automatic system showed superior overall classification performance for both supraventricular ectopic beats (SVEB or S beats) and ventricular ectopic beats (VEB or V beats) on the MIT-BIH arrhythmia database. It surpassed several state-of-art automatic classifiers and can perform on similar levels as some expert-assisted methods. In particular, the F1 score of SVEB has been improved by up to 10% over the top-performing automatic systems. Moreover, high sensitivity for both SVEB (87%) and VEB (93%) detection has been achieved, which is of great value for practical diagnosis. We, therefore, suggest our ACE-GAN (Generative Adversarial Network with Auxiliary Classifier for Electrocardiogram) based automatic system can be a promising and reliable tool for high throughput clinical screening practice, without any need of manual intervene or expert assisted labeling.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002505",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Deep learning",
      "Detector",
      "Discriminator",
      "Encoder",
      "Generative adversarial network",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Zhanhong"
      },
      {
        "surname": "Zhai",
        "given_name": "Xiaolong"
      },
      {
        "surname": "Tin",
        "given_name": "Chung"
      }
    ]
  },
  {
    "title": "Attention-aware metapath-based network embedding for HIN based recommendation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114601",
    "abstract": "Heterogeneous information network (HIN) attracts increasing attention from the communities of recommender systems. HIN based recommendation methods can help overcome the difficulties of data sparsity and cold start. The majority of the existing HIN based recommendation methods use path-based semantic similarity between users and/or between items on HINs. However, the existing HIN based recommendation methods using metapath disregard the semantic differences among multiple metapaths (i.e., inter-metapaths) and the influence differences among neighbor pairs in each individual metapath (i.e., intra-metapaths). To solve these problems, we propose an attention-aware metapath-based network embedding for HIN based recommendation. To obtain additional semantic information, our method generates multiple metapath-based weighted homogeneous networks to model the auxiliary information of users and items of HIN. Thereafter, we design a novel self-attention integration to integrate multiple semantic information from multiple weighted homogenous information networks. Lastly, we utilize three deep neural network methods to model the implicit relations between users and items for the rating prediction task. Experimental results of three real-world datasets demonstrate that the proposed model outperforms existing state-of-the-art recommendation methods, solves the data sparsity problem, and models the multiple semantic information of users and items.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000427",
    "keywords": [
      "Aerospace engineering",
      "Artificial intelligence",
      "Attention network",
      "Cold start (automotive)",
      "Computer science",
      "Data mining",
      "Economics",
      "Embedding",
      "Engineering",
      "Homogeneous",
      "Image (mathematics)",
      "Information retrieval",
      "Machine learning",
      "Management",
      "Physics",
      "Recommender system",
      "Semantic similarity",
      "Similarity (geometry)",
      "Task (project management)",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Surong"
      },
      {
        "surname": "Wang",
        "given_name": "Haosen"
      },
      {
        "surname": "Li",
        "given_name": "Yixiao"
      },
      {
        "surname": "Zheng",
        "given_name": "Yuan"
      },
      {
        "surname": "Han",
        "given_name": "Long"
      }
    ]
  },
  {
    "title": "RSigELU: A nonlinear activation function for deep neural networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114805",
    "abstract": "In deep learning models, the inputs to the network are processed using activation functions to generate the output corresponding to these inputs. Deep learning models are of particular importance in analyzing big data with numerous parameters and forecasting and are useful for image processing, natural language processing, object recognition, and financial forecasting. Sigmoid and tangent activation functions, which are traditional activation functions, are widely used in deep learning models. However, the sigmoid and tangent activation functions face the vanishing gradient problem. In order to overcome this problem, the ReLU activation function and its derivatives were proposed in the literature. However, there is a negative region problem in these activation functions. In this study, novel RSigELU activation functions, such as single-parameter RSigELU (RSigELUS) and double-parameter (RSigELUD), which are a combination of ReLU, sigmoid, and ELU activation functions, were proposed. The proposed RSigELUS and RSigELUD activation functions can overcome the vanishing gradient and negative region problems and can be effective in the positive, negative, and linear activation regions. Performance evaluation of the proposed RSigELU activation functions was performed on the MNIST, Fashion MNIST, CIFAR-10, and IMDb Movie benchmark datasets. Experimental evaluations showed that the proposed activation functions perform better than other activation functions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002463",
    "keywords": [
      "Activation function",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Deep learning",
      "Evolutionary biology",
      "Function (biology)",
      "Geodesy",
      "Geography",
      "Hyperbolic function",
      "MNIST database",
      "Mathematical analysis",
      "Mathematics",
      "Nonlinear system",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Sigmoid function"
    ],
    "authors": [
      {
        "surname": "Kiliçarslan",
        "given_name": "Serhat"
      },
      {
        "surname": "Celik",
        "given_name": "Mete"
      }
    ]
  },
  {
    "title": "A color fusion model based on Markowitz portfolio optimization for optic disc segmentation in retinal images",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114697",
    "abstract": "Retinal disorders are a severe health threat for older adults because they may lead to vision loss and blindness. Diabetic patients are particularly prone to suffer from Diabetic Retinopathy. Identifying relevant structural components in color fundus images like the optic disc (OD) is crucial to diagnose retinal diseases. Automatic OD detection is complex because of its location in an area where blood vessels converge, and color distribution is uneven. Several image processing techniques have been developed for OD detection so far, but vessel segmentation is sometimes required, increasing computational complexity and time. Moreover, precise OD segmentation methods utilize complex algorithms that need special hardware or extensive labeled datasets. We propose an OD detection approach based on the Modern Portfolio Theory of Markowitz to generate an innovative color fusion model. Specifically, the training phase calculates the optimal weights for each color channel. A fusion of weighted color channels is then applied in the testing phase. This approach acts as a powerful and real-time preprocessing stage. We use four heterogeneous datasets to validate the presented methodology. Three out of four datasets are publicly available (i.e., DRIVE, Messidor, and HRF), and the last corresponds to an in–house dataset acquired from Hospital Universitari Sant Joan de Reus (Spain). Two different segmentation methods are presented and compared with state-of-the-art computer vision techniques to analyze the model performance. An outstanding accuracy and overlap above 0.9 and 80%, respectively, and a minimal execution time of 0.05 s are reached. Therefore, our model could be integrated into daily clinical practice to accelerate the diagnosis of Diabetic Retinopathy due to its simplicity, performance, and speed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100138X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Fundus (uterus)",
      "Medicine",
      "Ophthalmology",
      "Optic disc",
      "Pattern recognition (psychology)",
      "Preprocessor",
      "Retinal",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Escorcia-Gutierrez",
        "given_name": "José"
      },
      {
        "surname": "Torrents-Barrena",
        "given_name": "Jordina"
      },
      {
        "surname": "Gamarra",
        "given_name": "Margarita"
      },
      {
        "surname": "Romero-Aroca",
        "given_name": "Pedro"
      },
      {
        "surname": "Valls",
        "given_name": "Aida"
      },
      {
        "surname": "Puig",
        "given_name": "Domenec"
      }
    ]
  },
  {
    "title": "Overlapping coalition formation in game theory: A state-of-the-art review",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114752",
    "abstract": "A coalition as a group of agents aims to work jointly to earn much more gains as a result of their cooperation. Many existing studies assumed that members take advantage of joining one coalition at a time, albeit the importance of coalition formation problems. Therefore, more attention to overlapping coalitions needs to be paid to optimise resource management by forming in multiple overlapping coalitions simultaneously. Roughly speaking, the related literature includes two main streams; (i) theoretical foundations of coalition formation games and, (ii) the coalition structure generation problems. This paper first provides a review of coalition structure generation at large to develop a taxonomic framework and classify the existing literature, viz., macro analysis. The paper then reviews studies on overlapping coalitions thoroughly, viz., micro analysis. The micro analysis presents and discusses different models of overlapping coalition games and related solution concepts as well as surveying all problem-solving approaches for overlapping coalition structure generation. Finally, the outstanding challenges and opportunities for future research considerations are discussed and shared.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001937",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Cooperative game theory",
      "Economics",
      "Engineering",
      "Game theory",
      "Macro",
      "Management science",
      "Mathematical economics",
      "Mathematics",
      "Mechanical engineering",
      "Operations research",
      "Programming language",
      "State (computer science)",
      "Work (physics)"
    ],
    "authors": [
      {
        "surname": "Mahdiraji",
        "given_name": "Hannan Amoozad"
      },
      {
        "surname": "Razghandi",
        "given_name": "Elham"
      },
      {
        "surname": "Hatami-Marbini",
        "given_name": "Adel"
      }
    ]
  },
  {
    "title": "Signal adaptive cooperative control of two adjacent traffic intersections using a two-stage algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114746",
    "abstract": "It is obvious that the traffic light plans of two intersections interact with each other, and the key problem for the system adjacent intersections is how to calculate of the arrival rates of the connection lanes of each phase. To treat the interaction of the two intersections, in this paper, the transition probabilities of the connection lanes of intersections 1 and 2 are introduced, which are utilized to compute the arrival rates of the connection lanes. In addition, since the green times of connection lanes interact with each other, a corresponding model of the phases of the connection lanes is proposed, which is employed to search for the optimal phase of the connection lanes. Moreover, the traffic time of the vehicles on the connection lanes is considered in the corresponding model. hen, an optimization model of the traffic signal of the two adjacent intersections is constructed. To solve the model, a novel two-stage algorithm is proposed. The first stage matches the arrival rates and the phase of the connection lanes of the two adjacent intersections. The second stage optimizes the model. Four numerical experiments are utilized to test the model and algorithm, and three other algorithms are chosen for comparison with our algorithm. The results of the numerical experiments demonstrate that the model and the algorithm are effective and that the model and algorithm can find more reasonable plans for the plans of two adjacent intersections.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001871",
    "keywords": [
      "Algorithm",
      "Biology",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Connection (principal bundle)",
      "Geometry",
      "Key (lock)",
      "Mathematics",
      "Organic chemistry",
      "Paleontology",
      "Phase (matter)",
      "Programming language",
      "SIGNAL (programming language)",
      "Stage (stratigraphy)"
    ],
    "authors": [
      {
        "surname": "Zou",
        "given_name": "Yuanyang"
      },
      {
        "surname": "Liu",
        "given_name": "Renhuai"
      },
      {
        "surname": "Li",
        "given_name": "Ya"
      },
      {
        "surname": "Ma",
        "given_name": "Yingshuang"
      },
      {
        "surname": "Wang",
        "given_name": "Guoxin"
      }
    ]
  },
  {
    "title": "NEclatClosed: A vertical algorithm for mining frequent closed itemsets",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114738",
    "abstract": "Frequent closed itemsets provide a lossless and concise collection of all frequent itemsets to reduce the runtime and memory requirement of frequent itemsets mining tasks. This study presents an algorithm named NEclatClosed for fast mining of frequent closed itemsets. We introduce concepts and techniques based on the vertical database format and employ them in the mining process. The experimental results show that NEclatClosed outperforms the leading algorithms in terms of runtime and memory usage, especially runtime, in most cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001792",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Data compression",
      "Data mining",
      "Lossless compression",
      "Process (computing)",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Aryabarzan",
        "given_name": "Nader"
      },
      {
        "surname": "Minaei-Bidgoli",
        "given_name": "Behrouz"
      }
    ]
  },
  {
    "title": "Solving arithmetic word problems by scoring equations with recursive neural networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114704",
    "abstract": "Solving arithmetic word problems is a cornerstone task in assessing language understanding and reasoning capabilities in NLP systems. Recent works use automatic extraction and ranking of candidate solution equations providing the answer to arithmetic word problems. In this work, we explore novel approaches to score such candidate solution equations using tree-structured recursive neural network (Tree-RNN) configurations. The advantage of this Tree-RNN approach over using more established sequential representations, is that it can naturally capture the structure of the equations. Our proposed method consists of transforming the mathematical expression of the equation into an expression tree. Further, we encode this tree into a Tree-RNN by using different Tree-LSTM architectures. Experimental results show that our proposed method (i) improves overall performance with more than 3% accuracy points compared to previous state-of-the-art, and with over 15% points on a subset of problems that require more complex reasoning, and (ii) outperforms sequential LSTMs by 4% accuracy points on such more complex problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001457",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Geometry",
      "Mathematics",
      "Natural language processing",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Zaporojets",
        "given_name": "Klim"
      },
      {
        "surname": "Bekoulis",
        "given_name": "Giannis"
      },
      {
        "surname": "Deleu",
        "given_name": "Johannes"
      },
      {
        "surname": "Demeester",
        "given_name": "Thomas"
      },
      {
        "surname": "Develder",
        "given_name": "Chris"
      }
    ]
  },
  {
    "title": "A novel hybrid deep learning approach including combination of 1D power signals and 2D signal images for power quality disturbance classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114785",
    "abstract": "As a result of the widespread use of power electronic equipment and the increase in consumption, the importance of effective energy policies and the smart grid begins to increase. Nonlinear loads and other loads in electric power systems are considered as the main reason for power quality disturbance. Distortions in signal quality and shape due to power quality disturbance cause a decrease in total efficiency. The proposed hybrid convolutional neural network method consists of a 1D convolutional neural network structure and a 2D convolutional neural network structure. The features acquired by these two convolutional neural network architectures are classified using the fully connected layer, which is traditionally used as the classifier of convolutional neural network architectures. Power signals are processed using a 1D convolutional neural network in their original form. Then these signals are converted into images and processed using a 2D convolutional neural network. Then, feature vectors generated by 1D and 2D convolutional neural networks are combined. Finally, this combined vector is classified by a fully connected layer. The proposed method is well suited to the nature of signal processing. It is a novel approach that covers the steps of an expert examining a signal. The proposed framework is compared with other state-of-the-art power quality disturbance classification methods in the literature. While the proposed method's classification performance is relatively high compared to other methods, the computational complexity is almost the same.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002268",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Classifier (UML)",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Pattern recognition (psychology)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Sindi",
        "given_name": "Hatem"
      },
      {
        "surname": "Nour",
        "given_name": "Majid"
      },
      {
        "surname": "Rawa",
        "given_name": "Muhyaddin"
      },
      {
        "surname": "Öztürk",
        "given_name": "Şaban"
      },
      {
        "surname": "Polat",
        "given_name": "Kemal"
      }
    ]
  },
  {
    "title": "SqSelect: Automatic assessment of Failed Error Propagation in state-based systems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114748",
    "abstract": "Current software systems are inherently complex and this fact strongly complicates, and makes more expensive, to validate them. Therefore, it is a must to provide methodologies, supported by tools, that can direct validation activities so that they focus on specific aspects of the system (e.g. its critical parts, common errors produced by developers, components that are expensive to fix after deployment, etc). Among the different validation techniques, testing is the most widely used. In this paper we focus on one of the main problems when testing systems with many components: the likelihood of Failed Error Propagation (FEP). FEP appears when we have faulty components such that their wrong behaviour is not revealed when isolatedly testing them but that might produce an error when they are combined with other components. Given a component, it is not possible to automatically assess the likelihood of FEP. However, previous work has shown that there is a strong correlation between the likelihood of FEP and an Information Theory notion called Squeeziness . Recent work has shown that it is possible to compute different values of Squeeziness (essentially, Squeeziness depends on a positive real value) and some of them are more suitable to estimate FEP. In this paper we present our tool SqSelect. Our tool receives either a specific system or its more important characteristics (number of states, maximum and minimum number of outgoing transitions from a state, size of the input and output alphabets) and returns interesting data that can help the tester to estimate the presence of FEP. In particular, our tool provides the most promising value(s) of the parameter associated with Squeeziness so that the likelihood of FEP can be more accurately estimated. In order to compute these values, our tool relies on an artificial neural network that has been extensively trained (compressing the information from around 250 , 000 systems and around 1 , 500 , 000 executions).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001895",
    "keywords": [
      "Algorithm",
      "Component (thermodynamics)",
      "Computer science",
      "Data mining",
      "Engineering",
      "Focus (optics)",
      "Machine learning",
      "Mechanical engineering",
      "Optics",
      "Physics",
      "Programming language",
      "Propagation of uncertainty",
      "Reliability engineering",
      "Software",
      "Software deployment",
      "Software engineering",
      "Software system",
      "State (computer science)",
      "Thermodynamics",
      "Value (mathematics)",
      "Work (physics)"
    ],
    "authors": [
      {
        "surname": "Ibias",
        "given_name": "Alfredo"
      },
      {
        "surname": "Núñez",
        "given_name": "Manuel"
      }
    ]
  },
  {
    "title": "Optimized HRNet for image semantic segmentation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114532",
    "abstract": "With the rapid development of deep learning, image semantic segmentation has made great progress and become a hot topic in scene understanding of computer vision. In this paper, we propose an optimized high-resolution net (HRNet) for image semantic segmentation. Unlike traditional networks usually extract feature maps based on a high-to-low encoder, which may easily loss important shape and boundary details especially for the deeper layers with lower resolutions, our optimized HRNet can maintain high resolution features at all times using a relatively shallow and parallel network structure. To improve the ability of our model in better recognizing the objects with various scales and irregular shapes, we introduce a mixed dilated convolution (MDC) module, which can not only increase the diversity of the receptive fields, but also tackle the “gridding” problem commonly existing in the conventional dilated convolution. By minimizing fine detail lost based on a DUpsample strategy, we further develop a multi-level data-dependent feature aggregation (MDFA) module to enhance the capability of our network in better identifying the fine details especially for the small objects with fuzzy boundaries. We evaluate the optimized HRNet on four different datasets, including Cityscapes, Pascal VOC2012, CamVid and the KITTI. Experimental results validate the effectiveness of our method in improving the accuracy of image semantic segmentation. Comparisons with state-of-the-art methods also verify the advantages of our optimized HRNet in achieving better semantic segmentation performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311763",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Deep learning",
      "Encoder",
      "Feature (linguistics)",
      "Image segmentation",
      "Linguistics",
      "Operating system",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Huisi"
      },
      {
        "surname": "Liang",
        "given_name": "Chongxin"
      },
      {
        "surname": "Liu",
        "given_name": "Mengshu"
      },
      {
        "surname": "Wen",
        "given_name": "Zhenkun"
      }
    ]
  },
  {
    "title": "Candidate point selection using a self-attention mechanism for generating a smooth volatility surface under the SABR model",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114640",
    "abstract": "In real markets, generating a smooth implied volatility surface requires an interpolation of the calibrated parameters by using smooth parametric functions. For this interpolation, practitioners do not use all the discrete parameter points but manually select candidate parameter points through time-consuming adjustments (e.g., removing outliers, comparing with the surface from the previous day, and considering daily market indexes) to generate a smooth and robust surface. In this paper, we propose neural network models that assist practitioners in generating a smooth implied volatility surface under the SABR (Hagan et al., 2002) model. Utilizing the self-attention mechanism of a transformer network (Vaswani et al., 2017) as a backbone network, we design two models: one that orders the parameter points by their likelihood to be selected as candidate parameter points and one that determines the candidate point set among the combinations of high-priority points. Experimental results from a 3-year period of real market S&P500 and KOSPI200 data show that the combination of two models can assist practitioners in the point selection task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000816",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Data mining",
      "Econometrics",
      "Implied volatility",
      "Interpolation (computer graphics)",
      "Mathematics",
      "Motion (physics)",
      "Outlier",
      "Parametric statistics",
      "Parametric surface",
      "SABR volatility model",
      "Statistics",
      "Volatility (finance)"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Hyeonuk"
      },
      {
        "surname": "Park",
        "given_name": "Kyunghyun"
      },
      {
        "surname": "Jeon",
        "given_name": "Junkee"
      },
      {
        "surname": "Song",
        "given_name": "Changhoon"
      },
      {
        "surname": "Bae",
        "given_name": "Jungwoo"
      },
      {
        "surname": "Kim",
        "given_name": "Yongsik"
      },
      {
        "surname": "Kang",
        "given_name": "Myungjoo"
      }
    ]
  },
  {
    "title": "A dynamic cooperative lane-changing model for connected and autonomous vehicles with possible accelerations of a preceding vehicle",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114675",
    "abstract": "The emerging connected and autonomous vehicle (CAV) technologies offer a promising solution to design better lane-changing maneuvers that can reduce the negative impacts of vehicle lane-changing behavior on traffic operations. Existing studies on this topic have predominantly focused on designing lane-changing maneuvers for a subject vehicle (SV) and typically assumed that a vehicle in the target lane must decelerate to make space for the SV due to safety considerations. Nevertheless, jointly designing the trajectories of the SV and surrounding vehicles and allowing possible accelerations of a preceding vehicle may further alleviate the negative impacts of CAV’s lane-changing maneuvers. To investigate this possibility, this paper proposes a dynamic cooperative lane-changing model for CAVs with possible accelerations of a preceding vehicle. This model collects information of the surrounding vehicles and updates the lane-changing decisions for the SV in real time via three steps, namely lane-changing decision making, cooperative trajectory planning, and trajectory tracking. This model applies a linearized vehicle kinematic model to make lane-changing decisions for the SV given the states of the SV and surrounding vehicles, the minimum safety distance, and requirements on the comfort level for passengers. Furthermore, it dynamically designs the longitudinal and lateral trajectories for the SV and surrounding vehicles. Extensive numerical simulation experiments are conducted to evaluate the effectiveness of the proposed model. Results show that the proposed model increases the success rate of the SV’s lane-changing maneuvers, smoothens the trajectories of the SV and vehicles in the upstream direction at the cost of a slightly more significant oscillation of the last vehicle in the downstream direction. Overall, the proposed model reduces the negative impacts of lane-changing maneuvers on the surrounding traffic. The results also reveal the robustness of the model performance by varying several key input parameters in the experiments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001160",
    "keywords": [
      "Acceleration",
      "Astronomy",
      "Automotive engineering",
      "Classical mechanics",
      "Computer science",
      "Engineering",
      "Kinematics",
      "Physics",
      "Simulation",
      "Trajectory",
      "Vehicle dynamics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zhen"
      },
      {
        "surname": "Zhao",
        "given_name": "Xiangmo"
      },
      {
        "surname": "Chen",
        "given_name": "Zhiwei"
      },
      {
        "surname": "Li",
        "given_name": "Xiaopeng"
      }
    ]
  },
  {
    "title": "Classification of severity of trachea stenosis from EEG signals using ordinal decision-tree based algorithms and ensemble-based ordinal and non-ordinal algorithms",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114707",
    "abstract": "Machine learning is integrated nowadays in many data-driven applications that attempt to model the behavior of a system. Thus, the implementation of machine-learning algorithms for medical applications is growing, enabling doctors to make decisions based on the output of the model of the system’s behavior. The upper airway is involved in a variety of disorders that lead to non-specific symptoms; thus, upper-airway obstruction is frequently unrecognized or misdiagnosed. Bronchoscopy, which is a minimally invasive procedure, and lung function (spirometry) tests, which are relatively demanding for the patient, are currently the most common methods for diagnosing respiratory diseases. In this study, a novel, non-invasive procedure is proposed in which tracheal obstruction is identified based on brain signals. Specifically, the spectral information in electroencephalogram (EEG) signals is used as an input to an ensemble learner approach based on ordinal and non-ordinal classification algorithms, where the classification problem involves identifying the degree of airway obstruction. An experiment was conducted in which four healthy subjects breathed through three-dimensional (3D) geometric models of the trachea that mimicked different obstruction rates. Multi-subject classification was carried out in which the classification model of each subject was produced by training the model on the other subjects' datasets. The main findings were as follows. Firstly, the in-house ordinal classification algorithms, which included a C4.5 and a random-forest algorithm, both based on a weighted information-gain ratio measure, yielded better classification results than their non-ordinal counterparts and other conventional classifiers. Additionally, the study showed that when integrating the two types of algorithms (ordinal and non-ordinal) into an ensemble approach, the performance was improved relative to each individual classifier. Finally, the classification accuracy is such that the proposed method of using EEG signals for the identification of the degree of tracheal obstruction by means of an ensemble approach shows promise as a supplemental clinical test.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001482",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Decision tree",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Random forest",
      "Statistical classification"
    ],
    "authors": [
      {
        "surname": "Singer",
        "given_name": "Gonen"
      },
      {
        "surname": "Ratnovsky",
        "given_name": "Anat"
      },
      {
        "surname": "Naftali",
        "given_name": "Sara"
      }
    ]
  },
  {
    "title": "Propagation path optimization of product attribute design changes based on Petri Net fusion ant colony algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114664",
    "abstract": "An effective and controlled design change in product attributes can improve product quality, promote product innovation, and meet customer demands. During design changes, the links among product attributes are intricate. A change in the design of one attribute may lead to a series of changes in other attributes, which result in the change propagation as well as the re-planning and re-design of the propagation path. Therefore, an Expanded Petri Net (EPN) model is constructed to describe the propagation relationship of product attribute design change in this paper. Firstly, the attribute change propagation rules are established based on change propagation impact values. Secondly, to optimize the dynamic scheme of change propagation path, the ant colony path optimization algorithm is integrated into the Extended Petri Net model (Expanded Petri Ant Colony Optimization Algorithm, EP-ACOA). In addition, the constraint test factor and the upper and lower limits of pheromone values are introduced into the algorithm to ensure the whole network traversal of the change propagation path and accelerate the solving speed of the model. Finally, taking the attribute design change of Langevin piezoelectric transducer in an ultrasonic knife as an example, the Petri Net fusion ant colony algorithm was verified to be able to quickly and effectively provide the optimal solution or approximate optimal solution to the propagation path problem of product attribute design changes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001056",
    "keywords": [
      "Algorithm",
      "Ant colony",
      "Ant colony optimization algorithms",
      "Computer science",
      "Geometry",
      "Mathematical optimization",
      "Mathematics",
      "Path (computing)",
      "Petri net",
      "Product (mathematics)",
      "Programming language",
      "Tree traversal"
    ],
    "authors": [
      {
        "surname": "Gan",
        "given_name": "Yi"
      },
      {
        "surname": "He",
        "given_name": "Yiqi"
      },
      {
        "surname": "Gao",
        "given_name": "Li"
      },
      {
        "surname": "He",
        "given_name": "Weiming"
      }
    ]
  },
  {
    "title": "AGWO: Advanced GWO in multi-layer perception optimization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114676",
    "abstract": "The Multi-Layer Perceptron (MLP) has been applied into many real-world problems as one of the most extensively used Neural Networks (NNs). It often suffers from local stagnation and premature convergence problems when treating the specific datasets. This paper proposes an Advanced Grey Wolf Optimization algorithm (AGWO) with elastic, circling and attacking mechanisms to alleviate those problems. The performance of the proposed AGWO is tested on the IEEE CEC 2014. Meanwhile, the efficiency of AGWO and five noted heuristic algorithms in training MLP are investigated by 7 classification datasets and 3 function approximation datasets. The experimental results show that AGWO is superior to other algorithms in terms of local stagnation and premature convergence. Simultaneously, training MLP by means of AGWO is clearly superior to the other heuristic algorithms in local optimum avoidance and computational accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001172",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Heuristic",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Particle swarm optimization",
      "Perceptron",
      "Premature convergence"
    ],
    "authors": [
      {
        "surname": "Meng",
        "given_name": "Xianqiu"
      },
      {
        "surname": "Jiang",
        "given_name": "Jianhua"
      },
      {
        "surname": "Wang",
        "given_name": "Huan"
      }
    ]
  },
  {
    "title": "Multivariate time-series clustering based on component relationship networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114649",
    "abstract": "Clustering is a powerful technique for providing class labels of data objects for learning guidance. Because traditional clustering methods rarely consider the component correlations of a multivariate time series(MTS), an MTS clustering method based on a component relationship network (CRN) is proposed in the present study. An MTS dataset is mapped to a multi-relationship network (MRN), which consists of a set of CRNs. Every CRN reflects the relationship of the MTS data under each component. The proposed method applies two steps. First, the distance function and K-nearest neighbors are combined to transform an MTS dataset into an MRN. Second, a non-negative matrix factorization is designed to identify time-series clusters. Because asynchronous time-series data exist in the form of an approximate case, we recommend an improved penalty-coefficient based dynamic time-warping algorithm to measure the similarity between two-time sequences. The similarity can reflect the correlation between the asynchronous MTS data. Through an experiment, we compared the proposed method to other clustering methods and discussed of the effects of the parameters in detail. The numerical results of the experiment demonstrate that the proposed method can improve the accuracy and quality of MTS data clustering.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000907",
    "keywords": [
      "Artificial intelligence",
      "Asynchronous communication",
      "Biology",
      "Cluster analysis",
      "Component (thermodynamics)",
      "Computer network",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Dynamic time warping",
      "Image (mathematics)",
      "Machine learning",
      "Measure (data warehouse)",
      "Multivariate statistics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Physics",
      "Series (stratigraphy)",
      "Similarity (geometry)",
      "Similarity measure",
      "Thermodynamics",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Hailin"
      },
      {
        "surname": "Du",
        "given_name": "Tian"
      }
    ]
  },
  {
    "title": "Driver stress detection via multimodal fusion using attention-based CNN-LSTM",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114693",
    "abstract": "Stress has been identified as one of major contributing factors in car crashes due to its negative impact on driving performance. It is in urgent need that the stress levels of drivers can be detected in real time with high accuracy so that intervening or navigating measures can be taken in time to mitigate the situation. Existing driver stress detection models mainly rely on traditional machine learning techniques to fuse multimodal data. However, due to the non-linear correlations among modalities, it is still challenging for traditional multimodal fusion methods to handle the real-time influx of complex multimodal and high dimensional data, and report drivers’ stress levels accurately. To solve this issue, a framework of driver stress detection through multimodal fusion using attention based deep learning techniques is proposed in this paper. Specifically, an attention based convolutional neural networks (CNN) and long short-term memory (LSTM) model is proposed to fuse non-invasive data, including eye data, vehicle data, and environmental data. Then, the proposed model can automatically extract features separately from each modality and give different levels of attention to features from different modalities through self-attention mechanism. To verify the validity of the proposed method, extensive experiments have been carried out on our dataset collected using an advanced driving simulator. Experimental results demonstrate that the performance of the proposed method on driver stress detection outperforms the state-of-the-art models with an average accuracy of 95.5%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001342",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Electrical engineering",
      "Engineering",
      "Fuse (electrical)",
      "Linguistics",
      "Machine learning",
      "Modalities",
      "Modality (human–computer interaction)",
      "Multimodal learning",
      "Philosophy",
      "Sensor fusion",
      "Social science",
      "Sociology",
      "Stress (linguistics)"
    ],
    "authors": [
      {
        "surname": "Mou",
        "given_name": "Luntian"
      },
      {
        "surname": "Zhou",
        "given_name": "Chao"
      },
      {
        "surname": "Zhao",
        "given_name": "Pengfei"
      },
      {
        "surname": "Nakisa",
        "given_name": "Bahareh"
      },
      {
        "surname": "Rastgoo",
        "given_name": "Mohammad Naim"
      },
      {
        "surname": "Jain",
        "given_name": "Ramesh"
      },
      {
        "surname": "Gao",
        "given_name": "Wen"
      }
    ]
  },
  {
    "title": "Energy-cost-aware resource-constrained project scheduling for complex product system with activity splitting and recombining",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114754",
    "abstract": "Energy-cost-aware scheduling in manufacturing process has recently been considered as an effective way to use energy. In addition, applying time-of-use is regarded as one of the optimal uses of energy which the governments extend to control energy consumption. Complex product system (CoPS) has more complex and huge amounts of multiple manufacturing activities with higher energy sensibility in project scheduling than commodity products. This paper presents a new variant of resource-constrained project scheduling problem (RCPSP), which is named energy-cost-aware resource-constrained project scheduling problem with activity splitting and recombining (eRCPSP-AS&R) for CoPS. In the proposed model, a dynamic activity splitting and recombining strategy for CoPS is used by considering energy consumption. Also, a bi-objective (project delay and energy consumption cost) mathematical model is established. The solution framework of eRCPSP-AS&R model consists of two parts: activity splitting and recombining, project scheduling. The dynamic strategy of activity splitting and recombining is used to perform energy-cost-aware based on hybrid intuitionistic fuzzy information entropy, TOPSIS and fuzzy clustering method. Moreover, an improved NSGA-II algorithm is employed to solve the Pareto-optimal solutions for the proposed bi-objective optimization problem. Series of computational experiments are conducted and verified the overall performance of proposed eRCPSP-AS&R model and approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001950",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Electrical engineering",
      "Energy consumption",
      "Engineering",
      "Fuzzy logic",
      "Job shop scheduling",
      "Mathematical optimization",
      "Mathematics",
      "Multi-objective optimization",
      "Operating system",
      "Schedule",
      "Scheduling (production processes)"
    ],
    "authors": [
      {
        "surname": "Du",
        "given_name": "Baigang"
      },
      {
        "surname": "Tan",
        "given_name": "Tian"
      },
      {
        "surname": "Guo",
        "given_name": "Jun"
      },
      {
        "surname": "Li",
        "given_name": "Yibing"
      },
      {
        "surname": "Guo",
        "given_name": "Shunsheng"
      }
    ]
  },
  {
    "title": "Modelling of supply chain disruption analytics using an integrated approach: An emerging economy example",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114690",
    "abstract": "The purpose of this paper is to develop a framework to identify, analyze, and to assess supply chain disruption factors and drivers. Based on an empirical analysis, four disruption factor categories including natural, human-made, system accidents, and financials with a total of sixteen disruption drivers are identified and examined in a real-world industrial setting. This research utilizes an integrated approach comprising both the Delphi method and the fuzzy analytic hierarchy process (FAHP). To test this integrated method, one of the well-known examples in industrial contexts of developing countries, the ready-made garment industry in Bangladesh is considered. To evaluate this industrial example, a sensitivity analysis is conducted to ensure the robustness and viability of the framework in practical settings. This study not only expands the literature scope of supply chain disruption risk assessment but through its application in any context or industry will reduce the impact of such disruptions and enhance the overall supply chain resilience. Consequently, these enhanced capabilities arm managers the ability to formulate relevant mitigation strategies that are robust and computationally efficient. These strategies will allow managers to take calculated decisions proactively. Finally, the results reveal that political and regulatory instability, cyclones, labor strikes, flooding, heavy rain, and factory fires are the top six disruption drivers causing disruptions to the ready-made garment industry in Bangladesh.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001317",
    "keywords": [
      "Analytic hierarchy process",
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Business",
      "Chemistry",
      "Computer science",
      "Context (archaeology)",
      "Delphi method",
      "Engineering",
      "Gene",
      "Marketing",
      "Operations research",
      "Paleontology",
      "Programming language",
      "Risk analysis (engineering)",
      "Robustness (evolution)",
      "Scope (computer science)",
      "Supply chain"
    ],
    "authors": [
      {
        "surname": "Mithun Ali",
        "given_name": "Syed"
      },
      {
        "surname": "Kumar Paul",
        "given_name": "Sanjoy"
      },
      {
        "surname": "Chowdhury",
        "given_name": "Priyabrata"
      },
      {
        "surname": "Agarwal",
        "given_name": "Renu"
      },
      {
        "surname": "Fathollahi-Fard",
        "given_name": "Amir Mohammad"
      },
      {
        "surname": "Jose Chiappetta Jabbour",
        "given_name": "Charbel"
      },
      {
        "surname": "Luthra",
        "given_name": "Sunil"
      }
    ]
  },
  {
    "title": "A LSTM based deep learning network for recognizing emotions using wireless brainwave driven system",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114516",
    "abstract": "Positive and Negative emotions are experienced by the majority of individuals in their day-to-day life. It is important to control access of negative emotions because it may lead to several chronic health issues like depression and anxiety. The purpose of this research work is to develop a portable brainwave driven system for recognizing positive, negative, and neutral emotions. This research considers the classification of four negative class of emotions using genres sadness, disgust, angry, and surprise along with the classification of three basic class of emotions i.e., positive, negative, and neutral. This paper introduces a long short term memory deep learning (LSTM) network to recognize emotions using EEG signals. The primary goal of this approach is to assess the classification performance of the LSTM model. The secondary goal is to assess the human behavior of different age groups and gender. We have compared the performance of Multilayer Perceptron (MLP), K-nearest neighbors (KNN), Support Vector Machine (SVM), LIB-Support Vector Machine (LIB-SVM), and LSTM based deep learning model for classification. The analysis shows that, for four class of emotions LSTM based deep learning model provides classification accuracy as 83.12%, 86.94%, 91.67%, and 94.12% for 50–50, 60–40, 70–30, and 10-fold cross-validations. For three class of emotions LSTM based deep learning model provides classification accuracy as 81.33%, 85.41%, 89.44%, and 92.66% for 50–50, 60–40, 70–30, and 10-fold cross-validation. The generalizability and reliability of this approach are evaluated by applying our approach to publicly available EEG datasets DEAP and SEED. In compliance with the self-reported feelings, brain signals of 18–25 years of age group provided the highest emotional identification. The results show that among genders, females are more emotionally active as compared to males. These results affirmed the potential use of our method for recognizing positive, negative, and neutral emotions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742031160X",
    "keywords": [
      "Anger",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Deep learning",
      "Developmental psychology",
      "Disgust",
      "Emotion classification",
      "Generalizability theory",
      "Machine learning",
      "Perceptron",
      "Psychiatry",
      "Psychology",
      "Sadness",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Sakalle",
        "given_name": "Aditi"
      },
      {
        "surname": "Tomar",
        "given_name": "Pradeep"
      },
      {
        "surname": "Bhardwaj",
        "given_name": "Harshit"
      },
      {
        "surname": "Acharya",
        "given_name": "Divya"
      },
      {
        "surname": "Bhardwaj",
        "given_name": "Arpit"
      }
    ]
  },
  {
    "title": "Reinforcement learning approach for resource allocation in humanitarian logistics",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114663",
    "abstract": "When a disaster strikes, it is important to allocate limited disaster relief resources to those in need. This paper considers the allocation of resources in humanitarian logistics using three critical performance indicators: efficiency, effectiveness and equity. Three separate costs are considered to represent these metrics, namely, the accessibility-based delivery cost, the starting state-based deprivation cost, and the terminal penalty cost. A mixed-integer nonlinear programming model with multiple objectives and multiple periods is proposed. A Q-learning algorithm, a type of reinforcement learning method, is developed to address the complex optimization problem. The principles of the proposed algorithm, including the learning agent and its actions, the environment and its states, and reward functions, are presented in detail. The parameter settings of the proposed algorithm are also discussed in the experimental section. In addition, the solution quality of the proposed algorithm is compared with that of the exact dynamic programming method and a heuristic algorithm. The experimental results show that the efficiency of the algorithm is better than that of the dynamic programming method and the accuracy of the algorithm is higher than that of the heuristic algorithm. Moreover, the Q-learning algorithm provides close to or even optimal solutions to the resource allocation problem by adjusting the value of the training episode K in practical applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001044",
    "keywords": [
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Heuristic",
      "Mathematical optimization",
      "Mathematics",
      "Q-learning",
      "Reinforcement learning",
      "Resource allocation"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Lina"
      },
      {
        "surname": "Zhang",
        "given_name": "Canrong"
      },
      {
        "surname": "Jiang",
        "given_name": "Jingyan"
      },
      {
        "surname": "Yang",
        "given_name": "Huasheng"
      },
      {
        "surname": "Shang",
        "given_name": "Huayan"
      }
    ]
  },
  {
    "title": "Metaheuristics-based ontology meta-matching approaches",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114578",
    "abstract": "Ontologies have emerged to establish a well-defined meaning for information, solving problems of heterogeneity in data semantics and facilitating the process of information exchange. However, ontologies have generated a new semantic problem, since using more than one ontology can generate ambiguity in the meaning of a given data. The problem of ontology matching is to search for relationships between entities of distinct ontologies, solving the problem of semantic heterogeneity of the data. The problem is a relevant issue in the area of knowledge representation and several approaches have been proposed to solve it. This work presents a systematic mapping of the main works published in the area with an emphasis on metaheuristics-based meta-matching approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000191",
    "keywords": [
      "Ambiguity",
      "Artificial intelligence",
      "Computer science",
      "Epistemology",
      "Information retrieval",
      "Law",
      "Matching (statistics)",
      "Mathematics",
      "Metaheuristic",
      "Ontology",
      "Ontology alignment",
      "Ontology-based data integration",
      "Philosophy",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Semantic Web",
      "Semantic heterogeneity",
      "Semantics (computer science)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Ferranti",
        "given_name": "Nicolas"
      },
      {
        "surname": "Rosário Furtado Soares",
        "given_name": "Stênio Sã"
      },
      {
        "surname": "de Souza",
        "given_name": "Jairo Francisco"
      }
    ]
  },
  {
    "title": "Multi-criteria approach to stochastic and fuzzy uncertainty in the selection of electric vehicles with high social acceptance",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114686",
    "abstract": "In recent years, a number of legal solutions have been adopted in Poland aimed at reaching the number of 1 million electric cars in use in 2025. Therefore, an important research issue is the assessment of electric vehicles available on the Polish market and the identification of vehicles that meet consumers' expectations to the greatest extent. The assessment of electric vehicles is a multi-criteria issue, characterized by a number of uncertainties related to their operational parameters, as well as the preferences of individual users and, more broadly, the preferences of society. A novelty of the article is the application of the fuzzy multi-criteria decision aid method called NEAT F-PROMETHEE (New Easy Approach To Fuzzy PROMETHEE), combined with the Monte Carlo method and elements of the SMAA (Stochastic Multicriteria Acceptability Analysis) method for the assessment of vehicles under uncertainty conditions. The theoretical contribution of this research therefore includes the synthesis of a fuzzy and stochastic approach to decision-making support, supported by outranking and incomparability relationships. Such a set of approaches to uncertainty makes it possible to improve the accuracy of decision-making, because the approaches indicated verify each other's results. Moreover, the application of the NEAT F-PROMETHEE method has solved the problem of evaluating electric vehicles from the perspective of a single decision-maker, while the combination of NEAT F-PROMETHEE and the stochastic approach has made it possible to simulate preferences of the society. Although there are many uncertainties in the decision-making problem, the approach has allowed to identify almost unambiguously the electric vehicle that is likely to gain the highest acceptance. As a result of the conducted research it was found that the approaches to uncertainty based on fuzzy sets, outranking relations and stochastic analysis complement each other, allowing the decision-maker to conduct a wider analysis of the imprecision of the obtained solution.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001275",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Decision maker",
      "Economics",
      "Fuzzy logic",
      "Fuzzy set",
      "Identification (biology)",
      "Management science",
      "Mathematical optimization",
      "Mathematics",
      "Novelty",
      "Operations research",
      "Philosophy",
      "Programming language",
      "Selection (genetic algorithm)",
      "Set (abstract data type)",
      "Theology"
    ],
    "authors": [
      {
        "surname": "Ziemba",
        "given_name": "Paweł"
      }
    ]
  },
  {
    "title": "Machine learning classification and regression models for predicting directional changes trend reversal in FX markets",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114645",
    "abstract": "Most forecasting algorithms in financial markets use physical time for studying price movements, making the flow of time discontinuous. The use of physical time scale can make traders oblivious to significant activities in the market, which poses a risk. Directional changes (DC) is an alternative approach that uses event-based time to sample data. In this work, we propose a novel DC-based framework, which uses machine learning algorithms to predict when a trend will reverse. This allows traders to be in a position to take an action before this happens and thus increase their profitability. We combine our approach with a novel DC-based trading strategy and perform an in-depth investigation, by applying it to 10-min data from 20 foreign exchange markets over a 10-month period. The total number of tested datasets is 1,000, which allows us to argue that our results can be generalised and are widely applicable. We compare our results to ten benchmarks (both DC and non-DC based, such as technical analysis and buy-and-hold). Our findings show that our proposed approach is able to return a significantly higher profit, as well as reduced risk, and statistically outperform the other trading strategies in a number of different performance metrics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000865",
    "keywords": [
      "Algorithmic trading",
      "Artificial intelligence",
      "Computer science",
      "Econometrics",
      "Economics",
      "Finance",
      "Financial market",
      "Foreign exchange",
      "Foreign exchange market",
      "Machine learning",
      "Mathematics",
      "Microeconomics",
      "Monetary economics",
      "Position (finance)",
      "Profit (economics)",
      "Profitability index",
      "Regression",
      "Statistics",
      "Trading strategy"
    ],
    "authors": [
      {
        "surname": "Adegboye",
        "given_name": "Adesola"
      },
      {
        "surname": "Kampouridis",
        "given_name": "Michael"
      }
    ]
  },
  {
    "title": "Overlapping community detection by constrained personalized PageRank",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114682",
    "abstract": "Given a network, local community detection (a.k.a. graph clustering) methods aim at finding communities around the selected initial nodes (also referred to as seeds, starting nodes or core nodes). Methods in this kind successfully address the efficiency problem confronted by global clustering methods. And techniques, such as personalized PageRank and heat kernel diffusion, for ranking the proximity score of vertices nearby with respect to the corresponding starting nodes are developed. However, most of the random-walk based metrics allow a walker to diffuse without any constraint, and the walker can easily run into irrelevant communities. As a result, the corresponding community could include irrelevant high-quality communities (communities with good fitness score) nearby, we refer to the case that a walker goes into irrelevant communities and causes inaccurate expansion of a community as redundant diffusion. In this work, we develop a constrained personalized PageRank method for community expansion to reduce the problem of redundant diffusion. In the mechanism, a walker moves with lower probability to neighbor nodes already in the existing communities, and a walker tends to walk out of the community if the walker walks into an irrelevant community. Extensive experiments on synthetic and large real-world networks demonstrate that the proposed method outperforms approaches in the state of the art by a large margin in accuracy and efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001238",
    "keywords": [
      "Clique percolation method",
      "Cluster analysis",
      "Computer science",
      "Constraint (computer-aided design)",
      "Data mining",
      "Epistemology",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "PageRank",
      "Philosophy",
      "Quality (philosophy)",
      "Random walk",
      "Ranking (information retrieval)",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Yang"
      },
      {
        "surname": "Yu",
        "given_name": "Xiangzhan"
      },
      {
        "surname": "Zhang",
        "given_name": "Hongli"
      }
    ]
  },
  {
    "title": "Artificial intelligence applications in supply chain: A descriptive bibliometric analysis and future research directions",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114702",
    "abstract": "Today’s supply chains are very different from those of just a few years ago, and they continue to evolve within an extremely competitive economy. Dynamic supply chain processes require a technology that can cope with their increasing complexity. In recent years, several functional supply chain applications based on artificial intelligence (AI) have emerged, yet very few studies have addressed the applications of AI in supply chain processes. Machine learning, natural language processing, and robotics are all potential enablers of supply chain transformation. Aware of the potential advantages of AI implementation in supply chains and of the paucity of work done regarding it, we explore what researchers have done so far with respect to AI and what needs further exploration. We reviewed 136 research papers published between 1996 and 2020 from the Scopus database and provided a classification of the research material according to four critical structural dimensions (level of analytics, AI algorithms or techniques, sector or industry of application, and supply chain processes). This study is the first attempt to study the AI applications in SC from a process perspective and provides a decisional framework for adequate use of AI techniques in the different SC processes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001433",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Data science",
      "Knowledge management",
      "Law",
      "MEDLINE",
      "Marketing",
      "Operating system",
      "Political science",
      "Process (computing)",
      "Scopus",
      "Supply chain",
      "Supply chain management"
    ],
    "authors": [
      {
        "surname": "Riahi",
        "given_name": "Youssra"
      },
      {
        "surname": "Saikouk",
        "given_name": "Tarik"
      },
      {
        "surname": "Gunasekaran",
        "given_name": "Angappa"
      },
      {
        "surname": "Badraoui",
        "given_name": "Ismail"
      }
    ]
  },
  {
    "title": "Customer purchase prediction from the perspective of imbalanced data: A machine learning framework based on factorization machine",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114756",
    "abstract": "Customer purchase prediction aims to predict customers' future purchases, and the prediction results are of great importance for conducting future commercial activities. To obtain accurate predictions of customer purchases, this paper develops a machine learning framework based on historical behavioural data. First, considering the sparsity of behavioural data, this paper proposes a feature combination method based on the improved factorization machine algorithm. Second, due to the imbalance of customer purchase data, this paper proposes an imbalanced prediction method based on the maximized marginal category and cost-sensitive ensemble learning. Finally, a real-word travel service purchase dataset is adopted to test the feasibility of the proposed prediction framework. The experimental results and comparative analysis verify the validity of the proposed model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001974",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Ensemble learning",
      "Feature (linguistics)",
      "Linguistics",
      "Machine learning",
      "Perspective (graphical)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Shui-xia"
      },
      {
        "surname": "Wang",
        "given_name": "Xiao-kang"
      },
      {
        "surname": "Zhang",
        "given_name": "Hong-yu"
      },
      {
        "surname": "Wang",
        "given_name": "Jian-qiang"
      }
    ]
  },
  {
    "title": "MVDF-RSC: Multi-view data fusion via robust spectral clustering for geo-tagged image tagging",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114657",
    "abstract": "Image tag recommendation, aiming at assigning a set of relevant tags for images, is a useful way to help users organize images’ content. Early methods in image tagging mainly demonstrated using low-level visual features. However, two visually similar photos may have different concepts (semantic gap). Although different multi-view tagging methods are proposed to learn the discriminative features, they usually do not consider the geographical correlation among images. Moreover, geographical-based image tagging models generally focused on the relevance criterion, i.e., how well the suggested tags describe image content. Diversity and redundancy should be controlled to guarantee the recommendation models’ effectiveness and promote complementary information among tags. This paper proposes a robust multi-view image tagging method, termed MVDF-RSC, which considers the relevance, diversity, and redundancy criteria. Precisely, the proposed method consists of two phases: training and prediction. We propose a new robust optimization problem in the training phase to determine the similarity between data via the early fusion of multiple views of images and obtain clusters. In the prediction phase, relevant tags are recommended to each test data using a search-based method and a late fusion strategy. Comprehensive experiments on two geo-tagged image datasets demonstrate the proposed method’s effectiveness over state-of-the-art alternatives.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000981",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Discriminative model",
      "Image (mathematics)",
      "Image retrieval",
      "Information retrieval",
      "Law",
      "Operating system",
      "Pattern recognition (psychology)",
      "Political science",
      "Redundancy (engineering)",
      "Relevance (law)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Zamiri",
        "given_name": "Mona"
      },
      {
        "surname": "Bahraini",
        "given_name": "Tahereh"
      },
      {
        "surname": "Yazdi",
        "given_name": "Hadi Sadoghi"
      }
    ]
  },
  {
    "title": "An application of deep reinforcement learning to algorithmic trading",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114632",
    "abstract": "This scientific research paper presents an innovative approach based on deep reinforcement learning (DRL) to solve the algorithmic trading problem of determining the optimal trading position at any point in time during a trading activity in the stock market. It proposes a novel DRL trading policy so as to maximise the resulting Sharpe ratio performance indicator on a broad range of stock markets. Denominated the Trading Deep Q-Network algorithm (TDQN), this new DRL approach is inspired from the popular DQN algorithm and significantly adapted to the specific algorithmic trading problem at hand. The training of the resulting reinforcement learning (RL) agent is entirely based on the generation of artificial trajectories from a limited set of stock market historical data. In order to objectively assess the performance of trading strategies, the research paper also proposes a novel, more rigorous performance assessment methodology. Following this new performance assessment approach, promising results are reported for the TDQN algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000737",
    "keywords": [
      "Algorithmic trading",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Deep learning",
      "Economics",
      "Engineering",
      "Financial economics",
      "Horse",
      "Machine learning",
      "Mechanical engineering",
      "Paleontology",
      "Portfolio",
      "Programming language",
      "Reinforcement learning",
      "Set (abstract data type)",
      "Sharpe ratio",
      "Stock (firearms)",
      "Stock market",
      "Trading strategy"
    ],
    "authors": [
      {
        "surname": "Théate",
        "given_name": "Thibaut"
      },
      {
        "surname": "Ernst",
        "given_name": "Damien"
      }
    ]
  },
  {
    "title": "User project planning in social and medico-social sector: Models and solution methods",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114684",
    "abstract": "Social and medico-social centers are the main structures in France where different categories of vulnerable populations are hosted. In addition to the daily care, these centers have to ensure the implementation of the personalized project for each resident in response to his/her needs in comply with national legal provisions. This work deals with the main issue of elaborating feasible and thoughtful personalized projects, which aims to improve the whole efficiency of project implementation in social and medico-social centers. A personalized project is composed of a set of activities chosen among available activities proposed by a center. The creation of the personalized projects for the residents requires the satisfaction of a number of imperative constraints while optimizing some objectives concerning the residents and the center. We present a general formulation of this problem and investigate two solution approaches based on mathematical programming and greedy search. This work provides the basis for elaborating a decision support system for efficient user project planning in social and medico-social centers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001251",
    "keywords": [
      "Business",
      "Center (category theory)",
      "Chemistry",
      "Computer science",
      "Crystallography",
      "Engineering",
      "Engineering management",
      "Knowledge management",
      "Management science",
      "Mechanical engineering",
      "Operations research",
      "Process management",
      "Programming language",
      "Set (abstract data type)",
      "Work (physics)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yinuo"
      },
      {
        "surname": "Hao",
        "given_name": "Jin-Kao"
      },
      {
        "surname": "Chabane",
        "given_name": "Brahim"
      }
    ]
  },
  {
    "title": "A probabilistic clustering model for hate speech classification in twitter",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114762",
    "abstract": "The key challenges for automatic hate-speech classification in Twitter are the lack of generic architecture, imprecision, threshold settings and fragmentation issues. Most studies used binary classifiers for hate speech classification, but these classifiers cannot really capture other emotions that may overlap between positive or negative class. Hence, a probabilistic clustering model for hate speech classification in twitter was developed to tackle problems with hate speech classification. A metadata extractor was used to collect tweets containing hate speech keywords and a crowd-sourced experts was employed to label the collected hate tweets into two categories: hate speech and non-hate speech. Features representation was done with Term Frequency- Inverse Document Frequency (TF-IDF) model and enhanced with topics inferred by a Bayes classifier. A rule-based clustering method was used to automatically classify real-time tweets into the correct topic clusters. Fuzzy logic was then used for hate speech classification using semantic fuzzy rules and a score computation module. From the evaluation results, it was observed that the developed model performed better in hate speech detection with F1-sore of 0.9256 using a 5-fold cross validation. Similarly, the developed model for hate speech classification performed better with F1-score of 91.5 compared to related models. The developed model also indicates a more perfect test having an AUC of 0.9645, when compared to similar methods. The Paired Sample t-Test validated the efficiency of the developed model for hate speech classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002037",
    "keywords": [
      "Artificial intelligence",
      "Binary classification",
      "Classifier (UML)",
      "Cluster analysis",
      "Computer science",
      "Machine learning",
      "Naive Bayes classifier",
      "Natural language processing",
      "Probabilistic logic",
      "Speech recognition",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Ayo",
        "given_name": "Femi Emmanuel"
      },
      {
        "surname": "Folorunso",
        "given_name": "Olusegun"
      },
      {
        "surname": "Ibharalu",
        "given_name": "Friday Thomas"
      },
      {
        "surname": "Osinuga",
        "given_name": "Idowu Ademola"
      },
      {
        "surname": "Abayomi-Alli",
        "given_name": "Adebayo"
      }
    ]
  },
  {
    "title": "Indoor surveillance video based feature recognition for pedestrian dead reckoning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114653",
    "abstract": "Driven by the connection of everything, various electronic devices and sensors have been applied to the Internet of Things (IoT) as sources of information. However, indoor surveillance cameras, which are a quality resource, are only used in the field of security monitoring. Based on the development of indoor surveillance camera about image features, this study proposes a positioning method of pedestrian dead reckoning (PDR) based on image feature recognition of indoor surveillance video. In this study, the images collected by traditional indoor surveillance cameras are used as the data source. By extracting the image features to establish an image landmark database, geographic coordinates are allocated to each feature image of the landmark database in order to establish indoor location benchmark points. These image landmarks are combined with pedestrian dead reckoning to obtain the accurate position estimation. The experimental results show that the proposed indoor surveillance video based feature recognition for pedestrian dead reckoning can effectively use the indoor camera resources as a position benchmark in order to achieve higher pedestrian positioning accuracy. Meanwhile, this strategy of establishing a landmark database based on existing monitoring videos effectively utilizes existing indoor monitoring resources and promotes the application of traditional surveillance cameras.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000944",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Dead reckoning",
      "Economics",
      "Feature (linguistics)",
      "Finance",
      "Geodesy",
      "Geography",
      "Global Positioning System",
      "Landmark",
      "Linguistics",
      "Pedestrian",
      "Pedestrian detection",
      "Philosophy",
      "Position (finance)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Yinfeng"
      },
      {
        "surname": "Zhao",
        "given_name": "Jingya"
      },
      {
        "surname": "Yu",
        "given_name": "Ning"
      },
      {
        "surname": "Feng",
        "given_name": "Renjian"
      }
    ]
  },
  {
    "title": "A clustering based Swarm Intelligence optimization technique for the Internet of Medical Things",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114648",
    "abstract": "Internet of Medical Things (IoMT) is a recently introduced paradigm which has gained relevance as an emerging technology for widely connected and heterogeneous networks. In the medical context, these networks involve many different processes run by different types of devices called objects that interact and collaborate to achieve a common goal (e.g, diagnosis, treatment, monitoring, or rehabilitation of a patient). An IoMT framework in a smart healthcare system dynamically monitors patients to respond to their assistance demands so that vital signs of critical or unusual cases can be uncovered based on the collected data. To this end, an effective technique called SIoMT (Swarm Intelligence optimization technique for the IoMT) is proposed in this paper for periodically discovering, clustering, analyzing, and managing useful data about potential patients. Notably, the SIoMT technique is widely used with distributed nodes for analyzing and managing data groups. Different from the existing clustering algorithms, SIoMT performs clustering based on the characteristics and distance between objects or swarms. More specifically, these data are collected and grouped, in early stage, using a clustering approach inspired by the Bee Colony Optimization algorithm (BCO), adopting some standard quality measures which helped minimizing the latency and required computational cost. To test the performance of the proposed SIoMT, one public dataset (Ward2ICU) was considered from the online source. Various experiments were done to analyze the effects of different parameters on the proposed SIoMT’s performance, and the results from the final variant of the proposed algorithm were compared against different variants of the same algorithm with different clustering algorithms and different optimization algorithms. Subsequently, after analyzing different components by solving various IoMT datasets, the capability and the superiority of the proposed SIoMT approach is well-established among its competitive counterparts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000890",
    "keywords": [
      "Artificial intelligence",
      "Big data",
      "Biology",
      "Cluster analysis",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Latency (audio)",
      "Law",
      "Machine learning",
      "Paleontology",
      "Particle swarm optimization",
      "Political science",
      "Relevance (law)",
      "Swarm intelligence",
      "Telecommunications",
      "The Internet",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "El-shafeiy",
        "given_name": "Engy"
      },
      {
        "surname": "Sallam",
        "given_name": "Karam M."
      },
      {
        "surname": "Chakrabortty",
        "given_name": "Ripon K."
      },
      {
        "surname": "Abohany",
        "given_name": "Amr A."
      }
    ]
  },
  {
    "title": "EA-MSCA: An effective energy-aware multi-objective modified sine-cosine algorithm for real-time task scheduling in multiprocessor systems: Methods and analysis",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114699",
    "abstract": "With the significant growth of multiprocessor systems (MPS) to deal with complex tasks and speed up their execution, the energy generated as a result of this growth becomes one of the significant limits to that growth. Although several traditional techniques are available to deal with this challenge, they don’t deal with this problem as multi-objective to optimize both energy and makespan metrics at the same time, in addition to expensive cost and memory usage. Therefore, this paper proposes a multi-objective approach to tackle the task scheduling for MPS based on the modified sine-cosine algorithm (MSCA) to optimize the makespan and energy using the Pareto dominance strategy; this version is abbreviated as energy-aware multi-objective MSCA (EA-M2SCA). The classical SCA is modified based on dividing the optimization process into three phases. The first phase explores the search space as much as possible at the start of the optimization process, the second phase searches around a solution selected randomly from the population to avoid becoming trapped into local minima within the optimization process, and the last searches around the best-so-far solution to accelerate the convergence. To further improve the performance of EA-M2SCA, it was hybridized with the polynomial mutation mechanism in two effective manners to accelerate the convergence toward the best-so-far solution with preserving the diversity of the solutions; this hybrid version is abbreviated as EA-MHSCA. Finally, the proposed algorithms were compared with a number of well-established multi-objective algorithms: EA-MHSCA is shown to be superior in most test cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001408",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Embedded system",
      "Job shop scheduling",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Maxima and minima",
      "Multiprocessing",
      "Parallel computing",
      "Pareto principle",
      "Routing (electronic design automation)",
      "Scheduling (production processes)"
    ],
    "authors": [
      {
        "surname": "Abdel-Basset",
        "given_name": "Mohamed"
      },
      {
        "surname": "Mohamed",
        "given_name": "Reda"
      },
      {
        "surname": "Abouhawwash",
        "given_name": "Mohamed"
      },
      {
        "surname": "Chakrabortty",
        "given_name": "Ripon K."
      },
      {
        "surname": "Ryan",
        "given_name": "Michael J."
      }
    ]
  },
  {
    "title": "Trajectory planning for multi-robot systems: Methods and applications",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114660",
    "abstract": "In the multiple fields covered by Artificial Intelligence (AI), path planning is undoubtedly one of the issues that cover a wide range of research lines. To be able to find an optimal solution, which allows one or several vehicles to establish a safe and effective way to reach a final state from an initial state, is a challenge that continues to be studied today. The increasingly widespread use of autonomous vehicles, both aerial and ground-based, make path planning an essential aspect for incorporating these systems into an endless number of applications. Besides, in recent years, the use of Multi-Robot Systems (MRS) has spread, consisting of both Unmanned Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs), gaining versatility and robustness in their operation. The possibility of using heterogeneous robotic teams allows tackling, autonomously, and simultaneously, a wide range of tasks with different characteristics in the same environment. For this purpose, path planning becomes a crucial aspect and, for this reason, this work aims to offer a general vision of trajectory planning, to establish a comparison between the methods and algorithms present in the literature for the resolution of this problem within MRS, and finally, to show the applicability of these methods in different areas, together with the importance of these methods for achieving autonomous and safe navigation of different types of vehicles.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001019",
    "keywords": [
      "Aerospace engineering",
      "Artificial intelligence",
      "Astronomy",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Engineering",
      "Gene",
      "Motion planning",
      "Physics",
      "Range (aeronautics)",
      "Robot",
      "Robustness (evolution)",
      "Trajectory",
      "Unmanned ground vehicle"
    ],
    "authors": [
      {
        "surname": "Madridano",
        "given_name": "Ángel"
      },
      {
        "surname": "Al-Kaff",
        "given_name": "Abdulla"
      },
      {
        "surname": "Martín",
        "given_name": "David"
      },
      {
        "surname": "de la Escalera",
        "given_name": "Arturo"
      }
    ]
  },
  {
    "title": "Predictive maintenance system for production lines in manufacturing: A machine learning approach using IoT data in real-time",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114598",
    "abstract": "In this study, a data driven predictive maintenance system was developed for production lines in manufacturing. By utilizing the data generated from IoT sensors in real-time, the system aims to detect signals for potential failures before they occur by using machine learning methods. Consequently, it helps address the issues by notifying operators early such that preventive actions can be taken prior to a production stop. In current study, the effectiveness of the system was also assessed using real-world manufacturing system IoT data. The evaluation results indicated that the predictive maintenance system was successful in identifying the indicators of potential failures and it can help prevent some production stops from happening. The findings of comparative evaluations of machine learning algorithms indicated that models of Random Forest, a bagging ensemble algorithm, and XGBoost, a boosting method, appeared to outperform the individual algorithms in the assessment. The best performing machine learning models in this study have been integrated into the production system in the factory.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000397",
    "keywords": [
      "Artificial intelligence",
      "Big data",
      "Boosting (machine learning)",
      "Computer science",
      "Data mining",
      "Economics",
      "Embedded system",
      "Engineering",
      "Ensemble learning",
      "Factory (object-oriented programming)",
      "Internet of Things",
      "Machine learning",
      "Macroeconomics",
      "Mechanical engineering",
      "Operating system",
      "Predictive maintenance",
      "Preventive maintenance",
      "Process (computing)",
      "Production (economics)",
      "Production line",
      "Production system (computer science)",
      "Programming language",
      "Random forest",
      "Reliability engineering"
    ],
    "authors": [
      {
        "surname": "Ayvaz",
        "given_name": "Serkan"
      },
      {
        "surname": "Alpay",
        "given_name": "Koray"
      }
    ]
  },
  {
    "title": "A bi-objective simulation-based optimization algorithm for redundancy allocation problem in series-parallel systems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114745",
    "abstract": "The present study proposes a bi-objective simulation-based optimization model applicable to the redundancy allocation problem (RAP) with heterogeneous components for the objective functions of system reliability maximization and system cost minimization. Proposed RAP identifies the optimal component types, the redundancy level, and the redundancy strategy, comprising active, cold-standby, mixed, or K-mixed configurations, with imperfect switching. Based on the stochastic nature and NP-hard complexity of the problem, except for the active redundancy strategy, there is no analytical closed-form method for accurately assessing system reliability. Hence, earlier studies carried out system reliability optimization by single-stage stochastic techniques that estimate the lower Lagrangian function bound. This limitation adds to the design cost and hinders greater system reliability. Generally, one cannot analytically evaluate system reliability. The present study employs simulation sampling in order to make efficient and unbiased reliability estimates. 4Dscript interpreting programming is utilized to design the computerized simulation model. Since RAP has a combinatorial nature, the present study exploits the controlled elitist non-dominated sorting genetic algorithm (NSGA-II) to obtain Pareto-optimal fronts with properly-distributed optimal points. Various benchmark solutions of the literature are investigated to validate the developed model and evaluate the proposed NSGA-II method efficiency. The findings revealed satisfactory performance in system reliability enhancement and total system cost reduction compared to the earlier methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100186X",
    "keywords": [
      "Algorithm",
      "Benchmark (surveying)",
      "Computer science",
      "GRASP",
      "Geodesy",
      "Geography",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Minification",
      "Operating system",
      "Physics",
      "Power (physics)",
      "Programming language",
      "Quantum mechanics",
      "Redundancy (engineering)",
      "Reliability (semiconductor)",
      "Sorting"
    ],
    "authors": [
      {
        "surname": "Chambari",
        "given_name": "Amirhossein"
      },
      {
        "surname": "Azimi",
        "given_name": "Parham"
      },
      {
        "surname": "Najafi",
        "given_name": "Amir Abbas"
      }
    ]
  },
  {
    "title": "Exploring user movie interest space: A deep learning based dynamic recommendation model",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114695",
    "abstract": "Exploring user interest behind massive user behaviors is essential for online recommendations. Although recommendation models have been proposed recently with great success, existing studies ignore not only the timeliness of online users’ behaviors in terms of their interest, but also the sequential characteristics of their behaviors. To overcome this limitation, we construct a User Movie Interest Space (UMIS) model based on the sequential ratings of users. We define three indexes to elucidate the features of the interest of users for UMIS, which describe different patterns of behaviors of users related to their interests. Based on UMIS we propose a deep learning model named Dynamic Interest Flow (DIF) to provide dynamic movie recommendations. The DIF model achieves intelligently multi-dimensional observations on a user’s interest space and to predict simultaneously a variety of their future interests. Experimental results indicate that DIF outperforms traditional rating-based models and other state-of-the-art deep learning models. Results also demonstrate that modeling a dynamic recommendation as a sequential prediction is supposed to obtain outstanding advantages.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001366",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Construct (python library)",
      "Deep learning",
      "Human–computer interaction",
      "Machine learning",
      "Operating system",
      "Programming language",
      "Recommender system",
      "Space (punctuation)",
      "Variety (cybernetics)"
    ],
    "authors": [
      {
        "surname": "Gan",
        "given_name": "Mingxin"
      },
      {
        "surname": "Cui",
        "given_name": "Hongfei"
      }
    ]
  },
  {
    "title": "Minimizing activity exposures in project scheduling under uncertainty",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114635",
    "abstract": "We propose a model to solve a project scheduling problem where resource assignments and activity schedules need to be determined to achieve a set of due-date requirements as well as possible. The concept of activity duration tolerance levels is introduced to describe the longest activity durations over which the due-dates are guaranteed to be achieved. Based on this, we propose a due-date achievement model using a proposed performance measure termed as the activities exposure level, which accounts for the total amount of durations exceeding tolerance levels for a given scheduling solution. We describe various properties and characteristics of the proposed scheduling optimization model and its practical implications. We show that the associated stochastic scheduling problem with resource constraints can be equivalently computed as the solution of a modest-sized mixed-integer linear program using the sample average approximation approach. Computational results show that our proposed models perform well when compared to different standard approaches across various performance measures.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000762",
    "keywords": [
      "Computer science",
      "Mathematical optimization",
      "Mathematics",
      "Scheduling (production processes)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zhiguo"
      },
      {
        "surname": "Ng",
        "given_name": "Tsan Sheng"
      },
      {
        "surname": "Pang",
        "given_name": "Chee Khiang"
      }
    ]
  },
  {
    "title": "Identifying the mobile application repertoire based on weighted formal concept analysis",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114678",
    "abstract": "Smartphones are one of the fastest-growing and most popular consumer devices introduced in the last decade, and the media environment is also changing dramatically. However, there is still a lack of research on recognizing smartphones as media and understanding their usage behavior. This study aims to understand smartphones' mobile application usage patterns from a media combination perspective by identifying mobile application repertoire and killer applications based on demographic segmentation. In this study, we propose the Weighted Formal Concept Analysis to specify the preference of mobile applications and identify their repertoires. The preference of mobile application evaluates the following four aspects in combination: relevance, commonly-shared, conformance, and contribution viewpoints. We conducted a case study, which identified the mobile application within-group and across-group repertoires based on the proposed method, and also discovered that mobile repertoire varies with not only age and gender but also their combinations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001196",
    "keywords": [
      "Acoustics",
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Economics",
      "Human–computer interaction",
      "Law",
      "Microeconomics",
      "Perspective (graphical)",
      "Physics",
      "Political science",
      "Preference",
      "Relevance (law)",
      "Repertoire",
      "Viewpoints",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Kwon",
        "given_name": "Sung Eun"
      },
      {
        "surname": "Kim",
        "given_name": "Youn Tae"
      },
      {
        "surname": "Suh",
        "given_name": "Hyo-won"
      },
      {
        "surname": "Lee",
        "given_name": "Heejung"
      }
    ]
  },
  {
    "title": "Speech emotion recognition using recurrent neural networks with directional self-attention",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114683",
    "abstract": "As an important branch of affective computing, Speech Emotion Recognition (SER) plays a vital role in human–computer interaction. In order to mine the relevance of signals in audios an increase the diversity of information, Bi-directional Long-Short Term Memory with Directional Self-Attention (BLSTM-DSA) is proposed in this paper. Long Short-Term Memory (LSTM) can learn long-term dependencies from learned local features. Moreover, Bi-directional Long-Short Term Memory (BLSTM) can make the structure more robust by direction mechanism because that the directional analysis can better recognize the hidden emotions in sentence. At the same time, autocorrelation of speech frames can be used to deal with the lack of information, so that Self-Attention mechanism is introduced into SER. The attention weight of each frame is calculated with the output of the forward and backward LSTM respectively rather than calculated after adding them together. Thus, the algorithm can automatically annotate the weights of speech frames to correctly select frames with emotional information in temporal network. When evaluate it on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) database and Berlin database of emotional speech (EMO-DB), the BLSTM-DSA demonstrates satisfactory performance on the task of speech emotion recognition. Especially in emotion recognizing of happiness and anger, BLSTM-DSA achieves the highest recognition accuracies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100124X",
    "keywords": [
      "Artificial intelligence",
      "Associative property",
      "Computer science",
      "Economics",
      "Emotion recognition",
      "Frame (networking)",
      "Law",
      "Management",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Pure mathematics",
      "Quantum mechanics",
      "Relevance (law)",
      "Sentence",
      "Speech recognition",
      "Task (project management)",
      "Telecommunications",
      "Term (time)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Dongdong"
      },
      {
        "surname": "Liu",
        "given_name": "Jinlin"
      },
      {
        "surname": "Yang",
        "given_name": "Zhuo"
      },
      {
        "surname": "Sun",
        "given_name": "Linyu"
      },
      {
        "surname": "Wang",
        "given_name": "Zhe"
      }
    ]
  },
  {
    "title": "A knowledge-based risk management tool for construction projects using case-based reasoning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114776",
    "abstract": "Construction projects are often deemed as complex and high-risk endeavours, mostly because of their vulnerability to external conditions as well as project-related uncertainties. Risk management (RM) is a critical success factor for companies operating in the construction industry. RM is a knowledge-intensive process that requires effective management of risk-related knowledge. Although some research has already been conducted to develop tools to support knowledge-based RM processes, most of these tools ignore some critical features, such as live knowledge capture, web-based platform for knowledge sharing and effective case retrieval for learning from past projects. Moreover, several RM phases, such as risk identification, analysis, response and monitoring are not usually integrated. Thus, this study aims to bridge these gaps by developing a knowledge-based RM tool (namely, CBRisk) via case-based reasoning (CBR). CBRisk has been developed as a web-based tool that supports the cyclic RM process and utilises an effective case retrieval method considering a comprehensive list of project similarity features in the form of fuzzy linguistic variables. Finally, the developed tool was evaluated and validated by conducting black-box testing and expert review meeting. Results demonstrated that CBRisk has a considerable potential to enhance the effectiveness of RM in construction projects and may be used in other project-based industries with minimal modifications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421002177",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Bridge (graph theory)",
      "Case-based reasoning",
      "Computer science",
      "Economics",
      "Fuzzy logic",
      "Identification (biology)",
      "Image (mathematics)",
      "Internal medicine",
      "Knowledge management",
      "Knowledge sharing",
      "Management",
      "Medicine",
      "Operating system",
      "Process (computing)",
      "Risk analysis (engineering)",
      "Risk management",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Okudan",
        "given_name": "Ozan"
      },
      {
        "surname": "Budayan",
        "given_name": "Cenk"
      },
      {
        "surname": "Dikmen",
        "given_name": "Irem"
      }
    ]
  },
  {
    "title": "Abnormal crowd density estimation in aerial images based on the deep and handcrafted features fusion",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114656",
    "abstract": "Abnormal crowd analysis has always been an interesting research area since it ensures people’s safety and prevents crowd disasters in large scale events. In particular, crowd density estimation in aerial images has the advantage of monitoring open areas by covering a very large view and reaching difficult access areas. In this context, we proposed a new method for crowd density estimation in aerial images in order to detect crowded areas showing abnormal densities. The proposed method consists of two phases: an offline phase and an inference phase. The offline phase aimed to generate a crowd model using a fusion of relevant deep and handcrafted features selected via the minimum-redundancy maximum-relevance (mRMR) technique. However, in the inference phase, we made use of the already generated model to classify the aerial images patches into four classes: None, Sparse, Medium and Dense. Our main contributions lie in the fact we relied on the fusion of semantic and low-level information to encode crowd density patches, as well as, the definition of the most salient crowd features to reduce confusion between crowd density classes. The experimental evaluation of the proposed method proves the validity of these contributions as well as the effectiveness and efficiency of our method compared to the state-of-the-art reference methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100097X",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Data mining",
      "Density estimation",
      "Estimator",
      "Geography",
      "Inference",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Redundancy (engineering)",
      "Salient",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Bouhlel",
        "given_name": "Fatma"
      },
      {
        "surname": "Mliki",
        "given_name": "Hazar"
      },
      {
        "surname": "Hammami",
        "given_name": "Mohamed"
      }
    ]
  },
  {
    "title": "A collective intelligence oriented three-layer framework for socialized and collaborative product design",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114742",
    "abstract": "Socialized and collective intelligence oriented product design (SCPD) is a new kind of design pattern emerged under the context of advanced internet technologies and sharing economic trend. It is usually carried out by large numbers of socialized and self-driven participators from different social backgrounds in an open, sharing, self-organized, distributed, and collaborative manner. These characteristics bring SCPD with advantages such as rich design resources, high innovation potentials, deep connection with customer-centric markets, etc., but the characteristics also cause the problems of low efficiency and low reliability during the collaborative design process of SCPD. To mitigate these problems, a collective intelligence oriented three-layer SCPD framework is established. The first layer focuses on design task decomposition and subtask analysis. The second layer focuses on generating alternative design solutions for the subtasks using a customized Blackboard model. The third layer focuses on identifying the most preferred solution using a fuzzy VIKOR algorithm driven consensus reaching model. The SCPD framework is able to support orderly interaction, mutual inspiration, group decision making, and participation stimulation among the socialized participators. In this way, the framework provides a more systematic and efficient approach to utilize the CI from SPs for product design, and thus enlarges the application scope of SCPD pattern from software and small-scale physical products to relatively complicated and high-value physical products. The operability of the framework is demonstrated through an innovative 3D printer design project.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001834",
    "keywords": [
      "Biology",
      "Business",
      "Chemistry",
      "Collective intelligence",
      "Computer science",
      "Context (archaeology)",
      "Geometry",
      "Knowledge management",
      "Layer (electronics)",
      "Marketing",
      "Mathematics",
      "New product development",
      "Operability",
      "Organic chemistry",
      "Paleontology",
      "Product (mathematics)",
      "Product design",
      "Software engineering"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Maolin"
      },
      {
        "surname": "Li",
        "given_name": "Weidong"
      },
      {
        "surname": "Jiang",
        "given_name": "Pingyu"
      }
    ]
  },
  {
    "title": "GRASP and tabu search for the generalized dispersion problem",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114703",
    "abstract": "The problem of maximizing dispersion requires the selection of a specific number of elements from a given set, in such a way that the minimum distance between the pairs of selected elements is maximized. In recent years, this problem has received a lot of attention and has been solved with many complex heuristics. However, there is a recent variant in which the selected elements have to satisfy two realistic constraints, a minimum capacity limit and a maximum budget, which in spite of its practical significance in facility location, has received little attention. In this paper, we first propose mathematical models to obtain the optimal solution of small- and medium-size instances, and then present new metaheuristic procedures for finding approximate solutions to target large-size instances. Specifically, we develop a GRASP and a tabu search to obtain high quality solutions in short computational times. We perform extensive experimentation to compare our heuristic proposals with the optimal solutions obtained with the models applied to the Gurobi optimizer, as well as with a previous heuristic. Statistical tests confirm the superiority of our methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001445",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "GRASP",
      "Geodesy",
      "Geography",
      "Heuristic",
      "Heuristics",
      "Limit (mathematics)",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Parametric statistics",
      "Programming language",
      "Selection (genetic algorithm)",
      "Set (abstract data type)",
      "Statistics",
      "Tabu search"
    ],
    "authors": [
      {
        "surname": "Martínez-Gavara",
        "given_name": "Anna"
      },
      {
        "surname": "Corberán",
        "given_name": "Teresa"
      },
      {
        "surname": "Martí",
        "given_name": "Rafael"
      }
    ]
  },
  {
    "title": "Experimental comparison of results provided by ranking methods in Data Envelopment Analysis",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114739",
    "abstract": "We consider the problem of ranking Decision Making Units (DMUs) in Data Envelopment Analysis. We illustrate the use of fifteen selected approaches on a numerical example. They represent different categories, including cross- and super-efficiency, multivariate statistics, decision analysis, benchmarking, virtual DMU, and social networks. Moreover, we formalize a new category of ranking methods based on the concept of Robustness Analysis. They exploit a space of feasible input/output weight vectors with the Monte Carlo simulation to derive the expected efficiencies or ranks, or to compute the priorities or net flow scores of DMUs based on the matrix of pairwise efficiency outranking indices. The rankings constructed by all methods are compared on both artificially generated and real-world datasets with different numbers units, inputs and outputs, and performance distributions. The considered datasets represent the most common application areas of the DEA methods, such as finances, education, transportation, healthcare, farming, and the energy industry. The results are quantified in terms of five measures. We indicate that the choice of a method has a significant impact on the ranking, revealing the procedures that offer similar results or differ vastly in terms of the recommended order or the most preferred DMU.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001809",
    "keywords": [
      "Artificial intelligence",
      "Benchmarking",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Data envelopment analysis",
      "Data mining",
      "Economics",
      "Exploit",
      "Gene",
      "Machine learning",
      "Management",
      "Mathematical optimization",
      "Mathematics",
      "Monte Carlo method",
      "Pairwise comparison",
      "Ranking (information retrieval)",
      "Robustness (evolution)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Labijak-Kowalska",
        "given_name": "Anna"
      },
      {
        "surname": "Kadziński",
        "given_name": "Miłosz"
      }
    ]
  },
  {
    "title": "High-dimensional lag structure optimization of fuzzy time series",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114698",
    "abstract": "Lag-selection is a high dimensional hyper-parameter in the fuzzy time series (FTS) which requires complex optimization process and computational capacity particularly in high frequency dataset (e.g. daily, hourly). Multivariate high order FTS suffers from establishing long logical relationships, and the difficulty of rule matching is proportional to the time lags and number of variables. In the vast majority of FTS literature, a grid search algorithm or evolutionary algorithms are run to find singular time-lags. In addition, some researchers determine the lag structure arbitrarily. However, grid search in high dimensional problems is not practical especially when recursive predictions are generated and evolutionary algorithms suffer from the randomness which may generate different solutions. This paper proposes an alternative approach to the lag selection problem by utilizing supervised principal component analysis (SPCA), and the lag structure is reduced to low dimensional space. SPCA has been developed to project the high dimensional lagged variables into the first principal component. An empirical study is conducted to validate the proposed approach by using global shipping industry data in the world.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001391",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer network",
      "Computer science",
      "Data mining",
      "Fuzzy logic",
      "Geometry",
      "Grid",
      "Lag",
      "Machine learning",
      "Matching (statistics)",
      "Mathematical optimization",
      "Mathematics",
      "Multivariate statistics",
      "Operating system",
      "Paleontology",
      "Principal component analysis",
      "Process (computing)",
      "Randomness",
      "Selection (genetic algorithm)",
      "Series (stratigraphy)",
      "Statistics",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Ruobin"
      },
      {
        "surname": "Duru",
        "given_name": "Okan"
      },
      {
        "surname": "Yuen",
        "given_name": "Kum Fai"
      }
    ]
  },
  {
    "title": "Unsupervised feature selection by non-convex regularized self-representation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114643",
    "abstract": "Feature selection, as a crucial pre-processing stage in expert and intelligent systems, aims at reducing the dimensionality of the high-dimensional data by selecting the optimal subset from original features set. It can enhance the interpretability, improve learning performance, and increase computational efficiency. In real-world applications, obtaining class labels of data is time consuming and labor intensive, thus unsupervised feature selection is more practically important but correspondingly more challenging. Self-representation learning provides some insights on unsupervised feature selection, whose goal is to identify a representative feature subset so that all the features can be well reconstructed by them. In this paper, we propose a new unsupervised feature selection method by using NOn-conVex Regularized Self-Representation (NOVRSR). Different from most prior researches resorting to pseudo labels of data, NOVRSR exploits importance and relevance of features by self-representation. Moreover, the ℓ 2 , 1 - 2 sparse regularization, which is non-convex yet Lipschitz continuous, is enforced on the representation coefficient matrix to perform feature selection. We show in theory that the utilization of ℓ 2 , 1 - 2 can guarantee the sparsity of the representation coefficient matrix. In addition, to find the solution of the resulting non-convex formula, we design an iterative algorithm in the framework of ConCave-Convex Procedure (CCCP) and prove that the iterative sequence converges to the stationary point satisfying the first-order optimality condition. An adopted Alternating Direction Method of Multipliers (ADMM) is embedded to solve the sequence of convex subproblems of CCCP efficiently. Extensive experimental studies on real-world datasets demonstrate that the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000841",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Convex optimization",
      "Curse of dimensionality",
      "Dimensionality reduction",
      "Feature (linguistics)",
      "Feature learning",
      "Feature selection",
      "Geometry",
      "Interpretability",
      "Law",
      "Linguistics",
      "Mathematical optimization",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Regular polygon",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Miao",
        "given_name": "Jianyu"
      },
      {
        "surname": "Ping",
        "given_name": "Yuan"
      },
      {
        "surname": "Chen",
        "given_name": "Zhensong"
      },
      {
        "surname": "Jin",
        "given_name": "Xiao-Bo"
      },
      {
        "surname": "Li",
        "given_name": "Peijia"
      },
      {
        "surname": "Niu",
        "given_name": "Lingfeng"
      }
    ]
  },
  {
    "title": "Effective feature representation using symbolic approach for classification and clustering of big data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114658",
    "abstract": "The tremendous growth in the technology has led to the accumulation of enormous Big Data. Techniques that efficiently analyse this Big Data are in great demand. Tweets from Social media and Sensor data are some of the most common forms of Big Data. Machine learning algorithms pave way for researchers to analyze Big Data. Most Machine learning algorithms depend on efficient feature extraction and feature selection for its success. Here, we explore feature selection methods like entropy and Rough set on the sensor data. Also a symbolic approach of feature extraction is proposed which represents both sensor and twitter data efficiently for further data analysis. Some popular classifiers like Naïve Bayes, K Nearest Neighbour, Support Vector Machine and Decision Tree are used for validating the efficacy of the features selected. An ensemble classifier technique is also proposed which is compared with various state of the art ensemble classifiers. Symbolic features perform better than both entropy and Rough set features for sensor data and improves the clustering efficiency of twitter data. The proposed ensemble weighted average classifier on Symbolic features outperform all the other ensemble classifiers and independent classifiers. The results obtained from these methods have the potential to aid the public health surveillance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000993",
    "keywords": [
      "Artificial intelligence",
      "Big data",
      "Classifier (UML)",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Decision tree",
      "Ensemble learning",
      "Entropy (arrow of time)",
      "Feature extraction",
      "Feature selection",
      "Machine learning",
      "Naive Bayes classifier",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Lavanya",
        "given_name": "P.G."
      },
      {
        "surname": "Kouser",
        "given_name": "K."
      },
      {
        "surname": "Suresha",
        "given_name": "Mallappa"
      }
    ]
  },
  {
    "title": "A data mining framework for financial prediction",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114651",
    "abstract": "In the financial markets, because real-time transactions directly relate to profit, it is important to process and analyze data on a real-time basis. In practice, decisions influenced by experts’ experiences from fundamental and technical analysis occur frequently compared to decisions using prediction algorithms. A domain-specific data mining framework was proposed recently to reduce related cost. Therefore, this study proposes a novel data mining framework suitable for financial markets according to expert knowledge. The proposed framework predominantly considers the following three perspectives as the standards for the effectiveness of research: interpretability, proper prediction metrics, and reporting methods. We applied our framework to the real-world financial prediction problems, such as the 3–10 year treasury spread forecasts. Consequently, we achieved an 84% prediction performance on the spread prediction and used hierarchical information to provide additional insight. In addition, we obtained practical knowledge and synergies through extraction of critical variables that can be used as a quick and accurate data-driven decision making support tool by active agents in the real world.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000920",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Domain knowledge",
      "Economics",
      "Finance",
      "Interpretability",
      "Machine learning",
      "Microeconomics",
      "Operating system",
      "Process (computing)",
      "Profit (economics)"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Misuk"
      }
    ]
  },
  {
    "title": "Multi-stage transfer learning for lung segmentation using portable X-ray devices for patients with COVID-19",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114677",
    "abstract": "One of the main challenges in times of sanitary emergency is to quickly develop computer aided diagnosis systems with a limited number of available samples due to the novelty, complexity of the case and the urgency of its implementation. This is the case during the current pandemic of COVID-19. This pathogen primarily infects the respiratory system of the afflicted, resulting in pneumonia and in a severe case of acute respiratory distress syndrome. This results in the formation of different pathological structures in the lungs that can be detected by the use of chest X-rays. Due to the overload of the health services, portable X-ray devices are recommended during the pandemic, preventing the spread of the disease. However, these devices entail different complications (such as capture quality) that, together with the subjectivity of the clinician, make the diagnostic process more difficult and suggest the necessity for computer-aided diagnosis methodologies despite the scarcity of samples available to do so. To solve this problem, we propose a methodology that allows to adapt the knowledge from a well-known domain with a high number of samples to a new domain with a significantly reduced number and greater complexity. We took advantage of a pre-trained segmentation model from brain magnetic resonance imaging of a unrelated pathology and performed two stages of knowledge transfer to obtain a robust system able to segment lung regions from portable X-ray devices despite the scarcity of samples and lesser quality. This way, our methodology obtained a satisfactory accuracy of 0.9761 ± 0.0100 for patients with COVID-19, 0.9801 ± 0.0104 for normal patients and 0.9769 ± 0.0111 for patients with pulmonary diseases with similar characteristics as COVID-19 (such as pneumonia) but not genuine COVID-19.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001184",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Disease",
      "Domain (mathematical analysis)",
      "Epistemology",
      "Infectious disease (medical specialty)",
      "Internal medicine",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Pathology",
      "Philosophy",
      "Pneumonia",
      "Quality (philosophy)",
      "Segmentation",
      "Transfer of learning",
      "USable",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Vidal",
        "given_name": "Plácido L."
      },
      {
        "surname": "de Moura",
        "given_name": "Joaquim"
      },
      {
        "surname": "Novo",
        "given_name": "Jorge"
      },
      {
        "surname": "Ortega",
        "given_name": "Marcos"
      }
    ]
  },
  {
    "title": "Classification of hyperspectral imagery using a fully complex-valued wavelet neural network with deep convolutional features",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114708",
    "abstract": "The number of spectral bands obtained by hyperspectral sensors improves the ability to distinguish physical objects and materials. But it also brings new challenges to image classification and analysis. In this study, a novel deep learning-based hybrid model called CNN-CVWNN is presented for the hyperspectral images classification (HSIs). The model uses a convolutional neural network (CNN) to extract multilayer image representation and uses the complex valued wavelet neural network (CVWNN) to classify the image using extracted features. The process steps of the proposed method are briefly as follows. First of all, the CNN algorithm has been applied to hyperspectral images. After this stage, efficient features have been obtained. These extracted features were then converted into a complex-valued number format using a novel random based transformation method. Thus, a novel complex-valued attribute set has been obtained for the HSI classification. The obtained features have been presented as input to the CVWNN algorithm. The hybrid method replaces real valued neural network inside CNN with CVWNN to enhance robustness and generalization of CNN. The experiments have been carried out on three data sets consisted of three popular hyperspectral airborne images. The developed method increases classification accuracy compared to other classification approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001494",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Contextual image classification",
      "Convolutional neural network",
      "Gene",
      "Hyperspectral imaging",
      "Image (mathematics)",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Wavelet"
    ],
    "authors": [
      {
        "surname": "Peker",
        "given_name": "Musa"
      }
    ]
  },
  {
    "title": "A discrete bat algorithm based on Lévy flights for Euclidean traveling salesman problem",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114639",
    "abstract": "Bat algorithm is a swarm-intelligence-based metaheuristic proposed in 2010. This algorithm was inspired by echolocation behavior of bats when searching their prey in nature. Since it first introduction, it continues to be used extensively until today, owing to its simplicity, easy handling and applicability to a wide range of problems. However, sometimes the major challenge faced by this technique is can be trapped in a local optimum when facing large complex problems. In this research work, a new discrete bat algorithm is proposed to solve the famous traveling salesman problem as NP-hard combinatorial optimization problem. To enhance the searching strategy and to avoid getting stuck in local minima, random walks based on Lévy's flights are combined with bat’s movement. In addition, to improve the diversity and convergence of the swarm, a neutral crossover operator is embedded to the proposed algorithm. To evaluate the performance of our proposal, two experiments are conducted on 38 benchmark datasets and the obtained results are compared with eight different approaches. Furthermore, the student’s t-test, the Friedman’s test and the post hoc Wilcoxon's test are performed to check whether there are significant differences between the proposed optimizer and the alternative techniques. The experimental results under comparative studies have shown that, in most cases, the proposed discrete bat algorithm yields significantly better results compared with its competitors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000804",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bat algorithm",
      "Benchmark (surveying)",
      "Composite material",
      "Computer science",
      "Convergence (economics)",
      "Crossover",
      "Economic growth",
      "Economics",
      "Geodesy",
      "Geography",
      "Lévy flight",
      "Materials science",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Maxima and minima",
      "Particle swarm optimization",
      "Premature convergence",
      "Random walk",
      "Range (aeronautics)",
      "Statistics",
      "Swarm behaviour",
      "Swarm intelligence",
      "Travelling salesman problem"
    ],
    "authors": [
      {
        "surname": "Saji",
        "given_name": "Yassine"
      },
      {
        "surname": "Barkatou",
        "given_name": "Mohammed"
      }
    ]
  },
  {
    "title": "The impact of the optimal buffer configuration on production line efficiency: A VNS-based solution approach",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114631",
    "abstract": "Intelligent design of manufacturing systems is an important research area because it directly affects the profitability of companies and is very costly. Production lines are the most encountered systems in manufacturing and optimal design of production lines is one of the basic research areas for both academy and practitioners. In this study, the buffer allocation problem (BAP), one of the most important optimization problems in designing production lines, is handled and a variable neighborhood search (VNS) based solution approach is proposed to solve the problem with the objectives of throughput maximization and total buffer size minimization. In addition to proposing a new solution approach, two initialization heuristics are employed to enhance the search efficiency. The existing benchmark problems were used to performance evaluation of the proposed approach. The computational study showed that the proposed VNS-based solution approach was highly effective in finding good-quality solutions for all the problem sets examined.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000725",
    "keywords": [
      "Algorithm",
      "Benchmark (surveying)",
      "Computer science",
      "Economics",
      "Engineering",
      "Finance",
      "Geodesy",
      "Geography",
      "Heuristics",
      "Industrial engineering",
      "Initialization",
      "Macroeconomics",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Mechanical engineering",
      "Metaheuristic",
      "Minification",
      "Production (economics)",
      "Production line",
      "Profitability index",
      "Programming language",
      "Variable neighborhood search"
    ],
    "authors": [
      {
        "surname": "Demir",
        "given_name": "Leyla"
      },
      {
        "surname": "Koyuncuoğlu",
        "given_name": "Mehmet Ulaş"
      }
    ]
  },
  {
    "title": "A survey and performance evaluation of deep learning methods for small object detection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114602",
    "abstract": "In computer vision, significant advances have been made on object detection with the rapid development of deep convolutional neural networks (CNN). This paper provides a comprehensive review of recently developed deep learning methods for small object detection. We summarize challenges and solutions of small object detection, and present major deep learning techniques, including fusing feature maps, adding context information, balancing foreground-background examples, and creating sufficient positive examples. We discuss related techniques developed in four research areas, including generic object detection, face detection, object detection in aerial imagery, and segmentation. In addition, this paper compares the performances of several leading deep learning methods for small object detection, including YOLOv3, Faster R-CNN, and SSD, based on three large benchmark datasets of small objects. Our experimental results show that while the detection accuracy on small objects by these deep learning methods was low, less than 0.4, Faster R-CNN performed the best, while YOLOv3 was a close second.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000439",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Convolutional neural network",
      "Deep learning",
      "Deep neural networks",
      "Face detection",
      "Facial recognition system",
      "Feature learning",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Object (grammar)",
      "Object detection",
      "Object-class detection",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yang"
      },
      {
        "surname": "Sun",
        "given_name": "Peng"
      },
      {
        "surname": "Wergeles",
        "given_name": "Nickolas"
      },
      {
        "surname": "Shang",
        "given_name": "Yi"
      }
    ]
  },
  {
    "title": "A Gaussian process regression approach to predict the k-barrier coverage probability for intrusion detection in wireless sensor networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114603",
    "abstract": "Sensors in a Wireless Sensor Network (WSN) sense, process, and transmit information simultaneously. They mainly find applications in agriculture monitoring, environment monitoring, smart city development and defence. These applications demand high-end performance from the WSN. However, the performance of a WSN is highly vulnerable to various types of security threats. Any intrusion may reduce the performance of the WSN and result in fatal problems. Hence, fast intrusion detection and prevention is of great use. This paper aims towards fast detection and prevention of any intrusion using a machine learning approach based on Gaussian Process Regression (GPR) model. We have proposed three methods (S-GPR, C-GPR and GPR) based on feature scaling for accurate prediction of k-barrier coverage probability. We have selected the number of nodes, sensing range, Sensor to Intruder Velocity Ratio (SIVR), Mobile to Static Node Ratio (MSNR), angle of the intrusion path, and required k as the potential features. These features are extracted using an analytical approach. Simulation results demonstrate that the proposed method III accurately predicts the k-barrier coverage probability and outperforms the other two methods (I and II) with a correlation coefficient (R = 0.85) and Root Mean Square Error (RMSE = 0.095). Further, the proposed methods achieve a higher accuracy as compared to other benchmark schemes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000440",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer network",
      "Computer science",
      "Data mining",
      "Engineering",
      "Gaussian",
      "Gaussian process",
      "Geodesy",
      "Geography",
      "Ground-penetrating radar",
      "Intrusion detection system",
      "Kriging",
      "Machine learning",
      "Mathematics",
      "Mean squared error",
      "Node (physics)",
      "Operating system",
      "Path (computing)",
      "Physics",
      "Process (computing)",
      "Quantum mechanics",
      "Radar",
      "Real-time computing",
      "Statistics",
      "Structural engineering",
      "Telecommunications",
      "Wireless sensor network"
    ],
    "authors": [
      {
        "surname": "Singh",
        "given_name": "Abhilash"
      },
      {
        "surname": "Nagar",
        "given_name": "Jaiprakash"
      },
      {
        "surname": "Sharma",
        "given_name": "Sandeep"
      },
      {
        "surname": "Kotiyal",
        "given_name": "Vaibhav"
      }
    ]
  },
  {
    "title": "Multi-dimensional trust for context-aware services computing",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114592",
    "abstract": "The paper addresses the problem of trust management of cloud, fog and IoT services in dynamically changing environments. The continuous dynamic environment is one of the challenges that trustworthy services management in the cloud, fog and IoT settings faces. Services in such an environmental context have difficulty securing an acceptable quality of service (QoS). This article proposes a trust management framework that establishes service trust by considering the direct trust from the truster (subjective trust), aggregating referrals about the service in a collusion-resistant manner (objective trust), and bootstrapping new services. We introduce a subjective trust model based on the formalism of dependency networks to dynamically predict the provided QoS in response to context environment changes. The proposed approach leverages the dependency relations that exist among the QoS metrics and environmental context variables. The novelty at the subjective trust level lies in considering the dynamic cyclic dependency relations that enhances the prediction accuracy. However, subjective trust based on direct interactions could be insufficient to make the trust estimate credible. Hence, on top of the subjective layer, we propose an objective trust management model resilient to collusion attacks by leveraging the power of mass collaboration among referees. Finally, we propose a bootstrapping mechanism that is resilient to the white-washing attacks by observing the behaviours of newcomer services with no trust resources using the concept of social adoption to estimate their initial trust values. Experiments conducted on real-life and synthetic datasets demonstrate the effectiveness of our approach compared with state-of-the-art approaches. We used the statistical log score to assess the model’s prediction accuracy and employed the estimation error of the objective trust as indicated by referees relative to the one calculated by the multi-round simulation. The ROC (Receiver Operating Characteristic) curves are finally used to measure the accuracy of the classifier used in the proposed trust bootstrapping mechanism in providing accurate initial trust values. The main findings of the paper are around the new trust model of cloud, fog and IoT services considering their dynamically changing environments. The first finding is that the prediction of the provided QoS shows better results when it is dynamic and responds to context environment changes by leveraging the dynamic dependency network linking the QoS metrics and context variables of the environment. The second finding is that the objective trust performs better when it is resilient to collusion attacks by leveraging the power of mass collaboration among referees. The third finding is that the bootstrapping mechanism that observes the behaviours of new comer services with no trust resources using the concept of social adoption to estimate their initial trust values excels by being resilient to white watching attacks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000336",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Business",
      "Cloud computing",
      "Collusion",
      "Computational trust",
      "Computer network",
      "Computer science",
      "Computer security",
      "Context (archaeology)",
      "Dependency (UML)",
      "Industrial organization",
      "Operating system",
      "Paleontology",
      "Quality of service",
      "Reputation",
      "Service-level agreement",
      "Social science",
      "Sociology",
      "Trust anchor",
      "Trust management (information system)",
      "Web of trust"
    ],
    "authors": [
      {
        "surname": "Mousa",
        "given_name": "Afaf"
      },
      {
        "surname": "Bentahar",
        "given_name": "Jamal"
      },
      {
        "surname": "Alam",
        "given_name": "Omar"
      }
    ]
  },
  {
    "title": "Detection of epileptic seizures from compressively sensed EEG signals for wireless body area networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114630",
    "abstract": "Wireless body area networks (WBANs) are gaining popularity for tele-monitoring of biomedical signals such as the electroencephalogram (EEG), with diagnosing and monitoring of epileptic seizures being one of the most important applications. Most seizure-detection algorithms cannot be applied directly to the compressed data in WBANs as they require full reconstruction of original EEG signals. In this study, we propose a novel feature for real-time automatic single-channel seizure detection which does not require complete reconstruction of original EEGs. The feature is based on iteratively applying the orthogonal matching pursuit (OMP) algorithm on the compressed EEG data and computing the rate by which the energies of partially reconstructed signals are increased. The feature, i.e. partial energy difference (PED), is then used for classifying seizure and non-seizure states. We also extend this method to the case for multichannel EEG. In multichannel case, the simultaneous OMP (SOMP) with a low number of iterations (1 and 15 iteration) is applied to the compressed data and the difference between the Frobenius norms of partially reconstructed signals is used as a multivariate feature for the classification of seizure and non-seizure states. The proposed features are used to detect seizure intervals in the EEG database provided by the University of Bonn and the CHB-MIT database. The results show that the proposed features can classify seizure epochs from non-seizures even for compression ratios as small as 0.05. The results also show that the proposed single-channel method achieves improvement of up to 4% for the area under the curve (AUC) with significantly less execution time compared to the benchmark matrix determinant (MD) classification approach. The results of applying the proposed multivariate feature to seizure and non-seizure segments of length 5 s from the CHB-MIT database show that it achieves mean AUC values of 0.941, 0.941, and 0.939 with mean execution times of 9.5, 10.7, and 13.1 s for compression ratios of 0.05, 0.1, and 0.2, respectively. Applying a threshold to the PED in a leave-one-out cross-validation (LOO-CV) scenario generates a sensitivity of 0.873,specificity of 0.710, and accuracy of 0.791 at the compression ratio of 0.05. The proposed single- and multichannel features have the potential for deployment in WBANs for the real-time tele-monitoring of epilepsy patients in healthcare applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000713",
    "keywords": [
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Compressed sensing",
      "Computer science",
      "Electroencephalography",
      "Epilepsy",
      "Epileptic seizure",
      "Feature (linguistics)",
      "Feature extraction",
      "Linguistics",
      "Matching pursuit",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Psychiatry",
      "Psychology",
      "Telecommunications",
      "Wireless"
    ],
    "authors": [
      {
        "surname": "Aghababaei",
        "given_name": "Mohammad H."
      },
      {
        "surname": "Azemi",
        "given_name": "Ghasem"
      },
      {
        "surname": "O'Toole",
        "given_name": "John M."
      }
    ]
  },
  {
    "title": "Entomopathogenic nematode dispensing robot: NEMABOT",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114661",
    "abstract": "Entomopathogenic nematodes (EPN) are obligate endoparasites of many insect species and they are important biocontrol agents. Application strategies that improve precision and reduce labor would increase their potential in many cropping systems. We developed a unique robotic system to apply EPNs to a surface area precisely. The robotic system picks up EPNs from a suspension in a reservoir with a peristaltic pump and transfers them to an exact point with an exact amount. Four suspensions were prepared with four concentrations of EPNs; 0.1, 0.2, 0.4 and 0.8 g of commercial EPN product per 2 L of water. All suspensions were applied in three different amounts of water (25, 50 and 100 mL per application). In total, 12 different applications were conducted with the robot. Conical falcon centrifuge tubes were used to collect applied EPNs. Five samples (10 µl) were taken from collected 25, 50 and 100 mL EPN suspensions and the average nematode number in the samples were scaled to the whole suspension. Results of the experiments showed that all robot applications, except 25 mL–0.1 g dose, were not significantly different from those of the control treatment, application with a pipette.. Thus, the robotic system has been found to make consistent applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001020",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Environmental science",
      "Homotopy",
      "Mathematics",
      "Physical chemistry",
      "Pipette",
      "Pure mathematics",
      "Robot",
      "Suspension (topology)"
    ],
    "authors": [
      {
        "surname": "Erdoğan",
        "given_name": "Hilal"
      },
      {
        "surname": "Ünal",
        "given_name": "Halil"
      },
      {
        "surname": "Lewis",
        "given_name": "Edwin E."
      }
    ]
  },
  {
    "title": "Applications of new hybrid algorithm based on advanced cuckoo search and adaptive Gaussian quantum behaved particle swarm optimization in solving ordinary differential equations",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114646",
    "abstract": "This article solves first and second order differential equations with initial and/or boundary conditions by transforming these equations into unconstrained/bound constrained optimization problems. In order to solve these problems, a hybrid algorithm based on advanced cuckoo search (CS) algorithm and adaptive Gaussian quantum behaved particle swarm optimization (AGQPSO) is proposed. The CS algorithm is modified first by changing the step size in the simplified version. After that half of the total population is upgraded by this modified CS algorithm and another half is upgraded by AGQPSO algorithm. Then deletion strategy of CS algorithm is applied on the whole updated population. Next, to test the performance of the proposed hybrid algorithm, a number of benchmarks bound constrained optimization problems with different dimensions are considered and solved. Then this algorithm is applied fruitfully in first and second order initial value problems and boundary value problems by expressing the said problems in the form of bound constrained optimization problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000877",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Constraint logic programming",
      "Constraint programming",
      "Cuckoo search",
      "Demography",
      "Differential equation",
      "Differential evolution",
      "Gaussian",
      "Hybrid algorithm (constraint satisfaction)",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Meta-optimization",
      "Metaheuristic",
      "Multi-swarm optimization",
      "Ordinary differential equation",
      "Particle swarm optimization",
      "Physics",
      "Population",
      "Quantum mechanics",
      "Sociology",
      "Stochastic programming",
      "Upper and lower bounds"
    ],
    "authors": [
      {
        "surname": "Kumar",
        "given_name": "Nirmal"
      },
      {
        "surname": "Shaikh",
        "given_name": "Ali Akbar"
      },
      {
        "surname": "Mahato",
        "given_name": "Sanat Kumar"
      },
      {
        "surname": "Bhunia",
        "given_name": "Asoke Kumar"
      }
    ]
  },
  {
    "title": "Two-stage approach to feature set optimization for unsupervised dataset with heterogeneous attributes",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114563",
    "abstract": "Unsupervised feature selection (UFS) is utilized in various application domains, such as data mining, pattern recognition, machine learning, etc. UFS follows three basic approaches, namely filter, wrapper, and hybrid (that is, a combination of both filter and wrapper) to select the relevant and non-redundant features. It has been observed that a filter method does not guarantee an optimal solution. However, a wrapper approach is computationally expensive. The hybrid method are known to give a better trade-off between filter and wrapper strategies. But, the practical applicability of schemes mentioned above are preferably restricted only to a numerical dataset and are not so suitable for a mixed dataset. Therefore, there is a need for a UFS scheme which can handle both the numerical and non-numerical features directly. In this paper, a robust and efficient two-phase (i.e., feature ranking (FR) and feature selection (FS)) UFS method is proposed. The proposed FR utilizes entropy and mutual information to produce maximum informative and non-redundant ranked features from a high-dimensional mixed dataset. Further, the proposed FS follows k-prototype clustering algorithm with improved Callinski-Harasbaz criteria-based selection methodology to choose optimal features. Experiments on real-life dataset substantiate that the proposed approach provides a better subset of features compared to the existing state of the art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100004X",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Linguistics",
      "Machine learning",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Set (abstract data type)",
      "Stage (stratigraphy)"
    ],
    "authors": [
      {
        "surname": "Chaudhuri",
        "given_name": "Arpita"
      },
      {
        "surname": "Samanta",
        "given_name": "Debasis"
      },
      {
        "surname": "Sarma",
        "given_name": "Monalisa"
      }
    ]
  },
  {
    "title": "Dynamics of customer segments: A predictor of customer lifetime value",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114606",
    "abstract": "Most studies in the literature have focused on past behavior of customers to measure customer lifetime value, however, the rapid developments of technology and products make new conditions that cannot be predicted by past records anymore. In the era of new media and social networks, customers’ needs and expectations change fast which lead to instability of customer lifetime value. In the present study, we studied the dynamics of bank customers through value segments using big data analytics. By mining patterns of associations between customer transitions, we found six major categories, including the pattern of Local Leaders whose transitions are repeated by some follower groups within next two periods. Such results suggest that the dynamics of customer segments can be considered as a predictor of customer lifetime value. This approach uses the current dynamics of customers to predict CLV and therefore, unlike to the conventional method, accommodates to changing market conditions. We also found a pattern by a few groups of Market Trend Initiators whose transitions are followed by overall market trends which gains insight into the dynamics of future markets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000476",
    "keywords": [
      "Acoustics",
      "Analytics",
      "Business",
      "Computer science",
      "Customer equity",
      "Customer intelligence",
      "Customer lifetime value",
      "Customer profitability",
      "Customer retention",
      "Customer to customer",
      "Customer value",
      "Data science",
      "Dynamics (music)",
      "Econometrics",
      "Economics",
      "Machine learning",
      "Market segmentation",
      "Marketing",
      "Microeconomics",
      "Physics",
      "Profit (economics)",
      "Service (business)",
      "Service quality",
      "Value (mathematics)"
    ],
    "authors": [
      {
        "surname": "Mosaddegh",
        "given_name": "Abdolreza"
      },
      {
        "surname": "Albadvi",
        "given_name": "Amir"
      },
      {
        "surname": "Sepehri",
        "given_name": "Mohammad Mehdi"
      },
      {
        "surname": "Teimourpour",
        "given_name": "Babak"
      }
    ]
  },
  {
    "title": "Designing dispatching rules with genetic programming for the unrelated machines environment with constraints",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114548",
    "abstract": "Scheduling problems constitute an important part in many everyday systems, where a variety of constraints have to be met to ensure the feasibility of schedules. These problems are often dynamic, meaning that changes occur during the execution of the system. In such cases, the methods of choice are dispatching rules (DRs), simple methods that construct the schedule by determining the next decision which needs to be performed. Designing DRs for every possible problem variant is unfeasible. Therefore, the attention has shifted towards automatic generation of DRs using different methods, most notably genetic programming (GP), which demonstrated its superiority over manually designed rules. Since many real world applications of scheduling problems include various constraints, it is required to create high quality DRs even when different constraints are considered. However, most studies focused on problems without additional constraints or only considered them briefly. The goal of this study is to examine the potential of GP to construct DRs for problems with constraints. This is achieved primarily by adapting the schedule generation scheme used in automatically designed DRs. Also, to provide GP with a better overview of the problem, a set of supplementary terminal nodes is proposed. The results show that automatically generated DRs obtain better performance than several manually designed DRs adapted for problems with constraints. Using additional terminals resulted in the construction of better DRs for some constraints, which shows that their usefulness depends on the considered constraint type. Therefore, automatically generating DRs for problems with constraints presents a better alternative than adapting existing manually designed DRs. This finding is important as it shows the capability of GP to construct high quality DRs for more complicated problems, which is useful for real world situations where a number of constraints can be present.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311921",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Constraint programming",
      "Construct (python library)",
      "Engineering",
      "Genetic programming",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Operations research",
      "Programming language",
      "Schedule",
      "Scheduling (production processes)",
      "Set (abstract data type)",
      "Stochastic programming"
    ],
    "authors": [
      {
        "surname": "Jaklinović",
        "given_name": "Kristijan"
      },
      {
        "surname": "Ðurasević",
        "given_name": "Marko"
      },
      {
        "surname": "Jakobović",
        "given_name": "Domagoj"
      }
    ]
  },
  {
    "title": "Detection of spoofing attacks for ear biometrics through image quality assessment and deep learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114600",
    "abstract": "Ear recognition systems are one of the popular person identification systems. These biometric systems need to be protected against attackers. In this paper, a novel method is proposed to detect spoof attacks within ear recognition systems. The proposed method employs Convolutional Neural Network (CNN) which is based on deep learning and Image Quality Measure (IQM) techniques to detect printed photo attacks against ear recognition systems. Full-reference and no-reference image quality measures are used to extract ear image features. Score-level fusion is used to combine the scores obtained from image quality measures. Finally, decision-level fusion is employed to fuse the decisions obtained from CNN and IQM systems. The final decision is obtained as real or fake image as the output of the whole system. The experiments are conducted on publicly available ear datasets namely, AMI, UBEAR, IITD, USTB set 1 and USTB set 2 and the obtained results are compared with the state-of-the-art methods that are focused on printed photo attacks as well.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000415",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Convolutional neural network",
      "Decision tree",
      "Deep learning",
      "Electrical engineering",
      "Engineering",
      "Fuse (electrical)",
      "Image (mathematics)",
      "Image quality",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Speech recognition",
      "Spoofing attack"
    ],
    "authors": [
      {
        "surname": "Toprak",
        "given_name": "İ."
      },
      {
        "surname": "Toygar",
        "given_name": "Ö."
      }
    ]
  },
  {
    "title": "A classification of countries and regions by degree of the spread of coronavirus based on statistical criteria",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114654",
    "abstract": "This paper presents models of the spread of SARS-CoV-2 coronavirus in individual countries and globally in 2020 based on the statistical characteristics of the spread in the given countries or regions (in particular, in Hubei province). Through modeling, we attempt to achieve a goal which is of vital interest to societies in a pandemic catastrophe, and to answer the question of what stage of spread the epidemic has reached in a given country. The country classifier we developed is based on the relative variability indicator of the confirmed cases variable. This classification indicator is compared with a set of data-driven thresholds, the crossing of which determines the degree of spread of the epidemic in a given country. The article was written between April 2020, when the pandemic had been suppressed in China and was raging in Europe and the USA, and August 2020, as a new wave of local resumed outbreaks appeared in many countries. We contend that the spread phases are predictable based on statistical similarity. There are four phases of epidemic spread: growth, duration, suppression and re-outbreak. The authors’ Matlab software, which allows simulations of the spread of coronavirus in any country based on data published by CSSE, is available in the public GitHub repository.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000956",
    "keywords": [
      "Acoustics",
      "Archaeology",
      "China",
      "Computer science",
      "Coronavirus",
      "Coronavirus disease 2019 (COVID-19)",
      "Degree (music)",
      "Demography",
      "Disease",
      "Econometrics",
      "Epidemic model",
      "Geography",
      "Infectious disease (medical specialty)",
      "Mathematics",
      "Medicine",
      "Outbreak",
      "Pandemic",
      "Pathology",
      "Physics",
      "Population",
      "Regional science",
      "Sociology",
      "Statistics",
      "Virology"
    ],
    "authors": [
      {
        "surname": "Wilinski",
        "given_name": "Antoni"
      },
      {
        "surname": "Szwarc",
        "given_name": "Eryk"
      }
    ]
  },
  {
    "title": "Experiment-based affect heuristic using fuzzy rules and Taguchi statistical method for tuning complex systems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114638",
    "abstract": "Affect heuristic is a subjective feeling that enables human's quick and efficient decision-making in unstructured and uncertain environments. It is an experience-based subconscious process in which information is framed for quick interpretation. Here, we propose a hybrid Experiment-based Decision-making architecture based on Affect Heuristics (EDAH) that uses possibility and probability theories in combination utilizing fuzzy logic's ability to address deterministic uncertainty and graduality of truth, along with Taguchi method's ability to exploit the statistical data as a robust and suboptimal experiment design methodology. Accordingly, the proposed paradigm aims to make the best decision from a set of feasible decisions using fewer experiments by inserting reliable subjective information by the expert in the random samples of experiments. The proposed method is mathematically defined and then applied to the problem of Targeted Drug Delivery (TDD) for cancer treatment in the two states of with and without noise. Furthermore, we study the effect of three membership functions, consisting of triangular, trapezoidal, and Gaussian forms on our decisions' selection for making an improved medical decision and avoiding an unprofitable one by recognizing existing opportunities. Results indicate considerable improvement of normoxic and hypoxic cell densities when compared with the competing approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000798",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Data mining",
      "Exploit",
      "Fuzzy logic",
      "Heuristic",
      "Heuristics",
      "Machine learning",
      "Operating system",
      "Taguchi methods"
    ],
    "authors": [
      {
        "surname": "Rady Raz",
        "given_name": "Nasibeh"
      },
      {
        "surname": "Akbarzadeh-T.",
        "given_name": "Mohammad-R."
      },
      {
        "surname": "Akbarzadeh",
        "given_name": "Alireza"
      }
    ]
  },
  {
    "title": "An improved elephant herding optimization using sine–cosine mechanism and opposition based learning for global optimization problems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114607",
    "abstract": "An improved elephant herding optimization (EHOI) is proposed for continuous function optimization, financial stress prediction problem and two engineering optimization problems in this work. Elephant Herding Optimization (EHO) is a swarm-based algorithm and was inspired by the social behaviour of elephant clans. In the literature, EHO has received great attention from researchers due to its global optimization capability and ease of implementation. However, it has few limitations like random replacing of worst individual and lack of exploitation, which leads to slow convergence. In this work, EHO was enhanced with the help of the position updating mechanism of sine–cosine algorithm (SCA) and opposition-based learning (OBL). The separating operator in original EHO was replaced by the sine–cosine mechanism and followed by opposition-based learning was introduced to increase the performance of EHO. The proposed EHOI was compared with eight well-known meta-heuristic optimization algorithms (MAs) by using 23 classical benchmark functions, 10 modern CEC2019 benchmark test functions and two engineering optimization problems. From the results, it was observed that the proposed EHOI outperformed most of the selected MAs in terms of solution quality. A kernel extreme learning machine (KELM) model was optimized by improved EHO and applied to handle financial stress prediction. The efficiency of the proposed EHOI_KELM model was tested on two popular financial datasets and compared with popular classifiers, EHO_KELM and SCA_KELM models. The results demonstrate that the proposed EHOI_KELM model shows excellent performance than the popular classifiers, EHO_KELM & SCA_KELM models and it can also serve as an effective tool for financial prediction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000488",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Continuous optimization",
      "Forestry",
      "Geodesy",
      "Geography",
      "Geometry",
      "Global optimization",
      "Herding",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Multi-swarm optimization",
      "Optimization problem",
      "Random optimization",
      "Sine",
      "Trigonometric functions"
    ],
    "authors": [
      {
        "surname": "Muthusamy",
        "given_name": "Hariharan"
      },
      {
        "surname": "Ravindran",
        "given_name": "Sindhu"
      },
      {
        "surname": "Yaacob",
        "given_name": "Sazali"
      },
      {
        "surname": "Polat",
        "given_name": "Kemal"
      }
    ]
  },
  {
    "title": "Color image segmentation using Kapur, Otsu and Minimum Cross Entropy functions based on Exchange Market Algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114636",
    "abstract": "Color image segmentation is the primary step to elicit detailed information of the image with RGB color space. The key to acquire the region of interest is the straightforward, instinctual technique called thresholding. Bi-level thresholding can be assessed concretely for simple images, whereas revealing various classes from a complex image is achieved only through multiple threshold values. Hence, multilevel thresholding with the most propitious maximizing objective functions such as Kapur, Otsu and Minimum Cross Entropy (MCE) to extract the precise threshold are employed in this paper. As the segmentation level increases, execution of non-parametric objective functions leads to exponential increase in computational time. Several researches to enhance the speed of objective functions along with various metaheuristics such as Krill Herd Algorithm (KHA), Teaching-Learning Based Optimization (TLBO) and Cuckoo Search Algorithm (CSA) are performed. The need to unlock the computational complexity drifted our view towards the most powerful, robust, recently introduced metaheuristic Exchange Market Algorithm (EMA). EMA involves exchange of shares among investors to gain profit. The strategic approach of share members in stable and unstable modes of the market results in precise exploration and exploitation. Furthermore, low fitness shareholders gaining experience from their high and moderate counterparts, stands out as an exceptional step to reduce the time consumption. This concept is utilized in our paper for the first time to obtain the exact details from the image without any delay. Empirical outcome of the results indicates that exceptional image segmentation is achieved by EMA in less time compared to extensive search techniques such as KHA, TLBO and CSA. Quantitative and qualitative validation furnished by metrics such as Structural Similarity Index (SSIM), computational time, Peak Signal to Noise Ratio (PSNR) and statistical Wilcoxon test affirm that Otsu, Kapur and MCE based EMA outperforms the existing techniques to analyze the real-world images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000774",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Image segmentation",
      "Market segmentation",
      "Marketing",
      "Otsu's method",
      "Pattern recognition (psychology)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Sathya",
        "given_name": "P.D."
      },
      {
        "surname": "Kalyani",
        "given_name": "R."
      },
      {
        "surname": "Sakthivel",
        "given_name": "V.P."
      }
    ]
  },
  {
    "title": "NeuroEvolution of augmenting topologies for solving a two-stage hybrid flow shop scheduling problem: A comparison of different solution strategies",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114666",
    "abstract": "The article investigates the application of NeuroEvolution of Augmenting Topologies (NEAT) to generate and parameterize artificial neural networks (ANN) on determining allocation and sequencing decisions in a two-stage hybrid flow shop scheduling environment with family setup times. NEAT is a machine-learning and neural architecture search algorithm, which generates both, the structure and the hyper-parameters of an ANN. Our experiments show that NEAT can compete with state-of-the-art approaches in terms of solution quality and outperforms them regarding computational efficiency. The main contributions of this article are: (i) A comparison of five different strategies, evaluated with 14 different experiments, on how ANNs can be applied for solving allocation and sequencing problems in a hybrid flow shop environment, (ii) a comparison of the best identified NEAT strategy with traditional heuristic and metaheuristic approaches concerning solution quality and computational efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100107X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Evolutionary algorithm",
      "Flow shop scheduling",
      "Heuristic",
      "Job shop scheduling",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Network topology",
      "Neuroevolution",
      "Operating system",
      "Schedule",
      "Scheduling (production processes)"
    ],
    "authors": [
      {
        "surname": "Lang",
        "given_name": "Sebastian"
      },
      {
        "surname": "Reggelin",
        "given_name": "Tobias"
      },
      {
        "surname": "Schmidt",
        "given_name": "Johann"
      },
      {
        "surname": "Müller",
        "given_name": "Marcel"
      },
      {
        "surname": "Nahhas",
        "given_name": "Abdulrahman"
      }
    ]
  },
  {
    "title": "Fast and efficient discovery of key bike stations in bike sharing systems big datasets",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114659",
    "abstract": "Bike Sharing Systems (BSS) became one of the popular transportation systems due to their environmental friendly, mobility endorsing, and outdoor activity nature. City residents tend to use particular bike stations more frequently and prevalent than other stations for different reasons, such as popularity and centrality of the locations. Discovering key bike stations is the exploration of frequent and mostly utilized bike stations which are highly preferred by BSS users in terms of spatial and temporal activities. Discovering key stations is important for optimal planning for BSS, bike repositioning methods, and urban land use applications. However, discovering key stations is challenging due to variability of bike user preferences, effect of weather conditions, and big data nature of BSS datasets. In this study, two interest measures are proposed to discover key bike stations using BSS big datasets. Proposed interest measures reveal frequency and prevalence of stations in terms of daily and dataset-wide usage of BSS users. Two algorithms are proposed to discover key stations using proposed interest measures. One of the proposed algorithms could better handle BSS big datasets within less execution time and more efficient memory usage by dividing and processing each section of dataset separately. The proposed algorithms are experimentally evaluated using Chicago Divvy Bikes dataset. The results show that the proposed algorithms are effective on discovering key stations among BSS big datasets, which could be beneficial for making decisions about city resident mobility behaviors in terms of bike user activities and for extracting knowledge about bike user preferences.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421001007",
    "keywords": [
      "Big data",
      "Bike sharing",
      "Centrality",
      "Combinatorics",
      "Computer science",
      "Computer security",
      "Data mining",
      "Data science",
      "Engineering",
      "Key (lock)",
      "Mathematics",
      "Popularity",
      "Psychology",
      "Social psychology",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Dokuz",
        "given_name": "Ahmet Sakir"
      }
    ]
  },
  {
    "title": "Sense the pen: Classification of online handwritten sequences (text, mathematical expression, plot/graph)",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114588",
    "abstract": "This paper has a threefold contribution. First, it presents a novel online handwriting database captured using a digital/sensor pen (Apple pencil) and digital/sensor screen (iPad). The captured data are continuous streams of multi-dimensional points, analyzed and processed to classify handwritten sequences into plain text, mathematical expressions, and plots/graphs. Second, a new feature set for online handwritten sequence classification is proposed. The said feature set is used to establish a benchmark for the proposed dataset using various machine-learning classifiers. Third, an ablation study is performed to look into the performance of the proposed feature set compared to the existing feature sets. Here, the proposed feature set has outperformed all the existing feature sets in various evaluation metrics. Furthermore, the proposed dataset and the feature set have been made publicly available along with the benchmark evaluation to enable further research in the field.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000294",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Feature extraction",
      "Geodesy",
      "Geography",
      "Graph",
      "Handwriting",
      "Handwriting recognition",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Plot (graphics)",
      "Programming language",
      "Set (abstract data type)",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Younas",
        "given_name": "Junaid"
      },
      {
        "surname": "Malik",
        "given_name": "Muhammad Imran"
      },
      {
        "surname": "Ahmed",
        "given_name": "Sheraz"
      },
      {
        "surname": "Shafait",
        "given_name": "Faisal"
      },
      {
        "surname": "Lukowicz",
        "given_name": "Paul"
      }
    ]
  },
  {
    "title": "Unsupervised neural networks for automatic Arabic text summarization using document clustering and topic modeling",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114652",
    "abstract": "Humans must easily handle the vast amounts of data being generated by the revolution of information technology. Thus, Automatic Text summarization has been applied to various domains in order to find the most relevant information and make critical decisions quickly. In the context of Arabic, text summarization techniques suffer from several problems. First, most existing methods do not consider the context or domain to which the document belongs. Second, the majority of the existing approaches are based on the traditional bag-of-words representation, which involves high dimensional and sparse data, and makes it difficult to capture relevant information. Third, research in Arabic Text summarization is fairly small and only recently compared to that on Anglo-Saxon and other languages due to the shortage of Arabic corpora, resources, and automatic processing tools. In this paper, we try to overcome these limitations by proposing a new approach using documents clustering, topic modeling, and unsupervised neural networks in order to build an efficient document representation model. First, a new document clustering technique using Extreme learning machine is performed on large text collection. Second, topic modeling is applied to documents collection in order to identify topics present in each cluster. Third, each document is represented in a topic space by a matrix where rows represent the document sentences and columns represent the cluster topics. The generated matrix is then trained using several unsupervised neural networks and ensemble learning algorithms in order to build an abstract representation of the document in the concept space. Important sentences are ranked and extracted according to a graph model with a redundancy elimination component. The proposed approach is evaluated on Essex Arabic Summaries Corpus and compared against other Arabic text summarization approaches using ROUGE measure. Experimental results showed that the models trained on topic representation learn better representations and improve significantly the summarization performance. In particular, ensemble learning models demonstrated an important improvement on Rouge recall and promising results on F-measure.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000932",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Automatic summarization",
      "Biology",
      "Cluster analysis",
      "Computer science",
      "Context (archaeology)",
      "Document clustering",
      "Information retrieval",
      "Law",
      "Multi-document summarization",
      "Natural language processing",
      "Paleontology",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Text processing"
    ],
    "authors": [
      {
        "surname": "Alami",
        "given_name": "Nabil"
      },
      {
        "surname": "Meknassi",
        "given_name": "Mohammed"
      },
      {
        "surname": "En-nahnahi",
        "given_name": "Noureddine"
      },
      {
        "surname": "El Adlouni",
        "given_name": "Yassine"
      },
      {
        "surname": "Ammor",
        "given_name": "Ouafae"
      }
    ]
  },
  {
    "title": "Machine learning based methods for software fault prediction: A survey",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114595",
    "abstract": "Several prediction approaches are contained in the arena of software engineering such as prediction of effort, security, quality, fault, cost, and re-usability. All these prediction approaches are still in the rudimentary phase. Experiments and research are conducting to build a robust model. Software Fault Prediction (SFP) is the process to develop the model which can be utilized by software practitioners to detect faulty classes/module before the testing phase. Prediction of defective modules before the testing phase will help the software development team leader to allocate resources more optimally and it reduces the testing effort. In this article, we present a Systematic Literature Review (SLR) of various studies from 1990 to June 2019 towards applying machine learning and statistical method over software fault prediction. We have cited 208 research articles, in which we studied 154 relevant articles. We investigated the competence of machine learning in existing datasets and research projects. To the best of our knowledge, the existing SLR considered only a few parameters over SFP’s performance, and they partially examined the various threats and challenges of SFP techniques. In this article, we aggregated those parameters and analyzed them accordingly, and we also illustrate the different challenges in the SFP domain. We also compared the performance between machine learning and statistical techniques based on SFP models. Our empirical study and analysis demonstrate that the prediction ability of machine learning techniques for classifying class/module as fault/non-fault prone is better than classical statistical models. The performance of machine learning-based SFP methods over fault susceptibility is better than conventional statistical purposes. The empirical evidence of our survey reports that the machine learning techniques have the capability, which can be used to identify fault proneness, and able to form well-generalized result. We have also investigated a few challenges in fault prediction discipline, i.e., quality of data, over-fitting of models, and class imbalance problem. We have also summarized 154 articles in a tabular form for quick identification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000361",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Human–computer interaction",
      "Machine learning",
      "Programming language",
      "Software",
      "Software development",
      "Software development process",
      "Software quality",
      "Usability"
    ],
    "authors": [
      {
        "surname": "Pandey",
        "given_name": "Sushant Kumar"
      },
      {
        "surname": "Mishra",
        "given_name": "Ravi Bhushan"
      },
      {
        "surname": "Tripathi",
        "given_name": "Anil Kumar"
      }
    ]
  },
  {
    "title": "A weighted fuzzy process neural network model and its application in mixed-process signal classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114642",
    "abstract": "A weighted fuzzy process neural network (WFPNN) model, and its corresponding classification algorithm, are proposed for quantitative and qualitative mixed-process signal classification problems. Based on predicate logic, fuzzy process neurons (FPN) are expressed as weighted fuzzy process inference rules on the semantic. Among them, the premise and conclusion are fuzzy predicates defined on a fuzzy set containing process information. Considering the applicability of rules to practical problems, confidence factor and applicable threshold are introduced to make the rules based on FPN have adaptive application ability. Based on FPN, the WFPNN was constructed with a process signal input layer, an FPN hidden layer, and a Takagi-Sugeno fuzzy classifier. It can integrate the learning properties and classification mechanisms of process neural networks (P-NNs) for time-vary signals with the logical inference abilities of fuzzy systems. So then to realize the quantitative and qualitative mixed process signal processing and direct classification. This study proposes a fusion analysis model that combining fuzzy decision theory with the process information processing method of P-NNs. It can automatically deduce the membership function and the fuzzy predicate logic rules from a set of given training examples, quickly build a fuzzy expert system prototype. And it can provide a new method for the construction and application of an expert system based on models or rules. Meanwhile, due to the adaptive learning property of WFPNN and the representation mechanism of inference rules based on the FPN, it is easy to realize the extension of the rule bases, model bases and application fields of the expert systems. This paper analyzes the properties of WFPNN, and establishes a corresponding learning algorithm. The discrimination of reservoir water-flooded states based on well-logging curves was used as an example for experimental analysis. The continuous measurement values of five physical quantities, such as resistivity, acoustic and radioactivity levels in the oil layer, were taken as input. The output was the discriminating result for four types of water-flooded states. The accuracy of the proposed method was 82.5%. Our accuracy was more than 7% higher than other existing automatic identification methods. These results demonstrate the viability and effectiveness of the proposed model and algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100083X",
    "keywords": [
      "Adaptive neuro fuzzy inference system",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Defuzzification",
      "Fuzzy classification",
      "Fuzzy control system",
      "Fuzzy logic",
      "Fuzzy number",
      "Fuzzy rule",
      "Fuzzy set",
      "Fuzzy set operations",
      "Machine learning",
      "Membership function",
      "Neuro-fuzzy"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Shaohua"
      },
      {
        "surname": "Feng",
        "given_name": "Naidan"
      },
      {
        "surname": "Liu",
        "given_name": "Kun"
      },
      {
        "surname": "Liang",
        "given_name": "Yongquan"
      },
      {
        "surname": "Liu",
        "given_name": "Xiaoyu"
      }
    ]
  },
  {
    "title": "GA-based approach to optimize an equivalent electric circuit model of a Li-ion battery-pack",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114647",
    "abstract": "This article presents the optimization procedure based on genetics algorithms (GA) to obtain an equivalent electric circuit model (EECM) of a Li-ion battery pack. In the first part, a series of experimental tests in time and frequency domains were carried out. These tests were used to identify the parameters of the EECM under different State-of-Charge (SoC) for a commercial battery-pack. Each EECM consists of a voltage source connected in series with a resistor and a set of k networks composed of a resistor in parallel with a capacitor, where k = 1, 2 o 3 (1RC, 2RC, and 3RC). Subsequently, parametric identification of the EECM was performed using optimization techniques. At this stage, the topology that gives the lowest estimation error was determined, where the options analyzed were to use 1RC, 2RC, or 3RC networks. The objective function consists of minimizing the mean square error between measured and calculated impedances of the different proposed circuit models. GA was used to solve this optimization problem. The minimum error obtained was 1.07% and 1.05% for the EECM with 2RC and 3RC networks, respectively. Finally, these EECMs were implemented in Matlab®/Simulink to validate the Li-ion battery-pack model response for an electric vehicle application. A hardware-in-the-loop (HIL) simulation platform was developed to simulate the performance of an electric vehicle (EV) under different driving cycles. The results show that the GA-based approach allows obtained an EECM of low order to represent the highly dynamic behavior of a Li-ion battery with high accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000889",
    "keywords": [
      "Battery (electricity)",
      "Battery pack",
      "Capacitor",
      "Computer science",
      "Electric vehicle",
      "Electrical engineering",
      "Electrical impedance",
      "Engineering",
      "Equivalent circuit",
      "MATLAB",
      "Mathematics",
      "Mean squared error",
      "Operating system",
      "Parametric statistics",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Resistor",
      "State of charge",
      "Statistics",
      "Topology (electrical circuits)",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Pizarro-Carmona",
        "given_name": "Victor"
      },
      {
        "surname": "Castano-Solís",
        "given_name": "Sandra"
      },
      {
        "surname": "Cortés-Carmona",
        "given_name": "Marcelo"
      },
      {
        "surname": "Fraile-Ardanuy",
        "given_name": "Jesus"
      },
      {
        "surname": "Jimenez-Bermejo",
        "given_name": "David"
      }
    ]
  },
  {
    "title": "A feedback-based prediction strategy for dynamic multi-objective evolutionary optimization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114594",
    "abstract": "Prediction methods are widely used to solve dynamic multi-objective optimization problems (DMOPs). The key to the success of prediction methods lies in the accurate tracking of the new location of the Pareto set (PS) or Pareto front (PF) in a new environment. To improve the prediction accuracy, this paper proposes a novel feedback-based prediction strategy (FPS), which consists of two feedback mechanisms, namely correction feedback (CF) and effectiveness feedback (EF). CF is used to correct an initial prediction model. When the environment changes, CF constructs a representative individual to reflect the characteristics of the current population. The predicted solution of this individual in the new environment is calculated based on the initial prediction model. Afterward, a step size exploration method based on variable classification is introduced to adaptively correct the prediction model. EF is applied to enhance the effectiveness of re-initialization in two stages. In the first stage, half of the individuals in the population are re-initialized based on the corrected prediction model. In the second stage, EF re-initializes the rest of the individuals in the population using two rounds of roulette method based on the re-initialization effectiveness feedback of the first stage. The proposed FPS is incorporated into a dynamic multi-objective optimization evolutionary algorithm (DMOEA) based on decomposition resulting in a new algorithm denoted as MOEA/D-FPS. MOEA/D-FPS is compared with six state-of-the-art DMOEAs on twenty-two different benchmark problems. The experimental results demonstrate the effectiveness and efficacy of MOEA/D-FPS in solving DMOPs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100035X",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer security",
      "Demography",
      "Evolutionary algorithm",
      "Geodesy",
      "Geography",
      "Initialization",
      "Key (lock)",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Multi-objective optimization",
      "Population",
      "Programming language",
      "Set (abstract data type)",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Liang",
        "given_name": "Zhengping"
      },
      {
        "surname": "Zou",
        "given_name": "Ya"
      },
      {
        "surname": "Zheng",
        "given_name": "Shunxiang"
      },
      {
        "surname": "Yang",
        "given_name": "Shengxiang"
      },
      {
        "surname": "Zhu",
        "given_name": "Zexuan"
      }
    ]
  },
  {
    "title": "Ensemble clustering using extended fuzzy k-means for cancer data analysis",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114622",
    "abstract": "Clustering analysis is a significant research topic in discovering cancer using different profiles of gene expression, which is very important to successfully diagnose and treat the cancer decease. Many ensemble clustering methods have been developed to perform clustering using tumor data. Only few of them incorporates a significant number of input clusterings, the optimal number of clusters in each input clustering, and an appropriate ensemble method to combine input clusterings into a final clustering. In this paper, we introduce two new steps in the standard fuzzy k-means algorithm to determine the optimal number of input clusterings, and the optimal number of clusters in each clustering for ensemble clustering. The first one is to incorporate a penalty term for making the algorithm insensitive to the initialization of cluster centroids. The second one is to automate a clustering process for iteratively updating the feature weights. This step addresses the noise values in the dataset. We propose an ensemble clustering method, which combines a set of input clusterings into a final clustering having better overall quality. Experiments on real cancer gene expression profiles illustrate that the proposed algorithm outperformed the well-known clustering algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000634",
    "keywords": [
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Canopy clustering algorithm",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Computer science",
      "Consensus clustering",
      "Correlation clustering",
      "Data mining",
      "Data stream clustering",
      "Determining the number of clusters in a data set",
      "Fuzzy clustering",
      "Initialization",
      "Pattern recognition (psychology)",
      "Programming language",
      "Single-linkage clustering",
      "k-medians clustering"
    ],
    "authors": [
      {
        "surname": "Khan",
        "given_name": "Imran"
      },
      {
        "surname": "Luo",
        "given_name": "Zongwei"
      },
      {
        "surname": "Shaikh",
        "given_name": "Abdul Khalique"
      },
      {
        "surname": "Hedjam",
        "given_name": "Rachid"
      }
    ]
  },
  {
    "title": "An integrated approach of fuzzy risk assessment model and data envelopment analysis for route selection in multimodal transportation networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114342",
    "abstract": "The main objective of this study is to first propose a novel integrated framework of fuzzy risk assessment model (FRAM), data envelopment analysis (DEA), and multiple criteria decision-making (MCDM) approaches for route selection in multimodal transportation networks. In the FRAM phase, the magnitude calculation of risks was operated by decision makers, who can provide their opinions on the probability of occurrence and severity of consequences through linguistic variables and triangular fuzzy numbers for risk likelihood and severity scales. The Mamdani fuzzy rule-based inference system including the rule’s firing strengths is established to convert the membership degrees for each term of aggregated likelihood and severity scales into those for each term of the risk magnitude scale. In the DEA phase, precise and crisp risk magnitudes are characterized by a new defuzzifier based on the DEA algorithm, which is applied instead of classical defuzzification methods. The three decision criteria of transportation cost, transportation time, and overall risk magnitude are weighted by a fuzzy analytic hierarchy process and then investigated based on the consensus of decision makers’ needs by a zero-one goal programming model, which is used to select the most appropriate multimodal route. Finally, the integrated approach is tested to reveal its capability and aptness using a real-world multimodal freight transportation route selection between Thailand and Cambodia. The proposed framework contributes to an easy-to-apply technique for risk predictability in multimodal transportation networks and provides powerful decision-making for the effective selection of the most appropriate alternative under the fuzzy context.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310290",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Data envelopment analysis",
      "Data mining",
      "Fuzzy logic",
      "Fuzzy rule",
      "Fuzzy set",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Multiple-criteria decision analysis",
      "Operations research",
      "Paleontology",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Koohathongsumrit",
        "given_name": "Nitidetch"
      },
      {
        "surname": "Meethom",
        "given_name": "Warapoj"
      }
    ]
  },
  {
    "title": "Deep learning with long short-term memory neural networks combining wavelet transform and principal component analysis for daily urban water demand forecasting",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114571",
    "abstract": "A reliable and accurate urban water demand forecasting plays a significant role in building intelligent water supplying system and smart city. Due to the high frequency noise and complicated relationships in water demand series, forecasting the urban water demand is not an easy task. In order to improve the model’s abilities in handling the complex patterns and catching the peaks in time series, we propose a hybrid long short-term memory model combining with discrete wavelet transform (DWT) and principal component analysis (PCA) pre-processing techniques for water demand forecasting, i.e., DWT-PCA-LSTM. First, the outliers of water demand series are identified and smoothed by 3σ criterion and weighted average method, respectively. Then, the noise component of water demand series is eliminated by DWT method and the principal components (PCs) among influencing factors of water demand are selected by PCA method. In addition, two LSTM networks are built to yield the daily urban water demand predictions using the results of DWT and PCA techniques. At last, the superiorities of the proposed model are demonstrated by comparing with the other benchmark predictive models. The water demand from 2016 to 2020 of a waterworks located in Suzhou, China is used for the experiment. The predictive performance of the experiments are evaluated by the mean absolute percentage error (MAPE), mean absolute percentage errors of peaks (pMAPE), explain variance score (EVS) and correlation coefficient (R). The results show that the DWT-PCA-LSTM model outperforms the other models and has satisfactory performance both in catching the peaks and the average prediction accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000129",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Discrete wavelet transform",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Machine learning",
      "Mean absolute percentage error",
      "Noise (video)",
      "Outlier",
      "Pattern recognition (psychology)",
      "Principal component analysis",
      "Time series",
      "Wavelet",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Du",
        "given_name": "Baigang"
      },
      {
        "surname": "Zhou",
        "given_name": "Qiliang"
      },
      {
        "surname": "Guo",
        "given_name": "Jun"
      },
      {
        "surname": "Guo",
        "given_name": "Shunsheng"
      },
      {
        "surname": "Wang",
        "given_name": "Lei"
      }
    ]
  },
  {
    "title": "Evolutionary community discovery in dynamic social networks via resistance distance",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114536",
    "abstract": "Traditional social community discovery methods concentrate mainly on static social networks, but the analysis of dynamic networks is a prerequisite for real-time and personalized social services. Through the study of community changes, the community structure in a dynamic network can be tracked over time, which helps in the mining of dynamic network information. In this paper, we propose a method of tracking dynamic community evolution that is based on resistance distance. Specifically, we model the time-varying features of dynamic networks using the convergence of a resistance-based distance. In our model, the heterogeneity of neighboring nodes can be obtained in the local topology of nodes by analyzing the resistance distance between nodes. We design a community discovery algorithm that essentially discovers community structures on dynamic networks by identifying the so-called core node. During the process of community evolution analysis, both the dynamic contribution of ordinary nodes and core nodes in each community are considered. In addition, to avoid the inclusion of spurious communities in the community structure, we define the notion of noise community and account for it in our algorithm. Experimental results show that the method proposed in this paper can yield better accuracy than other existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311805",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Community structure",
      "Complex network",
      "Computer network",
      "Computer science",
      "Core (optical fiber)",
      "Data mining",
      "Data science",
      "Distributed computing",
      "Dynamic network analysis",
      "Engineering",
      "Evolving networks",
      "Image (mathematics)",
      "Machine learning",
      "Mathematics",
      "Node (physics)",
      "Noise (video)",
      "Social media",
      "Social network (sociolinguistics)",
      "Spurious relationship",
      "Structural engineering",
      "Telecommunications",
      "Theoretical computer science",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Weimin"
      },
      {
        "surname": "Zhu",
        "given_name": "Heng"
      },
      {
        "surname": "Li",
        "given_name": "Shaohua"
      },
      {
        "surname": "Wang",
        "given_name": "Hao"
      },
      {
        "surname": "Dai",
        "given_name": "Hongning"
      },
      {
        "surname": "Wang",
        "given_name": "Can"
      },
      {
        "surname": "Jin",
        "given_name": "Qun"
      }
    ]
  },
  {
    "title": "Fusion loss and inter-class data augmentation for deep finger vein feature learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114584",
    "abstract": "Finger vein recognition (FVR) based on deep learning (DL) has gained rising attention in recent years. However, the performance of FVR is limited by the insufficient amount of finger vein training data and the weak generalization of learned features. To address these limitations and improve the performance, we propose a simple framework by jointly considering intensive data augmentation, loss function design and network architecture selection. Firstly, we propose a simple inter-class data augmentation technique that can double the number of finger vein training classes with new vein patterns via vertical flipping. Then, we combine it with conventional intra-class data augmentation methods to achieve highly diversified expansion, thereby effectively resolving the data shortage problem. In order to enhance the discrimination of deep features, we design a fusion loss by incorporating the classification loss and the metric learning loss. We find that the fusion of these two penalty signals will lead to a good trade-off between the intra-class similarity and inter-class separability, thereby greatly improving the generalization ability of learned features. We also investigate various network architectures for FVR application in terms of performances and model complexities. To examine the reliability and efficiency of our proposed framework, we implement a real-time FVR system to perform end-to-end verification in a near-realworld working condition. In challenging open-set evaluation protocol, extensive experiments conducted on three public finger vein databases and an in-house database confirm the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000257",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Economics",
      "Generalization",
      "Image (mathematics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Ou",
        "given_name": "Wei-Feng"
      },
      {
        "surname": "Po",
        "given_name": "Lai-Man"
      },
      {
        "surname": "Zhou",
        "given_name": "Chang"
      },
      {
        "surname": "Rehman",
        "given_name": "Yasar Abbas Ur"
      },
      {
        "surname": "Xian",
        "given_name": "Peng-Fei"
      },
      {
        "surname": "Zhang",
        "given_name": "Yu-Jia"
      }
    ]
  },
  {
    "title": "Analysis of sentiment expressions for user-centered design",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114604",
    "abstract": "Devising intelligent systems capable of identifying the idiosyncratic needs of users at scale and translating them into attribute-level design feedback and recommendations is a key prerequisite for successful user-centered design processes. Recent studies show that 49% of design firms lack systems and tools for monitoring external platforms, and only 8% have adopted digital, data-driven approaches for new product development despite acknowledging them as a high priority. The state-of-the-art attribute-level sentiment analysis approaches based on deep learning have achieved promising results; however, these methods pose strict preconditions, require manually labeled data for training and pre-defined attributes by experts, and only classify sentiments intro predefined categories which have limited implications for designers. This article develops a rule-based methodology for extracting and analyzing the sentiment expressions of users on a large scale, from myriad reviews available on social media and e-commerce platforms. The methodology further advances current unsupervised attribute-level sentiment analysis approaches by enabling efficient identification and mapping of sentiment expressions of individual users onto their respective attributes. Experiments on a large dataset scraped from a major e-commerce retail store for apparel and indicate 74.3%–93.8% precision in extracting attribute-level sentiment expressions of users and demonstrate the feasibility and potentials of the developed methodology for large-scale need finding from user reviews.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000452",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Clothing",
      "Computer science",
      "Computer security",
      "Data mining",
      "Data science",
      "Geometry",
      "History",
      "Identification (biology)",
      "Key (lock)",
      "Mathematics",
      "Physics",
      "Product (mathematics)",
      "Quantum mechanics",
      "Scale (ratio)",
      "Sentiment analysis",
      "Social media",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Han",
        "given_name": "Yi"
      },
      {
        "surname": "Moghaddam",
        "given_name": "Mohsen"
      }
    ]
  },
  {
    "title": "Hierarchical traffic signal optimization using reinforcement learning and traffic prediction with long-short term memory",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114580",
    "abstract": "Multi-agent systems can be used for modelling large-scale distributed systems in real world applications. In intelligent transportation system (ITS), many interacting entities influence the performance of the system. As part of ITS, traffic signal control can be modelled using a multi-agent system. In this paper, a hierarchical multi-agent system including two levels is employed to control traffic signals. Each traffic signal is controlled by an agent that sits in the physical level, i.e., in the first level. For the other levels, the traffic network is divided into a number of regions, each controlled by a region controller agent. The first level agents utilize reinforcement learning to find the best policy, while they send their local information to the above level agents. The local information is used to train a long short-term memory (LSTM) neural network for traffic status prediction. The agents in the above level can control the traffic signals by finding the best joint policy using the predicted traffic information. Experimental results show the effectiveness of the proposed method in a traffic network including 16 intersections.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100021X",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Civil engineering",
      "Computer science",
      "Controller (irrigation)",
      "Distributed computing",
      "Engineering",
      "Intelligent transportation system",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Real-time computing",
      "Reinforcement learning",
      "SIGNAL (programming language)",
      "Term (time)",
      "Traffic generation model"
    ],
    "authors": [
      {
        "surname": "Abdoos",
        "given_name": "Monireh"
      },
      {
        "surname": "Bazzan",
        "given_name": "Ana L.C."
      }
    ]
  },
  {
    "title": "An expert system for EMI data classification based on complex Bispectrum representation and deep learning methods",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114568",
    "abstract": "This paper presents expert system framework based on Machine Learning (ML) for High-Voltage (HV) asset condition monitoring. The work investigates the classification of insulation faults in HV environment, based on real-world time series signals labelled by condition monitoring experts. Extending on our previous work, the proposed approach exploits the Bispectrum analysis and deep learning for feature extraction and classification. The calculated Bispectrum on time series signals can be deployed as the complex-valued Bispectrum, which contains phase information, or as its real-valued magnitude. This can be approached as an image classification problem which can be implemented in various deep networks including Convolutional Neural Network (CNN), Residual Neural Network (ResNet) and their complex-valued version. The employed deep networks performance is compared in terms of their classification accuracy. High classification performance is obtained which produces comparable performance with expert diagnosis. Thus, it can be interpreted as transfer of expert system to an intelligent system.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000099",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Bispectrum",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Deep learning",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature learning",
      "Law",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Residual",
      "Spectral density",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Mitiche",
        "given_name": "Imene"
      },
      {
        "surname": "Jenkins",
        "given_name": "Mark D."
      },
      {
        "surname": "Boreham",
        "given_name": "Philip"
      },
      {
        "surname": "Nesbitt",
        "given_name": "Alan"
      },
      {
        "surname": "Morison",
        "given_name": "Gordon"
      }
    ]
  },
  {
    "title": "Two stage deep learning for prognostics using multi-loss encoder and convolutional composite features",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114569",
    "abstract": "Recent advancements in machine learning for remaining useful life (RUL) prediction are driven by the increased availability of data and computing power. Extraction of useful features from raw data leads to better prediction performance. Deep learning based feature generation is superior to labour intensive feature engineering that requires domain expertise. The presence of noise, temporal trends, and irrelevant features in sensor data pose difficulties in training efficient and reliable deep learning models. Proposed Multi-Loss Encoder with Convolutional Composite Features (MLE + CCF), improves feature discovery for deep learning using a two-stage approach, that separates feature generation and RUL prediction. MLE utilizes a multi-loss objective function to train a multi-layer convolutional encoder-decoder network. The relevant information maximizing encoder generates high-dimensional representation characterized by low-noise, low-redundancy, and high-correlation with degradation trend. The second stage implements depthwise separable convolution that learns temporal features from sequential data. The temporal features and encoded representation are concatenated, forming convolutional composite features (CCF). A fully-connected network is trained with CCF for RUL prediction. Experimental results for NASA turbofan engine dataset (C-MAPSS) show performances that are better than benchmark methods. An industrial application demonstrates blade wear prediction in shrink-wrapping equipment using the proposed algorithm. Results and analysis validate the suitability of MLE + CCF for predictive maintenance applications in industries.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000105",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Automotive engineering",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Deep learning",
      "Encoder",
      "Engineering",
      "Feature (linguistics)",
      "Feature engineering",
      "Feature extraction",
      "Feature learning",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Noise (video)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Prognostics",
      "Turbofan"
    ],
    "authors": [
      {
        "surname": "Pillai",
        "given_name": "Shanmugasivam"
      },
      {
        "surname": "Vadakkepat",
        "given_name": "Prahlad"
      }
    ]
  },
  {
    "title": "Ranking influencers of social networks by semantic kernels and sentiment information",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114599",
    "abstract": "Inspired by the importance of social media, a Social Network Opinion Leaders (SNOL) system has been proposed in this paper. The purpose of this system is to identify topic-based opinion leaders of social media. In order to accomplish this goal, several steps have been taken, such as data collection, data processing, data analysis, data classification, ranking of topic-based opinion leaders, and evaluation. The SNOL system has two main parts. In the first part, collected tweets are classified by semantic kernels for topic-based analysis. In the second part, leadership scores are given to each user in the network according to topic modeling and user modeling results. Leadership scores are then calculated with the formula generated and opinion leaders are determined for each category. Experiments are performed on data gathered from Twitter including 17,234,924 tweets from 38,727 users. The evaluation of opinion leader detection is a difficult job since there is no standard method for identifying opinion leaders. Therefore, the evaluation of the results of this study has been done using two different methods, retweet count and spread score, to prove that the suggested methodology outperforms the PageRank algorithm. The results have also been evaluated considering the user-topic sentiment correlation of the retrieved lists. Furthermore, SNOL has been compared against some opinion leader detection methods previously presented in the literature. The experimental results show that SNOL generates remarkably higher performance than the PageRank algorithm and other existing algorithms in the literature for nearly all topics and all selected top N opinion leaders.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000403",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Data mining",
      "Data science",
      "Influencer marketing",
      "Information retrieval",
      "Marketing",
      "Marketing management",
      "Opinion leadership",
      "PageRank",
      "Political science",
      "Public relations",
      "Ranking (information retrieval)",
      "Relationship marketing",
      "Sentiment analysis",
      "Social media",
      "Social network (sociolinguistics)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Altınel Girgin",
        "given_name": "Berna"
      }
    ]
  },
  {
    "title": "Group decision making with hesitant fuzzy linguistic preference relations based on modified extent measurement",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114235",
    "abstract": "The paper presents a consensus model for group decision making (GDM) with hesitant fuzzy linguistic preference relations (HFLPRs), which is composed of two parts: (1) clustering HFLPRs by mapping them into a higher dimension space based on a kernel function; (2) building a consensus model based on measuring the modified extents of decision makers’ HFLPRs for reducing the biased judgements existing in their less-familiar ways. The paper further makes comprehensive analyses for the proposed model on: (1) the influence of decision makers’ different sensitive attitudes towards the distances between the individual HFLPRs and the overall HFLPRs on decision-making results; (2) the differences and complexities of another model with a different consensus perspective and the proposed model. The experimental analyses provide the support for the maximum modified extent determination in different decision scenarios, and show that the proposed consensus model makes sense. Finally, the proposed model is illustrated by the application in choosing an optimal flood discharge technique for a hydropower station.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309544",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Decision-making models",
      "Dimension (graph theory)",
      "Group decision-making",
      "Machine learning",
      "Mathematics",
      "Perspective (graphical)",
      "Preference",
      "Psychology",
      "Pure mathematics",
      "Social psychology",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Ren",
        "given_name": "Peijia"
      },
      {
        "surname": "Xu",
        "given_name": "Zeshui"
      },
      {
        "surname": "Wang",
        "given_name": "Xinxin"
      },
      {
        "surname": "Zeng",
        "given_name": "Xiao-Jun"
      }
    ]
  },
  {
    "title": "A novel extractive multi-document text summarization system using quantum-inspired genetic algorithm: MTSQIGA",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114555",
    "abstract": "The explosive growth of textual data on the web and the problem of obtaining desired information through this enormous volume of data has led to a dramatic increase in demand for developing automatic text summarization systems. For this reason, this paper presents a novel multi-document text summarization approach, called MTSQIGA, which extracts salient sentences from source document collection to generate the summary. The proposed generic summarizer models extractive summarization as a binary optimization problem that applies a modified quantum-inspired genetic algorithm (QIGA) in its processing stage to find the best solution. Objective function of our approach plays an important role in optimizing linear combination of coverage, relevance, and redundancy factors which consists of six sentence scoring measures. To ensures the generation of a summary with predefined length limit, the presented QIGA employs a modified quantum measurement and a self-adaptive quantum rotation gate based on the quality and length of the summary. Evaluation of the proposed system was performed on DUC 2005 and 2007 benchmark datasets in terms of ROUGE standard measures. Comparison of MTSQIGA with existing state-of-the-art approaches for multi-document summarization shows superior performance of the proposed systems over other methods on both existing benchmark datasets. It also indicates promising efficiency of our proposed algorithm on applying quantum-inspired genetic algorithm to the text summarization tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311994",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Automatic summarization",
      "Computer science",
      "Data mining",
      "Genetic algorithm",
      "Information retrieval",
      "Machine learning",
      "Physics",
      "Quantum",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Mojrian",
        "given_name": "Mohammad"
      },
      {
        "surname": "Mirroshandel",
        "given_name": "Seyed Abolghasem"
      }
    ]
  },
  {
    "title": "International location selection for production fragmentation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114564",
    "abstract": "Today, many companies do not restrict their manufacturing activities to their home countries and are increasingly seeking new locations from which to operate cross-border production sharing. The location decisions involved in production fragmentation represent a strategic issue that has a significant impact on a company’s income and success. The location selection process considers many criteria, especially when the process involves international locations. Most criteria are difficult to assess precisely and quantitatively, and a rating scale may need to be employed. However, such scales, which are often based on linguistic variables, are not typically standardized across various decision-makers, making them dependent upon subjective judgments. To tackle such complications, fuzzy logic is adopted to reduce the uncertainties in group decision-making in such an environment. The purpose of this paper is to applya fuzzy technique for order preference by similarity to ideal solution (fuzzy TOPSIS) regarding the locations to be used in the international production fragmentation of an electronics company located in Thailand. This paper first examines the important criteria influencing location decisions for international production fragmentation. Fuzzy set theory is then applied to manipulate linguistic vagueness, ambiguities, and imprecise information and knowledge in such a manner as to determine weights for multiple criteria. The significant criteria for international production fragmentation are then identified and ranked through expert judgments. The results reveal that skill and competency level of labor force, adequacy of energy and electricity, and taxation and tax incentives are the most important criteria. The potential locations are finally ranked based on their overall performances. The paper also provides implications for management practices in international operations in terms of critical location criteria, decision-making methods, and uncertain situations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000051",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Economics",
      "Fragmentation (computing)",
      "Fuzzy logic",
      "Fuzzy set",
      "Incentive",
      "Mathematics",
      "Microeconomics",
      "Multiple-criteria decision analysis",
      "Operating system",
      "Operations research",
      "Production (economics)",
      "Profit (economics)",
      "Vagueness"
    ],
    "authors": [
      {
        "surname": "Arunyanart",
        "given_name": "Sirawadee"
      },
      {
        "surname": "Sureeyatanapas",
        "given_name": "Panitas"
      },
      {
        "surname": "Ponhan",
        "given_name": "Kowit"
      },
      {
        "surname": "Sessomboon",
        "given_name": "Weerapat"
      },
      {
        "surname": "Niyamosoth",
        "given_name": "Thanawath"
      }
    ]
  },
  {
    "title": "Saliency aware image cropping with latent region pair",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114596",
    "abstract": "As one of the fundamental problems for image enhancing editing, image cropping seeks the best crop with high aesthetic quality and removes extraneous areas. Many recent deep learning methods have been proposed to address the problem, but they do not reveal the intrinsic mechanism of image cropping. In this paper, we explore the latent region pair and then fulfill its potential in our proposed deep learning methodology (SAIC-Net) with saliency map for automatic image cropping. For each image, a lightweight multi-scale feature extraction network is first adopted to produce deep and informative features. Then, the features of latent region pair (ROI and ROD) are aligned and refined by the proposed saliency-aware align operators and context channel attentions. Finally, hybrid loss composed by ranking loss and Huber loss is minimized when training our model. In our experiments, to reduce the searching space for candidate crops, we conduct a saliency-aware grid cropping candidates generation method to eliminate irrelevant crops. Afterwards, we provide a thorough ablation study to better figure out the superiority of each part in our method, and conduct user study against the state-of-the-art methods on a fraction of performance metrics. The quantitative and qualitative results on three benchmark datasets demonstrate the superiority of our SAIC-Net in the task of automatic cropping.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000373",
    "keywords": [
      "Agriculture",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Cropping",
      "Data mining",
      "Deep learning",
      "Ecology",
      "Economics",
      "Feature (linguistics)",
      "Feature extraction",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Management",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Ranking (information retrieval)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Yifei"
      },
      {
        "surname": "Xu",
        "given_name": "Wujiang"
      },
      {
        "surname": "Wang",
        "given_name": "Mian"
      },
      {
        "surname": "Li",
        "given_name": "Li"
      },
      {
        "surname": "Sang",
        "given_name": "Genan"
      },
      {
        "surname": "Wei",
        "given_name": "Pingping"
      },
      {
        "surname": "Zhu",
        "given_name": "Li"
      }
    ]
  },
  {
    "title": "A column generation based heuristic algorithm for piecewise linear regression",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114539",
    "abstract": "Piecewise linear regression is a powerful and flexible regression technique where the dataset is divided into disjoint partitions and a separate regression is computed for each partition. Here, we consider the piecewise linear regression problem where the data partitioning is performed via a fixed number of break points on a predetermined dimension. We develop a column generation heuristic based on a set partitioning formulation of the problem and evaluate its prediction performance using a mixed integer programming formulation introduced earlier as a benchmark. Our results show that the proposed heuristic displays an efficient and robust performance, and also scales up smoothly as the dataset grows.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311830",
    "keywords": [
      "Algorithm",
      "Benchmark (surveying)",
      "Combinatorics",
      "Computer science",
      "Dimension (graph theory)",
      "Discrete mathematics",
      "Disjoint sets",
      "Geodesy",
      "Geography",
      "Geometry",
      "Heuristic",
      "Heuristics",
      "Integer programming",
      "Linear programming",
      "Linear regression",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Partition (number theory)",
      "Piecewise",
      "Piecewise linear function",
      "Polynomial regression",
      "Programming language",
      "Regression",
      "Segmented regression",
      "Set (abstract data type)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Tunc",
        "given_name": "Huseyin"
      },
      {
        "surname": "Genç",
        "given_name": "Burkay"
      }
    ]
  },
  {
    "title": "A graph-based approach to detect unexplained sequences in a log",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114556",
    "abstract": "In this paper we challenge the issue of detecting anomalous events in computer systems log files, through a novel graph mining approach. The basic idea is to model log temporal sequences as a particular graph and event detection as a particular path finding problem. Thus, anomalous sequences correspond to log parts that can not be “explained” by any path in the graph. We propose a novel Iterative Partitioning Log Mining technique to parse any kind of logs and to model their temporal sequence as a probabilistic penalty graph. The approach has been implemented in a framework supporting both real time and batch processing realized on the top of the Apache Spark analytics engine for large-scale data processing. Experimental results show the advantages of the proposed framework in terms of effectiveness for different system configurations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420312008",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Graph",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Cinque",
        "given_name": "Marcello"
      },
      {
        "surname": "Della Corte",
        "given_name": "Raffaele"
      },
      {
        "surname": "Moscato",
        "given_name": "Vincenzo"
      },
      {
        "surname": "Sperlí",
        "given_name": "Giancarlo"
      }
    ]
  },
  {
    "title": "Speaker identification through artificial intelligence techniques: A comprehensive review and research challenges",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114591",
    "abstract": "Speech is a powerful medium of communication that always convey rich and useful information, such as gender, accent, and other unique characteristics of a speaker. These unique characteristics enable researchers to recognize human voice using artificial intelligence techniques that are important in the areas of forensic voice verification, security and surveillance, electronic voice eavesdropping, mobile banking and mobile shopping. Recent advancements in deep learning and other hardware techniques have gained attention of researchers working in the field of automatic speaker identification (SI). However, to the best of our knowledge, there is no in-depth survey is available that critically appraises and summarizes the existing techniques with their strengths and weaknesses for SI. Hence, this study identified and discussed various areas of SI, presented a comprehensive survey of existing studies, and also presented the future research challenges that require significant research efforts in the field of SI systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000324",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Computer security",
      "Data science",
      "Eavesdropping",
      "Epistemology",
      "Field (mathematics)",
      "Identification (biology)",
      "Mathematics",
      "Philosophy",
      "Pure mathematics",
      "Speaker identification",
      "Speaker recognition",
      "Speech recognition",
      "Strengths and weaknesses",
      "Stress (linguistics)"
    ],
    "authors": [
      {
        "surname": "Jahangir",
        "given_name": "Rashid"
      },
      {
        "surname": "Teh",
        "given_name": "Ying Wah"
      },
      {
        "surname": "Nweke",
        "given_name": "Henry Friday"
      },
      {
        "surname": "Mujtaba",
        "given_name": "Ghulam"
      },
      {
        "surname": "Al-Garadi",
        "given_name": "Mohammed Ali"
      },
      {
        "surname": "Ali",
        "given_name": "Ihsan"
      }
    ]
  },
  {
    "title": "An improved quantum-inspired cooperative co-evolution algorithm with muli-strategy and its application",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114629",
    "abstract": "In order to overcome the slow convergence speed, poor global search ability and difficult designing rotation angle of quantum-inspired evolutionary algorithm (QEA), an improved quantum-inspired cooperative co-evolution algorithm based on combining the strategies of cooperative co-evolution, random rotation direction and Hamming adaptive rotation angle, namely MSQCCEA is proposed, which is employed to propose a new airport gate allocation optimization method in this paper. In the proposed MSQCCEA, the cooperative co-evolution strategy is used to improve the global search capability. The random rotation direction strategy is developed to change the quantum evolution direction from one to two in order to avoid local optimal solution and realize the full search of the solution space. A new Hamming adaptive rotation angle strategy is designed to enable individuals to adaptively adjust the rotation angle according to the difference degree between the individual and the target individual, so as to improve the global search ability and convergence speed. A new airport gate allocation optimization method using MSQCCEA is realized to effectively allocate airport gates to the flights. Finally, the knapsack problem and the actual airport gate allocation problem are used to verify the effectiveness of the proposed MSQCCEA and gate allocation optimization method, respectively. The comparison experiment results demonstrate that the proposed MSQCCEA has faster convergence speed and higher convergence accuracy, and the proposed gate allocation optimization method takes on great potential to make decisions for actual airport management.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000701",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Evolutionary algorithm",
      "Hamming distance",
      "Knapsack problem",
      "Lévy flight",
      "Mathematical optimization",
      "Mathematics",
      "Random walk",
      "Rotation (mathematics)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Cai",
        "given_name": "Xing"
      },
      {
        "surname": "Zhao",
        "given_name": "Huimin"
      },
      {
        "surname": "Shang",
        "given_name": "Shifan"
      },
      {
        "surname": "Zhou",
        "given_name": "Yongquan"
      },
      {
        "surname": "Deng",
        "given_name": "Wu"
      },
      {
        "surname": "Chen",
        "given_name": "Huayue"
      },
      {
        "surname": "Deng",
        "given_name": "Wuquan"
      }
    ]
  },
  {
    "title": "High quality error-tolerant phrase mining on text corpus",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114557",
    "abstract": "Phrases are widely used in many text-based expert and intelligent systems. Phrase mining is a critical and preprocessing operation for these systems. With the increase of text data, errors in text corpus widely exist. Existing approaches focus on mining phrases on clean text corpus. However, neglecting to handle these errors may generate inaccurate results, which further leads to quality decline. To address the problem, we propose an error-tolerant phrase mining method, which not only conducts phrase mining in text corpus but also correct those phrases from errors. It could help to improve the performance of text-based expert and intelligent systems. To improve the performance and scalability, we propose several efficient and effective techniques to optimize the mining process. Experimental results show that our method achieves higher performance compared with state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742031201X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Data pre-processing",
      "Database",
      "Epistemology",
      "Focus (optics)",
      "Machine learning",
      "Natural language processing",
      "Operating system",
      "Optics",
      "Philosophy",
      "Phrase",
      "Physics",
      "Preprocessor",
      "Process (computing)",
      "Quality (philosophy)",
      "Scalability",
      "Text corpus"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jiaying"
      },
      {
        "surname": "Shan",
        "given_name": "Jing"
      },
      {
        "surname": "Santos",
        "given_name": "Odafen Ehiaribho"
      },
      {
        "surname": "Bao",
        "given_name": "Jinling"
      }
    ]
  },
  {
    "title": "Multi-attribute decision making on mitigating a collision of an autonomous vehicle on motorways",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114581",
    "abstract": "Autonomous vehicles have the potential to improve automotive safety, largely by removing human error as a possible cause of collisions. However, it cannot be guaranteed that autonomous vehicles will be able to eliminate all collisions. Therefore, automotive safety will continue to be a necessity for automotive design. This paper proposes a decision making system which selects the least severe collision for an autonomous vehicle to take, when facing multiple imminent and unavoidable collisions on a motorway. The novel decision making system developed combines simulation results and multi-attribute decision making (MADM) methods. The simulator includes models of vehicle dynamics and the manoeuvre trajectory path. MADM methods are used to decide which vehicle(s) the autonomous vehicle should collide with, based on the severity of collisions. Severity of collisions is calculated in the simulator using the following variables: impact velocity between autonomous vehicle and vehicle ahead, impact velocity between vehicle behind and autonomous vehicle, manoeuvre acceleration and time-to-collision. Various MADM methods are investigated and three methods are selected including the Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS), the Analytical Hierarchy Process (AHP), and the Analytical Network Process (ANP). Various collision scenarios are defined and tested in order to understand the impact that small changes in parameters of the autonomous vehicle and vehicles ahead and behind have on the decision made. The analysed decision making results are promising and lead to the conclusion that MADM methods can be successfully applied in autonomous vehicles.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000221",
    "keywords": [
      "Acceleration",
      "Aerospace engineering",
      "Analytic hierarchy process",
      "Astronomy",
      "Automotive engineering",
      "Automotive industry",
      "Classical mechanics",
      "Collision",
      "Computer science",
      "Computer security",
      "Engineering",
      "Operating system",
      "Operations research",
      "Physics",
      "Process (computing)",
      "Simulation",
      "TOPSIS",
      "Trajectory"
    ],
    "authors": [
      {
        "surname": "Gilbert",
        "given_name": "Alex"
      },
      {
        "surname": "Petrovic",
        "given_name": "Dobrila"
      },
      {
        "surname": "Pickering",
        "given_name": "James E."
      },
      {
        "surname": "Warwick",
        "given_name": "Kevin"
      }
    ]
  },
  {
    "title": "Cross-project software defect prediction based on domain adaptation learning and optimization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114637",
    "abstract": "Software defect prediction (SDP) is very helpful for optimizing the resource allocation of software testing and improving the quality of software products. The cross-project defect prediction (CPDP) model based on machine learning is first learned through the existing training data with sufficient number and defect labels on one project, and then used to predict the defect labels of another new project with insufficient number and fewer labeled data. However, its prediction performance has a large gap compared with the within-project defect prediction (WPDP) model. The main reason is that there are usually differences between the distributions of training data in different software projects, and it has a greater impact on the prediction performance of the CPDP model. To solve this problem, the kernel twin support vector machines (KTSVMs) is used to implement domain adaptation (DA) to match the distributions of training data for different projects. Moreover, KTSVMs with DA function (called DA-KTSVM) is further used as the CPDP model in this paper. Since the parameters of DA-KTSVM have an impact on its predictive performance, these parameters are optimized by an improved quantum particle swarm optimization algorithm (IQPSO), and the optimized DA-KTSVM is called as DA-KTSVMO. In order to confirm the effectiveness of DA-KTSVMO, some experiments are implemented on 17 open source software projects. Experimental results and analysis show that DA-KTSVMO can not only achieve better prediction performance than other CPDP models compared, but also achieve almost the same or better compared performance than WPDP models when the training sample data is sufficient. In addition, DA-KTSVMO can make better use of existing sufficient data knowledge and realize the reuse of defective data to improve the prediction performance of DA-KTSVMO.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000786",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Domain (mathematical analysis)",
      "Kernel (algebra)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Particle swarm optimization",
      "Physics",
      "Programming language",
      "Software",
      "Software development",
      "Software quality",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Jin",
        "given_name": "Cong"
      }
    ]
  },
  {
    "title": "Medical image fusion based on convolutional neural networks and non-subsampled contourlet transform",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114574",
    "abstract": "Although many powerful convolutional neural networks (CNN) have been applied to various image processing fields, due to the lack of datasets for network training and the significant different intensities of diverse multi-modal source images at the same location, CNN cannot be directly used for the field of medical image fusion (MIF), which is a major problem and limits the development of this field. In this article, a novel multimodal medical image fusion method based on non-subsampled contourlet transform (NSCT) and CNN is presented. The proposed algorithm not only solves this problem, but also exploits the advantages of both NSCT and CNN to obtain better fusion results. In the proposed algorithm, source multi-modality images are decomposed into low and high frequency subbands. For high frequency subbands, a new perceptual high frequency CNN (PHF-CNN), which is trained in the frequency domain, is designed as an adaptive fusion rule. In the matter of the low frequency subband, two result maps are adopted to generate the decision map. Finally, fused frequency subbands are integrated by the inverse NSCT. To verify the effectiveness of the proposed algorithm, ten state-of-the-art MIF algorithms are selected as comparative algorithms. Subjective evaluations by five doctors as well as objective evaluations by seven image quality metrics, demonstrate that the proposed algorithm is superior to the other comparative algorithms in terms of fusing multimodal medical images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000154",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Contourlet",
      "Convolutional neural network",
      "Field (mathematics)",
      "Fusion rules",
      "Image (mathematics)",
      "Image fusion",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Wavelet",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zeyu"
      },
      {
        "surname": "Li",
        "given_name": "Xiongfei"
      },
      {
        "surname": "Duan",
        "given_name": "Haoran"
      },
      {
        "surname": "Su",
        "given_name": "Yanchi"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaoli"
      },
      {
        "surname": "Guan",
        "given_name": "Xinjiang"
      }
    ]
  },
  {
    "title": "Fast knot optimization for multivariate adaptive regression splines using hill climbing methods",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114565",
    "abstract": "Multivariate adaptive regression splines (MARS) is a statistical modeling approach with wide-ranging real-world applications. In the MARS model building process, knot positioning is a critical step that potentially affects the accuracy of the final MARS model. Identifying well-positioned knots entails assessing the quality of many knots in each model building iteration, which requires intensive computational effort. By exploring the change in the residual sum of squares (RSS) within MARS, we find that local optima from previous iterations can be very close to those of the current iteration. In our approach, the prior change in RSS information is used to “warm start” an optimal knot positioning. We propose two methods for MARS knot positioning. The first method is a hill climbing method (HCM), which ignores prior change in RSS information. The second method is a hill climbing method using prior change in RSS information (PHCM). Numerical experiments are conducted on data with up to 30 dimensions. Our results show that both versions of hill climbing methods outperform a standard MARS knot selection method on datasets with different noise levels. Further, PHCM using prior change in RSS information performs best in both accuracy and computational speed. In addition, an open source Python code will be available upon acceptance of the paper on GitHub ( https://github.com/JuXinglong/MARSHC).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000063",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Astronomy",
      "Chemical engineering",
      "Computer science",
      "Data mining",
      "Engineering",
      "Hill climbing",
      "Knot (papermaking)",
      "Machine learning",
      "Mars Exploration Program",
      "Mathematics",
      "Multivariate adaptive regression splines",
      "Operating system",
      "Physics",
      "Polynomial regression",
      "RSS",
      "Regression",
      "Regression analysis",
      "Residual",
      "Source code",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Ju",
        "given_name": "Xinglong"
      },
      {
        "surname": "Chen",
        "given_name": "Victoria C.P."
      },
      {
        "surname": "Rosenberger",
        "given_name": "Jay M."
      },
      {
        "surname": "Liu",
        "given_name": "Feng"
      }
    ]
  },
  {
    "title": "Two-stage human verification using HandCAPTCHA and anti-spoofed finger biometrics with feature selection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114583",
    "abstract": "This paper presents a human verification scheme in two independent stages to overcome the vulnerabilities of attacks and to enhance security. At the first stage, a hand image-based CAPTCHA (HandCAPTCHA) is tested to avert automated bot-attacks on the subsequent biometric stage. In the next stage, finger biometric verification of a legitimate user is performed with presentation attack detection (PAD) using the real hand images of the person who has passed a random HandCAPTCHA challenge. The electronic screen-based PAD is tested using image quality metrics. After this spoofing detection, geometric features are extracted from the four fingers (excluding the thumb) of real users. A modified forward–backward (M-FoBa) algorithm is devised to select relevant features for biometric authentication. The experiments are performed on the Boğaziçi University (BU) and the IIT-Delhi (IITD) hand databases using the k-nearest neighbor and random forest classifiers. The average accuracy of the correct HandCAPTCHA solution is 98.5%, and the false accept rate of a bot is 1.23%. The PAD is tested on 255 subjects of BU, and the best average error is 0%. The finger biometric identification accuracy of 98% and an equal error rate (EER) of 6.5% have been achieved for 500 subjects of the BU. For 200 subjects of the IITD, 99.5% identification accuracy, and 5.18% EER are obtained.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000245",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Biometrics",
      "Computer science",
      "Computer security",
      "Feature (linguistics)",
      "Feature selection",
      "Linguistics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Selection (genetic algorithm)",
      "Speaker recognition",
      "Speaker verification",
      "Spoofing attack",
      "Stage (stratigraphy)"
    ],
    "authors": [
      {
        "surname": "Bera",
        "given_name": "Asish"
      },
      {
        "surname": "Bhattacharjee",
        "given_name": "Debotosh"
      },
      {
        "surname": "Shum",
        "given_name": "Hubert P.H."
      }
    ]
  },
  {
    "title": "Shape autotuning activation function",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114534",
    "abstract": "The choice of activation function is essential for building state-of-the-art neural networks. At present, the most widely-used activation function with effectiveness is ReLU. However, ReLU suffers from the weakness including non-zero mean, negative missing, and unbounded output, thus it has potential disadvantages in the optimization process. In this paper, we propose a novel activation function, namely “Shape Autotuning Activation Function” (SAAF), to overcome these three challenges simultaneously. The SAAF inherits merits of smooth activation functions (such as Sigmoid and Tanh) and piecewise activation functions (such as ReLU and its variants), and avoids their deficiencies. Specifically, the SAAF adaptively adjusts a pair of independent trainable parameters to capture negative information and provide a near-zero mean output, resulting in better generalization performance and faster learning speed. At the same time, it provides bounded outputs to ensure a more stable distribution of output during network training. We evaluated SAAF on deep networks applied to a variety of tasks, including image classification, machine translation, and generative modeling. Comprehensive comparison study shows that the proposed SAAF is superior to state-of-the-art activation functions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311787",
    "keywords": [
      "Activation function",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Evolutionary biology",
      "Function (biology)",
      "Generalization",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Piecewise",
      "Sigmoid function"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Yuan"
      },
      {
        "surname": "Li",
        "given_name": "Dandan"
      },
      {
        "surname": "Huo",
        "given_name": "Shuwei"
      },
      {
        "surname": "Kung",
        "given_name": "Sun-Yuan"
      }
    ]
  },
  {
    "title": "Interactive visual clustering and classification based on dimensionality reduction mappings: A case study for analyzing patients with dermatologic conditions",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114605",
    "abstract": "Multidimensional data sets are becoming more frequent in practically all research fields, and require complex data analysis techniques in order to extract knowledge from them. In this paper, we propose an interactive visualization tool for performing exploratory data analysis. The tool combines unsupervised and supervised dimensionality reduction methods, such as linear discriminant analysis, or t-SNE, with clustering and classification techniques. Analysts can use several machine learning methods for extracting data structure, and can group data into clusters interactively or through clustering algorithms. In addition they can visualize projections of the data to evaluate the quality of obtained clusters, and to analyze the performance of classification methods. We have applied this tool to analyze a clinical data set related to patients with dermatologic conditions that are under photodynamic therapy. The analysis allowed medical doctors to identify several clinically interesting patient groups. In addition, clinicians discovered a greater efficacy in the treatment of patients with the photosensitizer 5-aminolaevulinic acid nanoemulsion gel compared to those treated with methyl-5-aminolaevulinate cream.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000464",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Data set",
      "Dimensionality reduction",
      "Exploratory data analysis",
      "Linear discriminant analysis",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Mohedano-Munoz",
        "given_name": "M.A."
      },
      {
        "surname": "Alique-García",
        "given_name": "S."
      },
      {
        "surname": "Rubio-Sánchez",
        "given_name": "M."
      },
      {
        "surname": "Raya",
        "given_name": "L."
      },
      {
        "surname": "Sanchez",
        "given_name": "A."
      }
    ]
  },
  {
    "title": "A multiverse optimization based colour image segmentation using variational mode decomposition",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114587",
    "abstract": "Multilevel thresholding using the histogram is the most popular and accepted technique of image segmentation. The computational time of multilevel thresholding rises exponentially as the number of the thresholds increases. Histogram suffers from irregularities and sharp details which leads to stagnation. In this article, a newly developed multiverse optimization is combined with variational mode decomposition to overcome this problem. The histogram of an input image divided into several band-limit modes using VMD then reconstruct histogram using meaningful modes. The reconstructed histogram is free from the high-frequency fluctuation which causes local optima. The proposed method utilized two entropy function to develop image segmentation by determining the optimal threshold. The result of the proposed algorithm is analyzed with other evolutionary algorithms such as artificial bee colony, sine cosine algorithm, and salp swarm algorithm. Comparison is made based on comparative parameters such as peak signal to noise ratio, structural similarity index, feature similarity index, uniformity, normalized absolute error, quality index based on local variance, computational time, and mean square error. The test results validate that the proposed algorithm presents more reliable results than other existing techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000282",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Balanced histogram thresholding",
      "Computer science",
      "Histogram",
      "Histogram matching",
      "Image (mathematics)",
      "Image histogram",
      "Image segmentation",
      "Image texture",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Peak signal-to-noise ratio",
      "Segmentation",
      "Thresholding"
    ],
    "authors": [
      {
        "surname": "Chouksey",
        "given_name": "Mausam"
      },
      {
        "surname": "Jha",
        "given_name": "Rajib Kumar"
      }
    ]
  },
  {
    "title": "A POI group recommendation method in location-based social networks based on user influence",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114593",
    "abstract": "Group recommendation has attracted researchers’ attention in various domains, specifically such approaches utilizing location-based social networks (LBSNs). However, point of interest (POI) group recommendation faces the challenge of aggregating diverse user preferences, while group members have different influences on the final decision of the group. Besides, the recommendation of spatial items is different from non-spatial items and the unique features of the spatial items such as distance must be considered in the recommendation. In this paper, a POI group recommendation method is proposed to tackle this problem. User influence is modeled fuzzy and taken into account the difference of users’ personality and their preferences when are alone or in a group, by using historical check-in data in LBSNs and in terms of category, distance and time. The proposed method is integrated with the weighted average aggregation to improve the efficiency of the POI group recommendation. Experimental results in a real dataset show improvement in the accuracy of POI group recommendations in varying sizes of groups. The results also get better when the user influence is calculated using the fuzzy approach. Besides, studying user behavior differences to choose the place to visit when alone or in a group shows that i) the flexibility of users in distance is less than time and category. It is also in the category less than time. ii) Time has a greater range of behavioral change than distance and category. iii) Users who actively participate in group decision making have a more significant number of visits in groups than when they are alone.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000348",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Flexibility (engineering)",
      "Fuzzy logic",
      "Geometry",
      "Group (periodic table)",
      "Information retrieval",
      "Mathematics",
      "Organic chemistry",
      "Point (geometry)",
      "Point of interest",
      "Recommender system",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Bahari Sojahrood",
        "given_name": "Zahra"
      },
      {
        "surname": "Taleai",
        "given_name": "Mohammad"
      }
    ]
  },
  {
    "title": "Improved search space shrinking for medical image retrieval using capsule architecture and decision fusion",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114543",
    "abstract": "Medical diagnosis is a challenging procedure that involves issues such as data imbalance, insufficient labels, obscure images, redundancy and lack of effective model training directions to shrink the semantic gap between human knowledge and computer algorithms. Due to privacy norms, sometimes medical images are difficult to access and therefore, retrieval of identical existing images from an existing repository is quite useful. This paper proposes a search space to narrow down the identical images in an archive by using (i) Capsule Networks, followed by a (ii) decision fusion with Wavelet-Discrete Cosine Transform (W-DCT) and Radon Barcodes (RBC). Empirical case study has been applied on IRMA (Image Retrieval in Medical Applications) dataset, ImageCLEFMed-2009, containing 14,410 X-ray images, but the proposed method is generic, reproducible and scalable. Subjective and quantitative performance has been compared with the state-of-art and it has been found superior to yield accuracy of 92.83% and IRMA error of 124.25 for 193 class-code category. Thus, the proof-of-concept helps to improves diagnosis efficiency for automatic image retrieval and annotation by clustering similar images from the underlying repository.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311878",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Database",
      "Discrete cosine transform",
      "Image (mathematics)",
      "Image retrieval",
      "Operating system",
      "Pattern recognition (psychology)",
      "Redundancy (engineering)",
      "Scalability",
      "Semantic gap"
    ],
    "authors": [
      {
        "surname": "Bhattacharya",
        "given_name": "Jhilik"
      },
      {
        "surname": "Bhatia",
        "given_name": "Tarunpreet"
      },
      {
        "surname": "Pannu",
        "given_name": "Husanbir Singh"
      }
    ]
  },
  {
    "title": "Machine learning with explainability or spatial hedonics tools? An analysis of the asking prices in the housing market in Alicante, Spain",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114590",
    "abstract": "Two sets of modelling tools are used to evaluate the precision of housing-price forecasts: machine learning and hedonic regression. Evidences on the prediction capacity of a range of methods points to the superiority of the random forest as it can calculate real-estate values with an error of less than 2%. This method also ranks the attributes that are most relevant to determining housing prices. Hedonic regression models are less precise but more robust as they can identify the housing attributes that most affect the level of housing prices. This empirical exercise adds new knowledge to the literature as it investigates the capacity of the random forest to identify the three dimensions of non-linearity which, from an economic theoretical point of view, would identify the reactions of different market agents. The intention of the robustness test is to check for these non-linear relationships using hedonic regression. The quantile tools also highlight non-linearities, depending on the price levels. The results show that a combination of techniques would add information on the unobservable (non-linear) relationships between housing prices and housing attributes on the real-estate market.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000312",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Composite material",
      "Computer science",
      "Econometrics",
      "Economics",
      "Finance",
      "Gene",
      "Geometry",
      "Hedonic index",
      "Hedonic regression",
      "Machine learning",
      "Materials science",
      "Mathematics",
      "Point (geometry)",
      "Price index",
      "Quantile regression",
      "Random forest",
      "Range (aeronautics)",
      "Real estate",
      "Regression",
      "Robustness (evolution)",
      "Statistics",
      "Unobservable"
    ],
    "authors": [
      {
        "surname": "Rico-Juan",
        "given_name": "Juan Ramón"
      },
      {
        "surname": "Taltavull de La Paz",
        "given_name": "Paloma"
      }
    ]
  },
  {
    "title": "Automated detection and counting of Artemia using U-shaped fully convolutional networks and deep convolutional networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114562",
    "abstract": "The brine shrimp Artemia is a widely used cost-effective diet in aquaculture. In many Artemia studies, e.g., in a quality assessment of Artemia hatching, an automated method for detecting and counting the Artemia objects in images would be highly desired. However, there are few such works in literature. Moreover, it is very challenging to separate Artemia objects that are highly adjacent. In this paper, we propose a two-stage method for Artemia detection and counting, combining a target marker proposal network with a target classification network. In the first stage, the marker proposal network is implemented using U-shaped fully convolutional networks. This module can indicate target candidates, separate adjacent objects and obtain the object structural information simultaneously. In the second stage, using deep convolutional networks, we design a classifier to classify the target candidates into categories or label as a non-target, thereby obtaining the Artemia detection and counting results. Furthermore, an Artemia detection and counting dataset is collected to train and test the proposed method. Experimental results confirm that the proposed method can accurately detect and count the Artemia objects that have high degrees of adjacency in images, outperforming an ad hoc method based on hand-crafted features and the state-of-the-art YOLO-v3 method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000038",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Brine shrimp",
      "Classifier (UML)",
      "Computer science",
      "Convolutional neural network",
      "Fishery",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Gang"
      },
      {
        "surname": "Van Stappen",
        "given_name": "Gilbert"
      },
      {
        "surname": "De Baets",
        "given_name": "Bernard"
      }
    ]
  },
  {
    "title": "A neural network approach for traffic prediction and routing with missing data imputation for intelligent transportation system",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114573",
    "abstract": "A robust traffic rerouting system is important in traffic management, alongside an accurate traffic simulation model. However, missing data continues to be a problem as it will inevitably cause errors in predicting the congestion levels, resulting in a less efficient rerouting. The lack of a realistic traffic simulation also serves to hamper the development of a better traffic management system. As such, this paper aims to address both problems by proposing three solutions: (i) a traffic simulation that would model a live-traffic, (ii) a pheromone-based, neural network traffic prediction and rerouting system, and (iii) a missing data handling method utilising weighted historical data method named Weighted Missing Data Imputation (WEMDI). The traffic simulation model was benchmarked using Google Maps rerouting system. WEMDI was tested by comparing the performance of the rerouting system with and without WEMDI’s integration for various levels of missing data. The results showed that the traffic simulation model displayed a high correlation to that of Google Maps, and the WEMDI-integrated system displayed 38% to 44% improvement in the related traffic factors, when compared to a situation with no rerouting system in place, and up to 19.39% increase in performance compared to the base rerouting system for missing data levels of 50%. The WEMDI system also displayed robustness in routing other locations, displaying a similarly high performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000142",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Data mining",
      "Engineering",
      "Imputation (statistics)",
      "Intelligent transportation system",
      "Machine learning",
      "Missing data",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Chan",
        "given_name": "Robin Kuok Cheong"
      },
      {
        "surname": "Lim",
        "given_name": "Joanne Mun-Yee"
      },
      {
        "surname": "Parthiban",
        "given_name": "Rajendran"
      }
    ]
  },
  {
    "title": "Exploring groups of opinion spam using sentiment analysis guided by nominated topics",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114585",
    "abstract": "Currently, it is common to see untruthful opinions (also known as review spam, fraud or shilling attack) that resemble each other explicitly or implicitly across multiple business-to-customer websites or opinion sharing communities. Unfortunately, these fake recommendations can be fabricated by individual spammers or results of a manipulation campaign. Considering its severe harmfulness in influencing a product’s reputation, grouped spam is more urgent to detect than individual fraud. Most state-of-the-art techniques of labeling grouped spam, e.g., Frequent Itemset Mining (FIM) or Latent Dirichlet Allocation (LDA), are completely unsupervised and incapable of making good use of officially recommended topics, such as appearance, speed and standby are three suggested aspects along a cell phone product in JD.com. In this paper, we introduce a novel approach based on aspect-oriented sentiment mining that can identify spam groups supported by nominated topics. Experiments show that our method is effective and outperforms several state-of-the-art solutions with statistical significance on two metrics, content duplication and burstiness of time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000269",
    "keywords": [
      "Artificial intelligence",
      "Burstiness",
      "Computer science",
      "Computer security",
      "Data mining",
      "Data science",
      "Geometry",
      "Latent Dirichlet allocation",
      "Linguistics",
      "Malware",
      "Mathematics",
      "Network packet",
      "Philosophy",
      "Phone",
      "Product (mathematics)",
      "Reputation",
      "Sentiment analysis",
      "Social science",
      "Sociology",
      "Topic model"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Jiandun"
      },
      {
        "surname": "Lv",
        "given_name": "Pin"
      },
      {
        "surname": "Xiao",
        "given_name": "Wei"
      },
      {
        "surname": "Yang",
        "given_name": "Liu"
      },
      {
        "surname": "Zhang",
        "given_name": "Pengpeng"
      }
    ]
  },
  {
    "title": "A decision support system for detecting and handling biased decision-makers in multi criteria group decision-making problems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114597",
    "abstract": "Detecting and handling biased decision-makers in the group decision-making process is overlooked in the literature. This paper aims to develop an anti-biased statistical approach, including extreme, moderate, and soft versions, as a decision support system for group decision-making (GDM) to detect and handle the bias. The extreme version starts with eliminating the biased decision-makers (DMs). For this purpose, the DMs with a lower Biasedness Index value than a predefined threshold are removed from the process. Next, it continues with a procedure to mitigate the effect of partially biased DMs by assigning different weights to DMs with respect to their biasedness level. To do so, two ratios for the remaining DMs are calculated: (i) Overlap Ratio, which shows the relative value of overlap between the confidence interval (CI) of each DM and the maximum possible overlap value. (ii) Relative confidence interval CI which reflects the relative value of CI for each DM compared to the confidence interval CI of all DMs. The final step is assigning weight to each DM, considering the two values Overlap Ratio and Relative confidence interval. DMs with closer opinions to the aggregated opinion of all DMs, or those with an adequate level of discrimination in their judgments gain more weight. The framework addresses and prescribes possible actions for all possible cases in GDM including without outliers, cases with partial outliers, and extreme cases with complete disagreement among DMs, or when none of the DMs show an adequate level of discrimination power. The moderate version preassigns a minimum weight to all unbiased DMs and then follows the weighting step for the remaining total weight. However, the soft version follows the preassignmnet of weights to all DMs in the initial pool, meaning there is no elimination in this setting. The proposed approach is tested for several scenarios with different sizes. Four performance measures are introduced to evaluate the effectiveness of the proposed method. The resulted performance measures show the reliability of the outcomes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000385",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Confidence interval",
      "Data mining",
      "Economics",
      "Extreme value theory",
      "Finance",
      "Mathematics",
      "Operating system",
      "Operations research",
      "Outlier",
      "Process (computing)",
      "Relative value",
      "Statistics",
      "Value (mathematics)"
    ],
    "authors": [
      {
        "surname": "Rabiee",
        "given_name": "Meysam"
      },
      {
        "surname": "Aslani",
        "given_name": "Babak"
      },
      {
        "surname": "Rezaei",
        "given_name": "Jafar"
      }
    ]
  },
  {
    "title": "Connectivity-maintaining obstacle avoidance approach for leader-follower formation tracking of uncertain multiple nonholonomic mobile robots",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114589",
    "abstract": "This paper investigates a connectivity-maintaining obstacle avoidance problem for guaranteed-performance-based leader-follower formation tracking of uncertain multiple nonholonomic mobile robots with communication and sensing range constraints. All nonlinearities in the robot dynamics are assumed to be unknown. The desired relative angles among robots for preserving connectivity between the leader and the follower while avoiding obstacles are derived to develop a novel connectivity-maintaining obstacle avoidance strategy. Then, a leader-follower formation tracker using these desired relative angles and connectivity-preserving and collision-avoiding performance functions is designed for accomplishing connectivity maintenance, collision avoidance, and obstacle avoidance among robots. The performance-functions-based avoidance starting range for the connectivity-maintaining obstacle avoidance achievement is induced from the Lyapunov stability analysis. Furthermore, the proposed formation tracking strategy does not require any potential-functions-based approaches and adaptive approximation techniques. Finally, simulation studies clarify and verify the proposed theoretical approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000300",
    "keywords": [
      "Aerospace engineering",
      "Artificial intelligence",
      "Collision",
      "Collision avoidance",
      "Computer science",
      "Computer security",
      "Control (management)",
      "Control theory (sociology)",
      "Engineering",
      "Law",
      "Lyapunov function",
      "Mobile robot",
      "Nonholonomic system",
      "Nonlinear system",
      "Obstacle",
      "Obstacle avoidance",
      "Physics",
      "Political science",
      "Quantum mechanics",
      "Range (aeronautics)",
      "Robot"
    ],
    "authors": [
      {
        "surname": "Park",
        "given_name": "Bong Seok"
      },
      {
        "surname": "Yoo",
        "given_name": "Sung Jin"
      }
    ]
  },
  {
    "title": "A novel approach based on Grasshopper optimization algorithm for medical image fusion",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114576",
    "abstract": "The fusion of multi-modal medical images makes a significant contribution to clinical diagnosis and analysis because it allows diagnostic imaging practitioners to make a more accurate diagnosis. According to our current knowledge, there are some limitations of current medical image fusion approaches. The first limitation is that the use of a weighted average rule for fusing low-frequency components. This limitation leads to a decrease in the intensity of the brightness of the fused image. The second limitation is that the utilizing of fusion rules for high-frequency components is not really optimal. This is likely to result in the loss of detailed information in the fused image. In this paper, a novel approach, including two algorithms, is proposed to address the above-mentioned limitations. The first algorithm is based on the Grasshopper optimization algorithm (GOA) to find optimal parameters with the aim of fusing low-frequency components. This allows the fused image to have good contrast. The second algorithm is based on the Kirsch compass operator to create an efficient rule for the fusion of high-frequency components. This allows the fused image to significantly preserve details transferred from input images. Experimental results show that the proposed approach not only effective in enhancing significantly the contrast of the fused image but also preserving edge information carried from input images to the composite image.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000178",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Ecology",
      "Fusion",
      "Grasshopper",
      "Image (mathematics)",
      "Image fusion",
      "Linguistics",
      "Mathematical optimization",
      "Mathematics",
      "Optimization algorithm",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Dinh",
        "given_name": "Phu-Hung"
      }
    ]
  },
  {
    "title": "Memetic Harris Hawks Optimization: Developments and perspectives on project scheduling and QoS-aware web service composition",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114529",
    "abstract": "Harris hawks optimization (HHO) is one of the leading optimization approaches due to its efficacy and multi-choice structure with time-varying components. The HHO has been applied in various areas due to its simplicity and outstanding performance. However,the original HHO can be improved and evolved in terms of convergence trends, and it is prone to local optimization under certain circumstances. Therefore, the performance and robustness of the algorithm need to be further improved. In our research, based on the core principle of evolutionary methods, we first developed an elite evolutionary strategy (EES) and then utilized it to advance HHO’s convergence speed and ability to jump out of the local optimum. We named such an enhanced hybrid algorithm EESHHO in this paper. To verify the effectiveness and robustness of the EESHHO, we tested it on 29 numerical optimization test functions, including 23 classic basic test functions and 6 composite test functions from the IEEE CEC2017 special session. Moreover, we apply the EESHHO on resource-constrained project scheduling and QoS-aware web service composition problems to further validate the effectiveness of EESHHO. The experimental results show that proposed EESHHO has faster convergence speed and better optimization performance by comparing it with other mainstream algorithms. The supplementary info and answers to possible queries will be publicly available at https://aliasgharheidari.com/publications/EESHHO.html. Also, the codes and info of HHO are available at: https://aliasgharheidari.com/HHO.html.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311738",
    "keywords": [
      "Artificial intelligence",
      "Composition (language)",
      "Computer network",
      "Computer science",
      "Evolutionary algorithm",
      "Linguistics",
      "Mathematical optimization",
      "Mathematics",
      "Memetic algorithm",
      "Operations research",
      "Philosophy",
      "Quality of service",
      "Scheduling (production processes)",
      "Service composition",
      "Web service",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "ChenYang"
      },
      {
        "surname": "Li",
        "given_name": "Jun"
      },
      {
        "surname": "Chen",
        "given_name": "HuiLing"
      },
      {
        "surname": "Heidari",
        "given_name": "Ali Asghar"
      }
    ]
  },
  {
    "title": "An end-to-end framework combining time–frequency expert knowledge and modified transformer networks for vibration signal classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114570",
    "abstract": "Vibration signal classification plays an important role in operation monitoring of mechanical structures. In this paper, a novel end-to-end framework is proposed for intelligent vibration signal classification including data preprocessing, time–frequency feature extraction, modified Transformer network as well as integral optimization. Two case studies in different engineering fields are conducted including the health monitoring of a bearing component through long-term running-to-failure experiments under constant loading conditions and the flight state identification of a novel self-sensing wing structure through a series of wind tunnel experiments under varying angles of attack and airspeeds. Multi-sensor fusion experiments are further conducted to enhance the classification accuracy. Results from both case studies demonstrate that the proposed method can not only extract distinguished high-level features but also be optimized jointly to achieve the best performance over convolutional neural network and recurrent neural network based methods in signal classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000117",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Data mining",
      "End-to-end principle",
      "Feature extraction",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Preprocessor",
      "Radar",
      "Support vector machine",
      "Telecommunications",
      "Time–frequency analysis",
      "Vibration"
    ],
    "authors": [
      {
        "surname": "Jin",
        "given_name": "Can-can"
      },
      {
        "surname": "Chen",
        "given_name": "Xi"
      }
    ]
  },
  {
    "title": "Tensor analysis with n-mode generalized difference subspace",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114559",
    "abstract": "The increasing use of multiple sensors, which produce a large amount of multi-dimensional data, requires efficient representation and classification methods. In this paper, we present a new method for multi-dimensional data classification that relies on two premises: (1) multi-dimensional data are usually represented by tensors, since this brings benefits from multilinear algebra and established tensor factorization methods; and (2) multilinear data can be described by a subspace of a vector space. The subspace representation has been employed for pattern-set recognition, and its tensor representation counterpart is also available in the literature. However, traditional methods do not use discriminative information of the tensors, degrading the classification accuracy. In this case, generalized difference subspace (GDS) provides an enhanced subspace representation by reducing data redundancy and revealing discriminative structures. Since GDS does not handle tensor data, we propose a new projection called n-mode GDS, which efficiently handles tensor data. We also introduce the n-mode Fisher score as a class separability index and an improved metric based on the geodesic distance for tensor data similarity. The experimental results on gesture and action recognition show that the proposed method outperforms methods commonly used in the literature without relying on pre-trained models or transfer learning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420312033",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Discriminant",
      "Discriminative model",
      "Geodesic",
      "Law",
      "Linear subspace",
      "Mathematical analysis",
      "Mathematics",
      "Multilinear map",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Projection (relational algebra)",
      "Pure mathematics",
      "Representation (politics)",
      "Subspace topology",
      "Tensor (intrinsic definition)"
    ],
    "authors": [
      {
        "surname": "Gatto",
        "given_name": "Bernardo B."
      },
      {
        "surname": "dos Santos",
        "given_name": "Eulanda M."
      },
      {
        "surname": "Koerich",
        "given_name": "Alessandro L."
      },
      {
        "surname": "Fukui",
        "given_name": "Kazuhiro"
      },
      {
        "surname": "S. Júnior",
        "given_name": "Waldir S."
      }
    ]
  },
  {
    "title": "Low overhead performance monitoring for shared infrastructures",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114558",
    "abstract": "Modern systems make use of virtualization to build scalable and elastic applications. A major example is cloud computing, which has emerged in the last decades as a cost-effective paradigm for hosting and delivering services over the Internet. Cloud providers adopt server consolidation strategies to manage resources, reducing underutilized hosts. However, collocated applications compete for physical resources. So, their particularities, as well as workload fluctuations, may negatively affect cloud applications’ performance. Performance monitors are useful tools for tracking performance behavior. The observation of events occurring in the system provides a detailed view of how applications affect resource usage. This way, performance counters can assist developers and enable the detection of bottlenecks, allowing administrators to verify whether service level agreements are met. However, the monitoring itself can be a contention source, especially in large scale systems, where hundreds of performance counters are collected, and the acquired data are stored at a high frequency. In this work, the influence of monitoring overhead in virtualized environments is analyzed. As a mean to reduce shared resource contention, it is proposed an approach to alleviate monitoring costs, bringing as benefits: an automatic method for performance counters selection; and overhead reduction by discarding unnecessary performance counters. Through experimental evaluation, this work characterizes the monitoring overhead in virtualized environments. A performance comparison using both the complete and the reduced set of performance counters highlights the benefits of adopting the proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420312021",
    "keywords": [
      "Cloud computing",
      "Computer science",
      "Distributed computing",
      "Economics",
      "Operating system",
      "Operations management",
      "Overhead (engineering)",
      "Performance improvement",
      "Scalability",
      "Temporal isolation among virtual machines",
      "Virtualization",
      "Workload"
    ],
    "authors": [
      {
        "surname": "Popiolek",
        "given_name": "Pedro Freire"
      },
      {
        "surname": "Machado",
        "given_name": "Karina dos Santos"
      },
      {
        "surname": "Mendizabal",
        "given_name": "Odorico Machado"
      }
    ]
  },
  {
    "title": "A Two-stage subgroup Decision-making method for processing Large-scale information",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114586",
    "abstract": "Large-scale group decision-making (LSGDM) problems generally involve a large number of decision-makers (DMs). In many situations, the number of DMs and alternatives simultaneously make traditional techniques inoperable. This paper proposes a two-stage subgroup decision-making (T-SSGDM) method. In contrast to the traditional LSGDM approaches, this framework does not need a clustering process to reduce the size of DMs/alternatives to a manageable level. Instead, we propose a T-SSGDM process to address inter-group heterogeneity. First, DMs and alternatives are randomly grouped so that the number of alternatives to be assessed by each DM is substantially reduced. Second, as the ratings obtained in the first stage of the decision-making process are incomparable, partial samples are selected for the second stage. The relationships among the ratings of different subgroups are then determined by applying the equivalence test. Additionally, to ensure the robustness of the results, three sampling methods and three kinds of functions for the equivalence test are implemented. Finally, an empirical application is used to verify the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000270",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Cluster analysis",
      "Computer science",
      "Contrast (vision)",
      "Data mining",
      "Discrete mathematics",
      "Equivalence (formal languages)",
      "Gene",
      "Group decision-making",
      "Law",
      "Machine learning",
      "Mathematics",
      "Political science",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Chonghui"
      },
      {
        "surname": "Su",
        "given_name": "Weihua"
      },
      {
        "surname": "Zeng",
        "given_name": "Shouzhen"
      },
      {
        "surname": "Balezentis",
        "given_name": "Tomas"
      },
      {
        "surname": "Herrera-Viedma",
        "given_name": "Enrique"
      }
    ]
  },
  {
    "title": "Application of genetic algorithm based support vector machine in selection of new EEG rhythms for drowsiness detection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114634",
    "abstract": "The electroencephalogram (EEG) signals are important for drowsiness detection. However, in some specific application scenarios, whether there is a more accurate rhythm for drowsiness detection is worth further study. Therefore, a method of finding the optimal EEG rhythm for drowsiness detection using the genetic algorithm based support vector machine (GA-SVM) has been proposed in this study. This study used the original EEG signals in the Sleep EDF [Expanded] database for analysis and experiments. First, the original signals were divided into several epochs, and the signals of each epoch were decomposed using db10 wavelet packet transform and haar wavelet packet transform, respectively. Then, the GA-SVM was used to select the most accurate rhythm for drowsiness detection. Finally, leave-one-subject-out cross-validation (LOSO-CV) was used to evaluate the performance of each rhythm for drowsiness detection. The results show that the gamma rhythm has the best detection efficiency in the five traditional rhythms, and the accuracy rate is 80.94%. The detection accuracy of the new rhythm Rhythm (III) (43.75–48.046875 Hz) proposed in this study is 89.52%. The new rhythm proposed in this study showed bast performance in drowsiness detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000750",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Electroencephalography",
      "Internal medicine",
      "Medicine",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Psychology",
      "Rhythm",
      "Speech recognition",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Hui"
      },
      {
        "surname": "Zhang",
        "given_name": "Lei"
      },
      {
        "surname": "Yao",
        "given_name": "Longxu"
      }
    ]
  },
  {
    "title": "Does our social life influence our nutritional behaviour? Understanding nutritional habits from egocentric photo-streams",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114506",
    "abstract": "Nutrition and social interactions are both key aspects of the daily lives of humans. In this work, we propose a system to evaluate the influence of social interaction in the nutritional habits of a person from a first-person perspective. In order to detect the routine of an individual, we construct a nutritional behaviour pattern discovery model, which outputs routines over a number of days. Our method evaluates similarity of routines with respect to visited food-related scenes over the collected days, making use of Dynamic Time Warping, as well as considering social engagement and its correlation with food-related activities. The nutritional and social descriptors of the collected days are evaluated and encoded using an LSTM Autoencoder. Later, the obtained latent space is clustered to find similar days unaffected by outliers using the Isolation Forest method. Moreover, we introduce a new score metric to evaluate the performance of the proposed algorithm. We validate our method on 104 days and more than 100 k egocentric images gathered by 7 users. Several different visualizations are evaluated for the understanding of the findings. Our results demonstrate good performance and applicability of our proposed model for social-related nutritional behaviour understanding. At the end, relevant applications of the model are discussed by analysing the discovered routine of particular individuals.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311507",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Computer science",
      "Computer security",
      "Construct (python library)",
      "Eating behavior",
      "Economics",
      "Image (mathematics)",
      "Internal medicine",
      "Key (lock)",
      "Machine learning",
      "Medicine",
      "Metric (unit)",
      "Obesity",
      "Operations management",
      "Outlier",
      "Programming language",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Glavan",
        "given_name": "Andreea"
      },
      {
        "surname": "Matei",
        "given_name": "Alina"
      },
      {
        "surname": "Radeva",
        "given_name": "Petia"
      },
      {
        "surname": "Talavera",
        "given_name": "Estefania"
      }
    ]
  },
  {
    "title": "Synchronization in collaboration network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114550",
    "abstract": "Cooperative relationship in social network can be described effectively by one-mode RDP model (‘RDP’ represents the initials of Ramasco, Dorogovtsev and Pastor-Satorras, who propose the model.), where n > 2 participants including m ⩾ 1 new participants and n - m > 1 ‘old’ participants collaborate on one project together at each time step. In this paper, the synchronization in this evolution network model is studied systematically. We discover that the synchronization capacity of one-mode RDP model is independent of the network size N, can be reduced by slightly increasing m when m is small, and also be increased by increasing n - m . By calculating the specific expression of average path length (APL) of one-mode RDP model, we analyze the correlation between APL and synchronizability of this model. In some cases, there is no correlation between the APL and the synchronizability of one-mode RDP model. But when N and m are fixed, as n - m increases, the synchronizability of this model is negatively correlated with APL. Furthermore, we explore the robustness and fragility of synchronization in one-mode RDP model. The synchronizability of this model against random fault is not correlated with m and N, and the synchronizability to resist intentional attack is independent of m and N. The synchronizability of this model against random fault and intentional attack can be improved by increasing n - m . Our study fills in the gaps of previous studies that did not consider the synchronization and average path length of one-mode RDP model. Our research results also contain the previous relevant research results and expand them.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311945",
    "keywords": [
      "Biochemistry",
      "Chemistry",
      "Combinatorics",
      "Complex network",
      "Computer network",
      "Computer science",
      "Correlation",
      "Fragility",
      "Gene",
      "Geometry",
      "Mathematics",
      "Mode (computer interface)",
      "Operating system",
      "Path (computing)",
      "Physics",
      "Robustness (evolution)",
      "Synchronization (alternating current)",
      "Thermodynamics",
      "Topology (electrical circuits)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Long"
      },
      {
        "surname": "Yan",
        "given_name": "Baoqiang"
      },
      {
        "surname": "Li",
        "given_name": "Guofeng"
      },
      {
        "surname": "Ma",
        "given_name": "Yinghong"
      },
      {
        "surname": "Yang",
        "given_name": "Lu"
      }
    ]
  },
  {
    "title": "Neighborhood global learning based flower pollination algorithm and its application to unmanned aerial vehicle path planning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114505",
    "abstract": "Flower pollination algorithm (FPA) is a meta-heuristic optimization algorithm that imitates the pollination phenomenon of flowering plants in nature. Due to this algorithm is prone to premature convergence when solving complex optimization problems. So this paper introduces a neighborhood global learning based flower pollination algorithm(NGFPA). Firstly, we analyze the FPA using the constant coefficient differential equation and change the FPA’s global equation. Secondly, we build a neighborhood global learning to enhance population diversity. Finally, the population reconstruction mechanism is added to inhibit the population premature convergence. The convergence of NGFPA is proven using the knowledge of differential equations and stochastic function analysis. We test the performance of NGFPA by optimizing CEC2017. Experiment results show that NGFPA has better performance in comparison with other swarm intelligence algorithms. Furthermore, NGFPA is used to solve the problem of unmanned aerial vehicle (UAV) path planning. Simulation results indicate that NGFPA can obtain smoother paths in different obstacle environments. Therefore, NGFPA is effective and valuable.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311490",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Ecology",
      "Machine learning",
      "Motion planning",
      "Path (computing)",
      "Pollen",
      "Pollination",
      "Programming language",
      "Robot"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yang"
      },
      {
        "surname": "Pi",
        "given_name": "Dechang"
      },
      {
        "surname": "Xu",
        "given_name": "Yue"
      }
    ]
  },
  {
    "title": "Towards the use of Data Engineering, Advanced Visualization techniques and Association Rules to support knowledge discovery for public policies",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114509",
    "abstract": "Education and employment are key aspects of a country’s well-being. Governments expend valuable resources on designing education plans and employment programs. These two aspects are usually analysed separately, although, as they are closely related, considering them together might improve their efficacy. The problem lies, at least in part, in the fact that different public entities manage their own data with their own isolated systems, and do not develop joint educational and employment policies. In order to facilitate working towards this goal, in this manuscript, we make use of Data Engineering, Data Visualization, and Intelligent Data Analytics methods to create a decision support system for the Government of Extremadura. Extremadura is a European Union Objective 1 region in Spain with high rates of unemployment and secondary school drop-out. Data Engineering is used to create a Data Warehouse that unifies the different data sources into a central repository for quick access and control. This allows dealing with the challenge of transforming, processing, storing and accessing the data. Data Visualization techniques are applied to create an interactive dashboard that assists users in analysing and interpreting the data in the Data Warehouse repository. Thus, charts, diagrams, and maps are created specifically to help technical or political decision-makers. Finally, Intelligent Data Analytics techniques are used to incorporate Association Rules into the visualization dashboard. Its goal is to identify associations, relationships, and patterns in data that, at least in plain sight, are not readable or interpretable by humans. It does this by inferring knowledge that humans cannot pick out by themselves. As a result, a complete system was defined and implemented to support public administrations in their decision-making and definition of precise evidence-based policies in the areas of education and employment. In particular, it allows the definition of unified strategies to reduce the unemployment rate.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311532",
    "keywords": [
      "Analytics",
      "Association rule learning",
      "Computer science",
      "Dashboard",
      "Data mining",
      "Data science",
      "Data visualization",
      "Data warehouse",
      "Decision support system",
      "Government (linguistics)",
      "Knowledge extraction",
      "Linguistics",
      "Philosophy",
      "Visual analytics",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Conejero",
        "given_name": "Jose María"
      },
      {
        "surname": "Preciado",
        "given_name": "Juan Carlos"
      },
      {
        "surname": "Fernández-García",
        "given_name": "Antonio Jesús"
      },
      {
        "surname": "Prieto",
        "given_name": "Alvaro E."
      },
      {
        "surname": "Rodríguez-Echeverría",
        "given_name": "Roberto"
      }
    ]
  },
  {
    "title": "Is neural always better? SMT versus NMT for Dutch text normalization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114500",
    "abstract": "Social media texts have become one of the most used forms of written language and a valuable source of information for companies. However, despite the wealth of information hidden in social media language for profiling, information extraction and sentiment analysis purposes, this user-generated content is also characterized by non-standard language. This makes it difficult to process using standard NLP techniques as these are typically trained on standard text. Text normalization is often used as a preprocessing step to overcome this problem. In this work, we take a machine translation perspective on text normalization and investigate both statistical and neural methods. We perform text normalization in a low-resource scenario and report both SMT and NMT experiments for three different Dutch user-generated text types: tweets, message board posts and text messages. Furthermore, we look into overcoming the over-normalization problem in NMT using different techniques, viz. teacher forcing, copy mechanism and byte pair encoding. Our results reveal that even though the SMT approach obtains the best results, the NMT system using CopyNet shows promising results even in this low-resource setting, solving more normalization issues than SMT although not solving all the over-normalization problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311441",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Computer science",
      "Economics",
      "Machine learning",
      "Machine translation",
      "Management",
      "Named-entity recognition",
      "Natural language processing",
      "Normalization (sociology)",
      "Preprocessor",
      "Profiling (computer programming)",
      "Programming language",
      "Sentiment analysis",
      "Social media",
      "Sociology",
      "Task (project management)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Matos Veliz",
        "given_name": "Claudia"
      },
      {
        "surname": "De Clercq",
        "given_name": "Orphée"
      },
      {
        "surname": "Hoste",
        "given_name": "Veronique"
      }
    ]
  },
  {
    "title": "Mining top-N high-utility operation patterns for taxi drivers",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114546",
    "abstract": "In recent years, the rapid development of mobile network and wireless sensor technology has brought opportunities to change the way of the existing taxi business operation. How to improve the operation revenues of taxi drivers has become a topic worthy of research. This paper analyzes and mines taxi operation data to provide taxi drivers with personalized sequence recommendation services, thereby increasing their expected revenues. Different from previous works, the proposed method in this paper recommends a series of future operation orders for taxi drivers, instead of recommending several discrete locations for the current order. In this paper, firstly, by performing spatial-temporal clustering on the origins and destinations of passengers, the spatial and temporal distribution characteristics of passengers in the city are identified. Secondly, the origin of the current passenger is used as the root node to construct a top-N high-utility sequence tree, and this process can be divided into two processes: top-down building tree and bottom-up sorting path utility. The two pruning strategies of node utility and path utility are used to reduce the generation of candidate sets. Finally, a series of potential orders based on dynamic context are recommended to taxi drivers, so as to maximize the expected revenues of taxi drivers. The experimental results demonstrate that there is a close relationship between taxi drivers’ operation behavior patterns and their revenues. The proposed system framework and algorithm in this paper can effectively mine global and long-term top-N high-utility operation patterns.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311908",
    "keywords": [
      "Computer science",
      "Engineering",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Caihong"
      },
      {
        "surname": "Guo",
        "given_name": "Chonghui"
      }
    ]
  },
  {
    "title": "A novel method for multispectral image pansharpening based on high dimensional model representation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114512",
    "abstract": "Pansharpening methods are used to enhance the spatial resolution of a low resolutional multispectral (MS) image by fusing with a high resolutional panchromatic image (PAN). The main difficulty of pansharpening is avoiding spectral distortion while getting a sharpened MS image with high spatial resolution. Intensity-Hue-Saturation (IHS) based methods are applied to transform from color space to IHS and provide equalization of a PAN component with an MS image to eliminate distortion problems. However, most of the modified IHS methods still cause spectral distortion. To overcome this problem, a novel pansharpening method, based on Adaptive High Dimensional Model Representation is proposed in this article. HDMR is a well-known decomposition method for multivariate functions and data sets. The algorithm we propose includes three stages: the first stage is to obtain HDMR components of the MS image using the HDMR decomposition and then to use scaling factors to optimize the effects of the information the components hold. The second stage requires the calculation of some weighting factors in each band to minimize the spectral distortion. Computing the spatial details obtained from the difference between the PAN image and the Adaptive HDMR expansion of the MS image, and adding the difference to the MS image constitutes the third stage. Our proposed algorithm is easy to implement in pansharpening similar to component substitution (CS) based methods, HDMR terms are calculated once and then used adaptively by employing scaling and weighting factors which are determined through a straightforward methodology. The method also provides greater spectral fidelity than the traditional CS based methods as a result of the scaling factors. The proposed method has been tested on different MS images and compared with state-of-the-art pansharpening methods. The results are given both in terms of visual quality and numerical assessments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311568",
    "keywords": [
      "Algorithm",
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Distortion (music)",
      "Image resolution",
      "Karhunen–Loève theorem",
      "Medicine",
      "Multispectral image",
      "Panchromatic film",
      "Pattern recognition (psychology)",
      "Radiology",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Korkmaz Özay",
        "given_name": "Evrim"
      },
      {
        "surname": "Tunga",
        "given_name": "Burcu"
      }
    ]
  },
  {
    "title": "Edge-centric multi-view network representation for link mining in signed social networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114552",
    "abstract": "Signed social networks which are using both negative and positive links are becoming a popular form of social networks. This paper seeks to gain insight into how a user creates a positive or negative connection towards other users. The sign of connections between the users can be predicted efficiently and reliably using a newly proposed metric. In this paper, a new representation which is named Inverse square Metric is proposed. Inverse square metrics is inspired from Inverse square law which uses node properties and distance-based metrics to represent a connection in social networks. Inverse square metric measures the importance and intensity of a pair of nodes using node properties and defines the distance between two nodes as penalty. 16 variant of networks are created to compute the distance between two nodes in the network. In this study, two main contributions are made. First, a new metric to represent edges is proposed which uses both neighbourhood and distance-based link prediction measures. In addition, a unified framework for sign and link prediction problem based on the new representation is proposed. The experimental results on a group of six large networks including Amazon, Facebook, arXiv ASTRO-PH, Epinions, Slashdot, and Wikipedia prove the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311969",
    "keywords": [
      "Artificial intelligence",
      "Complex network",
      "Computer network",
      "Computer science",
      "Economics",
      "Engineering",
      "Enhanced Data Rates for GSM Evolution",
      "Geometry",
      "Inverse",
      "Law",
      "Link (geometry)",
      "Mathematical analysis",
      "Mathematics",
      "Metric (unit)",
      "Neighbourhood (mathematics)",
      "Node (physics)",
      "Operations management",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Sign (mathematics)",
      "Structural engineering",
      "Theoretical computer science",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Ahmadalinezhad",
        "given_name": "Mahboubeh"
      },
      {
        "surname": "Makrehchi",
        "given_name": "Masoud"
      }
    ]
  },
  {
    "title": "Deep reinforcement learning based trading agents: Risk curiosity driven learning for financial rules-based policy",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114553",
    "abstract": "Financial markets are complex dynamic systems influenced by a high number of active agents, which produce a behavior with high randomness and noise. Trading strategies are well depicted as an online decision-making problem involving imperfect information and aiming to maximize the return while restraining the risk. However, it is challenging to obtain an optimal strategy in the complex and dynamic stock market. Therefore, recent developments in similar environments have pushed researchers towards exciting new horizons. In this paper, a novel rule-based policy approach is proposed to train a deep reinforcement learning agent for automated financial trading. Precisely, a continuous virtual environment has been created, with different versions of agents trading against one another. During this multiplex process, the agents which are trained on 504 risky datasets, use the fundamental concepts of proximal policy optimization to improve their own decision making by adjusting their action choice against the uncertainty of states. Risk curiosity-driven learning acts as an intrinsic reward function and is heavily laden with signals to find salient relationships between actions and market behaviors. The trained agent based on curiosity-driven risk has steadily and progressively improved actions quality. The self-learned rules driven by the agent curiosity push the policy towards actions that yield a high performance over the environment. Experiments on 8 real-world stocks are given to verify the appropriateness and efficiency of the self-learned rules. The proposed system has achieved promising performances, made better trades using fewer transactions, and outperformed the state-of-the-art baselines.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311970",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Curiosity",
      "Economics",
      "Finance",
      "Financial market",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Process (computing)",
      "Psychology",
      "Randomness",
      "Reinforcement learning",
      "Social psychology",
      "Statistics",
      "Trading strategy"
    ],
    "authors": [
      {
        "surname": "Hirchoua",
        "given_name": "Badr"
      },
      {
        "surname": "Ouhbi",
        "given_name": "Brahim"
      },
      {
        "surname": "Frikh",
        "given_name": "Bouchra"
      }
    ]
  },
  {
    "title": "A modified equilibrium optimizer using opposition-based learning and novel update rules",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114575",
    "abstract": "Equilibrium Optimizer (EO) is a newly developed physics-based metaheuristic algorithm that is based on control volume mass balance models, and has shown competitive performance with other state-of-the-art algorithms. However, the original EO has the disadvantages of a low exploitation ability, ease of falling into local optima, and an immature balance between exploration and exploitation. To address these shortcomings, this paper proposes a modified EO (m-EO) using opposition-based learning (OBL) and novel update rules that incorporates four main modifications: the definition of the concentrations of some particles based on OBL, a new nonlinear time control strategy, novel population update rules and a chaos-based strategy. Based on these modifications, the optimization precision and convergence speed of the original EO are greatly improved. The validity of m-EO is tested on 35 classical benchmark functions, 25 of which have variants belonging to multiple difficulty categories (Dim = 30, 100, 300, 500 and 1000). In addition, m-EO is used to solve three real-world engineering design problems. The experimental results and two different statistical tests demonstrate that the proposed m-EO shows higher performance than original EO and other state-of-the-art algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000166",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Demography",
      "Geodesy",
      "Geography",
      "Local optimum",
      "Mathematical optimization",
      "Mathematics",
      "Nonlinear system",
      "Physics",
      "Population",
      "Quantum mechanics",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "Qingsong"
      },
      {
        "surname": "Huang",
        "given_name": "Haisong"
      },
      {
        "surname": "Yang",
        "given_name": "Kai"
      },
      {
        "surname": "Zhang",
        "given_name": "Songsong"
      },
      {
        "surname": "Yao",
        "given_name": "Liguo"
      },
      {
        "surname": "Xiong",
        "given_name": "Qiaoqiao"
      }
    ]
  },
  {
    "title": "Online clustering reduction based on parametric and non-parametric correlation for a many-objective vehicle routing problem with demand responsive transport",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114467",
    "abstract": "In this paper, we address an online dimensionality reduction approach to deal with a many-objective formulation of a Vehicle Routing Problem with a Demand Responsive Transport (VRPDRT). The problem relates to a mode of transport similar to available carpooling services in which passengers are transported from their origin to their destination sharing the same vehicle. The goal is to reduce the operating/riding costs while meeting passenger needs and providing high-quality service. Due to the complexity and conflicting characteristics of the problem, an evolutionary approach based on a dimensionality reduction technique is applied to solve the VRPDRT in which eight different objective functions are used. The performance of the proposed approaches – OnCL τ -MOEA/D and OnCL ρ -MOEA/D – are compared to an a priori cluster dimensionality reduction with Pearson’s and τ of Kendall correlation coefficients using a realistic data set containing distances and travel time for Belo Horizonte, Brazil. The online and offline versions of the algorithms are also compared with a baseline approach, a classic version of MOEA/D. Results show that the online cluster-based approach achieves a better spread of solutions, when compared to its a priori versions. Moreover, there is no difference between the results obtained from the online Cluster-based approach and the original MOEA/D. It shows that the proposed dimensionality reduction is an effective technique presenting a positive effect on the search efficiency, computational cost, and in the application of usual visualization techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311180",
    "keywords": [
      "A priori and a posteriori",
      "Cluster analysis",
      "Computer network",
      "Computer science",
      "Curse of dimensionality",
      "Data mining",
      "Dimensionality reduction",
      "Epistemology",
      "Geometry",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Parametric statistics",
      "Philosophy",
      "Reduction (mathematics)",
      "Routing (electronic design automation)",
      "Statistics",
      "Vehicle routing problem"
    ],
    "authors": [
      {
        "surname": "Mendes",
        "given_name": "Renan S."
      },
      {
        "surname": "Lush",
        "given_name": "Victoria"
      },
      {
        "surname": "Wanner",
        "given_name": "Elizabeth F."
      },
      {
        "surname": "Martins",
        "given_name": "Flávio V.C."
      },
      {
        "surname": "Sarubbi",
        "given_name": "João F.M."
      },
      {
        "surname": "Deb",
        "given_name": "Kalyanmoy"
      }
    ]
  },
  {
    "title": "Online clustering reduction based on parametric and non-parametric correlation for a many-objective vehicle routing problem with demand responsive transport",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114467",
    "abstract": "In this paper, we address an online dimensionality reduction approach to deal with a many-objective formulation of a Vehicle Routing Problem with a Demand Responsive Transport (VRPDRT). The problem relates to a mode of transport similar to available carpooling services in which passengers are transported from their origin to their destination sharing the same vehicle. The goal is to reduce the operating/riding costs while meeting passenger needs and providing high-quality service. Due to the complexity and conflicting characteristics of the problem, an evolutionary approach based on a dimensionality reduction technique is applied to solve the VRPDRT in which eight different objective functions are used. The performance of the proposed approaches – OnCL τ -MOEA/D and OnCL ρ -MOEA/D – are compared to an a priori cluster dimensionality reduction with Pearson’s and τ of Kendall correlation coefficients using a realistic data set containing distances and travel time for Belo Horizonte, Brazil. The online and offline versions of the algorithms are also compared with a baseline approach, a classic version of MOEA/D. Results show that the online cluster-based approach achieves a better spread of solutions, when compared to its a priori versions. Moreover, there is no difference between the results obtained from the online Cluster-based approach and the original MOEA/D. It shows that the proposed dimensionality reduction is an effective technique presenting a positive effect on the search efficiency, computational cost, and in the application of usual visualization techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311180",
    "keywords": [
      "A priori and a posteriori",
      "Cluster analysis",
      "Computer network",
      "Computer science",
      "Curse of dimensionality",
      "Data mining",
      "Dimensionality reduction",
      "Epistemology",
      "Geometry",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Parametric statistics",
      "Philosophy",
      "Reduction (mathematics)",
      "Routing (electronic design automation)",
      "Statistics",
      "Vehicle routing problem"
    ],
    "authors": [
      {
        "surname": "Mendes",
        "given_name": "Renan S."
      },
      {
        "surname": "Lush",
        "given_name": "Victoria"
      },
      {
        "surname": "Wanner",
        "given_name": "Elizabeth F."
      },
      {
        "surname": "Martins",
        "given_name": "Flávio V.C."
      },
      {
        "surname": "Sarubbi",
        "given_name": "João F.M."
      },
      {
        "surname": "Deb",
        "given_name": "Kalyanmoy"
      }
    ]
  },
  {
    "title": "Multi-level fleet size optimization for containers handling using double-cycling strategy",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114526",
    "abstract": "Every few years, larger containerized vessels are introduced to the market to accommodate the increase in global trade. Although increasing the capacity of vessels results in maximizing the amount of imported and exported goods per voyage, yet it is accompanied with new challenges to terminal planners. One of the primary challenges is minimizing the vessel turnaround time with the least possible cost. In this context, this paper presents the development of a multi-level optimization model using the elitist non-dominated sorting genetic algorithm (NSGA-II) to determine the optimal or near-optimal fleet size combination of the different container handling equipment used in the terminal. The model aims to minimize two conflicting objective functions, namely, vessel turnaround time and total handling cost. Furthermore, the model considers a double-cycling strategy for the container handling process to achieve increased productivity and eventually more reduction in the vessel turnaround time. The model was implemented on a real-life case study to demonstrate its efficiency and the benefit of employing the double-cycling strategy compared with the traditional single-cycling strategy. The results demonstrated the efficiency of employing the double-cycling strategy by providing a reduction of above 20% in both the vessel turnaround time and the total handling cost and an increase of above 25% in the productivity when compared to the traditional single-cycling strategy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311702",
    "keywords": [
      "Archaeology",
      "Automotive engineering",
      "Computer science",
      "Cycling",
      "Engineering",
      "History",
      "Mathematics",
      "Operations research"
    ],
    "authors": [
      {
        "surname": "El-Abbasy",
        "given_name": "Mohammed Saeed"
      },
      {
        "surname": "Ahmed",
        "given_name": "Essmeil"
      },
      {
        "surname": "Zayed",
        "given_name": "Tarek"
      },
      {
        "surname": "Alfalah",
        "given_name": "Ghasan"
      },
      {
        "surname": "Alkass",
        "given_name": "Sabah"
      }
    ]
  },
  {
    "title": "Additive deep feature optimization for semantic image retrieval",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114545",
    "abstract": "Rapid increase in the distribution of multimedia content in recent times presents a challenging problem for content-based image retrieval systems. Image contents, such as position and shape of objects alongside contextual features such as background can be used to retrieve visually similar images. Variations in contrast, color, intensity and texture of contextually similar images make it an interesting research problem. A deep convolutional neural network-based model called MaxNet for content-based image retrieval is presented in this paper. The proposed system bypasses the reliance on handcrafted features and extracts deep features directly from the images, which are then used to retrieve contextually similar images from the database. The proposed MaxNet model is built by stacking the updated inception module in a hierarchical fashion. Features extracted from various pipelines in the inception module are aggregated after each inception maximizing the feature values. This novel aggregation step generates a model that is able to adapt to variety of datasets. Various types of aggregations are discussed in this study. Model overcomes the over-fitting problem by using a dropout layer after each inception block and just before the output layer. The system outputs softmax probabilities, which are stored in the feature database and are used to compute the similarity index to retrieve images similar to the query image. The MaxNet model is evaluated using four popular image retrieval datasets namely, Corel-1k, Corel-5k, Corel-10k and Caltech-101, where it outperforms state-of-the-art methods in key performance indicators.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311891",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Dropout (neural networks)",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Image retrieval",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Softmax function"
    ],
    "authors": [
      {
        "surname": "Hussain",
        "given_name": "Saddam"
      },
      {
        "surname": "Zia",
        "given_name": "Muhammad Ahmad"
      },
      {
        "surname": "Arshad",
        "given_name": "Waqas"
      }
    ]
  },
  {
    "title": "A comparison between metaheuristics for solving a capacitated fixed charge transportation problem with multiple objectives",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114491",
    "abstract": "The main focus of this work is to formulate and solve a Fixed Charge Transportation Problem (FCTP) considering multiple modes of transport with different capacities. The condition for the feasibility is that the total capacity of all the modes of transport at each origin must be at least the total quantity of commodity available at the origin. The problem is modeled as a multi-objective minimization problem with two conflicting objectives: the total transportation cost and the total transportation time. For this purpose, crossover and mutation operators have been designed to make them suitable for the problem. The problem is solved by a modified NSGA-II, obtained by adapting the newly developed genetic operators in the metaheuristic structure of NSGA-II. Five numerical example problems of various sizes are solved using the modified NSGA-II. For a fair comparison of the performance of the modified NSGA-II, the same example problems are solved using two other algorithms: modified SPEA-2 and a modified GrEA, obtained by incorporating the same newly developed crossover and mutation respectively into SPEA-2 and GrEA. The results obtained by the modified NSGA-II, modified SPEA-2 and modified GrEA are compared using four performance metrics: RNI value, HV, Spacing and GS. The modified NSGA-II performs best for all example problems according to the metrics RNI value and GS, and for all the example problems except the example Problem 3 according to the metrics HV and spacing. For the example Problem 3, the modified GrEA yields the best performance with respect to the metrics HV and spacing. The modified SPEA2 performs worst in terms of all the metric values for all the example problems. Finally, some potential future research directions are discussed in the conclusion.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311362",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Crossover",
      "Fixed charge",
      "Gene",
      "Genetic algorithm",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Minification",
      "Molecular physics",
      "Multi-objective optimization",
      "Mutation",
      "Transportation theory"
    ],
    "authors": [
      {
        "surname": "Biswas",
        "given_name": "Amiya"
      },
      {
        "surname": "Pal",
        "given_name": "Tandra"
      }
    ]
  },
  {
    "title": "Towards Emotion-aware Recommender Systems: an Affective Coherence Model based on Emotion-driven Behaviors",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114382",
    "abstract": "Decision making is the cognitive process of identifying and choosing alternatives based on preferences, beliefs, and degree of importance given by the decision maker to objects or actions. For instance, choosing which movie to watch is a simple, small-sized decision-making process. Recommender systems help people to make this kind of choices, usually by computing a short list of suggestions that reduces the space of possible options. These systems are strongly based on the knowledge of user preferences but, in order to fully support people, they should be grounded on a holistic view of the user behavior, that includes also how emotions, mood, and personality traits influence her choosing patterns. In this work, we investigate how to include emotional aspects in the recommendation process. We suggest that the affective state of the user, defined by a set of emotions (e.g., joy, surprise), constitutes part of choosing situation that should be taken into account when modeling user preferences. The main contribution of the paper is a general emotion-aware computational model based on affective user profiles in which each preference, such as a 5-star rating on a movie, is associated with the affective state felt by the user at the time when that preference was collected. The model estimates whether an unseen item is suitable for the current affective state of the user, by computing an affective coherence score that takes into account both the affective user profile and not-affective item features. The approach has been implemented into an Emotion-aware Music Recommender System, whose effectiveness has been assessed by performing in-vitro experiments on two benchmark datasets. The main outcome is that our system showed improved accuracy of recommendations compared to baselines which include no affective information in the recommendation model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310575",
    "keywords": [
      "Affective computing",
      "Affective science",
      "Artificial intelligence",
      "Cognitive psychology",
      "Coherence (philosophical gambling strategy)",
      "Computer science",
      "Emotion classification",
      "Emotion detection",
      "Emotion recognition",
      "Human–computer interaction",
      "Physics",
      "Psychology",
      "Quantum mechanics",
      "Recommender system",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Polignano",
        "given_name": "Marco"
      },
      {
        "surname": "Narducci",
        "given_name": "Fedelucio"
      },
      {
        "surname": "de Gemmis",
        "given_name": "Marco"
      },
      {
        "surname": "Semeraro",
        "given_name": "Giovanni"
      }
    ]
  },
  {
    "title": "A general ontological timetabling-model driven metaheuristics approach based on elite solutions",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114268",
    "abstract": "Timetabling is a managerial problem that recurringly appears in various domains such as education, transport, sports, and staff management. The combinatorial nature of this problem poses solution challenges that aggravate with an increase in the problem size. While heuristics and metaheuristics initially offered promise, the progress plateaued as attempts to solve even bigger problems showed exorbitant costs while sampling feasible solutions. This issue is criticized for the lack of exploiting the underlying problem structure and the prevailing fragmentation in modeling and solution approaches. To address these issues, we first propose a novel timetabling ontology that serves as a common modeling basis, resolving the existing heterogeneity across various application domains. This ontology facilitates mapping the anatomy of any real timetabling problem onto its general structure. Second, it offers a unique two-stage solution approach for solving this generalized problem. The first stage of this approach entails generating elite initial solutions by exploiting this general problem structure, while the second stage uses a metaheuristic to improve these solutions at a very low computational cost. Using a university timetabling problem, we demonstrate the applicability of this approach. The numerical results show that the proposed algorithm converges within a fraction of computational costs incurred by other techniques for comparable problem sizes. This research paves the way for consolidating efforts for the development of generalizable cross-domain timetabling approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309799",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Domain (mathematical analysis)",
      "Epistemology",
      "Heuristics",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Ontology",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Siddiqui",
        "given_name": "Atiq W."
      },
      {
        "surname": "Arshad Raza",
        "given_name": "Syed"
      }
    ]
  },
  {
    "title": "Parallel versus cascaded logistic regression trained single-hidden feedforward neural network for medical data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114538",
    "abstract": "Objective An important step towards a better healthcare system is fast and accurate diagnosis. In the last decade, the application of intelligent systems in healthcare has led to impressive results. The goal of this paper is to extend the LogSLFN (single-hidden layer feedforward neural network trained using logistic regression) algorithm, which has been deployed successfully in the past for the case of a two-class decision problem, to the case of multiple classes. We have considered and statistically analyzed two approaches: a parallel LogSLFN, and a cascaded LogSLFN. Materials and methods According to the universal approximation theorem, a single-hidden layer feedforward neural network has the ability to approximate arbitrarily closely continuous functions of several real variables under certain reasonable assumptions. Essentially, a single hidden layer containing a finite fixed number of neurons is sufficient to provide an arbitrarily well approximation to a given training set of inputs and a desired target output represented by a continuous function. Parallel LogSLFN and cascaded LogSLFN are two novel approaches that can be applied to multiple-class decision problems. Both methods are extensions of the LogSLFN, which uses logistic regression to compute the weights between the input and hidden layer of a single-hidden layer feedforward network. No error correction is needed, the weights between the hidden and the output layer being computed using the Moore-Penrose pseudoinverse matrix. The proposed models have been tested on two medical datasets regarding cancer diagnosis and liver fibrosis staging. Experimental results and the subsequent statistical analysis have proved the robustness of the proposed models with other machine learning techniques reported in literature. Main findings The experimental results showed that the Parallel approach surpasses the Cascaded one. Still, both models are competitive to the other state-of-the-art techniques. Conclusions The LogSFLN algorithm can be successfully extended to multiple-class decision problems. By embedding knowledge extracted from the data into the architecture, we obtained a raise by 20% in accuracy when applied on the liver fibrosis dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311829",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Control engineering",
      "Data mining",
      "Engineering",
      "Feed forward",
      "Feedforward neural network",
      "Gene",
      "Geometry",
      "Inverse",
      "Logistic regression",
      "Machine learning",
      "Mathematics",
      "Moore–Penrose pseudoinverse",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Belciug",
        "given_name": "Smaranda"
      }
    ]
  },
  {
    "title": "An investigation on Wu-Leung multi-scale information systems and multi-expert group decision-making",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114542",
    "abstract": "In this paper, we establish group decision-making (GDM) idea on Wu-Leung multi-scale information systems (Wu-Leung MSISs) from the perspective of multi-expert group decision-making (MEGDM). Based on the connection between a multi-expert group decision-making information system (MEGDMIS) and a Wu-Leung MSIS, we first use the idea of data unification to transform GDM on a Wu-Leung MSIS into MEGDM. We then construct a positive (and, respectively, a negative) ideal deviation distance information system. Subsequently, the complex proportional assessment (COPRAS) method is also used to obtain the optimal ranking scheme and the optimal alternative. Two numerical examples which can be formed respectively a Wu-Leung MSIS and an MEGDMIS are further employed to validate the effectiveness and feasibility of the proposed method. Comparative analysis and experimental examination are also presented to demonstrate the superiority of our approach. Finally, we summarize the link between MEGDM and GDM on Wu-Leung MSISs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311866",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Construct (python library)",
      "Data mining",
      "Epistemology",
      "Group (periodic table)",
      "Group decision-making",
      "Ideal (ethics)",
      "Law",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Organic chemistry",
      "Perspective (graphical)",
      "Philosophy",
      "Physics",
      "Political science",
      "Programming language",
      "Quantum mechanics",
      "Ranking (information retrieval)",
      "Scale (ratio)",
      "Scheme (mathematics)",
      "Unification"
    ],
    "authors": [
      {
        "surname": "Zhan",
        "given_name": "Jianming"
      },
      {
        "surname": "Zhang",
        "given_name": "Kai"
      },
      {
        "surname": "Wu",
        "given_name": "Wei-Zhi"
      }
    ]
  },
  {
    "title": "Towards optimal hydro-blasting in reconfigurable climbing system for corroded ship hull cleaning and maintenance",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114519",
    "abstract": "The operation of a ship in the ocean depends crucially on the quality of routine offshore dry dock maintenance. Automation by robotics is an efficient solution to address the issues of saving water, energy, time, and easing the labour workload when conducting hydro-blasting hulls in the dry dock ship maintenance industry. In this paper, the automated hydro-blasting in corroded ship hull cleaning by a novel robot platform with reconfigurable manipulators named Hornbill is proposed. The robot is able to maneuver smoothly on a vertical surface by permanent magnetic force, to carry the heavy load, to clean the corroded ship hull by hydro-blasting, and to self-evaluate hydro-blasting task by leveraging the Deep Convolutional Neural Network (DCNN) to synthesis the corrosion level map of the blasted workspace. We also propose an optimal complete waypoint path planning (CWPP) framework to help the robot re-blast the benchmarked workspace. The optimal CWPP problem, including objective functions of the shortest travel distance, the least upward moving direction to reduce water, energy spent while ensuring the visiting of the robot to all uncleaned waypoints defined by benchmarking output, is modeled as the classic Travel Salesman Problem (TSP). The evolutionary-based optimization techniques, including Genetic Algorithm (GA) and Ant Colony Optimization (ACO), are explored to derive the Pareto-optima solution for given TSP. The experimental results show that the magnetic force and motors torque are synchronized to enable the proposed system to navigate smoothly on the vertical surfaces tested with different corrosion levels. The proposed corrosion level benchmarking achieves a mean accuracy of 0.956 with an execution time of 30 fps. Besides, the proposed CWPP enables the proposed robot to yield about 15%, 26%, and 5% the energy, water, and time, respectively, less than the conventional methods when the experiments are conducted in various workspaces on the real ship hull.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311635",
    "keywords": [
      "Artificial intelligence",
      "Automation",
      "Benchmarking",
      "Business",
      "Computer science",
      "Engineering",
      "Graph",
      "Hull",
      "Marine engineering",
      "Marketing",
      "Mechanical engineering",
      "Motion planning",
      "Real-time computing",
      "Robot",
      "Shortest path problem",
      "Theoretical computer science",
      "Waypoint",
      "Workspace"
    ],
    "authors": [
      {
        "surname": "Le",
        "given_name": "Anh Vu"
      },
      {
        "surname": "Veerajagadheswar",
        "given_name": "Prabakaran"
      },
      {
        "surname": "Kyaw",
        "given_name": "Phone Thiha"
      },
      {
        "surname": "Viraj J. Muthugala",
        "given_name": "M.A."
      },
      {
        "surname": "Elara",
        "given_name": "Mohan Rajesh"
      },
      {
        "surname": "Kuma",
        "given_name": "Madhu"
      },
      {
        "surname": "Khanh Nhan",
        "given_name": "Nguyen Huu"
      }
    ]
  },
  {
    "title": "Algorithmically generated malicious domain names detection based on n-grams features",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114551",
    "abstract": "Botnets are one of the major cyber infections used in several criminal activities. In most botnets, a Domain Generation Algorithm (DGA) is used by bots to make DNS queries aimed at establishing the connection with the Command and Control (C&C) server. The identification of such queries by monitoring the network DNS traffic is then crucial for bot detection. In this paper we present a methodology to detect DGA generated domain names based on a supervised machine learning process, trained with a dataset of known benign and malicious domain names. The proposed approach represents the domain names through a set of features which express the similarity between the 2-grams and 3-grams in a single unclassified domain name and those in domain names known as malicious or benign. We used the Kullback-Leibner divergence and the Jaccard Index to estimate the similarity, and we tested different machine learning algorithms to classify each domain name as benign or DGA-based (with both binary and multi-class approach). The results of our experiments demonstrate that the proposed methodology, which only exploits lexical features of domain names, attains a good level of accuracy and results in a general model able to classify previously unseen domains in an effective way. It is also able to outperform some of the state-of-the-art featur eless classification methods based on deep learning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311957",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Botnet",
      "Computer science",
      "Computer security",
      "Data mining",
      "Domain (mathematical analysis)",
      "Domain Name System",
      "Domain name",
      "Exploit",
      "Identification (biology)",
      "Image (mathematics)",
      "Jaccard index",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Similarity (geometry)",
      "The Internet",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Cucchiarelli",
        "given_name": "Alessandro"
      },
      {
        "surname": "Morbidoni",
        "given_name": "Christian"
      },
      {
        "surname": "Spalazzi",
        "given_name": "Luca"
      },
      {
        "surname": "Baldi",
        "given_name": "Marco"
      }
    ]
  },
  {
    "title": "Fuzzy rule-based neural appointed-time control for uncertain nonlinear systems with aperiodic samplings",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114504",
    "abstract": "In this paper, a fuzzy rule-based neural appointed-time control scheme for uncertain nonlinear systems with aperiodic samplings is proposed. To guarantee tracking properties with preassigned convergence time and remove the high dependency on accurate initial states of system, we construct an improved prescribed performance control (IPPC) scheme on the basis of a hyperbolic cosecant performance function with a finite-time behavior. In light of the unavailability of partial states and uncertainties, by combining a fuzzy wavelet neural network (FWNN) with a state observer, a FWNN-based state observer is developed, which can simultaneously approximate unavailable system states and unknown lumped disturbances with a remarkable accuracy. In addition, aimed at eliminating the problem of parameter updating explosion caused by overlarge learning dimensions, a minimum-learning-parameter (MLP) technique is embedded in the FWNN-based state observer, where the norm of weight matrix is employed for online adaptive updating. Furthermore, an event-triggered control scheme with relative thresholds is synthesized within the framework of dynamic surface control (DSC), which can allow for aperiodic samplings to save communication and actuating resources. Meanwhile, a Nussbaum type function is introduced to solve the issue of unknown control coefficients. The significant features of our work are twofold: (1) Appointed-time tracking performances with much fewer sampling times are assured. (2) An enhanced robustness against uncertainties is achieved with a lower computational complexity and the requirement of full-state measurements is relaxed. Finally, two simulation examples are performed to validate the effectiveness and advantages of the proposed control scheme.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311489",
    "keywords": [
      "Aperiodic graph",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Fuzzy control system",
      "Fuzzy logic",
      "Gene",
      "Mathematical optimization",
      "Mathematics",
      "Nonlinear system",
      "Physics",
      "Quantum mechanics",
      "Robustness (evolution)",
      "State observer",
      "Statistics",
      "Unavailability"
    ],
    "authors": [
      {
        "surname": "Haonan",
        "given_name": "Si"
      },
      {
        "surname": "Xingling",
        "given_name": "Shao"
      },
      {
        "surname": "Wendong",
        "given_name": "Zhang"
      }
    ]
  },
  {
    "title": "Dynamic learning framework for epileptic seizure prediction using sparsity based EEG Reconstruction with Optimized CNN classifier",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114533",
    "abstract": "The World Health Organization (WHO) recently stated that epilepsy affects nearly 65 million people of the world population. Early forecast of the oncoming seizures is of paramount importance in saving the life of epileptic patients. This paper demonstrates a phase transition-based seizure prediction approach from multi-channel scalp electroencephalogram (EEG) recordings. The primary focus of this work is to discriminate the seizure and seizure-free EEG signals by learning the dynamics of preictal, interictal and ictal period. We propose an adaptive optimization approach using non-linear conjugate gradient technique in conjunction with Sparsity based EEG Reconstruction (SER) and three-dimensional Optimized Convolutional Neural Network (3D OCNN) classifier, based on Fletcher Reeves (FR) algorithm. Sparsity based artifact removal approach along with a 3D OCNN classifier, classifies the various states of seizures. FR algorithm is deployed with the deep neural network architecture to accelerate the convergence rate and to reduce the complexity of the proposed non-linear model. The Principle Component Analysis (PCA) algorithm replacing the Singular Value Decomposition (SVD) in the K-SVD algorithm, further reduces the time and complexity of the pre-processing stage. We further propose a Phase Transition based Kullback-Leibler divergence (PTB-KL) predictor for obtaining the Optimal Seizure Prediction Horizon (OSPH). The proposed model is evaluated using three diverse databases such as CHB-MIT, NINC and SRM respectively. Empirical results on the three EEG databases of 300 recordings outperforms the state-of-art approaches with an accuracy score of 0.98, sensitivity score of 0.99 and False Prediction Rate (FPR) of 0.07 FP/h. Statistical assessment of the proposed predictor gains an OSPH of about 1.1 h prior to the seizure onset. Experimental results prove that the phase transition-based seizure prediction approach is a promising one for accurate real-time prediction of epilepsy using scalp EEG data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311775",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Classifier (UML)",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Electroencephalography",
      "Epilepsy",
      "Epileptic seizure",
      "Ictal",
      "Machine learning",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Psychiatry",
      "Psychology",
      "Singular value decomposition"
    ],
    "authors": [
      {
        "surname": "Priya Prathaban",
        "given_name": "Banu"
      },
      {
        "surname": "Balasubramanian",
        "given_name": "Ramachandran"
      }
    ]
  },
  {
    "title": "A novel pipeline framework for multi oriented scene text image detection and recognition",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114549",
    "abstract": "Automatic text detection and recognition (end-to-end text recognition) in real-life images are the main elements of many applications including blind and low vision assistance systems and self-driving cars. However, it is challenging to detect curved and vertical texts due to their color bleeding, font size variation, and complicated background. In this paper, a convolutional neural network-based pipeline is introduced to obtain high-level visual features and improve text detection and recognition efficiency. A pre-trained ResNet-50 network on ImageNet and SynthText for extracting low-level visual features was used in this study. Moreover, new improved ReLU layer (new.i.ReLU) blocks are used with a varied receptive field with a strong ability to detect text components even on curved surfaces in the proposed structure. A new improved inception layer (new.i.inception layers) can obtain broadly varying-sized text more effectively than a linear chain of convolution layer. Also, we have proposed a pipeline framework for character recognition that is robust to irregular (curve and vertical) text. First, we introduced a novel algorithm for encoding pixel’s value to a new one called local word directional pattern (LWDP) that highlights the texture of the characters. Then, the output of LWDP was presented as an input image in the text recognition process. The experiments on standard benchmarks, including ICDAR 2013, ICDAR 2015, and ICDAR 2019 datasets, illustrated the superiority of the proposed architecture over prior works.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311933",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Image (mathematics)",
      "Layer (electronics)",
      "Operating system",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Pipeline (software)",
      "Pixel",
      "Process (computing)",
      "Programming language",
      "Text detection",
      "Text recognition"
    ],
    "authors": [
      {
        "surname": "Naiemi",
        "given_name": "Fatemeh"
      },
      {
        "surname": "Ghods",
        "given_name": "Vahid"
      },
      {
        "surname": "Khalesi",
        "given_name": "Hassan"
      }
    ]
  },
  {
    "title": "A Pareto based discrete Jaya algorithm for multi-objective flexible job shop scheduling problem",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114567",
    "abstract": "Research on the development of Pareto-based multi-objective algorithms to address scheduling problems has attracted a lot of attention in recent years. In this work, a multi-objective discrete Jaya algorithm (MODJA) is proposed to address the flexible job shop scheduling problem (FJSSP) considering the minimization of makespan, total workload of machines, and workload of critical machine as performance measures. A discrete Jaya algorithm is proposed to handle the problem under consideration. A problem specific neighborhood-based local search technique is integrated into the proposed approach to enhance its exploitation capability. Further, a dynamic mutation operator and a modified crowding distance measure are proposed to enhance the diversity in the search process. Extensive computational experiments are carried out considering 203 instances of FJSSP from literature. The Taguchi method of design is employed to identify the best set of key parameters based on three instances. In the experimentation phase, initially, the contribution of the proposed local search technique and crowding distance measure is investigated. Then, a comparison of MODJA with the weighted sum version of the approach and other multi-objective evolutionary algorithms is performed. Computational results demonstrate the effectiveness of the proposed MODJA in obtaining diverse and improved Pareto-optimal solutions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000087",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Data mining",
      "Flow shop scheduling",
      "Job shop",
      "Job shop scheduling",
      "Local search (optimization)",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Measure (data warehouse)",
      "Multi-objective optimization",
      "Operating system",
      "Pareto principle",
      "Schedule",
      "Scheduling (production processes)",
      "Workload"
    ],
    "authors": [
      {
        "surname": "Caldeira",
        "given_name": "Rylan H."
      },
      {
        "surname": "Gnanavelbabu",
        "given_name": "A."
      }
    ]
  },
  {
    "title": "Sparse Gaussian process for online seagrass semantic mapping",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114478",
    "abstract": "Over recent decades, we have witnessed a widescale deterioration in seagrass habitats in the Mediterranean Sea. Although the causes underlying this regression are not yet clear, there is a broad consensus that new methods are required to generate high-quality data on seagrass meadows. Specifically, there is a lack of available data in order to assess changes over time in the spatial distribution of seagrass. This article introduces a new methodology to ease data-gathering operations in Posidonia oceanica meadows for benthic mapping, by using the latest developments in lightweight autonomous vehicles and image processing. The proposed methodology has been designed to build a series of online maps, by employing an input data feed based on an encoder–decoder convolutional neural network (CNN) that automatically segments the images recorded by the vehicle. In turn, it uses a sparse Gaussian Process (GP) to map uncertainty linked to the spatial distribution of seagrass. This article presents the most suitable CNN and GP, as well as performance validation results for the method, based on data acquired in three field tests.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742031126X",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Ecology",
      "Field (mathematics)",
      "Gaussian",
      "Gaussian process",
      "Geography",
      "Geospatial analysis",
      "Habitat",
      "Mathematics",
      "Operating system",
      "Physics",
      "Posidonia oceanica",
      "Process (computing)",
      "Pure mathematics",
      "Quantum mechanics",
      "Remote sensing",
      "Seagrass"
    ],
    "authors": [
      {
        "surname": "Guerrero-Font",
        "given_name": "Eric"
      },
      {
        "surname": "Bonin-Font",
        "given_name": "Francisco"
      },
      {
        "surname": "Martin-Abadal",
        "given_name": "Miguel"
      },
      {
        "surname": "Gonzalez-Cid",
        "given_name": "Yolanda"
      },
      {
        "surname": "Oliver-Codina",
        "given_name": "Gabriel"
      }
    ]
  },
  {
    "title": "Discovery of stay area in indoor trajectories of moving objects",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114501",
    "abstract": "With the rapid development of location-based services, a large amount of moving object trajectory data has been generated. Mining users’ stay area from moving trajectory data can not only improve location-based service, but also take precautions to ensure personal safety to better promote the development of location-based services. However, the research on trajectory of indoor moving objects is still in its infancy, and there is a lack of relevant research in this field. It is urgent to meet the needs of the location service industry. We have done some novel work on the stay area in indoor trajectories. Firstly, a novel algorithm named SVBDSA is proposed for the stability value-based discovery of stay area. The stability value is calculated from the distance and velocity between the two points. By comparing the stability value with the threshold value, the two points are judged whether they are in a stable state, and then the point in the stable state is extended. Thus, the stay area in the moving trajectories can be discovered. Secondly, this paper proposes an improved algorithm named T - O P T I C S , which adds the time attribute and puts forward the concept of difference value, so that abnormal stay points can be detected in the discovered stay area. Finally, the trajectory generation tool Vita is used to conduct experiments and analysis and the effectiveness of the proposed algorithms in this paper is verified. Discovery of stay area is not only a new and difficult scientific issue that has not been studied in the field of indoor trajectories of the moving object, but also has important academic value and research significance in location-based service industry.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311453",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Astronomy",
      "Computer science",
      "Data mining",
      "Economics",
      "Economy",
      "Engineering",
      "Field (mathematics)",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Mechanical engineering",
      "Object (grammar)",
      "Operations research",
      "Physics",
      "Point (geometry)",
      "Pure mathematics",
      "Real-time computing",
      "Service (business)",
      "Stability (learning theory)",
      "State (computer science)",
      "Trajectory",
      "Value (mathematics)",
      "Work (physics)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Yang"
      },
      {
        "surname": "Chen",
        "given_name": "Yi"
      },
      {
        "surname": "Pi",
        "given_name": "Dechang"
      }
    ]
  },
  {
    "title": "Multi-criteria tensor model for tourism recommender systems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114537",
    "abstract": "Many tourism recommender systems have been studied to offer users the items meeting their interests. However, it is a non-trivial task to reflect the multi-criteria ratings and the cultural differences, which significantly influence users’ reviews of tourism facilities, into recommendation services. This paper proposes two “single tensor” models, consisting of users (or countries), items, multi-criteria ratings, and cultural groups, in order to consider simultaneously an inherent structure and interrelations of these factors into recommendation processes. With one Tripadvisor dataset, including 13 K users from 120 countries, experiments demonstrated that, in terms of MAE, the two proposed models for user and country give an improvement of 21.31% and 7.11% than other collaborative filtering and multi-criteria recommendation techniques. Besides, there were the positive influences of multiple-criteria ratings and cultural group factors on recommendation performances. The comparative analysis of several variants of the proposed models showed that considering Western and Eastern cultures is appropriate for improving predictive performances and their stability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311817",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "History",
      "Information retrieval",
      "Mathematics",
      "Pure mathematics",
      "Recommender system",
      "Tensor (intrinsic definition)",
      "Tourism"
    ],
    "authors": [
      {
        "surname": "Hong",
        "given_name": "Minsung"
      },
      {
        "surname": "Jung",
        "given_name": "Jason J."
      }
    ]
  },
  {
    "title": "Semi-automatic generation of multilingual datasets for stance detection in Twitter",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114547",
    "abstract": "Popular social media networks provide the perfect environment to study the opinions and attitudes expressed by users. While interactions in social media such as Twitter occur in many natural languages, research on stance detection (the position or attitude expressed with respect to a specific topic) within the Natural Language Processing field has largely been done for English. Although some efforts have recently been made to develop annotated data in other languages, there is a telling lack of resources to facilitate multilingual and crosslingual research on stance detection. This is partially due to the fact that manually annotating a corpus of social media texts is a difficult, slow and costly process. Furthermore, as stance is a highly domain- and topic-specific phenomenon, the need for annotated data is specially demanding. As a result, most of the manually labeled resources are hindered by their relatively small size and skewed class distribution. This paper presents a method to obtain multilingual datasets for stance detection in Twitter. Instead of manually annotating on a per tweet basis, we leverage user-based information to semi-automatically label large amounts of tweets. Empirical monolingual and cross-lingual experimentation and qualitative analysis show that our method helps to overcome the aforementioned difficulties to build large, balanced and multilingual labeled corpora. We believe that our method can be easily adapted to easily generate labeled social media data for other Natural Language Processing tasks and domains.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742031191X",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Data science",
      "Domain (mathematical analysis)",
      "Field (mathematics)",
      "Information retrieval",
      "Leverage (statistics)",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Operating system",
      "Process (computing)",
      "Pure mathematics",
      "Social media",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Zotova",
        "given_name": "Elena"
      },
      {
        "surname": "Agerri",
        "given_name": "Rodrigo"
      },
      {
        "surname": "Rigau",
        "given_name": "German"
      }
    ]
  },
  {
    "title": "A novel framework of graph Bayesian optimization and its applications to real-world network analysis",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114524",
    "abstract": "Network structure optimization is a fundamental task of many expert and intelligent systems, such as the intelligent tools for chemical molecular discovery and expert systems for road network design. However, traditional model-free methods suffer from the expensive computational cost of evaluating networks; almost all the research on Bayesian optimization is aimed at optimizing the objective functions with vectorial inputs, e.g., the hyper-parameters in any expert systems. This work focuses on applying Bayesian optimization to optimize network structure with graph-structured inputs, and presents a flexible framework, denoted as graph Bayesian optimization (GBO), to handle arbitrary graphs. By combining the proposed framework with graph kernels, it can take full advantage of implicit graph structural features to supplement explicit features guessed according to the experience, such as tags of nodes and any attributes of graphs. Simultaneously, the proposed framework can identify which features are more important during the optimization process. By collaboratively working with a down-stream decision tree, the GBO can not only find the optimum but also discover its knowledge represented by rules, which can further enhance its interpretability and assist expert decision-making. A novel problem of opening the gated residential areas is presented in this work, which can serve as one benchmark task of road network design. Intensive experiments conducted on three real-world applications, including robust network design, the most active node identification, and urban transportation network design, demonstrate its efficacy and potential applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311684",
    "keywords": [
      "Artificial intelligence",
      "Bayesian network",
      "Bayesian optimization",
      "Benchmark (surveying)",
      "Computer science",
      "Data mining",
      "Geodesy",
      "Geography",
      "Graph",
      "Interpretability",
      "Machine learning",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Cui",
        "given_name": "Jiaxu"
      },
      {
        "surname": "Tan",
        "given_name": "Qi"
      },
      {
        "surname": "Zhang",
        "given_name": "Chunxu"
      },
      {
        "surname": "Yang",
        "given_name": "Bo"
      }
    ]
  },
  {
    "title": "A novel association rule mining method for the identification of rare functional dependencies in Complex Technical Infrastructures from alarm data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114560",
    "abstract": "This work presents a data-driven method for identifying rare functional dependencies among components of different systems of Complex Technical Infrastructures (CTIs) from large-scale databases of alarm messages. It is based on the representation of the alarm data in a binary form, the use of a novel association rule mining algorithm properly tailored for discovering rare dependencies among components of different systems and on the identification of groups of functionally dependent components. The proposed method is applied to a synthetic alarm database generated by a simulated CTI model and to a real large-scale database of alarms collected in the CTI of CERN (European Organization for Nuclear Research). The obtained results show the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000014",
    "keywords": [
      "ALARM",
      "Artificial intelligence",
      "Association rule learning",
      "Biology",
      "Botany",
      "Composite material",
      "Computer science",
      "Data mining",
      "False alarm",
      "Identification (biology)",
      "Law",
      "Materials science",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Representation (politics)",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Antonello",
        "given_name": "Federico"
      },
      {
        "surname": "Baraldi",
        "given_name": "Piero"
      },
      {
        "surname": "Shokry",
        "given_name": "Ahmed"
      },
      {
        "surname": "Zio",
        "given_name": "Enrico"
      },
      {
        "surname": "Gentile",
        "given_name": "Ugo"
      },
      {
        "surname": "Serio",
        "given_name": "Luigi"
      }
    ]
  },
  {
    "title": "Compression effects and scene details on the source camera identification of digital videos",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114515",
    "abstract": "The continuous growth of technologies like 4G or 5G has led to a massive use of mobile devices such as smartphones and tablets. This phenomenon, combined with the fact that people use mobile phones for a longer period of time, results in mobile phones becoming the main source of creation of visual information. However, its reliability as a true representation of reality cannot be taken for granted due to the constant increase in editing software. This makes it easier to alter original content without leaving a noticeable trace in the modification. Therefore, it is essential to introduce forensic analysis mechanisms to guarantee the authenticity or integrity of a certain digital video, particularly if it may be considered as evidence in legal proceedings. This paper explains the branch of multimedia forensic analysis that allows to determine the identification of the source of acquisition of a certain video by exploiting the unique traces left by the camera sensor of the mobile device in visual content. To do this, a technique that performs the identification of the source of acquisition of digital videos from mobile devices is presented. It involves 3 stages: (1) Extraction of the sensor fingerprint by applying the block-based technique. (2) Filtering the strong component of the PRNU signal to improve the quality of the sensor fingerprint. (3) Classification of digital videos in an open scenario, that is, where the forensic analyst does not need to have access to the device that recorded the video to find out the origin of the video. The main contribution of the proposed technique eliminates the details of the scene to improve the PRNU fingerprint. It should be noted that these techniques are applied to digital images and not to digital videos. In this work, we show that it is necessary to take this improvement into account to improve the identification of digital videos. Experimental results are also presented that support the validity of the techniques used and show promising results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311593",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Block (permutation group theory)",
      "Botany",
      "Computer science",
      "Computer vision",
      "Fingerprint (computing)",
      "Geometry",
      "Identification (biology)",
      "Law",
      "Linguistics",
      "Mathematics",
      "Mobile device",
      "Multimedia",
      "Operating system",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "TRACE (psycholinguistics)",
      "Tracing",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Ramos López",
        "given_name": "Raquel"
      },
      {
        "surname": "Sandoval Orozco",
        "given_name": "Ana Lucila"
      },
      {
        "surname": "García Villalba",
        "given_name": "Luis Javier"
      }
    ]
  },
  {
    "title": "Multi-criteria decision analysis towards robust service quality measurement",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114508",
    "abstract": "Importance The role of airports is critical for a region in which it is viewed as an engine for the economic development. Facilities, infrastructure, information and in general the services offered by an airport represent the fuel for this engine. Evidently, customers and travelers expect standard-quality services that need to be framed and measured. Therefore, services in airports should be quantified and maintained, accordingly. Objectives This article reports a case study for evaluating quality of services offered by five main airports located in Spain. Quality of service was modelled based on a number of factors such as convenience, comfort, courtesy of staffs, information visibility, prices, security, and transportation facilities. The grey based multi-criteria decision analysis (MCDA) was employed towards a reliable evaluation process by airport experts and to accommodate the several qualitative and conflicting evaluation factors with distinct definitions. To this end, Grey Step-wise Weight Assessment Ratio Analysis (SWARA-G) and grey Measurement of Alternatives and Ranking according to COmpromise Solution (MARCOS-G) methods were applied for quantifying relative weights of decision factors and rating airports, respectively. Several sensitivity analysis, simulations, and comparisons were conducted for verifying the preciseness of the revealed results. Findings Research findings demonstrate that the proposed SWARA-G-MARCOS-G-based methodology (i) enables decision makers to express their preferences clearly; and (ii) attenuates the embedded subjectivity and uncertainty within the decision-making process. In addition, they revealed that access to the parking and Wi-Fi connection are amongst the critical factors in evaluating service quality of airport. Contribution This paper contributes to related literature in presenting a novel decision-making approach for measuring service quality of airports, validated via a real-case study. The employed interval and linguistic grey variables allow experts, in airport operations, to express their opinions with higher flexibility and comfortability. The presented model could be re-applied for other studies or practical cases as a user-friendly decision support system.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311520",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Engineering",
      "Epistemology",
      "Marketing",
      "Multiple-criteria decision analysis",
      "Operating system",
      "Operations research",
      "Philosophy",
      "Process (computing)",
      "Quality (philosophy)",
      "Quality of service",
      "Ranking (information retrieval)",
      "Risk analysis (engineering)",
      "Service (business)",
      "Service quality",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Pamucar",
        "given_name": "Dragan"
      },
      {
        "surname": "Yazdani",
        "given_name": "Morteza"
      },
      {
        "surname": "Montero-Simo",
        "given_name": "María José"
      },
      {
        "surname": "Araque-Padilla",
        "given_name": "Rafael A."
      },
      {
        "surname": "Mohammed",
        "given_name": "Ahmed"
      }
    ]
  },
  {
    "title": "Lichtenberg algorithm: A novel hybrid physics-based meta-heuristic for global optimization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114522",
    "abstract": "This paper proposes a novel global optimization algorithm called Lichtenberg Algorithm (LA), inspired by the Lichtenberg figures patterns. Optimization is an essential tool to minimize or maximize functions, obtaining optimal results on costs, mass, energy, gains, among others. Actual problems may be multimodal, nonlinear, and discontinuous and may not be minimized by classical analytical methods that depend on the gradient. In this context there are metaheuristics algorithms inspired by natural phenomena to optimize real problems. There is no algorithm that is the worst or the best, but more efficient for a given type of problem. Thus, an unprecedented metaheuristic algorithm was created inspired by the physical phenomenon of radial intra-cloud lightning and Lichtenberg figures, successfully exploiting the fractal power and it is different from many in the literature as it is a hybrid algorithm composed of methods of search based on population and trajectory. Several test functions, including a design problem in a welded beam, were used to verify the robustness and to validate the Lichtenberg Algorithm. In all cases, the results were satisfactory when compared to those in the literature. LA shown to be a powerful optimization tool for both unconstraint optimizations and real problems with linear and nonlinear constraints.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311660",
    "keywords": [
      "Algorithm",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Gene",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Nonlinear system",
      "Physics",
      "Quantum mechanics",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Pereira",
        "given_name": "João Luiz Junho"
      },
      {
        "surname": "Francisco",
        "given_name": "Matheus Brendon"
      },
      {
        "surname": "Diniz",
        "given_name": "Camila Aparecida"
      },
      {
        "surname": "Antônio Oliver",
        "given_name": "Guilherme"
      },
      {
        "surname": "Cunha",
        "given_name": "Sebastião Simões"
      },
      {
        "surname": "Gomes",
        "given_name": "Guilherme Ferreira"
      }
    ]
  },
  {
    "title": "Home location prediction with telecom data: Benchmarking heuristics with a predictive modelling approach.",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114507",
    "abstract": "Correctly identifying the home location is crucial for human mobility analysis with telecom data, more specifically call detail record (CDR) data. To that end, multiple heuristics have been developed in literature. Nevertheless, due to the lack of ground truth home location data, no study has thoroughly validated these widely used methods so far. We present a detailed performance analysis of existing home detection heuristics, using a unique dataset that enables this important validation on the lowest level, being the level of the cell tower. Our research indicates that simple heuristics surprisingly outperform their more complex counterparts. The benchmark study revealed that the best heuristic is able to identify the home location with an average error of approximately 4.5 km and selects the correct home tower in 60.69% of the cases. Based on the insights provided by our study, we propose a new heuristic that increases the accuracy to 61% and lowers the average distance error to 4.365 km. Secondly, if the home location is known for possibly only a fraction of the instances, we propose a labelled predictive modelling approach. Adding social network based variables in this predictive model further enhances the predictive performance. Our best model reduces the average distance error to 2.848 km and selects the correct home location in 72.08% of the cases. Furthermore, this result provides an indication of the upper bound for home detection with CDR data. Finally, models that only make use of social network based data are developed as well. Results show that even without using data of the focal individual, these models are able to select the correct home tower in 37.65% of the cases and achieve an average distance error of 8.1 km.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311519",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Benchmarking",
      "Business",
      "Computer science",
      "Data mining",
      "Geodesy",
      "Geography",
      "Ground truth",
      "Heuristic",
      "Heuristics",
      "Machine learning",
      "Marketing",
      "Operating system"
    ],
    "authors": [
      {
        "surname": "Oosterlinck",
        "given_name": "Dieter"
      },
      {
        "surname": "Baecke",
        "given_name": "Philippe"
      },
      {
        "surname": "Benoit",
        "given_name": "Dries F."
      }
    ]
  },
  {
    "title": "Interpretable vs. noninterpretable machine learning models for data-driven hydro-climatological process modeling",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114498",
    "abstract": "Due to their enhanced predictive capabilities, noninterpretable machine learning (ML) models (e.g. deep learning) have recently gained a growing interest in analyzing and modeling earth & planetary science data. However, noninterpretable ML models are often treated as “black boxes” by end-users, which could limit their applicability in critical decision making processes. In this paper, we compared the predictive capabilities of three interpretable ML models with three noninterpretable ML models to answer the overarching question: Is it essential to use noninterpretable ML models for enhanced model predictions from hydro-climatological datasets? The ML model development and comparative analysis were performed using measured climate data and synthetic reference crop evapotranspiration ( ET o ) data, with varying levels of missing values, from five weather stations across the karstic Edwards aquifer region in semi-arid south-central Texas. Our analysis revealed that interpretable tree-based ensemble models produce comparable results to noninterpretable deep learning models on structured hydro-climatological datasets. We showed that the tree-based ensemble model is also capable of imputing varying levels of missing climate data at the weather stations, employing the newly developed sequential transfer-learning technique. We applied an explainable machine learning (eXML) framework to quantify the global order of importance of hydro-climatic (predictor) variables on ET o , while highlighting the local dependencies and interactions amongst the predictors and ET o . The eXML framework also revealed the inflection points of the climate variables at which the transition from low to high daily ET o rates occur. The ancillary explainability of ML models are expected to increase users’ confidence and support any future decision-making process in water resource management.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311428",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Data mining",
      "Decision tree",
      "Ecology",
      "Ensemble forecasting",
      "Ensemble learning",
      "Evapotranspiration",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Missing data",
      "Random forest",
      "Support vector machine",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Chakraborty",
        "given_name": "Debaditya"
      },
      {
        "surname": "Başağaoğlu",
        "given_name": "Hakan"
      },
      {
        "surname": "Winterle",
        "given_name": "James"
      }
    ]
  },
  {
    "title": "ERV-Net: An efficient 3D residual neural network for brain tumor segmentation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114566",
    "abstract": "Brain tumors are the most aggressive and mortal cancers, which lead to short life expectancy. A reliable and efficient automatic or semi-automatic segmentation method is significant for clinical practice. In recent years, deep learning-based methods achieve great success in brain tumor segmentation. However, due to the limitation of parameters and computational complexity, there is still much room for improvement in these methods. In this paper, we propose an efficient 3D residual neural network (ERV-Net) for brain tumor segmentation, which has less computational complexity and GPU memory consumption. In ERV-Net, a computation-efficient network, 3D ShuffleNetV2, is firstly utilized as encoder to reduce GPU memory and improve the efficiency of ERV-Net, and then the decoder with residual blocks (Res-decoder) is introduced to avoid degradation. Furthermore, a fusion loss function, which is composed of Dice loss and Cross-entropy loss, is developed to solve the problems of network convergence and data imbalance. Moreover, a concise and effective post-processing method is proposed to refine the coarse segmentation result of ERV-Net. The experimental results on the dataset of multimodal brain tumor segmentation challenge 2018 (BRATS 2018) demonstrate that ERV-Net achieves the best performance with Dice of 81.8%, 91.21% and 86.62% and Hausdorff distance of 2.70 mm, 3.88 mm and 6.79 mm for enhancing tumor, whole tumor and tumor core, respectively. Besides, ERV-Net also achieves high efficiency compared to the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000075",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolutional neural network",
      "Cross entropy",
      "Deep learning",
      "Dice",
      "Geometry",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Residual",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Xinyu"
      },
      {
        "surname": "Li",
        "given_name": "Xuanya"
      },
      {
        "surname": "Hu",
        "given_name": "Kai"
      },
      {
        "surname": "Zhang",
        "given_name": "Yuan"
      },
      {
        "surname": "Chen",
        "given_name": "Zhineng"
      },
      {
        "surname": "Gao",
        "given_name": "Xieping"
      }
    ]
  },
  {
    "title": "Home location prediction with telecom data: Benchmarking heuristics with a predictive modelling approach.",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114507",
    "abstract": "Correctly identifying the home location is crucial for human mobility analysis with telecom data, more specifically call detail record (CDR) data. To that end, multiple heuristics have been developed in literature. Nevertheless, due to the lack of ground truth home location data, no study has thoroughly validated these widely used methods so far. We present a detailed performance analysis of existing home detection heuristics, using a unique dataset that enables this important validation on the lowest level, being the level of the cell tower. Our research indicates that simple heuristics surprisingly outperform their more complex counterparts. The benchmark study revealed that the best heuristic is able to identify the home location with an average error of approximately 4.5 km and selects the correct home tower in 60.69% of the cases. Based on the insights provided by our study, we propose a new heuristic that increases the accuracy to 61% and lowers the average distance error to 4.365 km. Secondly, if the home location is known for possibly only a fraction of the instances, we propose a labelled predictive modelling approach. Adding social network based variables in this predictive model further enhances the predictive performance. Our best model reduces the average distance error to 2.848 km and selects the correct home location in 72.08% of the cases. Furthermore, this result provides an indication of the upper bound for home detection with CDR data. Finally, models that only make use of social network based data are developed as well. Results show that even without using data of the focal individual, these models are able to select the correct home tower in 37.65% of the cases and achieve an average distance error of 8.1 km.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311519",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Benchmarking",
      "Business",
      "Computer science",
      "Data mining",
      "Geodesy",
      "Geography",
      "Ground truth",
      "Heuristic",
      "Heuristics",
      "Machine learning",
      "Marketing",
      "Operating system"
    ],
    "authors": [
      {
        "surname": "Oosterlinck",
        "given_name": "Dieter"
      },
      {
        "surname": "Baecke",
        "given_name": "Philippe"
      },
      {
        "surname": "Benoit",
        "given_name": "Dries F."
      }
    ]
  },
  {
    "title": "Optical-flow-based framework to boost video object detection performance with object enhancement",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114544",
    "abstract": "With image object detection techniques progressing, video object detection attracts a lot of attention more than ever before. However, the performance of detecting the object from a video suffers a lot as object occlusion can always lead to the object in the presence of appearance deterioration. To deal with this problem, the temporal correlation of video sequences is often used to reduce the effects of object occlusion in video object detection. In this paper, an optical-flow-feature fusion-based video object detection method is proposed with consideration of temporal coherence among video frames. To further reduce computational complexity, this paper also proposes a packet video processing method. Specifically, video frames are grouped first, and all frames in current group share the same optical flow feature map by feature fusion. Then the Target Image can be formed with object information enhanced by fusing the shared feature map with current frame. The proposed method gives rise to effective background information masking, so that the object detection network can focus more on the foreground object. This method effectively improves the object detection performance and the scene migration performance. Experimental results prove that the proposed method significantly improves the detection accuracy to 78.6% on ImageNet VID. In addition, VGG and ResNet are compared to further verify the effectiveness of the proposed method, and the results can be a persuasive evidence with the highest detection accuracy and acceptable time consumption.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742031188X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Face detection",
      "Facial recognition system",
      "Feature (linguistics)",
      "Feature extraction",
      "Image (mathematics)",
      "Linguistics",
      "Object (grammar)",
      "Object detection",
      "Object-class detection",
      "Optical flow",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Video tracking",
      "Viola–Jones object detection framework"
    ],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "Long"
      },
      {
        "surname": "Zhang",
        "given_name": "Tao"
      },
      {
        "surname": "Du",
        "given_name": "Wenli"
      }
    ]
  },
  {
    "title": "Collaborative logistics pickup and delivery problem with eco-packages based on time–space network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114561",
    "abstract": "Collaboration among logistics companies offers a simple and effective way of increasing logistics operation efficiency. This study designs an optimal collaboration strategy by solving the collaborative logistics pickup and delivery problem with eco-packages (CLPDPE). This problem seeks to minimize the total operational costs by forming collaborative alliances and allocating trucking resources based on time–space (TS) network properties. The synchronization of two-echelon logistics networks is improved by solving this problem. Moreover, this study considers the stability of collaboration (i.e., the willingness of logistics companies to join and remain in collaborative alliances) by comparing different profit allocation strategies in the CLPDPE solving process. A novel methodology that combines multi-objective mixed integer programming, multidimensional K-means clustering, reference point based non-dominated sorting genetic algorithm-II (RP-NSGA-II), forward dynamic programming and improved Shapley value method is developed to formulate and solve CLPDPE. Our results show that the proposed algorithm outperforms most other algorithms in minimizing the total cost, waiting time and number of vehicles. An empirical case study in Chongqing city, China suggests that the proposed collaborative mechanism and transportation resource sharing strategy based on TS network can reduce cost, improve distribution efficiency, and contribute to efficient, smart, intelligent and sustainable urban logistics and transportation systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000026",
    "keywords": [
      "Computer science",
      "Economics",
      "Engineering",
      "Game theory",
      "Mathematical optimization",
      "Mathematics",
      "Microeconomics",
      "Operations research",
      "Programming language",
      "Shapley value",
      "Sorting"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yong"
      },
      {
        "surname": "Peng",
        "given_name": "Shouguo"
      },
      {
        "surname": "Guan",
        "given_name": "Xiangyang"
      },
      {
        "surname": "Fan",
        "given_name": "Jianxin"
      },
      {
        "surname": "Wang",
        "given_name": "Zheng"
      },
      {
        "surname": "Liu",
        "given_name": "Yong"
      },
      {
        "surname": "Wang",
        "given_name": "Haizhong"
      }
    ]
  },
  {
    "title": "IF2CNN: Towards non-stationary time series feature extraction by integrating iterative filtering and convolutional neural networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114527",
    "abstract": "Time series is a common data type that appears in various fields. However, time series processing is still considered one of the most challenging problems in data mining because of its unique properties, such as noise and non-stationarity. In an effort to overcome these limitations, we present a framework, called IF2CNN, that integrates the iterative filtering (IF) method and convolutional neural networks (CNNs) for automatic feature learning for time series. First, IF is leveraged to decompose the raw non-stationary time series into intrinsic mode functions (IMFs), which are then converted into image format data. Second, CNN is designed to automatically learn features from the image format data, which can help to extract deep and global features of the time series. Besides, the use of IF and CNN technologies makes the proposed framework not only have the advantage of dealing with the non-stationarity of the time series, but also provides a good generalization ability for small training datasets. To evaluate the performance of IF2CNN, two different strategies are used to test the role of the derived features of CNN (called CNN features). The first strategy computes the feature importance through the methods based on decision trees, such as gradient boosting decision trees and random forest, and the other one tests the validity of the derived features by performing specific prediction tasks. Furthermore, three real datasets from different fields are used in our experiments. The results show that the CNN features have an overwhelming advantage in feature importance and significant improvements in prediction tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311714",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Boosting (machine learning)",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Feature (linguistics)",
      "Feature extraction",
      "Generalization",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Series (stratigraphy)",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Feng"
      },
      {
        "surname": "Zhou",
        "given_name": "Haomin"
      },
      {
        "surname": "Yang",
        "given_name": "Zhihua"
      },
      {
        "surname": "Gu",
        "given_name": "Linyan"
      }
    ]
  },
  {
    "title": "A probabilistic linguistic evaluation-based multi-stage medical scheme selection process related to referral system",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114523",
    "abstract": "This paper provides a multi-stage medical scheme selection process to obtain the suitable scheme in the referral system. Firstly, considering the uncertainty of the experts and physicians, the preference relations (PRs) under the probabilistic linguistic term environment are introduced to the system. Then, this paper checks the consistency of the PRs and selects out the inconsistent PRs. For the inconsistent one, this paper provides alternative evaluations by repairing the most inconsistent element. Then, let experts choose from the alternative evaluation set. Next, by introducing the Bonferroni mean (BM) operator and Choquet integral, this paper integrates the historical data and the evaluations with stage weights to obtain the comprehensive assessment. The weights are calculated by the provided discrete and continuous stage weight functions or the programming model. Finally, the medical scheme selection process for the lung cancer and its simulation experiment are investigated to demonstrate the effectiveness and application of provided methods. The simulation experiments on sensitivity analysis and comparative analysis with existing methods are also conducted to validate the provided method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311672",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Choquet integral",
      "Computer science",
      "Consistency (knowledge bases)",
      "Data mining",
      "Fuzzy logic",
      "Gene",
      "Machine learning",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Operator (biology)",
      "Probabilistic logic",
      "Process (computing)",
      "Programming language",
      "Repressor",
      "Scheme (mathematics)",
      "Selection (genetic algorithm)",
      "Set (abstract data type)",
      "Transcription factor"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Bo"
      },
      {
        "surname": "Xu",
        "given_name": "Zeshui"
      },
      {
        "surname": "Zhang",
        "given_name": "Yixin"
      }
    ]
  },
  {
    "title": "CNN with depthwise separable convolutions and combined kernels for rating prediction",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114528",
    "abstract": "Recently, deep learning based techniques exploiting reviews are extensively studied for rating prediction and result in good performance. Some studies consider word level review information along with attention mechanism to capture the most influential content, thus making the methods even complex. Deep neural networks with Depthwise Separable Convolutions have made significant progress in the area of image and video analysis. The success of these techniques encourages adopting them towards improvements in rating prediction using review text. In this paper, we present a novel CNN based architecture with Depthwise Separable Convolutions and Combined Kernels (CNN-DSCK) for rating prediction exploiting product reviews. In the proposed method, we use two parallel CNNs with Depthwise Separable Convolutions to extract semantic features from the text reviews of users and items using different kernels in parallel and then select the important information from these features through pooling. Finally concatenate the pooling information obtained from different kernels in each network. The features obtained through each network are then fused and the most relevant higher-order features are extracted through fully connected dense layer at the top of network. Extensive experiments on real-world datasets demonstrate that CNN-DSCK significantly outperforms state of the art baseline models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311726",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Data mining",
      "Deep learning",
      "Geometry",
      "Layer (electronics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Pooling",
      "Separable space",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Khan",
        "given_name": "Zahid Younas"
      },
      {
        "surname": "Niu",
        "given_name": "Zhendong"
      }
    ]
  },
  {
    "title": "A 6-DOFs event-based camera relocalization system by CNN-LSTM and image denoising",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114535",
    "abstract": "At present, in the research of simultaneous localization and mapping systems, many traditional relocalization methods have been replaced by camera relocalization techniques based on convolutional neural network (CNN) and long and short-term memory (LSTM). However, in a system using an event dataset to train the neural network, the complex scenes are chaotic, and the noise of the event images is excessive. Both issues make the model unable to return to the six-degrees of freedom (6-DOFs) pose well. This paper proposes a 6-DOFs pose camera relocalization method based on the CNN image denoising model and CNN-LSTM. Firstly, the CNN image denoising model is used to solve the problem of excessive noise points in complex scenes. Then, a network framework combining CNN and LSTM trains the event camera relocalization model to obtain better 6-DOFs pose accuracy. Finally, the study performs experimental simulations by using complex scene datasets without and with denoising images. Experimental results show that the proposed method of camera relocalization has many advantages. It enhances the robustness of the model in the training process, reduces the mutation situation, and the trained model has a smaller error and faster speed when predicting the pose, thus improving the accuracy and real-time of the camera relocalization model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311799",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Event (particle physics)",
      "Gene",
      "Image (mathematics)",
      "Noise (video)",
      "Noise reduction",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Jin",
        "given_name": "Yifan"
      },
      {
        "surname": "Yu",
        "given_name": "Lei"
      },
      {
        "surname": "Li",
        "given_name": "Guangqiang"
      },
      {
        "surname": "Fei",
        "given_name": "Shumin"
      }
    ]
  },
  {
    "title": "Solving continuous optimization problems using the tree seed algorithm developed with the roulette wheel strategy",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114579",
    "abstract": "The Tree seed algorithm (TSA) is a metaheuristic algorithm inspired by the relationship between trees and seeds. It has been proposed for very low-dimensional optimization problems and achieved promising results compared to other optimization algorithms. However, it has been determined that the performance of the TSA is lower than other algorithms for high-dimensional problems. This is due to the fact that TSA cannot scan the local optimum and search space effectively. A new TSA based on the roulette wheel strategy (R-TSA) has been proposed in this study to eliminate this disadvantage and solve high-dimensional problems. With this strategy, the trees selected at the seed production phase of TSA were diversified and the locations of the seeds were updated to prevent it from being stuck in local minima, with the aim of scanning the search space more effectively. The R-TSA was applied to high-dimensional (20, 50 and 100) benchmark functions and both convergence and box-plot graphs were obtained by using the results of these functions. Moreover, current algorithms in published literature were applied to these functions and the results obtained were compared with the R-TSA. It was observed from the analysis results that the performance of the R-TSA was higher than that of TSA.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417421000208",
    "keywords": [
      "Algorithm",
      "Benchmark (surveying)",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Geodesy",
      "Geography",
      "Geometry",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Maxima and minima",
      "Metaheuristic",
      "Plot (graphics)",
      "Roulette",
      "Statistics",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Beşkirli",
        "given_name": "Mehmet"
      }
    ]
  },
  {
    "title": "Digraph and matrix approach for risk evaluations under Pythagorean fuzzy information",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114518",
    "abstract": "Failure Mode and Effects Analysis (FMEA) is an effective framework that is extensively utilized to determine and eradicate the possible failures from substances, structures, designs, services or organizations. The traditional risk evaluating methods have been criticized because of reasons that include disregard for correlative significance of risk factors, complicated multiplications, and lack of exactness and accuracy in the evaluations. In this paper, the effectiveness of traditional techniques is improved by a novel approach to analyze risks in FMEA that makes use of digraphs and matrix techniques under the Pythagorean fuzzy environment. We begin by defining triangular Pythagorean fuzzy numbers. We use them to express both the linguistic terms and all other data and information regarding the risk factors (inclusive of occurrence, severity, and detection). Then a Pythagorean fuzzy digraph captures the interrelations connecting the risk factors and the relative significance among them. Finally, we form the corresponding Pythagorean fuzzy risk matrices for every identified failure mode, and we calculate the risk priority indexes in order to identify the risk priorities. We also study an example of a steam valve system in a power generating plant as an illustrative application of our model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311623",
    "keywords": [
      "Artificial intelligence",
      "Composite material",
      "Computer science",
      "Data mining",
      "Digraph",
      "Discrete mathematics",
      "Engineering",
      "Failure mode and effects analysis",
      "Fuzzy logic",
      "Geometry",
      "Materials science",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Medicine",
      "Pythagorean theorem",
      "Reliability engineering",
      "Risk analysis (engineering)"
    ],
    "authors": [
      {
        "surname": "Luqman",
        "given_name": "Anam"
      },
      {
        "surname": "Akram",
        "given_name": "Muhammad"
      },
      {
        "surname": "Alcantud",
        "given_name": "José Carlos R."
      }
    ]
  },
  {
    "title": "A fusion-domain color image watermarking based on Haar transform and image correction",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114540",
    "abstract": "The leapfrog development of computer technology has greatly enhanced the breadth of information dissemination. As a larger information carrier, color image becomes more and more popular, but the copyright protection problem becomes more and more serious. To solve this problem, this paper proposes a fusion-domain color watermarking based on Haar transform and image correction. Firstly, the maximum energy coefficient of Haar transform is directly obtained in spatial domain. Then, the coefficient is quantified with the help of variable quantization steps to embed the color watermark that encrypted by affine transform. If the watermarked image is processed by geometric attack, then the attacked image can be corrected by using of the geometric properties. Finally, the inverse embedding process is performed to extract the watermark. The performances of the proposed method are shown as follows: 1) all PSNR (Peak Signal-to-Noise Ratio) values are greater than 40 dB; 2) all SSIM (Structural Similarity Index Metric) values are greater than 0.96; 3) most NC (Normalized Cross-correlation) values are more than 0.9; 4) the key space is more than 2432; 5) the maximum embedded capacity is 0.25bpp; 6) the running time is about 6 s. Compared with the related methods, the experimental results show that the proposed method is feasible and has good performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311842",
    "keywords": [
      "Affine transformation",
      "Algorithm",
      "Artificial intelligence",
      "Color image",
      "Computer science",
      "Computer vision",
      "Digital watermarking",
      "Discrete wavelet transform",
      "Haar wavelet",
      "Image (mathematics)",
      "Image fusion",
      "Image processing",
      "Mathematics",
      "Peak signal-to-noise ratio",
      "Pure mathematics",
      "Watermark",
      "Wavelet",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Decheng"
      },
      {
        "surname": "Su",
        "given_name": "Qingtang"
      },
      {
        "surname": "Yuan",
        "given_name": "Zihan"
      },
      {
        "surname": "Zhang",
        "given_name": "Xueting"
      }
    ]
  },
  {
    "title": "Bi-objective traffic count location model for mean and covariance of origin–destination estimation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114554",
    "abstract": "This paper describes a bi-objective optimization model for the traffic count location problem in stochastic origin–destination (OD) traffic demand estimation. Two measures are defined to capture the maximum possible absolute error of the mean and the covariance of the estimated OD demand. The bounds of these two measures are mathematically deduced, and then the bi-objective optimization model is formulated to minimize the two upper bounds simultaneously. A surrogate-assisted genetic algorithm is proposed to solve this model, and a series of numerical examples are presented to demonstrate the applicability of the proposed model and the efficiency of the proposed algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311982",
    "keywords": [
      "Algorithm",
      "Biology",
      "Computer science",
      "Covariance",
      "Genetic algorithm",
      "Location model",
      "Mathematical optimization",
      "Mathematics",
      "Mean squared error",
      "Operations research",
      "Paleontology",
      "Series (stratigraphy)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Weiwei"
      },
      {
        "surname": "Shao",
        "given_name": "Hu"
      },
      {
        "surname": "Shen",
        "given_name": "Liang"
      },
      {
        "surname": "Wu",
        "given_name": "Ting"
      },
      {
        "surname": "Lam",
        "given_name": "William H.K."
      },
      {
        "surname": "Yao",
        "given_name": "Baozhen"
      },
      {
        "surname": "Yu",
        "given_name": "Bin"
      }
    ]
  },
  {
    "title": "Stratified opposition-based initialization for variable-length chromosome shortest path problem evolutionary algorithms",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114525",
    "abstract": "Initialization is the first and a major step in the implementation of evolutionary algorithms (EAs). Although there are many common general methods to initialize EAs such as the pseudo-random number generator (PRNG), there is no single method that can fit every problem. This study provides a new, flexible, diversity-aware, and easy-to-implement initialization method for a genetic algorithm for the shortest path problem. The proposed algorithm, called stratified opposition-based sampling (SOBS), considers phenotype and genotype diversity while striving to achieve the best fitness for the initialization population. SOBS does not depend on a specific type of sampling, because the main goal is to stratify the sampling space. SOBS aims at an initial population with higher fitness and diversity in the phenotype and genotype. To investigate the performance of SOBS, four network models were used to simulate real-world networks. Compared with the most frequently used initialization method, that is, PRNG, SOBS provides more accurate solutions, better running time with less memory usage, and an initial population with higher fitness. Statistical analysis showed that SOBS yields solutions with higher accuracy in 68–100% of the time. Although this study was focused on the genetic algorithm, it can be applied to other population-based EAs that solve the shortest path problem and use the same direct population representation such as particle swarm optimization (PSO).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311696",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Demography",
      "Evolutionary algorithm",
      "Genetic algorithm",
      "Graph",
      "Initialization",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Particle swarm optimization",
      "Population",
      "Programming language",
      "Shortest path problem",
      "Sociology",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Ghannami",
        "given_name": "Aiman"
      },
      {
        "surname": "Li",
        "given_name": "Jing"
      },
      {
        "surname": "Hawbani",
        "given_name": "Ammar"
      },
      {
        "surname": "Al-Dubai",
        "given_name": "Ahmed"
      }
    ]
  },
  {
    "title": "Evolving fuzzy reasoning approach using a novel nature-inspired optimization tool",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2021.114577",
    "abstract": "In general, fuzzy reasoning tool with Mamdani approach has good readability, but low accuracy; whereas the same with Takagi and Sugeno’s approach ensures high accuracy but at the cost of readability. In the developed combined form of fuzzy reasoning, the merits of both the above two approaches are utilized to obtain both the high accuracy as well as good readability. The above combined form is evolved using a recently-developed nature-inspired technique, namely Bonobo Optimizer (BO). This optimization method mimics the fission-fusion social structure and reproductive schemes adopted by bonobos. In addition, controlling parameters of the BO are designed to be adaptive and self-adjusting to perform efficiently for a variety of problems. The performances of the developed models have been tested and compared with that of the combined fuzzy reasoning tool evolved using Genetic Algorithm (GA), Particle Swarm Optimization (PSO), Grey-Wolf Optimizer (GWO) and Jaya Algorithm for three data sets. The novelty of this study lies with the ability of the recently proposed BO to evolve an efficient fuzzy reasoning approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742100018X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Fuzzy logic",
      "Machine learning"
    ],
    "authors": [
      {
        "surname": "Das",
        "given_name": "Amit Kumar"
      },
      {
        "surname": "Pratihar",
        "given_name": "Bitan"
      },
      {
        "surname": "Pratihar",
        "given_name": "Dilip Kumar"
      }
    ]
  },
  {
    "title": "CGenProg: Adaptation of cartesian genetic programming with migration and opposite guesses for automatic repair of software regression faults",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114503",
    "abstract": "In the last decade, the research community has been actively working to develop the techniques that can automatically find a solution to a software fault, namely, automatic program repair (APR). As of today, a multitude of APR techniques has been proposed. The techniques could have effectively repaired a wide variety of fault classes. However, the development of effective APR techniques for software regression faults, which are prevalently occurred in the maintenance stage of the software lifecycle, have received little attention. By incorporating specific knowledge in the domain of software regression faults, we have developed a novel technique for automatic repair of software regression faults in Java programs, which we call CGenProg. To achieve this, we have extensively adapted and modified the original cartesian genetic programming (CGP), biogeography-based optimization (BBO), and opposition-based learning (OBL). The modified CGP serves us as the core evolutionary process while the modified BBO and OBL act as crossover and mutation, respectively. The significance of CGenProg is that it contributes to the solution of a practical problem faced by developers in the maintenance stage of the software lifecycle. For expert and intelligent systems, it extends what is known about the application of optimization algorithms in the context of APR. Further, it demonstrates a novel use of CGP, BBO, and OBL for automatic repair of software regression faults. To evaluate CGenProg, we have developed a prototype tool using the Java language. Then, we conducted experiments on several programs in Code4Bench where each program is released with multiple consecutive versions comprising software regression faults. In the experiments, CGenProg could repair 17 out of 30 faulty programs. We conclude that CGenProg proves relevant and effective for repairing software regression faults. The impact of this study is to incentivize researchers for further exploitation and adaptation of the wealth of existing metaheuristics to develop effective APR techniques for different fault classes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311477",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Crossover",
      "Data mining",
      "Genetic programming",
      "Java",
      "Machine learning",
      "Paleontology",
      "Programming language",
      "Regression testing",
      "Software",
      "Software construction",
      "Software development",
      "Software engineering",
      "Software regression",
      "Symbolic regression"
    ],
    "authors": [
      {
        "surname": "Khalilian",
        "given_name": "Alireza"
      },
      {
        "surname": "Baraani-Dastjerdi",
        "given_name": "Ahmad"
      },
      {
        "surname": "Zamani",
        "given_name": "Bahman"
      }
    ]
  },
  {
    "title": "Separation linearization approach for the capacitated facility location problem under disruption",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114187",
    "abstract": "Facility location problems (FLP) are often solved as uncapacitated facility location (UFL) instances. Also, typical solution approaches in the literature assume that the established facilities are totally reliable. However, in practice, facilities have limited capacity and can be under risk of partial disruptions whereby their failure leads to a notably higher cost. In this context, this paper presents a novel integer programming formulation for the capacitated FLP under disruption, namely the reliable capacitated facility location (RCFL) problem. The latter considers heterogeneous facility failure probabilities, one layer of backup for supply locations, limited supply capacity and facility fortification within a limited budget to mitigate failure risk. The proposed solution approach involves a linearization of the proposed model and an iterative approach for the fortification budget allocation in conjunction with the CPLEX solver. Moreover, a relevant case study is used to illustrate the approach and benchmark result are also provided.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309210",
    "keywords": [
      "Backup",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Database",
      "Engineering",
      "Facility location problem",
      "Geodesy",
      "Geography",
      "Integer programming",
      "Linear programming",
      "Linearization",
      "Location model",
      "Mathematical optimization",
      "Mathematics",
      "Nonlinear system",
      "Operations research",
      "Paleontology",
      "Physics",
      "Quantum mechanics",
      "Reliability engineering",
      "Solver",
      "Unavailability"
    ],
    "authors": [
      {
        "surname": "Afify",
        "given_name": "Badr"
      },
      {
        "surname": "Soeanu",
        "given_name": "Andrei"
      },
      {
        "surname": "Awasthi",
        "given_name": "Anjali"
      }
    ]
  },
  {
    "title": "Criteria determination of analytic hierarchy process using a topic model",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114306",
    "abstract": "The purpose of this paper is to develop a technology-based model for identifying various criteria in a decision-making situation. We used topic modeling to discover critical criteria and their corresponding weights in the Analytic Hierarchy Process (AHP). Approximately 100,000 hotel reviews and 100,000 restaurant reviews were scraped from TripAdvisor.com for criteria determination. Next, an AHP model with criteria and 12 hotels/restaurants as alternatives were compared and ranked. The results compared favorably with more than 1000 reviews of these hotels/restaurants in TripAdvisor.com, thus validating the methodology.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310046",
    "keywords": [
      "Analytic hierarchy process",
      "Analytic network process",
      "Computer science",
      "Economics",
      "Hierarchy",
      "Market economy",
      "Mathematics",
      "Operating system",
      "Operations research",
      "Process (computing)"
    ],
    "authors": [
      {
        "surname": "Fang",
        "given_name": "Jin"
      },
      {
        "surname": "Partovi",
        "given_name": "Fariborz Y."
      }
    ]
  },
  {
    "title": "Batch recommendation of experts to questions in community-based question-answering with a sailfish optimizer",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114484",
    "abstract": "To facilitate question-answering in community-based question-answering (CQA), this paper proposes an approach for the batch recommendation of answerers by optimizing the utilization of expert resources. First, questions and experts are modeled with a biterm topic model (BTM). Next, the answered questions are clustered based on a novel discrete sailfish optimizer (SFO) with a genetic algorithm (GA), and the topic distribution is obtained. Then, experts are ranked in each cluster based on activeness, recency, and professionalism. Considering the limited number of experts, to ensure that core questions are answered and to avoid repeated answers to similar or duplicate questions, coverage, answerability and the consumption of expert resources are taken as objects to be optimized. This scenario is formulated as a multiobjective optimization problem and is addressed by the proposed novel binary multiobjective SFO (MOSFO) with a GA. The solution of the model includes not only the selected questions to be answered but also the matching between the questions and experts. The proposed approach is evaluated with a real dataset, and the experimental results show that the proposed approach is feasible and has superior performance to the question-priority method, the expert-priority method and other swarm intelligence (SI) methods. This study is the first to make batch recommendations, providing a new idea and extending research on expert recommendation. Additionally, the approach can be used practically to improve the satisfaction of the knowledge needs of users by improving the answerability of high-coverage questions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311313",
    "keywords": [
      "Computer science",
      "Data mining",
      "Data science",
      "Information retrieval",
      "Matching (statistics)",
      "Mathematics",
      "Question answering",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Ming"
      },
      {
        "surname": "Li",
        "given_name": "Ying"
      },
      {
        "surname": "Chen",
        "given_name": "Yueyun"
      },
      {
        "surname": "Xu",
        "given_name": "Yingcheng"
      }
    ]
  },
  {
    "title": "Outer product enhanced heterogeneous information network embedding for recommendation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114359",
    "abstract": "With the rapid development of the internet, more and more sophisticated data can be utilized by recommendation systems to improve their performance. Such data consist of heterogeneous information networks (HINs) made up of multiple nodes and link types. A critical challenge is how to effectively extract and apply the useful HIN information. In particular, the embedding-based recommendation approach has been widely used, as it can extract affluent semantic and structural information from HINs. However, the existing HIN embedding for recommendation methods only combine user embedding and item embedding through a simple concatenation or elementwise product, which does not suffer for an efficient recommendation model. In order to extract and utilize more comprehensive and subtle information from the embedding for recommendation, we propose Outer Product Enhanced Heterogeneous Information Network Embedding for Recommendation, called HopRec. The main idea is to utilize the outer product to model the pairwise relationship between user HIN embedding and item HIN embedding. Specifically, by performing an outer product between user HIN embedding and item HIN embedding, we can obtain a two-dimensional interaction matrix. Subsequently, we can obtain a rating prediction function by integrating matrix factorization (MF), user HIN embedding, item HIN embedding and interaction matrix. The results of experiments conducted on three open benchmark datasets show that HopRec significantly outperforms the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742031040X",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Combinatorics",
      "Computer science",
      "Concatenation (mathematics)",
      "Data mining",
      "Embedding",
      "Geodesy",
      "Geography",
      "Geometry",
      "Information retrieval",
      "Mathematics",
      "Pairwise comparison",
      "Product (mathematics)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Yunfei"
      },
      {
        "surname": "Zhang",
        "given_name": "Yiwen"
      },
      {
        "surname": "Qi",
        "given_name": "Lianyong"
      },
      {
        "surname": "Yan",
        "given_name": "Dengcheng"
      },
      {
        "surname": "He",
        "given_name": "Qiang"
      }
    ]
  },
  {
    "title": "Understanding panic buying during COVID-19: A text analytics approach",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114360",
    "abstract": "An area of consumer behaviour that caught retailers and supply chains unprepared during the initial stages of the COVID-19 pandemic was the increased prevalence of the purchase of utilitarian goods – referred to in the media as “panic buying.” In this study, we take a novel approach to understanding such panic buying during the pandemic using compensatory control theory (CCT), text analytics, and advanced data modelling. Using a big data set over 14 days from 24,153 Twitter users in Italy, we create dictionaries to capture CCT constructs and note the dates of two government announcements. We measure constructs in the longitudinal data and test the CCT model using generalized linear mixed models for both fixed effects and random variation across individuals and time. The results support CCT, with anxiety driving a lack of perceived control, moderated by effective government announcements, and a lack of perceived control leading to purchasing, negatively moderated by utilitarian qualities. The study demonstrates the benefit of the methods for studying social phenomena and for early warning of potential demand issues via social media.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310411",
    "keywords": [
      "Anxiety",
      "Artificial intelligence",
      "Big data",
      "Business",
      "Computer science",
      "Control (management)",
      "Coronavirus disease 2019 (COVID-19)",
      "Data mining",
      "Disease",
      "Government (linguistics)",
      "Infectious disease (medical specialty)",
      "Linguistics",
      "Longitudinal data",
      "Marketing",
      "Medicine",
      "Pandemic",
      "Panic",
      "Pathology",
      "Philosophy",
      "Programming language",
      "Psychiatry",
      "Psychology",
      "Purchasing",
      "Set (abstract data type)",
      "Social media",
      "Social media analytics",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Barnes",
        "given_name": "Stuart J."
      },
      {
        "surname": "Diaz",
        "given_name": "Melisa"
      },
      {
        "surname": "Arnaboldi",
        "given_name": "Michela"
      }
    ]
  },
  {
    "title": "TCD2: Tree-based community detection in dynamic social networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114493",
    "abstract": "Community detection in social networks is an important field of research in data mining and has an abundant literature. Time varying social networks require algorithms that can comply with temporal changes and are also feasible with limited resources. The performance of static algorithms are not well suited for such perturbing networks. Continuously updating community structure, light computations, on-demand results etc. are few of the new challenges introduced on account of dynamic networks. The aforementioned challenges are addressed in the proposed work. The work proposes a tree-based community detection in dynamic social networks (TCD2) algorithm which exploits two important properties of social network, connectedness and influence, for finding communities in the network. TCD2 uses a tree-structure to maintain the information of dynamically changing community structures of the network. The experimental results on real-world social networks along with synthetic networks validate the performance of TCD2. The tests also confirmed its superiority over the state-of-the-art algorithms. The results showed that the proposed algorithm achieves a significant trade-off between quality and accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311374",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Community structure",
      "Computation",
      "Computer network",
      "Computer science",
      "Computer security",
      "Data mining",
      "Data science",
      "Data structure",
      "Dynamic network analysis",
      "Epistemology",
      "Exploit",
      "Field (mathematics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Philosophy",
      "Programming language",
      "Psychology",
      "Psychotherapist",
      "Pure mathematics",
      "Quality (philosophy)",
      "Social connectedness",
      "Social media",
      "Social network (sociolinguistics)",
      "Tree (set theory)",
      "Tree structure",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Mishra",
        "given_name": "Sneha"
      },
      {
        "surname": "Singh",
        "given_name": "Shashank Sheshar"
      },
      {
        "surname": "Mishra",
        "given_name": "Shivansh"
      },
      {
        "surname": "Biswas",
        "given_name": "Bhaskar"
      }
    ]
  },
  {
    "title": "Efficient algorithms for discovering high-utility patterns with strong frequency affinities",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114464",
    "abstract": "In recent years, high-utility pattern mining has been studied extensively. However, most of these studies have addressed mining high-utility patterns (HUPs) without consideration for their frequencies, leading to the mining of meaningless HUPs. One of the approaches to solving this problem is to use HUP mining with strong affinity frequencies. In this paper, we propose two algorithms to discover HUPs with strong affinity frequencies: DHUP-Miner (Discriminative High-Utility pattern - Miner) and its parallel version, DHUP-Miner*. Several novel pruning strategies are applied to reduce the search space for potential DHUPs. Experimental results show that the proposed algorithms are faster than the state-of-the-art algorithm (FDHUP) for both sparse and dense benchmark datasets. Moreover, the parallel algorithm (DHUP-Miner*) was found to handle large datasets well.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311167",
    "keywords": [
      "Affinities",
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Discriminative model",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Pruning",
      "Stereochemistry"
    ],
    "authors": [
      {
        "surname": "Vuong",
        "given_name": "Nhan"
      },
      {
        "surname": "Le",
        "given_name": "Bac"
      },
      {
        "surname": "Truong",
        "given_name": "Tin"
      },
      {
        "surname": "Nguyen",
        "given_name": "Duy-Phuong"
      }
    ]
  },
  {
    "title": "Ordered fuzzy WASPAS method for selection of improvement projects",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114471",
    "abstract": "This paper proposes a new approach to selecting improvement projects (IPs). It allows the evaluation and prioritization of projects taking into account a proposed set of criteria as a key criteria for processes continuous improvement. The novelty of the research is the proposition of a OFN-WASPAS method (the WASPAS method, which uses the Ordered Fuzzy Numbers (OFNs)), in the selection of improvement projects. The advantage of the proposed method is its ability to provide project assessments in the form of fuzzy numbers or qualitative assessments together with the description of likely direction of assessment change in the nearest future. This is possible due to the use of orientation in OFNs. The approach verification is presented on the basis of the case study considered five projects constituting the portfolio of IPs selected, using the 19 most important subcriteria proposed within the 9 main criteria. The group AHP method is used to calculate the criteria weights. The conducted research demonstrated that the proposed approach to IPs selection is, to a large extent, applicable in an enterprise that operates per the CI concept. The resulting process improvement requires the implementation of many improvement projects. This approach handles the key aspects of decision making in a consistent and rational way and can be especially useful for portfolio analysis with projects using the latest technologies and implemented in frequently changing environmental conditions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311210",
    "keywords": [
      "Analytic hierarchy process",
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Computer security",
      "Finance",
      "Fuzzy logic",
      "Key (lock)",
      "Mathematics",
      "Novelty",
      "Operating system",
      "Operations research",
      "Philosophy",
      "Portfolio",
      "Prioritization",
      "Process (computing)",
      "Process management",
      "Programming language",
      "Selection (genetic algorithm)",
      "Set (abstract data type)",
      "Theology"
    ],
    "authors": [
      {
        "surname": "Rudnik",
        "given_name": "Katarzyna"
      },
      {
        "surname": "Bocewicz",
        "given_name": "Grzegorz"
      },
      {
        "surname": "Kucińska-Landwójtowicz",
        "given_name": "Aneta"
      },
      {
        "surname": "Czabak-Górska",
        "given_name": "Izabela D."
      }
    ]
  },
  {
    "title": "On the step size selection in variance-reduced algorithm for nonconvex optimization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114336",
    "abstract": "Nonconvex problems have recently received considerable attention in statistics, machine learning and vision areas. Variance reduction techniques like the SVRG-type and SARAH-type methods, originally proposed for convex optimization, have been proved to be effective for nonconvex optimization. The performance of variance-reduced stochastic gradient methods greatly depends on how step sizes are tuned and decreased over time. However, research on specifying online step size in nonconvex optimization is quite limited. Motivated by this gap, we propose using the random stabilized Barzilai–Borwein (RSBB) method to compute step size for variance-reduced stochastic gradient methods. In particular, we incorporate RSBB into the modern variance-reduced stochastic gradient method, the SARAH-type method, for nonconvex optimization. We theoretically prove that the proposed method converges sub-linearly for general nonconvex objective functions and converges linearly for gradient dominated functions. We also show that the complexity of our proposed method is comparable to modern stochastic gradient methods. To further show the efficacy of the RSBB method, we proposed MSVRG-RSBB, by introducing it into MSVRG (a variant of the SVRG method). We provide experimental results on various datasets to show the efficacy of our proposed methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310253",
    "keywords": [
      "Accounting",
      "Algorithm",
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Convex function",
      "Geometry",
      "Gradient method",
      "Mathematical optimization",
      "Mathematics",
      "Optimization problem",
      "Regular polygon",
      "Selection (genetic algorithm)",
      "Stochastic optimization",
      "Variance (accounting)",
      "Variance reduction"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Zhuang"
      }
    ]
  },
  {
    "title": "A reinforcement learning-based algorithm for the aircraft maintenance routing problem",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114399",
    "abstract": "With recent developments in the airline industry worldwide, the competition among the industry has increased largely with many key players in the market. In order to generate profits, the industry has paid much attention to generate optimal routes that are maintenance feasible. The main aim of operational aircraft maintenance routing problem (OAMRP) is to generate these optimal routes for each aircraft that are maintenance feasible and follow the constraints defined by the Federal Aviation Administration (FAA). In this paper, the OAMRP is studied with two main objectives. First, to propose a formulation of a network flow-based Integer Linear Programming (ILP) framework for the OAMRP that considers three main maintenance constraints simultaneously: maximum flying-hour, limit on the number of take-offs between two consecutive maintenance checks and the work-force capacity. Second, to develop a new reinforcement learning-based algorithm which can be used to solve the problem, quickly and efficiently, as compared to commonly available optimization software. Finally, the evaluation of the proposed algorithm on real case datasets obtained from a major airline located in the Middle East verifies that the algorithm generates high-quality solutions quickly for both medium and large-scale flight schedule dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742031071X",
    "keywords": [
      "Aeronautics",
      "Aerospace engineering",
      "Aircraft maintenance",
      "Algorithm",
      "Artificial intelligence",
      "Aviation",
      "Computer network",
      "Computer science",
      "Engineering",
      "Integer programming",
      "Limit (mathematics)",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Operations research",
      "Reinforcement learning",
      "Routing (electronic design automation)",
      "Schedule"
    ],
    "authors": [
      {
        "surname": "Ruan",
        "given_name": "J.H."
      },
      {
        "surname": "Wang",
        "given_name": "Z.X."
      },
      {
        "surname": "Chan",
        "given_name": "Felix T.S."
      },
      {
        "surname": "Patnaik",
        "given_name": "S."
      },
      {
        "surname": "Tiwari",
        "given_name": "M.K."
      }
    ]
  },
  {
    "title": "A review of deep learning methods for semantic segmentation of remote sensing imagery",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114417",
    "abstract": "Semantic segmentation of remote sensing imagery has been employed in many applications and is a key research topic for decades. With the success of deep learning methods in the field of computer vision, researchers have made a great effort to transfer their superior performance to the field of remote sensing image analysis. This paper starts with a summary of the fundamental deep neural network architectures and reviews the most recent developments of deep learning methods for semantic segmentation of remote sensing imagery including non-conventional data such as hyperspectral images and point clouds. In our review of the literature, we identified three major challenges faced by researchers and summarize the innovative development to address them. As tremendous efforts have been devoted to advancing pixel-level accuracy, the emerged deep learning methods demonstrated much-improved performance on several public data sets. As to handling the non-conventional, unstructured point cloud and rich spectral imagery, the performance of the state-of-the-art methods is, on average, inferior to that of the satellite imagery. Such a performance gap also exists in learning from small data sets. In particular, the limited non-conventional remote sensing data sets with labels is an obstacle to developing and evaluating new deep learning methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310836",
    "keywords": [
      "Artificial intelligence",
      "Cloud computing",
      "Computer science",
      "Computer security",
      "Data science",
      "Deep learning",
      "Field (mathematics)",
      "Geology",
      "Hyperspectral imaging",
      "Image (mathematics)",
      "Image retrieval",
      "Key (lock)",
      "Mathematics",
      "Operating system",
      "Point cloud",
      "Pure mathematics",
      "Remote sensing",
      "Satellite imagery",
      "Segmentation",
      "Semantic gap",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Yuan",
        "given_name": "Xiaohui"
      },
      {
        "surname": "Shi",
        "given_name": "Jianfang"
      },
      {
        "surname": "Gu",
        "given_name": "Lichuan"
      }
    ]
  },
  {
    "title": "CNN-based driving maneuver classification using multi-sliding window fusion",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114442",
    "abstract": "Driving behavior classification has received increasing attention in recent years, where driving maneuver classification plays an important role. The first step of building a driving maneuver classification system is to segment maneuvers, which is often realized by using a single sliding window in previous work. However, different types of driving maneuvers often have different maneuver duration. It is difficult to segment those maneuvers using only a single fixed-sized window. In this paper, we present a CNN-based method to classify driving maneuvers using multi-sliding window fusion. First, multi-sliding windows of both short and longer sizes are used for constructing a robust feature set. Then, CNN-based mid-fusion is used for classifying driving maneuvers. To evaluate the proposed approach, a public dataset named UAH-DriveSet with six drivers driving on the highway is used. Six driving maneuvers were labeled: lane keeping, braking, turning, acceleration, right lane change, and left lane change. The experimental results show that our proposed CNN-based driving maneuver classification can achieve a macro F1-score of 58.22% using single-window and early-fusion. Comparing four different fusion methods, All fusion achieves the best performance. With multi-sliding window fusion and mid-fusion based CNN, the highest macro F1-score can be up to 80.25%, which is higher than early- and late-fusion. In addition, the F1-score of CNN-based method is higher than both k-NN and RF-based methods. Finally, we verify the importance of label information for driving maneuver classification, and the highest macro F1-score is 87.67% with an assigned duration of 4s.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311003",
    "keywords": [
      "Acceleration",
      "Artificial intelligence",
      "Classical mechanics",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Fusion",
      "Linguistics",
      "Macro",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Programming language",
      "Set (abstract data type)",
      "Sliding window protocol",
      "Window (computing)"
    ],
    "authors": [
      {
        "surname": "Xie",
        "given_name": "Jie"
      },
      {
        "surname": "Hu",
        "given_name": "Kai"
      },
      {
        "surname": "Li",
        "given_name": "Guofa"
      },
      {
        "surname": "Guo",
        "given_name": "Ya"
      }
    ]
  },
  {
    "title": "An efficient differential-evolution-based moving compensation optimization approach for controlling differential column shortening in tall buildings",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114531",
    "abstract": "In this paper, an efficient procedure integrating the moving compensation optimization (MCO) and a modified differential evolution algorithm is presented for finding the optimal solution to compensate for the differential column shortening (DCS) in tall buildings. In the proposed compensation procedure, the number of compensation groups is minimized by maximizing the number of floors for each group stepwise with constraints placed on the compensation error at each floor level. Two optimal compensation problems are presented, including the deterministic optimal compensation (DOC) and the reliability-based optimal compensation (ROC), which correspond to ignoring or considering the uncertainties that are inherent in the predicted shortenings as well as the correction amounts. A parameter-free, adaptive differential evolution algorithm is established to solve MCO. Applications for a 70-story building and a 72-story building are examined to demonstrate the efficiency and reliability of the presented compensation approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311751",
    "keywords": [
      "Aerospace engineering",
      "Algorithm",
      "Artificial intelligence",
      "Compensation (psychology)",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Differential (mechanical device)",
      "Differential evolution",
      "Engineering",
      "Mathematical optimization",
      "Mathematics",
      "Physics",
      "Power (physics)",
      "Psychoanalysis",
      "Psychology",
      "Quantum mechanics",
      "Reliability (semiconductor)"
    ],
    "authors": [
      {
        "surname": "Pham",
        "given_name": "Hoang-Anh"
      },
      {
        "surname": "Nguyen",
        "given_name": "Duc-Xuan"
      },
      {
        "surname": "Truong",
        "given_name": "Viet-Hung"
      }
    ]
  },
  {
    "title": "Application of mutation operators to salp swarm algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114368",
    "abstract": "Salp swarm algorithm (SSA) based on the swarming behaviour of salps found in ocean, is a very competitive algorithm and has proved its worth as an excellent problem optimizer. Though SSA is a very challenging algorithm but it suffers from the problem of poor exploitation, local optima stagnation and unbalanced exploration and exploitation operations. Thus in order to mitigate these problems and improve the working properties, seven new versions of SSA are proposed in present work. All the new versions employ new set of mutation properties along with some common properties. The common properties of all the algorithms include division of generations, adaptive switching and adaptive population strategy. Overall, the proposed algorithms are self-adaptive in nature along with some added mutation properties. For performance evaluation, the proposed algorithms are subjected to variable initial population and dimension sizes. The best among the proposed is then tested on CEC 2005, CEC 2015 benchmark problems and real world problems from CEC 2011 benchmarks. Experimental and statistical results show that the proposed mutation clock SSA (MSSA) is best among all the algorithms under comparison.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310460",
    "keywords": [
      "Adaptive mutation",
      "Algorithm",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Demography",
      "Evolutionary algorithm",
      "Gene",
      "Genetic algorithm",
      "Geodesy",
      "Geography",
      "Imperialist competitive algorithm",
      "Local optimum",
      "Mathematical optimization",
      "Mathematics",
      "Multi-swarm optimization",
      "Mutation",
      "Optimization problem",
      "Population",
      "Sociology",
      "Swarm behaviour"
    ],
    "authors": [
      {
        "surname": "Salgotra",
        "given_name": "Rohit"
      },
      {
        "surname": "Singh",
        "given_name": "Urvinder"
      },
      {
        "surname": "Singh",
        "given_name": "Gurdeep"
      },
      {
        "surname": "Singh",
        "given_name": "Supreet"
      },
      {
        "surname": "Gandomi",
        "given_name": "Amir H."
      }
    ]
  },
  {
    "title": "Theoretical and empirical analysis of filter ranking methods: Experimental study on benchmark DNA microarray data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114485",
    "abstract": "DNA microarray experiments generate thousands of gene expression values that provide information about the state of cells and tissues. Though these expressive values are useful in disease classification, however, only a few genes contribute towards this classification. In this context, usage of feature selection algorithms can be beneficial, as the main goal of feature selection algorithms is to identify the relevant features (here genes) efficiently. In the recent past, many feature selection algorithms have been proposed in the literature that measure the relevancy and redundancy of the features using various evaluation criteria. An important type of feature selection techniques is feature ranking, which does not use any learning algorithm, rather assigns an important value or weight to a feature. In this paper, we provide an extensive study on 10 popularly used filter ranking methods. We have applied the methods to 10 microarray datasets (both binary class and multi-class) and tested the accuracies using three well-known classifiers namely Multi-layer Perceptron (MLP), Support Vector Machine (SVM) and K-Nearest Neighbor (KNN). We have conducted a wide variety of tests to assess the strength and weakness of various filter methods. This vast study provides a comparison amongst different filter methods helping researchers make an informed choice about selecting an appropriate filter method for their work. Three categories of filtering methods are tested, namely, Entropy based, Similarity based and Statistics based. The experiments show that out of all the methods Mutual Information (MI) gives the best results (also best among Entropy based methods). In the category of Similarity based methods ReliefF performs best and Chi-square performs best in the category of Statistics based methods. In case of bi-class datasets, Chi-square would be the better choice, while for multi-class datasets, MI gives better results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311325",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Entropy (arrow of time)",
      "Feature selection",
      "Filter (signal processing)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Ranking (information retrieval)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Kanti Ghosh",
        "given_name": "Kushal"
      },
      {
        "surname": "Begum",
        "given_name": "Shemim"
      },
      {
        "surname": "Sardar",
        "given_name": "Aritra"
      },
      {
        "surname": "Adhikary",
        "given_name": "Sukdev"
      },
      {
        "surname": "Ghosh",
        "given_name": "Manosij"
      },
      {
        "surname": "Kumar",
        "given_name": "Munish"
      },
      {
        "surname": "Sarkar",
        "given_name": "Ram"
      }
    ]
  },
  {
    "title": "Super-app behavioral patterns in credit risk models: Financial, statistical and regulatory implications",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114486",
    "abstract": "In this paper we present the impact of alternative data that originates from an app-based marketplace, in contrast to traditional bureau data, upon credit scoring models. These alternative data sources have shown themselves to be immensely powerful in predicting borrower behavior in segments traditionally underserved by banks and financial institutions. Our results, validated across two countries, show that these new sources of data are particularly useful for predicting financial behavior in low-wealth and young individuals, who are also the most likely to engage with alternative lenders. Furthermore, using the TreeSHAP method for Stochastic Gradient Boosting interpretation, our results also revealed interesting non-linear trends in the variables originating from the app, which would not normally be available to traditional banks. Our results represent an opportunity for technology companies to disrupt traditional banking by correctly identifying alternative data sources and handling this new information properly. At the same time alternative data must be carefully validated to overcome regulatory hurdles across diverse jurisdictions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311337",
    "keywords": [
      "Artificial intelligence",
      "Boosting (machine learning)",
      "Business",
      "Computer science",
      "Contrast (vision)",
      "Credit risk",
      "Econometrics",
      "Economics",
      "Finance",
      "Gradient boosting",
      "Machine learning",
      "Random forest"
    ],
    "authors": [
      {
        "surname": "Roa",
        "given_name": "Luisa"
      },
      {
        "surname": "Correa-Bahnsen",
        "given_name": "Alejandro"
      },
      {
        "surname": "Suarez",
        "given_name": "Gabriel"
      },
      {
        "surname": "Cortés-Tejada",
        "given_name": "Fernando"
      },
      {
        "surname": "Luque",
        "given_name": "Maria A."
      },
      {
        "surname": "Bravo",
        "given_name": "Cristián"
      }
    ]
  },
  {
    "title": "Matrix factorization based Bayesian network embedding for efficient probabilistic inferences",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114294",
    "abstract": "Bayesian network (BN) is a well adopted framework for representing and inferring uncertain knowledge. By the existing methods, multiple probabilistic inferences on the same BN are often fulfilled one by one via repeated searches and calculations of probabilities. However, lots of intermediate results of probability calculations cannot be shared and reused among different probabilistic inferences. It is necessary to improve the overall efficiency of multiple probabilistic inferences on the same BN by incorporating an easy-to-calculate representation of BN and an easy-to-reuse technique for common calculations in multiple inferences. In this paper, we first propose the method of Bayesian network embedding to generate the easy-to-reuse node embeddings. Specifically, we transform BN into the point mutual information (PMI) matrix to simultaneously preserve the directed acyclic graph (DAG) and conditional probability tables (CPTs). Then, we give the singular value decomposition (SVD) based method to factorize the PMI matrix for generating node embeddings. Secondly, we propose a novel method of random sampling to make multiple probabilistic inferences via similarity calculation between node embeddings. Experimental results show that the runtime of our proposed BNERS performing 10 times of inferences is 30% faster than Gibbs sampling (GS) and 50% faster than forward sampling (FS) on LINK BN (very large network), while maintaining almost the same results as GS and FS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309969",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bayesian network",
      "Bayesian probability",
      "Computer science",
      "Directed acyclic graph",
      "Eigenvalues and eigenvectors",
      "Embedding",
      "Engineering",
      "Gibbs sampling",
      "Matrix decomposition",
      "Node (physics)",
      "Physics",
      "Probabilistic logic",
      "Quantum mechanics",
      "Singular value decomposition",
      "Structural engineering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Qi",
        "given_name": "Zhiwei"
      },
      {
        "surname": "Yue",
        "given_name": "Kun"
      },
      {
        "surname": "Duan",
        "given_name": "Liang"
      },
      {
        "surname": "Wang",
        "given_name": "Jiahui"
      },
      {
        "surname": "Qiao",
        "given_name": "Shaojie"
      },
      {
        "surname": "Fu",
        "given_name": "Xiaodong"
      }
    ]
  },
  {
    "title": "Entropy-like Divergence Based Kernel Fuzzy Clustering for Robust Image Segmentation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114327",
    "abstract": "Gaussian kernel is defined by Euclidean distance and has been widely used in many fields. In the view of Euclidean distance is sensitive to outliers or noise and it is difficult to obtain satisfactory results for complex non-convex data. Entropy-like divergence is firstly induced by combining Jenson-Shannon/Bregman divergence with convex function, and its mercer kernel function called entropy-like divergence-based kernel is also constructed in this paper. Secondly, a new fuzzy weighted factor based on entropy-like divergence-based kernel is proposed by improving the existing trade-off weighting factor of kernel-based fuzzy local information C-means clustering (KWFLICM). In the end, a weighted fuzzy local information clustering based on entropy-like divergence-based kernel (EKWFLICM) is presented, which combines a new weighted fuzzy factor and entropy-like divergence-based kernel. Experiment results show that the proposed algorithm outperforms the segmentation performance of existing state-of-the-art fuzzy clustering-related algorithms for the image in presence of high noise.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310198",
    "keywords": [
      "Applied mathematics",
      "Artificial intelligence",
      "Bregman divergence",
      "Cluster analysis",
      "Computer science",
      "Discrete mathematics",
      "Fuzzy clustering",
      "Kernel (algebra)",
      "Mathematics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Chengmao"
      },
      {
        "surname": "Cao",
        "given_name": "Zhuo"
      }
    ]
  },
  {
    "title": "Optimal sepsis patient treatment using human-in-the-loop artificial intelligence",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114476",
    "abstract": "Sepsis is one of the leading causes of death in Intensive Care Units (ICU). The strategy for treating sepsis involves the infusion of intravenous (IV) fluids and administration of antibiotics. Determining the optimal quantity of IV fluids is a challenging problem due to the complexity of a patient’s physiology. In this study, we develop a data-driven optimization solution that derives the optimal quantity of IV fluids for individual patients. The proposed method minimizes the probability of severe outcomes by controlling the prescribed quantity of IV fluids and utilizes human-in-the-loop artificial intelligence. We demonstrate the performance of our model on 1122 ICU patients with sepsis diagnosis extracted from the MIMIC-III dataset. The results show that, on average, our model can reduce mortality by 22%. This study has the potential to help physicians synthesize optimal, patient-specific treatment strategies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311258",
    "keywords": [
      "Antibiotics",
      "Artificial intelligence",
      "Biology",
      "Combinatorics",
      "Computer science",
      "Intensive care",
      "Intensive care medicine",
      "Loop (graph theory)",
      "Mathematics",
      "Medicine",
      "Microbiology",
      "Nursing",
      "Patient care",
      "Sepsis",
      "Surgery"
    ],
    "authors": [
      {
        "surname": "Gupta",
        "given_name": "Akash"
      },
      {
        "surname": "Lash",
        "given_name": "Michael T."
      },
      {
        "surname": "Nachimuthu",
        "given_name": "Senthil K."
      }
    ]
  },
  {
    "title": "Stock price prediction using deep learning and frequency decomposition",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114332",
    "abstract": "Nonlinearity and high volatility of financial time series have made it difficult to predict stock price. However, thanks to recent developments in deep learning and methods such as long short-term memory (LSTM) and convolutional neural network (CNN) models, significant improvements have been obtained in the analysis of this type of data. Further, empirical mode decomposition (EMD) and complete ensemble empirical mode decomposition (CEEMD) algorithms decomposing time series to different frequency spectra are among the methods that could be effective in analyzing financial time series. Based on these theoretical frameworks, we propose novel hybrid algorithms, i.e., CEEMD-CNN-LSTM and EMD-CNN-LSTM, which could extract deep features and time sequences, which are finally applied to one-step-ahead prediction. The concept of the suggested algorithm is that when combining these models, some collaboration is established between them that could enhance the analytical power of the model. The practical findings confirm this claim and indicate that CNN alongside LSTM and CEEMD or EMD could enhance the prediction accuracy and outperform other counterparts. Further, the suggested algorithm with CEEMD provides better performance compared to EMD.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310228",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Econometrics",
      "Filter (signal processing)",
      "Hilbert–Huang transform",
      "Machine learning",
      "Mathematics",
      "Nonlinear system",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Time series",
      "Volatility (finance)"
    ],
    "authors": [
      {
        "surname": "Rezaei",
        "given_name": "Hadi"
      },
      {
        "surname": "Faaljou",
        "given_name": "Hamidreza"
      },
      {
        "surname": "Mansourfar",
        "given_name": "Gholamreza"
      }
    ]
  },
  {
    "title": "A proactive decision support system for reviewer recommendation in academia",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114331",
    "abstract": "Peer review is an essential part of scientific communications to ensure the quality of publications and a healthy scientific evaluation process. Assigning appropriate reviewers poses a great challenge for program chairs and journal editors for many reasons, including relevance, fair judgment, no conflict of interest, and qualified reviewers in terms of scientific impact. With a steady increase in the number of research domains, scholarly venues, researchers, and papers in academia, manually selecting and accessing adequate reviewers is becoming a tedious and time-consuming task. Traditional approaches for reviewer selection mainly focus on the matching of research relevance by keywords or disciplines. However, in real-world systems, various factors are often needed to be considered. Therefore, we propose a multilayered approach integrating Topic Network, Citation Network, and Reviewer Network into a reviewer Recommender System (TCRRec). We explore various aspects, including relevance between reviewer candidates and submission, authority, expertise, diversity, and conflict of interest and integrate them into the proposed framework TCRRec. The paper also addresses cold start issues for researchers having unique areas of interest or for isolated researchers. Experiments based on the NIPS and AMiner dataset demonstrate that the proposed TCRRec outperforms state-of-the-art recommendation techniques in terms of standard metrics of precision@k, MRR, nDCG@k, authority, expertise, diversity, and coverage.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310216",
    "keywords": [
      "Artificial intelligence",
      "Citation",
      "Computer science",
      "Data science",
      "Diversity (politics)",
      "Economics",
      "Epistemology",
      "Focus (optics)",
      "Information retrieval",
      "Law",
      "Management",
      "Matching (statistics)",
      "Mathematics",
      "Operating system",
      "Optics",
      "Philosophy",
      "Physics",
      "Political science",
      "Process (computing)",
      "Quality (philosophy)",
      "Recommender system",
      "Relevance (law)",
      "Selection (genetic algorithm)",
      "Statistics",
      "Task (project management)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Pradhan",
        "given_name": "Tribikram"
      },
      {
        "surname": "Sahoo",
        "given_name": "Suchit"
      },
      {
        "surname": "Singh",
        "given_name": "Utkarsh"
      },
      {
        "surname": "Pal",
        "given_name": "Sukomal"
      }
    ]
  },
  {
    "title": "Predicting tweet impact using a novel evidential reasoning prediction method",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114400",
    "abstract": "This study presents a novel evidential reasoning (ER) prediction model called MAKER-RIMER to examine how different features embedded in Twitter posts (tweets) can predict the number of retweets achieved during an electoral campaign. The tweets posted by the two most voted candidates during the official campaign for the 2017 Ecuadorian Presidential election were used for this research. For each tweet, five features including type of tweet, emotion, URL, hashtag, and date are identified and coded to predict if tweets are of either high or low impact. The main contributions of the new proposed model include its suitability to analyse tweet datasets based on likelihood analysis of data. The model is interpretable, and the prediction process relies only on the use of available data. The experimental results show that MAKER-RIMER performed better, in terms of misclassification error, when compared against other predictive machine learning approaches. In addition, the model allows observing which features of the candidates’ tweets are linked to high and low impact. Tweets containing allusions to the contender candidate, either with positive or negative connotations, without hashtags, and written towards the end of the campaign, were persistently those with the highest impact. URLs, on the other hand, is the only variable that performs differently for the two candidates in terms of achieving high impact. MAKER-RIMER can provide campaigners of political parties or candidates with a tool to measure how features of tweets are predictors of their impact, which can be useful to tailor Twitter content during electoral campaigns.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310721",
    "keywords": [
      "Artificial intelligence",
      "Business decision mapping",
      "Computer science",
      "Data mining",
      "Decision maker",
      "Decision support system",
      "Evidential reasoning approach",
      "Information retrieval",
      "Machine learning",
      "Mathematics",
      "Natural language processing",
      "Operating system",
      "Operations research",
      "Process (computing)"
    ],
    "authors": [
      {
        "surname": "Rivadeneira",
        "given_name": "Lucía"
      },
      {
        "surname": "Yang",
        "given_name": "Jian-Bo"
      },
      {
        "surname": "López-Ibáñez",
        "given_name": "Manuel"
      }
    ]
  },
  {
    "title": "A fractal interpolation approach to improve neural network predictions for difficult time series data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114474",
    "abstract": "Deep Learning methods, such as Long Short-Term Memory (LSTM) neural networks prove capable of predicting real-life time series data. Crucial for this technique to work is a sufficient amount of data. This can either be very long time-series data or fine-grained time-series data. If the data is insufficient, in terms of length or complexity, LSTM approaches perform poorly. We propose a fractal interpolation approach to generate a more fine-grained time series out of insufficient data sets. The interpolation is dynamically adapted to the time series using a time-dependent complexity feature so that the complexity properties of the interpolated time series are related to that of the original one. Also we perform a linear interpolation with the same number of interpolation points to compare results. This paper shows that predictions of fractal interpolated and linear interpolated time series clearly outperform the ones of the original data for a test fit on unknown data. Though predictions of linear and fractal interpolated time series data perform very similar, the fractal interpolated ones outperform the linear interpolated ones on difficult time series data. Also, though the complexities of sub-intervals are tailored to match the one from the original data, the interpolated time series shows a much higher degree of persistency and (in terms of the Hurst exponent) a much higher degree of long term memory.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311234",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Fractal",
      "Hurst exponent",
      "Interpolation (computer graphics)",
      "Linear interpolation",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Motion (physics)",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Series (stratigraphy)",
      "Statistics",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Raubitzek",
        "given_name": "Sebastian"
      },
      {
        "surname": "Neubauer",
        "given_name": "Thomas"
      }
    ]
  },
  {
    "title": "Detection of DDoS attacks with feed forward based deep neural network model",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114520",
    "abstract": "As a result of the increase in the services provided over the internet, it is seen that the network infrastructure is more exposed to cyber attacks. The most widely used of these attacks are Distributed Denial of Service (DDoS) attacks that easily disrupt services. The most important factor in the fight against DDoS attacks is the early detection and separation of network traffic. In this study, it is suggested to use the deep neural network (DNN) as a deep learning model that detects DDoS attacks on the sample of packets captured from network traffic. DNN model can work quickly and with high accuracy even in small samples because it contains feature extraction and classification processes in its structure and has layers that update itself as it is trained. As a result of the experiments carried out on the CICDDoS2019 dataset containing the current DDoS attack types created in 2019, it was observed that the attacks on network traffic were detected with 99.99% success and the attack types were classified with an accuracy rate of 94.57%. The high accuracy values obtained show that the deep learning model can be used effectively in combating DDoS attacks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311647",
    "keywords": [
      "Application layer DDoS attack",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer network",
      "Computer science",
      "Computer security",
      "Data mining",
      "Deep learning",
      "Denial-of-service attack",
      "Machine learning",
      "Network packet",
      "The Internet",
      "Trinoo",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Cil",
        "given_name": "Abdullah Emir"
      },
      {
        "surname": "Yildiz",
        "given_name": "Kazim"
      },
      {
        "surname": "Buldu",
        "given_name": "Ali"
      }
    ]
  },
  {
    "title": "The impact of term-weighting schemes and similarity measures on extractive multi-document text summarization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114510",
    "abstract": "Automatic text summarization is currently a topic of great interest in many knowledge fields. Extractive multi-document text summarization methods aim to reduce the textual information from a document collection by covering the main content and reducing the redundant information. In the scientific literature, there are different approaches related to term-weighting schemes and similarity measures, which are necessary for implementing an automatic summary system. However, to the best of the authors’ knowledge, there are no studies to analyze the performance of the different schemes and measures. In this paper, all possible combinations of the most common term-weighting schemes and similarity measures used in the extractive multi-document text summarization field have been implemented, compared, and analyzed. Experiments have been performed with Document Understanding Conferences (DUC) datasets, and the model performance has been assessed with eight Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics and the execution time. Results show that the best term-weighting scheme is the term-frequency inverse-sentence-frequency scheme, and the best similarity measure is the cosine similarity. Even more, the combination formed by both of them has obtained the best average results in 87.5% of ROUGE scores compared to the other combinations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311544",
    "keywords": [
      "Artificial intelligence",
      "Automatic summarization",
      "Computer science",
      "Cosine similarity",
      "Data mining",
      "Field (mathematics)",
      "Image (mathematics)",
      "Information retrieval",
      "Mathematics",
      "Measure (data warehouse)",
      "Medicine",
      "Multi-document summarization",
      "Pattern recognition (psychology)",
      "Physics",
      "Pure mathematics",
      "Quantum mechanics",
      "Radiology",
      "Similarity (geometry)",
      "Similarity measure",
      "Term (time)",
      "Weighting",
      "tf–idf"
    ],
    "authors": [
      {
        "surname": "Sanchez-Gomez",
        "given_name": "Jesus M."
      },
      {
        "surname": "Vega-Rodríguez",
        "given_name": "Miguel A."
      },
      {
        "surname": "Pérez",
        "given_name": "Carlos J."
      }
    ]
  },
  {
    "title": "Neural network forecasting of news feeds",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114521",
    "abstract": "The paper considers a problem of forecasting of news feeds content. Analysis of existing approaches to this problem solution reveals the need for development of methods with enhanced forecasting capabilities. A method is proposed with expanded accounting for space and time relationships of the processed data. The method is revealed through an example of a neural network forecasting system that implements it. Implementation includes data retrieval from news feeds, their special preprocessing, coding and forecasting of words sets and their interconnections, followed by highlighting news topics and describing the of news feeds content. Some variants of stream recurrent neural networks with spiral layer structures were investigated with due regard to their forecasting capabilities under direction and strength control of the associative call of signals from the network memory. The paper also presents and discusses experimental results, a description of the methodological contribution and recommendations on the method practical application.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311659",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Machine learning"
    ],
    "authors": [
      {
        "surname": "Osipov",
        "given_name": "Vasiliy"
      },
      {
        "surname": "Kuleshov",
        "given_name": "Sergey"
      },
      {
        "surname": "Zaytseva",
        "given_name": "Alexandra"
      },
      {
        "surname": "Levonevskiy",
        "given_name": "Dmitriy"
      },
      {
        "surname": "Miloserdov",
        "given_name": "Dmitriy"
      }
    ]
  },
  {
    "title": "Spectral embedded generalized mean based k -nearest neighbors clustering with S-distance",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114326",
    "abstract": "The spectral clustering algorithm is extensively employed in different aspects, especially in the field of pattern recognition. However, the efficient construction of the neighborhood graph is the main reason for its promising results. Generally, the similarity matrix relies on the applied similarity measure between two data points, selection of k − nearest neighbors (KNN), and approach for the construction of a neighborhood graph. In this study, we integrate S-distance to spectral clustering, which is capable to find out the complex and non-linear cluster structures. Moreover, generalized mean distance-based KNN is proposed to decrease the sensitiveness towards the value of the k . Also, a symmetry-favored KNN method is applied to construct the neighborhood graph, which reduces the impact of outliers and noisy data points. However, spectral clustering faces scalability and speedup issues in the case of large size datasets. Thus, the proposed spectral clustering algorithm is also executed in distributed environments. Several experiments are performed to validate the proposed clustering algorithm on 20 real-world datasets and 3 large size datasets. Experimental results demonstrate that the proposed clustering algorithm outperforms some of the baseline methods in terms of accuracy and clustering error rates. Finally, we conduct Wilcoxon’s Rank-Sum test and illustrate that the proposed spectral clustering algorithm is statistically significant.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310186",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Graph",
      "Image (mathematics)",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Similarity (geometry)",
      "Single-linkage clustering",
      "Spectral clustering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Sharma",
        "given_name": "Krishna Kumar"
      },
      {
        "surname": "Seal",
        "given_name": "Ayan"
      }
    ]
  },
  {
    "title": "An effective multi-start iterated greedy algorithm to minimize makespan for the distributed permutation flowshop scheduling problem with preventive maintenance",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114495",
    "abstract": "In recent years, distributed scheduling problems have been well studied for their close connection with multi-factory production networks. However, the maintenance operations that are commonly carried out on a system to restore it to a specific state are seldom taken into consideration. In this paper, we study a distributed permutation flowshop scheduling problem with preventive maintenance operation (PM/DPFSP). A multi-start iterated greedy (MSIG) algorithm is proposed to minimize the makespan. An improved heuristic is presented for the initialization and re-initialization by adding a dropout operation to NEH2 to generate solutions with a high level of quality and disperstiveness. A destruction phase with the tournament selection and a construction phase with an enhanced strategy are introduced to avoid local optima. A local search based on three effective operators is integrated into the MSIG to reinforce local neighborhood solution exploitation. In addition, a restart strategy is adpoted if a solution has not been improved in a certain number of consecutive iterations. We conducted extensive experiments to test the performance of the presented MSIG. The computational results indicate that the presented MSIG has many promising advantages in solving the PM/DPFSP under consideration.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311398",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Engineering",
      "Greedy algorithm",
      "Initialization",
      "Iterated function",
      "Iterated local search",
      "Job shop scheduling",
      "Local optimum",
      "Local search (optimization)",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Preventive maintenance",
      "Programming language",
      "Reliability engineering",
      "Schedule",
      "Scheduling (production processes)"
    ],
    "authors": [
      {
        "surname": "Mao",
        "given_name": "Jia-yang"
      },
      {
        "surname": "Pan",
        "given_name": "Quan-ke"
      },
      {
        "surname": "Miao",
        "given_name": "Zhong-hua"
      },
      {
        "surname": "Gao",
        "given_name": "Liang"
      }
    ]
  },
  {
    "title": "Customized frequent patterns mining algorithms for enhanced Top-Rank-K frequent pattern mining",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114530",
    "abstract": "Mining frequent patterns (FP) is an essential task in data mining. The parameter required for this task is typically the minimum support threshold. Tuning this parameter to a suitable value is a difficult task, especially for inexperienced users. Thus, the Top-Rank-K frequent patterns mining problem was introduced. It requires the user to input an easily-evaluated parameter, K , in order to obtain the set of all frequent patterns from the most frequent to the K th rank of frequency. In this paper, we customize three general Frequent Pattern Mining (FPM) algorithms, namely FIN, PrePost, and PrePost+, to develop specialized Top-Rank-K FP mining algorithms: TK_FIN, TK_PrePost, and TK_PrePost+. The Dynamic Minimum Support Raising strategy is applied on these algorithms to ensure efficiency. Experimentally, we evaluate the performance of these algorithms against an original, efficient, Top-Rank-K algorithm, BTK. The three presented algorithms perform 90% better than BTK in most of the experiments, with respect to runtime. Between the three Top-Rank-K FPM algorithms we present, TK_FIN achieves the best performance from both runtime and memory consumption perspectives.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742031174X",
    "keywords": [
      "Algorithm",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Engineering",
      "Mathematics",
      "Programming language",
      "Rank (graph theory)",
      "Set (abstract data type)",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Abdelaal",
        "given_name": "Areej Ahmad"
      },
      {
        "surname": "Abed",
        "given_name": "Sa'ed"
      },
      {
        "surname": "Al-Shayeji",
        "given_name": "Mohammad"
      },
      {
        "surname": "Allaho",
        "given_name": "Mohammad"
      }
    ]
  },
  {
    "title": "Exploiting discourse structure of traditional digital media to enhance automatic fake news detection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114340",
    "abstract": "This paper presents a novel architecture for dealing with Automatic Fake News detection. The architecture factors in the discourse structure of news in traditional digital media and is based on two premises. First, fake news tends to mix true and false information with the purpose of confusing readers. Second, this research is focused on fake news delivered in traditional digital media, so our approach considers the influence of the journalistic structure of news, and the way journalists tend to introduce the essential content in a news story using 5W1H answer. Considering both premises, this proposal deals with the news components separately because some may be true or false, instead of considering the veracity value of the news article as a unit. A two-layer architecture is proposed, Structure and Veracity layers. To demonstrate the validity of the proposal, a new dataset was created and annotated with a new fine-grained annotation scheme (FNDeepML) that considers the different elements of the news document and their veracity. Due to the severity of the COVID-19 pandemic crisis, health is the chosen domain, and Spanish is the language used to validate the architecture, given the lack of research in this language. However, the proposal can be applied to any other language or domain. The performance of the Veracity layer of our proposal, which factors in the traditional news article structure and the 5W1H annotation, is capable of delivering a result of F 1 =0.807. This represents a strong improvement when compared to the baseline, which uses the whole document with a single veracity value, obtaining F 1 =0.605. These findings validate the suitability and effectiveness of our approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310277",
    "keywords": [
      "Annotation",
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Digital media",
      "Domain (mathematical analysis)",
      "Information retrieval",
      "Mathematical analysis",
      "Mathematics",
      "Visual arts",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Bonet-Jover",
        "given_name": "Alba"
      },
      {
        "surname": "Piad-Morffis",
        "given_name": "Alejandro"
      },
      {
        "surname": "Saquete",
        "given_name": "Estela"
      },
      {
        "surname": "Martínez-Barco",
        "given_name": "Patricio"
      },
      {
        "surname": "Ángel García-Cumbreras",
        "given_name": "Miguel"
      }
    ]
  },
  {
    "title": "Money laundering and terrorism financing detection using neural networks and an abnormality indicator",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114470",
    "abstract": "This study proposes a comprehensive model that helps improve self-comparisons and group-comparisons for customers to detect suspicious transactions related to money laundering (ML) and terrorism financing (FT) in financial systems. The self-comparison is improved by establishing a more comprehensive know your customer (KYC) policy, adding non-transactional characteristics to obtain a set of variables that can be classified into four categories: inherent, product, transactional, and geographic. The group-comparison involving the clustering process is improved by using an innovative transaction abnormality indicator, based on the variance of the variables. To illustrate the way this methodology works, random samples were extracted from the data warehouse of an important financial institution in Mexico. To train the algorithms, 26,751 and 3527 transactions and their features, involving natural and legal persons, respectively, were selected randomly from January 2020. To measure the prediction accuracy, test sets of 1000 and 600 transactions were selected randomly for natural and legal persons, respectively, from February 2020. The proposed model manages to decrease the proportion of false positives and increase accuracy when compared to the rule-based system. On reducing the false positive rate, the company’s costs for investigating suspicious customers also decrease significantly.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311209",
    "keywords": [
      "Accounting",
      "Artificial intelligence",
      "Business",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Database",
      "Database transaction",
      "Economics",
      "False positive paradox",
      "Finance",
      "Financial institution",
      "Geometry",
      "Management",
      "Mathematics",
      "Money laundering",
      "Operating system",
      "Process (computing)",
      "Product (mathematics)",
      "Programming language",
      "Set (abstract data type)",
      "Transaction data",
      "Transactional leadership",
      "Variance (accounting)"
    ],
    "authors": [
      {
        "surname": "Rocha-Salazar",
        "given_name": "José-de-Jesús"
      },
      {
        "surname": "Segovia-Vargas",
        "given_name": "María-Jesús"
      },
      {
        "surname": "Camacho-Miñano",
        "given_name": "María-del-Mar"
      }
    ]
  },
  {
    "title": "Deception in the eyes of deceiver: A computer vision and machine learning based automated deception detection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114341",
    "abstract": "There is growing interest in the use of automated psychological profiling systems, specifically applying machine learning to the field of deception detection. Several psychological studies and machine-based models have been reporting the use of eye interaction, gaze and facial movements as important clues to deception detection. However, the identification of very specific and distinctive features is still required. For the first time, we investigate the fine-grained level eyes and facial micro-movements to identify the distinctive features that provide significant clues for the automated deception detection. A real-time deception detection approach was developed utilizing advanced computer vision and machine learning approaches to model the non-verbal deceptive behavior. Artificial neural networks, random forests and support vector machines were selected as base models for the data on the total of 262,000 discrete measurements with 1,26,291 and 128,735 of deceptive and truthful instances, respectively. The data set used in this study is part of an ongoing programme to collect a larger dataset on the effects of gender and ethnicity on deception detection. Some observations are made based on this data which should not be interpreted as scientific conclusions, but pointers for future work. Analysis of the above models revealed that eye movements carry relatively important clues to distinguish truthful and deceptive behaviours. The research outcomes align with the findings from forensic psychologists who also reported the eye movements as distinctive for the truthful and deceptive behavior. The research outcomes and proposed approach are beneficial for human experts and has many applications within interdisciplinary domains.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310289",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Deception",
      "Field (mathematics)",
      "Human–computer interaction",
      "Identification (biology)",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Profiling (computer programming)",
      "Programming language",
      "Psychology",
      "Pure mathematics",
      "Set (abstract data type)",
      "Social psychology",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Khan",
        "given_name": "Wasiq"
      },
      {
        "surname": "Crockett",
        "given_name": "Keeley"
      },
      {
        "surname": "O'Shea",
        "given_name": "James"
      },
      {
        "surname": "Hussain",
        "given_name": "Abir"
      },
      {
        "surname": "Khan",
        "given_name": "Bilal M."
      }
    ]
  },
  {
    "title": "A novel intuitionistic fuzzy soft set entrenched mammogram segmentation under Multigranulation approximation for breast cancer detection in early stages",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114329",
    "abstract": "Automated mammogram image segmentation is one of the most important methods in the domain of medical diagnosis and decision systems. Accurate segmentation of mammogram plays a key role for the detection of any kind of abnormality like lesion tissues, cyst in mammogram images for medical diagnosis. In this study, a novel hybrid soft computing entrenched segmentation method for mammograms is introduced for the detection of breast cancer in early stages. Here, we have designed a novel automatic mammogram segmentation method using intuitionistic fuzzy soft sets (IFSS) and Multigranulation rough set. First, the proposed clustering algorithm accomplishes a soft information structure from the source image using IFSSs via multiple fuzzy membership functions with Yager generating function. The IFSS handles the ambiguity among lesion and non-lesion pixels through the hesitant degree while shaping the membership function. To reduce distant pixels which do not belong to the region of interest (ROI), the lesion tissues in mammogram image is segregated by decision making scheme via a rough approximation of a fuzzy concept under the field of multigranulation space. Later, the proposed scheme utilizes soft-information builder with accuracy and roughness scores on multigranulation approximation space for segregation of normal and abnormal (lesion tissues) pixel from mammograms. The proposed model has generated a threshold image from accuracy and roughness scores via defuzzification process. The proposed segmentation model performs better than the existing methods using evaluation metrics like segmentation accuracy, Jeccards similarity coefficient.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310204",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Fuzzy logic",
      "Fuzzy set",
      "Image segmentation",
      "Mathematics",
      "Membership function",
      "Pattern recognition (psychology)",
      "Pixel",
      "Rough set",
      "Segmentation",
      "Soft set"
    ],
    "authors": [
      {
        "surname": "Ghosh",
        "given_name": "Swarup Kr"
      },
      {
        "surname": "Mitra",
        "given_name": "Anirban"
      },
      {
        "surname": "Ghosh",
        "given_name": "Anupam"
      }
    ]
  },
  {
    "title": "A hybrid approach to cardinality constraint portfolio selection problem based on nonlinear neural network and genetic algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114517",
    "abstract": "Portfolio Optimization (PO) is an extremely popular method that selects the best portfolio for an investor. When the classic methods fail to find an exact solution, heuristic techniques are designed to find an approximate solution. In the literature, such techniques have often been used in the portfolio selection problem. However, hardly any of these methods utilize a neural network to apportion the extent of stocks. The basic objective of portfolio optimization is to minimize the risk of the portfolio while maximizing the expected return of the portfolio. In reality, the Standard Portfolio Optimization Model does not fulfill stock market requirements in a money related world. Indeed, this problem cannot limit the amount of stock in the portfolio. On the other hand, the precise number of stocks are taken into account by the Cardinality Constraint Portfolio Optimization model, which is the Mixed-Integer Quadratic Programming problem. While minimizing the risk of the portfolio, limiting the expected return and the number of stocks is the main subject of this study. In this study, a hybrid approach is proposed, based on the Nonlinear Neural Network and the Genetic Algorithm to solve the Cardinality Constraint Portfolio Optimization Model. To investigate the effectiveness of the proposed hybrid approach, the Istanbul Stock Exchange (ISE-301) data is used. The ISE-30 data1 consists of daily prices, from May 2015 to May 2017. The ISE-30 data1 from May 2017 to July 2018 is used as out-of-sample. To clarify the effectiveness proposed, the method was applied to publicly data sets which are used for numerous different portfolio selection strategies in many articles. The proposed hybrid approach to the cardinality constraint portfolio optimization problem has more viable outcomes than current strategies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311611",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Cardinality (data modeling)",
      "Computer science",
      "Constraint (computer-aided design)",
      "Data mining",
      "Economics",
      "Financial economics",
      "Genetic algorithm",
      "Geometry",
      "Mathematical optimization",
      "Mathematics",
      "Optimization problem",
      "Portfolio",
      "Portfolio optimization"
    ],
    "authors": [
      {
        "surname": "Yaman",
        "given_name": "Ilgım"
      },
      {
        "surname": "Erbay Dalkılıç",
        "given_name": "Türkan"
      }
    ]
  },
  {
    "title": "Online bus-pooling service at the railway station for passengers and parcels sharing buses: A case in Dalian",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114354",
    "abstract": "Taking a taxi at the railway station is difficult in many large Chinese cities, especially during the night. Passengers off trains always have carry-on luggage, which may affect the travel choices for the next trip. This study integrates the passenger and freight transportation at the railway station by bus-pooling service. A two-stage model with passenger incentive is built. In the first stage, passengers are matched in a fair way while in the second stage, parcels are inserted into the bus routes. An algorithm is designed to find the fair bus-pooling plan in which a large neighborhood search is tailored to generate group rides. To assess the performance of the proposed model and algorithm, cases are presented based on real-life taxi data related to Dalian North Railway Station. The results indicate that the fairness, waiting time of passengers, walking distance of recipients and time span have impacts on the matching rate. Relaxing the fairness constraint could improve the application of the fair bus-pooling in practice.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742031037X",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Cartography",
      "Computer science",
      "Constraint (computer-aided design)",
      "Engineering",
      "Geography",
      "Marketing",
      "Matching (statistics)",
      "Mathematics",
      "Mechanical engineering",
      "Operations research",
      "Passenger transport",
      "Pooling",
      "Service (business)",
      "Statistics",
      "Train",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Zixuan"
      },
      {
        "surname": "Feng",
        "given_name": "Rui"
      },
      {
        "surname": "Wang",
        "given_name": "Chenyu"
      },
      {
        "surname": "Jiang",
        "given_name": "Yonglei"
      },
      {
        "surname": "Yao",
        "given_name": "Baozhen"
      }
    ]
  },
  {
    "title": "Online bus-pooling service at the railway station for passengers and parcels sharing buses: A case in Dalian",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114354",
    "abstract": "Taking a taxi at the railway station is difficult in many large Chinese cities, especially during the night. Passengers off trains always have carry-on luggage, which may affect the travel choices for the next trip. This study integrates the passenger and freight transportation at the railway station by bus-pooling service. A two-stage model with passenger incentive is built. In the first stage, passengers are matched in a fair way while in the second stage, parcels are inserted into the bus routes. An algorithm is designed to find the fair bus-pooling plan in which a large neighborhood search is tailored to generate group rides. To assess the performance of the proposed model and algorithm, cases are presented based on real-life taxi data related to Dalian North Railway Station. The results indicate that the fairness, waiting time of passengers, walking distance of recipients and time span have impacts on the matching rate. Relaxing the fairness constraint could improve the application of the fair bus-pooling in practice.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742031037X",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Cartography",
      "Computer science",
      "Constraint (computer-aided design)",
      "Engineering",
      "Geography",
      "Marketing",
      "Matching (statistics)",
      "Mathematics",
      "Mechanical engineering",
      "Operations research",
      "Passenger transport",
      "Pooling",
      "Service (business)",
      "Statistics",
      "Train",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Zixuan"
      },
      {
        "surname": "Feng",
        "given_name": "Rui"
      },
      {
        "surname": "Wang",
        "given_name": "Chenyu"
      },
      {
        "surname": "Jiang",
        "given_name": "Yonglei"
      },
      {
        "surname": "Yao",
        "given_name": "Baozhen"
      }
    ]
  },
  {
    "title": "A dynamic algorithm based on cohesive entropy for influence maximization in social networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114207",
    "abstract": "The problem of influence maximization in social networks has been widely investigated, but most previous studies have usually ignored the dynamic nature of propagation and the effects of local aggregation factors on diffusion. This paper presents a Dynamic algorithm based on cohesive Entropy for Influence Maximization (DEIM), the goal of which is to find the most influential nodes in social networks. Firstly, the Community Overlap Propagation Algorithm based on Cohesive Entropy (CECOPA) is put forward for the discovery of overlapping communities in networks, and potential nodes in the gathering area are selected to construct the candidate seed set. Then, the Optional Dynamic influence Propagation algorithm (ODP) is designed based on narrowing down the selection range of seeds. It utilizes a variety of entropy calculations to obtain the cohesive power between neighboring nodes and then determines whether the node has the ability to become a propagable pioneer of another node; thus, information continues to diffuse effectively. Finally, via many times experiments on several data sets, it is confirmed that the proposed DEIM algorithm in this paper can successfully affect the ideal number of users in different scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309350",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Construct (python library)",
      "Data mining",
      "Engineering",
      "Entropy (arrow of time)",
      "Entropy maximization",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Node (physics)",
      "Physics",
      "Principle of maximum entropy",
      "Programming language",
      "Quantum mechanics",
      "Structural engineering"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Weimin"
      },
      {
        "surname": "Zhong",
        "given_name": "Kexin"
      },
      {
        "surname": "Wang",
        "given_name": "Jianjia"
      },
      {
        "surname": "Chen",
        "given_name": "Dehua"
      }
    ]
  },
  {
    "title": "Linguistic feature based learning model for fake news detection and classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114171",
    "abstract": "Social media is used as a dominant source of news distribution among users. The world's preeminent decisions such as politics are acclaimed by social media to influence users for enclosing users' decisions in their favor. However, the adoption of social media is much needed for awareness but the authenticity of content is an unknown factor in the current scenario. Therefore, this research work finds it imperative to propose a solution to fake news detection and classification. In the case of fake news, content is the prime entity that captures the human mind towards trust for specific news. Therefore, a linguistic model is proposed to find out the properties of content that will generate language-driven features. This linguistic model extracts syntactic, grammatical, sentimental, and readability features of particular news. Language driven model requires an approach to handle time-consuming and handcrafted features problems in order to deal with the curse of dimensionality problem. Therefore, the neural-based sequential learning model is used to achieve superior results for fake news detection. The results are drawn to validate the importance of the linguistic model extracted features and finally combined linguistic feature-driven model is able to achieve the average accuracy of 86% for fake news detection and classification. The sequential neural model results are compared with machine learning based models and LSTM based word embedding based fake news detection model as well. Comparative results show that features based sequential model is able to achieve comparable evaluation performance in discernable less time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030909X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Curse of dimensionality",
      "Deep learning",
      "Embedding",
      "Feature (linguistics)",
      "Feature engineering",
      "Language model",
      "Linguistics",
      "Machine learning",
      "Natural language processing",
      "Philosophy",
      "Programming language",
      "Readability",
      "Social media",
      "Word embedding",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Choudhary",
        "given_name": "Anshika"
      },
      {
        "surname": "Arora",
        "given_name": "Anuja"
      }
    ]
  },
  {
    "title": "Minority manifold regularization by stacked auto-encoder for imbalanced learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114317",
    "abstract": "Imbalanced learning is considered one of the challenging problems in machine learning. This problem arises when a learning algorithm is biased toward the majority class due to the large proportion of the majority class data while detecting the minority class is of greater importance. In the present study, a novel method (MMRAE) is presented for imbalanced learning encompassing feature learning and classification steps. In the feature learning step, meaningful features are extracted from the minority data and their underlying manifold are captured by taking advantage of one-class learning approach through stacking two regularized auto-encoders. The existence of novel and different regularizers in each auto-encoder leads to a new representation with proper data discrimination which improves the between-class and within-class imbalanced problems. Then, in the classification step, the classification between the minority and majority class is performed by constructing a multilayer neural network using features learned throughout pre-training. The proposed method is extensively studied on six artificial and twenty real datasets in order to have a precise evaluation. Based on different criteria such as F-measure, G-mean, and AUC, the results represent considerable performance of the proposed method compared to several other existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310125",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Class (philosophy)",
      "Computer science",
      "Dimensionality reduction",
      "Encoder",
      "Feature (linguistics)",
      "Feature learning",
      "Law",
      "Linguistics",
      "Machine learning",
      "Nonlinear dimensionality reduction",
      "Nuclear magnetic resonance",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Regularization (linguistics)",
      "Representation (politics)",
      "Stacking"
    ],
    "authors": [
      {
        "surname": "Farajian",
        "given_name": "Nima"
      },
      {
        "surname": "Adibi",
        "given_name": "Peyman"
      }
    ]
  },
  {
    "title": "Dynamic Boundary Time Warping for sub-sequence matching with few examples",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114344",
    "abstract": "The paper presents a novel method of finding a fragment in a long temporal sequence similar to the set of shorter sequences. We are the first to propose an algorithm for such a search that does not rely on computing the average sequence from query examples. Instead, we use query examples as is, utilizing all of them simultaneously. The introduced method based on the Dynamic Time Warping (DTW) technique is suited explicitly for few-shot query-by-example retrieval tasks. We evaluate it on two different few-shot problems from the field of Natural Language Processing. The results show it either outperforms baselines and previous approaches or achieves comparable results when a low number of examples is available.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310307",
    "keywords": [],
    "authors": [
      {
        "surname": "Borchmann",
        "given_name": "Łukasz"
      },
      {
        "surname": "Jurkiewicz",
        "given_name": "Dawid"
      },
      {
        "surname": "Graliński",
        "given_name": "Filip"
      },
      {
        "surname": "Górecki",
        "given_name": "Tomasz"
      }
    ]
  },
  {
    "title": "A hybrid approach to cardinality constraint portfolio selection problem based on nonlinear neural network and genetic algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114517",
    "abstract": "Portfolio Optimization (PO) is an extremely popular method that selects the best portfolio for an investor. When the classic methods fail to find an exact solution, heuristic techniques are designed to find an approximate solution. In the literature, such techniques have often been used in the portfolio selection problem. However, hardly any of these methods utilize a neural network to apportion the extent of stocks. The basic objective of portfolio optimization is to minimize the risk of the portfolio while maximizing the expected return of the portfolio. In reality, the Standard Portfolio Optimization Model does not fulfill stock market requirements in a money related world. Indeed, this problem cannot limit the amount of stock in the portfolio. On the other hand, the precise number of stocks are taken into account by the Cardinality Constraint Portfolio Optimization model, which is the Mixed-Integer Quadratic Programming problem. While minimizing the risk of the portfolio, limiting the expected return and the number of stocks is the main subject of this study. In this study, a hybrid approach is proposed, based on the Nonlinear Neural Network and the Genetic Algorithm to solve the Cardinality Constraint Portfolio Optimization Model. To investigate the effectiveness of the proposed hybrid approach, the Istanbul Stock Exchange (ISE-301) data is used. The ISE-30 data1 consists of daily prices, from May 2015 to May 2017. The ISE-30 data1 from May 2017 to July 2018 is used as out-of-sample. To clarify the effectiveness proposed, the method was applied to publicly data sets which are used for numerous different portfolio selection strategies in many articles. The proposed hybrid approach to the cardinality constraint portfolio optimization problem has more viable outcomes than current strategies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311611",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Cardinality (data modeling)",
      "Computer science",
      "Constraint (computer-aided design)",
      "Data mining",
      "Economics",
      "Financial economics",
      "Genetic algorithm",
      "Geometry",
      "Mathematical optimization",
      "Mathematics",
      "Optimization problem",
      "Portfolio",
      "Portfolio optimization"
    ],
    "authors": [
      {
        "surname": "Yaman",
        "given_name": "Ilgım"
      },
      {
        "surname": "Erbay Dalkılıç",
        "given_name": "Türkan"
      }
    ]
  },
  {
    "title": "On oversampling imbalanced data with deep conditional generative models",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114463",
    "abstract": "Class imbalanced datasets are common in real-world applications ranging from credit card fraud detection to rare disease diagnosis. Recently, deep generative models have proved successful for an array of machine learning problems such as semi-supervised learning, transfer learning, and recommender systems. However their application to class imbalance situations is limited. In this paper, we consider class conditional variants of generative adversarial networks and variational autoencoders and apply them to the imbalance problem. The main question we seek to answer is whether or not deep conditional generative models can effectively learn the distributions of minority classes so as to produce synthetic observations that ultimately lead to improvements in the performance of a downstream classifier. The numerical results show that this is indeed true and that deep generative models outperform traditional oversampling methods in many circumstances, especially in cases of severe imbalance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311155",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer network",
      "Computer science",
      "Deep learning",
      "Generative adversarial network",
      "Generative grammar",
      "Generative model",
      "Machine learning",
      "Oversampling"
    ],
    "authors": [
      {
        "surname": "Fajardo",
        "given_name": "Val Andrei"
      },
      {
        "surname": "Findlay",
        "given_name": "David"
      },
      {
        "surname": "Jaiswal",
        "given_name": "Charu"
      },
      {
        "surname": "Yin",
        "given_name": "Xinshang"
      },
      {
        "surname": "Houmanfar",
        "given_name": "Roshanak"
      },
      {
        "surname": "Xie",
        "given_name": "Honglei"
      },
      {
        "surname": "Liang",
        "given_name": "Jiaxi"
      },
      {
        "surname": "She",
        "given_name": "Xichen"
      },
      {
        "surname": "Emerson",
        "given_name": "D.B."
      }
    ]
  },
  {
    "title": "Multi-Objective Passing Vehicle Search algorithm for structure optimization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114511",
    "abstract": "A novel Multi-Objective Passing Vehicle Search (MOPVS) algorithm is proposed for structural design optimization. MOPVS is inspired by the two-lane highway passing vehicle mechanism. This multi-objective version is modified and further improved from the single-objective version of passing vehicle search through a Pareto dominance-based approach. For performance evaluation of MOPVS, five daunting benchmark structural design problems have been used. Two conflicting objectives i.e. structure weight minimization and minimization of maximum nodal displacement along with discrete design variables have been considered to ensure its real-world applications. For fitness and efficiency evaluation of the proposed algorithm, the results obtained from the new algorithm are compared with four other state-of-the-art multi-objective algorithms. Moreover, two performance indicators test called Hypervolume and Spacing-to-Extent were performed for the rigorous evaluation of the performance and feasibility of the proposed algorithm. The findings demonstrate the superiority of the MOPVS algorithm over the others while the potential to find a non-dominated solution set with diverse individual solutions. Present work considers the Friedman’s rank test for the statistical investigation of the experiment work. The solutions and convergence behavior achieved by MOPVS show its high efficiency in solving challenging design problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311556",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Mathematical optimization",
      "Mathematics",
      "Optimization algorithm"
    ],
    "authors": [
      {
        "surname": "Kumar",
        "given_name": "Sumit"
      },
      {
        "surname": "Tejani",
        "given_name": "Ghanshyam G."
      },
      {
        "surname": "Pholdee",
        "given_name": "Nantiwat"
      },
      {
        "surname": "Bureerat",
        "given_name": "Sujin"
      }
    ]
  },
  {
    "title": "Multi-hour and multi-site air quality index forecasting in Beijing using CNN, LSTM, CNN-LSTM, and spatiotemporal clustering",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114513",
    "abstract": "Effective air quality forecasting models are helpful for timely prevention and control of air pollution. However, the spatiotemporal distribution characteristics of air quality have not been fully considered in previous model development. This study attempts to establish a multi-time, multi-site forecasting model of Beijing’s air quality by using deep learning network models based on spatiotemporal clustering and to compare them with a back-propagationneural network (BPNN). For the overall forecasting, the performances in next-hour forecasting were ranked in ascending order of the BPNN, the convolutional neural network (CNN), the long short-term memory (LSTM) model, and the CNN-LSTM, with the LSTM as the optimal model in the multiple-hour forecasting. The performance of the seasonal forecasting was not significantly improved compared to the overall forecasting. For the spatial clustering-based forecasting, cluster 2 forecasting generally outperforms cluster 1 and the overall forecasting. Overall, either the seasonal or the spatial clustering-based forecasting is more suitable for the improvement of the forecasting in a certain season or cluster. In terms of model type, both the CNN-LSTM and the LSTM generally have better performance than the CNN and the BPNN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742031157X",
    "keywords": [
      "Air quality index",
      "Artificial intelligence",
      "Artificial neural network",
      "Beijing",
      "China",
      "Cluster analysis",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Deep learning",
      "Law",
      "Machine learning",
      "Meteorology",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Rui"
      },
      {
        "surname": "Liao",
        "given_name": "Jiaqiang"
      },
      {
        "surname": "Yang",
        "given_name": "Jie"
      },
      {
        "surname": "Sun",
        "given_name": "Wei"
      },
      {
        "surname": "Nong",
        "given_name": "Mingyue"
      },
      {
        "surname": "Li",
        "given_name": "Feipeng"
      }
    ]
  },
  {
    "title": "DGTL-Net: A Deep Generative Transfer Learning Network for Fault Diagnostics on New Hard Disks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114379",
    "abstract": "Intelligent fault diagnosis of hard disks becomes significantly important to guarantee reliability of current cloud-based industrial systems. Most intelligent diagnostic methods are commonly based on assumptions that data from different disks are subject to the same distribution and there are sufficient faulty samples for training the models. However, in reality, there are types of hard disks from different manufacturers and their SMART encoding varies widely across manufacturers. It results in distribution discrepancy among disks and influences the generalization of machine learning methods. Moreover, hard disks usually work in healthy state that faulty events rarely happen on most of them, or especially never occur on new ones. Thus, this paper proposes a deep generative transfer learning network (DGTL-Net) for intelligent fault diagnostics on new hard disks. The DGTL-Net combines the deep generative network that generates fake faulty samples and the deep transfer network that solves the problem of distribution discrepancy between hard disks. An iterative end-end training strategy is also proposed for DGTL-Net to get the most optimal parameters of generative and transfer network simultaneously. Experiments have been conducted to prove that our method achieves better performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310551",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Cloud computing",
      "Computer science",
      "Deep learning",
      "Fault (geology)",
      "Generalization",
      "Generative grammar",
      "Generative model",
      "Geology",
      "Geometry",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Net (polyhedron)",
      "Operating system",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Reliability (semiconductor)",
      "Seismology",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Chang"
      },
      {
        "surname": "Wu",
        "given_name": "Zhenyu"
      },
      {
        "surname": "Lv",
        "given_name": "Xiaomeng"
      },
      {
        "surname": "Ji",
        "given_name": "Yang"
      }
    ]
  },
  {
    "title": "Selective spatiotemporal features learning for dynamic gesture recognition",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114499",
    "abstract": "Gesture recognition, which aims to understand meaningful movements of human bodies, plays an essential role in human–computer interaction. The key to gesture recognition is to learn compact and effective spatiotemporal information. However, it remains a challenging task due to the barriers of gesture-irrelevant factors. A number of attempts have been taken to address this problem by cascading deep heterogeneous architectures. However, this cascading strategy cannot capture both local and global spatiotemporal features at each stage of feature learning. In this paper, we propose a novel refined fusion model architecture combining the ResC3D network and Convolutional LSTM (ConvLSTM) with a dynamic select mechanism called Selective Spatiotemporal features learning (SeST). Such a heterogeneous network system is able to simultaneously learn short-term and long-term spatiotemporal features, and they are complementary to each other. The SeST block enables the ResC3D network and ConvLSTM to adaptively adjust their contributions to classification during feature learning with soft-attention. The method has been evaluated on the three publicly available datasets: the Sheffield Kinect Gesture (SKIG) dataset, the ChaLearn LAP large scale isolated gesture dataset (IsoGD), and the EgoGesture dataset. Experiment results show that the proposed method outperforms other state-of-the-art methods. Besides, our model is an end-to-end model, which can be embedded in many intelligent systems applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742031143X",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Economics",
      "Feature (linguistics)",
      "Geometry",
      "Gesture",
      "Gesture recognition",
      "Linguistics",
      "Machine learning",
      "Management",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Xianlun"
      },
      {
        "surname": "Yan",
        "given_name": "Zhenfu"
      },
      {
        "surname": "Peng",
        "given_name": "Jiangping"
      },
      {
        "surname": "Hao",
        "given_name": "Bohui"
      },
      {
        "surname": "Wang",
        "given_name": "Huiming"
      },
      {
        "surname": "Li",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "Earned benefit maximization in social networks under budget constraint",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114346",
    "abstract": "Given a social network where the users are associated with non-uniform selection cost, the problem of Budgeted Influence Maximization (BIM in short) asks for selecting a subset of the nodes within an allocated budget for initial activation, such that due to the cascading effect, influence in the network is maximized. In this paper, we study this problem with a variation, where a subset of the users are marked as target users, each of them is assigned with a benefit and this can be earned by influencing them. The goal here is to maximize the earned benefit by initially activating a set of nodes within the budget. This problem is referred to as the Earned Benefit Maximization Problem. First, we show that this problem is NP-Hard and the benefit function follows the monotonicity, sub-modularity property under the Independent Cascade Model of diffusion. We propose an incremental greedy strategy for this problem and show, with minor modification it gives ( 1 − 1 e ) -factor approximation guarantee on the earned benefit. Next, by exploiting the sub-modularity property of the benefit function, we improve the efficiency of the proposed greedy algorithm. Then, we propose a hop-based heuristic method, which works based on the computation of the ‘expected earned benefit’. Finally, we perform a series of extensive experiments with four publicly available, real-life social network datasets. From the experiments, we observe that the seed sets selected by the proposed algorithms can achieve more benefit compared to many existing methods. Particularly, the hop-based approach is found to be more efficient than the other ones for solving this problem.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310319",
    "keywords": [
      "Biology",
      "Budget constraint",
      "Combinatorics",
      "Computer science",
      "Constraint (computer-aided design)",
      "Discrete mathematics",
      "Economics",
      "Evolutionary biology",
      "Function (biology)",
      "Genetics",
      "Geometry",
      "Greedy algorithm",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Modularity (biology)",
      "Monotone polygon",
      "Neoclassical economics",
      "Submodular set function"
    ],
    "authors": [
      {
        "surname": "Banerjee",
        "given_name": "Suman"
      },
      {
        "surname": "Jenamani",
        "given_name": "Mamata"
      },
      {
        "surname": "Pratihar",
        "given_name": "Dilip Kumar"
      }
    ]
  },
  {
    "title": "Highlighting keyphrases using senti-scoring and fuzzy entropy for unsupervised sentiment analysis",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114323",
    "abstract": "Sentiment Analysis is a process that aids in assessing the performance of products or services from user generated online posts. In present time, there are various websites that allow customers to post reviews about movies, products, events or services, etc. This has led to cumulative aggregation of a lot of reviews written in natural language. Prevailing factors such as availability of online reviews and raised end-user expectations have motivated the evolution of opinion mining systems that can automatically classify customers' reviews. It is observed that in Sentiment Analysis (SA), to highlight the significant keyphrases which contribute towards correct sentiment cognition is a tedious task. In this paper, we have proposed an unsupervised sentiment classification system that comprehensively formulates phrases, computes their senti-scores (sentiment scores) and polarity using the SentiWordNet lexicon and fuzzy linguistic hedges. Further it extracts the keyphrases significant for SA using fuzzy entropy filter and k-means clustering. We have deployed document level SA on online reviews using n-gram techniques, specifically combination of unigram, bigram and trigram. Experiments on two benchmark movie review datasets- polarity dataset by Pang and Lee and IMDB dataset, achieve high accuracy for our approach as compared to the other state-of-the-art-methods for phrase-level SA.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310162",
    "keywords": [
      "Artificial intelligence",
      "Bigram",
      "Computer science",
      "Computer vision",
      "Filter (signal processing)",
      "Lexicon",
      "Machine learning",
      "Natural language processing",
      "Phrase",
      "Sentiment analysis",
      "Trigram"
    ],
    "authors": [
      {
        "surname": "Vashishtha",
        "given_name": "Srishti"
      },
      {
        "surname": "Susan",
        "given_name": "Seba"
      }
    ]
  },
  {
    "title": "DEBOHID: A differential evolution based oversampling approach for highly imbalanced datasets",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114482",
    "abstract": "Class distribution of the samples in the dataset is one of the critical factors affecting the classification success. Classifiers trained with imbalanced datasets classify majority class samples more successfully than minority class samples. Oversampling, which is based on increasing the minority class samples, is a frequently used method to overcome the class imbalance. More than two decades, many oversampling methods are presented for the class imbalance problem. Differential Evolution is a metaheuristic algorithm that achieves successful results in a lot of domains. One of the main reasons for this success is that DE has an effective candidate individual generation mechanism. In this work, we propose a novel oversampling method based on a differential evolution algorithm for highly imbalanced datasets, and it is named as DEBOHID (A differential evolution based oversampling approach for highly imbalanced datasets). In order to show the success of DEBOHID, 44 highly imbalanced ratio datasets are used in experiments. The obtained results are compared with nine different state-of-art oversampling methods. In order to show the independence of the experimental results to classifier, Support Vector Machines (SVM), k-Nearest Neighbor (kNN), and Decision Tree (DT) are used as a classifier in the experiments. AUC and G-Mean metrics are used for the performance measurements. The experimental results and statistical analyses have shown the triumph of the DEBOHID.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311295",
    "keywords": [
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer network",
      "Computer science",
      "Data mining",
      "Decision tree",
      "Differential evolution",
      "Machine learning",
      "Oversampling",
      "Pattern recognition (psychology)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Kaya",
        "given_name": "Ersin"
      },
      {
        "surname": "Korkmaz",
        "given_name": "Sedat"
      },
      {
        "surname": "Sahman",
        "given_name": "Mehmet Akif"
      },
      {
        "surname": "Cinar",
        "given_name": "Ahmet Cevahir"
      }
    ]
  },
  {
    "title": "CSGAN: Cyclic-Synthesized Generative Adversarial Networks for image-to-image transformation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114431",
    "abstract": "The primary motivation of image-to-image transformation is to convert an image of one domain to another domain. The Generative Adversarial Network (GAN) is the recent trend for image-to-image transformation. The existing GAN models suffer due to the lack of utilization of proper synthesization objectives. In this paper, we propose a new Cyclic-Synthesized Generative Adversarial Networks (CSGAN) for the development of expert and intelligent systems for image-to-image transformation. The proposed CSGAN uses a new objective function based on the proposed cyclic-synthesized loss between the synthesized image of one domain and cycled image of another domain. The proposed CSGAN enforces the mapping from one domain to another domain more accurately by limiting the scope of redundant transformation with the help of the cyclic-synthesized loss. The performance of the proposed CSGAN is evaluated on four benchmark image-to-image transformation datasets, including CUHK Face dataset, WHU-IIP Thermal-Visible Face Dataset, CMP Facades dataset, and NYU-Depth Dataset. The results are computed using the widely used evaluation metrics such as MSE, SSIM, PSNR, and LPIPS. The experimental results of the proposed CSGAN approach are compared with the latest state-of-the-art approaches, such as GAN, Pix2Pix, DualGAN, CycleGAN, and PS2GAN. The proposed CSGAN technique outperforms all the methods over CUHK dataset, WHU-IIP dataset, NYU-Depth dataset, and exhibits promising and comparable performance over Facades dataset in terms of both qualitative and quantitative measures. The code is available at https://github.com/KishanKancharagunta/CSGAN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310940",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Code (set theory)",
      "Computer science",
      "Domain (mathematical analysis)",
      "Face (sociological concept)",
      "Gene",
      "Generative adversarial network",
      "Generative grammar",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Scope (computer science)",
      "Set (abstract data type)",
      "Social science",
      "Sociology",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Babu",
        "given_name": "Kancharagunta Kishan"
      },
      {
        "surname": "Dubey",
        "given_name": "Shiv Ram"
      }
    ]
  },
  {
    "title": "Identification of rice plant diseases using lightweight attention networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114514",
    "abstract": "Rice is one of the most important crops in the world, and most people consume rice as a staple food, especially in Asian countries. Various rice plant diseases have a negative effect on crop yields. If proper detection is not taken, they can spread and lead to a significant decline in agricultural productions. In severe cases, they may even cause no grain harvest entirely, thus having a devastating impact on food security. The deep learning-based CNN methods have become the standard methods to address most of the technical challenges related to image identification and classification. In this study, to enhance the learning capability for minute lesion features, we chose the MobileNet-V2 pre-trained on ImageNet as the backbone network and added the attention mechanism to learn the importance of inter-channel relationship and spatial points for input features. In the meantime, the loss function was optimized and the transfer learning was performed twice for model training. The proposed procedure presents a superior performance relative to other state-of-the-art methods. It achieves an average identification accuracy of 99.67% on the public dataset. Even under complicated backdrop conditions, the average accuracy reaches 98.48% for identifying rice plant diseases. Experimental findings demonstrate the validity of the proposed procedure, and it is accomplished efficiently for rice disease identification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311581",
    "keywords": [
      "Agricultural engineering",
      "Agriculture",
      "Agronomy",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Deep learning",
      "Ecology",
      "Engineering",
      "Epistemology",
      "Evolutionary biology",
      "Feature (linguistics)",
      "Food security",
      "Function (biology)",
      "Identification (biology)",
      "Linguistics",
      "Machine learning",
      "Mechanism (biology)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Rice plant",
      "Staple food",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Junde"
      },
      {
        "surname": "Zhang",
        "given_name": "Defu"
      },
      {
        "surname": "Zeb",
        "given_name": "Adnan"
      },
      {
        "surname": "Nanehkaran",
        "given_name": "Yaser A."
      }
    ]
  },
  {
    "title": "Time series classification based on multi-feature dictionary representation and ensemble learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114162",
    "abstract": "Time series classification is an important task for mining time series data, and many high level representations of time series have been proposed to address it. Symbolic Aggregate approXimation (SAX) is a classic high level symbolic representation method which can effectively reduce the dimensionality of time series. However, SAX-based methods for time series classification cannot achieve promising results, because SAX only extracts the mean feature of subsequence to make symbolization. In this paper, we present a novel ensemble method based on SAX called TBOPE, which is based on multi-feature dictionary representation and ensemble learning. Specifically, we first extract both the mean feature and trend feature of time series. Second, we create the histograms of two kinds of feature based on the Bag-of-Feature mode and construct multiple single classifiers. Finally, we build an ensemble classifier to improve the classification performance. Experimental results on various time series datasets have shown that the proposed method is competitive to state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309027",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Bounded function",
      "Classifier (UML)",
      "Computer science",
      "Curse of dimensionality",
      "Data mining",
      "Discriminative model",
      "Ensemble learning",
      "Feature (linguistics)",
      "Feature learning",
      "Histogram",
      "Image (mathematics)",
      "Law",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Series (stratigraphy)",
      "Subsequence",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Bai",
        "given_name": "Bing"
      },
      {
        "surname": "Li",
        "given_name": "Guiling"
      },
      {
        "surname": "Wang",
        "given_name": "Senzhang"
      },
      {
        "surname": "Wu",
        "given_name": "Zongda"
      },
      {
        "surname": "Yan",
        "given_name": "Wenhe"
      }
    ]
  },
  {
    "title": "Feature concatenation for adversarial domain adaptation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114490",
    "abstract": "Domain adaptation aims to mitigate the domain gap between the source and target domains so that knowledge can be transferred between domains. There are two key factors that determine the adaptation performance: transferability and discriminability. Transferability depends on the similarity of two domains. With transferability, the model learnt on the source domain can be used in the target domain. Discriminability indicates the separability of different classes. With discriminability, the adapted target features can be classified more accurately. Adversarial domain adaptation methods learn domain-invariant feature representations through adversarial learning. The domain-invariant feature representation guarantees the transferability. However, to obtain domain-invariant features, certain domain-specific information is suppressed, which may cause the loss of discriminability. To this end, we aim to enhance the discriminability by enriching the information contained in the domain-invariant features. We propose a Feature Concatenation for adversarial Domain Adaptation (FCDA) method. FCDA learns two feature extractors that can generate two different feature views for a sample. The concatenation of these two views is used as the feature representation of a sample, which we call the concatenation feature. Distribution alignment is performed on the concatenation features. We find that when the distributions of the concatenation features are aligned, the two feature views involved in a concatenation feature have different distributions. Thus, the concatenation feature contains more discriminative information, thereby enhancing the discriminative ability of the domain-invariant features. Experiments are carried out on four widely used datasets and FCDA exceeds some recent domain adaptation methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311350",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Concatenation (mathematics)",
      "Discriminative model",
      "Domain (mathematical analysis)",
      "Feature (linguistics)",
      "Feature vector",
      "Invariant (physics)",
      "Linguistics",
      "Mathematical analysis",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Jingyao"
      },
      {
        "surname": "Li",
        "given_name": "Zhanshan"
      },
      {
        "surname": "Lü",
        "given_name": "Shuai"
      }
    ]
  },
  {
    "title": "A two-phase iterative machine learning method in identifying mechanical biomarkers of peripheral neuropathy",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114333",
    "abstract": "Peripheral neuropathy that interrupts sensorimotor integration in the motor control process will lead to hand and upper limb motor function deficits in daily life. However, behavioral biomechanics and motor functions have never been considered in available diagnoses and clinical evaluations. Previous studies that investigate the behavioral biomechanics to delineate a specific peripheral neuropathy and its severity have shown evidences that certain biomechanical parameters have the potential to be identified as biomarkers for the detection of the neuropathy from an early stage. Nevertheless, datasets formed by behavioral biomechanical parameters are often characterized by the high dimensionality, the small sample size, and the high redundancy, which brings us challenges for making binary classification between patients and healthy controls. We propose a two-phase machine learning protocol using Random Forests (RFs) for the early variable screening and the (K)PCA-SVM system for the prediction and the final identification of biomarkers. We apply the proposed protocol to an example application of Carpal Tunnel Syndrome (CTS) and its prediction accuracy reaches 90.3% with 6 biomarker variables identified from 700 initial input variables. These promising results provide a paradigm shift of guidelines and directions of clinical test designs toward novel diagnostic optimization in the future.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742031023X",
    "keywords": [
      "Artificial intelligence",
      "Biomechanics",
      "Carpal tunnel syndrome",
      "Computer science",
      "Diabetes mellitus",
      "Endocrinology",
      "Machine learning",
      "Medicine",
      "Operating system",
      "Peripheral neuropathy",
      "Physical medicine and rehabilitation",
      "Physiology",
      "Redundancy (engineering)",
      "Support vector machine",
      "Surgery"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Yuan"
      },
      {
        "surname": "Gu",
        "given_name": "Feng"
      },
      {
        "surname": "Zhang",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "DTI based Alzheimer’s disease classification with rank modulated fusion of CNNs and random forest",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114338",
    "abstract": "Automated classification of Alzheimer’s disease (AD) plays a key role in the diagnosis of dementia. In this paper, we solve for the first time a direct four-class classification problem, namely, AD, Normal Control (CN), Early Mild Cognitive Impairment (EMCI) and Late Mild Cognitive Impairment (LMCI) by processing Diffusion Tensor Imaging (DTI) in 3D. DTI provides information on brain anatomy in form of Fractional Anisotropy (FA) and Mean Diffusivity (MD) along with Echo Planar Imaging (EPI) intensities. We separately train CNNs, more specifically, VoxCNNs on FA values, MD values, and EPI intensities on 3D DTI scan volumes. In addition, we feed average FA and MD values for each brain region, derived according to the Colin27 brain atlas, into a random forest classifier (RFC). These four (three separately trained VoxCNNs and one RFC) models are first applied in isolation for the above four-class classification problem. Individual classification results are then fused at the decision level using a modulated rank averaging strategy leading to a classification accuracy of 92.6%. Comprehensive experimentation on publicly available ADNI database clearly demonstrates the effectiveness of the proposed solution.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310265",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Fusion",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Random forest",
      "Rank (graph theory)"
    ],
    "authors": [
      {
        "surname": "De",
        "given_name": "Arijit"
      },
      {
        "surname": "Chowdhury",
        "given_name": "Ananda S."
      }
    ]
  },
  {
    "title": "Measuring inferred gaze direction to support analysis of people in a meeting",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114398",
    "abstract": "This paper introduces a method to infer gaze direction and the point of focus of participants involved in a collaborative activity, such as a meeting. It uses a single depth-based sensor placed overhead to capture the meeting, which has the benefit of avoiding occlusion and is unobtrusive, minimising possible changes in behaviour that might arise if people are aware of the sensor. The inferred gaze direction of each participant is estimated in the horizontal plane from the orientation of the head (yaw), derived from a segmentation of the depth image to generate an outline of the head. A common focus of attention is inferred by intersecting the gaze directions of each participant. Performance evaluation using a depth camera to record a meeting achieved a head detection performance of 99.6% and a valid gaze detection of 96.9%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310708",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Focus (optics)",
      "Gaze",
      "Geology",
      "Geometry",
      "Geomorphology",
      "Head (geology)",
      "Human–computer interaction",
      "Mathematics",
      "Operating system",
      "Optics",
      "Orientation (vector space)",
      "Overhead (engineering)",
      "Physics",
      "Point (geometry)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Wright",
        "given_name": "Robert"
      },
      {
        "surname": "Ellis",
        "given_name": "TJ"
      },
      {
        "surname": "Makris",
        "given_name": "Dimitrios"
      }
    ]
  },
  {
    "title": "A small-sample faulty line detection method based on generative adversarial networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114378",
    "abstract": "This paper presents a small-sample faulty line detection method for a small current grounded system (SCGS) using improved generative adversarial networks (GANs) to overcome the problems of low accuracy and poor economy associated with traditional methods. First, we collected real-time fault data of the dispatching system as the original sample and used the linear interpolation and normalization methods to preprocess the data. We extracted and labeled the features of the original sample to construct a feature vector that represents the single-phase grounded fault. Then, to solve the problem of insufficient faulty samples, this paper used the GAN to generate synthetic samples to expand the training sample size. To stabilize the GAN training, the Wasserstein distance was introduced to improve the GAN loss function (WGAN). Finally, for the synthetic samples, we used Batch Normalization (BN) and an Early Stopping algorithm to train the lightweight depthwise separable convolutional network (DSCN) classifier and obtain a faulty line detection model with fast convergence and high accuracy. With the data of an actual dispatching system in China as an example, the proposed method determined the faulty line within 100–200 ms with an accuracy of more than 96%, which is approximately 26% higher than the accuracy of traditional methods. Compared with mainstream machine learning algorithms, the proposed method realizes the self-extraction of faulty features and simplifies the algorithm. Moreover, the accuracy of the proposed method is much higher than that of other machine learning algorithms on small samples. It is worth noting that this method needs only the existing real-time dispatching data and does not need to install additional data acquisition and fault detection equipment, eliminating the complicated equipment installation and debugging process and greatly reducing equipment costs human operations and maintenance costs. The proposed method has been implemented in an actual power system in China for two years. The results show that it is highly accurate and cost-effective, performs well in real time, and has bright prospects for future applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742031054X",
    "keywords": [
      "Algorithm",
      "Anthropology",
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Classifier (UML)",
      "Computer science",
      "Normalization (sociology)",
      "Pattern recognition (psychology)",
      "Sample (material)",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Le"
      },
      {
        "surname": "Wei",
        "given_name": "Hua"
      },
      {
        "surname": "Lyu",
        "given_name": "Zhongliang"
      },
      {
        "surname": "Wei",
        "given_name": "Hongbo"
      },
      {
        "surname": "Li",
        "given_name": "Peijie"
      }
    ]
  },
  {
    "title": "A semantic approach for document classification using deep neural networks and multimedia knowledge graph",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114320",
    "abstract": "The amount of available multimedia data in different formats and from different sources increases everyday. From an information retrieval point of view, this high volume and heterogeneity of data involves several issues to be addressed related to information overload and lacks of well structured information. Even if modern information retrieval systems offer to the user manifold search options, it is still hard to find systems with optimal performances in the document seeking process starting from a given topic. In recent years, several frameworks have been proposed and developed to support this task based on different models and techniques. In this paper we propose a semantic approach to document classification using both textual and visual topic detection techniques based on deep neural networks and multimedia knowledge graph. A semantic multimedia knowledge base has been exploited and several experimental results show the effectiveness of our proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310149",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Economics",
      "Geometry",
      "Graph",
      "Information overload",
      "Information retrieval",
      "Knowledge base",
      "Management",
      "Mathematics",
      "Multimedia",
      "Operating system",
      "Point (geometry)",
      "Process (computing)",
      "Task (project management)",
      "Theoretical computer science",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Rinaldi",
        "given_name": "Antonio M."
      },
      {
        "surname": "Russo",
        "given_name": "Cristiano"
      },
      {
        "surname": "Tommasino",
        "given_name": "Cristian"
      }
    ]
  },
  {
    "title": "A reinforcement learning based multi-method approach for stochastic resource constrained project scheduling problems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114479",
    "abstract": "The Resource-Constrained Project Scheduling Problem (RCPSP) has been widely accepted as a challenging research topic due to its NP-hard nature. Because of the dynamic nature of real-world problems, stochastic-RCPSPs (SRCPSPs) are also receiving greater attention among researchers. To solve these extended RCPSPs (i.e., SRCPSPs), this paper proposes an reinforcement learning based meta-heuristic switching approach that utilizes the powers of both multi-operator differential evolution (MODE) and discrete cuckoo search (DCS) algorithms in single algorithmic framework. Reinforcement learning (RL) is introduced as a technique to select either MODE or DCS based on the diversity of population and quality of solutions. To deal with uncertain durations, a chance-constrained based approach with some belief degrees is also considered and solved by this proposed RL based multi-method approach (i.e., DECSwRL-CC). Extensive experimentation with benchmark data from the project scheduling library (PSPLIB) demonstrates the efficacy of this proposed multi-method approach. Numerous state of the art chance constrained approaches are taken from the literature to compare the proposed approach and to validate the efficacy of this multi-method approach. This particular strategy is applicable to the risk-averse decision-makers who want to realize the project schedule with a high degree of certainty.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311271",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Cuckoo search",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Particle swarm optimization",
      "Reinforcement learning",
      "Schedule",
      "Scheduling (production processes)"
    ],
    "authors": [
      {
        "surname": "Sallam",
        "given_name": "Karam M."
      },
      {
        "surname": "Chakrabortty",
        "given_name": "Ripon K."
      },
      {
        "surname": "Ryan",
        "given_name": "Michael J."
      }
    ]
  },
  {
    "title": "Analysis of concept drift in fake reviews detection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114318",
    "abstract": "Online reviews have a substantial impact on decision making in various areas of society, predominantly in the arena of buying and selling of goods. As such, the truthfulness of internet reviews is critical for both consumers and vendors. Fake reviews not only mislead innocent clients and influence customers' choice, leading to inaccurate descriptions and sales. This raises the need for efficient fake review detection models and tools that can address these issues. Analysing a text data stream of fake reviews in concept drift appears to reduce the effectiveness of the detection models. Despite several efforts to develop algorithms for detecting fake reviews, one crucial aspect that has not been addressed is finding a real correlation between the concept drift score and the classification of performance over-time in the real-world data stream. Consequently, we have introduced a comprehensive analysis to investigate the concept drift problem within fake review detection. There are two methods to achieve this goal: benchmarking concept drift detection method and content-based classification methods. We conducted our experiment using four real-world datasets from Yelp.com. The results demonstrated that there is a strong negative correlation between concept drift and the performance of fake review detection/prediction models, which indicates the difficulty of building more efficient models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310137",
    "keywords": [
      "Artificial intelligence",
      "Benchmarking",
      "Business",
      "Computer science",
      "Concept drift",
      "Data mining",
      "Data science",
      "Data stream mining",
      "Machine learning",
      "Marketing",
      "Sentiment analysis"
    ],
    "authors": [
      {
        "surname": "Mohawesh",
        "given_name": "Rami"
      },
      {
        "surname": "Tran",
        "given_name": "Son"
      },
      {
        "surname": "Ollington",
        "given_name": "Robert"
      },
      {
        "surname": "Xu",
        "given_name": "Shuxiang"
      }
    ]
  },
  {
    "title": "Atrial fibrillation detection using heart rate variability and atrial activity: A hybrid approach",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114452",
    "abstract": "Goal: Develop a real-time hybrid scheme for the automatic detection of atrial fibrillation (AF), based on the RR interval (RRI) time series and the atrial activity (AA) derived from the electrocardiogram (ECG) signals. Method: The whole scheme was developed and tested on the MIT-BIH AF database (AFDB). First the R-peak detection and the filtering was performed. Following, all features regarding the RRI time series and AA were extracted. These features were then fed into three popular classifiers (boosted trees (BoT), random forest (RF), and linear discriminant analysis (LDA) with random subspace method (RSM)). Sampling training and test data from the same subject (23 overall) was strictly avoided. Furthermore, for each ECG, individual performance statistics were analyzed to elaborate on the subject-wise performance dependencies. Results: From a 4-fold cross validation (CV) analysis, the RF classifier provided the best results with a sensitivity (Sn), specificity (Sp), accuracy (Acc), and F1 score of 98.0%, 97.4%, 97.6%, and 97.1%, respectively for the AF prediction. Test results on individual ECG’s however, have slightly reduced these performances to 95.9%, 96.1%, 97.4% and 88.4%, respectively. Conclusion: Using the RRI features alone were found to provide satisfying prediction performance of the model. The addition of AA features to the model enhanced the model performance by up to 3%. Overall, the results obtained in this study are comparable or even superior to the state-of-the-art algorithms using RRI and AA based features. Significance The hybrid model allows us to detect AF even with regular RRI. The performance was evaluated under real-world conditions, and no manual labelling, exclusion, or pre-processing was performed. Furthermore, we evaluated the performance for each ECG individually and kept the subjects strictly unknown for the classifier. Finally, we show that the overall performance on a data set, especially from a standard CV, results in an over-optimistic estimation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311064",
    "keywords": [
      "Artificial intelligence",
      "Atrial fibrillation",
      "Cardiology",
      "Computer science",
      "Linear discriminant analysis",
      "Mathematics",
      "Medicine",
      "Pattern recognition (psychology)",
      "Random forest",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Hirsch",
        "given_name": "Gerald"
      },
      {
        "surname": "Jensen",
        "given_name": "Søren H."
      },
      {
        "surname": "Poulsen",
        "given_name": "Erik S."
      },
      {
        "surname": "Puthusserypady",
        "given_name": "Sadasivan"
      }
    ]
  },
  {
    "title": "Dual-path attention network for single image super-resolution",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114450",
    "abstract": "Deep convolutional neural networks (CNNs) have recently made remarkable advances in single image super-resolution (SISR). The CNN structures of most existing SISR methods are just based on residual structures, dense structures, or their variants. However, these methods almost all adopt single-path structures, which makes them difficult to make full use of the complementary contextual information of the different ways of feature extraction (e.g., residual and dense connections). In this paper, we develop a novel dual-path attention network, which includes the dual-path attention groups (DPAGs) with dual skip connections (DSCs), in order to combine the advantages of both residual and dense connections for better SR performance. Each DPAG has several dual-path blocks (DPBs) and a path attention fusion (PAF). The DPBs realize the structure of the dual-path topology, while the PAF can further improve the discriminative representation ability by a channel attention (CA) mechanism, adaptively fuse the complementary contextual information produced by the two paths, and stabilize the network. Our DPAN can well pay attention to high-frequency information because each DSC contains a local skip connection and an adaptively weighted global skip connection (AWGSC), which can further adaptively bypass low-frequency features. Extensive experimental results demonstrate the superiority of the proposed DPAN in terms of both quantitative metrics and visual quality, compared with the current state-of-the-art SISR methods. For instance, compared with recent typical methods, for Bicubic (BI) degradation on the difficult dataset Urban100, our DPAN achieved the best PSNR of 33.22 dB for scale × 2 , 29.20 dB for scale × 3 , and 26.99 dB for scale × 4 , respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311040",
    "keywords": [
      "Algorithm",
      "Art",
      "Artificial intelligence",
      "Attention network",
      "Combinatorics",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Discriminative model",
      "Dual (grammatical number)",
      "Feature (linguistics)",
      "Law",
      "Linguistics",
      "Literature",
      "Mathematics",
      "Path (computing)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Residual",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Zhiyong"
      },
      {
        "surname": "Li",
        "given_name": "Wenbin"
      },
      {
        "surname": "Li",
        "given_name": "Jinxin"
      },
      {
        "surname": "Zhou",
        "given_name": "Dengwen"
      }
    ]
  },
  {
    "title": "SuperDeConFuse: A supervised deep convolutional transform based fusion framework for financial trading systems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114206",
    "abstract": "This work proposes a supervised multi-channel time-series learning framework for financial stock trading. Although many deep learning models have recently been proposed in this domain, most of them treat the stock trading time-series data as 2-D image data, whereas its true nature is 1-D time-series data. Since the stock trading systems are multi-channel data, many existing techniques treating them as 1-D time-series data are not suggestive of any technique to effectively fusion the information carried by the multiple channels. To contribute towards both of these shortcomings, we propose an end-to-end supervised learning framework inspired by the previously established (unsupervised) convolution transform learning framework. Our approach consists of processing the data channels through separate 1-D convolution layers, then fusing the outputs with a series of fully-connected layers, and finally applying a softmax classification layer. The peculiarity of our framework, that we call SuperDeConFuse (SDCF), is that we remove the nonlinear activation located between the multi-channel convolution layers and the fully-connected layers, as well as the one located between the latter and the output layer. We compensate for this removal by introducing a suitable regularization on the aforementioned layer outputs and filters during the training phase. Specifically, we apply a logarithm determinant regularization on the layer filters to break symmetry and force diversity in the learnt transforms, whereas we enforce the non-negativity constraint on the layer outputs to mitigate the issue of dead neurons. This results in the effective learning of a richer set of features and filters with respect to a standard convolutional neural network. Numerical experiments confirm that the proposed model yields considerably better results than state-of-the-art deep learning techniques for the real-world problem of stock trading.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309349",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Softmax function",
      "Supervised learning"
    ],
    "authors": [
      {
        "surname": "Gupta",
        "given_name": "Pooja"
      },
      {
        "surname": "Majumdar",
        "given_name": "Angshul"
      },
      {
        "surname": "Chouzenoux",
        "given_name": "Emilie"
      },
      {
        "surname": "Chierchia",
        "given_name": "Giovanni"
      }
    ]
  },
  {
    "title": "A hesitant fuzzy wind speed forecasting system with novel defuzzification method and multi-objective optimization algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114364",
    "abstract": "Owing to the nondeterministic nature of wind speed, the conventional fuzzy time series forecasting model has difficulty in establishing a common membership level. Therefore, in this study, the fuzzy series forecasting model was improved based on hesitant fuzzy sets. A hesitant fuzzy wind speed forecasting system with a novel defuzzification method and multiobjective optimization algorithm was developed. First, an advanced decomposition model is employed to extract the effective feature and remove the noise component from the raw wind speed series. Then, the universe of discourse is partitioned into equal and unequal intervals by multifuzzification methods and merged by aggregating hesitant information. A multiobjective intelligent optimization algorithm is applied to determine the optimal weights of different intervals accurately and stably. Furthermore, a novel defuzzification model based on an ordered weighted averaging operator and a regular increasing monotone quantifier is proposed to calculate the final forecasting results. The crucial strengths of the developed system are verifying the possibility of enhancing the performance of wind speed forecasting models by improving conventional fuzzy time series forecasting models and integrating them with decomposition models and artificial-intelligence models. Typical wind speed series datasets with different resolutions were selected to evaluate the performance of the proposed system, and experimental results prove that the proposed system outperforms other comparison models with high forecasting accuracy and computing efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310447",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Data mining",
      "Defuzzification",
      "Feature (linguistics)",
      "Fuzzy logic",
      "Fuzzy number",
      "Fuzzy set",
      "Linguistics",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Paleontology",
      "Philosophy",
      "Series (stratigraphy)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jianzhou"
      },
      {
        "surname": "Li",
        "given_name": "Hongmin"
      },
      {
        "surname": "Wang",
        "given_name": "Ying"
      },
      {
        "surname": "Lu",
        "given_name": "Haiyan"
      }
    ]
  },
  {
    "title": "A new modular neural network approach with fuzzy response integration for lung disease classification based on multiple objective feature optimization in chest X-ray images",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114361",
    "abstract": "This paper describes a new hybrid approach, based on modular artificial neural networks with fuzzy logic integration, for the diagnosis of pulmonary diseases such as pneumonia and lung nodules. In particular, the proposed approach analyzes medical images, which are digitized chest X-rays, focusing on a classification method based on descriptors, such as grayscale histogram features, gray-level co-occurrence matrix (GLCM) texture-based features, and local binary pattern texture features. Then, to perform feature reduction, a multi-objective genetic algorithm is used to obtain an optimized neuro-fuzzy classifier, which is able to classify the pathology found in the analyzed chest X-ray. The main contribution of this paper is the proposed modular neural network approach, which divides features to achieve specialized analysis in the modules for digital image analysis and classification. The proposed approach achieves high classification accuracy after evaluating the neuro-fuzzy model with three large datasets of chest X-rays.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310423",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Classifier (UML)",
      "Computer science",
      "Feature (linguistics)",
      "Fuzzy logic",
      "Grayscale",
      "Histogram",
      "Image (mathematics)",
      "Linguistics",
      "Local binary patterns",
      "Modular design",
      "Modular neural network",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Segmentation",
      "Time delay neural network"
    ],
    "authors": [
      {
        "surname": "Varela-Santos",
        "given_name": "Sergio"
      },
      {
        "surname": "Melin",
        "given_name": "Patricia"
      }
    ]
  },
  {
    "title": "Evaluating MFCC-based speaker identification systems with data envelopment analysis",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114448",
    "abstract": "The concept of the efficiency of speaker recognition systems varies in the literature. Although many authors have defined efficiency as recognition accuracy, others have defined it as low energy consumption, memory storage, or computational burden. In our study, for a novel approach, speaker recognition was evaluated following a multi-criteria decision-making approach in two stages. First, speaker identification based on Mel-frequency cepstrum coefficients (MFCC) was conducted for various parameters and methods, including number of speakers, number of MFCCs, test speech duration, training utterance length and the various classifiers. Classification metrics, memory storage, testing, and training time of the trials were measured as well, and the performance of the trials was examined for each criterion. Verifying the literature, the study revealed that no parameters or methods achieved the best performance for all criteria. In the second stage, a multi-criteria efficiency analysis, as suggested in the literature, was conducted according to various application scenarios. By using data envelopment analysis, the efficiency of trials according to the scenarios was determined. After ranking the efficiency scores, it was revealed that the best solution was task-dependent. From the perspective of classifiers, artificial neural networks outperformed the others considering benefits to cost; however, some of their costs were high, whereas the other classifiers provided the best solutions in light of cost criteria. Last, the number of MFCCs was the least effective parameter for efficiency. Altogether, the findings indicate that the efficiency of a speaker identification system cannot be defined as recognition accuracy, memory storage, testing time or training time but as a function of those criteria.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311039",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Data envelopment analysis",
      "Feature extraction",
      "Identification (biology)",
      "Machine learning",
      "Mathematics",
      "Mel-frequency cepstrum",
      "Pattern recognition (psychology)",
      "Ranking (information retrieval)",
      "Speaker recognition",
      "Speech recognition",
      "Statistics",
      "Utterance"
    ],
    "authors": [
      {
        "surname": "Özcan",
        "given_name": "Zübeyir"
      },
      {
        "surname": "Kayıkçıoğlu",
        "given_name": "Temel"
      }
    ]
  },
  {
    "title": "A computational optimization method for scheduling resource-constraint sequence-dependent changeovers on multi-machine production lines",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114265",
    "abstract": "This paper presents an algorithmic method that hybridizes solution procedures with an optimization model for solving the problem of scheduling and allocating changeover tasks. This problem involves a limited number of changeover workers with different skills and a set of sequence-dependent changeover tasks on a multi-machines production line, where executing each task requires a particular type and number of workers. For solving this allocation and scheduling problem, the paper identifies and investigates three priority rules expressed as objective functions to minimize the total changeover time and maximize the workers' utilization while satisfying changeover task-sequence-dependency and worker-limit constraints. For illustrating the applicability and suitability of the proposed approach, a real-world case problem of a 10-machine production line is considered for scheduling and allocating 34 sequence-dependent changeover tasks to 4 workers. The computational effort and time for optimizing each of the three identified objective functions using the proposed method were significantly undersized. The key results are the effective changeover time and workers' utilization provided by the proposed hybrid approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309775",
    "keywords": [
      "Biology",
      "Changeover",
      "Computer science",
      "Engineering",
      "Genetics",
      "Job shop scheduling",
      "Law",
      "Mathematical optimization",
      "Mathematics",
      "Mechanical engineering",
      "Operating system",
      "Political science",
      "Production line",
      "Schedule",
      "Scheduling (production processes)",
      "Sequence (biology)",
      "Systems engineering",
      "Task (project management)",
      "Telecommunications",
      "Time constraint",
      "Time limit",
      "Transmission (telecommunications)"
    ],
    "authors": [
      {
        "surname": "Saeed Osman",
        "given_name": "Mojahid"
      }
    ]
  },
  {
    "title": "Monarch butterfly optimization: A comprehensive review",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114418",
    "abstract": "Swarm intelligence (SI) is the collective behavior of decentralized, self-organized natural or artificial systems. Monarch butterfly optimization (MBO) algorithm is a class of swarm intelligence metaheuristic algorithm inspired by the migration behavior of monarch butterflies. Through the migration operation and butterfly adjusting operation, individuals in MBO are updated. MBO can outperform many state-of-the-art optimization techniques when solving global numerical optimization and engineering problems. This paper presents a comprehensive review of the MBO algorithm including its modifications, hybridizations, variants, and applications. Additionally, further research directions for MBO are discussed. This review study serves as a solid reference for future studies in the arena of SI and in particular the MBO algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310848",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Butterfly",
      "Computer science",
      "Ecology",
      "Engineering",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Operations research",
      "Particle swarm optimization",
      "Swarm behaviour",
      "Swarm intelligence"
    ],
    "authors": [
      {
        "surname": "Feng",
        "given_name": "Yanhong"
      },
      {
        "surname": "Deb",
        "given_name": "Suash"
      },
      {
        "surname": "Wang",
        "given_name": "Gai-Ge"
      },
      {
        "surname": "Alavi",
        "given_name": "Amir H."
      }
    ]
  },
  {
    "title": "Construction site layout planning problem: Past, present and future",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114247",
    "abstract": "Purpose Designing the construction site layout is a facilities allocation problem where allocating facilities in the right place would help in reducing the overall construction cost. Most of the relevant peer-reviewed literature discussed static site layout models where facilities are allocated once throughout the construction period. More recently, dynamic site layout models were introduced to allow relocation of facilities in between the stages. This is because most of the site facilities are not required throughout the whole construction period. Methodology This article presents a comprehensive literature review of the construction site layout problem for both static and dynamic approaches. Findings Peer-reviewed articles on construction site layout optimization were collected and their methods, assumptions, limitations and contributions were summarized and critically analyzed to provide a key-reference point for decision makers and researchers in this field. A gap analysis of the literature was done and recommendations for future research are presented.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309647",
    "keywords": [
      "Civil engineering",
      "Computer science",
      "Computer security",
      "Construction engineering",
      "Engineering",
      "Facility location problem",
      "Field (mathematics)",
      "Geometry",
      "Key (lock)",
      "Mathematics",
      "Operations research",
      "Point (geometry)",
      "Programming language",
      "Pure mathematics",
      "Relocation",
      "Site planning"
    ],
    "authors": [
      {
        "surname": "Hawarneh",
        "given_name": "Alaa Al"
      },
      {
        "surname": "Bendak",
        "given_name": "Salaheddine"
      },
      {
        "surname": "Ghanim",
        "given_name": "Firas"
      }
    ]
  },
  {
    "title": "Optimized neural network combined model based on the induced ordered weighted averaging operator for vegetable price forecasting",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114232",
    "abstract": "Vegetable price predictions are of great significance to vegetable growers, particularly regarding production and management to ensure a balance in the regional supply and demand of vegetables. In the current paper, in order to improve the accuracy and efficiency of vegetable price forecasting, we propose an optimized neural network combined model based on the induced ordered weighted averaging operator. Our frameworks integrate the fruit fly algorithm (FOA) with the induced ordered weighted averaging (IWOA) operator for an enhanced performance. In particular, the FOA is employed for the parameter optimization of the generalized regression neural network (GRNN) and radial basis function neural network (RBFNN), reducing the adverse influence of man-induced factors in the model construction process and improving the learning ability of both GRNN and RBFNN. The IWOA operator calculates the weights of the single GRNN and RBFNN to address the problem of fixed weights in combination forecasting models. Monthly vegetable price data in Beijing was used to compare our method with nine single forecasting models, revealing that the optimization of the GRNN and RBFNN parameters by the FOA, the prediction accuracy of the FOA-GRNN model and FOA-RBFNN model surpass those of GRNN and RBFNN, respectively. Furthermore, results from four evaluation indexes reveal that the IOWA-based optimized neural network model exhibited a stronger predictive ability than the other nine prediction models. Results demonstrate the effectiveness of our framework for the prediction of vegetable price series, with potential applications in agricultural products of similar characteristics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309520",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Gene",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Operator (biology)",
      "Repressor",
      "Transcription factor"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Bo"
      },
      {
        "surname": "Ding",
        "given_name": "Junqi"
      },
      {
        "surname": "Yin",
        "given_name": "Zhengqing"
      },
      {
        "surname": "Li",
        "given_name": "Kaiyu"
      },
      {
        "surname": "Zhao",
        "given_name": "Xue"
      },
      {
        "surname": "Zhang",
        "given_name": "Lingxian"
      }
    ]
  },
  {
    "title": "A simplistic approach without epsilon to choose the most efficient unit in data envelopment analysis",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114472",
    "abstract": "Data envelopment analysis is a very effective mathematical instrument in assessing the performance of decision-making units. In most real cases, the decision-maker need to identify a single most efficient unit. Several approaches were proposed for this necessity in the literature using data envelopment analysis. This study, based on the two steps model suggested by Toloo and Salahi (2018), proposes a new model without epsilon to choose the most efficient unit. The proposed model has fewer constraints than their model and is solved by a one-step linear programming model without epsilon. The proposed model determines exactly one DMU as the most efficient one and other decision-making units have efficiency scores strictly less than one. A simulation study was designed to test the proposed model in terms of some criteria such as correlation. In addition, the examples of real cases whose real rank is known and frequently used in literature of the most efficient unit were preferred for the validity of the proposed model. The results illustrated that the discrimination power problem was experienced in the previous models whereas no such problem was observed in the new proposed model for the same real cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311222",
    "keywords": [
      "Algorithm",
      "Combinatorics",
      "Computer science",
      "Data envelopment analysis",
      "Decision maker",
      "Linear programming",
      "Mathematical optimization",
      "Mathematics",
      "Mathematics education",
      "Operations research",
      "Rank (graph theory)",
      "Unit (ring theory)"
    ],
    "authors": [
      {
        "surname": "Özsoy",
        "given_name": "Volkan Soner"
      },
      {
        "surname": "Örkcü",
        "given_name": "H. Hasan"
      },
      {
        "surname": "Örkcü",
        "given_name": "Mediha"
      }
    ]
  },
  {
    "title": "FuzzyGCP: A deep learning architecture for automatic spoken language identification from speech signals",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114416",
    "abstract": "In this modern era, language has no geographic boundary. Therefore, for developing an automated system for search engines using audio, tele-medicine, emergency service via phone etc., the first and foremost requirement is to identify the language. The fundamental difficulty of automatic speech recognition is that the speech signals vary significantly due to different speakers, speech variation, language variation, age and sex wise voice modulation variation, contents and acoustic conditions and so on. In this paper, we have proposed a deep learning based ensemble architecture, called FuzzyGCP, for spoken language identification from speech signals. This architecture combines the classification principles of a Deep Dumb Multi Layer Perceptron (DDMLP), Deep Convolutional Neural Network (DCNN) and Semi-supervised Generative Adversarial Network (SSGAN) to increase the precision to maximum and finally applies Ensemble learning using Choquet integral to predict the final output, i.e., the language class. We have evaluated our model on four standard benchmark datasets comprising of two Indic language datasets and two foreign language datasets. Irrespective of the languages, the F1-score of the proposed language identification model is as high as 98% in MaSS dataset and worst performance is that of 67% on the VoxForge dataset which is much better compared to maximum of 44% by state-of-the-art models on multi-class classification. The link to the source code of our model is available here.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310824",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Astrophysics",
      "Benchmark (surveying)",
      "Biology",
      "Botany",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Geodesy",
      "Geography",
      "Identification (biology)",
      "Language identification",
      "Language model",
      "Multilayer perceptron",
      "Natural language",
      "Natural language processing",
      "Perceptron",
      "Physics",
      "Speech recognition",
      "Spoken language",
      "Variation (astronomy)"
    ],
    "authors": [
      {
        "surname": "Garain",
        "given_name": "Avishek"
      },
      {
        "surname": "Singh",
        "given_name": "Pawan Kumar"
      },
      {
        "surname": "Sarkar",
        "given_name": "Ram"
      }
    ]
  },
  {
    "title": "How to exploit fitness landscape properties of timetabling problem: A new operator for quantum evolutionary algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114211",
    "abstract": "The fitness landscape of the timetabling problems is analyzed in this paper to provide some insight into the properties of the problem. The analyses suggest that the good solutions are clustered in the search space and there is a correlation between the fitness of a local optimum and its distance to the best solution. Inspired by these findings, a new operator for Quantum Evolutionary Algorithms is proposed which, during the search process, collects information about the fitness landscape and tried to capture the backbone structure of the landscape. The knowledge it has collected is used to guide the search process towards a better region in the search space. The proposed algorithm consists of two phases. The first phase uses a tabu mechanism to collect information about the fitness landscape. In the second phase, the collected data are processed to guide the algorithm towards better regions in the search space. The algorithm clusters the good solutions it has found in its previous search process. Then when the population is converged and trapped in a local optimum, it is divided into sub-populations and each sub-population is designated to a cluster. The information in the database is then used to reinitialize the q-individuals, so they represent better regions in the search space. This way the population maintains diversity and by capturing the fitness landscape structure, the algorithm is guided towards better regions in the search space. The algorithm is compared with some state-of-the-art algorithms from PATAT competition conferences and experimental results are presented.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309386",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Cultural algorithm",
      "Demography",
      "Evolutionary algorithm",
      "Exploit",
      "Fitness landscape",
      "Gene",
      "Guided Local Search",
      "Local optimum",
      "Local search (optimization)",
      "Mathematical optimization",
      "Mathematics",
      "Meta-optimization",
      "Operating system",
      "Operator (biology)",
      "Optimization problem",
      "Population",
      "Process (computing)",
      "Repressor",
      "Search algorithm",
      "Sociology",
      "Space (punctuation)",
      "Tabu search",
      "Transcription factor"
    ],
    "authors": [
      {
        "surname": "Najaran",
        "given_name": "Mohammad Hassan Tayarani"
      }
    ]
  },
  {
    "title": "Survival exploration strategies for Harris Hawks Optimizer",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114243",
    "abstract": "This paper proposes new versions of Harris Hawks Optimizer (HHO) incorporated the survival-of-the-fittest principle of evolutionary algorithms. HHO is the recent swarm-based optimization algorithm imitating the surprise pounce behaviour of Harris’ hawks chasing style. HHO can show different patterns of the exploration and exploitation. It has a simple and time-varying structure, which further assist a smooth transition between the core phases. It has two main phases to iterate toward the optimal solution: exploration and exploitation. In the exploration phase, the current solution is either randomly modified based on any solution selected randomly or rebuilt from scratch. In evolutionary algorithms, selecting any solution from swarm basically relies on the natural selection principle of the survival-of-the-fittest to accelerate convergence. To make use of such principle, three selection strategies (i.e., tournament, proportional and linear rank-based methods) are employed in the exploration phase of HHO and introduces three new versions, which are Tournament HHO (THHO), Proportional HHO (PHHO), and Linear-Rank HHO (LHHO). In order to evaluate the performance of the proposed HHO versions, 23 well-regarded benchmark functions with various sizes and complexities are utilized as well as three real-world engineering problems. The sensitivity of proposed HHO versions to their parameter settings are studied and analyzed. Thereafter, a scalability study is conducted to show the effect of the population dimensions on the proposed HHO versions. Comparative evaluation shows that THHO version has superiority over other proposed HHO versions. Furthermore, the proposed HHO versions show enhanced trade off between the exploratory and exploitative trends and a better local optima avoidance. They are able to produce viable results competitively comparable with other eleven state-of-the-art methods using the same benchmark functions. Interestingly, the proposed variants of HHO are able to yield new results for some benchmark functions. Furthermore, three real-world engineering optimization problem of IEEE CEC2011 are also used in the evaluation process. Again, the proposed variants of HHO are able to achieve the best results. The information, guides and supplementary accessible files for this research will be publicly available at https://aliasgharheidari.com.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309611",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Combinatorics",
      "Computer science",
      "Demography",
      "Evolutionary algorithm",
      "Evolutionary biology",
      "Geodesy",
      "Geography",
      "Mathematical optimization",
      "Mathematics",
      "Population",
      "Rank (graph theory)",
      "Sociology",
      "Survival of the fittest"
    ],
    "authors": [
      {
        "surname": "Al-Betar",
        "given_name": "Mohammed Azmi"
      },
      {
        "surname": "Awadallah",
        "given_name": "Mohammed A."
      },
      {
        "surname": "Heidari",
        "given_name": "Ali Asghar"
      },
      {
        "surname": "Chen",
        "given_name": "Huiling"
      },
      {
        "surname": "Al-khraisat",
        "given_name": "Habes"
      },
      {
        "surname": "Li",
        "given_name": "Chengye"
      }
    ]
  },
  {
    "title": "An interpretable sequential three-way recommendation based on collaborative topic regression",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114454",
    "abstract": "Owing to the imbalance of observed data and user’s preference, it is necessary and meaningful to think over the multilevel characteristics of recommendation information (RI) during the recommendation process. At the same time, to better summarize user’s preference and enhance user’s beliefs, the interpretability of recommendation results also become more and more important in recommender system (RS). In view of the multilevel characteristics of RI and the interpretability of recommendation results, this paper proposes a novel interpretable sequential three-way recommendation strategy, namely, CTR-based cost-sensitive sequential three-way recommendation (CTR-CS3WR). First, in order to construct the interpretable granular features and multilevel information, we introduce collaborative topic regression (CTR) and design three novel granulation methods: PMF-based, LDA-based and CTR-based granulation method. Then, with the consideration of decision cost and time cost, a sequential three-way recommendation strategy is proposed to realize the multilevel recommendation. Finally, extensive experiments on two CiteUlike datasets verify the effectiveness of our proposed granulation methods and recommendation strategy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311076",
    "keywords": [
      "Artificial intelligence",
      "Collaborative filtering",
      "Computer science",
      "Construct (python library)",
      "Data mining",
      "Information retrieval",
      "Interpretability",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Preference",
      "Process (computing)",
      "Programming language",
      "Recommender system",
      "Regression",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Ye",
        "given_name": "Xiaoqing"
      },
      {
        "surname": "Liu",
        "given_name": "Dun"
      }
    ]
  },
  {
    "title": "Efficiency of evolutionary search for analog filter synthesis",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114267",
    "abstract": "Design automation presents a trade off between: using expert knowledge to restrict the possible solutions examined; or spending time searching through many, possibly ineffective, solutions. Incorrect assumptions or misapplication of constraints can miss otherwise superior results. Automated search of possible solutions has been shown effective, limited by the available computational resources. This paper presents an evolutionary search method for finding optimized circuit topologies and component values. An example analog filter problem is exhaustively enumerated, to test the efficiency of different stochastic search methods in finding globally optimal solutions. Evolutionary methods are shown to be efficient for this problem. Impacts of varying parameters and techniques of evolutionary search are compared for a more complex asymmetric bandpass filter problem. Genetic Algorithm and Evolutionary Strategy methods are found to have similar performance. Hybrid evolutionary methods using Differential Evolution for component value optimization are found to be more efficient with limited: component count; or computational resources. The hybrid method used is shown to scale to more complex problems without changing parameters. Proposed comparison metrics, normalized for experimental variables, show the efficiency of this work improves upon published benchmarks. This is achieved without restricting the synthesized topologies to known structures, producing novel results more effective than prior works.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309787",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Component (thermodynamics)",
      "Computer science",
      "Computer vision",
      "Differential evolution",
      "Evolutionary algorithm",
      "Evolutionary computation",
      "Filter (signal processing)",
      "Genetic algorithm",
      "Local search (optimization)",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Network topology",
      "Operating system",
      "Physics",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Pillans",
        "given_name": "John"
      }
    ]
  },
  {
    "title": "Maximum likelihood and maximum product of spacings estimations for the parameters of skew-normal distribution under doubly type II censoring using genetic algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114407",
    "abstract": "In this study, we use the maximum likelihood (ML) and the maximum product of spacings (MPS) methodologies to estimate the location, scale and skewness parameters of the skew-normal distribution under doubly type II censoring. However, it is known that these estimators cannot be obtained analytically because of nonlinear functions in the estimating equations. Therefore, numerical methods such as Nelder–Mead (NM), Newton–Raphson (NR), iteratively re-weighting algorithm (IRA), etc., are used to overcome this problem. In this study, different than the earlier studies, we use the genetic algorithm (GA) which is a population-based heuristic method based on the random search to find the estimates of the unknown parameters. In constructing the search space for GA, we utilize the robust confidence intervals to have a high convergence rate in GA. Then, we compare the efficiencies of the ML estimators obtained by using NM, NR, IRA, and GA methods and the efficiencies of the MPS estimators obtained by using NM and GA methods via an extensive Monte-Carlo simulation study. At the end of the study, we analyze the deep-groove ball bearings data to show the implementation of the proposed methodology.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310769",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Censoring (clinical trials)",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Demography",
      "Estimator",
      "Expectation–maximization algorithm",
      "Mathematical optimization",
      "Mathematics",
      "Maximum likelihood",
      "Medicine",
      "Monte Carlo method",
      "Population",
      "Radiology",
      "Rate of convergence",
      "Skew",
      "Skewness",
      "Sociology",
      "Statistics",
      "Telecommunications",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Yalçınkaya",
        "given_name": "Abdullah"
      },
      {
        "surname": "Yolcu",
        "given_name": "Ufuk"
      },
      {
        "surname": "Şenoǧlu",
        "given_name": "Birdal"
      }
    ]
  },
  {
    "title": "Center-aligned domain adaptation network for image classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114381",
    "abstract": "For a target task where the labeled data are unavailable, unsupervised domain adaptive learning performs transfer learning from labeled source data to unlabeled target data. Previous deep domain adaption methods mainly learned the global domain shift between different domains, the global distributions are aligned without considering the correspondence information between the same class data of different domains. Recently, more and more researchers pay attention to semantic alignment that focuses on accurately aligning the distributions of the same class data from different domains. However, most of them ignore two points: the learning of the global distribution of the target domain data; the compactness of intra-class domain data and the discrimination of inter-class domain data, which lead to unsatisfying transfer learning performance. To resolve this problem, we propose a Center-aligned Domain Adaptation Network (CenterDA) to facilitate the semantic alignment, In this study, for each class in label space, we learn a common class center for all data with the same class label in the source and target domains, which allows us to learn the global distribution of the target domain data under the supervised learning of the source domain data. Furthermore, we minimize the distance between the deep features and its common class center to compact the feature representations of data. In this manner, we achieve the desired goals: The global distribution of the target domain data is learned by common class center. Second, the source and the target domain data of the same class are aligned near the common center. Third, we model the intra-class compactness and the inter-class separability modeling. Extensive experiments on three datasets show that our method achieves remarkable results on image classification and has comparable performance with the latest methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310563",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Guanqun"
      },
      {
        "surname": "Wei",
        "given_name": "Zhiqiang"
      },
      {
        "surname": "Huang",
        "given_name": "Lei"
      },
      {
        "surname": "Nie",
        "given_name": "Jie"
      },
      {
        "surname": "Li",
        "given_name": "Xiaojing"
      }
    ]
  },
  {
    "title": "Modified DFS-based term weighting scheme for text classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114438",
    "abstract": "With the rapid growth of textual data on the Internet, text classification (TC) has attracted increasing attention. As a widely used text representation method, the vector space model (VSM) represents the content of a document as a vector composed of term frequency (TF) in the term space. Because different terms have different levels of importance in a document, designing an appropriate term weighting scheme is crucial to improve the performance of TC. In this study, we first conducted a comprehensive survey of the existing well-known term weighting schemes and found that they are not fully effective and that researchers are still focused on proposing new term weighting schemes. To further improve the performance of TC, we propose a new term weighting scheme based on the modified distinguishing feature selector (DFS), which we call TF–MDFS (modified DFS-based TF). Experimental results show that TF–MDFS is overall better than existing state-of-the-art term weighting schemes in terms of the classification accuracy of widely used base classifiers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310988",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Data mining",
      "Distributed File System",
      "Feature (linguistics)",
      "Feature vector",
      "Law",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Radiology",
      "Representation (politics)",
      "Scheme (mathematics)",
      "Term (time)",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Long"
      },
      {
        "surname": "Jiang",
        "given_name": "Liangxiao"
      },
      {
        "surname": "Li",
        "given_name": "Chaoqun"
      }
    ]
  },
  {
    "title": "A neural Entity Coreference Resolution review",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114466",
    "abstract": "Entity Coreference Resolution is the task of resolving all mentions in a document that refer to the same real world entity and is considered as one of the most difficult tasks in natural language understanding. It is of great importance for downstream natural language processing tasks such as entity linking, machine translation, summarization, chatbots, etc. This work aims to give a detailed review of current progress on solving Coreference Resolution using neural-based approaches. It also provides a detailed appraisal of the datasets and evaluation metrics in the field, as well as the subtask of Pronoun Resolution that has seen various improvements in the recent years. We highlight the advantages and disadvantages of the approaches, the challenges of the task, the lack of agreed-upon standards in the task and propose a way to further expand the boundaries of the field.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311143",
    "keywords": [
      "Artificial intelligence",
      "Automatic summarization",
      "Computer science",
      "Coreference",
      "Economics",
      "Field (mathematics)",
      "Machine translation",
      "Management",
      "Mathematics",
      "Named-entity recognition",
      "Natural language",
      "Natural language processing",
      "Natural language understanding",
      "Pure mathematics",
      "Resolution (logic)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Stylianou",
        "given_name": "Nikolaos"
      },
      {
        "surname": "Vlahavas",
        "given_name": "Ioannis"
      }
    ]
  },
  {
    "title": "Multi-mode resource-constrained project scheduling with uncertain activity cost",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114475",
    "abstract": "The multi-mode resource-constrained project scheduling problem under uncertain activity cost (MRCPSP-UAC) has a wide range of applications in production planning and project management. We first build a new mixed-integer nonlinear programming (MINLP) model with the objective of minimizing the risk of project cost overrun, which provides a vehicle to obtain optimal solutions. To overcome the computational challenge of exact method for solving large instances, we devise a construction heuristic (CH) with a multi-pass greedy improvement procedure to obtain a feasible solution efficiently. To further improve solution quality, a hybrid CH and genetic algorithm (CH-GA) is developed with a custom fitness function to properly calibrate the quality of an individual. A comprehensive computational study is performed to examine the impact of various problem parameters on the optimal solutions, and the performance of our algorithms. Our hybrid CH-GA performs well for large instances with significantly less computational time than the exact method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311246",
    "keywords": [
      "Algorithm",
      "Composite material",
      "Computer science",
      "Fitness function",
      "Genetic algorithm",
      "Greedy algorithm",
      "Heuristic",
      "Integer programming",
      "Job shop scheduling",
      "Materials science",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Range (aeronautics)",
      "Schedule",
      "Scheduling (production processes)"
    ],
    "authors": [
      {
        "surname": "Xie",
        "given_name": "Fang"
      },
      {
        "surname": "Li",
        "given_name": "Haitao"
      },
      {
        "surname": "Xu",
        "given_name": "Zhe"
      }
    ]
  },
  {
    "title": "Fear not, vote truthfully: Secure Multiparty Computation of score based rules",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114434",
    "abstract": "We propose a secure voting protocol for score-based voting rules, where independent talliers perform the tallying procedure. The protocol outputs the winning candidate(s) while preserving the privacy of the voters and the secrecy of the ballots. It offers perfect secrecy, in the sense that apart from the desired output, all other information – the ballots, intermediate values, and the final scores received by each of the candidates – is not disclosed to any party, including the talliers. Such perfect secrecy may increase the voters’ confidence and, consequently, encourage them to vote according to their true preferences. The protocol is extremely lightweight, and therefore it can be easily deployed in real life voting scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310964",
    "keywords": [
      "Algorithm",
      "Alternative medicine",
      "Artificial intelligence",
      "Computation",
      "Computer science",
      "Computer security",
      "Law",
      "Majority rule",
      "Medicine",
      "Pathology",
      "Political science",
      "Politics",
      "Protocol (science)",
      "Secrecy",
      "Theoretical computer science",
      "Voting"
    ],
    "authors": [
      {
        "surname": "Dery",
        "given_name": "Lihi"
      },
      {
        "surname": "Tassa",
        "given_name": "Tamir"
      },
      {
        "surname": "Yanai",
        "given_name": "Avishay"
      }
    ]
  },
  {
    "title": "I/O efficient structural clustering and maintenance of clusters for large-scale graphs",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114221",
    "abstract": "In recent years, the size of graph data has increased significantly, but most existing graph clustering algorithms do not consider the case where the size of main memory is not sufficient to handle large amount of graph data. Exploring entire region of graph for clustering causes too many random disk accesses to use data that are not loaded into memory, resulting in excessive disk I/O and thrashing. To address this problem, we propose an I/O-efficient algorithm for structural clustering of a graph, called pm-SCAN. In the proposed method, if memory is insufficient, an input graph is partitioned into several subgraphs smaller than memory, and clustering is first performed for each subgraph. And then clusters from the subgraphs are merged based on connectivity between clusters so that global results can be obtained in the point of view of an original input graph. Not only does pm-SCAN produce scalable performance even for very large graphs, i.e., significant shortage of available memory, but also the result of pm-SCAN is the same as that of the original structural clustering algorithm SCAN. We also propose a cluster maintenance method for large-scale dynamic graphs that change over time. Instead of reclustering with a whole graph, only a small set of nodes whose structural connectivities are subject to change by a given update operation is first identified, and we access only those nodes in disk and update their clusters to reduce maintenance costs. This dynamic graph handling mechanism shows significant performance improvement compared to the existing method and the baseline that performs clustering from scratch.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309453",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Database",
      "Graph",
      "Parallel computing",
      "Scalability",
      "Theoretical computer science",
      "Thrashing"
    ],
    "authors": [
      {
        "surname": "Seo",
        "given_name": "Jung Hyuk"
      },
      {
        "surname": "Kim",
        "given_name": "Myoung Ho"
      }
    ]
  },
  {
    "title": "Learning meta-knowledge for few-shot image emotion recognition",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114274",
    "abstract": "Previous studies have demonstrated that images are of great importance in attracting people’s attention and motivating them to take action. Various attributes (e.g., colors, aesthetics, and embedded objects) related to images are considered driving factors. Among which emotions in images, in particular, play a critical role in stimulating individuals, based on the Stimulus–Organism–Response theory. Consequently, many researchers put great efforts to understand image emotions, ranging from developing theoretical models to a broad spectrum of applications. Due to the complex and unstructured characteristics of images, identifying image emotions is challenging. Although some significant progress in image emotion classification has been achieved, inherent constraints still remain unaddressed. For example, acquiring a sufficiently large amount of labeled data to train a good model is costly and inevitably requires lots of human efforts. Besides, building a generalized model applicable to different datasets still needs a deep exploration. Image emotions are very subjective, which also makes such a classification task difficult. This paper proposes a general meta-learning framework for the few-shot image emotion classification, called Meta-IEC. Meta-IEC provides the capability of: (i) adapting to a similar dataset but new classes that have not been encountered before, and (ii) generalizing to a completely different dataset where emotion classes are unseen in the training dataset and only very few labeled images are available. Meta-IEC is also able to capture the uncertainty and ambiguity during the meta-testing, where we implement a hierarchical Bayesian graphical model to understand latent relationships among various parameters between meta-training and meta-testing. Extensive experiments on three commonly used datasets empirically demonstrate the superiority of our method over several state-of-the-art baselines. For example, our meta-learning based model can achieve performance improvement up to 5+%. We also provide some managerial implications on parameter sensitivity and label selection of meta-training and meta-testing.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309830",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Emotion recognition",
      "Image (mathematics)",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Shot (pellet)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Fan"
      },
      {
        "surname": "Cao",
        "given_name": "Chengtai"
      },
      {
        "surname": "Zhong",
        "given_name": "Ting"
      },
      {
        "surname": "Geng",
        "given_name": "Ji"
      }
    ]
  },
  {
    "title": "A taxonomy of blockchain consensus protocols: A survey and classification framework",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114384",
    "abstract": "Blockchain, the underlying technology of Bitcoin, refers to the public ledger used in a distributed network. Because blockchain does not rely on a central authority, peers have to agree on the state of the ledger among themselves, i.e., they have to reach a consensus on the state of the transactions. The way nodes reach that consensus has gained incredible attention in the literature. Bitcoin uses the Proof-of-Work (PoW) mechanism, as did Ethereum at first. The latter decided to move from PoW to Proof-of-Stake (PoS) because of the high energy consumption required by PoW. To date, many other consensus protocols have been proposed to address the limitations of the seminal ones. In this paper, we inform researchers and practitioners about the current state of consensus protocols research. The aim is to provide an analysis of the research introducing new consensus protocols in order to enable a more unified treatment. To that end, we review 28 new consensus protocols and we propose a four-category classification framework: Origin, Design, Performance and Security. We demonstrate the applicability of the framework by classifying the 28 protocols. Many surveys have already been proposed in the literature and some of them will be discussed later in the paper. Yet, we believe that this work is relevant and important for two reasons. Firstly, blockchain being a fast evolving topic, new consensus protocols emerge regularly and improvements are also put forward on a regular basis. Hence, this work aims at reflecting the latest state-of-the-art in terms of consensus protocols. Secondly, we aim to propose a comprehensive classification framework, integrating knowledge from multiple works in the literature, as well as introducing classification dimensions that have not been proposed before. This work demonstrates that multiple consensus have been proposed in a short period of time, and highlights the differences between these protocols. Furthermore, it is suggested that researchers and practitioners who aim to propose consensus protocols in the future should pay attention to all the dimensions presented in the classification framework.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310587",
    "keywords": [
      "Algorithm",
      "Alternative medicine",
      "Artificial intelligence",
      "Biology",
      "Blockchain",
      "Botany",
      "Computer science",
      "Computer security",
      "Consensus",
      "Consensus algorithm",
      "Data science",
      "Distributed ledger",
      "Medicine",
      "Multi-agent system",
      "Pathology",
      "Proof-of-work system",
      "Protocol (science)",
      "State (computer science)",
      "Taxonomy (biology)"
    ],
    "authors": [
      {
        "surname": "Bouraga",
        "given_name": "Sarah"
      }
    ]
  },
  {
    "title": "Q-Managed: A new algorithm for a multiobjective reinforcement learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114228",
    "abstract": "Multi-objective reinforcement learning (MORL) involves the use of reinforcement learning techniques to address problems with multiple objectives, conflicting or not. Among the main techniques used to treat this class of problems, we saw that they are limited by some factors, such as the Pareto Front shape and computational cost. This paper proposes a new iterative algorithm based on the single-policy approach, called Q-Managed. We use a hybrid multi-objective optimization (MOO) method that provides the mathematical guarantee that all policies belonging to the Pareto Front can be found, regardless of whether it is concave, convex or a mixture of both. Another important aspect that is worth mentioning is that its simplicity and performance are from a single-policy algorithms. To validate our proposal, we use the traditional MORL benchmarks and with different configurations of the Pareto Front. The Q-Managed shows success in finding all the optimal policies in all environments, surpassing all the single-policy algorithms in the literature in terms of policy quality. Based on the used benchmarks, its effectiveness can also be equated to the best multi-policy algorithms. The hypervolume metric was used to compare the quality of the policies found by our algorithm with those found in the state of the art. Extensions for non-episodic environments and stochastic transition functions are also introduced.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309490",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Economics",
      "Epistemology",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Metric (unit)",
      "Multi-objective optimization",
      "Operations management",
      "Pareto principle",
      "Philosophy",
      "Quality (philosophy)",
      "Reinforcement learning",
      "Simplicity",
      "State (computer science)"
    ],
    "authors": [
      {
        "surname": "Oliveira",
        "given_name": "Thiago Henrique Freire de"
      },
      {
        "surname": "Medeiros",
        "given_name": "Luiz Paulo de Souza"
      },
      {
        "surname": "Neto",
        "given_name": "Adrião Duarte Dória"
      },
      {
        "surname": "Melo",
        "given_name": "Jorge Dantas"
      }
    ]
  },
  {
    "title": "Existential active integrity constraints",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114297",
    "abstract": "Active integrity constraints (AICs) are a useful formalism to express integrity constraints and policies to restore consistency in databases violating them. However, AICs do not allow users to express different kinds of constraints commonly arising in practice, such as foreign keys. In this paper, we propose existential active integrity constraints (EAICs), a powerful extension of AICs that allows us to express a wide range of constraints used in databases and ontological systems. We investigate different properties of EAICs. Specifically, we show that there exists a “representative” set of founded updates, called universal, which suffices for query answering. As such a set might contain an infinite number of founded updates, each of infinite size, we study syntactic restrictions ensuring finiteness, as well as the existence of a single universal founded update.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309982",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Consistency (knowledge bases)",
      "Data integrity",
      "Database",
      "Discrete mathematics",
      "Existential quantification",
      "Extension (predicate logic)",
      "Formalism (music)",
      "Mathematics",
      "Musical",
      "Programming language",
      "Set (abstract data type)",
      "Theoretical computer science",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Calautti",
        "given_name": "Marco"
      },
      {
        "surname": "Caroprese",
        "given_name": "Luciano"
      },
      {
        "surname": "Greco",
        "given_name": "Sergio"
      },
      {
        "surname": "Molinaro",
        "given_name": "Cristian"
      },
      {
        "surname": "Trubitsyna",
        "given_name": "Irina"
      },
      {
        "surname": "Zumpano",
        "given_name": "Ester"
      }
    ]
  },
  {
    "title": "A novel process to determine consensus thresholds and its application in probabilistic linguistic group decision-making",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114315",
    "abstract": "When solving group decision-making (GDM) problems, the proper values of consensus thresholds are vital for consensus checking and improving. Decision makers are usually not confident to select a proper threshold. In the linguistic setting, this paper presents a new process to support the determination of consensus thresholds. A new concept, namely random consensus index, is presented to serve as a reference object so that the risk preference of a decision maker could be induced by comparing the quality of the information in hands with the averaging quality of randomly generated information. In this sense, admissible consensus thresholds could be determined automatically, and decision makers are not necessary to focus on the detail of consensus measures. Furthermore, the process is applied to the GDM problems with probabilistic linguistic preference relations, and a new GDM approach is presented. Different from the existing contributions, the consensus is measured by the probability of a set of preference relations being with acceptable consensus, and then the consensus is improved by revising the involved probability distributions. All the involved parameters are intuitive and interpretable. A case study, regarding the departure audit in China, demonstrates that the proposed GDM approach is effective even if the original preferences of experts are with low consistency and consensus.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310101",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Consistency (knowledge bases)",
      "Data mining",
      "Epistemology",
      "Group decision-making",
      "Machine learning",
      "Mathematics",
      "Natural language processing",
      "Operating system",
      "Philosophy",
      "Preference",
      "Probabilistic logic",
      "Process (computing)",
      "Programming language",
      "Psychology",
      "Quality (philosophy)",
      "Set (abstract data type)",
      "Social psychology",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Hai"
      },
      {
        "surname": "Yu",
        "given_name": "Dejian"
      },
      {
        "surname": "Xu",
        "given_name": "Zeshui"
      }
    ]
  },
  {
    "title": "vertTIRP: Robust and efficient vertical frequent time interval-related pattern mining",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114276",
    "abstract": "Time-interval-related pattern (TIRP) mining algorithms find patterns such as “A starts B” or “A overlaps B”. The discovery of TIRPs is computationally highly demanding. In this work, we introduce a new efficient algorithm for mining TIRPs, called vertTIRP which combines an efficient representation of these patterns, using their temporal transitivity properties to manage them, with a pairing strategy that sorts the temporal relations to be tested, in order to speed up the mining process. Moreover, this work presents a robust definition of the temporal relations that eliminates the ambiguities with other relations when taking into account the uncertainty in the start and end time of the events (epsilon-based approach), and includes two constraints that enable the user to better express the types of TIRPs to be learnt. An experimental evaluation of the method was performed with both synthetic and real datasets, and the results show that vertTIRP requires significantly less computation time than other state-of-the-art algorithms, and is an effective approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309842",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Business",
      "Business process",
      "Business process modeling",
      "Combinatorics",
      "Computation",
      "Computer science",
      "Data mining",
      "Interval (graph theory)",
      "Law",
      "Machine learning",
      "Marketing",
      "Mathematics",
      "Operating system",
      "Pairing",
      "Physics",
      "Political science",
      "Politics",
      "Process (computing)",
      "Process mining",
      "Quantum mechanics",
      "Representation (politics)",
      "Superconductivity",
      "Transitive relation",
      "Work in process"
    ],
    "authors": [
      {
        "surname": "Mordvanyuk",
        "given_name": "Natalia"
      },
      {
        "surname": "López",
        "given_name": "Beatriz"
      },
      {
        "surname": "Bifet",
        "given_name": "Albert"
      }
    ]
  },
  {
    "title": "3D multi-view tumor detection in automated whole breast ultrasound using deep convolutional neural network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114410",
    "abstract": "In recent years, automated whole breast ultrasound (ABUS) has drawn attention to breast disease detection and diagnosis applications. However, reviewing ABUS volumes is a time-costing task and some subtle tumors may be missed. In this paper, a 3D multi-view tumor detection method is proposed for ABUS volumes. Firstly, a layer connected feature extraction network is designed for Faster R-CNN. Then, orthogonal multi-view slices are reconstructed and detected using this modified Faster R-CNN to extract 2D candidates. Finally, a 3D multi-view position analysis scheme is designed to fuse 2D detection results and get final 3D bounding boxes. The performance of this proposed method is evaluated on a data set of 158 volumes from 75 patients by 5-fold cross-validation. Experimental results show that our method achieves a sensitivity of 95.06% with 0.57 false positives (FPs) per volume. Compared with existing detection methods, the proposed method is more effective and general.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310782",
    "keywords": [
      "Artificial intelligence",
      "Bounding overwatch",
      "Breast cancer",
      "Breast ultrasound",
      "Cancer",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Electrical engineering",
      "Engineering",
      "False positive paradox",
      "Feature (linguistics)",
      "Feature extraction",
      "Fuse (electrical)",
      "Internal medicine",
      "Linguistics",
      "Mammography",
      "Medicine",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Yue"
      },
      {
        "surname": "Chen",
        "given_name": "Houjin"
      },
      {
        "surname": "Li",
        "given_name": "Yanfeng"
      },
      {
        "surname": "Wang",
        "given_name": "Shu"
      },
      {
        "surname": "Cheng",
        "given_name": "Lin"
      },
      {
        "surname": "Li",
        "given_name": "Jupeng"
      }
    ]
  },
  {
    "title": "Monitoring linear profiles using Artificial Neural Networks with run rules",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114237",
    "abstract": "In some applications, a relation between a response variable and one or more explanatory variables (referred as a “profile”) characterizes the quality of a process. Profile monitoring is commonly performed through statistical methods, while machine learning schemes have not received much attention in this regard. In this paper, a control chart based on Artificial Neural Networks (ANN) is proposed to monitor linear profiles in phase II. In the proposed control chart, some novel run rules as the major contribution of this paper are also used to enhance the efficiency of the control chart and for faster detection of shifts. Simulation results revealed a good performance of the proposed control chart based on average run length (ARL) criterion. Further, a systematic ANN-based diagnostic procedure was proposed to identify which parameter has changed in the process. Finally, the implementation of the proposed scheme was illustrated through a real calibration example from the field of chemical engineering.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309568",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Data mining",
      "Machine learning",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Yeganeh",
        "given_name": "Ali"
      },
      {
        "surname": "Shadman",
        "given_name": "Alireza"
      }
    ]
  },
  {
    "title": "Forecasting daily stock trend using multi-filter feature selection and deep learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114444",
    "abstract": "Stock market forecasting has attracted significant attention mainly due to the potential monetary benefits. Predicting these markets is a challenging task due to numerous interrelated factors, and needs a complete and efficient feature selection process to identify the most informative factors. As a time series problem, stock price movements are also dependent on movements on its previous trading days. Feature selection techniques have been widely applied in stock forecasting, but existing approaches usually use a single feature selection technique, which may overlook some important assumptions about the underlying regression function linking the input and output variables. In this study, we combine features selected by multiple feature selection techniques to generate an optimal feature subset and then use a deep generative model to predict future price movements. First, we compute an extended set of forty-four technical indicators from daily stock data of eighty-eight stocks and then compute their importance by independently training logistic regression model, support vector machine and random forests. Based on a prespecified threshold, the lowest ranked features are dropped and the rest are grouped into clusters. The variable importance measure is reused to select the most important feature from each cluster to generate the final subset. The input is then fed to a deep generative model comprising of a market signal extractor and an attention mechanism. The market signal extractor recurrently decodes market movement from the latent variables to deal with stochastic nature of the stock data and the attention mechanism discriminates between predictive dependencies of different temporal auxiliary outputs. The results demonstrate that combining features selected by multiple feature selection approaches and using them as input into a deep generative model outperforms state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742031099X",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Data mining",
      "Econometrics",
      "Economics",
      "Engineering",
      "Feature selection",
      "Generative grammar",
      "Generative model",
      "Horse",
      "Machine learning",
      "Mechanical engineering",
      "Paleontology",
      "Stock (firearms)",
      "Stock market"
    ],
    "authors": [
      {
        "surname": "Haq",
        "given_name": "Anwar Ul"
      },
      {
        "surname": "Zeb",
        "given_name": "Adnan"
      },
      {
        "surname": "Lei",
        "given_name": "Zhenfeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Defu"
      }
    ]
  },
  {
    "title": "Selection of product recycling channels based on extended TODIM method",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114295",
    "abstract": "The evaluation and selection of product recovery channels is a key decision issue in closed-loop supply chain management. Although existing research does not take into account the presence of inaccurate information and the influence of the decision maker’s psychological state during the selection of recycling channels, we study the issue of recycling channel selection under the consideration of inaccurate information and the influence of the decision maker’s psychological state. (1) First, we establish a new evaluation index system for recycling channels for policy makers. (2) We propose a TODIM (an acronym in Portuguese of Interactive and Multi-criteria Decision Making) method based on fuzzy evaluation and Shapley index for decision makers to imitate the psychological behavior characteristics of FMEA (Failure mode and effect analysis) team members. It can also be used to estimate the risk priority of several failure modes. The interval type 2 fuzzy is used to describe the uncertainty in the risk assessment process. (3) Finally, we obtain the practical significance of the method proposed in this paper through case studies, sensitivity analysis and comparative analysis of traditional methods. We also find the key impact factors in the choice of recycling channels.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309970",
    "keywords": [
      "Acronym",
      "Artificial intelligence",
      "Business",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Computer security",
      "Decision maker",
      "Fuzzy logic",
      "Geometry",
      "Key (lock)",
      "Linguistics",
      "Marketing",
      "Mathematics",
      "Operating system",
      "Operations research",
      "Philosophy",
      "Process (computing)",
      "Product (mathematics)",
      "Risk analysis (engineering)",
      "Selection (genetic algorithm)",
      "Supply chain"
    ],
    "authors": [
      {
        "surname": "Hong",
        "given_name": "Xianpei"
      },
      {
        "surname": "Bai",
        "given_name": "Xuyang"
      },
      {
        "surname": "Song",
        "given_name": "Yan"
      }
    ]
  },
  {
    "title": "Parameters optimization of hybrid strategy recommendation based on particle swarm algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114388",
    "abstract": "With the unprecedented development in the internet technology, the information overload issues have become more and more complex, resulting in users being unable to obtain the target information accurately and effectively in selecting the required information from a large pool of surfed data. In view of this a recommendation system can be used to predict the user's selection probability for different potential objects as an important tool, which can help to solve the information overload issues. So far, many personalized recommendation algorithms based on bipartite graphS have been proposed, most of which are based on the similarity degree among users or items, such as collaborative filtering (CF), mass diffusion (MD) and heat conduction (HC). Among many recommendation algorithms, the performances of algorithms are varied. MD algorithm has high recommendation accuracy but poor diversity, while HC algorithm has good diversity but low accuracy. In order to solve the dilemma in accuracy and diversity, some hybrid recommendation algorithm have been proposed. This paper has mainly focused on the hybrid recommendation algorithm HHM, and pointed out its shortcomings. Based on the reconsideration of the effect of item popularity in the recommendation process, an improved hybrid recommendation algorithm using dual parameter called IHM was proposed. The particle swarm optimization (PSO) algorithm was applied to the parameter optimal of the hybrid recommendation algorithm to obtain the parameters of the algorithm. Experiments on 3 real datasets indicated that the IHM algorithm is better than HHM algorithms in terms of the recommendation accuracy, diversity and novelty. Meanwhile, the IHM algorithm can also improve the recommendation for items with lower popularity and solve the cold start problem.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310617",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bipartite graph",
      "Collaborative filtering",
      "Computer science",
      "Constraint logic programming",
      "Constraint satisfaction",
      "Data mining",
      "Graph",
      "Hybrid algorithm (constraint satisfaction)",
      "Image (mathematics)",
      "Information overload",
      "Machine learning",
      "Operating system",
      "Particle swarm optimization",
      "Probabilistic logic",
      "Process (computing)",
      "Recommender system",
      "Similarity (geometry)",
      "Theoretical computer science",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Cai",
        "given_name": "Biao"
      },
      {
        "surname": "Zhu",
        "given_name": "Xinping"
      },
      {
        "surname": "Qin",
        "given_name": "Yangxin"
      }
    ]
  },
  {
    "title": "A reduced VNS based approach for the dynamic continuous berth allocation problem in bulk terminals with tidal constraints",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114215",
    "abstract": "This paper deals with the problem of allocating berth positions for vessels in export bulk port terminals considering tidal constraints and was first formulated by Ernst et al., (2017). This study investigates the dynamic and continuous berth allocation problem (BAP) with respect to tidal constraints (BAP_TC), and seeks to minimize the total service time of berthed vessels. Since the BAP problem is NP-hard the BAP_TC is also NP-hard. A reduced variable neighborhood search (RVNS) based approach is developed to solve the problem. For parameters tuning a machine learning algorithm is developed and used. Problem instances are benchmarked with CPLEX and the numerical experiments proved that the proposed algorithm is capable of generating high-quality solutions in rather short time. Both small and large-scale instances in the literature are tested to evaluate the metaheuristic effectiveness using other solution approaches from the literature. The computational experiment proves that the proposed algorithm provides state of the art results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309416",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Electrical engineering",
      "Engineering",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Port (circuit theory)",
      "Variable (mathematics)",
      "Variable neighborhood search"
    ],
    "authors": [
      {
        "surname": "Cheimanoff",
        "given_name": "Nicolas"
      },
      {
        "surname": "Fontane",
        "given_name": "Frédéric"
      },
      {
        "surname": "Kitri",
        "given_name": "Mohamed Nour"
      },
      {
        "surname": "Tchernev",
        "given_name": "Nikolay"
      }
    ]
  },
  {
    "title": "Classifying and resolving software product line redundancies using an ontological first-order logic rule based method",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114167",
    "abstract": "Software product line engineering improves software quality and diminishes development cost and time by efficiently developing software products. Its success lies in identifying the commonalities and variabilities of a set of software products which are generally modeled using feature models. The success of software product lines heavily relies upon the quality of feature models to derive high quality products. However, there are various defects that reduce profits of software product line. One of such defect is redundancy. While the majority of research work focuses on the identification of redundancies, their causes and corrections have been poorly explored. Causes and corrections must be as accurate and comprehensible as possible in order to support the developer in resolving the cause of a redundancy. This research work classified redundancies in the form of a typology. An ontological first-order logic rule based method is proposed to deal with redundancies. A two-step process is presented for mapping model to ontology based on predicate logic. First-order logic based rules are developed and applied to the generated ontology for identifying redundancies, their causes and corrections to resolve redundancies. The proposed method is illustrated using a case study from software product lines online tools repository. The results of experiments performed on 35 models with varied sizes of real world models as well as automatically-generated models from the Software Product Line Online Tools repository and models created via FeatureIDE tool conclude that the method is accurate, efficient and scalable with FM up to 30,000 features. Thus, enables deriving redundancy free end products from the product line and ultimately, improves its quality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309052",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Feature model",
      "Operating system",
      "Programming language",
      "Redundancy (engineering)",
      "Software",
      "Software development",
      "Software engineering",
      "Software product line",
      "Software quality"
    ],
    "authors": [
      {
        "surname": "Bhushan",
        "given_name": "Megha"
      },
      {
        "surname": "Ángel Galindo Duarte",
        "given_name": "José"
      },
      {
        "surname": "Samant",
        "given_name": "Piyush"
      },
      {
        "surname": "Kumar",
        "given_name": "Ashok"
      },
      {
        "surname": "Negi",
        "given_name": "Arun"
      }
    ]
  },
  {
    "title": "Brain tumor segmentation in MR images using a sparse constrained level set algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114262",
    "abstract": "Brain tumor segmentation using Magnetic Resonance (MR) Imaging technology plays a significant role in computer-aided brain tumor diagnosis. However, when applying classic segmentation methods, limitations such as inhomogeneous intensity, complex physiological structure and blurred tissues boundaries in brain MR images usually lead to unsatisfactory results. To address these issues, this paper proposes an automatic sparse constrained level set method to realize the brain tumor segmentation in MR images. By studying brain tumor images, this method finds out common characteristics of brain tumors’ shape and constructs a sparse representation model. By considering this model as a prior constraint, an energy function based on level set method is constructed. In experiments, the proposed method can achieve an average accuracy of 96.20% for the MR images from the dataset Brats2017 and performs better than the others. With lower false positive rate and stronger robustness, the experimental results show that the proposed method can segment brain tumor from MR image accurately and stably.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309751",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image segmentation",
      "Level set (data structures)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Segmentation",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Lei",
        "given_name": "Xiaoliang"
      },
      {
        "surname": "Yu",
        "given_name": "Xiaosheng"
      },
      {
        "surname": "Chi",
        "given_name": "Jianning"
      },
      {
        "surname": "Wang",
        "given_name": "Ying"
      },
      {
        "surname": "Zhang",
        "given_name": "Jingsi"
      },
      {
        "surname": "Wu",
        "given_name": "Chengdong"
      }
    ]
  },
  {
    "title": "A reinforcement learning and deep learning based intelligent system for the support of impaired patients in home treatment",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114285",
    "abstract": "A clinical treatment process typically carries out in two stages; i.e., hospital stay and treatment at home after hospitalization. The correct completion of the treatment process is essential, but it becomes challenging for elders and patients with any physical or cognitive disability since they need assistance in the execution of the treatment itself. This work presents an intelligent system able to provide automatic assistance to those patients that have to follow a planned treatment at home. The system can support the patient with both customized reminders whenever it is the time to take medication and alerts to avoid possible medication errors when the patient is going to assume an incorrect drug by mistake. The core of the proposed solution consists of a multi-agent system that relies on algorithms of both Reinforcement Learning and Deep Learning. Experimental results show that the system improves the quality of home assistance services reducing medication errors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030988X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Core (optical fiber)",
      "Law",
      "Mistake",
      "Operating system",
      "Political science",
      "Process (computing)",
      "Reinforcement learning",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Naeem",
        "given_name": "Muddasar"
      },
      {
        "surname": "Paragliola",
        "given_name": "Giovanni"
      },
      {
        "surname": "Coronato",
        "given_name": "Antonio"
      }
    ]
  },
  {
    "title": "Graph convolutional network-based credit default prediction utilizing three types of virtual distances among borrowers",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114411",
    "abstract": "Machine learning models have been actively utilized to quantitatively predict the default probability based on the personal information obtained from loan applicants. Although the relationship between loan applicants is receiving attention as important soft information, only the simple individual network features and their relation have been reflected in the prediction model. In this study, we propose a graph convolutional network (GCN)-based credit default prediction model, which can reflect nonlinear relationships between borrower’s attributes and default risk as well as high-order relationships between the borrowers. Three types of information pertaining to the borrowers are separately employed for their relations, namely loan information, credit history information, and soft information. We compare our GCN model to baseline models using the data from an online peer-to-peer lending platform. The results show that our approach outperforms existing classification models and identifies the relative contribution of input attributes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310794",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Graph",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Jong Wook"
      },
      {
        "surname": "Lee",
        "given_name": "Won Kyung"
      },
      {
        "surname": "Sohn",
        "given_name": "So Young"
      }
    ]
  },
  {
    "title": "An overview and a benchmark of active learning for outlier detection with one-class classifiers",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114372",
    "abstract": "Active learning methods increase classification quality by means of user feedback. An important subcategory is active learning for outlier detection with one-class classifiers. While various methods in this category exist, selecting one for a given application scenario is difficult. This is because existing methods rely on different assumptions, have different objectives, and often are tailored to a specific use case. All this calls for a comprehensive comparison, the topic of this article. This article starts with a categorization of the various methods. Interestingly, many assumptions in the literature are implicit, and their impact has not been discussed so far. Based on this, we propose a novel approach to evaluate active learning results by quantifying how classification results evolve with more user feedback, in a compact and nuanced manner. We run over 84,000 experiments to compare state-of-the-art one-class active learning methods, for a broad variety of scenarios. One key finding is that there is no single active learning method that stands out in a competitive evaluation. Instead, we found that selecting a good query strategy alone is not sufficient, since results hinge significantly on other factors, such as the selection of hyperparameter values. Our results show that some configurations are more robust than others. We conclude by phrasing our findings as guidelines on how to select active learning methods for outlier detection with one-class classifiers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310496",
    "keywords": [],
    "authors": [
      {
        "surname": "Trittenbach",
        "given_name": "Holger"
      },
      {
        "surname": "Englhardt",
        "given_name": "Adrian"
      },
      {
        "surname": "Böhm",
        "given_name": "Klemens"
      }
    ]
  },
  {
    "title": "Application of convolutional neural network to traditional data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114185",
    "abstract": "Convolutional neural networks (ConvNets) have been applied to various types of data, including image, text, and speech, but not to traditional data. In this study, traditional data are defined as data whose features have no spatial or temporal dependencies but might have statistical correlations. We construct a feature grid-based ConvNet (FGCN) model for classification tasks on traditional data. The FGCN model is composed of two functional parts: The first is used to convert traditional data in the form of a 1-D feature vector into a 1-D, 2-D, or higher-dimensional feature grid; and the second is a ConvNet classifier for the converted data. The experimental results show that the FGCN model performs well; therefore, it is worth considering this model for classification tasks on traditional data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309192",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Construct (python library)",
      "Convolutional neural network",
      "Data mining",
      "Feature (linguistics)",
      "Feature vector",
      "Geometry",
      "Grid",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Xiaohang"
      },
      {
        "surname": "Wu",
        "given_name": "Fengmin"
      },
      {
        "surname": "Li",
        "given_name": "Zhengren"
      }
    ]
  },
  {
    "title": "Unified deep neural networks for end-to-end recognition of multi-oriented billet identification number",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114377",
    "abstract": "In this study, a novel framework for the recognition of a billet identification number (BIN) using deep learning is proposed. Because a billet, which is a semi-finished product, could be rolled, the BIN may be rotated at various angles. Most product numbers, including BIN, are a combination of individual characters. Such product numbers are determined based on the class of each character and its order (or the positioning). In addition, the two pieces of information are constant even if the product number is rotated. Inspired by this concept, the proposed framework of deep neural networks has two outputs. One is for the class of an individual character, and the other is the order of an individual character within BIN. Compared with a previous study, the proposed network requires an additional annotation but does not require additional labor for labeling. The multi-task learning for two annotations has a positive role in the representation learning of a network, which is shown in the experiment results. Furthermore, to achieve a good performance of the BIN identification, we analyzed various networks using the proposed framework. The proposed algorithm was then compared with a conventional algorithm to evaluate the performance of the BIN identification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310538",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Bin",
      "Biology",
      "Botany",
      "Character (mathematics)",
      "Class (philosophy)",
      "Computer science",
      "Deep learning",
      "Engineering",
      "Geometry",
      "Identification (biology)",
      "Law",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Product (mathematics)",
      "Representation (politics)",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Koo",
        "given_name": "Gyogwon"
      },
      {
        "surname": "Yun",
        "given_name": "Jong Pil"
      },
      {
        "surname": "Choi",
        "given_name": "Hyeyeon"
      },
      {
        "surname": "Kim",
        "given_name": "Sang Woo"
      }
    ]
  },
  {
    "title": "A simulation-optimization approach for adaptive manufacturing capacity planning in small and medium-sized enterprises",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114451",
    "abstract": "Manufacturing capacity planning is one of the critical processes in every manufacturing company, and, with increasing exploitation of data and information technology, has necessarily become more efficient than before. However, the power to harness data and information for planning requires specific knowledge and resources, mostly limited to large enterprises. Small and medium-sized enterprises (SMEs) generally do not have sufficient resources to collect large amounts of data or the know-how to process and exploit data. Moreover, SMEs often fail to implement advanced techniques and tools (e.g., optimization tools or enterprise resource planning (ERP) software), owing to the cost and a lack of specific knowledge and personnel. This paper proposes a solution for reducing the burden on SMEs in collecting and utilizing data for the planning of manufacturing capacity. A simulation-optimization approach is adopted because of the complex nature of labor-intensive manufacturing in SMEs. The approach includes an artificial neural network for model simulation and data relationship recognition, combined with a genetic algorithm for optimizing manufacturing resource configuration. The proposed method can facilitate the process of planning manufacturing capacity for different yield targets, as tested in a case study of a pastry company and providing the means for the company to exploit both empirical and observational data for the purpose.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311052",
    "keywords": [
      "Business",
      "Capacity planning",
      "Computer network",
      "Computer science",
      "Computer security",
      "Computer-integrated manufacturing",
      "Discrete manufacturing",
      "Economics",
      "Engineering",
      "Enterprise resource planning",
      "Exploit",
      "Finance",
      "Industrial engineering",
      "Knowledge management",
      "Macroeconomics",
      "Manufacturing",
      "Manufacturing engineering",
      "Manufacturing execution system",
      "Marketing",
      "Operating system",
      "Process (computing)",
      "Production (economics)",
      "Production planning",
      "Resource (disambiguation)",
      "Small and medium-sized enterprises"
    ],
    "authors": [
      {
        "surname": "Teerasoponpong",
        "given_name": "Siravat"
      },
      {
        "surname": "Sopadang",
        "given_name": "Apichat"
      }
    ]
  },
  {
    "title": "Differentially private user-based collaborative filtering recommendation based on k -means clustering",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114366",
    "abstract": "Collaborative filtering (CF) recommendation is well-known for its outstanding recommendation performance, but previous researches showed that it could cause privacy leakage for users due to k -nearest neighboring (KNN) attacks. Recently, the notion of differential privacy (DP) has been applied to privacy preservation in recommendation systems. However, as far as we know, existing differentially private CF recommendation systems degrade the recommendation performance (such as recall and precision) to an unacceptable level. In this paper, to address the performance degradation problem, we propose a differentially private user-based CF recommendation system based on k -means clustering (KDPCF). Specifically, to improve the recommendation performance, KDPCF first clusters the dataset into categories by k -means clustering and appropriately adjusts the size of the target category to which the target user belongs, so that only users in the well-sized target category are used for recommendation. Then, it selects efficiently a set of neighbors from the target category at one time by employing only one instance of exponential mechanism instead of the composition of multiple ones, and then uses a CF algorithm to recommend based on this set of neighbors. We theoretically prove that our system achieves differential privacy. Empirically, we use two public datasets to evaluate our recommendation system. The experimental results demonstrate that our system has a significant performance improvement compared to existing ones.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310459",
    "keywords": [
      "Cluster analysis",
      "Collaborative filtering",
      "Computer science",
      "Computer security",
      "Data mining",
      "Differential privacy",
      "Information retrieval",
      "Machine learning",
      "Private information retrieval",
      "Programming language",
      "Recommender system",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Zhili"
      },
      {
        "surname": "Wang",
        "given_name": "Yu"
      },
      {
        "surname": "Zhang",
        "given_name": "Shun"
      },
      {
        "surname": "Zhong",
        "given_name": "Hong"
      },
      {
        "surname": "Chen",
        "given_name": "Lin"
      }
    ]
  },
  {
    "title": "TransPhrase: A new method for generating phrase embedding from word embedding in Chinese",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114387",
    "abstract": "Currently, there are two main methods of learning phrase embedding: the distribution method and the composition method. The distribution method treats a phrase as an entirety and learns phrase embedding based on the context of the phrase. Its disadvantage is that it completely ignores the semantics of the component words of the phrase and the data sparseness problem. The composition method calculates the phrase embedding from the embedding of component words. The existing composition methods fail to represent the semantics of phrases well. Because of the above problems, we take Chinese, for example, and propose a new composition method to generate phrase embedding from the component word embedding, named TransPhrase. It is a neural network that can use LSTM to learn the order information of component words, use the attention mechanism to learn the important information of component words, and use a fully connected network to learn the semantic information of component words, and finally predict phrase embedding. It can solve the data sparseness problem and properly and fully represent the semantics of phrases. Our evaluation of three Chinese phrase-level semantic tasks shows that the comprehensive performance of TransPhrase's phrase representation is better than the composition method, the distribution method, and the pre-trained language model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310605",
    "keywords": [
      "Artificial intelligence",
      "Component (thermodynamics)",
      "Computer science",
      "Embedding",
      "Linguistics",
      "Natural language processing",
      "Philosophy",
      "Phrase",
      "Physics",
      "Programming language",
      "Semantics (computer science)",
      "Thermodynamics",
      "Word (group theory)",
      "Word embedding"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Rongsheng"
      },
      {
        "surname": "Huang",
        "given_name": "Shaobin"
      },
      {
        "surname": "Mao",
        "given_name": "Xiangke"
      },
      {
        "surname": "He",
        "given_name": "Jie"
      },
      {
        "surname": "Shen",
        "given_name": "Linshan"
      }
    ]
  },
  {
    "title": "A hybrid adaptive large neighbourhood search algorithm for the capacitated location routing problem",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114304",
    "abstract": "This paper proposes a new hybrid metaheuristic algorithm that is composed of the adaptive large neighbourhood search (ALNS) and the variable neighbourhood search (VNS) algorithms to tackle the location routing problem (LRP) with capacity constraints. The rationale of the proposed hybrid metaheuristic algorithm is to enhance the performance of the ALNS algorithm by incorporating the VNS algorithm as an elitist local search. Therefore, the diversification and intensification strategies of the proposed hybrid metaheuristic algorithm are realized via the ALNS and VNS algorithms, respectively. The performance evaluation tests of the proposed hybrid metaheuristic algorithm are performed on the three classical LRP benchmark sets taken from the related literature, and the obtained results are compared against some of the formerly proposed and published methods in terms of solution quality. Computational results indicate that the proposed hybrid metaheuristic algorithm has a satisfactory performance in solving the LRP instances and is a competitive algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310022",
    "keywords": [
      "Algorithm",
      "Benchmark (surveying)",
      "Computer network",
      "Computer science",
      "Constraint logic programming",
      "Constraint programming",
      "Geodesy",
      "Geography",
      "Guided Local Search",
      "Hybrid algorithm (constraint satisfaction)",
      "Local search (optimization)",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Neighbourhood (mathematics)",
      "Routing (electronic design automation)",
      "Stochastic programming",
      "Variable neighborhood search",
      "Vehicle routing problem"
    ],
    "authors": [
      {
        "surname": "Şatir Akpunar",
        "given_name": "Özge"
      },
      {
        "surname": "Akpinar",
        "given_name": "Şener"
      }
    ]
  },
  {
    "title": "Learning individual preferences from aggregate data: A genetic algorithm for discovering baskets of television shows with affinities to political and social interests",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114184",
    "abstract": "This paper presents a flexible general-purpose framework using genetic and multi-objective evolutionary algorithms that can leverage “unlabeled” (and anonymized) panel data on television viewership along with aggregate-level vote or public opinions statistics to (i) identify sets of programs that have affinities with politics and social issues, and (ii) estimate individual preferences from unlabeled data. The applications of this framework are significant given the wide interest in using big data for political advertising and building election forecasting models with non-polling data. Analyzing viewership spanning over seven billion minutes from Nielsen’s TV panel for an entire year (2016), we illustrate how this framework can learn interesting baskets of programs whose viewership can help estimate individual attitudes toward politics, global warming, same-sex marriage, and abortion.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309180",
    "keywords": [
      "Advertising",
      "Aggregate (composite)",
      "Artificial intelligence",
      "Audience measurement",
      "Business",
      "Composite material",
      "Computer science",
      "Data science",
      "Econometrics",
      "Economics",
      "Genetic algorithm",
      "Law",
      "Leverage (statistics)",
      "Machine learning",
      "Materials science",
      "Operating system",
      "Panel data",
      "Political science",
      "Politics",
      "Polling",
      "Social media",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Padmanabhan",
        "given_name": "Balaji"
      },
      {
        "surname": "Barfar",
        "given_name": "Arash"
      }
    ]
  },
  {
    "title": "Breast calcification detection based on multichannel radiofrequency signals via a unified deep learning framework",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114218",
    "abstract": "Breast calcifications in radiographic images suggest a high likelihood of breast lesion malignancy. However, it is difficult to detect calcifications in traditional B-mode ultrasound images due to resolution limits and speckle noise. In this paper, we propose a unified deep learning framework for automatic calcification detection based on multichannel ultrasound radio frequency (RF) signals. First, beamforming is used during preprocessing to merge and blend multichannel signals into one-channel RF signals. Each scan line is converted into a spectrogram by the short-time Fourier transform (STFT) to utilize the frequency domain characteristics. Then, an improved fully convolutional neural network called the RF signal Spectrogram-Calcification-Detection-Net (SCD-Net) is proposed to detect calcifications from spectrograms. This method employs a deep learning architecture based on YOLOv3 and combines features via convolutional long short-term memory (ConvLSTM). Next, a Kalman filter for tracking calcifications between consecutive spectrograms based on SCD-Net detection results is applied since the spatial coherence of calcifications in neighboring frames can be taken into account. Finally, the detected calcification is mapped from the time domain of spectrograms to B-mode images for clinical diagnosis. Experiments were conducted on a database of 337 experienced doctor-marked breast tumors with calcifications. Compared to the state-of-the-art methods for detecting calcifications, the proposed method achieved an average precision (AP) of 88.25%, an accuracy of 84% and an F1 score of 91%. The experimental results demonstrate that the unified framework has great performance for tumor calcification detection. The system can be effectively applied in a portable ultrasound instrument to accurately help radiologists and provide guidance for breast tumor diagnosis. This implies that the proposed approach can be implemented in real practice for analyzing breast RF signals, which have many useful medical applications in clinical breast tumor diagnosis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030943X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Frequency domain",
      "Pattern recognition (psychology)",
      "Speckle noise",
      "Speckle pattern",
      "Spectrogram",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Qiao",
        "given_name": "Menyun"
      },
      {
        "surname": "Fang",
        "given_name": "Zhou"
      },
      {
        "surname": "Guo",
        "given_name": "Yi"
      },
      {
        "surname": "Zhou",
        "given_name": "Shichong"
      },
      {
        "surname": "Chang",
        "given_name": "Cai"
      },
      {
        "surname": "Wang",
        "given_name": "Yuanyuan"
      }
    ]
  },
  {
    "title": "User profiling via application usage pattern on digital devices for digital forensics",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114488",
    "abstract": "In digital forensics, user profiling aims to predict characteristics of the user from digital evidence extracted from digital devices (e.g. smartphone, laptop, tablet). Previous researches showed promising results, but there are limitations to apply practical investigations. The researches so far have focused only on specific applications, devices, or operating systems by analyzing the order of execution or volatile data such as network traffic and online content. This paper introduces a user profiling method, named Entity Profiling with Binary Predicates (EPBP) model, which analyzes non-volatile data remained on digital devices. The proposed model defines that a user has two properties: tendency and impact, which indicate patterns of application usage. Based on the attributes, the EPBP model generates users’ profiles and performs similarity analysis to differentiate between the users. We also present methods for clustering and anomaly detection through real case studies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311349",
    "keywords": [
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Digital forensics",
      "Laptop",
      "Machine learning",
      "Mobile device",
      "Network forensics",
      "Operating system",
      "Profiling (computer programming)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Kwon",
        "given_name": "Hongkyun"
      },
      {
        "surname": "Lee",
        "given_name": "Sangjin"
      },
      {
        "surname": "Jeong",
        "given_name": "Doowon"
      }
    ]
  },
  {
    "title": "Learning with continuous piecewise linear decision trees",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114214",
    "abstract": "In this paper, we propose a piecewise linear decision tree and its generalized form, namely the (G)PWL-DT, which introduces piecewise linearity and overcomes the discontinuity of the existing piecewise constant decision trees (PWC-DT). The proposed (G)PWL-DT inherits the basic topology and interpretability of decision trees by recursively partitioning the domain into subregions, which are represented by leaf nodes. Rather than the indicator function, the (G)PWL-DT employs rectifier linear units (ReLU) to interpret domain partitions, where the nested ReLUs are combined to formulate the corresponding PWL decision rules. Due to the piecewise linearity of each leaf node, additional boundaries among linear areas are obtained to approach greater flexibility than the existing PWC-DT under the same tree structure, where the continuity can also be guaranteed. Then, an optimization algorithm is constructed analogously based on the second-order approximation. The proposed (G)PWL-DT can be flexibly applied as a novel decision tree in different tree learning methods and it can also be regarded as a simple extension of ReLUs to the framework of tree learning. Numerical experiments verify the effectiveness of the proposed (G)PWL-DT and its potential as an alternative of the existing PWC-DT to approach better performance even with more concise structures.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309404",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Decision tree",
      "Discrete mathematics",
      "Domain (mathematical analysis)",
      "Domain theory",
      "Engineering",
      "Geometry",
      "Interpretability",
      "Linearity",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Node (physics)",
      "Physics",
      "Piecewise",
      "Piecewise linear function",
      "Piecewise linear manifold",
      "Quantum mechanics",
      "Structural engineering",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Tao",
        "given_name": "Qinghua"
      },
      {
        "surname": "Li",
        "given_name": "Zhen"
      },
      {
        "surname": "Xu",
        "given_name": "Jun"
      },
      {
        "surname": "Xie",
        "given_name": "Na"
      },
      {
        "surname": "Wang",
        "given_name": "Shuning"
      },
      {
        "surname": "Suykens",
        "given_name": "Johan A.K."
      }
    ]
  },
  {
    "title": "Applications of picture fuzzy similarity measures in pattern recognition, clustering, and MADM",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114264",
    "abstract": "Picture fuzzy set (PFS) is a direct extension of fuzzy sets (FSs) and intuitionistic fuzzy sets (IFSs) and is quite powerful than FSs and IFSs in expressing the uncertainty and vagueness in our daily life problems. In this article, we propose some new similarity measures for PFSs which are capable of distinguishing highly similar but inconsistent PFSs. We also demonstrate their applications in pattern recognition using some illustrative examples as well as with real data. We assess the performance of the proposed measures using the concept of degree of confidence. We also extend the maximum spanning tree (MST) clustering algorithm to PF (picture fuzzy)-environment and propose a picture fuzzy maximum spanning tree (PFMST) clustering method. Further, we introduce a new attribute weight determining formula based on PF-similarity measures in multi-attribute decision-making (MADM) problem. We also establish the superiority of our proposed PF-similarity measures over some existing PF-similarity measures in view of the structured linguistic variables.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309763",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Fuzzy clustering",
      "Fuzzy logic",
      "Fuzzy set",
      "Image (mathematics)",
      "Mathematics",
      "Minimum spanning tree",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Similarity (geometry)",
      "Vagueness"
    ],
    "authors": [
      {
        "surname": "Singh",
        "given_name": "Surender"
      },
      {
        "surname": "Ganie",
        "given_name": "Abdul Haseeb"
      }
    ]
  },
  {
    "title": "Customized bus route design with pickup and delivery and time windows: Model, case study and comparative analysis",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114242",
    "abstract": "The customized bus (CB) is an emerging type of public transportation system, which not only provides a flexible and reliable demand-responsive service, but also reduces the usage of private car to alleviate traffic congestion in metropolitan cities. The customized bus route design problem (CBRDP) is a crucial procedure in the CB service system designing. In this work, we develop a new type of problem scenario: Multi-Trip Multi-Pickup and Delivery Problem with Time Windows, to describe CBRDP by simultaneously optimizing the operating cost and passenger profit, where excess travel time is introduced to estimate passenger extra cost compared with taxi service, and each vehicle is allowed to perform multiple trips for operational cost savings. To solve this problem, a constructive two-stage heuristic algorithm is presented to obtain the Pareto solution. Taking a benchmark problem and Beijing commuting corridor as case studies, we calculate and compare the monetary and travel costs of CB with other travel modes, and quantitatively confirm that the CB can be a cost-effective choice for passengers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030960X",
    "keywords": [
      "Artificial intelligence",
      "Beijing",
      "Benchmark (surveying)",
      "Business",
      "China",
      "Computer science",
      "Engineering",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Law",
      "Marketing",
      "Medicine",
      "Metropolitan area",
      "Operating cost",
      "Operations research",
      "Parallel computing",
      "Pathology",
      "Pickup",
      "Political science",
      "Public transport",
      "Service (business)",
      "Service level",
      "TRIPS architecture",
      "Traffic congestion",
      "Transport engineering",
      "Waste management"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Xi"
      },
      {
        "surname": "Wang",
        "given_name": "Yinhai"
      },
      {
        "surname": "Wang",
        "given_name": "Yong"
      },
      {
        "surname": "Qu",
        "given_name": "Xiaobo"
      },
      {
        "surname": "Ma",
        "given_name": "Xiaolei"
      }
    ]
  },
  {
    "title": "Effects of the entropy weight on TOPSIS",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114186",
    "abstract": "The Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) is a classical multi-attribute decision-making method, which is widely used in various fields for decision-making or evaluation. The entropy method (EM) is frequently used in determining attribute weights for TOPSIS, and the weight determined by the EM is always called entropy weight (EW). In this paper, based on a large number of data and theoretical analysis, the effects of the EW on TOPSIS are analyzed. It is found that the EW can enhance the function of the attribute with the highest diversity of attribute data (DAD) as well as weaken the function of the attributes with a low DAD in decision-making or evaluation. Sometimes the EW even causes the decision-making or evaluation result to be seriously affected by the attribute with the highest DAD (called primacy attribute, abbreviated as PA). Since the EW can enhance the function of the PA in decision-making or evaluation, it is conducive to increase the dipartite degree of the relative closeness (RC), but reduces the comprehensiveness of the RC, and may even lead to unreasonable decision-making or evaluation result. In order to adjust the effects of the EW on TOPSIS, the entropy-based TOPSIS with adjustable weight coefficient is proposed in this paper. Some discussions on the application of the proposed method are also given.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309209",
    "keywords": [
      "Closeness",
      "Computer science",
      "Data mining",
      "Entropy (arrow of time)",
      "Ideal solution",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Operations research",
      "Physics",
      "TOPSIS",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Pengyu"
      }
    ]
  },
  {
    "title": "Effect of the Municipal Human Development Index on the results of the 2018 Brazilian presidential elections",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114305",
    "abstract": "This paper explores the impact of the development indicators of Brazilian municipalities in the results of the 2018 Brazilian presidential elections. As the development indicator of a municipality we used the Municipal Human Development Index (MHDI), a known index derived from the Human Development Index (HDI) that has been adapted to Brazilian reality. The MHDI is composed by three sub-indices: education, income and longevity. Based on data publicly available, we used six different supervised machine learning classification algorithms for predicting the winner of the elections in municipalities. The results reached 87% of accuracy when using the education and income sub-indices as predictors of the elections. Based on the results obtained, we were able to confirm that municipalities with higher values for education and income sub-indices overwhelmingly preferred the right-wing candidate, while the left-wing option was preferred by those with lower values of the same sub-indices.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310034",
    "keywords": [
      "Computer science",
      "Econometrics",
      "Economic growth",
      "Economics",
      "Human Development Index",
      "Human development (humanity)",
      "Index (typography)",
      "Law",
      "Political science",
      "Politics",
      "Presidential system",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Huerta Yero",
        "given_name": "Eduardo Javier"
      },
      {
        "surname": "Sacco",
        "given_name": "Nilton Cesar"
      },
      {
        "surname": "Nicoletti",
        "given_name": "Maria do Carmo"
      }
    ]
  },
  {
    "title": "A hybrid CAD system for lung nodule detection using CT studies based in soft computing",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114259",
    "abstract": "Lung modules are an initial indicator as to whether or not a patient will develop lung carcinoma which has a significant mortality rate if it is not detected at an early stage. The detection of these lung nodules is a complex task and is time-consuming for the radiologist. For these reasons, CAD systems have been developed and employed for the detection of lung nodules. In this article, we present a CAD system that uses a hybrid strategy: techniques for the analysis of medical images and soft computing (fuzzy clustering, SVM and ANN) with a description of the main stages: preprocessing, identification of ROIs (region of interest), creation of VOIs (volume interests) and ROI classifications. Remarkable elements of the system are: detection automation, a new phase to reduce the ROIs false positives and a new algorithm to build the VOIs in order to improve the location and a better detection. For its development, helical CT studies proceeding from the LIDC public database have been used. The system achieves similar and even better results to other CADs for the same purpose with a sensitivity of 82% and a number of false positives of 7.3 per study.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309738",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "CAD",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Engineering",
      "Engineering drawing",
      "False positive paradox",
      "Fuzzy logic",
      "Identification (biology)",
      "Pattern recognition (psychology)",
      "Preprocessor",
      "Region of interest",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Rey",
        "given_name": "Alberto"
      },
      {
        "surname": "Arcay",
        "given_name": "Bernardino"
      },
      {
        "surname": "Castro",
        "given_name": "Alfonso"
      }
    ]
  },
  {
    "title": "Assessing determinants influencing continued use of live streaming services: An extended perceived value theory of streaming addiction",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114241",
    "abstract": "Streaming services are becoming very popular among people and are considered as an entertainment alternative to the traditional model of broadcasting services due to its exclusiveness as well as better quality and variety of contents. The present article examines various factors influencing the continued intention to use live streaming services in India. To this end, the study fills the research gaps and extends perceived value theory by including a few important determinants, namely effort expectancy, performance expectancy, perceived innovativeness, perceived risk, perceived enjoyment and addiction to heavy viewing. The study contributes to the existing literature on streaming services addiction and extends its association with heavy viewing. Existing studies on addiction were found insufficient in explaining users’ heavy viewing of live streaming content, which has the potential to become a serious social problem in the future. The study’s findings suggest that the convenience value has the highest impact on users’ continued intention to use streaming services, followed by perceived enjoyment. The addiction to heavy viewing due to the use of streaming services by users has new social implications. The findings suggest that managers of streaming apps should promote their apps to consumers by highlighting various consumption values and make sure that their apps are attractive and provide personalized experience to the users. Moreover, the study discusses the growing addictive behaviour with regard to streaming services.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309593",
    "keywords": [
      "Addiction",
      "Advertising",
      "Art",
      "Business",
      "Computer science",
      "Consumption (sociology)",
      "Entertainment",
      "Epistemology",
      "Expectancy theory",
      "Internet privacy",
      "Live streaming",
      "Machine learning",
      "Multimedia",
      "Neuroscience",
      "Philosophy",
      "Psychology",
      "Quality (philosophy)",
      "Social psychology",
      "Social science",
      "Sociology",
      "Value (mathematics)",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Singh",
        "given_name": "Sonali"
      },
      {
        "surname": "Singh",
        "given_name": "Nidhi"
      },
      {
        "surname": "Kalinić",
        "given_name": "Zoran"
      },
      {
        "surname": "Liébana-Cabanillas",
        "given_name": "Francisco J."
      }
    ]
  },
  {
    "title": "A fast and accurate similarity measure for long time series classification based on local extrema and dynamic time warping",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114374",
    "abstract": "The problem of similarity measures is a major area of interest within the field of time series classification (TSC). With the ubiquitous of long time series and the increasing demand for analyzing them on limited resource devices, there is a crucial need for efficient and accurate measures to deal with such kind of data. In fact, there are a plethora of good time series similarity measures in the literature. However, most existing methods achieve good performance for short time series, but their effectiveness decreases quickly as time series are longer. In this paper, we develop a new parameter-free measure for the specific purpose of quickly and accurately assessing the similarity between two given long time series. The proposed “Local Extrema Dynamic Time Warping” (LE-DTW) consists of two steps. The first is a time series representation technique that starts by reducing the dimensionality of a given time series using its local extrema. Next, it physically separates the minima and maxima points for more intuitiveness and consistency of the so-obtained time series representation. The second step consists in adapting the Dynamic Time Warping (DTW) measure so as to evaluate the score of similarity between the generated representations. We test the performance of LE-DTW on a wide range of real-world problems from the UCR time series archive for TSC. Experimental results indicate that for short time series, the proposed method achieves reasonable classification accuracy as compared to DTW. However, for long time series, LE-DTW performs much better. Indeed, it outperforms DTW while providing competitive results against popular distance-based classifiers. Moreover, in terms of efficiency, LE-DTW is orders of magnitude faster than DTW.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310514",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Consistency (knowledge bases)",
      "Curse of dimensionality",
      "Data mining",
      "Dynamic time warping",
      "Image (mathematics)",
      "Law",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Maxima and minima",
      "Measure (data warehouse)",
      "Nearest neighbor search",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Series (stratigraphy)",
      "Similarity (geometry)",
      "Similarity measure",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Lahreche",
        "given_name": "Abdelmadjid"
      },
      {
        "surname": "Boucheham",
        "given_name": "Bachir"
      }
    ]
  },
  {
    "title": "Optimized scheduling method for office building renovation projects",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114212",
    "abstract": "Office building renovation works allow for dynamic schedule planning, which breaks traditional logical relationships among construction work tasks. Consequently, the duration of renovation projects can be shortened significantly through dynamic scheduling. However, there are limited studies addressing this aspect. Therefore, in this study, we developed a scheduling method for office building renovation projects enabling the creation of schedule alternatives and suggesting optimized schedules in terms of project duration and cost efficiency. The developed method was applied to an actual renovation project, and the following results were obtained: (i) The method could develop six schedule alternatives reflecting project conditions and improve their cost and duration performance by 14.83% to 23.17% through optimization. (ii) Specifically, the sixth and fourth alternatives could yield a 36.18% lower project duration and 63.54% higher cost performance, respectively, compared to the other alternatives. Using this optimized scheduling method, project owners and managers can successfully plan their renovation projects while satisfying the requirements in terms of the project duration and cost.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309398",
    "keywords": [
      "Archaeology",
      "Art",
      "Computer science",
      "Duration (music)",
      "Engineering",
      "History",
      "Literature",
      "Operating system",
      "Operations management",
      "Operations research",
      "Plan (archaeology)",
      "Project charter",
      "Project management",
      "Project planning",
      "Schedule",
      "Scheduling (production processes)",
      "Systems engineering",
      "Work breakdown structure",
      "Work schedule"
    ],
    "authors": [
      {
        "surname": "Cho",
        "given_name": "Kyuman"
      },
      {
        "surname": "Kim",
        "given_name": "Taehoon"
      }
    ]
  },
  {
    "title": "A Recurrent Neural Network based deep learning model for offline signature verification and recognition system",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114249",
    "abstract": "With the recent advancement in information technology field, the demand to develop a person authentication system through verifying their offline signatures is gradually increasing. This type of system may be used to verify various official documents through verifying the signatures of the concerned persons present in the documents. This article proposes a Recurrent Neural Network (RNN), a deep learning network, based method to verify and recognize offline signatures of different persons. Various structural and directional features have been extracted locally from each signature sample and the generated feature vectors have been studied using two different models of RNN—long-short term memory (LSTM) and bidirectional long–short term memory (BLSTM). The performance of the proposed system has been tested on six widely used public signature databases—GPDS synthetic, GPDS-300, MCYT-75, CEDAR, BHSig260 Hindi, and BHSig260 Bengali. Experiment has also been performed using Convolutional Neural Network (CNN) to have a comparison with RNN based results. Experimental results demonstrate that the proposed RNN based signature verification and recognition system is superior over CNN and also outperforms the existing state-of-the-art results in this regard.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309659",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Bengali",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Feature (linguistics)",
      "Geometry",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Recurrent neural network",
      "Signature (topology)"
    ],
    "authors": [
      {
        "surname": "Ghosh",
        "given_name": "Rajib"
      }
    ]
  },
  {
    "title": "BinDeep: A deep learning approach to binary code similarity detection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114348",
    "abstract": "Binary code similarity detection (BCSD) plays an important role in malware analysis and vulnerability discovery. Existing methods mainly rely on the expert’s knowledge for the BCSD, which may not be reliable in some cases. More importantly, the detection accuracy (or performance) of these methods are not so satisfied. To address these issues, we propose BinDeep, a deep learning approach for binary code similarity detection. This method firstly extracts the instruction sequence from the binary function and then uses the instruction embedding model to vectorize the instruction features. Next, BinDeep applies a Recurrent Neural Network (RNN) deep learning model to identify the specific types of two functions for later comparison. According to the type information, BinDeep selects the corresponding deep learning model for similarity comparison. Specifically, BinDeep uses the Siamese neural networks, which combine the LSTM and CNN to measure the similarities of two target functions. Different from the traditional deep learning model, our hybrid model takes advantage of the CNN spatial structure learning and the LSTM sequence learning. The evaluation shows that our approach can achieve good BCSD between cross-architecture, cross-compiler, cross-optimization, and cross-version binary code.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310332",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Code (set theory)",
      "Computer science",
      "Deep learning",
      "Image (mathematics)",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Tian",
        "given_name": "Donghai"
      },
      {
        "surname": "Jia",
        "given_name": "Xiaoqi"
      },
      {
        "surname": "Ma",
        "given_name": "Rui"
      },
      {
        "surname": "Liu",
        "given_name": "Shuke"
      },
      {
        "surname": "Liu",
        "given_name": "Wenjing"
      },
      {
        "surname": "Hu",
        "given_name": "Changzhen"
      }
    ]
  },
  {
    "title": "Feature selection using Binary Crow Search Algorithm with time varying flight length",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114288",
    "abstract": "Crow Search Algorithm (CSA) is a simple yet effective meta-heuristic algorithm that has been applied to solve many engineering problems. In CSA, fl parameter controls the search capability of crows and AP parameter balances the trade-off between exploration and exploitation. The parameter fl is initialized to a constant value in CSA. However, CSA faces the problem of being trapped in local minima. This work proposes the solution to this problem by introducing the new concept of time varying flight length in CSA. The value of fl should be large in initial stages of algorithm in order to support random exploration and it should gradually decrease in later iterations to encourage the exploitation of good solutions found so far. The proposed approach, Binary Crow Search Algorithm with Time Varying Flight Length (BCSA-TVFL) is applied to feature selection problems in wrapper mode. Eight variants of BCSA-TVFL based on eight different transfer functions are tested. The best performing variant is then selected and compared with other state-of-the-art wrapper feature selection techniques and standard filter feature selection techniques. Performance of proposed approach is tested on 20 standard UCI datasets. Experimental result comparison shows that the proposed feature selection technique performs better than other competitors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309908",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Binary number",
      "Binary search algorithm",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Feature selection",
      "Filter (signal processing)",
      "Heuristic",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Maxima and minima",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Search algorithm",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Chaudhuri",
        "given_name": "Abhilasha"
      },
      {
        "surname": "Sahu",
        "given_name": "Tirath Prasad"
      }
    ]
  },
  {
    "title": "Efficient and compact face descriptor for driver drowsiness detection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114334",
    "abstract": "Current advances in driver drowsiness detection consist of a variety of innovative technologies generally based on driver state monitoring systems. Extracting effective and relevant features to characterize drowsy symptoms in images and videos is still an open topic. In this work, we introduce a face monitoring system based on a compact face texture descriptor able to cover the most discriminant drowsy features. The compactness has been achieved by both a multi-scale pyramidal face representation that capture the main characteristics of local and global information, and the feature selection process applied on the raw extracted features. The proposed framework is rolled out in four phases: (i) face detection and alignment; (ii) Pyramid-Multi Level (PML) face representation; (iii) face description using a multi-level multi scale feature extraction; and (vi) feature subset selection and classification. Experiments conducted on the public dataset NTH Drowsy Driver Detection (NTHUDDD) show the effectiveness of the proposed face descriptor and the associated selection schemes. The results show that the proposed method compares favorably with several approaches including those based on deep Convolutional Neural Networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310241",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Face (sociological concept)",
      "Face detection",
      "Facial recognition system",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature selection",
      "Geometry",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pyramid (geometry)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Moujahid",
        "given_name": "Abdelmalik"
      },
      {
        "surname": "Dornaika",
        "given_name": "Fadi"
      },
      {
        "surname": "Arganda-Carreras",
        "given_name": "Ignacio"
      },
      {
        "surname": "Reta",
        "given_name": "Jorge"
      }
    ]
  },
  {
    "title": "Learning with continuous piecewise linear decision trees",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114214",
    "abstract": "In this paper, we propose a piecewise linear decision tree and its generalized form, namely the (G)PWL-DT, which introduces piecewise linearity and overcomes the discontinuity of the existing piecewise constant decision trees (PWC-DT). The proposed (G)PWL-DT inherits the basic topology and interpretability of decision trees by recursively partitioning the domain into subregions, which are represented by leaf nodes. Rather than the indicator function, the (G)PWL-DT employs rectifier linear units (ReLU) to interpret domain partitions, where the nested ReLUs are combined to formulate the corresponding PWL decision rules. Due to the piecewise linearity of each leaf node, additional boundaries among linear areas are obtained to approach greater flexibility than the existing PWC-DT under the same tree structure, where the continuity can also be guaranteed. Then, an optimization algorithm is constructed analogously based on the second-order approximation. The proposed (G)PWL-DT can be flexibly applied as a novel decision tree in different tree learning methods and it can also be regarded as a simple extension of ReLUs to the framework of tree learning. Numerical experiments verify the effectiveness of the proposed (G)PWL-DT and its potential as an alternative of the existing PWC-DT to approach better performance even with more concise structures.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309404",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Decision tree",
      "Discrete mathematics",
      "Domain (mathematical analysis)",
      "Domain theory",
      "Engineering",
      "Geometry",
      "Interpretability",
      "Linearity",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Node (physics)",
      "Physics",
      "Piecewise",
      "Piecewise linear function",
      "Piecewise linear manifold",
      "Quantum mechanics",
      "Structural engineering",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Tao",
        "given_name": "Qinghua"
      },
      {
        "surname": "Li",
        "given_name": "Zhen"
      },
      {
        "surname": "Xu",
        "given_name": "Jun"
      },
      {
        "surname": "Xie",
        "given_name": "Na"
      },
      {
        "surname": "Wang",
        "given_name": "Shuning"
      },
      {
        "surname": "Suykens",
        "given_name": "Johan A.K."
      }
    ]
  },
  {
    "title": "LCP-Net: A local context-perception deep neural network for medical image segmentation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114234",
    "abstract": "Automatic image segmentation is an indispensable step in medical image analysis, and it plays an important role in computer-assisted radiotherapy, disease diagnosis and treatment effect evaluation. The difficulty of medical image segmentation is greatly enhanced by the blurry nature of medical image, the complex shape of objects and the existence of noise. In recent years, segmentation methods based on deep learning, especially convolutional neural network, have made great progress in improving the accuracy of medical image segmentation. However, these methods also have poor ability to distinguish similar objects in different environments, because of insufficient use of the local context information of images during the process of feature extraction. To address this problem, this paper proposes a deep neural network (LCP-Net) that can perceive multi-scale context information of images. LCP-Net improves the utilization of context information of feature encoders by using Parallel Dilated Convolution (PDC) and Local Context Embedding (LCE), which are beneficial to get feature map rich in environmental information. In addition, to improve the segmentation accuracy of the model for small objects and alleviate the swing issue during training, we propose a novel improved cross-entropy loss (DDCLoss), which can adaptively adjust the weight of loss according to the certainty and deviation distance of the predicted pixel value and enable the model to focus on optimizing the sample points with low certainty and tend to be mislabeled. Experimental results on three different medical datasets demonstrate that compared with the state-of-the-art medical image segmentation models, our proposed LCP-Net can achieve better segmentation performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309532",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Convolutional neural network",
      "Deep learning",
      "Feature (linguistics)",
      "Image segmentation",
      "Linguistics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Scale-space segmentation",
      "Segmentation",
      "Segmentation-based object categorization"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Dunlu"
      },
      {
        "surname": "Xiong",
        "given_name": "Shiyong"
      },
      {
        "surname": "Peng",
        "given_name": "Wenjia"
      },
      {
        "surname": "Lu",
        "given_name": "Jianping"
      }
    ]
  },
  {
    "title": "Bearing remaining useful life prediction under starved lubricating condition using time domain acoustic emission signal processing",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114391",
    "abstract": "In this paper, the estimation of the remaining useful life (RUL) of angular contact ball bearing using time-domain signal processing method is discussed. An experimental setup based on acoustic emission (AE) signal is used to extract and collect the desired data. The residual life test is performed on the SKF 7202 BEP angular contact ball bearing. Sixty-time domain features have been introduced and used for fault detection. Improved Distance Evaluation (IDE) method has been used for feature dimensionality reduction and the best 10 features have been selected. K-Nearest Neighbors (KNN) algorithm has been used to investigate the classification accuracy of IDE based on selected features for classifying healthy and faulty bearings. The results show that the IDE method enables natural fault detection in bearings with high precision. To validate the performance of the KNN classifier, performance indices such as accuracy, precision, and specificity are applied. The results show that kurtosis, FM4, k factor, energy, and peak are the best features and kurtosis has the highest KNN rank with accuracy, precision, and specificity of 97%, 93%, and 94%, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310630",
    "keywords": [
      "Acoustic emission",
      "Acoustics",
      "Algorithm",
      "Artificial intelligence",
      "Bearing (navigation)",
      "Classifier (UML)",
      "Computer hardware",
      "Computer science",
      "Computer vision",
      "Curse of dimensionality",
      "Digital signal processing",
      "Kurtosis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Residual",
      "Signal processing",
      "Statistics",
      "Time domain",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Motahari-Nezhad",
        "given_name": "Mohsen"
      },
      {
        "surname": "Jafari",
        "given_name": "Seyed Mohammad"
      }
    ]
  },
  {
    "title": "A multimodal approach using deep learning for fall detection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114226",
    "abstract": "A computational system able to automatically and efficiently detect and classify falls would be beneficial for monitoring the elderly population and speed up the assistance proceedings, reducing the risk of prolonged injuries and death. One of the most common problems in such systems is the high number of false-positives in their recognition scheme, which may cause an overload on surveillance system calls. We address this problem by proposing different topologies of a multimodal convolution neural network, which is trained to detect falls based on RGB images and information from accelerometers. We train and evaluate our networks with the UR Fall Detection dataset and UP-Fall dataset, and provide an extensive comparison with state-of-the-art models. Our model reached good results on UR Fall Detection dataset and achieved the state-of-art on UP-Fall detection dataset, relying on easily available sensors to do so, demonstrating it can be a scalable solution for robust fall detection in the real world.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309489",
    "keywords": [
      "Accelerometer",
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Database",
      "Deep learning",
      "Demography",
      "False positive paradox",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Population",
      "RGB color model",
      "Scalability",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Galvão",
        "given_name": "Yves M."
      },
      {
        "surname": "Ferreira",
        "given_name": "Janderson"
      },
      {
        "surname": "Albuquerque",
        "given_name": "Vinícius A."
      },
      {
        "surname": "Barros",
        "given_name": "Pablo"
      },
      {
        "surname": "Fernandes",
        "given_name": "Bruno J.T."
      }
    ]
  },
  {
    "title": "Improving DEA cross-efficiency optimization in portfolio selection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114280",
    "abstract": "This paper deals with the role of alternative optimal solutions existing in the data envelopment analysis (DEA) models for cross-efficiency evaluation in portfolio selection. The paper shows that incorporating alternative optimal solutions for constructing cross-efficiency matrix improves the result of the mean-variance portfolio selection method. This improvement means that building portfolios with lower risk and higher expected return is possible when alternative optimal solutions are considered. The proposed method in this paper is applied to stock portfolio selection in the Tehran stock market.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309866",
    "keywords": [
      "Computer science",
      "Data envelopment analysis",
      "Econometrics",
      "Economics",
      "Engineering",
      "Finance",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Mechanical engineering",
      "Modern portfolio theory",
      "Portfolio",
      "Portfolio optimization",
      "Selection (genetic algorithm)",
      "Stock (firearms)"
    ],
    "authors": [
      {
        "surname": "Amin",
        "given_name": "Gholam R."
      },
      {
        "surname": "Hajjami",
        "given_name": "Mohaddeseh"
      }
    ]
  },
  {
    "title": "Context expansion approach for graph-based word sense disambiguation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114313",
    "abstract": "Word sense disambiguation is a process to correctly identify the meanings of words in a given context. Being important in many natural language processing applications, this process is crucial in automatically understanding natural language expressions. Herein, we propose a variation of a well-known unsupervised graph-based word sense disambiguation method that utilizes all possible semantic information from a used lexical resource to increase graph-semantic connectivity for identifying the intended meanings of words in a given context. If the words have multiple potential meanings (senses) based on context, the proposed method builds an expanded graph representing most relevant semantic information of the words to be disambiguated. Nodes in the graph correspond to the context expansion set, which contains all associated information of each possible meaning of the word (word sense), and edges represent the semantic similarity between the expanded sets (nodes). Simultaneously, actual meaning is assigned to each target word using a locate graph centrality measure, which provides the degree of importance between graph nodes. Unlike most existing graph-based word sense disambiguation methods, wherein semantic relations (edges) between nodes are measured at the word level, the proposed method measures graph node semantic relations at the sentence level by expanding the words’ context, which contains all associated information for each possible word sense. Consequently, the proposed method can capture a higher degree of semantic information than existing approaches, thereby increasing semantic connectivity through a graph’s edges. Empirical results on benchmark datasets demonstrate that the proposed method outperforms all compared state-of-the-art graph-based word sense disambiguation approaches reported herein. We also report results obtained by applying the proposed method to a sentiment analysis task. These results demonstrate that the proposed method can determine the overall sentiment orientation of a given textual context.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310095",
    "keywords": [
      "Artificial intelligence",
      "Centrality",
      "Combinatorics",
      "Computer science",
      "Economics",
      "Explicit semantic analysis",
      "Graph",
      "Management",
      "Mathematics",
      "Natural language processing",
      "SemEval",
      "Semantic Web",
      "Semantic computing",
      "Semantic similarity",
      "Semantic technology",
      "Sentence",
      "Task (project management)",
      "Text graph",
      "Text mining",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Abdalgader",
        "given_name": "Khaled"
      },
      {
        "surname": "Al Shibli",
        "given_name": "Aysha"
      }
    ]
  },
  {
    "title": "Evaluation of feature selection methods based on artificial neural network weights",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114312",
    "abstract": "Weight-based feature selection (WBFS) are methods used to measure the contribution of input to output in a trained artificial neural network (ANN). Furthermore, algorithms such as Garson’s rely upon a single best neural network model or the mean importance value of several ANNs. However, different initialization weights lead to different importance values, as reported in other studies. These differences are misleading since each rank could result in different scores, altering the position of a variable in a given rank. Therefore, we propose a new methodology to assess the stability of a WBFS method. In essence, the idea is to use a voting approach to evaluate the importance of rankings. The results showed that Garson’s, Olden’s and Yoon’s algorithms are more stable methods when applied to artificial datasets. Nevertheless, its stability is considerably reduced when applied to real-world datasets. Hence, we concluded that future work should take into consideration the aforementioned instability of existing WBFS methods as applied to complex real-world data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310083",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Feature selection",
      "Initialization",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Rank (graph theory)",
      "Selection (genetic algorithm)",
      "Stability (learning theory)"
    ],
    "authors": [
      {
        "surname": "Luíza da Costa",
        "given_name": "Nattane"
      },
      {
        "surname": "Dias de Lima",
        "given_name": "Márcio"
      },
      {
        "surname": "Barbosa",
        "given_name": "Rommel"
      }
    ]
  },
  {
    "title": "A comparison of classification methods across different data complexity scenarios and datasets",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114217",
    "abstract": "Recent research assessed the performance of classification methods mainly on concrete datasets whose statistical characteristics are unknown or unreported. The performance furthermore is often determined by only one performance measure, such as the area under the receiver operating characteristic curve. The performance of several classification methods in four different complexity scenarios and on datasets described by five data characteristics is compared in this paper. Synthetical datasets are used to control their statistical characteristics and real datasets are used to verify our findings. The performance of each classification method is determined by six measures. The investigation reveals that heterogeneous classifiers perform best on average, bagged CART is especially recommendable for datasets with low dimensionality and high sample size, kernel-based classification methods perform very well especially with a polynomial kernel, but require a rather long time for training and a nearest shrunken neighbor classifier is recommendable in case of unbalanced datasets. These findings help researchers and practitioners finding an appropriate method for their binary classification problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309428",
    "keywords": [
      "Artificial intelligence",
      "Binary classification",
      "Classifier (UML)",
      "Combinatorics",
      "Computer science",
      "Curse of dimensionality",
      "Data mining",
      "Kernel (algebra)",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Receiver operating characteristic",
      "Support vector machine",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Scholz",
        "given_name": "Michael"
      },
      {
        "surname": "Wimmer",
        "given_name": "Tristan"
      }
    ]
  },
  {
    "title": "The parallelization of a two-phase distributed hybrid ruin-and-recreate genetic algorithm for solving multi-objective vehicle routing problem with time windows",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114408",
    "abstract": "Solving multi-objective vehicle routing problem with time windows (MOVRPTW) involves satisfying two or more objectives. However, not many algorithms can achieve a broader set of Pareto optimal front and consistent solutions that have the least difference in magnitude. Therefore, it is rewarding to develop a coveted algorithm that solves these shortcomings. In this paper, we propose the parallelization of a two-phase distributed hybrid ruin-and-recreate genetic algorithm (HRRGA). The algorithms in HRRGA run in either the HRRGA and the hybrid ruin-and-recreate (HRR) phase or the HRR phase. In HRRGA phase, different HRR strategies are used with the hybrid genetic algorithm (HGA) in the near-optimal solution computation. However, in the HRR phase, only the HRR strategies are used. The algorithms in HRRGA are executed in parallel and harness its power of exploitation and exploration. These strategy combinations improve the diversity of the Pareto optimal front while delivering the generated solutions with the least difference in magnitude. Our experiment with Solomon’s benchmark set shows that HRRGA has superior results compared to the recently published hybrid algorithm, has diverse set of Pareto optimal fronts, and achieves several novel Pareto optimal fronts compared to the best-known solutions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310770",
    "keywords": [
      "Algorithm",
      "Benchmark (surveying)",
      "Chemistry",
      "Computation",
      "Computer network",
      "Computer science",
      "Genetic algorithm",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Multi-objective optimization",
      "Organic chemistry",
      "Parallel computing",
      "Pareto principle",
      "Phase (matter)",
      "Programming language",
      "Routing (electronic design automation)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Khoo",
        "given_name": "Thau Soon"
      },
      {
        "surname": "Mohammad",
        "given_name": "Babrdel Bonab"
      }
    ]
  },
  {
    "title": "A dynamic framework for tuning SVM hyper parameters based on Moth-Flame Optimization and knowledge-based-search",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114139",
    "abstract": "In the real world, most of the collections of data are dynamic in nature, i.e. their size may grow with time. This dynamic nature of the data not only reduces the performance of the classifiers but also demands more optimized models for retaining the performance. Due to this, machine learning models developed in a static environment cannot be deployed efficiently to solve the real-world problems. Nowadays, maximum existing works consider only the static behaviour of the data for the training of machine learning models where the size of the collection of training data does not change over time. This paperwork imposes Support Vector Machine (SVM) in a dynamic environment. It has been identified that shifting of the optimum values of two hyper-parameters C (Penalty Parameter) and γ (Kernel Parameter) in the search space is one of the primary reasons for the performance degradation of SVM in dynamic environment. This paper proposes a novel framework that uses a new optimization module Knowledge-Based-Search (KBS) along with Moth –Flame Optimization (MFO) to optimize C and γ in a dynamic environment to train SVM efficiently. KBS uses knowledge gathered at various instances of time, which are the bi-products of MFO. MFO in our framework is the base optimization algorithm which works underneath KBS. The experiments have shown that KBS helps in controlling the exponential growth of the time complexity of the optimization process where only MFO is used to optimize C and γ . Integration of KBS with MFO brings down the time complexity to a large extent. To validate the proposed framework we have used a simulated dynamic environment for profit/loss classification problem for organizations. The experiments have also shown that KBS's integration with MFO outperforms integration of KBS with other modern optimization techniques such as Particle Swarm Optimization (PSO), Multi-Verse Optimization (MVO), Grey-Wolf Optimization (GWO), Cuckoo Search (CS), Whale Optimization Algorithm (WOA), Genetic Algorithm (GA), Fire-Fly Algorithm (FFA) and Salp Swarm Algorithm (SSA).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308861",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Kernel (algebra)",
      "Knowledge base",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Process (computing)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Kalita",
        "given_name": "Dhruba Jyoti"
      },
      {
        "surname": "Singh",
        "given_name": "Vibhav Prakash"
      },
      {
        "surname": "Kumar",
        "given_name": "Vinay"
      }
    ]
  },
  {
    "title": "DBIG-US: A two-stage under-sampling algorithm to face the class imbalance problem",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114301",
    "abstract": "The class imbalance problem occurs when one class far outnumbers the other classes, causing most traditional classifiers perform poorly on the minority classes. To tackle this problem, a plethora of techniques have been proposed, especially centered around resampling methods. This paper introduces a two-stage method that combines the DBSCAN clustering algorithm to filter noisy majority class instances with a graph-based procedure to overcome the class imbalance. We then experimentally evaluate the behavior of the proposed method on a collection of two-class imbalanced data sets. The experimental results show an improvement in the classification performance measured by the geometric mean of the accuracy on each class and also a higher reduction in the imbalance ratio when compared to several state-of-the-art under-sampling techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310009",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Class (philosophy)",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "DBSCAN",
      "Data mining",
      "Face (sociological concept)",
      "Geometry",
      "Graph",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Reduction (mathematics)",
      "Resampling",
      "Social science",
      "Sociology",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Guzmán-Ponce",
        "given_name": "A."
      },
      {
        "surname": "Sánchez",
        "given_name": "J.S."
      },
      {
        "surname": "Valdovinos",
        "given_name": "R.M."
      },
      {
        "surname": "Marcial-Romero",
        "given_name": "J.R."
      }
    ]
  },
  {
    "title": "Fully Statistical, Wavelet-based conditional random field (FSWCRF) for SAR image segmentation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114370",
    "abstract": "Recently, the conditional random field (CRF) model has been greatly considered in synthetic aperture radar (SAR) image segmentation. This model not only directly considers the posterior distribution of the label field conditioned on images but also gives the interactions between the observations. In this paper, we propose a new CRF-based algorithm for SAR image segmentation. We consider the statistical approach jointly in feature extraction and similarity measurement in the proposed conditional random field model. Using the benefit of the 2-D wavelet transform, we define the generalized Gaussian distribution (GGD) on the wavelet coefficients to extract texture-based features. Then, to improve the CRF potential functions a new unary function is proposed which exactly matches the statistical properties of the wavelet coefficients and produces more accurate parameters for different regions. As the advantage of this function, it is no longer necessary to apply the multinomial logistic regression (MLR) model used in previous CRFs. Moreover, using the Kullback–Leibler distance (KLD) between distribution functions, the similarity measure in our pairwise potential is proposed very effectively and efficiently. The superiority of this scheme is that the similarity measure can be entirely computed using the parameters of the GGD that are typically of small size compared with the feature vectors in the previous methods. Comprehensive experiments on both synthetic and real SAR images indicate that our proposed algorithm achieves accuracy improvement in SAR image segmentation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310484",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Conditional random field",
      "Feature (linguistics)",
      "Image segmentation",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Segmentation",
      "Similarity measure",
      "Synthetic aperture radar",
      "Wavelet"
    ],
    "authors": [
      {
        "surname": "Golpardaz",
        "given_name": "Maryam"
      },
      {
        "surname": "Helfroush",
        "given_name": "Mohammad Sadegh"
      },
      {
        "surname": "Danyali",
        "given_name": "Habibollah"
      },
      {
        "surname": "Ghaffari",
        "given_name": "Reyhane"
      }
    ]
  },
  {
    "title": "Tracing knowledge diffusion of TOPSIS: A historical perspective from citation network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114238",
    "abstract": "A citation network technology named main path analysis is used in this study, which provides a historical perspective of Technique for Order Preference by Similarity to an Ideal Solution (TOPSIS). The citations between related papers are regarded as edges in social network and the corresponding weights are allocated according to their role in knowledge diffusion. Several different main paths are implemented in this work to investigate the knowledge structure of TOPSIS. The Louvain clustering algorithm is conjoined with main path analysis to present the development trend in several scientific communities. The visualization of multiple main paths shows the overall knowledge structure instead of a single development trajectory. This is the first article to use this unique method to process such large-scale data in the TOPSIS domain, which provides an insightful view of TOPSIS publications for future research.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030957X",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Complex network",
      "Computer science",
      "Data mining",
      "Data science",
      "Ideal solution",
      "Image (mathematics)",
      "Information retrieval",
      "Mathematics",
      "Operating system",
      "Operations research",
      "Perspective (graphical)",
      "Physics",
      "Preference",
      "Process (computing)",
      "Similarity (geometry)",
      "Social media",
      "Social network analysis",
      "Statistics",
      "TOPSIS",
      "Thermodynamics",
      "Tracing",
      "Visualization",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Dejian"
      },
      {
        "surname": "Pan",
        "given_name": "Tianxing"
      }
    ]
  },
  {
    "title": "The parallelization of a two-phase distributed hybrid ruin-and-recreate genetic algorithm for solving multi-objective vehicle routing problem with time windows",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114408",
    "abstract": "Solving multi-objective vehicle routing problem with time windows (MOVRPTW) involves satisfying two or more objectives. However, not many algorithms can achieve a broader set of Pareto optimal front and consistent solutions that have the least difference in magnitude. Therefore, it is rewarding to develop a coveted algorithm that solves these shortcomings. In this paper, we propose the parallelization of a two-phase distributed hybrid ruin-and-recreate genetic algorithm (HRRGA). The algorithms in HRRGA run in either the HRRGA and the hybrid ruin-and-recreate (HRR) phase or the HRR phase. In HRRGA phase, different HRR strategies are used with the hybrid genetic algorithm (HGA) in the near-optimal solution computation. However, in the HRR phase, only the HRR strategies are used. The algorithms in HRRGA are executed in parallel and harness its power of exploitation and exploration. These strategy combinations improve the diversity of the Pareto optimal front while delivering the generated solutions with the least difference in magnitude. Our experiment with Solomon’s benchmark set shows that HRRGA has superior results compared to the recently published hybrid algorithm, has diverse set of Pareto optimal fronts, and achieves several novel Pareto optimal fronts compared to the best-known solutions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310770",
    "keywords": [
      "Algorithm",
      "Benchmark (surveying)",
      "Chemistry",
      "Computation",
      "Computer network",
      "Computer science",
      "Genetic algorithm",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Multi-objective optimization",
      "Organic chemistry",
      "Parallel computing",
      "Pareto principle",
      "Phase (matter)",
      "Programming language",
      "Routing (electronic design automation)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Khoo",
        "given_name": "Thau Soon"
      },
      {
        "surname": "Mohammad",
        "given_name": "Babrdel Bonab"
      }
    ]
  },
  {
    "title": "A dynamic framework for tuning SVM hyper parameters based on Moth-Flame Optimization and knowledge-based-search",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114139",
    "abstract": "In the real world, most of the collections of data are dynamic in nature, i.e. their size may grow with time. This dynamic nature of the data not only reduces the performance of the classifiers but also demands more optimized models for retaining the performance. Due to this, machine learning models developed in a static environment cannot be deployed efficiently to solve the real-world problems. Nowadays, maximum existing works consider only the static behaviour of the data for the training of machine learning models where the size of the collection of training data does not change over time. This paperwork imposes Support Vector Machine (SVM) in a dynamic environment. It has been identified that shifting of the optimum values of two hyper-parameters C (Penalty Parameter) and γ (Kernel Parameter) in the search space is one of the primary reasons for the performance degradation of SVM in dynamic environment. This paper proposes a novel framework that uses a new optimization module Knowledge-Based-Search (KBS) along with Moth –Flame Optimization (MFO) to optimize C and γ in a dynamic environment to train SVM efficiently. KBS uses knowledge gathered at various instances of time, which are the bi-products of MFO. MFO in our framework is the base optimization algorithm which works underneath KBS. The experiments have shown that KBS helps in controlling the exponential growth of the time complexity of the optimization process where only MFO is used to optimize C and γ . Integration of KBS with MFO brings down the time complexity to a large extent. To validate the proposed framework we have used a simulated dynamic environment for profit/loss classification problem for organizations. The experiments have also shown that KBS's integration with MFO outperforms integration of KBS with other modern optimization techniques such as Particle Swarm Optimization (PSO), Multi-Verse Optimization (MVO), Grey-Wolf Optimization (GWO), Cuckoo Search (CS), Whale Optimization Algorithm (WOA), Genetic Algorithm (GA), Fire-Fly Algorithm (FFA) and Salp Swarm Algorithm (SSA).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308861",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Kernel (algebra)",
      "Knowledge base",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Process (computing)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Kalita",
        "given_name": "Dhruba Jyoti"
      },
      {
        "surname": "Singh",
        "given_name": "Vibhav Prakash"
      },
      {
        "surname": "Kumar",
        "given_name": "Vinay"
      }
    ]
  },
  {
    "title": "Chaos-assisted multi-population salp swarm algorithms: Framework and case studies",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114369",
    "abstract": "Salp swarm algorithm (SSA) is a recently presented algorithm, which is simple in structure and relatively mediocre in its performance. However, the original SSA still has features to be improved because it may face problems in convergence trends or easily being trapped into local optima for more advanced problems. To alleviate this limitation, we propose a new SSA-based method (MCSSA) that performs the chaotic exploitative trends and has a multi-population structure. The new structure can assist SSA in making a more stable tradeoff between global exploration and local exploitation capabilities. First, the exploitation trends and neighborhood searching commands of SSA are enriched using the chaos-assisted exploitation strategy. Next, we arrange a multi-population structure with three sub-strategies to augment the global exploration capabilities of the algorithm. To test the performance of this proposed MCSSA, a set of comprehensive algorithms is used, including 11 other original methods, conventional SSA, and 13 advanced techniques including SCA, SSA, GWO, MFO, WOA, BA, FPA, PSO, ALO, MVO, DE, ABC, CSSA, ESSA, CLSGMFO, LGCMFO, SaDE, jDE, EPSO, ALCPSO, CBA, RCBA, BWOA, CCMWOA, and GA-MPC based on 30 IEEE CEC2017 benchmark functions and 5 IEEE CEC2011 practical test problems. Also, the non-parametric statistics Wilcoxon signed-rank test and Friedman test are also used as an enabling tool to validate the performance of the proposed algorithm. From the result analysis, it can be concluded that the introduced strategy significantly improves the speed of the algorithm converging to the optimal value, and the improvement of the search ability also helps the algorithm to find a better solution than the basic SSA. As a conclusion, it can be said that MCSSA is reliable and efficient in solving complex optimization problems. An online website at https://aliasgharheidari.com supports this research for any guide or info.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310472",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Chaotic",
      "Computer science",
      "Convergence (economics)",
      "Demography",
      "Economic growth",
      "Economics",
      "Friedman test",
      "Geodesy",
      "Geography",
      "Local optimum",
      "Mann–Whitney U test",
      "Mathematical optimization",
      "Mathematics",
      "Parametric statistics",
      "Particle swarm optimization",
      "Population",
      "Sociology",
      "Statistical hypothesis testing",
      "Statistics",
      "Wilcoxon signed-rank test"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yun"
      },
      {
        "surname": "Shi",
        "given_name": "Yanqing"
      },
      {
        "surname": "Chen",
        "given_name": "Hao"
      },
      {
        "surname": "Heidari",
        "given_name": "Ali Asghar"
      },
      {
        "surname": "Gui",
        "given_name": "Wenyong"
      },
      {
        "surname": "Wang",
        "given_name": "Mingjing"
      },
      {
        "surname": "Chen",
        "given_name": "Huiling"
      },
      {
        "surname": "Li",
        "given_name": "Chengye"
      }
    ]
  },
  {
    "title": "An integrated modeling method for collaborative vehicle routing: Facilitating the unmanned micro warehouse pattern in new retail",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114307",
    "abstract": "This study focuses on horizontal collaboration and addresses a collaborative multi-center vehicle routing problem (CMCVRP) deriving from the unmanned micro warehouse pattern in new retail in China. In the CMCVRP, suppliers form a coalition to reduce their operating costs and improve service levels. Traditional approaches cannot coordinate the interests of all decision subjects in this process. To fill this gap, an integrated modeling method is proposed considering the objectives at both coalition and partner levels. In the model, two constraints are presented to guarantee the interests of all decision subjects. The constraint at the coalition level defines the acceptable region of solutions to ensure the efficiency of the coalition. And the constraint at the partner level limits the difference among partners' benefits to avoid excessive profit imbalance in the alliance. After that, a metaheuristic algorithm called the non-dominated sorting genetic algorithm-large neighborhood search is proposed to solve the model. Numerical experiments are conducted on thirty instances with three kinds of logistics networks, and the integrated method is compared with an existing approach (centralized method). The results show that the integrated method has a better performance to coordinate the benefits of all decision-makers when forming the coalition than the centralized method. Also, the impact of the network structures in the collaboration on the performance of the integrated method is investigated, and the conclusions can provide guidance for suppliers to choose their cooperators.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310058",
    "keywords": [
      "Alliance",
      "Business",
      "Computer network",
      "Computer science",
      "Constraint (computer-aided design)",
      "Economics",
      "Engineering",
      "Genetic algorithm",
      "Law",
      "Machine learning",
      "Marketing",
      "Mathematical optimization",
      "Mathematics",
      "Mechanical engineering",
      "Microeconomics",
      "Operating system",
      "Operations research",
      "Political science",
      "Process (computing)",
      "Process management",
      "Profit (economics)",
      "Programming language",
      "Routing (electronic design automation)",
      "Service (business)",
      "Service level",
      "Sorting",
      "Vehicle routing problem"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xuping"
      },
      {
        "surname": "Lin",
        "given_name": "Na"
      },
      {
        "surname": "Li",
        "given_name": "Ya"
      },
      {
        "surname": "Shi",
        "given_name": "Yan"
      },
      {
        "surname": "Ruan",
        "given_name": "Junhu"
      }
    ]
  },
  {
    "title": "Resolving cross-site scripting attacks through genetic algorithm and reinforcement learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114386",
    "abstract": "Cross Site Scripting (XSS) is one of the most frequently occurring vulnerability. The impact of XSS can vary from cosmetic to catastrophic damages. However, detection of XSS efficiently is still an open issue. Cross site scripting has been dealt with static and dynamic analysis previously. Both techniques have shortcomings and fail due to frequent variations in XSS payloads. Therefore, in this paper, we have proposed the use of Genetic Algorithm (GA) along with Reinforcement Learning (RL) and threat intelligence to overcome XSS attacks. For validation, the proposed approach is applied on a real dataset of XSS attacks. Results show better performance of our proposed approach when compared to the approaches reported in the literature. In addition to better performance, our method is not only flexible to changes in XSS payloads, but the results are also more understandable to end users. Moreover, our approach shows improvement when the number of attacks is increased.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310599",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Cross-site scripting",
      "Genetic algorithm",
      "Machine learning",
      "Programming language",
      "Reinforcement learning",
      "Scripting language",
      "The Internet",
      "Vulnerability (computing)",
      "Web application security",
      "Web development",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Tariq",
        "given_name": "Iram"
      },
      {
        "surname": "Sindhu",
        "given_name": "Muddassar Azam"
      },
      {
        "surname": "Abbasi",
        "given_name": "Rabeeh Ayaz"
      },
      {
        "surname": "Khattak",
        "given_name": "Akmal Saeed"
      },
      {
        "surname": "Maqbool",
        "given_name": "Onaiza"
      },
      {
        "surname": "Siddiqui",
        "given_name": "Ghazanfar Farooq"
      }
    ]
  },
  {
    "title": "Two-sided matching decision making with multi-granular hesitant fuzzy linguistic term sets and incomplete criteria weight information",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114311",
    "abstract": "Two-sided matching decision making (TSMDM) problems exist widely in human being’s daily life. For practical TSMDM problems, matching objects with different culture and knowledge backgrounds usually tend to provide linguistic assessments using different linguistic term sets (i.e., multi-granular linguistic information). Moreover, for TSMDM problems with high uncertainty, it is possible that matching objects may have some hesitancy and thus provide hesitant fuzzy linguistic term sets (HFLTSs). To model these situations, an approach to TSMDM with multi-granular HFLTSs is developed in the paper. In the proposed approach, some optimization models are first constructed to determine criteria weights for matching objects who do not provide clear criteria weight vectors. Afterwards, each matching object’s hesitant fuzzy linguistic decision matrix is aggregated to obtain his/her collective assessments over matching objects on the other side, which are denoted by multi-granular linguistic distribution assessments. These multi-granular linguistic distribution assessments are unified to obtain matching objects’ satisfaction degrees. Furthermore, an optimization model which aims to maximize the overall satisfaction degree of matching objects by considering the stable matching condition is then established and solved to determine the matching between matching objects. Eventually, an example for the matching of green building technology supply and demand is provided to demonstrate the characteristics of the proposed approach. Compared with previous studies, the proposed approach allows matching objects to provide linguistic assessments flexibly and can deal with the situations when incomplete criteria weight information is provided.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310071",
    "keywords": [
      "Artificial intelligence",
      "Composite material",
      "Computer science",
      "Fuzzy logic",
      "Granular computing",
      "Matching (statistics)",
      "Materials science",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Object (grammar)",
      "Optimal matching",
      "Physics",
      "Quantum mechanics",
      "Rough set",
      "Statistics",
      "Term (time)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zhen"
      },
      {
        "surname": "Gao",
        "given_name": "Junliang"
      },
      {
        "surname": "Gao",
        "given_name": "Yuan"
      },
      {
        "surname": "Yu",
        "given_name": "Wenyu"
      }
    ]
  },
  {
    "title": "Sequence-based dynamic handwriting analysis for Parkinson’s disease detection with one-dimensional convolutions and BiGRUs",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114405",
    "abstract": "Parkinson’s disease (PD) is commonly characterized by several motor symptoms, such as bradykinesia, akinesia, rigidity, and tremor. The analysis of patients’ fine motor control, particularly handwriting, is a powerful tool to support PD assessment. Over the years, various dynamic attributes of handwriting, such as pen pressure, stroke speed, in-air time, etc., which can be captured with the help of online handwriting acquisition tools, have been evaluated for the identification of PD. Motion events, and their associated spatio-temporal properties captured in online handwriting, enable effective classification of PD patients through the identification of unique sequential patterns. This paper proposes a novel classification model based on one-dimensional convolutions and Bidirectional Gated Recurrent Units (BiGRUs) to assess the potential of sequential information of handwriting in identifying Parkinsonian symptoms. One-dimensional convolutions are applied to raw sequences as well as derived features; the resulting sequences are then fed to BiGRU layers to achieve the final classification. The proposed method outperformed state-of-the-art approaches on the PaHaW dataset and achieved competitive results on the NewHandPD dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310757",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Handwriting",
      "Identification (biology)",
      "Pattern recognition (psychology)",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Diaz",
        "given_name": "Moises"
      },
      {
        "surname": "Moetesum",
        "given_name": "Momina"
      },
      {
        "surname": "Siddiqi",
        "given_name": "Imran"
      },
      {
        "surname": "Vessio",
        "given_name": "Gennaro"
      }
    ]
  },
  {
    "title": "Fully Statistical, Wavelet-based conditional random field (FSWCRF) for SAR image segmentation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114370",
    "abstract": "Recently, the conditional random field (CRF) model has been greatly considered in synthetic aperture radar (SAR) image segmentation. This model not only directly considers the posterior distribution of the label field conditioned on images but also gives the interactions between the observations. In this paper, we propose a new CRF-based algorithm for SAR image segmentation. We consider the statistical approach jointly in feature extraction and similarity measurement in the proposed conditional random field model. Using the benefit of the 2-D wavelet transform, we define the generalized Gaussian distribution (GGD) on the wavelet coefficients to extract texture-based features. Then, to improve the CRF potential functions a new unary function is proposed which exactly matches the statistical properties of the wavelet coefficients and produces more accurate parameters for different regions. As the advantage of this function, it is no longer necessary to apply the multinomial logistic regression (MLR) model used in previous CRFs. Moreover, using the Kullback–Leibler distance (KLD) between distribution functions, the similarity measure in our pairwise potential is proposed very effectively and efficiently. The superiority of this scheme is that the similarity measure can be entirely computed using the parameters of the GGD that are typically of small size compared with the feature vectors in the previous methods. Comprehensive experiments on both synthetic and real SAR images indicate that our proposed algorithm achieves accuracy improvement in SAR image segmentation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310484",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Conditional random field",
      "Feature (linguistics)",
      "Image segmentation",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Segmentation",
      "Similarity measure",
      "Synthetic aperture radar",
      "Wavelet"
    ],
    "authors": [
      {
        "surname": "Golpardaz",
        "given_name": "Maryam"
      },
      {
        "surname": "Helfroush",
        "given_name": "Mohammad Sadegh"
      },
      {
        "surname": "Danyali",
        "given_name": "Habibollah"
      },
      {
        "surname": "Ghaffari",
        "given_name": "Reyhane"
      }
    ]
  },
  {
    "title": "Characteristics of viral messages on Telegram; The world’s largest hybrid public and private messenger",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114303",
    "abstract": "Telegram is a new Instant Messaging application providing key features for both public and private messaging. Telegram is similar to group broadcast or micro-blogging platforms, while on the other hand, it has features of ordinary Instant Messaging applications such as WhatsApp. In this paper, investigating a real dataset crawled from Telegram, we provide several observations which can explain the information flow, business model of content providers, and social sensing aspects of Telegram. The crawled dataset which is manually labeled by six persons contains two months of public messages of selected Telegram channels. Moreover, we introduce the viral messages in instant messaging services and propose formal definition of these messages as well as deeply analyzing their characteristics and features. Detection of virality characteristics of messages in Telegram can be beneficial for both end-users and digital marketers. Consequently, we propose statistical and word embedding approaches to detect viral messages and their sentiment and message category.Our experiments indicate that the word embedding approach can significantly outperform other baseline models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310010",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Embedding",
      "Instant messaging",
      "Key (lock)",
      "Linguistics",
      "Philosophy",
      "Word (group theory)",
      "Word embedding",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Dargahi Nobari",
        "given_name": "Arash"
      },
      {
        "surname": "Sarraf",
        "given_name": "Malikeh Haj Khan Mirzaye"
      },
      {
        "surname": "Neshati",
        "given_name": "Mahmood"
      },
      {
        "surname": "Erfanian Daneshvar",
        "given_name": "Farnaz"
      }
    ]
  },
  {
    "title": "WASPAS-based decision making methodology with unknown weight information under uncertain evaluations",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114143",
    "abstract": "The uncertain probabilistic linguistic term set (UPLTS) one of the modern development in fuzzy set theory, can express not only the decision makers (DMs) linguistic assessment information but also the uncertain probability/weight/importance degree of each linguistic assessment value, so it is an efficient tool for addressing the ignorance problems. The current study mainly focuses on developing a more effective way to cope with multiple criteria group decision making (MCGDM) problems in which the assessment information are in the form of UPLTSs, and the weight information is also entirely unknown. Firstly, some weaknesses of the existing operational laws and score function of UPLTSs are pointed out through some critical examples and then redefined them to overcome existing flaws in order to acquire more accurate results in practical decision making problems. Also, we establish various properties of the revised operational laws along with proofs. To design a novel comparison method, the concept of deviation degree is introduced in order to accommodate the situation in which two different UPLTSs have the same score values. After that, based on the proposed operational laws, several existing aggregation operators are modified, and a novel aggregation operator, namely uncertain probabilistic linguistic simple weighted geometry (UPLSWG) operator is designed. Meanwhile, some interesting properties of these proposed operators are carefully analysed. Furthermore, an entropy technique under uncertain probabilistic linguistic information is structured for computing the completely unknown weights of criteria. Following this, a new extension of weighted aggregated sum product assessment (WASPAS) method called uncertain probabilistic linguistic-WASPAS (UPL-WASPAS) methodology based on the proposed aggregation operators is studied under the UPLTS context for ranking objects in MCGDM problems. To show the applicability and potentiality of the developed method, an example of supplier selection is addressed, and a detailed performance comparison analysis is conducted. Furthermore, sensitivity analysis is also made to determine the impact of the parameter on the ranking of alternatives.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308903",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Entropy (arrow of time)",
      "Fuzzy logic",
      "Fuzzy set",
      "Gene",
      "Geometry",
      "Group decision-making",
      "Ignorance",
      "Law",
      "Machine learning",
      "Mathematical optimization",
      "Mathematical proof",
      "Mathematics",
      "Operations research",
      "Operator (biology)",
      "Physics",
      "Political science",
      "Probabilistic logic",
      "Programming language",
      "Quantum mechanics",
      "Repressor",
      "Score",
      "Set (abstract data type)",
      "Transcription factor"
    ],
    "authors": [
      {
        "surname": "Ali",
        "given_name": "Jawad"
      },
      {
        "surname": "Bashir",
        "given_name": "Zia"
      },
      {
        "surname": "Rashid",
        "given_name": "Tabasam"
      }
    ]
  },
  {
    "title": "Deep hybrid neural-like P systems for multiorgan segmentation in head and neck CT/MR images",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114446",
    "abstract": "Automatic segmentation of organs-at-risk (OARs) of the head and neck, such as the brainstem, the left and right parotid glands, mandible, optic chiasm, and the left and right optic nerves, are crucial when formulating radiotherapy plans. However, there are difficulties due to (1) the small sizes of these organs (especially the optic chiasm and optic nerves) and (2) the different positions and phenotypes of the OARs. In this paper, we propose a novel, automatic multiorgan segmentation algorithm based on a new hybrid neural-like P system, to alleviate the above challenges. The new P system possesses the joint advantages of cell-like and neural-like P systems and includes new structures and rules, allowing it to solve more real-world problems in parallelism. In the new P system, effective ensemble convolutional neural networks (CNNs) are implemented with different initializations simultaneously to perform pixel-wise segmentations of OARs, which can obtain more effective features and leverage the strength of ensemble learning. Evaluations on three public datasets show the effectiveness and robustness of the proposed algorithm for accurate OARs segmentation in various image modalities.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311027",
    "keywords": [
      "Anatomy",
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Gene",
      "Medicine",
      "Optic chiasm",
      "Optic nerve",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Xue",
        "given_name": "Jie"
      },
      {
        "surname": "Wang",
        "given_name": "Yuan"
      },
      {
        "surname": "Kong",
        "given_name": "Deting"
      },
      {
        "surname": "Wu",
        "given_name": "Feiyang"
      },
      {
        "surname": "Yin",
        "given_name": "Anjie"
      },
      {
        "surname": "Qu",
        "given_name": "Jianhua"
      },
      {
        "surname": "Liu",
        "given_name": "Xiyu"
      }
    ]
  },
  {
    "title": "A multi-scale image watermarking based on integer wavelet transform and singular value decomposition",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114272",
    "abstract": "Image watermarking technique is one of effective solutions to protect copyright, and it is applied to a variety of information security application domains. It needs to meet four requirements of imperceptibility, robustness, capacity and security. A multi-scale and secure image watermarking method is proposed in this work, which is based on the Integer Wavelet Transform (IWT) and Singular Value Decomposition (SVD). Four IWT sub-bands are firstly obtained after 1-level IWT on the host image, and the corresponding singular diagonal matrices of four sub-bands can be obtained using SVD. Then, each singular diagonal matrix is divided into four non-overlapping sections in terms of the size of embedding watermark. Particularly, the size of upper left part is same as the size of watermark. The watermark can be directly embedded into four upper left parts afterwards by multiplying different scaling factors to complete the final watermarking operation. Especially, a novel optimized authentication mechanism is designed to resolve the false positive problem, which exists in the SVD-based watermarking algorithms. In addition, three-dimensional optimal mapping algorithm is proposed to search the optimal scaling factors through a novel objective evaluation function, and it can significantly improve the imperceptibility and robustness. The experimental test and comparison analysis illustrate that the proposed watermark scheme demonstrates a high imperceptibility with peak signal to noise ratio values of 45 dB and strong robustness with average normalized correlation values of 0.92.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309829",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Diagonal",
      "Digital watermarking",
      "Eigenvalues and eigenvectors",
      "Embedding",
      "Gene",
      "Geometry",
      "Image (mathematics)",
      "Integer (computer science)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Robustness (evolution)",
      "Scaling",
      "Singular value",
      "Singular value decomposition",
      "Watermark",
      "Wavelet"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Yuling"
      },
      {
        "surname": "Li",
        "given_name": "Liangjia"
      },
      {
        "surname": "Liu",
        "given_name": "Junxiu"
      },
      {
        "surname": "Tang",
        "given_name": "Shunbin"
      },
      {
        "surname": "Cao",
        "given_name": "Lvchen"
      },
      {
        "surname": "Zhang",
        "given_name": "Shunsheng"
      },
      {
        "surname": "Qiu",
        "given_name": "Senhui"
      },
      {
        "surname": "Cao",
        "given_name": "Yi"
      }
    ]
  },
  {
    "title": "A hesitant fuzzy linguistic bi-objective clustering method for large-scale group decision-making",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114355",
    "abstract": "Large-scale group decision-making process has received an increasing attention in recent years. After making the general survey of the existing large-scale group decision-making methods, we have found that: 1) consistency threshold value of hesitant fuzzy linguistic preference relation is fixed in traditional consistency measures; 2) the clustering process of LSGDM does not consider the similar relationship between different evaluation information and the information quality simultaneously. Thus, in order to tackle the above issues and describe the hesitancy of experts in the decision-making process, the paper proposes a hesitant fuzzy linguistic bi-objective clustering method considering consensus and information entropy for tackling large-scale group decision-making problems. Firstly, a selection procedure for preference information is developed to quickly select suitable experts who meet the consistency requirements. Then, a bi-objective clustering method based on the group consensus degree indicator and group information entropy indicator is proposed to divide the experts into different clusters, considering the similar relationship and the quality of evaluation information simultaneously. After that, comprehensive preference information and the overall ranking of alternatives can be obtained. In the end, an illustrative example of choosing the optimal way to protect the personal information while defending against COVID-19 and some comparative study show that the proposed method is valid for large-scale group decision-making problems and has good performance and strong robustness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310381",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Consistency (knowledge bases)",
      "Data mining",
      "Entropy (arrow of time)",
      "Fuzzy clustering",
      "Fuzzy logic",
      "Group decision-making",
      "Machine learning",
      "Physics",
      "Psychology",
      "Quantum mechanics",
      "Ranking (information retrieval)",
      "Social psychology"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Yuanhang"
      },
      {
        "surname": "Xu",
        "given_name": "Zeshui"
      },
      {
        "surname": "He",
        "given_name": "Yue"
      },
      {
        "surname": "Tian",
        "given_name": "Yuhang"
      }
    ]
  },
  {
    "title": "RESI: A Region-Splitting Imputation method for different types of missing data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114425",
    "abstract": "A certain degree of data loss seriously affects the accuracy and availability of data, especially on the effects of the subsequent in-depth data analysis and mining. It is of great value in practical applications to construct a data imputation model, which is suitable for completing different types of missing data, including numerical only, categorical only and mixed-type data, and has strong capability of generalization. To address this issue, this paper defines a new metric, mean integrity rate, to measure the missing degree of a dataset, and proposes RESI , a novel tuple-based RE gion- S plitting I mputation model, to impute different type missing data. We first select features and assign weights to each attribute by using the entropy weight method, and then partition the tuples into a subset of complete tuples and several subsets of incomplete tuples based on their integrity rate, which is formulated with the weights of attributes and the missing degree of tuples. The model performs training iterations on the complete tuple subset. In each iteration, the trained model is used to impute the next missing subset, and the computed subset is merged into the complete subset for training the next model. To improve the imputation accuracy, we leverage k -fold cross validation to correct errors. Besides imputing diverse types of missing data, extensive experimental results have shown that our model, RESI, significantly outperforms the state-of-the-art methods in the sensitivity to missing rate and accuracy of imputed data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310903",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Imputation (statistics)",
      "Machine learning",
      "Missing data"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Dunlu"
      },
      {
        "surname": "Zou",
        "given_name": "Mengping"
      },
      {
        "surname": "Liu",
        "given_name": "Cong"
      },
      {
        "surname": "Lu",
        "given_name": "Jing"
      }
    ]
  },
  {
    "title": "Multidimensional KNN algorithm based on EEMD and complexity measures in financial time series forecasting",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114443",
    "abstract": "Stock time series forecasting is a universal purpose of academic researchers, even a slight improvement in the accuracy of the forecast may have a fabulous impact on participants’ trading decisions. An appropriate forecasting method will help investors to obtain a reasonable portfolio, to some extent avoid potential financial risks, which makes our research more profound. To provide stock market participants and researchers with better prediction results, in this paper, a modified modeling procedure is proposed which aims to heighten the function of the k-nearest neighbors (KNN) method in financial time series prediction. The key idea is to combine Ensemble empirical mode decomposition (EEMD) method with Multidimensional k-nearest neighbors-time series prediction with invariance (MKNN–TSPI). This new time series forecasting framework is called EEMD–MKNN–TSPI. Take note of the facilitation of the experiment, we present an example of two-dimensional stock series, the modified method is utilized to forecast the opening and closing prices of NAS, DJI, S&P 500, Russell 2000 and other stocks simultaneously. Besides, we compare our method with EEMD–MKNN and MKNN–TSPI, the experimental results indicate that the proposed model outperforms EEMD–MKNN model and MKNN–TSPI model. At the end of the paper, we select some stock sequences and traffic sequences to conduct a series of experiments, the purpose is to explore the impact of data types and complexity on the prediction results about the EEMD–MKNN–TSPI method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311015",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Data mining",
      "Economics",
      "Filter (signal processing)",
      "Finance",
      "Hilbert–Huang transform",
      "Horse",
      "Key (lock)",
      "Machine learning",
      "Paleontology",
      "Series (stratigraphy)",
      "Stock market",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Guancen"
      },
      {
        "surname": "Lin",
        "given_name": "Aijing"
      },
      {
        "surname": "Cao",
        "given_name": "Jianing"
      }
    ]
  },
  {
    "title": "Unsupervised feature selection for attributed graphs",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114402",
    "abstract": "Many real-world applications generate attributed graphs that contain both link structures and content information associated with nodes. Content information in real networks always contains high dimensional feature space. In recent years, unsupervised feature selection has been widely used in handling high dimensional data without label information. Most existing unsupervised feature selection methods assume that instances in datasets are independent and identically distributed. However, instances in attributed graphs are intrinsically correlated. Considering the wide applications of feature selection in attributed graphs, we propose a new unsupervised feature selection method based on regularized sparse learning. We use pseudo class labels to learn the interdependency from both link and content information, and embed the obtained information into a sparse learning based feature selection framework. In particular, a new regularization term is designed to learn link information, which capture group behavior among the connected instances utilizing latent social dimensions. To solve the proposed feature selection model, we consider both convex and nonconvex cases and design the corresponding algorithms based on the Alternating Direction Method of Multipliers (ADMM) combined with ConCave Convex Procedure (CCCP). Numerical studies are implemented on real-world datasets to validate the advantage of our new method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310733",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Feature learning",
      "Feature selection",
      "Feature vector",
      "Geometry",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Regular polygon",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Ruizhi"
      },
      {
        "surname": "Niu",
        "given_name": "Lingfeng"
      },
      {
        "surname": "Yang",
        "given_name": "Hong"
      }
    ]
  },
  {
    "title": "A differential evolution algorithm for estimating mobile channel parameters α − η − μ",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114357",
    "abstract": "The statistical modeling of mobile radio signals requires the estimation of parameters that describe the probability distribution that hypothetically models this channel, so that this probabilistic model guarantees a good adjustment to the experimental data. This article proposes the use of differential evolution (DE) algorithms for estimating parameters of the α − η − μ fading channel, and to compare these to the traditional method of moments (MM) and maximum likelihood estimation (MLE) method. These traditional parameter estimation methods use nonlinear numerical methods, and the solution, if found, may be the optimal value, an approximation of the optimal value, or a local maximum. The authors demonstrate through comparative experiments using the MM and the MLE method that the DE algorithm for the proposed estimation demands a lower run time. In addition, it presents the error performance measured by the mean square error (MSE), near or above, as well as high robustness measured by the statistical analysis. Essentially, this algorithm always finds acceptable physical estimations with a good goodness of fit to experimental data. This estimating DE algorithm along with its proposed fitness function are original contributions of this paper. The received signal samples, used in the experiments of this paper, were randomly generated by the α − η − μ fading simulator, which is another contribution of this paper. This proposed α − η − μ fading simulator is based on the Clarke and Gans fading model and expands the generation range of current simulators, from μ integer multiples of 0.5, to μ integer multiples of 0.25.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310393",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Decoding methods",
      "Estimation theory",
      "Fading",
      "Goodness of fit",
      "Mathematics",
      "Mean squared error",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Lemos",
        "given_name": "Carlos Paula"
      },
      {
        "surname": "Veiga",
        "given_name": "Antônio Cláudio Paschoarelli"
      },
      {
        "surname": "Fasolo",
        "given_name": "Sandro Adriano"
      }
    ]
  },
  {
    "title": "Probabilistic physics-guided machine learning for fatigue data analysis",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114316",
    "abstract": "A Probabilistic Physics-guided Neural Network (PPgNN) is proposed in this paper for probabilistic fatigue S-N curve estimation. The proposed model overcomes the limitations in existing parametric regression models and classical machine learning models for fatigue data analysis. Compared with explicit regression-type models (such as power law fitting), the PPgNN is flexible and does not impose restrictions on function types at different stress levels, mean stresses, or other factors. One unique benefit is that the proposed method includes the known physics/knowledge constraints in the machine learning model; the method can produce both accurate and physically consistent results compared with the classical machine learning model, such as neural network models. In addition, the PPgNN uses both failure and runout data in the training process, which encodes the runout data using a new proposed loss function, and is beneficial when compared with some existing models using only numerical point value data. A mathematical formulation is derived to include different types of physics constraints, which can deal with mean value, variance, and derivative/curvature constraints. Several data sets from open literature for fatigue S-N curve testing are used for model demonstration and model validation. Next, the proposed network architecture is extended to include multi-factor (e.g., mean stress, corrosion, frequency effect, etc.) fatigue data analysis. It is shown that the proposed PPgNN can serve as a flexible and robust model for general fitting and uncertainty quantification of fatigue data. This paper provides a feasible way to incorporate known physics/knowledge in neural network-based machine learning. This is achieved by properly designing the network topology and constraining the neural network’s biases and weights. The benefits for the proposed physics-guided learning for fatigue data analysis are illustrated by comparing results from neural network models with and without physics guidance. The neural network model, without physics guidance, produces results contradictory to the common knowledge, such as a monotonic decrease of S-N curve slope and a monotonic increase of fatigue life variance as the stress level decreases. This problem can be avoided using the physics-guided learning model with encoded prior physics knowledge.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310113",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data science",
      "Machine learning",
      "Probabilistic logic"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Jie"
      },
      {
        "surname": "Liu",
        "given_name": "Yongming"
      }
    ]
  },
  {
    "title": "The application of spatial domain in optimum initialization for clustering image data using particle swarm optimization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114224",
    "abstract": "Clustering algorithms are affected by the initial seeds, therefore any improvement of the initialization process can improve the final clustering results. There exist several initialization algorithms that most of them are focused on using the distance and density based metrics defined in the feature space. However image space has a great potential to be used as the search space for initial seeds. In this research, developing clustering initialization using spatial information (image space) and spectral information (feature space) with the help of particle swarm optimization has been examined. Standard deviation and homogeneity of pixels in the image space in addition to distance and density of points in the feature space have been utilized in the objective function of the particle swarm optimization. Two different search spaces (feature and image spaces) and 26 objective functions have been applied to a simulated image and two real satellite multi-spectral images. Comparing the results of 26 cases with four prevailing initialization methods, demonstrated that searching for initial seeds in the image space using PSO with a full objective function (using four spectral–spatial criteria) can produce better results than the other tested cases. Using this case for k-means clustering initialization, led to about 20% improvement in overall accuracy relative to the clustering results with commonly used initialization algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309477",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Feature (linguistics)",
      "Feature vector",
      "Initialization",
      "Linguistics",
      "Mathematics",
      "Multi-swarm optimization",
      "Particle swarm optimization",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Dadjoo",
        "given_name": "Mehran"
      },
      {
        "surname": "Fatemi Nasrabadi",
        "given_name": "Sayyed Bagher"
      }
    ]
  },
  {
    "title": "On optimistic, pessimistic and mixed approaches under different membership functions for fully intuitionistic fuzzy multiobjective nonlinear programming problems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114309",
    "abstract": "The concept of measuring the membership degree along with a non-membership degree, gives rise to the intuitionistic fuzzy set theory. The present study formulates a nonlinear programming problem with multiple objectives, including all the parameters and decision variables as intuitionistic fuzzy numbers. The problem is further investigated subject to optimistic, pessimistic and mixed approaches under linear, exponential and hyperbolic membership functions. The article redefines pessimistic and mixed point of view to be in the true spirit compatible with the definition of an intuitionistic fuzzy number. Accuracy function is used to reduce the problem to an equivalent crisp multiobjective nonlinear programming problem and then optimal compromise solution is obtained under different approaches using various membership/non-membership functions. At appropriate places, theorems have also been proved to establish the equivalence between the original formulation and its crisp counterparts under each approach. Further, practical applications in production planning and transportation problem are illustrated to explain the optimistic, pessimistic and mixed approaches using the proposed algorithm and finally a comparison is also drawn.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742031006X",
    "keywords": [
      "Acoustics",
      "Arithmetic",
      "Artificial intelligence",
      "Computer science",
      "Degree (music)",
      "Discrete mathematics",
      "Equivalence (formal languages)",
      "Fuzzy logic",
      "Fuzzy set",
      "Mathematical optimization",
      "Mathematics",
      "Membership function",
      "Nonlinear programming",
      "Nonlinear system",
      "Physics",
      "Quantum mechanics",
      "sort"
    ],
    "authors": [
      {
        "surname": "Mahajan",
        "given_name": "Sumati"
      },
      {
        "surname": "Gupta",
        "given_name": "S.K."
      }
    ]
  },
  {
    "title": "Credibility assessment of financial stock tweets",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114351",
    "abstract": "Social media plays an important role in facilitating conversations and news dissemination. Specifically, Twitter has recently seen use by investors to facilitate discussions surrounding stock exchange-listed companies. Investors depend on timely, credible information being made available in order to make well-informed investment decisions, with credibility being defined as the believability of information. Much work has been done on assessing credibility on Twitter in domains such as politics and natural disaster events, but the work on assessing the credibility of financial statements is scant within the literature. Investments made on apocryphal information could hamper efforts of social media’s aim of providing a transparent arena for sharing news and encouraging discussion of stock market events. This paper presents a novel methodology to assess the credibility of financial stock market tweets, which is evaluated by conducting an experiment using tweets pertaining to companies listed on the London Stock Exchange. Three sets of traditional machine learning classifiers (using three different feature sets) are trained using an annotated dataset. We highlight the importance of considering features specific to the domain in which credibility needs to be assessed for – in the case of this paper, financial features. In total, after discarding non-informative features, 34 general features are combined with over 15 novel financial features for training classifiers. Results show that classifiers trained on both general and financial features can yield improved performance than classifiers trained on general features alone, with Random Forest being the top performer, although the Random Forest model requires more features (37) than that of other classifiers (such as K-Nearest Neighbours − 9) to achieve such performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310356",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Business",
      "Classifier (UML)",
      "Computer science",
      "Credibility",
      "Engineering",
      "Finance",
      "Horse",
      "Law",
      "Machine learning",
      "Mechanical engineering",
      "Paleontology",
      "Political science",
      "Random forest",
      "Social media",
      "Stock (firearms)",
      "Stock market",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Evans",
        "given_name": "Lewis"
      },
      {
        "surname": "Owda",
        "given_name": "Majdi"
      },
      {
        "surname": "Crockett",
        "given_name": "Keeley"
      },
      {
        "surname": "Fernandez Vilas",
        "given_name": "Ana"
      }
    ]
  },
  {
    "title": "A new topic modeling based approach for aspect extraction in aspect based sentiment analysis: SS-LDA",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114231",
    "abstract": "With the widespread use of social networks, blogs, forums and e-commerce web sites, the volume of user generated textual data is growing exponentially. User opinions in product reviews or in other textual data are crucial for manufacturers, retailers and providers of the products and services. Therefore, sentiment analysis and opinion mining have become important research areas. In user reviews mining, topic modeling based approaches and Latent Dirichlet Allocation (LDA) are significant methods that are used in extracting product aspects in aspect based sentiment analysis. However, LDA cannot be directly applied on user reviews and on other short texts because of data sparsity problem and lack of co-occurrence patterns. Several studies have been published for the adaptation of LDA for short texts. In this study, a novel method for aspect based sentiment analysis, Sentence Segment LDA (SS-LDA) is proposed. SS-LDA is a novel adaptation of LDA algorithm for product aspect extraction. The experimental results reveal that SS-LDA is quite competitive in extracting products aspects.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309519",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Data science",
      "Geometry",
      "Information retrieval",
      "Latent Dirichlet allocation",
      "Mathematics",
      "Natural language processing",
      "Optics",
      "Physics",
      "Product (mathematics)",
      "Sentence",
      "Sentiment analysis",
      "Social media",
      "Topic model",
      "User-generated content",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Ozyurt",
        "given_name": "Baris"
      },
      {
        "surname": "Akcayol",
        "given_name": "M. Ali"
      }
    ]
  },
  {
    "title": "Kernel meets recommender systems: A multi-kernel interpolation for matrix completion",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114436",
    "abstract": "A primary research direction for recommender systems is matrix completion, which attempts to recover the missing values in a user–item rating matrix. There are numerous approaches for rating tasks, which are mainly classified into latent factor models and neighborhood-based models. Most neighborhood-based models seek similar neighbors by computing similarities in the original data space for final predictions. In this paper, we propose a new neighborhood-based interpolation model with a kernelized matrix completion framework, with the impact weights provided by neighbors computed in a new Hilbert space containing more features. In our model, the kernel function is combined with a similarity measurement to achieve better approximation for unknown ratings. Furthermore, we extend our model with a non-linear multi-kernel framework which learns weights automatically to improve the model. Finally, we conduct extensive experiments on several real-world datasets. The outcomes show that the proposed methods work effectively and improve the performance of the rating prediction task compared to both the traditional and state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310976",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Economics",
      "Eigenvalues and eigenvectors",
      "Hilbert space",
      "Image (mathematics)",
      "Interpolation (computer graphics)",
      "Kernel (algebra)",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Matrix decomposition",
      "Physics",
      "Quantum mechanics",
      "Recommender system",
      "Reproducing kernel Hilbert space",
      "Similarity (geometry)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Zhaoliang"
      },
      {
        "surname": "Zhao",
        "given_name": "Wei"
      },
      {
        "surname": "Wang",
        "given_name": "Shiping"
      }
    ]
  },
  {
    "title": "Development of artificial neural network for condition assessment of bridges based on hybrid decision making method – Feasibility study",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114271",
    "abstract": "Managing a bridge at an appropriate level of reliability requires knowledge of its technical condition, which is decisive in terms of maintenance and repair activities. This is a multi-criteria decision-making problem which results from the need to allocate limited financial resources to this work. Although many calculation models have been suggested in published sources, none of them has ever met these requirements. The algorithm presented by the authors allows for the assessment of any number of bridges, taking into account the diversity of solutions in terms of materials and structures, and can provide a solution to this problem. This hybrid calculation model, combining the modified Extent Analysis Fuzzy Analytic Hierarchy Process (EA FAHP) and Dominant Analytic Hierarchy Process (DAHP), has been verified on many existing bridges. However, due to the advanced mathematical algorithm, its practical use may create some difficulty for engineers applying it in practice. To avoid implementation problems, the authors have proposed a neural network model to facilitate the work of the staff operating the bridges. The application of the proposed model was presented in an article about a group of selected railway bridges that have been operating in Poland for over 100 years. The objects selected for analysis have open decks, which means that they are more susceptible to damage compared to road bridges. The accuracy of the artificial neural network (ANN) model was verified by comparing the results provided with the results obtained using the aggregated EA FAHP + DAHP model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309817",
    "keywords": [
      "Analytic hierarchy process",
      "Analytic network process",
      "Artificial intelligence",
      "Artificial neural network",
      "Bridge (graph theory)",
      "Computer science",
      "Economics",
      "Engineering",
      "Fuzzy logic",
      "Hierarchy",
      "Internal medicine",
      "Market economy",
      "Medicine",
      "Operating system",
      "Operations research",
      "Physics",
      "Power (physics)",
      "Process (computing)",
      "Quantum mechanics",
      "Reliability (semiconductor)",
      "Reliability engineering"
    ],
    "authors": [
      {
        "surname": "Fabianowski",
        "given_name": "Dariusz"
      },
      {
        "surname": "Jakiel",
        "given_name": "Przemysław"
      },
      {
        "surname": "Stemplewski",
        "given_name": "Sławomir"
      }
    ]
  },
  {
    "title": "A propositionalization method of multi-relational data based on Grammar-Guided Genetic Programming",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114263",
    "abstract": "The propositionalization process tries to find distinctive features of the examples in a database to transform such relational data into a simpler representation. More informative features have a positive impact on the classification capabilities of the learning algorithms. In this work, we propose a new propositionalization method, which generates complex Boolean attributes using Grammar-Guided Genetic Programming (G3P). The generated attributes are compound formulas that combine word items coming from a Bag-of-Words (BoW) representation using Boolean operators. The proposal was assessed against three state-of-the-art simple-instance and multiple-instance propositionalization methods. The experimental results show that the proposed method achieves an improvement in terms of classification accuracy and a considerable reduction in the dimensionality of the resulting datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030974X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Curse of dimensionality",
      "Data mining",
      "Dimensionality reduction",
      "Genetic programming",
      "Geometry",
      "Grammar",
      "Law",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Relational database",
      "Representation (politics)",
      "Theoretical computer science",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Quintero-Domínguez",
        "given_name": "Luis A."
      },
      {
        "surname": "Morell",
        "given_name": "Carlos"
      },
      {
        "surname": "Ventura",
        "given_name": "Sebastián"
      }
    ]
  },
  {
    "title": "A new topic modeling based approach for aspect extraction in aspect based sentiment analysis: SS-LDA",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114231",
    "abstract": "With the widespread use of social networks, blogs, forums and e-commerce web sites, the volume of user generated textual data is growing exponentially. User opinions in product reviews or in other textual data are crucial for manufacturers, retailers and providers of the products and services. Therefore, sentiment analysis and opinion mining have become important research areas. In user reviews mining, topic modeling based approaches and Latent Dirichlet Allocation (LDA) are significant methods that are used in extracting product aspects in aspect based sentiment analysis. However, LDA cannot be directly applied on user reviews and on other short texts because of data sparsity problem and lack of co-occurrence patterns. Several studies have been published for the adaptation of LDA for short texts. In this study, a novel method for aspect based sentiment analysis, Sentence Segment LDA (SS-LDA) is proposed. SS-LDA is a novel adaptation of LDA algorithm for product aspect extraction. The experimental results reveal that SS-LDA is quite competitive in extracting products aspects.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309519",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Data science",
      "Geometry",
      "Information retrieval",
      "Latent Dirichlet allocation",
      "Mathematics",
      "Natural language processing",
      "Optics",
      "Physics",
      "Product (mathematics)",
      "Sentence",
      "Sentiment analysis",
      "Social media",
      "Topic model",
      "User-generated content",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Ozyurt",
        "given_name": "Baris"
      },
      {
        "surname": "Akcayol",
        "given_name": "M. Ali"
      }
    ]
  },
  {
    "title": "Multi-objective Grammatical Evolution of Decision Trees for Mobile Marketing user conversion prediction",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114287",
    "abstract": "The worldwide adoption of mobile devices is raising the value of Mobile Performance Marketing, which is supported by Demand-Side Platforms (DSP) that match mobile users to advertisements. In these markets, monetary compensation only occurs when there is a user conversion. Thus, a key DSP issue is the design of a data-driven model to predict user conversion. To handle this nontrivial task, we propose a novel Multi-objective Optimization (MO) approach to evolve Decision Trees (DT) using a Grammatical Evolution (GE), under two main variants: a pure GE method (MGEDT) and a GE with Lamarckian Evolution (MGEDTL). Both variants evolve variable-length DTs and perform a simultaneous optimization of the predictive performance and model complexity. To handle big data, the GE methods include a training sampling and parallelism evaluation mechanism. The algorithms were applied to a recent database with around 6 million records from a real-world DSP. Using a realistic Rolling Window (RW) validation, the two GE variants were compared with a standard DT algorithm (CART), a Random Forest and a state-of-the-art Deep Learning (DL) model. Competitive results were obtained by the GE methods, which present affordable training times and very fast predictive response times.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309891",
    "keywords": [
      "Artificial intelligence",
      "Big data",
      "Computer hardware",
      "Computer science",
      "Computer security",
      "Data mining",
      "Decision tree",
      "Deep learning",
      "Digital signal processing",
      "Economics",
      "Key (lock)",
      "Machine learning",
      "Management",
      "Mobile device",
      "Operating system",
      "Random forest",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Pereira",
        "given_name": "Pedro José"
      },
      {
        "surname": "Cortez",
        "given_name": "Paulo"
      },
      {
        "surname": "Mendes",
        "given_name": "Rui"
      }
    ]
  },
  {
    "title": "Fast heuristic method to detect people in frontal depth images",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114483",
    "abstract": "This paper presents a new method for detecting people using only depth images captured by a camera in a frontal position. The approach is based on first detecting all the objects present in the scene and determining their average depth (distance to the camera). Next, for each object, a 3D Region of Interest (ROI) is processed around it in order to determine if the characteristics of the object correspond to the biometric characteristics of a human head. The results obtained using three public datasets captured by three depth sensors with different spatial resolutions and different operation principle (structured light, active stereo vision and Time of Flight) are presented. These results demonstrate that our method can run in realtime using a low-cost CPU platform with a high accuracy, being the processing times smaller than 1 ms per frame for a 512 × 424 image resolution with a precision of 99.26% and smaller than 4 ms per frame for a 1280 × 720 image resolution with a precision of 99.77%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311301",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Computer vision",
      "Economics",
      "Finance",
      "Frame (networking)",
      "Frame rate",
      "Heuristic",
      "Image resolution",
      "Object (grammar)",
      "Position (finance)",
      "Stereopsis",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Luna",
        "given_name": "Carlos A."
      },
      {
        "surname": "Losada-Gutiérrez",
        "given_name": "Cristina"
      },
      {
        "surname": "Fuentes-Jiménez",
        "given_name": "David"
      },
      {
        "surname": "Mazo",
        "given_name": "Manuel"
      }
    ]
  },
  {
    "title": "Visual saliency detection by integrating spatial position prior of object with background cues",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114219",
    "abstract": "In this paper, we propose an effective visual saliency-detection model based on spatial position prior of attractive objects and sparse background features. Firstly, since multi-orientation features are among the key visual stimuli in the human visual system (HVS) to perceive object spatial information, discrete wavelet frame transform (DWDT) is applied to extract directionality characteristics for calculating the centoid of remarkable objects in the original image. Then, the color contrast feature is used to represent the physical characteristics of salient objects. Thirdly, in order to explore and utilize the background features of an input image, sparse dictionary learning is performed to statistically analyze and estimate the background feature map. Finally, three distinctive cues of the directional feature including the color contrast feature and the background feature are combined to generate a final robust saliency map. Experimental results on three widely used image datasets show that our proposed method is effective and efficient, and is superior to other state-of-the-art saliency-detection models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309441",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Feature (linguistics)",
      "Geometry",
      "Kadir–Brady saliency detector",
      "Linguistics",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Orientation (vector space)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Jian",
        "given_name": "Muwei"
      },
      {
        "surname": "Wang",
        "given_name": "Jing"
      },
      {
        "surname": "Yu",
        "given_name": "Hui"
      },
      {
        "surname": "Wang",
        "given_name": "Guodong"
      },
      {
        "surname": "Meng",
        "given_name": "Xianjing"
      },
      {
        "surname": "Yang",
        "given_name": "Lu"
      },
      {
        "surname": "Dong",
        "given_name": "Junyu"
      },
      {
        "surname": "Yin",
        "given_name": "Yilong"
      }
    ]
  },
  {
    "title": "DeepThin: A novel lightweight CNN architecture for traffic sign recognition without GPU requirements",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114481",
    "abstract": "For a safe and automated vehicle driving application, it is a prerequisite to have a robust and highly accurate traffic sign detection system. In this paper, we proposed a novel energy-efficient Thin yet Deep convolutional neural network architecture for traffic sign recognition. Within the proposed architecture, each convolutional layer contains less than 50 features enabling our convolutional neural network to be trained quickly even without the aid of a graphics processing unit. The performance of the proposed architecture is measured using two publicly available traffic sign datasets, namely the German Traffic Sign Recognition Benchmark and the Belgian Traffic Sign Classification dataset. First, we train and test the performance of the proposed architecture using the large German Traffic Sign Recognition Benchmark dataset. Then, we retrain the network models using transfer learning on the more challenging Belgian Traffic Sign Classification dataset to evaluate test performance. The proposed architecture outperforms the performance of the state-of-the-art traffic sign methods with at least five times less parameter in the individual end-to-end network for training.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311283",
    "keywords": [
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer network",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Geodesy",
      "Geography",
      "Graphics processing unit",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Network architecture",
      "Operating system",
      "Pattern recognition (psychology)",
      "Sign (mathematics)",
      "Traffic sign",
      "Traffic sign recognition",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Haque",
        "given_name": "Wasif Arman"
      },
      {
        "surname": "Arefin",
        "given_name": "Samin"
      },
      {
        "surname": "Shihavuddin",
        "given_name": "A.S.M."
      },
      {
        "surname": "Hasan",
        "given_name": "Muhammad Abul"
      }
    ]
  },
  {
    "title": "CPIN: Comprehensive present-interest network for CTR prediction",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114469",
    "abstract": "Personalized recommendation is a popular research direction in both industry and academia. Some research on recommender systems utilizes the users’ interaction history on items to represent the users’ interests, which has achieved remarkable success. Users’ interests in the real world are dynamically changing and have a strong correlation with the interaction sequence. However, sometimes users’ interests are less relevant to the order of the current interaction sequence, but are more relevant to certain items in the user interaction history. In this paper, a novel deep neural network model is proposed to deal with this situation. The developed model consists of two parts: the present interest relevant to the order of the interaction sequence and the comprehensive interest relevant to some items in the interaction sequence. An ancillary multi-layer perceptron (MLP) is constructed to improve the training of our model. Experiments on public and industrial datasets are conducted. The experimental results show that our proposed model outperforms the state-of-the-art models which demonstrates the effectiveness of the ancillary MLP.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420311192",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Data mining",
      "Economics",
      "Finance",
      "Genetics",
      "Machine learning",
      "Multilayer perceptron",
      "Order (exchange)",
      "Perceptron",
      "Recommender system",
      "Sequence (biology)"
    ],
    "authors": [
      {
        "surname": "Hong",
        "given_name": "Wenxing"
      },
      {
        "surname": "Xiong",
        "given_name": "Ziang"
      },
      {
        "surname": "You",
        "given_name": "Jinjie"
      },
      {
        "surname": "Wu",
        "given_name": "Xiaolin"
      },
      {
        "surname": "Xia",
        "given_name": "Min"
      }
    ]
  },
  {
    "title": "Adaptive query relaxation and result categorization of fuzzy spatiotemporal data based on XML",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114222",
    "abstract": "With the rapid development of spatiotemporal information and its applications, querying spatiotemporal data has received considerable attention. Meanwhile, the imprecision and uncertainty of information cannot be ignored in many practical applications. Querying fuzzy spatiotemporal data have become one of the most important topics in academia and industry. Although there have been some achievements in querying aspect, the study about fuzzy spatiotemporal query relaxation is still few. In fact, query relaxation is necessary when the amount of query results is small or even empty, especially in the process of querying fuzzy spatiotemporal data. In this paper, we propose an adaptive query relaxation and result categorization approach for fuzzy spatiotemporal data based on XML, which is compatible with spatiotemporal features when spatiotemporal related queries are performed. The approach does not depend on any specific domain or user, it can adaptively relax the initial query requirements, and classify the results by user context preferences and data distribution after query relaxation. In order to locate the spatiotemporal data quickly and show the corresponding fuzziness apparently, we adopted XML to construct the fuzzy spatiotemporal model for query relaxation because XML data can be represented as a tree model which can flexibly arrange the spatiotemporal nodes at the specified position and mark the fuzziness of nodes on the path. In addition, after query relaxation, we present the results categorization algorithm to address the problem of information overload, and then return a navigation tree to the user. Finally, we launch a comprehensive set of experiments to demonstrate the effectiveness and efficiency of our proposed approach. Results of experiments demonstrate that our adaptive query relaxation and result categorization approach based on XML has higher recall and precision in spatiotemporal related query, and can capture the user’s needs and preferences effectively as well.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309465",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Categorization",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Fuzzy logic",
      "Fuzzy set",
      "Information retrieval",
      "Operating system",
      "Paleontology",
      "Psychology",
      "Relaxation (psychology)",
      "Search engine",
      "Social psychology",
      "Uncertain data",
      "Web query classification",
      "Web search query",
      "XML"
    ],
    "authors": [
      {
        "surname": "Bai",
        "given_name": "Luyi"
      },
      {
        "surname": "He",
        "given_name": "Aijia"
      },
      {
        "surname": "Liu",
        "given_name": "Minghao"
      },
      {
        "surname": "Zhu",
        "given_name": "Lin"
      },
      {
        "surname": "Xing",
        "given_name": "Yizong"
      }
    ]
  },
  {
    "title": "A salient object segmentation framework using diffusion-based affinity learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114428",
    "abstract": "In this paper, a salient object segmentation framework by using diffusion-based affinity learning and based on absorbing Markov chain (AMC) is proposed. Traditional approaches for structural modeling of images via local information and pairwise similarity graph by using, e.g. Gaussian heat kernel function, are insufficient for capturing the faithful relationships among the regions. According to the AMC principles, the more strong relationships result in lowering the time that a transient node becomes an absorbed one and consequently increases the transition probability between those nodes. To this end, a dense transition probability matrix is constructed based on an affinity matrix which learned using a diffusion process. Computing tensor product of the initial similarity graph with itself provides credible information about inter-relationships of nodes. Since conducting similarity propagation over such a tensor product graph imposes high computational costs, an iterative diffusion process is leveraged that introduces the same complexity as applying traditional diffusion processes on the original graph. As a fundamental benefit, such a process will enhance the accuracy and preciseness of saliency detection. Finally, as a complementary step, the saliency map will be refined by revisiting the saliency value of every single pixel. The experimental results on three major benchmark datasets demonstrate the efficiency of the proposed framework. More specifically, as expected, taking advantage of full learned affinity matrix can significantly improve the precision of the process.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310927",
    "keywords": [
      "Adjacency matrix",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Graph",
      "Machine learning",
      "Markov chain",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Moradi",
        "given_name": "Morteza"
      },
      {
        "surname": "Bayat",
        "given_name": "Farhad"
      }
    ]
  },
  {
    "title": "A review on rumour prediction and veracity assessment in online social network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114208",
    "abstract": "In the present era, the social network is used as an important medium for sharing thoughts and opinions of an individual. The main reason behind this is, it provides a fast-spreading of information among the public easily, requiring a very low cost of access. This leads to having online social media as one of the stepping stones to encourage false content and influencing public opinion and its decision. Rumour is one of the prominent forms of misleading information on social media and should be detected as early as possible for avoiding their significant effects. Due to these reasons, the researchers have put their keen interest in developing an effective rumour detection framework in the last years. In this paper, we mainly focused on six main aspects. Firstly, we discuss rumours from a definition perspective that have been considered in the state-of-the-art and describe the generalized model of rumour detection. Secondly, we discuss how to get access to data from different social media platforms, and presents various state-of-the-art methods to gather these data, as well as publicly available datasets. Third, we describe a different set of features that have been considered in rumour detection approaches. Fourth, we provide deep insight into the various methods used to employ rumour detection and its veracity assessment on multimedia data (Text and Images) with some practical implications. Whereas in the fifth aspect, the constraints of the study have been discussed. Finally, we concluded with useful findings and suggested future directions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309362",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Data science",
      "Perspective (graphical)",
      "Programming language",
      "Set (abstract data type)",
      "Social media",
      "Social network (sociolinguistics)",
      "State (computer science)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Varshney",
        "given_name": "Deepika"
      },
      {
        "surname": "Vishwakarma",
        "given_name": "Dinesh Kumar"
      }
    ]
  },
  {
    "title": "Supervised kernel density estimation K-means",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114350",
    "abstract": "K-means is a well-known unsupervised-learning algorithm. It assigns data points to k clusters, the centers of which are termed centroids. However, these centroids have a structure usually represented by a list of quantized vectors, so that kernel density estimation models can better represent complex data distributions. This paper proposes a k-means-based supervised-learning clustering method termed supervised kernel-density-estimation k-means. The proposed approach uses kernel density estimation for class examples inside each cluster to obtain a better representation of the data distribution. The algorithm constructs an initial model using supervised k-means with an equal seed distribution among the classes so that a balance between majority and minority classes is achieved. We also incorporate incremental semi-supervised learning into the proposed method. Experiments were conducted using publicly available benchmark datasets. The results demonstrated that, compared with state-of-the-art supervised methods, the proposed algorithm, which can also perform incremental semi-supervised learning, achieved highly satisfactory performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310344",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Centroid",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Density estimation",
      "Estimator",
      "Geodesy",
      "Geography",
      "Kernel (algebra)",
      "Kernel density estimation",
      "Kernel method",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Semi-supervised learning",
      "Statistics",
      "Supervised learning",
      "Support vector machine",
      "Unsupervised learning",
      "Variable kernel density estimation"
    ],
    "authors": [
      {
        "surname": "Bortoloti",
        "given_name": "Frederico Damasceno"
      },
      {
        "surname": "Oliveira",
        "given_name": "Elias de"
      },
      {
        "surname": "Ciarelli",
        "given_name": "Patrick Marques"
      }
    ]
  },
  {
    "title": "A Neighborhood Undersampling Stacked Ensemble (NUS-SE) in imbalanced classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114246",
    "abstract": "Stacked ensemble, which formulates an ensemble by using a meta-learner to combine (stack) the predictions of multiple base classifiers, suffers from the problem of suboptimal performance on imbalanced classification. To improve the classification performance of stacked ensemble on imbalanced datasets, we proposed a method named Neighborhood Undersampling Stacked Ensemble (NUS-SE) in this paper. In general, the NUS-SE can be broken down into two proposed components, an undersampling based stacked ensemble framework (US-SE) component and an undersampling technique component. In the metadata generation step of stacked ensemble, a cross-validation-like procedure (CV-prediction) is commonly used. Unfortunately, incomplete metadata with missing prediction values is generated when undersampling is performed within a stacked ensemble which utilized CV-prediction as the metadata generation procedure. Therefore, in the proposed US-SE component, we replaced the standard CV-prediction procedure with our proposed method coined as Subset and Out-of-Subset (S-OOS) prediction procedure as the metadata generation method. S-OOS prediction procedure will generate metadata without missing prediction values and thus enabling the integration of undersampling within stacked ensemble. By integrating undersampling within stacked ensemble, multiple undersampled-data-subsets are used in the training of US-SE’s base learners. While in the undersampling component, we further proposed a novel undersampling technique — Neighborhood Undersampling (NUS) which selects majority instances based on their local neighborhood information. The performance of the NUS-SE is evaluated against those non-resampling based stacked ensemble as baseline methods. The experiment demonstrates that the proposed NUS-SE, which is an undersampling based stacked ensemble, is capable of achieving a better performance when compared to the non-resampling based stacked ensemble.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309635",
    "keywords": [
      "Artificial intelligence",
      "Component (thermodynamics)",
      "Computer science",
      "Data mining",
      "Ensemble forecasting",
      "Ensemble learning",
      "Metadata",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Resampling",
      "Thermodynamics",
      "Undersampling"
    ],
    "authors": [
      {
        "surname": "Seng",
        "given_name": "Zian"
      },
      {
        "surname": "Kareem",
        "given_name": "Sameem Abdul"
      },
      {
        "surname": "Varathan",
        "given_name": "Kasturi Dewi"
      }
    ]
  },
  {
    "title": "Multi-objective evolutionary algorithms with heuristic decoding for hybrid flow shop scheduling problem with worker constraint",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114282",
    "abstract": "The classical hybrid flow shop scheduling problem (HFSSP) considers the operation and machine constraints but not the worker constraint. Acknowledging the influence and potential of human factors as a key element in improving production efficiency in a real hybrid flow shop, we consider a new realistic HFSSP with worker constraint (HFSSPW) and construct its mixed integer linear programming model. Seven multi-objective evolutionary algorithms with heuristic decoding (HD) (MOEAHs) are proposed to solve the HFSSPW. According to list scheduling, we first present four HD methods for four MOEAHs, and these methods incorporate four priority rules of machine and worker assignments. The earliest due date (EDD) rule is further introduced into the HD methods for the other three MOEAHs. The developed model is solved using CPLEX based on 20 loose instances under a time limit, and the four proposed MOEAHs are evaluated by comparing them with the results from CPLEX and two best-performing algorithms in the literature. The computational results reveal that the proposed MOEAHs perform excellently in terms of the makespan objective. Additionally, comprehensive experiments, including 150 tight instances, are conducted. In terms of solution quality and efficiency, the computational results show that the proposed MOEAHs demonstrate highly effective performance, and integrating EDD into the HD can substantially enhance algorithm performance. Finally, a real-life problem of the foundry plant is solved by MOEAHs and the scheduling solutions totally meet the delivery requirement.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309854",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Decoding methods",
      "Evolutionary algorithm",
      "Flow shop scheduling",
      "Job shop scheduling",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Schedule",
      "Scheduling (production processes)"
    ],
    "authors": [
      {
        "surname": "Han",
        "given_name": "Wenwu"
      },
      {
        "surname": "Deng",
        "given_name": "Qianwang"
      },
      {
        "surname": "Gong",
        "given_name": "Guiliang"
      },
      {
        "surname": "Zhang",
        "given_name": "Like"
      },
      {
        "surname": "Luo",
        "given_name": "Qiang"
      }
    ]
  },
  {
    "title": "Enhanced multi-verse optimizer for task scheduling in cloud computing environments",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114230",
    "abstract": "Cloud computing is a trending technology that allows users to use computing resources remotely in a pay-per-use model. One of the main challenges in cloud computing environments is task scheduling, in which tasks should be scheduled efficiently to minimize execution time and cost while maximizing resources’ utilization. Many meta-heuristic algorithms are used for task scheduling in cloud environments in the literature such as Multi-Verse Optimizer (MVO) and Particle Swarm Optimization (PSO). In this paper, an Enhanced version of the Multi-Verse Optimizer (EMVO) is proposed as a superior task scheduler in this area. The proposed EMVO is compared with both original MVO and the PSO algorithms in cloud environments. The results show that EMVO substantially outperforms both MVO and PSO algorithms in terms of achieving minimized makespan time and increasing resources’ utilization.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309507",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cloud computing",
      "Computer science",
      "Distributed computing",
      "Economics",
      "Execution time",
      "Heuristic",
      "Job shop scheduling",
      "Management",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Particle swarm optimization",
      "Schedule",
      "Scheduling (production processes)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Shukri",
        "given_name": "Sarah E."
      },
      {
        "surname": "Al-Sayyed",
        "given_name": "Rizik"
      },
      {
        "surname": "Hudaib",
        "given_name": "Amjad"
      },
      {
        "surname": "Mirjalili",
        "given_name": "Seyedali"
      }
    ]
  },
  {
    "title": "Three-dimensional guillotine cutting problems with constrained patterns: MILP formulations and a bottom-up algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114257",
    "abstract": "In this paper, we address the Constrained Three-dimensional Guillotine Cutting Problem (C3GCP), which consists of cutting a larger cuboid block (object) to produce a limited number of smaller cuboid pieces (items) using orthogonal guillotine cuts only. This way, all cuts must be parallel to the object’s walls and generate two cuboid sub-blocks, and there is a maximum number of copies that can be manufactured for each item type. The C3GCP arises in industrial manufacturing settings, such as the cutting of steel and foam for mattresses. To model this problem, we propose a new compact mixed-integer non-linear programming (MINLP) formulation by extending its two-dimensional version, and develop a mixed-integer linear programming (MILP) version. We also propose a new model for a particular case of the problem which considers 3-staged patterns. As a solution method, we extend the algorithm of Wang (1983) to the three-dimensional case. We emphasise that the C3GCP is different from 3D packing problems, namely from the Container Loading Problem, because of the guillotine cut constraints. All proposed approaches are evaluated through computational experiments using benchmark instances. The results show that the approaches are effective on different types of instances, mainly when the maximum number of copies per item type is small, a situation typically encountered in practical settings with low demand for each item type. These approaches can be easily embedded into existing expert systems for supporting the decision-making process.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309726",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Block (permutation group theory)",
      "Combinatorics",
      "Computer science",
      "Container (type theory)",
      "Cuboid",
      "Cutting stock problem",
      "Ecology",
      "Engineering",
      "Geodesy",
      "Geography",
      "Geometry",
      "Integer (computer science)",
      "Integer programming",
      "Linear programming",
      "Mathematical optimization",
      "Mathematics",
      "Mechanical engineering",
      "Object (grammar)",
      "Operating system",
      "Optimization problem",
      "Packing problems",
      "Process (computing)",
      "Programming language",
      "Type (biology)"
    ],
    "authors": [
      {
        "surname": "Martin",
        "given_name": "Mateus"
      },
      {
        "surname": "Oliveira",
        "given_name": "José Fernando"
      },
      {
        "surname": "Silva",
        "given_name": "Elsa"
      },
      {
        "surname": "Morabito",
        "given_name": "Reinaldo"
      },
      {
        "surname": "Munari",
        "given_name": "Pedro"
      }
    ]
  },
  {
    "title": "A novel approach for b-value optimization in Intravoxel Incoherent Motion Imaging using Metaheuristic algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114270",
    "abstract": "Intravoxel Incoherent Motion (IVIM) based Magnetic Resonance Imaging (MRI) technique allows the quantitative evaluation of perfusion and diffusion without the use of contrast agents. The correctness of the diagnosis depends upon the accuracy and precision of IVIM parameter estimation. To achieve this, several diffusion weighted images must be acquired. The criteria of selection of diffusion weights varies among researchers. The diffusion weights are incorporated by a factor called ‘b value’, which reflects the intensity and duration of the diffusion gradient pulses used for imaging. IVIM imaging takes more time for image acquisition with multiple b values and the selection of the absolute b values as well as the number of b values, is a real challenge. As the b value count increases the scan time increases, which leads to increased patient discomfort and motion artifacts, resulting in poor image quality. Moreover, in most cases, these b values are found using trial and error methods during image acquisition. These issues can be addressed to a large extent by finding, optimum number and range of b values. In this paper, we propose a population based Metaheuristic algorithm for arriving at the optimal b value count and the range of absolute b values for liver, which is an organ with high perfusion. Three separate models are developed to appropriately choose all possible b values ranging between 0 s/mm 2 and 850 s/mm 2 , to observe the effects of diffusion and perfusion as well as to increase the global search space. The effect of low (0 s/mm 2 to 50 s/mm 2 ), medium (55 s/mm 2 to 220 s/mm 2 ) and high (230 s/mm 2 to 800 s/mm 2 ) b values has been studied to find the optimal number of b values that can be used for IVIM imaging. In order to define a b value count that minimizes the error in IVIM parameters, simulation experiments are performed for different b value counts specifically 16, 14, 12, 10, 8, 6 and 4. For each of these experiments, repeated observations are made to analyze the parameter uncertainty. The results obtained show that the b value count can be minimized for a better quantitative estimation of IVIM parameters with the least possible errors and the difference in error between each of these observations is found to be less than 0.001. Minimization of b value count results in the reduction of overall image acquisition time and hence patient discomfort. It is also observed that for a very good SNR, b value count can be reduced to 4 although for a reasonable SNR, 8 or 10 b values are to be used for accurate quantitative estimation of IVIM parameters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309805",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Composite material",
      "Computer science",
      "Contrast (vision)",
      "Correctness",
      "Diffusion",
      "Diffusion MRI",
      "Environmental health",
      "Image (mathematics)",
      "Image quality",
      "Intravoxel incoherent motion",
      "Magnetic resonance imaging",
      "Materials science",
      "Mathematics",
      "Medicine",
      "Physics",
      "Population",
      "Radiology",
      "Range (aeronautics)",
      "Ranging",
      "Telecommunications",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Raju",
        "given_name": "Jini"
      },
      {
        "surname": "C.",
        "given_name": "Ushadevi Amma"
      },
      {
        "surname": "John",
        "given_name": "Ansamma"
      }
    ]
  },
  {
    "title": "Air compressor load forecasting using artificial neural network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114209",
    "abstract": "Air compressor systems are responsible for approximately 10% of the electricity consumed in United States and European Union industry. As many researches have proven the effectiveness of using Artificial Neural Network in air compressor performance prediction, there is still a need to forecast the air compressor electrical load profile. The objective of this study is to predict compressed air systems' electrical load profile, which is valuable to industry practitioners as well as software providers in developing better practice and tools for load management and look-ahead scheduling programs. Two artificial neural networks, Two-Layer Feed-Forward Neural Network and Long Short-Term Memory were used to predict an air compressors electrical load. Compressors with three different control mechanisms are evaluated with a total number of 11,874 observations. The forecasts were validated using out-of-sample datasets with 5-fold cross-validation. Models produced average coefficient of determination values from 0.24 to 0.94, average root-mean-square errors from 0.05 kW - 5.83 kW, and mean absolute scaled errors from 0.20 to 1.33. The results indicate that both artificial neural networks yield good results for compressors using variable speed drive (average R2 = 0.8 and no naïve forecasting), only the long short-term memory model gives acceptable results for compressors using on/off control (average R2 = 0.82 and no naïve forecasting), and no satisfactory results are obtained for load/unload type air compressors (models constituting naïve forecasting).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309374",
    "keywords": [
      "Air compressor",
      "Artificial intelligence",
      "Artificial neural network",
      "Automotive engineering",
      "Computer science",
      "Electrical engineering",
      "Electrical load",
      "Engineering",
      "Gas compressor",
      "Mathematics",
      "Mean squared error",
      "Mechanical engineering",
      "Operations management",
      "Scheduling (production processes)",
      "Simulation",
      "Statistics",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Da-Chun"
      },
      {
        "surname": "Bahrami Asl",
        "given_name": "Babak"
      },
      {
        "surname": "Razban",
        "given_name": "Ali"
      },
      {
        "surname": "Chen",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "Trustworthiness prediction of cloud services based on selective neural network ensemble learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114390",
    "abstract": "Cloud services have become a popular and flexible solution for providing components to build service-based systems. A component’s trustworthiness is a key measure that can guide service requesters when making a service selection decision. Prediction of this trustworthiness, based on the component’s multi-faceted quality of service (QoS) attributes, is therefore an important problem to address. In this paper, selective ensemble learning is introduced to address the trust problem for cloud services: We use back-propagation neural networks (BPNNs) as the basic classifiers, with two swarm intelligence algorithms adapted to search for the optimal aggregation weights to create the ensemble: Basic particle swarm optimization (PSO) is used for decimal weights; and quantum discrete PSO (QPSO) is used for binary (0-1) weights. The optimized ensemble learning model, based on BPNNs, is then used to predict the trustworthiness of a given cloud service. Extensive experiments are performed on a well-known, public dataset to verify the effectiveness of the proposed trust prediction algorithms. The experimental results show that our algorithms are not only better than the basic BPNN method in prediction precision, but also outperform current state-of-the-art trust prediction algorithms. The proposed algorithms also exhibit a strong robustness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310629",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Cloud computing",
      "Component (thermodynamics)",
      "Computer network",
      "Computer science",
      "Computer security",
      "Data mining",
      "Ensemble learning",
      "Gene",
      "Machine learning",
      "Operating system",
      "Particle swarm optimization",
      "Physics",
      "Quality of service",
      "Robustness (evolution)",
      "Thermodynamics",
      "Trustworthiness"
    ],
    "authors": [
      {
        "surname": "Mao",
        "given_name": "Chengying"
      },
      {
        "surname": "Lin",
        "given_name": "Rongru"
      },
      {
        "surname": "Towey",
        "given_name": "Dave"
      },
      {
        "surname": "Wang",
        "given_name": "Wenle"
      },
      {
        "surname": "Chen",
        "given_name": "Jifu"
      },
      {
        "surname": "He",
        "given_name": "Qiang"
      }
    ]
  },
  {
    "title": "A DWT-SVD based adaptive color multi-watermarking scheme for copyright protection using AMEF and PSO-GWO",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114414",
    "abstract": "Color multi-watermarking is a challenge in the field of copyright protection, and the key in the color multi-watermarking technology is how to determine the optimal embedding regions and embedding strengths to achieve the trade-off among multiple watermarks while maintaining great invisibility, sufficient robustness, and large capacity. In order to tackle these problems, an adaptive multiple embedding factors (AMEF) algorithm for calculating the optimal embedding regions and the optimal embedding strengths is proposed in this paper to embed multiple color watermarks simultaneously. In the presented AMEF algorithm, we propose that the optimal embedding regions are determined by the contrast function of different blocks. Then we determine the multiple embedding strengths to depend on the weighted ratio of the contrast values of blocks and the eigenvalues of different marks. Furthermore, in order to calculate the weight value in the AMEF algorithm, we define a single objective function and utilize a hybrid particle swarm optimization and grey wolf optimizer (PSO-GWO) algorithm to optimize the objective function. In this work, by the use of the discrete wavelet transform (DWT), singular value decomposition (SVD) and AMEF, four encrypted color watermarks are inserted into the selected regions of the color (normal or medical) host image, simultaneously. Then watermarked host image is tested under various attacks and compared to other recent existing schemes. The experimental results demonstrate that the proposed scheme can effectively achieve the trade-off among the invisibility, robustness and capacity, simultaneously. And from the comparison results, the proposed scheme possesses high security, large capacity, and strong robustness against various attacks while maintaining good invisibility.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310812",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Digital watermarking",
      "Embedding",
      "Gene",
      "Image (mathematics)",
      "Invisibility",
      "Mathematical optimization",
      "Mathematics",
      "Particle swarm optimization",
      "Robustness (evolution)",
      "Singular value decomposition",
      "Steganography"
    ],
    "authors": [
      {
        "surname": "Shen",
        "given_name": "Yuxin"
      },
      {
        "surname": "Tang",
        "given_name": "Chen"
      },
      {
        "surname": "Xu",
        "given_name": "Min"
      },
      {
        "surname": "Chen",
        "given_name": "Mingming"
      },
      {
        "surname": "Lei",
        "given_name": "Zhenkun"
      }
    ]
  },
  {
    "title": "Boosting expensive synchronizing heuristics",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114203",
    "abstract": "For automata, synchronization, the problem of bringing an automaton to a particular state regardless of its initial state, is important. It has several applications in practice and is related to a fifty-year-old conjecture on the length of the shortest synchronizing word. Although using shorter words increases the effectiveness in practice, finding a shortest one (which is not necessarily unique) is NP-hard. For this reason, there exist various heuristics in the literature. However, high-quality heuristics such as SynchroP producing relatively shorter sequences are very expensive and can take hours when the automaton has tens of thousands of states. The SynchroP heuristic has been frequently used as a benchmark to evaluate the performance of the new heuristics. In this work, we first improve the runtime of SynchroP and its variants by using algorithmic techniques. We then focus on adapting SynchroP for many-core architectures, and overall, we obtain more than 1000 × speedup on GPUs compared to naive sequential implementation that has been frequently used as a benchmark to evaluate new heuristics in the literature. We also propose two SynchroP variants and evaluate their performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309325",
    "keywords": [
      "Artificial intelligence",
      "Automaton",
      "Benchmark (surveying)",
      "Boosting (machine learning)",
      "Computer science",
      "Geodesy",
      "Geography",
      "Heuristic",
      "Heuristics",
      "Operating system",
      "Parallel computing",
      "Speedup",
      "Synchronizing",
      "Telecommunications",
      "Theoretical computer science",
      "Transmission (telecommunications)"
    ],
    "authors": [
      {
        "surname": "Saraç",
        "given_name": "N. Ege"
      },
      {
        "surname": "Altun",
        "given_name": "Ömer Faruk"
      },
      {
        "surname": "Atam",
        "given_name": "Kamil Tolga"
      },
      {
        "surname": "Karahoda",
        "given_name": "Sertaç"
      },
      {
        "surname": "Kaya",
        "given_name": "Kamer"
      },
      {
        "surname": "Yenigün",
        "given_name": "Hüsnü"
      }
    ]
  },
  {
    "title": "Deep belief network based intrusion detection techniques: A survey",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114170",
    "abstract": "With the recent growth in the number of IoT devices, the amount of personal, sensitive, and important data flowing through the global network have grown rapidly. Additionally, the malicious attempt to access important information or damage the network have also become more complex and advanced. Thus, cybersecurity has become an important issue for the evolution toward future networks that can react and counter such threats. Intrusion detection is an important part of the cybersecurity technology with the goal of monitoring and analyzing network traffic from various resources and detect malicious activities. In recent years, deep learning base deep neural network (DNN) techniques have been utilized as the key solution to detect malicious attacks and among many DNNs, deep belief network (DBN) has been the most influential technique. There have been many attempts to survey wide range of machine learning and deep learning technique based intrusion detection research works, including DBN, but failed to provide a complete review of all the aspects related to the DBN based intrusion detection models. Unlike previous survey papers, we first provide basic concepts on data set, performance metric, and restricted Boltzmann machines, to help understand the basic DBN based intrusion detection model. Finally, a complete review and analysis on the previously published works on DBN based IDS models is provided.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309088",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer security",
      "Data mining",
      "Deep belief network",
      "Deep learning",
      "Economics",
      "Intrusion detection system",
      "Key (lock)",
      "Machine learning",
      "Metric (unit)",
      "Operations management",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Sohn",
        "given_name": "Insoo"
      }
    ]
  },
  {
    "title": "Understanding static code warnings: An incremental AI approach",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114134",
    "abstract": "Knowledge-based systems reason over some knowledge base. Hence, an important issue for such systems is how to acquire the knowledge needed for their inference. This paper assesses active learning methods for acquiring knowledge for “static code warnings”. Static code analysis is a widely-used method for detecting bugs and security vulnerabilities in software systems. As software becomes more complex, analysis tools also report lists of increasingly complex warnings that developers need to address on a daily basis. Such static code analysis tools are usually over-cautious; i.e. they often offer many warnings about spurious issues. Previous research work shows that about 35% to 91 % warnings reported as bugs by SA tools are actually unactionable (i.e., warnings that would not be acted on by developers because they are falsely suggested as bugs). Experienced developers know which errors are important and which can be safely ignored. How can we capture that experience? This paper reports on an incremental AI tool that watches humans reading false alarm reports. Using an incremental support vector machine mechanism, this AI tool can quickly learn to distinguish spurious false alarms from more serious matters that deserve further attention. In this work, nine open-source projects are employed to evaluate our proposed model on the features extracted by previous researchers and identify the actionable warnings in a priority order given by our algorithm. We observe that our model can identify over 90% of actionable warnings when our methods tell humans to ignore 70 to 80% of the warnings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308824",
    "keywords": [
      "Artificial intelligence",
      "Code (set theory)",
      "Computer science",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Xueqi"
      },
      {
        "surname": "Yu",
        "given_name": "Zhe"
      },
      {
        "surname": "Wang",
        "given_name": "Junjie"
      },
      {
        "surname": "Menzies",
        "given_name": "Tim"
      }
    ]
  },
  {
    "title": "Shapley-Lorenz eXplainable Artificial Intelligence",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114104",
    "abstract": "Explainability of artificial intelligence methods has become a crucial issue, especially in the most regulated fields, such as health and finance. In this paper, we provide a global explainable AI method which is based on Lorenz decompositions, thus extending previous contributions based on variance decompositions. This allows the resulting Shapley-Lorenz decomposition to be more generally applicable, and provides a unifying variable importance criterion that combines predictive accuracy with explainability, using a normalised and easy to interpret metric. The proposed decomposition is illustrated within the context of a real financial problem: the prediction of bitcoin prices.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308575",
    "keywords": [
      "Accounting",
      "Artificial intelligence",
      "Biology",
      "Chaotic",
      "Computer science",
      "Context (archaeology)",
      "Decomposition",
      "Ecology",
      "Econometrics",
      "Economics",
      "Lorenz system",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Paleontology",
      "Variable (mathematics)",
      "Variance (accounting)",
      "Variance decomposition of forecast errors"
    ],
    "authors": [
      {
        "surname": "Giudici",
        "given_name": "Paolo"
      },
      {
        "surname": "Raffinetti",
        "given_name": "Emanuela"
      }
    ]
  },
  {
    "title": "Integrating underground line design with existing public transportation systems to increase transit network connectivity: Case study in Greater Cairo",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114183",
    "abstract": "Connectivity is a significant problem in large-scale transit networks because the number of transfers required to conduct a trip is considered a discomfort by transit users. This paper presents a practical solution for an underground metro line planning problem by integrating existing bus and metro networks into a single connected transit network. The proposed method aims to obviate the usual combinatorial complexity when solving a transit route design problem. It aims to increase the overall transit system connectivity by selecting a consistent and non-demand-oriented criterion for the design. The metro lines are designed by minimizing passenger transfers through the transit network according to predefined demand node pairs. The design scheme offers a set of ring route alternatives for a sizeable case study in Greater Cairo. The case study selected sixteen traffic analysis zones, an existing metro network consisting of three main lines (113.6 km long), and twelve main bus lines (487.7 km long) for analysis. TransCAD software was used as the basis for coordinating the stations and lines of both the bus and metro systems. Subsequently, a passenger transfer counting algorithm was implemented to determine the number of transfers required between stations from each origin to each destination. A passenger origin–destination transfer matrix was created using the NetBeans integrated development environment to help determine the number of transfers required to complete trips on the transit network before and after proposing the new line. Based on the evaluation, the ring lines were highly efficient at significantly decreasing passenger transfers between stations with the minimum construction cost. This study will be of value during the strategic stages of the transit line design and will assist in rapidly generating initial solutions when certain demand information is unavailable.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309179",
    "keywords": [
      "Bus network",
      "Computer hardware",
      "Computer network",
      "Computer science",
      "Control bus",
      "Engineering",
      "Geometry",
      "Line (geometry)",
      "Mathematics",
      "Network planning and design",
      "Node (physics)",
      "Parallel computing",
      "Public transport",
      "Rail transit",
      "Structural engineering",
      "System bus",
      "TRIPS architecture",
      "Transfer (computing)",
      "Transit (satellite)",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Owais",
        "given_name": "Mahmoud"
      },
      {
        "surname": "Ahmed",
        "given_name": "Abdou S."
      },
      {
        "surname": "Moussa",
        "given_name": "Ghada S."
      },
      {
        "surname": "Khalil",
        "given_name": "Ahmed A."
      }
    ]
  },
  {
    "title": "Deep and machine learning techniques for medical imaging-based breast cancer: A comprehensive review",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114161",
    "abstract": "Breast cancer is the second leading cause of death for women, so accurate early detection can help decrease breast cancer mortality rates. Computer-aided detection allows radiologists to detect abnormalities efficiently. Medical images are sources of information relevant to the detection and diagnosis of various diseases and abnormalities. Several modalities allow radiologists to study the internal structure, and these modalities have been met with great interest in several types of research. In some medical fields, each of these modalities is of considerable significance. This study aims at presenting a review that shows the new applications of machine learning and deep learning technology for detecting and classifying breast cancer and provides an overview of progress in this area. This review reflects on the classification of breast cancer utilizing multi-modalities medical imaging. Details are also given on techniques developed to facilitate the classification of tumors, non-tumors, and dense masses in various medical imaging modalities. It first provides an overview of the different approaches to machine learning, then an overview of the different deep learning techniques and specific architectures for the detection and classification of breast cancer. We also provide a brief overview of the different image modalities to give a complete overview of the area. In the same context, this review was performed using a broad variety of research databases as a source of information for access to various field publications. Finally, this review summarizes the future trends and challenges in the classification and detection of breast cancer.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309015",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Breast cancer",
      "Breast imaging",
      "Cancer",
      "Cancer detection",
      "Computer science",
      "Context (archaeology)",
      "Deep learning",
      "Internal medicine",
      "Machine learning",
      "Mammography",
      "Medical imaging",
      "Medical physics",
      "Medicine",
      "Modalities",
      "Modality (human–computer interaction)",
      "Paleontology",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Houssein",
        "given_name": "Essam H."
      },
      {
        "surname": "Emam",
        "given_name": "Marwa M."
      },
      {
        "surname": "Ali",
        "given_name": "Abdelmgeid A."
      },
      {
        "surname": "Suganthan",
        "given_name": "Ponnuthurai Nagaratnam"
      }
    ]
  },
  {
    "title": "Two-echelon collaborative multi-depot multi-period vehicle routing problem",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114201",
    "abstract": "Collaboration among logistics operators offers an effective way to improve customer service and freight transportation efficiency. One form of collaboration is the sharing of logistics resources (e.g., delivery vehicles). Existing studies on collaboration and resource sharing have not sufficiently accounted for the time frame within which collaboration happens, and they mostly assume that collaboration among logistics operators occurs in a single time period. This study addresses the issue of collaboration across multiple time periods, in which logistics resources can be shared between different service time periods, by formulating and solving a two-echelon collaborative multi-depot multi-period vehicle routing problem (2E-CMDPVRP). The 2E-CMDPVRP is formulated as a multi-objective integer programming model that minimizes logistics operational costs, service waiting times, and number of vehicles in multiple service periods. A hybrid heuristic algorithm with three-dimensional k-means clustering and improved reference point-based non-dominated sorting genetic algorithm-III (IR-NSGA-III) is proposed to solve the multi-objective optimization model. Comparative analysis results show that the proposed IR-NSGA-III outperforms existing algorithms in terms of the minimization of logistics operational costs, service waiting times, and number of vehicles. The minimum costs remaining saving method and strictly monotonic path selection principle are combined to determine the best profit allocation schemes and the optimal coalition sequences. An empirical case study of a multi-depot multi-period logistics network in Chongqing, China, is used to validate the proposed model and solution algorithm. Results suggest that the proposed collaborative mechanism with multi-depot and multi-period resource sharing can improve the degree of synchronization within a collaborative logistics network, and thus contribute to sustainable development of urban logistics distribution networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309301",
    "keywords": [
      "Computer network",
      "Computer science",
      "Economics",
      "Economy",
      "Mathematical optimization",
      "Mathematics",
      "Operations research",
      "Routing (electronic design automation)",
      "Service (business)",
      "Vehicle routing problem"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yong"
      },
      {
        "surname": "Li",
        "given_name": "Qin"
      },
      {
        "surname": "Guan",
        "given_name": "Xiangyang"
      },
      {
        "surname": "Xu",
        "given_name": "Maozeng"
      },
      {
        "surname": "Liu",
        "given_name": "Yong"
      },
      {
        "surname": "Wang",
        "given_name": "Haizhong"
      }
    ]
  },
  {
    "title": "An extensible quality-related fault isolation framework based on dual broad partial least squares with application to the hot rolling process",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114166",
    "abstract": "The stability of product quality is a crucial issue in the process industries. Quality-related fault isolation usually assists in the real-time monitoring of process industries, thus allowing for better product quality and higher economic benefits. However, quality variables are usually difficult to be measured online due to economic and technical constraints, which makes traditional fault isolation methods inadequate for quality-related faults. In this work, an extensible quality-related fault isolation framework is proposed based on dual broad partial least squares (DBPLS). First, broad learning system (BLS) is integrated with partial least squares (PLS) to develop the offline model. Then, just-in-time-learning (JITL) PLS based on a new weighted similarity index is used for online soft sensor modeling. After that, the extensible version of DBPLS is given when new faults occur. In addition, an alternative formulation of DBPLS are further discussed. Finally, the proposed framework is applied to a real hot rolling process, where it can be found that DBPLS can extract the valuable information from the process variables related to quality variables and have better classification performance than other existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309040",
    "keywords": [
      "Actuator",
      "Art",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Data mining",
      "Dual (grammatical number)",
      "Engineering",
      "Epistemology",
      "Extensibility",
      "Fault (geology)",
      "Fault detection and isolation",
      "Geology",
      "Geometry",
      "Image (mathematics)",
      "Isolation (microbiology)",
      "Literature",
      "Machine learning",
      "Mathematics",
      "Microbiology",
      "Operating system",
      "Partial least squares regression",
      "Philosophy",
      "Process (computing)",
      "Product (mathematics)",
      "Quality (philosophy)",
      "Reliability engineering",
      "Seismology",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Chuanfang"
      },
      {
        "surname": "Peng",
        "given_name": "Kaixiang"
      },
      {
        "surname": "Dong",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "A hyper-heuristic based ensemble genetic programming approach for stochastic resource constrained project scheduling problem",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114174",
    "abstract": "In project scheduling studies, to the best of our knowledge, the hyper-heuristic collaborative scheduling is first-time applied to project scheduling with random activity durations. A hyper-heuristic based ensemble genetic programming (HH-EGP) method is proposed for solving stochastic resource constrained project scheduling problem (SRCPSP) by evolving an ensemble of priority rules (PRs). The proposed approach features with (1) integrating the critical path method into the resource-based policy class to generate schedules; (2) improving the existing single hyper-heuristic project scheduling research to construct a suitable solution space for solving SRCPSP; and (3) bettering genetic evolution of each subpopulation from a decision ensemble with three different local searches in corporation with discriminant mutation and discriminant population renewal. In addition, a sequence voting mechanism is designed to deal with collaborative decision-making in the scheduling process for SRCPSP. The benchmark PSPLIB is performed to verify the advantage of the HH-EGP over heuristics, meta-heuristics and the single hyper-heuristic approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309118",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Critical path method",
      "Dynamic priority scheduling",
      "Engineering",
      "Genetic programming",
      "Heuristics",
      "Job shop scheduling",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Schedule",
      "Scheduling (production processes)",
      "Systems engineering"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "HaoJie"
      },
      {
        "surname": "Ding",
        "given_name": "Guofu"
      },
      {
        "surname": "Qin",
        "given_name": "Shengfeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "Multi-period portfolio optimization using coherent fuzzy numbers in a credibilistic environment",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114135",
    "abstract": "In this paper, we use an extension of fuzzy numbers, called coherent fuzzy numbers, to model asset returns and an investor’s perception of the stock market (pessimistic, optimistic, or neutral) simultaneously. Two multi-period multi-objective portfolio optimization models are formulated using mean absolute semi-deviation and Conditional Value-at-Risk (CVaR) as risk measures, respectively. We aim to provide more flexibility to the investor in specifying the risk tolerance and devise optimum investment plans for different investment horizons. The proposed models also incorporate bound, cardinality, and skewness constraints for each investment period to capture various stock market scenarios. A real-coded genetic algorithm is employed to solve the resultant models. Two real-life case studies involving 20 assets of the National Stock Exchange (NSE), India, and another involving 50 assets listed in the S&P 500 and NASDAQ-100 indexes have been provided to illustrate the efficacy and advantages of the models. An in-sample and out-of-sample analysis have been done for both the models to analyze the performance in the real-world scenario. The conclusion drawn from the analysis strongly emphasizes on accurately assessing the current stock market prospects, i.e., adopting the right attitude (pessimistic, optimistic, or neutral), is of paramount importance and must be included in the portfolio optimization problem.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308836",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "CVAR",
      "Computer science",
      "Econometrics",
      "Economics",
      "Engineering",
      "Expected shortfall",
      "Financial economics",
      "Fuzzy logic",
      "Horse",
      "Mathematical optimization",
      "Mathematics",
      "Mechanical engineering",
      "Paleontology",
      "Portfolio",
      "Portfolio optimization",
      "Stock (firearms)",
      "Stock market"
    ],
    "authors": [
      {
        "surname": "Gupta",
        "given_name": "Pankaj"
      },
      {
        "surname": "Mehlawat",
        "given_name": "Mukesh Kumar"
      },
      {
        "surname": "Khan",
        "given_name": "Ahmad Zaman"
      }
    ]
  },
  {
    "title": "Representational primitives using trend based global features for time series classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114376",
    "abstract": "Feature based learning of time series sequences contains a systematic step of preprocessing, representing and analyzing the properties of time series elements. Representational features include the mapping of time series properties namely trend, seasonality and stationarity. Usually, the segmented generation of representational structures does not contain the global features of a time series sequence which can influence the learning algorithms. Global information of each time series sequence reinforces the respective segmental properties present in it. Identifying, extracting and processing of global features which are common to all time series sequences are challenging tasks in time series feature learning. Hence, we propose a novel set of global features which provides an additional representational leverage to feature based time series learning scenarios. The feature enriched primitives can provide an additional information on the global trend pattern in each of the time series sequences. This enables the learning algorithms to process the time series sequences with the awareness of trend information. We formed a minimum number of most influential trend features which describe the behavior of time series sequences. Thus the dimensionality of the features are preserved which influence the performance of various learning algorithms. The experiments on this novel representational structures are performed on UCR-2018 time series archive which contains 128 datasets. We also represented the trend sequences in a pictorial form named positional size diagram (PSD) and aggregated all the instances of the datasets into an auxiliary data representation named positional dataset (PD). We compared six traditional classification algorithms namely k-nearest neighbor (k-NN), logistic regression (LR), support vector (SV), decision tree (DC), gaussian naive bayes (GNB) and random forest (RF) with trendlets. The additional set of global features enrich the trendlets with supplementary information about the trend of time series sequences. The classification accuracy of the aforementioned algorithms shows a significant improvement with this additional set of global features.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310526",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Data mining",
      "Machine learning",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Series (stratigraphy)",
      "Time series"
    ],
    "authors": [
      {
        "surname": "C.I.",
        "given_name": "Johnpaul"
      },
      {
        "surname": "Prasad",
        "given_name": "Munaga V.N.K."
      },
      {
        "surname": "Nickolas",
        "given_name": "S."
      },
      {
        "surname": "Gangadharan",
        "given_name": "G.R."
      }
    ]
  },
  {
    "title": "An unsupervised method for extractive multi-document summarization based on centroid approach and sentence embeddings",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114152",
    "abstract": "Extractive multi-document summarization (MDS) is the process of automatically summarizing a collection of documents by ranking sentences according to their importance and informativeness. Text representation is a fundamental process that affects the effectiveness of many text summarization methods. Word embedding representations have shown to be effective for several Natural Language Processing (NLP) tasks including Automatic Text Summarization (ATS). However, most of these representations do not consider the order and the semantic relationships between words in a sentence. This does not fully allow grasping the sentence semantics and the syntactic relationships between sentences constituents. In this paper, to overcome this problem, we propose an unsupervised method for generic extractive multi-document summarization based on the sentence embedding representations and the centroid approach. The proposed method selects relevant sentences according to the final score obtained by combining three scores: sentence content relevance, sentence novelty, and sentence position scores. The sentence content relevance score is computed as the cosine similarity between the centroid embedding vector of the cluster of documents and the sentence embedding vectors. The sentence novelty metric is explicitly adopted to deal with redundancy. The sentence position metric assumes that the first sentences of a document are more relevant to the summary, and it assigns high scores to these sentences. Moreover, this paper provides a comparative analysis of nine sentence embedding models used to represent sentences as dense vectors in a low dimensional vector space in the context of extractive multi-document summarization. Experiments are performed on the standard DUC’2002–2004 benchmark datasets and the Multi-News dataset. The overall obtained results have shown that our method outperforms several state-of-the-art methods and achieves promising results compared to the best performing methods including supervised deep learning based methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308952",
    "keywords": [
      "Artificial intelligence",
      "Automatic summarization",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Cosine similarity",
      "Embedding",
      "Information retrieval",
      "Law",
      "Natural language processing",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Political science",
      "Relevance (law)",
      "Sentence"
    ],
    "authors": [
      {
        "surname": "Lamsiyah",
        "given_name": "Salima"
      },
      {
        "surname": "El Mahdaouy",
        "given_name": "Abdelkader"
      },
      {
        "surname": "Espinasse",
        "given_name": "Bernard"
      },
      {
        "surname": "El Alaoui Ouatik",
        "given_name": "Saïd"
      }
    ]
  },
  {
    "title": "YAC2: An α-proximity based clustering algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114138",
    "abstract": "Clustering is the process of identifying objects with similar intrinsic properties and grouping them together into separate clusters. It also serves as a preliminary step in data classification by separating heterogeneous data into reasonably homogeneous groups, which can be further processed. Existing literature discusses a host of different clustering algorithms and their applications, albeit no single approach for all applications has yet emerged. In this paper, we introduce a novel clustering algorithm, YAC2, based on data binning and α-proximity/neighborhood. The binning process is a data transformation step that converts cardinal values of the attributes into their ordinal equivalence. The algorithm introduces a specialized centroid that is used with the α-proximity and a matching algorithm to partition data set into a sequentially generated clusters. We present the results of applying YAC2 to a set of established benchmark data sets, using a host of evaluation metrics. Our results show YAC2 to perform well besting a number of well-established algorithms. For several metrics, YAC2 has provided improvements averagely in the range of 6% (in the Iris data set)-180% (in the Wholesale Customer data set) over other algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030885X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "CURE data clustering algorithm",
      "Centroid",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Data set",
      "Geodesy",
      "Geography",
      "Mathematics",
      "Partition (number theory)",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Ghiassi",
        "given_name": "M."
      },
      {
        "surname": "Saidane",
        "given_name": "H."
      },
      {
        "surname": "Oswal",
        "given_name": "R."
      }
    ]
  },
  {
    "title": "A concrete reformulation of fuzzy arithmetic",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113818",
    "abstract": "Advancement in fuzzy arithmetic is essential to advancing the applicability of fuzzy set theory (FST) to approximate reasoning problems arose in engineering, medicine, managerial science and many other domains. In the recent FST literature, many of the noteworthy progresses in fuzzy arithmetic have been built upon increasingly sophisticated mathematical concepts and structures, leading to highly non-trivial results. In this article, we take a different approach, of offering a simple, concrete framework that could serve as an entry point for the classically-trained engineers (those who might not be well-acquainted with deep theoretical FST advances) to understand and use fuzzy arithmetic in their applications. Specifically, the contributions of this work are: (i) We extend a foundational model previously proposed in Ngan (2018) to arrive at a simple fuzzy arithmetic framework, in which each key element within the framework possesses simple, concrete meaning to these classically-trained engineers; (ii) We describe how to use these key elements as building blocks to create various concrete fuzzy arithmetic operators that are adaptive to various assumptions. This means that the classically-trained engineers can now easily customize and use fuzzy arithmetic robustly in their application domains; (iii) We demonstrate that the proposed framework can overcome the possibility of generating pathological results that were seen in other fuzzy arithmetic approaches in the FST literature; (iv) We demonstrate the utility of the proposed framework in solving multiple-criteria-decision-making (MCDM) problems concretely and in an easy-to-interpret manner (which are highly important considerations for the users of MCDM). Last but not least, we offer a new avenue for extending this fuzzy arithmetic framework to higher-order fuzzy settings, such as the type-2 fuzzy and other type-2-like fuzzy settings. Thus, our framework can potentially provide a unified, robust approach for developing fuzzy arithmetic in a wide range of FST settings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306308",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Epistemology",
      "Fuzzy logic",
      "Fuzzy set",
      "Geometry",
      "Key (lock)",
      "Mathematics",
      "Philosophy",
      "Point (geometry)",
      "Simple (philosophy)"
    ],
    "authors": [
      {
        "surname": "Ngan",
        "given_name": "Shing-Chung"
      }
    ]
  },
  {
    "title": "Granular fuzzy PID controller",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114182",
    "abstract": "In this paper an uncertain dynamical system is investigated in which the coefficients are as a class of fuzzy sets and the fuzzy derivative is considered as the granular derivative. Furthermore, the notions of granular second order derivative of a fuzzy function, fuzzy overshoot, fuzzy rise-time, and fuzzy peak-time are introduced. As a result, designing a class of PID controllers called granular Fuzzy PID (gr-FPID) controller is presented based on fuzzy mathematics. The gr-FPID consists of granular integral, granular derivative with fuzzy coefficients. Moreover, the Particle Swarm Optimization (PSO) algorithm is used to tune gr-FPID fuzzy coefficients. It is demonstrated that the gr-FPID controller can effectively control the temperature in a continuous stirred tank reactor in which the parameters are uncertain.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309155",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Control (management)",
      "Control engineering",
      "Control theory (sociology)",
      "Controller (irrigation)",
      "Defuzzification",
      "Derivative (finance)",
      "Economics",
      "Engineering",
      "Financial economics",
      "Fuzzy control system",
      "Fuzzy logic",
      "Fuzzy number",
      "Fuzzy set",
      "Mathematical optimization",
      "Mathematics",
      "Overshoot (microwave communication)",
      "PID controller",
      "Particle swarm optimization",
      "Telecommunications",
      "Temperature control"
    ],
    "authors": [
      {
        "surname": "Najariyan",
        "given_name": "Marzieh"
      },
      {
        "surname": "Zhao",
        "given_name": "Yi"
      }
    ]
  },
  {
    "title": "A population based hybrid FCM-PSO algorithm for clustering analysis and segmentation of brain image",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114121",
    "abstract": "Fuzzy c-means (FCM) is a well-known unsupervised clustering algorithm based on fuzzy logic and used in many applications. However, it has some disadvantages. One disadvantage of FCM is that, while dealing with complex problems such as medical image data, it is frequently trapped into local minima during execution, which leads the undesired clustering results. Particle swarm optimization (PSO) is a population based metaheuristic optimization algorithm regarded as a global search approach and used in many optimization problems. To overcome the problem in FCM and in order to achieve better results, a hybrid FCM-PSO algorithm has been proposed by combining the excellent features of FCM and PSO algorithms. The experiment has been executed on a triangular dataset and publicly available real brain datasets and compared their results numerically and visually. The obtained experimental results demonstrate the efficacy of the proposed hybrid FCM-PSO algorithm. Friedman’s statistical test is also carried out to demonstrate the statistically significant performance of all discussed algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308691",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Demography",
      "Image (mathematics)",
      "Image segmentation",
      "Particle swarm optimization",
      "Pattern recognition (psychology)",
      "Population",
      "Segmentation",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Verma",
        "given_name": "Hanuman"
      },
      {
        "surname": "Verma",
        "given_name": "Deepa"
      },
      {
        "surname": "Tiwari",
        "given_name": "Pawan Kumar"
      }
    ]
  },
  {
    "title": "initKmix-A novel initial partition generation algorithm for clustering mixed data using k-means-based clustering",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114149",
    "abstract": "Mixed datasets consist of both numeric and categorical attributes. Various k-means-based clustering algorithms have been developed for these datasets. Generally, these algorithms use random partition as a starting point, which tends to produce different clustering results for different runs. In this paper, we propose, initKmix, a novel algorithm for finding an initial partition for k-means-based clustering algorithms for mixed datasets. In the initKmix algorithm, a k-means-based clustering algorithm is run many times, and in each run, one of the attributes is used to create initial clusters for that run. The clustering results of various runs are combined to produce the initial partition. This initial partition is then used as a seed to a k-means-based clustering algorithm to cluster mixed data. Experiments with various categorical and mixed datasets showed that initKmix produced accurate and consistent results, and outperformed the random initial partition method and other state-of-the-art initialization methods. Experiments also showed that k-means-based clustering for mixed datasets with initKmix performed similar to or better than many state-of-the-art clustering algorithms for categorical and mixed datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308939",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Canopy clustering algorithm",
      "Categorical variable",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Correlation clustering",
      "Data mining",
      "Data stream clustering",
      "Fuzzy clustering",
      "Initialization",
      "Machine learning",
      "Mathematics",
      "Partition (number theory)",
      "Programming language",
      "Single-linkage clustering"
    ],
    "authors": [
      {
        "surname": "Ahmad",
        "given_name": "Amir"
      },
      {
        "surname": "Khan",
        "given_name": "Shehroz S."
      }
    ]
  },
  {
    "title": "Tree-RNN: Tree structural recurrent neural network for network traffic classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114363",
    "abstract": "Network traffic classification plays an important role in network monitoring and network management. With the continuous development of network technology, traditional methods of traffic classification have more limitations in accuracy to deal with encrypted traffic. Fortunately, deep neural network (DNN) is an effective method for handling traffic classification due to its ability to learn inherent data features. However, this method generally classifies network traffic with only the single classifier, which makes it relatively less effective in some classes for the problem of large classification. In this paper, we propose a tree structural recurrent neural network (Tree-RNN), which divides a large classification into small classifications by using the tree structure. A specific classifier is set for each small classification after division. With multiple classifiers employed, Tree-RNN can complement each other in classification performance, and the problem of the single classifier is solved. Since multiple classifiers are all end-to-end frameworks, Tree-RNN can automatically learn the nonlinear relationship between input data and output data without feature extraction. To verify the validity of our model, we compare Tree-RNN with state-of-the-art methods using the ISCX public traffic dataset. Experimental results show that Tree-RNN can achieve higher performance in less training time. The average accuracy of Tree-RNN is 4.88% higher than other state-of-the-art methods, and it has higher average precision and average recall.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310435",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Binary tree",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Decision tree",
      "Decision tree learning",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Recurrent neural network",
      "The Internet",
      "Traffic classification",
      "Tree (set theory)",
      "Tree structure",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Ren",
        "given_name": "Xinming"
      },
      {
        "surname": "Gu",
        "given_name": "Huaxi"
      },
      {
        "surname": "Wei",
        "given_name": "Wenting"
      }
    ]
  },
  {
    "title": "A state-of-the-art review on mobile robotics tasks using artificial intelligence and visual data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114195",
    "abstract": "Nowadays, the field of mobile robotics has experienced an important evolution and these robots are more commonly proposed to solve different tasks autonomously. The use of visual sensors has played an important role in mobile robotics tasks during the past few years due to the advances in computer vision hardware and algorithms. It is worth remarking the use of AI tools to solve a variety of problems in mobile robotics based on the use of images either as the only source of information or combining them with other sensors such as laser or GPS. The improvement of the autonomy of mobile robots has attracted the attention of the scientific community. A considerable amount of works have been proposed over the past few years, leading to an extensive variety of approaches. Building a robust model of the environment (mapping), estimating the position within the model (localization) and controlling the movement of the robot from one place to another (navigation) are important abilities that any mobile robot must have. Considering this, this review focuses on analyzing these problems; how researchers have addressed them by means of AI tools and visual information; and how these approaches have evolved in recent years. This topic is currently open and a large number of works can be found in the related literature. Therefore, it can be of interest making an analysis of the current state of the topic. From this review, we can conclude that AI has provided robust solutions to some specific tasks in mobile robotics, such as information retrieval from scenes, mapping, localization and exploration. However, it is worth continuing to develop this line of research to find more integral solutions to the navigation problem so that mobile robots can increase their autonomy in large, complex and heterogeneous environments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030926X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Field (mathematics)",
      "Human–computer interaction",
      "Mathematics",
      "Mobile robot",
      "Pure mathematics",
      "Robot",
      "Robotics",
      "Variety (cybernetics)"
    ],
    "authors": [
      {
        "surname": "Cebollada",
        "given_name": "Sergio"
      },
      {
        "surname": "Payá",
        "given_name": "Luis"
      },
      {
        "surname": "Flores",
        "given_name": "María"
      },
      {
        "surname": "Peidró",
        "given_name": "Adrián"
      },
      {
        "surname": "Reinoso",
        "given_name": "Oscar"
      }
    ]
  },
  {
    "title": "American sign language recognition and training method with recurrent neural network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114403",
    "abstract": "Though American sign language (ASL) has gained recognition from the American society, few ASL applications have been developed with educational purposes. Those designed with real-time sign recognition systems are also lacking. Leap motion controller facilitates the real-time and accurate recognition of ASL signs. It allows an opportunity for designing a learning application with a real-time sign recognition system that seeks to improve the effectiveness of ASL learning. The project proposes an ASL learning application prototype. The application would be a whack-a-mole game with a real-time sign recognition system embedded. Since both static and dynamic signs (J, Z) exist in ASL alphabets, Long-Short Term Memory Recurrent Neural Network with k-Nearest-Neighbour method is adopted as the classification method is based on handling of sequences of input. Characteristics such as sphere radius, angles between fingers and distance between finger positions are extracted as input for the classification model. The model is trained with 2600 samples, 100 samples taken for each alphabet. The experimental results revealed that the recognition rate for 26 ASL alphabets yields an average of 99.44% accuracy rate and 91.82% in 5-fold cross-validation with the use of leap motion controller.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310745",
    "keywords": [
      "Alphabet",
      "American Sign Language",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Motion (physics)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Recurrent neural network",
      "Sign (mathematics)",
      "Sign language",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "C.K.M."
      },
      {
        "surname": "Ng",
        "given_name": "Kam K.H."
      },
      {
        "surname": "Chen",
        "given_name": "Chun-Hsien"
      },
      {
        "surname": "Lau",
        "given_name": "H.C.W."
      },
      {
        "surname": "Chung",
        "given_name": "S.Y."
      },
      {
        "surname": "Tsoi",
        "given_name": "Tiffany"
      }
    ]
  },
  {
    "title": "A robust cross-efficiency data envelopment analysis model with undesirable outputs",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114117",
    "abstract": "Degenerate optimal weights and uncertain data are two challenging problems in conventional data envelopment analysis (DEA). Cross-efficiency and robust optimization are commonly used to handle such problems. We develop two DEA adaptations to rank decision-making units (DMUs) characterized by uncertain data and undesirable outputs. The first adaptation is an interval approach, where we propose lower- and upper-bounds for the efficiency scores and apply a robust cross-efficiency model to avoid problems of non-unique optimal weights and uncertain data. We initially use the proposed interval approach and categorize DMUs into fully efficient, efficient, and inefficient groups. The second adaptation is a robust approach, where we rank the DMUs, with a measure of cross-efficiency that extends the traditional classification of efficient and inefficient units. Results show that we can obtain higher discriminatory power and higher-ranking stability compared with the interval models. We present an example from the literature and a real-world application in the banking industry to demonstrate this capability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308666",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Data envelopment analysis",
      "Data mining",
      "Interval (graph theory)",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Measure (data warehouse)",
      "Rank (graph theory)",
      "Ranking (information retrieval)",
      "Stability (learning theory)",
      "Uncertain data"
    ],
    "authors": [
      {
        "surname": "Tavana",
        "given_name": "Madjid"
      },
      {
        "surname": "Toloo",
        "given_name": "Mehdi"
      },
      {
        "surname": "Aghayi",
        "given_name": "Nazila"
      },
      {
        "surname": "Arabmaldar",
        "given_name": "Aliasghar"
      }
    ]
  },
  {
    "title": "A multi-objective co-evolutionary algorithm of scheduling on parallel non-identical batch machines",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114145",
    "abstract": "This paper investigates the problem scheduling a set of jobs on parallel batch processing machines with different capacities and non-identical processing powers for minimizing the makespan and the total energy consumption, where the jobs have non-identical sizes, dynamical arrival time and different processing time. To address the bi-objective optimization problem, a three-populations co-evolutionary algorithm is proposed, which is based on exploration and coordination searches among three colonies. For guaranteeing the diversity of solutions, an adaptive search strategy based on the largest angle among adjacent solutions is designed, and a new method is proposed to select ants to update pheromone trails for improving the convergence of solutions. Finally, the proposed algorithm is compared with the existing multi-objective algorithms through extensive simulated experiments, and the simulated results are statistically analyzed. And the experimental results show that the proposed algorithm outperforms all the compared algorithms, which verify the validity of the algorithm proposed in this paper.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308915",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Evolutionary algorithm",
      "Job shop scheduling",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Programming language",
      "Schedule",
      "Scheduling (production processes)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yan"
      },
      {
        "surname": "Jia",
        "given_name": "Zhao-hong"
      },
      {
        "surname": "Li",
        "given_name": "Kai"
      }
    ]
  },
  {
    "title": "Sentiment analysis and its applications in fighting COVID-19 and infectious diseases: A systematic review",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114155",
    "abstract": "The COVID-19 pandemic caused by the novel coronavirus SARS-CoV-2 occurred unexpectedly in China in December 2019. Tens of millions of confirmed cases and more than hundreds of thousands of confirmed deaths are reported worldwide according to the World Health Organisation. News about the virus is spreading all over social media websites. Consequently, these social media outlets are experiencing and presenting different views, opinions and emotions during various outbreak-related incidents. For computer scientists and researchers, big data are valuable assets for understanding people’s sentiments regarding current events, especially those related to the pandemic. Therefore, analysing these sentiments will yield remarkable findings. To the best of our knowledge, previous related studies have focused on one kind of infectious disease. No previous study has examined multiple diseases via sentiment analysis. Accordingly, this research aimed to review and analyse articles about the occurrence of different types of infectious diseases, such as epidemics, pandemics, viruses or outbreaks, during the last 10 years, understand the application of sentiment analysis and obtain the most important literature findings. Articles on related topics were systematically searched in five major databases, namely, ScienceDirect, PubMed, Web of Science, IEEE Xplore and Scopus, from 1 January 2010 to 30 June 2020. These indices were considered sufficiently extensive and reliable to cover our scope of the literature. Articles were selected based on our inclusion and exclusion criteria for the systematic review, with a total of n = 28 articles selected. All these articles were formed into a coherent taxonomy to describe the corresponding current standpoints in the literature in accordance with four main categories: lexicon-based models, machine learning-based models, hybrid-based models and individuals. The obtained articles were categorised into motivations related to disease mitigation, data analysis and challenges faced by researchers with respect to data, social media platforms and community. Other aspects, such as the protocol being followed by the systematic review and demographic statistics of the literature distribution, were included in the review. Interesting patterns were observed in the literature, and the identified articles were grouped accordingly. This study emphasised the current standpoint and opportunities for research in this area and promoted additional efforts towards the understanding of this research field.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308988",
    "keywords": [
      "Artificial intelligence",
      "Bibliometrics",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Data science",
      "Disease",
      "Infectious disease (medical specialty)",
      "Law",
      "MEDLINE",
      "Medicine",
      "Meta-analysis",
      "Outbreak",
      "Pandemic",
      "Pathology",
      "Political science",
      "Scopus",
      "Sentiment analysis",
      "Social media",
      "Web of science",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Alamoodi",
        "given_name": "A.H."
      },
      {
        "surname": "Zaidan",
        "given_name": "B.B."
      },
      {
        "surname": "Zaidan",
        "given_name": "A.A."
      },
      {
        "surname": "Albahri",
        "given_name": "O.S."
      },
      {
        "surname": "Mohammed",
        "given_name": "K.I."
      },
      {
        "surname": "Malik",
        "given_name": "R.Q."
      },
      {
        "surname": "Almahdi",
        "given_name": "E.M."
      },
      {
        "surname": "Chyad",
        "given_name": "M.A."
      },
      {
        "surname": "Tareq",
        "given_name": "Z."
      },
      {
        "surname": "Albahri",
        "given_name": "A.S."
      },
      {
        "surname": "Hameed",
        "given_name": "Hamsa"
      },
      {
        "surname": "Alaa",
        "given_name": "Musaab"
      }
    ]
  },
  {
    "title": "An AIC-based approach to identify the most influential variables in eco-efficiency evaluation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113883",
    "abstract": "Eco-efficiency evaluation has received increasing public attention and plays an important role in the business community. In many practical applications, the decision-makers are interested in which eco-variables take a significant effect on eco-efficiency evaluation and how to select proper variables in situations where there are a large number of alternative variables. This paper approaches these problems based upon the Akaike information criteria (AIC) rule. The proposed approach can investigate all possible variable sets and identify the most influential variables. A real data set about 30 industrial systems in China has been used to illustrate the proposed approach. We find the most influential undesirable output in determining provincial industrial systems' eco-efficiency of China is Sulphur dioxide emission. This result is robust under different eco-efficiency measurements. It is of great significance for decision-makers to achieve eco-efficiency improvement.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306874",
    "keywords": [
      "Akaike information criterion",
      "Biology",
      "Computer science",
      "Data mining",
      "Eco-efficiency",
      "Ecology",
      "Econometrics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Operations research",
      "Programming language",
      "Set (abstract data type)",
      "Sustainable development",
      "Variable (mathematics)",
      "Variables"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yongjun"
      },
      {
        "surname": "Zhang",
        "given_name": "Qian"
      },
      {
        "surname": "Wang",
        "given_name": "Lizheng"
      },
      {
        "surname": "Liang",
        "given_name": "Liang"
      }
    ]
  },
  {
    "title": "Reactive heuristics for disrupted multi-mode resource-constrained project scheduling problem",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114132",
    "abstract": "This paper accounts for the mode change disruption in the Multi-mode Resource-Constrained Project Scheduling Problem (MRCPSP). In fact, during its execution, the project may face some unexpected events, which can lead to the schedule deterioration or even unfeasibility. A reactive mathematical modeling is proposed. Moreover, three heuristics with several variants are suggested to repair the initial disrupted schedule. These heuristics aim to perform a very quick response to provide the project manager with, hopefully, a feasible schedule. An extensive computational study is performed on PSPLIB benchmark instances with complementary generated data. Finally, the experimental results have shown that the three heuristics outperform Cplex in terms of the CPU running time. Moreover, Lexicographic Regret-Based Heuristic (LRBH) outranks the other heuristics in finding feasible solutions and in terms of the makespan deviation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308800",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Combinatorics",
      "Computer science",
      "Geodesy",
      "Geography",
      "Heuristic",
      "Heuristics",
      "Job shop scheduling",
      "Lexicographical order",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Regret",
      "Schedule",
      "Scheduling (production processes)"
    ],
    "authors": [
      {
        "surname": "Elloumi",
        "given_name": "Sonda"
      },
      {
        "surname": "Loukil",
        "given_name": "Taïcir"
      },
      {
        "surname": "Fortemps",
        "given_name": "Philippe"
      }
    ]
  },
  {
    "title": "Evaluation of text summaries without human references based on the linear optimization of content metrics using a genetic algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113827",
    "abstract": "The Evaluation of Text Summaries (ETS) has been a task of constant challenges to the development of Automatic Text Summarization (ATS). Within the ATS task, the ETS is crucial to determine the performance of text summaries. Over the last two decades, the scientific community has used the ROUGE system as a standard package to assess the content of automatic summaries. However, if there are not human-made summaries (called human references), then the evaluation cannot be carried out. For this reason, the different state-of-the-art evaluation methods have been proposed that analyze the summary content using the source documents. Nonetheless, these methods do not highly correlate with human assessment. In this paper, a linear optimization of content-based metrics is proposed using a Genetic Algorithm (GA) to improve the correlation between automatic and manual evaluation. The proposed method combines 31 content metrics based on the evaluation without human references. The results of the linear optimization show correlation improvements concerning other evaluation metrics on DUC01 and DUC02 datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306394",
    "keywords": [
      "Artificial intelligence",
      "Automatic summarization",
      "Computer science",
      "Constant (computer programming)",
      "Content (measure theory)",
      "Data mining",
      "Economics",
      "Genetic algorithm",
      "Information retrieval",
      "Linear correlation",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Programming language",
      "Statistics",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Rojas-Simón",
        "given_name": "Jonathan"
      },
      {
        "surname": "Ledeneva",
        "given_name": "Yulia"
      },
      {
        "surname": "García-Hernández",
        "given_name": "René Arnulfo"
      }
    ]
  },
  {
    "title": "On novelty detection for multi-class classification using non-linear metric learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114193",
    "abstract": "Novelty detection is a binary task aimed at identifying whether a test sample is novel or unusual compared to a previously observed training set. A typical approach is to consider distance as a criterion to detect such novelties. However, most previous work does not focus on finding an optimum distance for each particular problem. In this paper, we propose to detect novelties by exploiting non-linear distances learned from multi-class training data. For this purpose, we adopt a kernelization technique jointly with the Large Margin Nearest Neighbor (LMNN) metric learning algorithm. The optimum distance tries to keep each known class’ instances together while pushing instances from different known classes to remain reasonably distant. We propose a variant of the K-Nearest Neighbors (KNN) classifier that employs the learned distance to detect novelties. Besides, we use the learned distance to perform multi-class classification. We show quantitative and qualitative experiments conducted on synthetic and real data sets, revealing that the learned metrics are effective in improving novelty detection compared to other metrics. Our method also outperforms previous work regularly used for novelty detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309258",
    "keywords": [
      "Artificial intelligence",
      "Binary classification",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Economics",
      "Machine learning",
      "Margin (machine learning)",
      "Metric (unit)",
      "Novelty",
      "Novelty detection",
      "One-class classification",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Support vector machine",
      "Theology",
      "k-nearest neighbors algorithm"
    ],
    "authors": [
      {
        "surname": "Silva",
        "given_name": "Samuel Rocha"
      },
      {
        "surname": "Vieira",
        "given_name": "Thales"
      },
      {
        "surname": "Martínez",
        "given_name": "Dimas"
      },
      {
        "surname": "Paiva",
        "given_name": "Afonso"
      }
    ]
  },
  {
    "title": "Hybrid microblog recommendation with heterogeneous features using deep neural network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114191",
    "abstract": "With the development of mobile Internet, microblog has become one of the most popular social platforms. The enormous user-generated microblogs have caused the problem of information overload, which makes users difficult to find the microblogs they actually need. Hence, how to provide users with accurate microblogs has become a hot and urgent issue. In this paper, we propose an approach of hybrid microblog recommendation, which is developed on a framework of deep neural network with a group of heterogeneous features as its input. Specifically, two new recommendation strategies are first constructed in terms of the extended user-interest tags and user interest topics, respectively. These two strategies additionally with the collaborative filtering are employed together to obtain the candidate microblogs for final recommendation. Then, we propose the heterogeneous features related to personal interests of users, interest in authors and microblog quality to describe the candidate microblogs. Finally, a deep neural network with multiple hidden layers is designed to predict and rank the microblogs. Extensive experiments conducted on the datasets of Sina Weibo and Twitter indicate that our proposed approach significantly outperforms the state-of-the-art methods. The code and the two datasets of this paper are publicly available at GitHub.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309246",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Information retrieval",
      "Mathematics",
      "Microblogging",
      "Rank (graph theory)",
      "Recommender system",
      "Social media",
      "Social network (sociolinguistics)",
      "The Internet",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Jiameng"
      },
      {
        "surname": "Zhang",
        "given_name": "Chunxia"
      },
      {
        "surname": "Xu",
        "given_name": "Yanyan"
      },
      {
        "surname": "Luo",
        "given_name": "Meiqiu"
      },
      {
        "surname": "Niu",
        "given_name": "Zhendong"
      }
    ]
  },
  {
    "title": "A scheduling decision support model for minimizing the number of drones with dynamic package arrivals and personalized deadlines",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114157",
    "abstract": "Unmanned Aerial Vehicles (UAVs, commonly known as drones) hold great potential to reduce operational costs and guarantee on-time delivery of packages. This paper aims to minimize the number of drones used in a depot, in which each package has its own customized release time, distance to the depot, and personalized deadline. For decision-makers, it is difficult to determine the optimal number of drones to ensure that all packages can be delivered before the corresponding deadline. We propose a mixed integer programming model formulate the problem. Due to the NP-hardness of the problem, a scheduling decision support model with a genetic algorithm (SDSMGA) is developed to address the problem. A fitness function that can determine the minimum number of drones required by a package delivery sequence is proposed. We develop a swap-based correction algorithm to correct unqualified individuals in SDSMGA. Experimental results show that compared with CPLEX for small instances, SDSMGA can obtain solutions of the same quality or sub-optimal solutions. Computational results among SDSMGA, Estimation of Distribution Algorithm (EDA), and Particle Swarm Optimization (PSO) indicate that SDSMGA can effectively and efficiently address the problem. As the number of packages increases, SDSMGA outperforms the other two algorithms. Sensitivity analysis shows that the smaller the dense factor, or the more extensive the service radius, the more drones are needed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030899X",
    "keywords": [
      "Algorithm",
      "Biology",
      "Computer network",
      "Computer science",
      "Drone",
      "Economics",
      "Finance",
      "Genetics",
      "Job shop scheduling",
      "Mathematical optimization",
      "Mathematics",
      "Particle swarm optimization",
      "Routing (electronic design automation)",
      "Scheduling (production processes)",
      "Swap (finance)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Chuang"
      },
      {
        "surname": "Chen",
        "given_name": "Huaping"
      },
      {
        "surname": "Li",
        "given_name": "Xueping"
      },
      {
        "surname": "Liu",
        "given_name": "Zeyu"
      }
    ]
  },
  {
    "title": "A hybrid fine-tuned VMD and CNN scheme for untrained compound fault diagnosis of rotating machinery with unequal-severity faults",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114094",
    "abstract": "In the case of a compound fault diagnosis of rotating machinery, when two failures with unequal severity occur in distinct parts of the system, the detection of a minor fault is a complicated and challenging task. In this case, the minor fault is overshadowed by the more severe one, and the characteristics of the compound fault are prone to the more severe one. Generally, the proposed methods in the literature consider compound failure as an individual fault type and unrelated to the corresponding single faults, either at the different locations of a sensitive component or in two separate parts, such as the bearing and gear, with approximately the same fault severity. Considering these issues, this study proposes a novel end-to-end fault diagnosis method based on fine-tuned VMD and convolutional neural network (CNN). The main idea is that CNN is trained only on a healthy and single fault dataset, without the use of compound fault data in training. In the test stage of the CNN model, the intelligent method alarms an untrained compound fault state if acquired probabilities of CNN output satisfy a set of probabilistic conditions. The performance of the fine-tuned VMD and the proposed hybrid method is evaluated by the decomposition of a simulated vibration signal and the analysis of a gearbox system with a compound fault scenario in such a way that one fault is minor and the other severe. The results obtained show the high accuracy of the proposed method in compound fault diagnosis and the feature extraction and classification of a minor fault in the presence of a more severe one.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308496",
    "keywords": [
      "Actuator",
      "Artificial intelligence",
      "Bearing (navigation)",
      "Computer science",
      "Convolutional neural network",
      "Electrical engineering",
      "Electronic circuit",
      "Engineering",
      "Fault (geology)",
      "Fault coverage",
      "Fault detection and isolation",
      "Fault indicator",
      "Geology",
      "Pattern recognition (psychology)",
      "Seismology",
      "Stuck-at fault"
    ],
    "authors": [
      {
        "surname": "Dibaj",
        "given_name": "Ali"
      },
      {
        "surname": "Ettefagh",
        "given_name": "Mir Mohammad"
      },
      {
        "surname": "Hassannejad",
        "given_name": "Reza"
      },
      {
        "surname": "Ehghaghi",
        "given_name": "Mir Biuok"
      }
    ]
  },
  {
    "title": "Wavelet-based logistic discriminator of dermoscopy images",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113760",
    "abstract": "Proper diagnosis of cutaneous melanoma is a life-saving factor. The most important limitation is the early and sensitive recognition of melanoma relative to dysplastic nevi. We have studied wavelet-based features extracted from dermoscopic images as efficient signals of neoplastic changes. We recursively treat the dermoscopic images and all their transformation channels (wavelet packets) through the Mallat transform. All the four decomposition filters from each decomposition level are the source of features based on three functions of the pixel values. We train the logistic classifier regularized by either the Lasso or the Ridge penalty and test its AUC metric for a set of different wavelet bases, and as a function of image resolution. A total of three different data sets with respectively 185, 117, and 413 images, and 52 wavelet bases are tested. Classification performance as a function of the wavelet number strongly depends on the image resolution and image compression. There is also a large variation in the classification performance within the self same wavelet family. Degradation of image resolution makes the overall classification performance lower and more dispersed between the regularizes. Some wavelets do not lower, but increase the learning performance at the reduced image resolutions, which is consistent with the melanoma feature-extraction studies based on other learning paradigms. The logistic classifier can extract high-performance, resolution-invariant wavelet features of melanoma.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305844",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Contextual image classification",
      "Feature extraction",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Stationary wavelet transform",
      "Wavelet",
      "Wavelet packet decomposition",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Surówka",
        "given_name": "Grzegorz"
      },
      {
        "surname": "Ogorzalek",
        "given_name": "Maciej"
      }
    ]
  },
  {
    "title": "Binding data mining and expert knowledge for one-day-ahead prediction of hourly global solar radiation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114147",
    "abstract": "A new methodology to predict one-day-ahead hourly solar global radiation is proposed in this paper. This information is very useful to address many real problems; for instance, energy-market decision making is one of the contexts where that information is essential to ensure the correct integration of grid-connected photovoltaic solar systems. The developed methodology is based on the contribution of different experts to obtain improved data-driven models when included in the data mining process. The modelling phase, when models are induced and new patterns can be identified, is the one that most benefits from that expert knowledge. In this case, it is achieved by combining clustering, regression and classification methods that exploit meteorological data (directly measured or predicted by weather services). The developed models have been embedded in a prediction system that offers reliable forecasts on next-day hourly global solar radiation. As a result of the automatic learning process including the knowledge of different experts, 14 different types of day were identified based on the shape of hourly solar radiation throughout a day. The conventional definitions of types of days, that usually consider 4 options, are updated with this new proposal. The next-day prediction of hourly global radiation is obtained in two phases: in the first one, the next-day type is obtained from among the 14 possible types of day; in the second one, values of hourly global radiation are obtained using the centroid of the predicted type of day and extraterrestrial solar radiation. The relative root mean square error of the prediction model is less than 20%, meaning a significant reduction compared to previous models. Moreover, the proposed models can be recognized in the context of eXplainable Artificial Intelligence.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308927",
    "keywords": [
      "Artificial intelligence",
      "Centroid",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Electrical engineering",
      "Engineering",
      "Geodesy",
      "Geography",
      "Grid",
      "Machine learning",
      "Mathematics",
      "Mean squared error",
      "Meteorology",
      "Operating system",
      "Photovoltaic system",
      "Process (computing)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "del Campo-Ávila",
        "given_name": "José"
      },
      {
        "surname": "Takilalte",
        "given_name": "Abdelatif"
      },
      {
        "surname": "Bifet",
        "given_name": "Albert"
      },
      {
        "surname": "Mora-López",
        "given_name": "Llanos"
      }
    ]
  },
  {
    "title": "Binary chemical reaction optimization based feature selection techniques for machine learning classification problems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114169",
    "abstract": "Feature selection is an important pre-processing technique for dimensionality reduction of high-dimensional data in machine learning (ML) field. In this paper, we propose a binary chemical reaction optimization (BCRO) and a hybrid binary chemical reaction optimization-binary particle swarm optimization (HBCRO-BPSO) based feature selection techniques to optimize the number of selected features and improve the classification accuracy. Three objective functions have been used for the proposed feature selection techniques to compare their performances with a BPSO and advanced binary ant colony optimization (ABACO) along with an implemented GA based feature selection approach called as binary genetic algorithm (BGA). Five ML algorithms including K-nearest neighbor (KNN), logistic regression, Naïve Bayes, decision tree, and random forest are considered for classification tasks. Experimental results tested on eleven benchmark datasets from UCI ML repository show that the proposed HBCRO-BPSO algorithm improves the average percentage of reduction in features (APRF) and average percentage of improvement in accuracy (APIA) by 5.01% and 3.83%, respectively over the existing BPSO based feature selection method; 4.58% and 3.12% over BGA; and 4.15% and 2.27% over ABACO when used with a KNN classifier.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309076",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary classification",
      "Binary number",
      "Computer science",
      "Data mining",
      "Dimensionality reduction",
      "Feature (linguistics)",
      "Feature selection",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Naive Bayes classifier",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Random forest",
      "Selection (genetic algorithm)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Srinivasa Rao",
        "given_name": "P.C."
      },
      {
        "surname": "Sravan Kumar",
        "given_name": "A.J."
      },
      {
        "surname": "Niyaz",
        "given_name": "Quamar"
      },
      {
        "surname": "Sidike",
        "given_name": "Paheding"
      },
      {
        "surname": "Devabhaktuni",
        "given_name": "Vijay K"
      }
    ]
  },
  {
    "title": "Deep learning-based dynamic object classification using LiDAR point cloud augmented by layer-based accumulation for intelligent vehicles",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113861",
    "abstract": "An intelligent vehicle must identify the exact position and class of the surrounding object in various situations to consider the interaction with them. For this reason, the light detection and range sensor, called LiDAR, is widely used in intelligent vehicles. The LiDAR provides information in the form of a point cloud that can be used to localize and classify the surrounding objects. However, unlike vision-based object detection and classification system, the LiDAR-based recognition system cannot provide sufficient classification performance even with deep learning technologies. The reason is that the LiDAR point cloud does not have enough shape information to classify the dynamic object due to the sparsity of the points. To address this problem, we proposed a framework to enhance the deep learning-based classification performance by augmenting the shape information of the LiDAR point cloud. The augmented shape information not only improves classification performance of the networks, but also allows deep learning networks to train effectively by using artificial data-set which is generated with 3D computer-aided design model without tedious efforts of labeling. In order to enhance this shape information effectively, also, this paper proposes a layer-based accumulation algorithm considering the three degree-of-freedom motion of a dynamic object. In the experimental results, the proposed accumulation method outperformed existing registration-based methods. In real-vehicle data test, moreover, the deep learning networks trained with artificial data showed better performance when the LiDAR point cloud was accumulated.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306710",
    "keywords": [
      "Artificial intelligence",
      "Cloud computing",
      "Cognitive neuroscience of visual object recognition",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Geology",
      "Geometry",
      "Lidar",
      "Machine learning",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Operating system",
      "Pattern recognition (psychology)",
      "Point (geometry)",
      "Point cloud",
      "Programming language",
      "Remote sensing",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Kyungpyo"
      },
      {
        "surname": "Kim",
        "given_name": "Chansoo"
      },
      {
        "surname": "Jang",
        "given_name": "Chulhoon"
      },
      {
        "surname": "Sunwoo",
        "given_name": "Myoungho"
      },
      {
        "surname": "Jo",
        "given_name": "Kichun"
      }
    ]
  },
  {
    "title": "Binding data mining and expert knowledge for one-day-ahead prediction of hourly global solar radiation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114147",
    "abstract": "A new methodology to predict one-day-ahead hourly solar global radiation is proposed in this paper. This information is very useful to address many real problems; for instance, energy-market decision making is one of the contexts where that information is essential to ensure the correct integration of grid-connected photovoltaic solar systems. The developed methodology is based on the contribution of different experts to obtain improved data-driven models when included in the data mining process. The modelling phase, when models are induced and new patterns can be identified, is the one that most benefits from that expert knowledge. In this case, it is achieved by combining clustering, regression and classification methods that exploit meteorological data (directly measured or predicted by weather services). The developed models have been embedded in a prediction system that offers reliable forecasts on next-day hourly global solar radiation. As a result of the automatic learning process including the knowledge of different experts, 14 different types of day were identified based on the shape of hourly solar radiation throughout a day. The conventional definitions of types of days, that usually consider 4 options, are updated with this new proposal. The next-day prediction of hourly global radiation is obtained in two phases: in the first one, the next-day type is obtained from among the 14 possible types of day; in the second one, values of hourly global radiation are obtained using the centroid of the predicted type of day and extraterrestrial solar radiation. The relative root mean square error of the prediction model is less than 20%, meaning a significant reduction compared to previous models. Moreover, the proposed models can be recognized in the context of eXplainable Artificial Intelligence.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308927",
    "keywords": [
      "Artificial intelligence",
      "Centroid",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Electrical engineering",
      "Engineering",
      "Geodesy",
      "Geography",
      "Grid",
      "Machine learning",
      "Mathematics",
      "Mean squared error",
      "Meteorology",
      "Operating system",
      "Photovoltaic system",
      "Process (computing)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "del Campo-Ávila",
        "given_name": "José"
      },
      {
        "surname": "Takilalte",
        "given_name": "Abdelatif"
      },
      {
        "surname": "Bifet",
        "given_name": "Albert"
      },
      {
        "surname": "Mora-López",
        "given_name": "Llanos"
      }
    ]
  },
  {
    "title": "Methodology for assessing the contribution of knowledge services during the new product development process to business performance",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113860",
    "abstract": "Knowledge intensive service (KIS) is a key resource for new product development (NPD) of a firm. As a KIS provider, public research organization plays a critical role in supporting the success of firms’ NPD process, thereby promoting national/regional/local economic growth. In this study, we examine the contribution of KISs at stages in an NPD process to firms’ performance considering the correlations between input variables. For the explicit quantification of the contributions at the NPD stages, we propose a novel methodology using a variable importance assessment method. In doing so, the proposed method alleviates the correlation effects and facilitates a direct interpretation of the importance of input variables. The proposed methodology is evaluated in the case of a public research organization in South Korea, using a survey dataset collected from Korean small and medium-sized firms. The empirical results show that, with the inter-stage correlations, KISs differently contribute to firms at the NPD stages depending on performance measures: KISs mostly at the both-end NPD stages assist firms’ managerial decision-making, and KISs at all the stages except for product implementation help firms save time and cost.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306709",
    "keywords": [
      "Business",
      "Computer network",
      "Computer science",
      "Geometry",
      "Industrial organization",
      "KISS (TNC)",
      "Knowledge management",
      "Marketing",
      "Mathematical analysis",
      "Mathematics",
      "New product development",
      "Operating system",
      "Process (computing)",
      "Process management",
      "Product (mathematics)",
      "Resource (disambiguation)",
      "Service (business)",
      "Variable (mathematics)"
    ],
    "authors": [
      {
        "surname": "Choi",
        "given_name": "Jeongsub"
      },
      {
        "surname": "Kim",
        "given_name": "Byunghoon"
      },
      {
        "surname": "Han",
        "given_name": "Chang Hee"
      },
      {
        "surname": "Hahn",
        "given_name": "Hyuk"
      },
      {
        "surname": "Park",
        "given_name": "Hun"
      },
      {
        "surname": "Yoo",
        "given_name": "Jaeyoung"
      },
      {
        "surname": "Jeong",
        "given_name": "Myong Kee"
      }
    ]
  },
  {
    "title": "Unsupervised supervoxel-based lung tumor segmentation across patient scans in hybrid PET/MRI",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114244",
    "abstract": "Tumor segmentation is a crucial but difficult task in treatment planning and follow-up of cancerous patients. The challenge of automating the tumor segmentation has recently received a lot of attention, but the potential of utilizing hybrid positron emission tomography (PET)/magnetic resonance imaging (MRI), a novel and promising imaging modality in oncology, is still under-explored. Recent approaches have either relied on manual user input and/or performed the segmentation patient-by-patient, whereas a fully unsupervised segmentation framework that exploits the available information from all patients is still lacking. We present an unsupervised across-patients supervoxel-based clustering framework for lung tumor segmentation in hybrid PET/MRI. The method consists of two steps: First, each patient is represented by a set of PET/MRI supervoxel-features. Then the data points from all patients are transformed and clustered on a population level into tumor and non-tumor supervoxels. The proposed framework is tested on the scans of 18 non-small cell lung cancer patients with a total of 19 tumors and evaluated with respect to manual delineations provided by clinicians. Experiments study the performance of several commonly used clustering algorithms within the framework and provide analysis of (i) the effect of tumor size, (ii) the segmentation errors, (iii) the benefit of across-patient clustering, and (iv) the noise robustness. The proposed framework detected 15 out of 19 tumors in an unsupervised manner. Moreover, performance increased considerably by segmenting across patients, with the mean dice score increasing from 0 . 169 ± 0 . 295 (patient-by-patient) to 0 . 470 ± 0 . 308 (across-patients). Results demonstrate that both spectral clustering and Manhattan hierarchical clustering have the potential to segment tumors in PET/MRI with a low number of missed tumors and a low number of false-positives, but that spectral clustering seems to be more robust to noise.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309623",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Magnetic resonance imaging",
      "Medicine",
      "Pattern recognition (psychology)",
      "Positron emission tomography",
      "Radiology",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Hansen",
        "given_name": "Stine"
      },
      {
        "surname": "Kuttner",
        "given_name": "Samuel"
      },
      {
        "surname": "Kampffmeyer",
        "given_name": "Michael"
      },
      {
        "surname": "Markussen",
        "given_name": "Tom-Vegard"
      },
      {
        "surname": "Sundset",
        "given_name": "Rune"
      },
      {
        "surname": "Øen",
        "given_name": "Silje Kjærnes"
      },
      {
        "surname": "Eikenes",
        "given_name": "Live"
      },
      {
        "surname": "Jenssen",
        "given_name": "Robert"
      }
    ]
  },
  {
    "title": "Quantum particles-enhanced multiple Harris Hawks swarms for dynamic optimization problems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114202",
    "abstract": "Dynamic optimization problems (DOPs) have been a subject of considerable research interest mainly due to their widespread application potential. In the literature, various mechanisms have been reported to cope with the challenges of DOPs. The proposed mechanisms have usually been adopted by well-known population-based optimization algorithms, such as genetic algorithms or particle swarm optimization. Although new generation swarm-intelligence algorithms are continuously being developed and have much to offer in DOPs, their performance is usually tested on stationary optimization problems. In this study, a recently introduced optimization algorithm, Harris Hawk Optimizer, is redesigned as a multi-population based algorithm to deal with possible multiple optima. Thus, the proposed modification is allowed to search diverse parts of the search space more efficiently, particularly in multimodal environments. Next, it is further enhanced by using quantum particles to tackle with diversification and intensification challenges in DOPs. As shown in the present work, this mechanism can maintain population diversity and intensification depending on a user-supplied parameter. Finally, based on different algorithmic components, four different variants of HHO are proposed. The performances of the developed algorithms are tested on both stationary and dynamic test problems. Dynamic test functions introduced in the IEEE Congress on Evolutionary Computation 2009 (CEC 2009) are used and further extended to test the proposed algorithms' performances. Finally, appropriate statistical analysis is conducted to demonstrate significant improvements over the existing algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309313",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Demography",
      "Evolutionary algorithm",
      "Evolutionary computation",
      "Genetic algorithm",
      "Local optimum",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Meta-optimization",
      "Metaheuristic",
      "Multi-objective optimization",
      "Multi-swarm optimization",
      "Optimization problem",
      "Particle swarm optimization",
      "Population",
      "Sociology",
      "Swarm intelligence",
      "Test functions for optimization"
    ],
    "authors": [
      {
        "surname": "Gölcük",
        "given_name": "İlker"
      },
      {
        "surname": "Ozsoydan",
        "given_name": "Fehmi Burcin"
      }
    ]
  },
  {
    "title": "Dynamic evolution model of pedestrian cooperation behavior based on coordination game",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114173",
    "abstract": "Game theory was widely used to model the decision making strategy of pedestrians in the process of emergency evacuation. However, the dynamic evolution of cooperation relationships during this process has rarely been studied. Here we propose a spatial evacuation model of pedestrians with replicator dynamics and coordination game to address this problem. The novelty of this paper is to study the evolution of pedestrians’ cooperative behavior based on the coevolution of coordination game and evacuee interactive network. A pedestrian game network is established with different strategy sets guiding two types of pedestrians, i.e., cooperators and defectors. The dynamic network theory is used to analyze the changing social relationship between pedestrians, while the coordination game is adopted to model the decision-making process of pedestrians. Rules initiated from replicator dynamics are implemented to update strategies during the evacuation process. The simulation results show that the strategy evolution during evacuation vary with the panic level, the proportion of initial cooperators, and the pedestrian flow density. Especially, a higher proportion of initial cooperators make the system evolve toward cooperation, while high panic level and high pedestrian density make the system evolve toward defection. The effect of crucial parameters on evacuation efficiency is also discussed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309106",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Coevolution",
      "Computer science",
      "Demography",
      "Economics",
      "Engineering",
      "Evolutionary game theory",
      "Game theory",
      "Microeconomics",
      "Operating system",
      "Operations research",
      "Paleontology",
      "Pedestrian",
      "Population",
      "Process (computing)",
      "Replicator equation",
      "Sociology",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Niu",
        "given_name": "Yunyun"
      },
      {
        "surname": "Chen",
        "given_name": "Yulin"
      },
      {
        "surname": "Yuan",
        "given_name": "Bo"
      },
      {
        "surname": "Xiao",
        "given_name": "Jianhua"
      }
    ]
  },
  {
    "title": "A conceptual and practical comparison of PSO-style optimization algorithms",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114430",
    "abstract": "Optimization algorithms are widely employed for finding optimal solutions in many applications. Stochastic optimization algorithms including nature-inspired optimization algorithms are simple and easy to implement, and this is the reason why there is a growing interest in this research area. Recently, many nature-inspired optimization algorithms have been proposed for solving many optimization problems. Moreover, with the aim of improving the performance of optimization algorithms, some modifications were applied such as combining different algorithms and employing some sampling techniques for replacing critical parameters in the optimization algorithms. This paper compares five different widely used PSO-style optimization algorithms to investigate if there is a significant difference between them or not. Theoretically, we explain different PSO-style algorithms and discuss the similarities and differences between them. Practically, a number of experiments were conducted to compare these algorithms. Theoretical analysis and practical results prove that there is not any significant difference between the PSO-style algorithms regarding their performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310939",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Engineering optimization",
      "Mathematical optimization",
      "Mathematics",
      "Optimization algorithm",
      "Optimization problem",
      "Stochastic optimization"
    ],
    "authors": [
      {
        "surname": "Tharwat",
        "given_name": "Alaa"
      },
      {
        "surname": "Schenck",
        "given_name": "Wolfram"
      }
    ]
  },
  {
    "title": "Using spectral entropy and bernoulli map to handle concept drift",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114114",
    "abstract": "Data stream mining is a relevant task to extract information from large amounts of data that continuously evolve over time. In this context, learning algorithms may combine a classifier and a drift detector to identify changes in the distribution of the predictions error in order to rapidly adapt or replace the predictive model. Several proposals have been presented in the literature for the detection of concept changes based on the error rate of the predictive models. In general, the error rate distribution grounds most of the approaches based on sequential analysis and statistical process control, or by monitoring distributions using sliding windows, which assume the prediction errors are generated independently. However, empirical studies have shown that the error rate can be influenced by temporal dependence. In addition, new approaches considering dynamical system tools have been proposed for concept drift detection in unsupervised scenarios containing temporal dependencies. Motivated by these approaches, this article proposes the Spectral Entropy Drift Detector (SEDD), which is based on Spectral Entropy, Bernoulli Map and on the surrogate stability concept. Experimental results using abrupt and gradual concept drift versions of different dataset generators as well as real-world data streams, run in the Massive Online Analysis (MOA) framework, suggest that SEDD was competitive with the state-of-the-art methods, especially when considering accuracy and false alarms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308642",
    "keywords": [
      "Aerospace engineering",
      "Algorithm",
      "Artificial intelligence",
      "Bernoulli's principle",
      "Classifier (UML)",
      "Computer science",
      "Concept drift",
      "Data mining",
      "Data stream",
      "Data stream mining",
      "Detector",
      "Engineering",
      "Entropy (arrow of time)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Stability (learning theory)",
      "Telecommunications",
      "Word error rate"
    ],
    "authors": [
      {
        "surname": "Chikushi",
        "given_name": "Rohgi Toshio Meneses"
      },
      {
        "surname": "Barros",
        "given_name": "Roberto Souto Maior de"
      },
      {
        "surname": "Silva",
        "given_name": "Marilu Gomes N. Monte da"
      },
      {
        "surname": "Maciel",
        "given_name": "Bruno Iran Ferreira"
      }
    ]
  },
  {
    "title": "Wavelet-based logistic discriminator of dermoscopy images",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113760",
    "abstract": "Proper diagnosis of cutaneous melanoma is a life-saving factor. The most important limitation is the early and sensitive recognition of melanoma relative to dysplastic nevi. We have studied wavelet-based features extracted from dermoscopic images as efficient signals of neoplastic changes. We recursively treat the dermoscopic images and all their transformation channels (wavelet packets) through the Mallat transform. All the four decomposition filters from each decomposition level are the source of features based on three functions of the pixel values. We train the logistic classifier regularized by either the Lasso or the Ridge penalty and test its AUC metric for a set of different wavelet bases, and as a function of image resolution. A total of three different data sets with respectively 185, 117, and 413 images, and 52 wavelet bases are tested. Classification performance as a function of the wavelet number strongly depends on the image resolution and image compression. There is also a large variation in the classification performance within the self same wavelet family. Degradation of image resolution makes the overall classification performance lower and more dispersed between the regularizes. Some wavelets do not lower, but increase the learning performance at the reduced image resolutions, which is consistent with the melanoma feature-extraction studies based on other learning paradigms. The logistic classifier can extract high-performance, resolution-invariant wavelet features of melanoma.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305844",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Contextual image classification",
      "Feature extraction",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Stationary wavelet transform",
      "Wavelet",
      "Wavelet packet decomposition",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Surówka",
        "given_name": "Grzegorz"
      },
      {
        "surname": "Ogorzalek",
        "given_name": "Maciej"
      }
    ]
  },
  {
    "title": "An effective deep recurrent network with high-order statistic information for fault monitoring in wastewater treatment process",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114141",
    "abstract": "The wastewater treatment process (WWTP) is a complex biochemical reaction process that features highly nonlinear, non-Gaussian and time correlation. As a new monitoring method, deep recurrent neural net-work (DRNN) has an effective performance in dealing with the nonlinear and time correlation of the data, but it is insufficient in dealing with the non-Gaussian characteristics. In this study, an effective deep recurrent network with high-order statistic information (HSI-DRN) is proposed for solving the insufficiency in dealing with the non-Gaussian characteristics. The proposed method extracts the high-order statistics characteristics by the over-complete independent component analysis (OICA) method. After that, weights in DRNN can be trained based on the obtained high-order statistics information and their corresponding fault labels. Because of the architecture of the network, the ability of extracting the non-Gaussian characteristics by HSI-DRN can be improved by the high-order statistics characteristics. Finally, HSI-DRN can generate visual monitoring results by discretizing the output data, which leads to more intuitively reflection of faults. Simulation studies on the BSM1 model has been performed to verify the performance of the method. For the different faults, the proposed method have higher fault monitoring ability with average false alarm rate (FAR) and missed detection rate (MAR) for respective 0.0215% and 0.586% when compared to other state-of-the-art fault monitoring methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308885",
    "keywords": [
      "Actuator",
      "Artificial intelligence",
      "Computer science",
      "Constant false alarm rate",
      "Data mining",
      "Discretization",
      "Fault (geology)",
      "Fault detection and isolation",
      "Gaussian",
      "Gaussian process",
      "Geology",
      "Mathematical analysis",
      "Mathematics",
      "Nonlinear system",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Process (computing)",
      "Quantum mechanics",
      "Seismology",
      "Statistic",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Chang"
      },
      {
        "surname": "Zeyu",
        "given_name": "Li"
      },
      {
        "surname": "Gongming",
        "given_name": "Wang"
      },
      {
        "surname": "Pu",
        "given_name": "Wang"
      }
    ]
  },
  {
    "title": "A multimodal LIBRAS-UFOP Brazilian sign language dataset of minimal pairs using a microsoft Kinect sensor",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114179",
    "abstract": "Sign language recognition has made significant advances in recent years. Many researchers show interest in encouraging the development of different applications to simplify the daily life of deaf people and to integrate them into the hearing society. The use of the Kinect sensor (developed by Microsoft) for sign language recognition is steadily increasing. However, there are limited publicly available RGB-D and skeleton joint datasets that provide complete information for dynamic signs captured by Kinect sensor, most of them lack effective and accurate labeling or stored in a single data format. Given the limitations of existing datasets, this article presents a challenging public dataset, named LIBRAS-UFOP. The dataset is based on the concept of minimal pairs, which follows specific categorization criteria; the signs are labeled correctly, and validated by an expert in sign language; the dataset presents complete RGB-D and skeleton data. The dataset consists of 56 different signs with high similarity grouped into four categories. Besides, a baseline method is presented that consists of the generation of dynamic images from each multimodal data, which are the input to two flow stream CNN architectures. Finally, we propose an experimental protocol to conduct evaluations on the proposed dataset. Due to the high similarity between signs, the experimental results using a baseline method reports a recognition rate of 74.25% on the proposed dataset. This result highlights how challenging this dataset is for sign language recognition and let room for future research works to improve the recognition rate.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309143",
    "keywords": [
      "Artificial intelligence",
      "Baseline (sea)",
      "Categorization",
      "Computer science",
      "Data mining",
      "Geology",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Oceanography",
      "Pattern recognition (psychology)",
      "Philosophy",
      "RGB color model",
      "Sign (mathematics)",
      "Sign language",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Cerna",
        "given_name": "Lourdes Ramirez"
      },
      {
        "surname": "Cardenas",
        "given_name": "Edwin Escobedo"
      },
      {
        "surname": "Miranda",
        "given_name": "Dayse Garcia"
      },
      {
        "surname": "Menotti",
        "given_name": "David"
      },
      {
        "surname": "Camara-Chavez",
        "given_name": "Guillermo"
      }
    ]
  },
  {
    "title": "Two approaches to handle the dynamism in a scheduling problem with sequence-dependent setup times",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114137",
    "abstract": "In this work we address the minimization of the makespan in a scheduling problem where the machine setup times are sequence-dependent. The jobs, which arrive throughout the production process, has a release time that is unknown in advance. Considering both, dynamic environments and sequence-dependent setup times, make the problem more realistic, but also more challenging from an algorithmic and modeling point of view. To deal with the addressed problem, we implement the continuous and periodic rescheduling approaches, providing them with the same re-optimization methods. To implement the re-optimization methods, we design two insertion procedures for adding the new released jobs in the processing sequence, as well as improvement procedures, based on Iterated Greedy strategies, to reduce the sequence makespan. In the improvement phase of the Iterated Greedy strategies we implement three improvement methods that combine four local searches. The developed algorithms are assessed based on the quality of the solutions they find and the CPU time they consume to reach these solutions. We use instances from the literature and larger instances generated in this work. The three algorithm versions showed quality solutions when compared with optimal solutions for the static problem and with solutions of the Perfect Information Model for the dynamic problem. Additionally, they showed a good performance for both the continuous and periodic approach. When these two approaches are compared, results indicate that the continuous approach would be the most appropriate when the proportion of dynamic jobs is low, while when the proportion is high, it would seem more advisable to use the periodic approach, appropriately selecting the frequency of re-optimization processes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308848",
    "keywords": [
      "Algorithm",
      "Biology",
      "Computer science",
      "Dynamism",
      "Genetics",
      "Greedy algorithm",
      "Iterated function",
      "Job shop scheduling",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Minification",
      "Operating system",
      "Physics",
      "Quantum mechanics",
      "Schedule",
      "Scheduling (production processes)",
      "Sequence (biology)"
    ],
    "authors": [
      {
        "surname": "Angel-Bello",
        "given_name": "Francisco"
      },
      {
        "surname": "Vallikavungal",
        "given_name": "Jobish"
      },
      {
        "surname": "Alvarez",
        "given_name": "Ada"
      }
    ]
  },
  {
    "title": "KGEL: A novel end-to-end embedding learning framework for knowledge graph completion",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114164",
    "abstract": "Knowledge graphs (KGs) have recently become increasingly popular due to the broad range of essential applications in various downstream tasks including intelligent search, personalized recommendations, intelligent financial data analytics, etc. During an automated construction of a KG, the knowledge facts from multiple knowledge sources are automatically extracted in the form of triples, and these observed triples are used to derive new unobserved triples for KG completion (also known as link prediction). State-of-the-art link prediction methods are known to be primarily KG embedding models, among which tensor factorization models have recently drawn much attention due to their scalability and expressive feature embeddings, and hence, perform well for link prediction. However, these embedding models consider each KG triple individually and fail to capture the useful information present in the neighborhood of a node. To this end, we propose a novel end-to-end KG embedding learning framework that consists of an encoder of a dual weighted graph convolutional network, and a decoder of a novel fully expressive tensor factorization model. The proposed encoder extends weighted graph convolutional network to generate two rich and high quality embedding vectors for each node by aggregating information from the neighboring nodes. The proposed decoder has a flexible and powerful tensor representation form of the Tensor Train decomposition that takes benefit of the two representations of each node in its embedding space to accurately model the KG triples. We also derive a bound on the size of the embeddings for full expressivity and show that our proposed tensor factorization model is fully expressive. Additionally, we show the relationship of our tensor factorization model to previous tensor factorization models. The experimental results show the effectiveness of the proposed framework that consistently marks performance gains over several previous models on recent standard link prediction datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309039",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Database",
      "Embedding",
      "Encoder",
      "Engineering",
      "Graph",
      "Graph embedding",
      "Machine learning",
      "Mathematics",
      "Node (physics)",
      "Operating system",
      "Pure mathematics",
      "Scalability",
      "Structural engineering",
      "Tensor (intrinsic definition)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zeb",
        "given_name": "Adnan"
      },
      {
        "surname": "Ul Haq",
        "given_name": "Anwar"
      },
      {
        "surname": "Zhang",
        "given_name": "Defu"
      },
      {
        "surname": "Chen",
        "given_name": "Junde"
      },
      {
        "surname": "Gong",
        "given_name": "Zhiguo"
      }
    ]
  },
  {
    "title": "A study of the effects of negative transfer on deep unsupervised domain adaptation methods",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114088",
    "abstract": "Intelligent systems driven by deep learning have become relevant in real-world applications with the increasing availability of technology and data. However, real-world settings require effective and robust deep learning models that are able to deal with unforeseen samples and a variety of data distributions. Recently, Unsupervised Domain Adaptation (UDA) for deep learning models (D-UDA) addresses such limitations by transferring knowledge from a labeled source domain to an unlabeled target domain, reducing the dataset shift between domain distributions. However, despite recent advances in D-UDA, current works have not been focused on studying specific cases in the distribution shifts under which D-UDA methods can ensure that transfer is helpful, avoiding a ‘negative transfer’ risk. In this paper, we present a study about the effect of different cases of negative transfer over the most popular and recent D-UDA methods reported in the literature. For this, we evaluate the accuracy performance of D-UDA methods over different scenarios containing different types of distribution shifts. Experimental results show that specific cases of distribution shifts generate negative transfer over the evaluated D-UDA methods. From this study, we provide some insights to select and design robust D-UDA methods in intelligent systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308459",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "First language",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Negative transfer",
      "Optics",
      "Parallel computing",
      "Philosophy",
      "Physics",
      "Transfer (computing)",
      "Transfer of learning",
      "Variety (cybernetics)"
    ],
    "authors": [
      {
        "surname": "Jiménez-Guarneros",
        "given_name": "Magdiel"
      },
      {
        "surname": "Gómez-Gil",
        "given_name": "Pilar"
      }
    ]
  },
  {
    "title": "A novel Black Widow Optimization algorithm for multilevel thresholding image segmentation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114159",
    "abstract": "Segmentation is a crucial step in image processing applications. This process separates pixels of the image into multiple classes that permits the analysis of the objects contained in the scene. Multilevel thresholding is a method that easily performs this task, the problem is to find the best set of thresholds that properly segment each image. Techniques as Otsu’s between class variance or Kapur’s entropy helps to find the best thresholds but they are computationally expensive for more than two thresholds. To overcome such problem this paper introduces the use of the novel meta-heuristic algorithm called Black Widow Optimization (BWO) to find the best threshold configuration using Otsu or Kapur as objective function. To evaluate the performance and effectiveness of the BWO-based method, it has been considered the use of a variety of benchmark images, and compared against six well-known meta-heuristic algorithms including; the Gray Wolf Optimization (GWO), Moth Flame Optimization (MFO), Whale Optimization Algorithm (WOA), Sine–Cosine Algorithm (SCA), Slap Swarm Algorithm (SSA), and Equilibrium Optimization (EO). The experimental results have revealed that the proposed BWO-based method outperform the competitor algorithms in terms of the fitness values as well as the others performance measures such as PSNR, SSIM and FSIM. The statistical analysis manifests that the BWO-based method achieves efficient and reliable results in comparison with the other methods. Therefore, BWO-based method was found to be most promising for multi-level image segmentation problem over other segmentation approaches that are currently used in the literature.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309003",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Geodesy",
      "Geography",
      "Heuristic",
      "Image (mathematics)",
      "Image segmentation",
      "Otsu's method",
      "Pattern recognition (psychology)",
      "Pixel",
      "Segmentation",
      "Thresholding"
    ],
    "authors": [
      {
        "surname": "Houssein",
        "given_name": "Essam H."
      },
      {
        "surname": "Helmy",
        "given_name": "Bahaa El-din"
      },
      {
        "surname": "Oliva",
        "given_name": "Diego"
      },
      {
        "surname": "Elngar",
        "given_name": "Ahmed A."
      },
      {
        "surname": "Shaban",
        "given_name": "Hassan"
      }
    ]
  },
  {
    "title": "A novel direct measure of exploration and exploitation based on attraction basins",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114353",
    "abstract": "Exploration, the process of visiting a new region in a search space, and exploitation, the process of searching in the neighborhood of previously visited regions, are two centerpieces of any metaheuristic algorithm. It is a common belief that good results can be obtained only if there is a good balance between exploration and exploitation. Hence, there is an urgent need to control the balance between exploration and exploitation in a direct manner. But, currently, direct measures of exploration and exploitation are almost non-existent, and researchers rely on indirect measures of exploration and exploitation, such as diversity, entropy, and fitness improvements. To remedy this situation, in this paper, a novel direct measure of exploration and exploitation is proposed that is based on attraction basins — parts of a search space where each part has its own point called an attractor, to which neighboring points tend to evolve. Each search point can be associated with a particular attraction basin. If a newly generated search point belongs to the same attraction basin as its parent then the search process is identified as exploitation, otherwise as exploration. In this paper, a new technique to compute attraction basins is presented, as well as a novel direct measure ( E x p B a s ) of exploration and exploitation based on attraction basins. On the selected set of unimodal and multimodal optimization problems it is shown that the newly proposed direct measure of exploration and exploitation is more accurate than our previously proposed direct measure ( E x p D i s t ), as well as the common indirect measure based on diversity (Diversity).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310368",
    "keywords": [
      "Artificial intelligence",
      "Attraction",
      "Computer science",
      "Data mining",
      "Entropy (arrow of time)",
      "Geology",
      "Linguistics",
      "Mathematical optimization",
      "Mathematics",
      "Measure (data warehouse)",
      "Operating system",
      "Paleontology",
      "Philosophy",
      "Physics",
      "Process (computing)",
      "Quantum mechanics",
      "Structural basin"
    ],
    "authors": [
      {
        "surname": "Jerebic",
        "given_name": "Jernej"
      },
      {
        "surname": "Mernik",
        "given_name": "Marjan"
      },
      {
        "surname": "Liu",
        "given_name": "Shih-Hsi"
      },
      {
        "surname": "Ravber",
        "given_name": "Miha"
      },
      {
        "surname": "Baketarić",
        "given_name": "Mihael"
      },
      {
        "surname": "Mernik",
        "given_name": "Luka"
      },
      {
        "surname": "Črepinšek",
        "given_name": "Matej"
      }
    ]
  },
  {
    "title": "A boosted SVM classifier trained by incremental learning and decremental unlearning approach",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114154",
    "abstract": "The Support Vector Machines (SVM) classifier is a margin-based supervised machine learning method used for categorization and classification tasks. A Linear SVM classifier uses linear Kernels while a non-linear SVM classifier adopts non-linear Kernels. The Linear SVM classifier is considered an efficient technique, especially for big datasets of high dimensionality in various applications, such as document categorization, time-series classification, outliers’ detection, to name a few. It is much faster to train a linear SVM classifier than a non-linear SVM classifier. For large-scale datasets with various shapes, configurations, and distributions, the computational complexity of training a non-linear SVM classifier is continuously evolving. Current research methods have introduced various problem formulations, solvers, and strategies for speeding up the training process of a non-linear SVM classifier. However, solving a quadratic programming (QP) problem is still challenging, especially for “Big Data”, which poses a great challenge for traditional methods to train a non-linear classifier efficiently. In this paper, we are proposing a novel boosting algorithm to enhance the performance of weak non-linear SVM classifiers using the notion of incremental learning and decremental unlearning. Experimental results over artificial and real datasets with different sizes, shapes, and distributions show that the proposed ensemble boosting algorithm outperforms the individual SVM classifiers measured by the classification accuracy and the Speedup.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308976",
    "keywords": [
      "Artificial intelligence",
      "Boosting (machine learning)",
      "Classifier (UML)",
      "Computer science",
      "Curse of dimensionality",
      "Linear classifier",
      "Machine learning",
      "Margin classifier",
      "Outlier",
      "Pattern recognition (psychology)",
      "Quadratic classifier",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Kashef",
        "given_name": "Rasha"
      }
    ]
  },
  {
    "title": "Dermatological expert system implementing the ABCD rule of dermoscopy for skin disease identification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114204",
    "abstract": "Doctors and radiologists generally follow the standard ABCD rule of dermoscopy for differentiating the malignant and benign skin lesions. The estimation of the dermoscopic score by visual inspection only, may lead to the inaccurate diagnosis of the disease at an early stage. In this work, the ABCD attributes have been improvised and quantified in a dermatological expert system (DermESy) for the differentiation of malignant and benign lesions. DermESy, a rule based expert system has been developed by implementing dermatologist’s knowledge with proper quantification of the dermoscopic findings. Using DermESy, the dermoscopic images have been categorized as malignant, benign and suspicious lesions based on the estimated total dermoscopic score (TDS), similar to the findings of an expert. To estimate the TDS, shape, brightness and color variations are considered to modify the ‘A’ score. The color information extraction algorithm is introduced to extract significant color regions to quantify the ‘C’ score. To find the appropriate ‘D’ score of a skin lesion, dermoscopic structures segmentation algorithms have been introduced. In this work, the ABCD rule of dermoscopy has been improvised by considering the spatial properties of dermoscopic structures for improved identification of malignant lesions. An explanatory subsystem is implemented in DermESy to assist the dermatologist with proper in-detail visualization. DermESy has differentiated the benign and malignant skin lesions with 97.69% sensitivity, 97.97% specificity and 97.86% accuracy. The TDS evaluated by DermESy is verified and compared against expert dermatologist’s TDS scores of same dermoscopy images to establish the reliability and robustness of the proposed system.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309337",
    "keywords": [
      "Artificial intelligence",
      "Cancer research",
      "Computer science",
      "Dermatology",
      "Dermatoscopy",
      "Expert system",
      "Medicine",
      "Melanoma",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Skin lesion"
    ],
    "authors": [
      {
        "surname": "Chatterjee",
        "given_name": "Saptarshi"
      },
      {
        "surname": "Dey",
        "given_name": "Debangshu"
      },
      {
        "surname": "Munshi",
        "given_name": "Sugata"
      },
      {
        "surname": "Gorai",
        "given_name": "Surajit"
      }
    ]
  },
  {
    "title": "Automatic detection of the best performing priority rule for the resource-constrained project scheduling problem",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114116",
    "abstract": "Priority rules are applied in many commercial software tools for scheduling projects under limited resources because of their known advantages such as the ease of implementation, their intuitive working, and their fast speed. Moreover, while numerous research papers present comparison studies between different priority rules, managers often do not know which rules should be used for their specific project, and therefore have no other choice than selecting a priority rule at random and hope for the best. This paper introduces a decision tree approach to classify and detect the best performing priority rule for the resource-constrained project scheduling problem (RCPSP). The research relies on two classification models to map project indicators onto the performance of the priority rule. Using such models, the performance of each priority rule can be predicted, and these predictions are then used to automatically select the best performing priority rule for a specific project with known network and resource indicator values. A set of computational experiment is set up to evaluate the performance of the newly proposed classification models using the most well-known priority rules from the literature. The experiments compare the performance of multi-label classification models with multi-class classification models, and show that these models can outperform the average performance of using any single priority rule. It will be argued that this approach can be easily extended to any extension of the RCPSP without changing the methodology used in this study.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308654",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Decision tree",
      "Dynamic priority scheduling",
      "Engineering",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Operations research",
      "Priority inheritance",
      "Programming language",
      "Rate-monotonic scheduling",
      "Rule-based system",
      "Schedule",
      "Scheduling (production processes)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Weikang"
      },
      {
        "surname": "Vanhoucke",
        "given_name": "Mario"
      },
      {
        "surname": "Coelho",
        "given_name": "José"
      },
      {
        "surname": "Luo",
        "given_name": "Jingyu"
      }
    ]
  },
  {
    "title": "SPBC: A self-paced learning model for bug classification from historical repositories of open-source software",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113808",
    "abstract": "One of the areas most in need of improvement in the field of automated bug fixing, localization and triaging systems is that of an effective categorization, as this would bugs to reduce the time, cost and effort required to locate, assign and fix the bug. The existing approaches depend upon the textual similarity of the bug description and category in a given reported bug; accordingly, the challenges of unstructured bugs, technical terms, versatile ways of reporting the same bug, the diverse nature and sizes of datasets etc. are often overlooked. Consequently, this limits the classifier performance to a specific type of dataset, resulting in classification inefficiency. To this end, we propose a novel Self-Paced Bug Classifier (SPBC) that is capable of locating the target categories from the bug description of the historical data, maintained by multiple open-source software packages (Bugzilla, Mentis, Redmine). The proposed model introduces a self-paced back-traceable algorithm, controlled by a self-paced regularizer, which classifies textually independent bug descriptions with weighted data-independent tokens (the easy samples). Later on, the regularizer sets comparatively hard samples for textually dependent classification by capturing intra-class and inter-class discrimination features from bug descriptions, based on the weighted similarities of words; this is done with the help of a Key Feature Identification Matrix (KFIM), a Non-Independent and Identically Distributed (NIID) matrix. Easy-to-hard self-pace learning, integrated with textually dependent and independent classification, makes SPBC capable of simultaneously enhancing the effectiveness and robustness of intelligent systems through a substantial increase in precision (5–15% on average). The main advantage of SPBC is that it targets the spatial relationship between the data and the system, which makes it an apt learner of data and allows it to maintains sample insertion into the classifier at a controlled pace. Additionally, it maintains stability, which is not affected by the dataset’s dimensionality and traits. As is evidenced by the experimental results on four different datasets from open-source projects, our model outperforms the baseline and state-of-the-art methods through a single-stroke solution with improved accuracy and stable performance (average 95% precision and 4% decrease in kappa); hence, it is significant for improving intelligent bug fixing and triaging systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306230",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Categorization",
      "Chemistry",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Gene",
      "Machine learning",
      "Programming language",
      "Robustness (evolution)",
      "Software",
      "Software bug"
    ],
    "authors": [
      {
        "surname": "Mohsin",
        "given_name": "Hufsa"
      },
      {
        "surname": "Shi",
        "given_name": "Chongyang"
      }
    ]
  },
  {
    "title": "Dependency-aware software requirements selection using fuzzy graphs and integer programming",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113748",
    "abstract": "One of the critical activities in software development is Requirements Selection, which is to find an optimal subset of the software requirements (features) with the highest value for a given budget. The values of the requirements, however, may depend on one another. Such Value Dependencies have not been considered by the existing requirements selection methods, leading to user dissatisfaction and loss of value and reputation in software projects. To mitigate this, we propose Dependency-Aware Requirements Selection (DARS) as an expert system, which explicitly accounts for value dependencies in software projects. At the heart of DARS is an Integer Linear Programming (ILP) model that reduces the risk of value loss by considering value dependencies among the requirements. These value dependencies are identified from the preferences of the users for the requirements. The validly of DARS is verified by studying a real-world software project as well as carrying out simulations. Our results demonstrate a significant reduction in value loss when DARS is employed. Also, the ILP model of DARS proved scalable to large requirement sets (experimented for up to 3000). The results of our study can be extrapolated to a wide range of expert systems that concern selecting value-dependent items.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305728",
    "keywords": [],
    "authors": [
      {
        "surname": "Mougouei",
        "given_name": "Davoud"
      },
      {
        "surname": "Powers",
        "given_name": "David M.W."
      }
    ]
  },
  {
    "title": "Video object segmentation based on motion-aware ROI prediction and adaptive reference updating",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114153",
    "abstract": "Video object segmentation (VOS) is a research hotspot in the field of computer vision. Traditional video object segmentation methods based on deep learning have some problems such as difficulty in adapting to the change of object appearance and low segmentation speed. In this manuscript, we propose a robust VOS method based on motion-aware region of interest (ROI) prediction and adaptive reference updating. Firstly, based on the historical movement trajectory of target region to perceive motion trend dynamically, we predict the motion-aware ROI of target object in the current frame and use it as the input of segmentation network. Then, in order to adapt to the appearance changes of target in the video, the adaptive updating strategy of reference is given to dynamically update the reference frame during the segmentation process. Finally, VOS Siamese network is designed for fast segmentation. Experiments on three public benchmark datasets, DAVIS-2016 and DAVIS-2017, show that the proposed method performs better than the state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308964",
    "keywords": [
      "Artificial intelligence",
      "Astronomy",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Frame (networking)",
      "Geodesy",
      "Geography",
      "Image segmentation",
      "Object (grammar)",
      "Physics",
      "Reference frame",
      "Region of interest",
      "Segmentation",
      "Telecommunications",
      "Trajectory"
    ],
    "authors": [
      {
        "surname": "Fu",
        "given_name": "Lihua"
      },
      {
        "surname": "Zhao",
        "given_name": "Yu"
      },
      {
        "surname": "Sun",
        "given_name": "Xiaowei"
      },
      {
        "surname": "Huang",
        "given_name": "Jialiang"
      },
      {
        "surname": "Wang",
        "given_name": "Dan"
      },
      {
        "surname": "Ding",
        "given_name": "Yu"
      }
    ]
  },
  {
    "title": "New hybrid genetic algorithms to solve dynamic berth allocation problem",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114198",
    "abstract": "Berth allocation problem (BAP) is concerned to assign ships to port terminal positions, seeking to minimize the total service time and maximize the quay occupation. A dynamic model with special features is developed to deal with real scenarios from Port Administration of Paranaguá and Antonina (APPA), located on the Brazilian coast. To solve the problem, this work proposes two metaheuristics comprise by a novel combination of genetic algorithm and an approximated dynamic programming employed as a local search. Two heuristics for solution space reductions, a confinement procedure in a reduced neighborhood, known as Corridor Method, and an elimination process for unpromising solution regions are designed for local search approach. Case studies present a problem complexity discussion and comparative analysis of the metaheuristics regarding two standard genetic algorithms. The computational experiments explore the optimal solutions for small instances and the best results, solutions variability, and probabilistic plots resorting by ten instances based on real data available by APPA. The results show the reliability of the metaheuristics to deal with large instances and tight schedules in busy port systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309283",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Electrical engineering",
      "Engineering",
      "Genetic algorithm",
      "Heuristics",
      "Local search (optimization)",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Port (circuit theory)",
      "Probabilistic logic",
      "Tabu search"
    ],
    "authors": [
      {
        "surname": "Bacalhau",
        "given_name": "Eduardo Tadeu"
      },
      {
        "surname": "Casacio",
        "given_name": "Luciana"
      },
      {
        "surname": "de Azevedo",
        "given_name": "Anibal Tavares"
      }
    ]
  },
  {
    "title": "Why pay more? A simple and efficient named entity recognition system for tweets",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114101",
    "abstract": "The current paper investigates the problem of multimodal named entity recognition from Twitter data. Named entity recognition (NER) is an important task in natural language processing and has been carefully studied in recent decades. NER from tweets is particularly challenging because of 1) tweets are limited in length, 2) contains noisy text, and 3) contains hashtags. Moreover often tweets are associated with images and hyperlinks. Existing works on tweet-NER mostly concentrate on multimodal deep learning based models neglecting the use of hand-crafted features and usage of hyperlinks. The current paper investigates the incorporation of hand-crafted features extracted from different modalities like images, hyperlinks while extracting named entities from tweet-text. A large set of hand-crafted features are extracted from different modalities (images, hyperlinks) and those are added with the features extracted by a hybrid deep-neural model, bi-directional LSTM and CNN, followed by a conditional random field to perform this task. Several variants of these models in association with different hand-crafted feature sets are designed. Extensive experimentations on a multimodal Twitter data (containing text, images and urls) illustrate that character level hand-crafted features significantly improve the performance of the systems. In a part of the paper, results of the proposed models are also shown on a standard NER dataset, CoNLL 2003 dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308551",
    "keywords": [
      "Artificial intelligence",
      "CRFS",
      "Computer science",
      "Conditional random field",
      "Economics",
      "Feature (linguistics)",
      "Field (mathematics)",
      "Hyperlink",
      "Information retrieval",
      "Linguistics",
      "Machine learning",
      "Management",
      "Mathematics",
      "Modalities",
      "Named-entity recognition",
      "Natural language processing",
      "Philosophy",
      "Programming language",
      "Pure mathematics",
      "Sequence labeling",
      "Set (abstract data type)",
      "Social science",
      "Sociology",
      "Task (project management)",
      "Web page",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Suman",
        "given_name": "Chanchal"
      },
      {
        "surname": "Reddy",
        "given_name": "Saichethan Miriyala"
      },
      {
        "surname": "Saha",
        "given_name": "Sriparna"
      },
      {
        "surname": "Bhattacharyya",
        "given_name": "Pushpak"
      }
    ]
  },
  {
    "title": "New hybrid genetic algorithms to solve dynamic berth allocation problem",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114198",
    "abstract": "Berth allocation problem (BAP) is concerned to assign ships to port terminal positions, seeking to minimize the total service time and maximize the quay occupation. A dynamic model with special features is developed to deal with real scenarios from Port Administration of Paranaguá and Antonina (APPA), located on the Brazilian coast. To solve the problem, this work proposes two metaheuristics comprise by a novel combination of genetic algorithm and an approximated dynamic programming employed as a local search. Two heuristics for solution space reductions, a confinement procedure in a reduced neighborhood, known as Corridor Method, and an elimination process for unpromising solution regions are designed for local search approach. Case studies present a problem complexity discussion and comparative analysis of the metaheuristics regarding two standard genetic algorithms. The computational experiments explore the optimal solutions for small instances and the best results, solutions variability, and probabilistic plots resorting by ten instances based on real data available by APPA. The results show the reliability of the metaheuristics to deal with large instances and tight schedules in busy port systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309283",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Electrical engineering",
      "Engineering",
      "Genetic algorithm",
      "Heuristics",
      "Local search (optimization)",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Port (circuit theory)",
      "Probabilistic logic",
      "Tabu search"
    ],
    "authors": [
      {
        "surname": "Bacalhau",
        "given_name": "Eduardo Tadeu"
      },
      {
        "surname": "Casacio",
        "given_name": "Luciana"
      },
      {
        "surname": "de Azevedo",
        "given_name": "Anibal Tavares"
      }
    ]
  },
  {
    "title": "Why pay more? A simple and efficient named entity recognition system for tweets",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114101",
    "abstract": "The current paper investigates the problem of multimodal named entity recognition from Twitter data. Named entity recognition (NER) is an important task in natural language processing and has been carefully studied in recent decades. NER from tweets is particularly challenging because of 1) tweets are limited in length, 2) contains noisy text, and 3) contains hashtags. Moreover often tweets are associated with images and hyperlinks. Existing works on tweet-NER mostly concentrate on multimodal deep learning based models neglecting the use of hand-crafted features and usage of hyperlinks. The current paper investigates the incorporation of hand-crafted features extracted from different modalities like images, hyperlinks while extracting named entities from tweet-text. A large set of hand-crafted features are extracted from different modalities (images, hyperlinks) and those are added with the features extracted by a hybrid deep-neural model, bi-directional LSTM and CNN, followed by a conditional random field to perform this task. Several variants of these models in association with different hand-crafted feature sets are designed. Extensive experimentations on a multimodal Twitter data (containing text, images and urls) illustrate that character level hand-crafted features significantly improve the performance of the systems. In a part of the paper, results of the proposed models are also shown on a standard NER dataset, CoNLL 2003 dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308551",
    "keywords": [
      "Artificial intelligence",
      "CRFS",
      "Computer science",
      "Conditional random field",
      "Economics",
      "Feature (linguistics)",
      "Field (mathematics)",
      "Hyperlink",
      "Information retrieval",
      "Linguistics",
      "Machine learning",
      "Management",
      "Mathematics",
      "Modalities",
      "Named-entity recognition",
      "Natural language processing",
      "Philosophy",
      "Programming language",
      "Pure mathematics",
      "Sequence labeling",
      "Set (abstract data type)",
      "Social science",
      "Sociology",
      "Task (project management)",
      "Web page",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Suman",
        "given_name": "Chanchal"
      },
      {
        "surname": "Reddy",
        "given_name": "Saichethan Miriyala"
      },
      {
        "surname": "Saha",
        "given_name": "Sriparna"
      },
      {
        "surname": "Bhattacharyya",
        "given_name": "Pushpak"
      }
    ]
  },
  {
    "title": "Tourism recommendation system based on semantic clustering and sentiment analysis",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114324",
    "abstract": "Numerous number of tourism attractions along with a huge amount of information about them on web and social platforms have made the decision-making process for selecting and visiting them complicated. In this regard, the tourism recommendation systems have become interesting for tourists, but challenging for designers because they should be able to provide personalized services. This paper introduces a tourism recommendation system that extracts users' preferences in order to provide personalized recommendations. To this end, users' reviews on tourism social networks are used as a rich source of information to extract their preferences. Then, the comments are preprocessed, semantically clustered, and sentimentally analyzed to detect a tourist's preferences. Similarly, all users aggregated reviews about an attraction are utilized to extract the features of these points of interest. Finally, the proposed recommendation system, semantically compares the preferences of a user with the features of attractions to suggest the most matching points of interest to the user. In addition, the system utilizes the vital contextual information of time, location, and weather to filter unsuitable items and increase the quality of suggestions regarding the current situation. The proposed recommendation system is developed by Python and evaluated on a dataset gathered from TripAdvisor platform. The evaluation results show that the proposed system improves the f-measure criterion in comparison with the previous systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420310174",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Data science",
      "Epistemology",
      "Filter (signal processing)",
      "Information retrieval",
      "Law",
      "Matching (statistics)",
      "Mathematics",
      "Operating system",
      "Philosophy",
      "Point of interest",
      "Political science",
      "Process (computing)",
      "Quality (philosophy)",
      "Recommender system",
      "Sentiment analysis",
      "Statistics",
      "Tourism",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Abbasi-Moud",
        "given_name": "Zahra"
      },
      {
        "surname": "Vahdat-Nejad",
        "given_name": "Hamed"
      },
      {
        "surname": "Sadri",
        "given_name": "Javad"
      }
    ]
  },
  {
    "title": "Semantic-driven watermarking of relational textual databases",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114013",
    "abstract": "In relational database watermarking, the semantic consistency between the original database and the distorted one is a challenging issue which is disregarded by most watermarking proposals, due to the well-known assumption for which a small amount of errors in the watermarked database is tolerable. We propose a semantic-driven watermarking approach of relational textual databases, which marks multi-word textual attributes, exploiting the synonym substitution technique for text watermarking together with notions in semantic similarity analysis, and dealing with the semantic perturbations provoked by the watermark embedding. We show the effectiveness of our approach through an experimental evaluation, highlighting the resulting capacity, robustness and imperceptibility watermarking requirements. We also prove the resilience of our approach with respect to the random synonym substitution attack.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307867",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Botany",
      "Chemistry",
      "Computer science",
      "Consistency (knowledge bases)",
      "Digital watermarking",
      "Embedding",
      "Gene",
      "Genus",
      "Image (mathematics)",
      "Information retrieval",
      "Natural language processing",
      "Programming language",
      "Relational database",
      "Robustness (evolution)",
      "Scrambling",
      "Semantic similarity",
      "Substitution (logic)",
      "Synonym (taxonomy)",
      "Watermark"
    ],
    "authors": [
      {
        "surname": "Pérez Gort",
        "given_name": "Maikel Lázaro"
      },
      {
        "surname": "Olliaro",
        "given_name": "Martina"
      },
      {
        "surname": "Cortesi",
        "given_name": "Agostino"
      },
      {
        "surname": "Feregrino Uribe",
        "given_name": "Claudia"
      }
    ]
  },
  {
    "title": "Quantum-inspired neuro coevolution model applied to coordination problems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114133",
    "abstract": "In many real-world problems, some coordination between agents is necessary to enable the task to be optimally performed. However, obtaining this coordination can be challenging due to the quantity and characteristics of the agents, the dynamics of the environment and/or the complexity of the task, requiring much computation time. Furthermore, some problems require different types of agent specialization. In this case, it is very difficult for the programmers to define learning strategies and their parameters. Optimization of these parameters using standard evolutionary algorithms is also inadequate due to the high computational cost in these real multi-agent situations. The main objective of this study is therefore to propose a new neuroevolution model to be applied to agent coordination problems, termed the Quantum-Inspired Neuro Coevolution (QNCo) Model. QNCo makes use of paradigms from quantum physics and biological coevolution to evolve sub-populations of quantum individuals aiming convergence gains. The model has the capacity to autonomously obtain the best neural network topology of each agent, eliminating the need for the programmer to set this configuration. New quantum crossover and mutation operators were proposed and compared during function optimization of different dimensions. The proposed model was tested in two simulation problems, prey-predator and multi-rover tasks, and one real problem of mobile telephony coverage. The QNCo model yielded promising results compared to similar algorithms, with good solutions in terms of learning strategies and a great reduction in convergence time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308812",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Coevolution",
      "Computer science",
      "Convergence (economics)",
      "Crossover",
      "Economic growth",
      "Economics",
      "Evolutionary algorithm",
      "Evolutionary biology",
      "Function (biology)",
      "Management",
      "Mathematical optimization",
      "Mathematics",
      "Neuroevolution",
      "Paleontology",
      "Programming language",
      "Set (abstract data type)",
      "Task (project management)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Dias",
        "given_name": "Eduardo Dessupoio Moreira"
      },
      {
        "surname": "Vellasco",
        "given_name": "Marley Maria Bernardes Rebuzzi"
      },
      {
        "surname": "da Cruz",
        "given_name": "André Vargas Abs"
      }
    ]
  },
  {
    "title": "An experimental methodology to evaluate machine learning methods for fault diagnosis based on vibration signals",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114022",
    "abstract": "This paper presents a systematic procedure to fairly compare experimental performance scores for machine learning methods for fault diagnosis based on vibration signals. In the vast majority of related scientific publications, the estimated accuracy and similar performance criteria are the sole quality parameter presented. However, the experimental design giving rise to these results is mostly biased, based on unacceptably simple validation methods and on recycling identical patterns in test data sets, previously used for training. Moreover, the methods in general overfit their hyperparameters, introducing additional overoptimistic results. In order to remedy this defect, we critically analyse the usual training-validation-test division and propose an algorithmic guideline in the form of a validation framework. This allows a well defined comparison of experimental results. In order to illustrate the ideas of the paper, the Case Western Reserve University Bearing Data benchmark is used as a case study. Four distinct classifiers are experimentally compared, under gradually more difficult generalization tasks using the proposed evaluation framework: K-Nearest-Neighbor, Support Vector Machine, Random Forest and One-Dimensional Convolutional Neural Network. An extensive literature review suggests that most vibration based research papers, particularly for the Case Western Reserve University Bearing Data, use similar patterns for training and testing, making their classification an easy task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307934",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Bearing (navigation)",
      "Benchmark (surveying)",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Economics",
      "Generalization",
      "Geodesy",
      "Geography",
      "Hyperparameter",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Overfitting",
      "Random forest",
      "Support vector machine",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Rauber",
        "given_name": "Thomas Walter"
      },
      {
        "surname": "da Silva Loca",
        "given_name": "Antonio Luiz"
      },
      {
        "surname": "Boldt",
        "given_name": "Francisco de Assis"
      },
      {
        "surname": "Rodrigues",
        "given_name": "Alexandre Loureiros"
      },
      {
        "surname": "Varejão",
        "given_name": "Flávio Miguel"
      }
    ]
  },
  {
    "title": "A new local covariance matrix estimation for the classification of gene expression profiles in high dimensional RNA-Seq data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114200",
    "abstract": "Recent developments in the next-generation sequencing based on RNA-sequencing (RNA-Seq) allow researchers to measure the expression levels of thousands of genes for multiple samples simultaneously. In order to analyze these kinds of data sets, many classification models have been proposed in the literature. Most of the existing classifiers assume that genes are independent; however, this is not a realistic approach for real RNA-Seq classification problems. For this reason, some other classification methods, which incorporates the dependence structure between genes into a model, are proposed. Quantile transformed Quadratic Discriminant Analysis (qtQDA) proposed recently is one of those classifiers, which estimates covariance matrix by Maximum Likelihood Estimator. However, MLE may not reflect the real dependence between genes. For this reason, we propose a new approach based on local dependence function to estimate the covariance matrix to be used in the qtQDA classification model. This new approach assumes the dependencies between genes are locally defined rather than complete dependency. The performances of qtQDA classifier based on two different covariance matrix estimates are compared over two real RNA-Seq data sets, in terms of classification error rates. The results show that using local dependence function approach yields a better estimate of covariance matrix and increases the performance of qtQDA classifier.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309295",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "CMA-ES",
      "Classifier (UML)",
      "Computer science",
      "Covariance",
      "Covariance function",
      "Covariance matrix",
      "Discriminative model",
      "Estimation of covariance matrices",
      "Estimator",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Kochan",
        "given_name": "Necla"
      },
      {
        "surname": "Yazgı Tütüncü",
        "given_name": "G."
      },
      {
        "surname": "Giner",
        "given_name": "Göknur"
      }
    ]
  },
  {
    "title": "MLT-DNet: Speech emotion recognition using 1D dilated CNN based on multi-learning trick approach",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114177",
    "abstract": "Speech is the most dominant source of communication among humans, and it is an efficient way for human–computer interaction (HCI) to exchange information. Nowadays, speech emotion recognition (SER) is an active research area that plays a crucial role in real-time applications. In this era, the SER system has lacked real-time speech processing. To address this problem, we propose an end-to-end real-time SER model that is based on a one-dimensional dilated convolutional neural network (DCNN). Our model used a multi-learning strategy to parallel extract spatial salient emotional features and learn long term contextual dependencies from the speech signals. We used residual blocks with a skip connection (RBSC) module, in order to find a correlation, the emotional cues, and the sequence learning (Seq_L) module, to learn the long term contextual dependencies in the input features. Furthermore, we used a fusion layer to concatenate these learned features for the final emotion recognition task. Our model structure is quite simple, and it is capable of automatically learning salient discriminative features from the speech signals. We evaluated our model using benchmark IEMOCAP and EMO-DB datasets and obtained a high recognition accuracy, which were 73% and 90%, respectively. The experimental results indicated the significance and the efficiency of our proposed model have shown excessive assistance with the implementation of a real-time SER system. Hence, our model is capable of processing original speech signals for the emotion recognition that utilizes lightweight dilated CNN architecture that implements the multi-learning trick (MLT) approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309131",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Discriminative model",
      "Emotion recognition",
      "Geodesy",
      "Geography",
      "Pattern recognition (psychology)",
      "Recurrent neural network",
      "Residual",
      "Salient",
      "Speech processing",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Mustaqeem",
        "given_name": ""
      },
      {
        "surname": "Kwon",
        "given_name": "Soonil"
      }
    ]
  },
  {
    "title": "Gradient and Newton boosting for classification and regression",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114080",
    "abstract": "Boosting algorithms are frequently used in applied data science and in research. To date, the distinction between boosting with either gradient descent or second-order Newton updates is often not made in both applied and methodological research, and it is thus implicitly assumed that the difference is irrelevant. The goal of this article is to clarify this situation. In particular, we present gradient and Newton boosting, as well as a hybrid variant of the two, in a unified framework. We compare these boosting algorithms with trees as base learners using various datasets and loss functions. Our experiments show that Newton boosting outperforms gradient and hybrid gradient-Newton boosting in terms of predictive accuracy on the majority of datasets. We also present evidence that the reason for this is not faster convergence of Newton boosting. In addition, we introduce a novel tuning parameter for tree-based Newton boosting which is interpretable and important for predictive accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308381",
    "keywords": [],
    "authors": [
      {
        "surname": "Sigrist",
        "given_name": "Fabio"
      }
    ]
  },
  {
    "title": "An efficient path-based approach for influence maximization in social networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114168",
    "abstract": "It is no secret that the word-of-mouth has very powerful effect on the social interconnections, but the question is “which factors influence the word-of-mouth effectiveness?” The answer hinges on a small set of nodes if activated, would spread information all over the network. This is a major issue in the social network analysis, known as the influence maximization, through which the influence spreading through a small set of seed nodes is maximized. Finding this highly influential node set has still remained challenging due to the non-deterministic polynomial-time (NP)-hard nature of the influence maximization problem and the significant increase in the size of social networks. Thus, in this paper, an efficient path-based algorithm was proposed to address this problem from two complementary perspectives adapting the proposed algorithm in the large-scale networks. One perspective is approximating the influence spread efficiently using two features of the nodes: the degrees and the independent influence path. The second perspective is pruning out the uninfluential nodes and reducing the computation volume in approximating the influence spread using a practical preprocessing heuristic method. Extensive empirical experiments were conducted to evaluate the performance of the proposed algorithm in the seven real-world networks and the results were compared with a plenty of the state-of-the-art algorithms. The results revealed that the proposed algorithm outperforms its counterparts, by offering an outstanding trade-off between the quality and efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309064",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computation",
      "Computer science",
      "Engineering",
      "Heuristic",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Node (physics)",
      "Path (computing)",
      "Perspective (graphical)",
      "Preprocessor",
      "Programming language",
      "Pruning",
      "Set (abstract data type)",
      "Social media",
      "Social network (sociolinguistics)",
      "Structural engineering",
      "Theoretical computer science",
      "Time complexity",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Kianian",
        "given_name": "Sahar"
      },
      {
        "surname": "Rostamnia",
        "given_name": "Mehran"
      }
    ]
  },
  {
    "title": "SuFMoFPA: A superpixel and meta-heuristic based fuzzy image segmentation approach to explicate COVID-19 radiological images",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114142",
    "abstract": "Coronavirus disease 2019 or COVID-19 is one of the biggest challenges which are being faced by mankind. Researchers are continuously trying to discover a vaccine or medicine for this highly infectious disease but, proper success is not achieved to date. Many countries are suffering from this disease and trying to find some solution that can prevent the dramatic spread of this virus. Although the mortality rate is not very high, the highly infectious nature of this virus makes it a global threat. RT-PCR test is the only means to confirm the presence of this virus to date. Only precautionary measures like early screening, frequent hand wash, social distancing use of masks, and other protective equipment can prevent us from this virus. Some researches show that the radiological images can be quite helpful for the early screening purpose because some features of the radiological images indicate the presence of the COVID-19 virus and therefore, it can serve as an effective screening tool. Automated analysis of these radiological images can help the physicians and other domain experts to study and screen the suspected patients easily and reliably within the stipulated amount of time. This method may not replace the traditional RT-PCR method for detection but, it can be helpful to filter the suspected patients from the rest of the community that can effectively reduce the spread in the of this virus. A novel method is proposed in this work to segment the radiological images for the better explication of the COVID-19 radiological images. The proposed method will be known as SuFMoFPA (Superpixel based Fuzzy Modified Flower Pollination Algorithm). The type 2 fuzzy clustering system is blended with this proposed approach to get the better-segmented outcome. Obtained results are quite promising and outperforming some of the standard approaches which are encouraging for the practical uses of the proposed approach to screening the COVID-19 patients.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308897",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Disease",
      "Infectious disease (medical specialty)",
      "Medicine",
      "Pathology",
      "Radiological weapon",
      "Radiology",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Chakraborty",
        "given_name": "Shouvik"
      },
      {
        "surname": "Mali",
        "given_name": "Kalyani"
      }
    ]
  },
  {
    "title": "MOSOA: A new multi-objective seagull optimization algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114150",
    "abstract": "This study introduces the extension of currently developed Seagull Optimization Algorithm (SOA) in terms of multi-objective problems, which is entitled as Multi-objective Seagull Optimization Algorithm (MOSOA). In this algorithm, a concept of dynamic archive is introduced, which has the feature to cache the non-dominated Pareto optimal solutions. The roulette wheel selection approach is utilized to choose the effective archived solutions by simulating the migration and attacking behaviors of seagulls. The proposed algorithm is approved by testing it with twenty-four benchmark test functions, and its performance is compared with existing metaheuristic algorithms. The developed algorithm is analyzed on six constrained problems of engineering design to assess its appropriateness for finding the solutions of real-world problems. The outcomes from the empirical analyzes depict that the proposed algorithm is better than other existing algorithms. The proposed algorithm also considers those Pareto optimal solutions, which demonstrate high convergence.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308940",
    "keywords": [
      "Algorithm",
      "Benchmark (surveying)",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Feature selection",
      "Geodesy",
      "Geography",
      "Geometry",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Multi-objective optimization",
      "Pareto principle",
      "Roulette",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Dhiman",
        "given_name": "Gaurav"
      },
      {
        "surname": "Singh",
        "given_name": "Krishna Kant"
      },
      {
        "surname": "Soni",
        "given_name": "Mukesh"
      },
      {
        "surname": "Nagar",
        "given_name": "Atulya"
      },
      {
        "surname": "Dehghani",
        "given_name": "Mohammad"
      },
      {
        "surname": "Slowik",
        "given_name": "Adam"
      },
      {
        "surname": "Kaur",
        "given_name": "Amandeep"
      },
      {
        "surname": "Sharma",
        "given_name": "Ashutosh"
      },
      {
        "surname": "Houssein",
        "given_name": "Essam H."
      },
      {
        "surname": "Cengiz",
        "given_name": "Korhan"
      }
    ]
  },
  {
    "title": "Fingerprinting-assisted UWB-based localization technique for complex indoor environments",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114188",
    "abstract": "Among the numerous radio-based solutions for indoor localization, ultra-wideband (UWB) technology is of particular interest due to its signal characteristics. The wide bandwidth of the UWB signal provides a fine time resolution of the transmitted pulses that enables a centimetre-level ranging accuracy under line-of-sight (LOS) conditions even in multipath intensive indoor environments. Nevertheless, it is still a challenge to implement accurate UWB-based localization in complex multi-room indoor environments at low cost because of a large number of static UWB anchors that may need to be deployed in order to provide an adequate LOS coverage in every segment of the environment. Therefore, there is a strong interest in developing UWB-based localization techniques that will provide acceptable accuracy under partially LOS coverage conditions. In this paper, we present a novel hybrid method that combines two conventional localization techniques, trilateration and fingerprinting, to address the problem of cost-effective UWB-based localization in complex indoor environments. With the proposed method, the target location is determined by a trilateration algorithm, while a fingerprinting-based algorithm is used to provide additional distances for trilateration in cases when there is an insufficient number of available LOS measurements. The additional distances are generated by a non-parametric regression algorithm that relies on a fingerprint database to map all available online range measurements (LOS as well non-LOS) to distances between the target and the set of pre-defined reference points. To minimize human effort in fingerprint collection, the indoor environment is site-surveyed in a room-by-room fashion with auxiliary UWB anchors temporarily placed at up to three reference points in the surveyed room. The method is validated through an extensive indoor measurement campaign with commercially available UWB transceivers. The experimental results show that the proposed method achieves sub-decimetre level localization accuracy under typical real-world conditions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309222",
    "keywords": [
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Cartography",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Fingerprint (computing)",
      "Geography",
      "Mathematics",
      "Multipath propagation",
      "Non-line-of-sight propagation",
      "Parametric statistics",
      "Ranging",
      "Real-time computing",
      "Statistics",
      "Telecommunications",
      "Triangulation",
      "Trilateration",
      "Ultra-wideband",
      "Wireless"
    ],
    "authors": [
      {
        "surname": "Djosic",
        "given_name": "Sandra"
      },
      {
        "surname": "Stojanovic",
        "given_name": "Igor"
      },
      {
        "surname": "Jovanovic",
        "given_name": "Milica"
      },
      {
        "surname": "Nikolic",
        "given_name": "Tatjana"
      },
      {
        "surname": "Djordjevic",
        "given_name": "Goran Lj."
      }
    ]
  },
  {
    "title": "Ultrasonic guided wave based structural damage detection and localization using model assisted convolutional and recurrent neural networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114189",
    "abstract": "The inverse problem of damage identification involves real-time, continuous observation of structures to detect any undesired, abnormal behavior and ultrasonic guided waves are considered as one of the preferred candidates for this. A parallel implementation of a reduced-order spectral finite element model is utilized to formulate the forward problem in an isotropic and a composite waveguide. In this work, along with a time-series dataset, a 2D representation of continuous wavelet transformation based time–frequency dataset is also developed. The datasets are corrupted with several levels of Gaussian random noise to incorporate different kinds of uncertainties and noise present in the real scenario. Deep learning networks like convolutional and recurrent neural networks are utilized to numerically approximate the solution of the inverse problem. A hybrid strategy of classification and regression in a supervised setting is proposed for combined damage detection and localization. The performance of the networks is compared based on metrics like accuracy, loss value, mean absolute error, mean absolute percentage error, and coefficient of determination. The predictions from conventional machine learning algorithms, trained on feature engineered dataset are compared with the deep learning algorithms. The generalization of the trained deep networks on different excitation frequencies and a higher level of uncertainties is also highlighted in this work.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309234",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Composite material",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Feature (linguistics)",
      "Generalization",
      "Image (mathematics)",
      "Inverse problem",
      "Linguistics",
      "Materials science",
      "Mathematical analysis",
      "Mathematics",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Structural health monitoring"
    ],
    "authors": [
      {
        "surname": "Rautela",
        "given_name": "Mahindra"
      },
      {
        "surname": "Gopalakrishnan",
        "given_name": "S."
      }
    ]
  },
  {
    "title": "A robust personalized location recommendation based on ensemble learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114065",
    "abstract": "Recommender systems (RSs) have attracted considerable attention with the aim of optimizing location service efficiency since a large volume of information is generated by location-based social networks. Prediction accuracy is generally considered the main performance evaluation criterion, while the stability of RSs, which might be affected by uncertainty, has rarely been documented. To guarantee the robustness of RSs on the basis of two essential elements, accuracy and stability, this paper proposes an ensemble-based personalized location recommendation (EPLR) algorithm, in which several different categories of individual models are pre-trained to provide a knowledge base, and the accuracy metric in terms of F1 and the information gain (IG) are individually calculated for each user and are used as personalized weights to integrate the individual models. Notably, IG is used as an evaluation index of system stability. To demonstrate and evaluate EPLR in detail, four representative recommendation algorithms, i.e., user-based collaborative filtering (UBCF), singular value decomposition (SVD), friend-based collaborative filtering (FCF) and kernel density estimation (KDE), are selected as individual models for demonstration. Extensive experiments are conducted on two popular datasets: Brightkite and Gowalla. Additionally, three published ensemble recommendation algorithms and four individual models are implemented for comparison. The experimental results show that EPLR outperforms the other considered algorithms in terms of prediction accuracy and exhibits a promising advantage in system stability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308277",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Ensemble learning",
      "Machine learning",
      "Recommender system"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Jun"
      },
      {
        "surname": "Han",
        "given_name": "Lixin"
      },
      {
        "surname": "Gou",
        "given_name": "Zhinan"
      },
      {
        "surname": "Yang",
        "given_name": "Yi"
      },
      {
        "surname": "Yuan",
        "given_name": "Xiaofeng"
      },
      {
        "surname": "Li",
        "given_name": "Jingxian"
      },
      {
        "surname": "Li",
        "given_name": "Shu"
      }
    ]
  },
  {
    "title": "Kernelized Unified Domain Adaptation on Geometrical Manifolds",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114078",
    "abstract": "Primitive machine learning algorithms like the k-nearest Neighbor (k-NN) and Support Vector Machine (SVM) are a major challenge for expert and intelligent systems that recognize objects with large-scale variations in lighting conditions, backgrounds, color, size, etc. The variations may be due to the fact that the training and test data may come from related but different domains. Considerable effort has been put into the advancement of domain adaptation methods. However, most of the existing work only concentrates on considering only a few of the following goals or objectives: (i) subspace alignment; (ii) Minimization of distribution divergence by using the Maximum Mean Discrepancy (MMD) criterion; (iii) Preservation of source domain discrimination information; (iv) Preservation of original similarity of the data samples; (v) Maximization of target domain variance. Current approaches to preserve source domain discriminant information can easily mis-classify target domain samples which are distributed near the edge of the cluster. In order to overcome the limitations of existing domain adaptation methods, and expert and intelligent systems, we propose the Unified Domain Adaptation on Geometrical Manifolds (UDAGM) framework. UDAGM optimizes all the aforementioned objectives jointly as well as uses the Regularized Coplanar Discriminant Analysis (RCDA) method for better inter-class separability and intra-class compactness. In addition, we extend our proposed framework UDAGM to a kernelised version in order to deal with non-linear separable datasets. Extensive experimentation on two real-world problems datasets (PIE face recognition and Office-Caltech) has proven that the proposed frameworks outperform several approaches to domain adaptation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030837X",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Discriminant",
      "Divergence (linguistics)",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Similarity (geometry)",
      "Subspace topology",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Sanodiya",
        "given_name": "Rakesh Kumar"
      },
      {
        "surname": "Mathew",
        "given_name": "Jimson"
      },
      {
        "surname": "Aditya",
        "given_name": "Rohan"
      },
      {
        "surname": "Jacob",
        "given_name": "Ashish"
      },
      {
        "surname": "Nayanar",
        "given_name": "Bharadwaj"
      }
    ]
  },
  {
    "title": "Ant colony optimization with horizontal and vertical crossover search: Fundamental visions for multi-threshold image segmentation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114122",
    "abstract": "The ant colony optimization (ACO) is the most exceptionally fundamental swarm-based solver for realizing discrete problems. In order to make it also suitable for solving continuous problems, a variant of ACO (ACOR) has been proposed already. The deep-rooted ACO always stands out in the eyes of well-educated researchers as one of the best-designed metaheuristic ways for realizing the solutions to real-world problems. However, ACOR has some stochastic components that need to be further improved in terms of solution quality and convergence speed. Therefore, to effectively improve these aspects, this in-depth research introduced horizontal crossover search (HCS) and vertical crossover search (VCS) into the ACOR and improved the selection mechanism of the original ACOR to form an improved algorithm (CCACO) for the first time. In CCACO, the HCS is mainly intended to increase the convergence rate. Meanwhile, the VCS and the developed selection mechanism are mainly aimed at effectively improving the ability to avoid dwindling into local optimal (LO) and the convergence accuracy. To reach next-level strong results for image segmentation and better illustrate its effectiveness, we conducted a series of comparative experiments with 30 benchmark functions from IEEE CEC 2014. In the experiment, we compared the developed CCACO with well-known conventional algorithms and advanced ones. All experimental results also show that its convergence speed and solution quality are superior to other algorithms, and its ability to avoid dropping into local optimum (LO) is more reliable than that of its peers. Furthermore, to further illustrate its enhanced performance, we applied it to image segmentation based on multi-threshold image segmentation (MTIS) method with a non-local means 2D histogram and Kapur's entropy. In the experiment, it was compared with existing competitive algorithms at low and high threshold levels. The experimental results show that the proposed CCACO achieves excellent segmentation results at both low and high threshold levels. For any help and guidance regarding this research, readers, and industry activists can refer to the background info at http://aliasgharheidari.com/.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308708",
    "keywords": [
      "Algorithm",
      "Ant colony optimization algorithms",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer security",
      "Convergence (economics)",
      "Crossover",
      "Economic growth",
      "Economics",
      "Geodesy",
      "Geography",
      "Key (lock)",
      "Local search (optimization)",
      "Mathematical optimization",
      "Mathematics",
      "Meta-optimization",
      "Metaheuristic",
      "Parallel metaheuristic",
      "Particle swarm optimization",
      "Rate of convergence",
      "Segmentation",
      "Selection (genetic algorithm)",
      "Solver",
      "Swarm intelligence"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Dong"
      },
      {
        "surname": "Liu",
        "given_name": "Lei"
      },
      {
        "surname": "Yu",
        "given_name": "Fanhua"
      },
      {
        "surname": "Heidari",
        "given_name": "Ali Asghar"
      },
      {
        "surname": "Wang",
        "given_name": "Mingjing"
      },
      {
        "surname": "Oliva",
        "given_name": "Diego"
      },
      {
        "surname": "Muhammad",
        "given_name": "Khan"
      },
      {
        "surname": "Chen",
        "given_name": "Huiling"
      }
    ]
  },
  {
    "title": "Medical image based breast cancer diagnosis: State of the art and future directions",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114095",
    "abstract": "The intervention of medical imaging has significantly improved early diagnosis of breast cancer. Different radiological and microscopic imaging modalities are frequently utilized by medical practitioners for identification and categorization of different breast abnormalities by manual scrutiny. The meticulous classification of different breast abnormalities is challenging, because of ambiguous imaging data and due to indistinguishable characteristics of benign and malignant breast lesions. However, with the advent in applications of Artificial Intelligence (AI) in healthcare, researchers have turned their focus towards designing of efficient intelligent computer aided detection and diagnosis systems for prognosis of this catastrophic disease using image processing and computer vision (CV) techniques. An abundance of work could be found in literature on classification of different breast abnormalities, where majority of them has dealt with binary classification (i.e. benign and malignant). In current study, a comprehensive review has been presented to analyze and evaluate state of the art proposed methodologies for breast cancer diagnosis based over commonly used breast screening imaging modalities. The studies under consideration are mainly categorized into statistical machine learning based and deep learning based classifier, where deep classifiers further sub-categorized into models built from scratch and transfer learning based models. A number of factors have been taken to compare the performance of these classification models, on the basis of which some recommendations are provided for researcher to precede this work in future.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308502",
    "keywords": [
      "Artificial intelligence",
      "Binary classification",
      "Breast cancer",
      "Breast cancer screening",
      "Breast imaging",
      "Cancer",
      "Categorization",
      "Classifier (UML)",
      "Computer science",
      "Computer-aided diagnosis",
      "Contextual image classification",
      "Deep learning",
      "Image (mathematics)",
      "Internal medicine",
      "Machine learning",
      "Mammography",
      "Medical imaging",
      "Medical physics",
      "Medicine",
      "Modalities",
      "Social science",
      "Sociology",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Tariq",
        "given_name": "Mehreen"
      },
      {
        "surname": "Iqbal",
        "given_name": "Sajid"
      },
      {
        "surname": "Ayesha",
        "given_name": "Hareem"
      },
      {
        "surname": "Abbas",
        "given_name": "Ishaq"
      },
      {
        "surname": "Ahmad",
        "given_name": "Khawaja Tehseen"
      },
      {
        "surname": "Niazi",
        "given_name": "Muhammad Farooq Khan"
      }
    ]
  },
  {
    "title": "Survey of feature extraction and classification techniques to identify plant through leaves",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114181",
    "abstract": "This paper provides a comprehensive survey of various techniques used in computer vision for the automatic identification of plants with the help of leaf images. The extracted information is used by the botanists to identify different species of plants and use their medicinal or other properties. With the upsurge of human interference, the number of plants seems to decrease, but their automatic identification can lead to conservation. The Leaf images may be acquired by a phone camera or a digital camera mounted on a tripod stand. The leaves may be covered by dirt, shadows, or hidden under other leaves. Real-life applications based on the automatic identification of plants can successfully identify even similar-looking plant leaves in all environmental conditions. This paper provides a state-of-the-art review of different leaf extraction techniques which are categorized according to the features of leaf used and their pros and cons. We also discuss and compare the various classifiers used in the identification process. The conclusion of the paper also provides different areas of improvement and future work.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309167",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Digital camera",
      "Dirt",
      "Feature extraction",
      "Geography",
      "Identification (biology)",
      "Pattern recognition (psychology)",
      "Plant identification"
    ],
    "authors": [
      {
        "surname": "Sachar",
        "given_name": "Silky"
      },
      {
        "surname": "Kumar",
        "given_name": "Anuj"
      }
    ]
  },
  {
    "title": "Exploring the forecasting approach for road accidents: Analytical measures with hybrid machine learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113855",
    "abstract": "Urban traffic forecasting models generally follow either a Gaussian Mixture Model (GMM) or a Support Vector Classifier (SVC) to estimate the features of potential road accidents. Although SVC can provide good performances with less data than GMM, it incurs a higher computational cost. This paper proposes a novel framework that combines the descriptive strength of the Gaussian Mixture Model with the high-performance classification capabilities of the Support Vector Classifier. A new approach is presented that uses the mean vectors obtained from the GMM model as input to the SVC. Experimental results show that the approach compares very favorably with baseline statistical methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306667",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Gaussian",
      "Gaussian process",
      "Machine learning",
      "Mixture model",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Sangare",
        "given_name": "Mamoudou"
      },
      {
        "surname": "Gupta",
        "given_name": "Sharut"
      },
      {
        "surname": "Bouzefrane",
        "given_name": "Samia"
      },
      {
        "surname": "Banerjee",
        "given_name": "Soumya"
      },
      {
        "surname": "Muhlethaler",
        "given_name": "Paul"
      }
    ]
  },
  {
    "title": "Improved few-shot learning method for transformer fault diagnosis based on approximation space and belief functions",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114105",
    "abstract": "Incomplete and uncertain information is frequently observed in the data analysis processes, which has become one of main challenges for the development of fault diagnosis techniques of transformers. To address few fault cases and deficient monitoring information in diagnostic tasks, this paper provides an improved few-shot learning method based on approximation space and belief functions to accomplish fault diagnosis of transformers. The decision-making table, as an efficient structure to map the weakly correlated attributes, is extracted from transformer cases and maintenance experience. Then the approximation space is used to describe attribute correlations between diagnostic rules and the diagnostic task. We employ the 0.5-approximation set strategy to obtain the diagnostic results when the information is sufficient. Furthermore, we propose a modified basic probability assignment (BPA) calculation method to build belief functions for diagnosis when information is scanty. The modified method is verified capable of improving the decision-making reliability. The overall recognition accuracy of fault diagnosis by our improved few-shot learning algorithm is over 87% which is higher than other four peer methods. This method also shows a potential for good expandability when new diagnostic rules of transformers are discovered.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308587",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Electrical engineering",
      "Engineering",
      "Machine learning",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Yaoyu"
      },
      {
        "surname": "Li",
        "given_name": "Yuan"
      },
      {
        "surname": "Wang",
        "given_name": "Yijing"
      },
      {
        "surname": "Zhong",
        "given_name": "Dexing"
      },
      {
        "surname": "Zhang",
        "given_name": "Guanjun"
      }
    ]
  },
  {
    "title": "Catering for unique tastes: Targeting grey-sheep users recommender systems through one-class machine learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114061",
    "abstract": "In recommendation systems, the grey-sheep problem refers to users with unique preferences and tastes that make it difficult to develop accurate profiles. That is, the similarity search approach typically followed during the recommendation process fails to yield good results. Most research does not focus on such users and thus fails to cater to more exotic tastes and emerging trends, leading to a subsequent loss in revenue and marketing opportunities. One suggested solution is to use one-class classification to generate a prediction list for these users, where decision boundaries are learned that distinguish between normal and grey-sheep users. In this paper, we present the grey-sheep one-class recommendation (GSOR) framework designed to create accurate prediction models while taking both regular and grey-sheep users into account. In addition, we introduce a novel grey-sheep movie recommendation benchmark to be used by current and future researchers. When evaluating our GSOR framework against this benchmark, our results indicate the value of combining cluster analysis, outlier detection, and one-class learning to generate relevant and timely recommendation lists from data sets that contain grey-sheep users. Specifically, by employing one-class decision tree algorithms, our GSOR framework was able to outperform traditional collaborative filtering-based recommendation systems in both accuracy and model construction time. Furthermore, we report that having grey-sheep users in the system often had a positive impact on the learning and recommendation processes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308241",
    "keywords": [
      "Accounting",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Business",
      "Class (philosophy)",
      "Collaborative filtering",
      "Computer science",
      "Data mining",
      "Decision tree",
      "Focus (optics)",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Information retrieval",
      "Machine learning",
      "Operating system",
      "Optics",
      "Outlier",
      "Physics",
      "Process (computing)",
      "Recommender system",
      "Revenue",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Alabdulrahman",
        "given_name": "Rabaa"
      },
      {
        "surname": "Viktor",
        "given_name": "Herna"
      }
    ]
  },
  {
    "title": "A time series-based statistical approach for outbreak spread forecasting: Application of COVID-19 in Greece",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114077",
    "abstract": "The aim of this paper is the generation of a time-series based statistical data-driven procedure in order to track an outbreak. At first are used univariate time series models in order to predict the evolution of the reported cases. Moreover, are considered combinations of the models in order to provide more accurate and robust results. Additionally, statistical probability distributions are considered in order to generate future scenarios. Final step is the build and use of an epidemiological model (tSIR) and the calculation of an epidemiological ratio (R0) for estimating the termination of the outbreak. The time series models include Exponential Smoothing and ARIMA approaches from the classical models, also Feed-Forward Artificial Neural Networks and Multivariate Adaptive Regression Splines from the machine learning toolbox. Combinations include simple mean, Newbolt-Granger and Bates-Granger approaches. Finally, the tSIR model and the R0 ratio are used for estimating the spread and the reversion of the pandemic. The suggested procedure is used to track the COVID-19 epidemic in Greece. This epidemic has appeared in China in December 2019 and has been widespread since then to all over the world. Greece is the center of this empirical study as is considered an early successful paradigm of resistance against the virus.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308368",
    "keywords": [
      "Autoregressive integrated moving average",
      "Biology",
      "Computer science",
      "Econometrics",
      "Exponential smoothing",
      "Machine learning",
      "Mathematics",
      "Multivariate statistics",
      "Paleontology",
      "Series (stratigraphy)",
      "Statistics",
      "Time series",
      "Univariate"
    ],
    "authors": [
      {
        "surname": "Katris",
        "given_name": "Christos"
      }
    ]
  },
  {
    "title": "A new item similarity based on α -divergence for collaborative filtering in sparse data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114074",
    "abstract": "In big data era, collaborative filtering as one of the most popular recommendation techniques plays an important role to promote the development of online trade. Similarity measurement is a core step in collaborative filtering as it not only determines the selection of neighbors but also has a decisive influence on the recommendation quality. However, most of existing similarity measures depend on the co-rated cases(i.e., cases where different users rated the same items or different items were rated by the same users), which usually leads to low data utilization and even poor recommendation results in a sparse dataset. To alleviate this problem, we proposed a new item similarity measure based on α -divergence, which does the computation according to the probability density distribution of ratings and greatly reduces the dependence on co-rated cases. Furthermore, the presented item similarity measure also considers the impact of the absolute number of ratings and the proportion of co-rated cases on the computation results, which effectively improves the accuracy of recommendation. Experiments on three open datasets suggest that the proposed scheme has high prediction accuracy and good adaptability to sparse data. Therefore, it has high potential to be applied in recommender systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308344",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Collaborative filtering",
      "Computation",
      "Computer science",
      "Data mining",
      "Divergence (linguistics)",
      "Image (mathematics)",
      "Information retrieval",
      "Linguistics",
      "Machine learning",
      "Measure (data warehouse)",
      "Philosophy",
      "Recommender system",
      "Similarity (geometry)",
      "Similarity measure"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yong"
      },
      {
        "surname": "Wang",
        "given_name": "Pengyu"
      },
      {
        "surname": "Liu",
        "given_name": "Zhuo"
      },
      {
        "surname": "Zhang",
        "given_name": "Leo Yu"
      }
    ]
  },
  {
    "title": "Decision modeling and analysis in new product development considering supply chain uncertainties: A multi-functional expert based approach",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114016",
    "abstract": "Successful new product development projects and extant research literature advocate for inclusion of inputs pertaining to the supply chain at early stages of product development to proactively identify risk averse product design concepts. To this end, we devise an analytical framework to converge upon product design concept(s) that would be associated with lesser supply chain risks, usually function of both technical and commercialization considerations. The high-level and constituent lower-level supply chain risks are represented by parent and root nodes respectively within the devised Bayesian network driven research framework. Thereafter, a quantitative measure denoted as SCRI (supply chain risk index) is evolved that yields overall composite risk numbers corresponding to respective design concepts at different risk states. Validation and comparison of the devised method with an extant study illustrates the consistency and reliability of the study. It is found that the risk propensity of a particular design concept is inversely related to the probabilistic utility of that particular concept. The case of a construction power tool of a global firm is used to demonstrate the methodology. Our research addresses an important future research pathway as argued by Hosseini et al. (2020) that extant research literature is devoid of decision-making frameworks focused on measurement and analysis the propagation of risks on complex networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307892",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Business",
      "Computer science",
      "Consistency (knowledge bases)",
      "Evolutionary biology",
      "Extant taxon",
      "Geometry",
      "Marketing",
      "Mathematics",
      "New product development",
      "Operations research",
      "Probabilistic logic",
      "Product (mathematics)",
      "Risk analysis (engineering)",
      "Service management",
      "Supply chain",
      "Supply chain management",
      "Supply chain risk management"
    ],
    "authors": [
      {
        "surname": "Goswami",
        "given_name": "Mohit"
      },
      {
        "surname": "Daultani",
        "given_name": "Yash"
      },
      {
        "surname": "De",
        "given_name": "Arijit"
      }
    ]
  },
  {
    "title": "Numerical simulation of the novel coronavirus spreading",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114109",
    "abstract": "The COVID-19 virus outbreak has affected most of the world in 2020. This paper deals with artificial intelligence (AI) methods that can address the problem of predicting scale, dynamics and sensitivity of the outbreak to preventive actions undertaken with a view to combatting the epidemic. In our study, we developed a cellular automata (CA) model for simulating the COVID-19 disease spreading. The enhanced infectious disease dynamics S E I R (Susceptible, Exposed, Infectious, and Recovered) model was applied to estimate the epidemic trends in Poland, France, and Spain. We introduced new parameters into the simulation framework which reflect the statistically confirmed dependencies such as age-dependent death probability, a different definition of the contact rate and enhanced parameters reflecting population mobility. To estimate key epidemiological measures and to predict possible dynamics of the disease, we juxtaposed crucial CA framework parameters to the reported COVID-19 values, e.g. length of infection, mortality rates and the reproduction number. Moreover, we used real population density and age structures of the studied epidemic populations. The model presented allows for the examination of the effectiveness of preventive actions and their impact on the spreading rate and the duration of the disease. It also shows the influence of structure and behavior of the populations studied on key epidemic parameters, such as mortality and infection rates. Although our results are critically dependent on the assumptions underpinning our model and there is considerable uncertainty associated with the outbreaks at such an early epidemic stage, the obtained simulation results seem to be in general agreement with the observed behavior of the real COVID-19 disease, and our numerical framework can be effectively used to analyze the dynamics and efficacy of epidemic containment methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308605",
    "keywords": [
      "Artificial intelligence",
      "Basic reproduction number",
      "Cellular automaton",
      "Computer science",
      "Coronavirus disease 2019 (COVID-19)",
      "Demography",
      "Disease",
      "Econometrics",
      "Environmental health",
      "Epidemic model",
      "Infectious disease (medical specialty)",
      "Mathematics",
      "Medicine",
      "Mortality rate",
      "Outbreak",
      "Pathology",
      "Population",
      "Sociology",
      "Statistics",
      "Susceptible individual",
      "Virology"
    ],
    "authors": [
      {
        "surname": "Medrek",
        "given_name": "M."
      },
      {
        "surname": "Pastuszak",
        "given_name": "Z."
      }
    ]
  },
  {
    "title": "Ensemble of convolutional neural networks trained with different activation functions",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114048",
    "abstract": "Activation functions play a vital role in the training of Convolutional Neural Networks. For this reason, developing efficient and well-performing functions is a crucial problem in the deep learning community. The idea of these approaches is to allow a reliable parameter learning, avoiding vanishing gradient problems. The goal of this work is to propose an ensemble of Convolutional Neural Networks trained using several different activation functions. Moreover, a novel activation function is here proposed for the first time. Our aim is to improve the performance of Convolutional Neural Networks in small/medium sized biomedical datasets. Our results clearly show that the proposed ensemble outperforms Convolutional Neural Networks trained with a standard ReLU as activation function. The proposed ensemble outperforms with a p-value of 0.01 each tested stand-alone activation function; for reliable performance comparison we tested our approach on more than 10 datasets, using two well-known Convolutional Neural Networks: Vgg16 and ResNet50.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308150",
    "keywords": [
      "Activation function",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Code (set theory)",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Evolutionary biology",
      "Function (biology)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Maguolo",
        "given_name": "Gianluca"
      },
      {
        "surname": "Nanni",
        "given_name": "Loris"
      },
      {
        "surname": "Ghidoni",
        "given_name": "Stefano"
      }
    ]
  },
  {
    "title": "Decision modeling and analysis in new product development considering supply chain uncertainties: A multi-functional expert based approach",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114016",
    "abstract": "Successful new product development projects and extant research literature advocate for inclusion of inputs pertaining to the supply chain at early stages of product development to proactively identify risk averse product design concepts. To this end, we devise an analytical framework to converge upon product design concept(s) that would be associated with lesser supply chain risks, usually function of both technical and commercialization considerations. The high-level and constituent lower-level supply chain risks are represented by parent and root nodes respectively within the devised Bayesian network driven research framework. Thereafter, a quantitative measure denoted as SCRI (supply chain risk index) is evolved that yields overall composite risk numbers corresponding to respective design concepts at different risk states. Validation and comparison of the devised method with an extant study illustrates the consistency and reliability of the study. It is found that the risk propensity of a particular design concept is inversely related to the probabilistic utility of that particular concept. The case of a construction power tool of a global firm is used to demonstrate the methodology. Our research addresses an important future research pathway as argued by Hosseini et al. (2020) that extant research literature is devoid of decision-making frameworks focused on measurement and analysis the propagation of risks on complex networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307892",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Business",
      "Computer science",
      "Consistency (knowledge bases)",
      "Evolutionary biology",
      "Extant taxon",
      "Geometry",
      "Marketing",
      "Mathematics",
      "New product development",
      "Operations research",
      "Probabilistic logic",
      "Product (mathematics)",
      "Risk analysis (engineering)",
      "Service management",
      "Supply chain",
      "Supply chain management",
      "Supply chain risk management"
    ],
    "authors": [
      {
        "surname": "Goswami",
        "given_name": "Mohit"
      },
      {
        "surname": "Daultani",
        "given_name": "Yash"
      },
      {
        "surname": "De",
        "given_name": "Arijit"
      }
    ]
  },
  {
    "title": "Fractal Neural Network: A new ensemble of fractal geometry and convolutional neural networks for the classification of histology images",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114103",
    "abstract": "Classification of histology images is a task that has been widely explored on recent computer vision researches. The most studied approach for this task has been the application of deep learning through a convolutional neural network (CNN) model. However, the use of CNNs in the context of histological images classification has yet some limitations such as the need of large datasets, the slow training time and the difficult to implement a generalized model able to classify different types of histology tissues. In this paper, we propose an ensemble model based on handcrafted fractal features and deep learning that consists of combining the classification of two CNNs by applying the sum rule. We apply feature extraction to obtain 300 fractal features from different histological datasets. These features are reshaped into a 10 × 10 × 3 matrix to compose an artificial image that is given as input to the first CNN. The second CNN model receives as input the correspondent original image. After combining the results of both CNNs, accuracies that range from 89.66% up to 99.62% were obtained from five different datasets. Moreover, our model was able to classify images from datasets with imbalanced classes, without the need for images having the same resolution, and in relative fast training time. We also verified that the obtained results are compatible with the most recent and relevant studies recently published in the context of histology image classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308563",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Contextual image classification",
      "Convolutional neural network",
      "Deep learning",
      "Feature (linguistics)",
      "Feature extraction",
      "Fractal",
      "Image (mathematics)",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Roberto",
        "given_name": "Guilherme Freire"
      },
      {
        "surname": "Lumini",
        "given_name": "Alessandra"
      },
      {
        "surname": "Neves",
        "given_name": "Leandro Alves"
      },
      {
        "surname": "do Nascimento",
        "given_name": "Marcelo Zanchetta"
      }
    ]
  },
  {
    "title": "CSFCM: An improved fuzzy C-Means image segmentation algorithm using a cooperative approach",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114063",
    "abstract": "Fuzzy c-means (FCM) is one of the most widely used classification algorithms specially in image segmentation. Like any algorithm, FCM has some drawbacks such as the choice of the number of clusters and the cluster’s center initialization. In this work, we propose new approaches to deal with these two drawbacks. We propose for the first problem two approaches. The first proposed approach exploits neural networks and the Xie and Beni index, while the second one exploits the histogram. Concerning the second problem, we propose a new metaheuristics cooperation approach using the Genetic Algorithm (GA), Biogeography Based Algorithm(BBO), and Firefly Algorithm (FA). This cooperation is managed by a multi-agent system allowing to determine automatically the fittest metaheuristics parameters. Finally, we propose to use a histogram-based version of FCM to reduce the execution time of the algorithm. Experimental results show that our proposed approach improves the performance of the basic FCM algorithm and outperforms other methods proposed in the literature.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308253",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Data mining",
      "Exploit",
      "Firefly algorithm",
      "Fuzzy logic",
      "Genetic algorithm",
      "Histogram",
      "Image (mathematics)",
      "Initialization",
      "Machine learning",
      "Metaheuristic",
      "Particle swarm optimization",
      "Programming language",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Abdellahoum",
        "given_name": "Hamza"
      },
      {
        "surname": "Mokhtari",
        "given_name": "Nassim"
      },
      {
        "surname": "Brahimi",
        "given_name": "Abderrahmane"
      },
      {
        "surname": "Boukra",
        "given_name": "Abdelmadjid"
      }
    ]
  },
  {
    "title": "Gene selection and classification of microarray data method based on mutual information and moth flame algorithm.",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114012",
    "abstract": "Several techniques or methods may help in detecting diseases and cancer. Creating an effective method for extracting disease information is one of the major challenges in the classification of gene expression data as long as there is (in the presence) a massive amount of redundant data and noise. Bio-inspired algorithms are among the most effective when used for solving gene selection. Moth Flame Optimization Algorithm (MFOA) is computationally less expensive and can converge faster than other methods. In this paper, we propose a new extension of the MFOA called the modified Moth Flame Algorithm (mMFA), the mMFA is combined with Mutual Information Maximization (MIM) to solve gene selection in microarray data classification. Our approach Called Mutual Information Maximization – modified Moth Flame Algorithm (MIM-mMFA), the MIM based pre-filtering technique is used to measure the relevance and the redundancy of the genes, and the mMFA is used to evolve gene subsets and evaluated by the fitness function, which uses a Support Vector Machine (SVM) with Leave One Out Cross Validation (LOOCV) classifier and the number of selected genes. In order to test the performance of the proposed MIM-mMFA algorithm, we compared the MIM-mMFA algorithm with other recently published algorithms in the literature. The experiment results which have been conducted on sixteen benchmark datasets either binary-class or multi-class, confirm that MIM-mMFA algorithm provides a greater classification accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307855",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Feature selection",
      "Fitness function",
      "Genetic algorithm",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Mutual information",
      "Pattern recognition (psychology)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Dabba",
        "given_name": "Ali"
      },
      {
        "surname": "Tari",
        "given_name": "Abdelkamel"
      },
      {
        "surname": "Meftali",
        "given_name": "Samy"
      },
      {
        "surname": "Mokhtari",
        "given_name": "Rabah"
      }
    ]
  },
  {
    "title": "Comparing pre-trained language models for Spanish hate speech detection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114120",
    "abstract": "Nowadays, due to the great uncontrolled content posted daily on the Web, there has also been a huge increase in the dissemination of hate speech worldwide. Social media, blogs and community forums are examples where people are freely allowed to communicate. However, freedom of expression is not always respectful since offensive or insulting language is sometimes used. Social media companies often rely on users and content moderators to report on this type of content. Nevertheless, due to the large amount of content generated every day on the Web, automatic systems based on Natural Language Processing techniques are required for identifying abusive language online. To date, most of the systems developed to combat this problem are mainly focused on English content, but this issue is a worldwide concern and therefore other languages such as Spanish are involved. In this paper, we address the task of Spanish hate speech identification on social media and provide a deeper understanding of the capabilities of new techniques based on machine learning. In particular, we compare the performance of Deep Learning methods with recently pre-trained language models based on Transfer Learning as well as with traditional machine learning models. Our main contribution is the achievement of promising results in Spanish by applying multilingual and monolingual pre-trained language models such as BERT, XLM and BETO.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030868X",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Computer science",
      "Economics",
      "Identification (biology)",
      "Language identification",
      "Management",
      "Natural language",
      "Natural language processing",
      "Offensive",
      "Social media",
      "Task (project management)",
      "Transfer of learning",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Plaza-del-Arco",
        "given_name": "Flor Miriam"
      },
      {
        "surname": "Molina-González",
        "given_name": "M. Dolores"
      },
      {
        "surname": "Ureña-López",
        "given_name": "L. Alfonso"
      },
      {
        "surname": "Martín-Valdivia",
        "given_name": "M. Teresa"
      }
    ]
  },
  {
    "title": "An efficient multivariate feature ranking method for gene selection in high-dimensional microarray data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113971",
    "abstract": "Classification of microarray data plays a significant role in the diagnosis and prediction of cancer. However, its high-dimensionality (>tens of thousands) compared to the number of observations (<tens of hundreds) may lead to poor classification accuracy. In addition, only a fraction of genes is really important for the classification of a certain cancer, and thus feature selection is very essential in this field. Due to the time and memory burden for processing the high-dimensional data, univariate feature ranking methods are widely-used in gene selection. However, most of them are not that accurate because they only consider the relevance of features to the target without considering the redundancy among features. In this study, we propose a novel multivariate feature ranking method to improve the quality of gene selection and ultimately to improve the accuracy of microarray data classification. The method can be efficiently applied to high-dimensional microarray data. We embedded the formal definition of relevance into a Markov blanket (MB) to create a new feature ranking method. Using a few microarray datasets, we demonstrated the practicability of MB-based feature ranking having high accuracy and good efficiency. The method outperformed commonly-used univariate ranking methods and also yielded the better result even compared with the other multivariate feature ranking method due to the advantage of data efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030751X",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Computer science",
      "Data mining",
      "Dimensionality reduction",
      "Feature (linguistics)",
      "Feature selection",
      "Linguistics",
      "Machine learning",
      "Markov blanket",
      "Markov chain",
      "Markov model",
      "Markov property",
      "Minimum redundancy feature selection",
      "Multivariate statistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Ranking (information retrieval)",
      "Univariate"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Junghye"
      },
      {
        "surname": "Choi",
        "given_name": "In Young"
      },
      {
        "surname": "Jun",
        "given_name": "Chi-Hyuck"
      }
    ]
  },
  {
    "title": "A hybrid metaheuristic algorithm for location inventory routing problem with time windows and fuel consumption",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114034",
    "abstract": "We introduce a multi-period location-inventory-routing problem with time windows and fuel consumption, which simultaneously optimizes the location, routing, and inventory decisions for both the distribution center and customers in a multi-echelon supply chain. To better reflect reality, the fuel consumption is also incorporated into the variable transportation cost. The problem is formulated as a mixed integer nonlinear programming model. We then propose a two-stage hybrid metaheuristic algorithm to address this problem. In the first stage, a customized genetic algorithm is proposed. In the second stage, a gradient descent algorithm is used to improve the inventory decision to further reduce the total cost. Results of numerical experimentations on generated instances confirm the effectiveness of the algorithm. Results show that inventory management activities contribute considerably to total cost saving. Given the cost trade-off between transportation and inventory, the retailers’ inventory level shows more shortages after post-optimization, while the inventory level in the distribution centers can be either reduced or increased, depending on the spatial distribution of retailers in the vicinity of the distribution center. Sensitivity analysis on the model parameters is also conducted to provide managerial insights.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308046",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Automotive engineering",
      "Computer science",
      "Constraint logic programming",
      "Constraint satisfaction",
      "Consumption (sociology)",
      "Embedded system",
      "Engineering",
      "Fuel efficiency",
      "Hybrid algorithm (constraint satisfaction)",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Probabilistic logic",
      "Routing (electronic design automation)",
      "Social science",
      "Sociology",
      "Vehicle routing problem"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Weitiao"
      },
      {
        "surname": "Zhou",
        "given_name": "Wei"
      },
      {
        "surname": "Lin",
        "given_name": "Yue"
      },
      {
        "surname": "Xie",
        "given_name": "Yuanqi"
      },
      {
        "surname": "Jin",
        "given_name": "Wenzhou"
      }
    ]
  },
  {
    "title": "A n o m a l P : An approach for detecting anomalous protein conformations using deep autoencoders",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114070",
    "abstract": "Proteomics is nowadays one of the most important and relevant fields from computational biology, raising a lot of challenging and provocative questions. Gaining an understanding of protein dynamic and function as well as obtaining additional insights into the protein folding process is still of great interest in bioinformatics and medicine. This paper introduces a new approach A n o m a l P for detecting anomalous protein conformational transitions using deep autoencoders for encoding information about the structural similarity between proteins belonging to the same superfamily. Experiments are conducted on real protein data and the obtained results emphasize the potential of autoencoders to learn biological relevant patterns, such as proteins’ structural characteristics and that they are useful for detecting conformations or proteins which are likely to be anomalous with respect to a superfamily. The study performed in this paper is aimed to provide better insights of proteins structural similarity, with the broader goal of learning to predict proteins conformational transitions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308319",
    "keywords": [
      "Computer graphics (images)",
      "Computer science",
      "Scalable Vector Graphics",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Czibula",
        "given_name": "Gabriela"
      },
      {
        "surname": "Codre",
        "given_name": "Carmina"
      },
      {
        "surname": "Teletin",
        "given_name": "Mihai"
      }
    ]
  },
  {
    "title": "Critical components of data analytics in organizations: A research model",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114118",
    "abstract": "This study sought to propose and build a research model that explains the impact of specific critical components (i.e., Data analytics (DA) leadership, Data analytics (DA) management capabilities, and Data analytics (DA) talent quality) on performance (i.e., financial, market, and customer satisfaction) in organizations that utilize data analytics to gain competitive advantage. An instrument with six constructs was used and administered to collect data from employees of various organizations in the USA. Collected data were analyzed using partial least square structural equation modeling. Results revealed strong support for the strength of the proposed research model and that DA leadership, DA management capabilities, and DA talent are significant factors in achieving performance. Specifically, DA leadership significantly affects DA management capabilities; DA management capabilities significantly affect DA talent quality; and DA talent quality significantly affects performance (financial, market, and customer satisfaction). The theoretical and practical implications of the findings are discussed and recommendations for future research are provided.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308678",
    "keywords": [
      "Analytics",
      "Business",
      "Computer science",
      "Customer satisfaction",
      "Data analysis",
      "Data mining",
      "Data science",
      "Epistemology",
      "Knowledge management",
      "Machine learning",
      "Marketing",
      "Philosophy",
      "Quality (philosophy)",
      "Structural equation modeling"
    ],
    "authors": [
      {
        "surname": "Koohang",
        "given_name": "Alex"
      },
      {
        "surname": "Nord",
        "given_name": "Jeretta Horn"
      }
    ]
  },
  {
    "title": "Corrigendum to “Clustering and classification of time series using topological data analysis with applications to finance” [Expert Syst. Appl. 162 (2020) 113868]",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114140",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308873",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Machine learning",
      "Paleontology",
      "Series (stratigraphy)",
      "Time series",
      "Topological data analysis"
    ],
    "authors": [
      {
        "surname": "Majumdar",
        "given_name": "Sourav"
      },
      {
        "surname": "Kumar Laha",
        "given_name": "Arnab"
      }
    ]
  },
  {
    "title": "Multi-view ensemble learning method for microblog sentiment classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113987",
    "abstract": "With the rise of microblog services in recent years, microblog sentiment classification has been widely studied and applied in many fields like public opinion monitoring, commodity evaluation and market forecasting. Ensemble methods have been widely used in the feature construction and classification stages of microblog sentiment classification due to their excellent performance. In feature construction, most researchers use feature concatenation or ensemble methods to combine different features while the fusion of two methods is ignored. In classification, most ensemble classification methods combine classifiers based on majority voting or weighted averaging, and they do not fully consider the differences in the information contained in classifiers. In this paper, a novel multi-view ensemble learning method is proposed to fuse the information contained in different features for better microblog sentiment classification. This method consists of two stages: the local fusion stage and the global fusion stage. In the local fusion stage, the raw features and concatenation features are used to construct basic classifiers, and these basic classifiers are combined into five classifier groups to identify the microblog sentiment among all raw feature information. In the global fusion stage, these classifier groups with a global view are further integrated to make more accurate and comprehensive predictions. Two public microblog benchmark datasets provided by Sina Weibo are used in the experiment, and the experimental results show that our method outperforms other compared methods in identifying the polarities of microblog posts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307648",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Classifier (UML)",
      "Combinatorics",
      "Computer science",
      "Concatenation (mathematics)",
      "Data mining",
      "Ensemble learning",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Machine learning",
      "Majority rule",
      "Mathematics",
      "Microblogging",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Random subspace method",
      "Sentiment analysis",
      "Social media",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Ye",
        "given_name": "Xin"
      },
      {
        "surname": "Dai",
        "given_name": "Hongxia"
      },
      {
        "surname": "Dong",
        "given_name": "Lu-an"
      },
      {
        "surname": "Wang",
        "given_name": "Xinyue"
      }
    ]
  },
  {
    "title": "Optimal selection of heterogeneous ensemble strategies of time series forecasting with multi-objective programming",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114091",
    "abstract": "The excellent generalization performance of time series ensemble forecasting depends on the accuracy and diversity of the individual models. In this paper, a heterogeneous ensemble forecasting model with multi-objective programming for nonlinear time series is proposed. Accordingly, an improved multi-objective particle swarm optimization (MOPSO) algorithm integrated with a dynamic heterogeneous mutation operator is designed. The nonlinear time series of the Baltic Dry Index (BDI) is selected as the forecasting object to train, validate and test the ensemble forecasting model established in this paper. To verify the superior forecasting performance of the proposed model, 20 forecasting models including statistical models, machine learning models, and optimization algorithm–based ensemble models are utilized and compared. The experimental results under different lead times revealed that: 1) the forecasting approach with multi-objective programming has excellent robustness and can effectively exert out-of-sample prediction under different lead times for nonlinear time series; 2) with the increase of lead time, the out-of-sample forecasting performance would gradually decrease for all models, and the precision of the ensemble forecasting model is better than that of the individual forecasting model; 3) the forecasting performance of the MOPSO with crowding distance (MOPSOCD)-based ensemble forecasting model is better than that of benchmark machine learning models and other optimal ensemble forecasting models in terms of the prediction accuracy and statistical test results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308472",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Ensemble forecasting",
      "Ensemble learning",
      "Gene",
      "Generalization",
      "Genetic programming",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Paleontology",
      "Particle swarm optimization",
      "Robustness (evolution)",
      "Series (stratigraphy)",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Jianping"
      },
      {
        "surname": "Hao",
        "given_name": "Jun"
      },
      {
        "surname": "Feng",
        "given_name": "QianQian"
      },
      {
        "surname": "Sun",
        "given_name": "Xiaolei"
      },
      {
        "surname": "Liu",
        "given_name": "Mingxi"
      }
    ]
  },
  {
    "title": "A hybrid model for finding abbreviation–definition pairs from biomedical abstracts using heuristics-based sequence labeling and perceptron linear classifier",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114049",
    "abstract": "Automatic extraction of abbreviation and its definition from free format text is a constructive task in text mining. The previous work pertinent to automatic abbreviation/definition extraction from text followed either heuristics or machine learning approach. This paper proposes a hybrid model to identify abbreviation definition pairs from biomedical text. The proposed system uses two approaches i) To identify abbreviation-definition pairs, pattern matching is done through sequence labeling based on the heuristics approach. Three mapping strategies such as Predecessor Term Mapping, Word Level Mapping, and Character Level Mapping are used in sequence labeling tasks. ii) To validate the identified abbreviation-definition pair, an ANN-based approach such as a single layer neural network (perceptron) is used in this work. PubMed biomedical abstracts are utilized as a data source to find the abbreviation-definition pairs. The system performance is analyzed across six different entities in biomedical abstracts. The experiment result shows that our model achieves precision of 96.2%, recall of 92.4%, and F1 of 94.6%. To cross-validate the system performance, the proposed model is validated by using two corpuses AB3P and BioADI, the outcomes of which are discussed in the results section.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308162",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Classifier (UML)",
      "Computer science",
      "Constructive",
      "Data mining",
      "Economics",
      "Genetics",
      "Heuristics",
      "Linguistics",
      "Machine learning",
      "Management",
      "Matching (statistics)",
      "Mathematics",
      "Natural language processing",
      "Operating system",
      "Pattern recognition (psychology)",
      "Perceptron",
      "Philosophy",
      "Process (computing)",
      "Sequence (biology)",
      "Sequence labeling",
      "Statistics",
      "Task (project management)",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Menaha",
        "given_name": "R."
      },
      {
        "surname": "Jayanthi",
        "given_name": "VE."
      }
    ]
  },
  {
    "title": "An entropy empowered hybridized aggregation technique for group recommender systems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114111",
    "abstract": "Group recommender systems aim to suggest appropriate products/services to a group of users rather than individuals. These recommendations rely solely on determining group preferences, which is accomplished by an aggregation technique that combines individuals’ preferences. A plethora of aggregation techniques of various types have been developed so far. However, they consider only one particular aspect of the provided ratings in aggregating (e.g., counts, rankings, high averages), which imposes some limitations in capturing group members’ propensities. Besides, maximizing the number of satisfied members with the recommended items is as significant as producing items tailored to the individual users. Therefore, the ratings’ distribution is an essential element for aggregation techniques to discover items on which the majority of the members provided a consensus. This study proposes two novel aggregation techniques by hybridizing additive utilitarian and approval voting methods to feature popular items on which group members provided a consensus. Experiments conducted on three real-world benchmark datasets demonstrate that the proposed hybridized techniques significantly outperform all traditional methods. For the first time in the literature, we offer to use entropy to analyze rating distributions and detect items on which group members have reached no or little consensus. Equipping the proposed hybridized type aggregation techniques with the entropy calculation, we end up with an ultimate enhanced aggregation technique, Agreement without Uncertainty, which was proven to be even better than the hybridized techniques and outperform two recent state-of-the-art techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308617",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Data mining",
      "Entropy (arrow of time)",
      "Geodesy",
      "Geography",
      "Information retrieval",
      "Law",
      "Machine learning",
      "Majority rule",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Recommender system",
      "Voting"
    ],
    "authors": [
      {
        "surname": "Yalcin",
        "given_name": "Emre"
      },
      {
        "surname": "Ismailoglu",
        "given_name": "Firat"
      },
      {
        "surname": "Bilge",
        "given_name": "Alper"
      }
    ]
  },
  {
    "title": "An enhanced fitness function to recognize unbalanced human emotions data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114011",
    "abstract": "In cognitive science and human-computer interaction, automatic human emotion recognition using physiological stimuli is a key technology. This research considers two-class (positive and negative) of emotions recognition using electroencephalogram (EEG) signals in response to an emotional clip from the genres happy, amusement, sad, and horror. This paper introduces an enhanced fitness function named as eD-score to recognize emotions using EEG signals. The primary goal of this research is to assess how genres affect human emotions. We also analyzed human behaviour based on age and gender responsiveness. We have compared the performance of Multilayer Perceptron (MLP), K-nearest neighbors (KNN), Support Vector Machine (SVM), D-score Genetic Programming (DGP), and enhanced D-score Genetic Programming (eDGP) for classification of emotions. The analysis shows that for two class of emotion eDGP provides classification accuracy as 83.33%, 84.69%, 85.88%, and 87.61% for 50-50, 60-40, 70-30, and 10-fold cross-validations. Generalizability and reliability of this approach is evaluated by applying the proposed approach to publicly available EEG datasets DEAP and SEED. When participants in this research are exposed to amusement genre, their reaction is positive emotion. In compliance with the self-reported feelings, brain signals of 26–35 years of age group provided the highest emotional identification. Among genders, females are more emotionally active as compared to males. These results affirmed the potential use of our method for recognizing emotions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307843",
    "keywords": [
      "Affective computing",
      "Amusement",
      "Artificial intelligence",
      "Computer science",
      "Developmental psychology",
      "Electroencephalography",
      "Emotion classification",
      "Feeling",
      "Fitness function",
      "Generalizability theory",
      "Genetic algorithm",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Psychiatry",
      "Psychology",
      "Social psychology",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Acharya",
        "given_name": "Divya"
      },
      {
        "surname": "Varshney",
        "given_name": "Nandana"
      },
      {
        "surname": "Vedant",
        "given_name": "Anindiya"
      },
      {
        "surname": "Saxena",
        "given_name": "Yashraj"
      },
      {
        "surname": "Tomar",
        "given_name": "Pradeep"
      },
      {
        "surname": "Goel",
        "given_name": "Shivani"
      },
      {
        "surname": "Bhardwaj",
        "given_name": "Arpit"
      }
    ]
  },
  {
    "title": "Eye tracking algorithms, techniques, tools, and applications with an emphasis on machine learning and Internet of Things technologies",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114037",
    "abstract": "Eye tracking is the process of measuring where one is looking (point of gaze) or the motion of an eye relative to the head. Researchers have developed different algorithms and techniques to automatically track the gaze position and direction, which are helpful in different applications. Research on eye tracking is increasing owing to its ability to facilitate many different tasks, particularly for the elderly or users with special needs. This study aims to explore and review eye tracking concepts, methods, and techniques by further elaborating on efficient and effective modern approaches such as machine learning (ML), Internet of Things (IoT), and cloud computing. These approaches have been in use for more than two decades and are heavily used in the development of recent eye tracking applications. The results of this study indicate that ML and IoT are important aspects in evolving eye tracking applications owing to their ability to learn from existing data, make better decisions, be flexible, and eliminate the need to manually re-calibrate the tracker during the eye tracking process. In addition, they show that eye tracking techniques have more accurate detection results compared with traditional event-detection methods. In addition, various motives and factors in the use of a specific eye tracking technique or application are explored and recommended. Finally, some future directions related to the use of eye tracking in several developed applications are described.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308071",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Eye movement",
      "Eye tracking",
      "Gaze",
      "Human–computer interaction",
      "Machine learning",
      "Operating system",
      "Pedagogy",
      "Process (computing)",
      "Psychology",
      "The Internet",
      "Tracking (education)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Klaib",
        "given_name": "Ahmad F."
      },
      {
        "surname": "Alsrehin",
        "given_name": "Nawaf O."
      },
      {
        "surname": "Melhem",
        "given_name": "Wasen Y."
      },
      {
        "surname": "Bashtawi",
        "given_name": "Haneen O."
      },
      {
        "surname": "Magableh",
        "given_name": "Aws A."
      }
    ]
  },
  {
    "title": "Forecasting wavelet neural hybrid network with financial ensemble empirical mode decomposition and MCID evaluation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114097",
    "abstract": "By considering the properties of nonlinear data and the impact of historical data, this paper combines ensemble empirical mode decomposition (EEMD) into wavelet neural network with random time effective (WNNRT) to establish a hybrid neural network prediction model to improve the prediction accuracy of energy prices The EEMD is a noise-aided data analyze method, since it can effectively suppress pattern confusion and restore signal essence. Different from traditional models, the random time effective function that considers the timeliness of historical data and the random change of market environment is applied to the wavelet neural network to establish the WNNRT model. Moreover, multiscale complexity invariant distance (MCID) is utilized to evaluate the predicting performance of EEMD-WNNRT model. Further, the proposed model which is tested in predicting the impact on the global energy prices has carried on the empirical research, and it has also proved the corresponding superiority.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308514",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Data mining",
      "Hilbert–Huang transform",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Telecommunications",
      "Wavelet",
      "White noise"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Yu"
      },
      {
        "surname": "Wang",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Joint representation learning for multi-view subspace clustering",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113913",
    "abstract": "Multi-view subspace clustering has made remarkable achievements in the field of multi-view learning for high-dimensional data. However, many existing multi-view subspace clustering methods still have two disadvantages. First, most of them only recover the subspace structure from either consistent or specific perspective. Second, they often fail to take advantage of the high-order information among different views. To alleviate these two issues, this paper proposes a novel multi-view subspace clustering method, which aims to learn the view-specific representation as well as the low-rank tensor representation in a unified framework. Particularly, our method learns the view-specific representation from data samples by exploiting the local structure within each view. In the meantime, we generate the low-rank tensor representation from the view-specific representation to capture the high-order correlation across multiple views. Based on the joint representation learning framework, the proposed method is able to explore the intra-view pairwise information and the inter-view complementary information, so that the underlying data structure can be revealed and then the final clustering result can be obtained through the subsequent spectral clustering. Furthermore, in the proposed Joint Representation Learning for Multi-view Subspace Clustering (JRL-MSC) method, a unified objective function is formulated, which can be efficiently optimized by the alternating direction method of multipliers. Experimental results on multiple real-world data sets have demonstrated that our method outperforms the state-of-the-art counterparts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307077",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Feature learning",
      "Field (mathematics)",
      "Law",
      "Machine learning",
      "Mathematics",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Pure mathematics",
      "Rank (graph theory)",
      "Representation (politics)",
      "Subspace topology",
      "Tensor (intrinsic definition)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Guang-Yu"
      },
      {
        "surname": "Zhou",
        "given_name": "Yu-Ren"
      },
      {
        "surname": "Wang",
        "given_name": "Chang-Dong"
      },
      {
        "surname": "Huang",
        "given_name": "Dong"
      },
      {
        "surname": "He",
        "given_name": "Xiao-Yu"
      }
    ]
  },
  {
    "title": "An improved grey wolf optimizer for solving engineering problems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113917",
    "abstract": "In this article, an Improved Grey Wolf Optimizer (I-GWO) is proposed for solving global optimization and engineering design problems. This improvement is proposed to alleviate the lack of population diversity, the imbalance between the exploitation and exploration, and premature convergence of the GWO algorithm. The I-GWO algorithm benefits from a new movement strategy named dimension learning-based hunting (DLH) search strategy inherited from the individual hunting behavior of wolves in nature. DLH uses a different approach to construct a neighborhood for each wolf in which the neighboring information can be shared between wolves. This dimension learning used in the DLH search strategy enhances the balance between local and global search and maintains diversity. The performance of the proposed I-GWO algorithm is evaluated on the CEC 2018 benchmark suite and four engineering problems. In all experiments, I-GWO is compared with six other state-of-the-art metaheuristics. The results are also analyzed by Friedman and MAE statistical tests. The experimental results and statistical tests demonstrate that the I-GWO algorithm is very competitive and often superior compared to the algorithms used in the experiments. The results of the proposed algorithm on the engineering design problems demonstrate its efficiency and applicability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307107",
    "keywords": [
      "Algorithm",
      "Archaeology",
      "Benchmark (surveying)",
      "Computer science",
      "Convergence (economics)",
      "Demography",
      "Dimension (graph theory)",
      "Economic growth",
      "Economics",
      "Genetic algorithm",
      "Geodesy",
      "Geography",
      "Local search (optimization)",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Population",
      "Premature convergence",
      "Pure mathematics",
      "Sociology",
      "Suite"
    ],
    "authors": [
      {
        "surname": "Nadimi-Shahraki",
        "given_name": "Mohammad H."
      },
      {
        "surname": "Taghian",
        "given_name": "Shokooh"
      },
      {
        "surname": "Mirjalili",
        "given_name": "Seyedali"
      }
    ]
  },
  {
    "title": "An adjustable fuzzy chance-constrained network DEA approach with application to ranking investment firms",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113938",
    "abstract": "This paper presents a novel approach for performance appraisal and ranking of decision-making units (DMUs) with two-stage network structure in the presence of imprecise and vague data. In order to achieve this goal, two-stage data envelopment analysis (DEA) model, adjustable possibilistic programming (APP), and chance-constrained programming (CCP) are applied to propose the new fuzzy network data envelopment analysis (FNDEA) approach. The main advantages of the proposed FNDEA approach can be summarized as follows: linearity of the proposed FNDEA models, unique efficiency decomposing under ambiguity, capability to extending for other network structures. Moreover, FNDEA approach can be applied for ranking of two-stage DMUs under fuzzy environment in three stages: 1) solving the proposed FNDEA model for all optimistic-pessimistic viewpoints and confidence levels, 2) then plotting the results and drawing the surface of all efficiency scores, 3) and finally calculate the volume of the three-dimensional shape in below the efficiency surface. This volume can be as ranking criterion. Remarkably, the presented fuzzy network DEA approach is implemented for performance appraisal and ranking of investment firms (IFs) with two-stage processes including operational and portfolio management process. Illustrative results of the real-life case study show that the proposed approach is effective and practically very useful.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307284",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data envelopment analysis",
      "Data mining",
      "Fuzzy logic",
      "Mathematical optimization",
      "Mathematics",
      "Operations research",
      "Ranking (information retrieval)"
    ],
    "authors": [
      {
        "surname": "Peykani",
        "given_name": "Pejman"
      },
      {
        "surname": "Mohammadi",
        "given_name": "Emran"
      },
      {
        "surname": "Emrouznejad",
        "given_name": "Ali"
      }
    ]
  },
  {
    "title": "Third-party reverse logistics provider selection: A computational semantic analysis-based multi-perspective multi-attribute decision-making approach",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114051",
    "abstract": "In the era of circular economies, governments and consumers are increasingly aware of environmental protection, which encourages enterprises to devote more attention to reverse logistics (RL). However, the limited resources and technical limitations of most manufacturing companies have motivated them to outsource their RL activities to professional third-party RL providers (3PRLPs). Optimal 3PRLP selection is instrumentally valuable in RL outsourcing practices because it has the potential to increase enterprises’ economic profitability and to improve their long-term development. Generally, 3PRLP selection is treated as a multiple-attribute decision-making (MADM) problem. To this end, this paper aims to build a multi-perspective MADM (MPMADM) framework to offer systematic decision support for enterprises to select the optimal 3PRLPs. Attribute assessments in the proposed framework take the form of generalized comparative linguistic expressions (GCLEs), which can be transformed into hesitant fuzzy linguistic term set (HFLTS) possibility distributions with semantic analysis in order to enhance information quality and reliability. Expert weights are then assigned in the use of an optimization model based on the correlation consensus measurement. Afterwards, the two-stage aggregation paradigm for computing with HFLTS possibility distributions is used to gather assessments at expert and attribute levels to compile overall assessments of each alternative 3PRLP. Compared with existing studies, our proposal considers environmental and social sustainability for attribute system establishment and introduces GCLEs for 3PRLP selection, which offer greater flexibility for experts to articulate their evaluations. In addition, the two-stage aggregation paradigm eliminates distortion and loss of information and provides decision makers with the capability to control the outcome’s precision. Moreover, the proposed expert weight determination approach is conducive to generating reliable weight vectors. Several illustrative examples, sensitivity analysis, and comparative analysis further demonstrate the flexibility and practicability of our proposal.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308174",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Perspective (graphical)",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Zhen-Song"
      },
      {
        "surname": "Zhang",
        "given_name": "Xuan"
      },
      {
        "surname": "Govindan",
        "given_name": "Kannan"
      },
      {
        "surname": "Wang",
        "given_name": "Xian-Jia"
      },
      {
        "surname": "Chin",
        "given_name": "Kwai-Sang"
      }
    ]
  },
  {
    "title": "SemSeq4FD: Integrating global semantic relationship and local sequential order to enhance text representation for fake news detection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114090",
    "abstract": "The wide spread of fake news has caused huge losses to both governments and the public. Many existing works on fake news detection utilized spreading information like propagators profiles and the propagation structure. However, such methods face the difficulty of data collection and cannot detect fake news at the early stage. An alternative approach is to detect fake news solely based on its content. Early content-based methods rely on manually designed linguistic features. Such shallow features are domain-dependent, and cannot easily be generalized to cross-domain data. Recently, many natural language processing tasks resort to deep learning methods to learn word, sentence, and document representations. In this paper, we propose a novel graph-based neural network model named SemSeq4FD for early fake news detection based on enhanced text representations. In SemSeq4FD, we model the global pair-wise semantic relations between sentences as a complete graph, and learn the global sentence representations via a graph convolutional network with self-attention mechanism. Considering the importance of local context in conveying the sentence meaning, we employ a 1D convolutional network to learn the local sentence representations. The two representations are combined to form the enhanced sentence representations. Then a LSTM-based network is used to model the sequence of enhanced sentence representations, yielding the final document representation for fake news detection. Experiments conducted on four real-world datasets in English and Chinese, including cross-source and cross-domain datasets, demonstrate that our model can outperform the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308460",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Convolutional neural network",
      "Domain (mathematical analysis)",
      "Graph",
      "Law",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Paleontology",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Sentence",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yuhang"
      },
      {
        "surname": "Wang",
        "given_name": "Li"
      },
      {
        "surname": "Yang",
        "given_name": "Yanjie"
      },
      {
        "surname": "Lian",
        "given_name": "Tao"
      }
    ]
  },
  {
    "title": "The adaptive algorithm of a four way intersection regulated by traffic lights with four phases within a cycle",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114073",
    "abstract": "In many urban areas, road traffic is mostly regulated by traffic lights, which – if not configured effectively – can lead to unnecessary long waiting periods when crossing the intersection and increasing the overall congestion delay. Inefficient configuration of traffic lights is still present, particularly in traffic zones where the functioning of most traffic lights is based on a fixed mode of operation. In order to improve the functioning of traffic lights and reduce the overall congestion delay, the aim of this paper is to suggest a software solution for traffic light system operation that enables an adaptive mode of operation on Four Way Crossing Regulated by Traffic Light-Four phases. In such a mode of operation, the cycle duration, green light, and the duration of the phases can be adjusted to the traffic conditions at the intersection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308332",
    "keywords": [
      "Algorithm",
      "Art",
      "Computer science",
      "Duration (music)",
      "Engineering",
      "Intersection (aeronautics)",
      "Literature",
      "Mode (computer interface)",
      "Operating system",
      "Real-time computing",
      "Simulation",
      "Traffic congestion",
      "Traffic signal",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Radivojević",
        "given_name": "Milan"
      },
      {
        "surname": "Tanasković",
        "given_name": "Marko"
      },
      {
        "surname": "Stević",
        "given_name": "Zoran"
      }
    ]
  },
  {
    "title": "The adaptive algorithm of a four way intersection regulated by traffic lights with four phases within a cycle",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114073",
    "abstract": "In many urban areas, road traffic is mostly regulated by traffic lights, which – if not configured effectively – can lead to unnecessary long waiting periods when crossing the intersection and increasing the overall congestion delay. Inefficient configuration of traffic lights is still present, particularly in traffic zones where the functioning of most traffic lights is based on a fixed mode of operation. In order to improve the functioning of traffic lights and reduce the overall congestion delay, the aim of this paper is to suggest a software solution for traffic light system operation that enables an adaptive mode of operation on Four Way Crossing Regulated by Traffic Light-Four phases. In such a mode of operation, the cycle duration, green light, and the duration of the phases can be adjusted to the traffic conditions at the intersection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308332",
    "keywords": [
      "Algorithm",
      "Art",
      "Computer science",
      "Duration (music)",
      "Engineering",
      "Intersection (aeronautics)",
      "Literature",
      "Mode (computer interface)",
      "Operating system",
      "Real-time computing",
      "Simulation",
      "Traffic congestion",
      "Traffic signal",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Radivojević",
        "given_name": "Milan"
      },
      {
        "surname": "Tanasković",
        "given_name": "Marko"
      },
      {
        "surname": "Stević",
        "given_name": "Zoran"
      }
    ]
  },
  {
    "title": "Machine learning and data mining in manufacturing",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114060",
    "abstract": "Manufacturing organizations need to use different kinds of techniques and tools in order to fulfill their foundation goals. In this aspect, using machine learning (ML) and data mining (DM) techniques and tools could be very helpful for dealing with challenges in manufacturing. Therefore, in this paper, a comprehensive literature review is presented to provide an overview of how machine learning techniques can be applied to realize manufacturing mechanisms with intelligent actions. Furthermore, it points to several significant research questions that are unanswered in the recent literature having the same target. Our survey aims to provide researchers with a solid understanding of the main approaches and algorithms used to improve manufacturing processes over the past two decades. It presents the previous ML studies and recent advances in manufacturing by grouping them under four main subjects: scheduling, monitoring, quality, and failure. It comprehensively discusses existing solutions in manufacturing according to various aspects, including tasks (i.e., clustering, classification, regression), algorithms (i.e., support vector machine, neural network), learning types (i.e., ensemble learning, deep learning), and performance metrics (i.e., accuracy, mean absolute error). Furthermore, the main steps of knowledge discovery in databases (KDD) process to be followed in manufacturing applications are explained in detail. In addition, some statistics about the current state are also given from different perspectives. Besides, it explains the advantages of using machine learning techniques in manufacturing, expresses the ways to overcome certain challenges, and offers some possible further research directions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030823X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Cluster analysis",
      "Computer science",
      "Engineering",
      "Machine learning",
      "Operating system",
      "Operations management",
      "Process (computing)",
      "Scheduling (production processes)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Dogan",
        "given_name": "Alican"
      },
      {
        "surname": "Birant",
        "given_name": "Derya"
      }
    ]
  },
  {
    "title": "Learning user interest with improved triplet deep ranking and web-image priors for topic-related video summarization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114036",
    "abstract": "Video summarization facilitates rapid browsing and efficient video indexing in many video browsing website applications, such as sport video highlights, dynamic video cover. In these applications, it is most important to generate user video summaries that capture interesting video content that users prefer. While many existing methods generate video summaries based on low-level features, this paper first proposes to mine large-scale Flickr images and find “interest” and “non-interest” images from Flickr for the same query to learn what is of interest to users. Unlike existing pairwise ranking-based methods for video summarization, we then propose an improved triplet deep ranking model that is easier to converge to learn the relationship between “interest” and “non-interest” Flickr images, and exploit what visual content of the original video is indeed preferred by users. In the training process, triplets (interest image p + , interest image p ′ + , non-interest image p - ) are selected as input to train a model with three parallel deep convolutional networks. In the video summarization process, an efficient entropy-based video segmentation method is proposed for dividing the original video into segments and the visual interest scores of the segments are estimated using the trained ranking network for summarization (SumNet). Then, an optimal subset of the segments is selected to create a summary capturing interesting visual content. We evaluate and compare our method with several state-of-the-art methods, experimental results show that our method achieves an improvement over the best baseline method by 9.6 % in terms of mean Average Precision (mAP) accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030806X",
    "keywords": [
      "Artificial intelligence",
      "Automatic summarization",
      "Computer science",
      "Information retrieval",
      "Pairwise comparison",
      "Ranking (information retrieval)",
      "Region of interest",
      "Search engine indexing",
      "Segmentation",
      "Video browsing",
      "Video processing",
      "Video tracking",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Fei",
        "given_name": "Mengjuan"
      },
      {
        "surname": "Jiang",
        "given_name": "Wei"
      },
      {
        "surname": "Mao",
        "given_name": "Weijie"
      }
    ]
  },
  {
    "title": "Enhanced fuzzy time series forecasting model based on hesitant differential fuzzy sets and error learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114056",
    "abstract": "Most of existing fuzzy time series forecasting models lead to unsatisfactory forecasting performance due to deficiencies in constructing fuzzy intervals. This paper adopts the Fuzzy Silhouette criterion to determine the optimal number and length of fuzzy intervals effectively and objectively. The universe of discourse with equal and unequal intervals are merged by aggregating hesitant information. Second, multiple relevant factors are considered to reveal the complex attributes of the real-world data, and differential fuzzy sets are included to reconnoiter deep trends. Finally, an optimal error learning mechanism is applied to enhance the forecasting performance. Key strengths of the developed model are verifying the possibility of increasing the performance of conventional fuzzy time series models by integrating them with new components and processes. Three typical stock index datasets are selected to evaluate the performance of the proposed model and experimental results prove that it outperforms compared models in forecasting accuracy and profitable ability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308216",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer security",
      "Data mining",
      "Domain of discourse",
      "Fuzzy logic",
      "Key (lock)",
      "Machine learning",
      "Paleontology",
      "Programming language",
      "Series (stratigraphy)",
      "Silhouette",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Dong",
        "given_name": "Qingli"
      },
      {
        "surname": "Ma",
        "given_name": "Xuejiao"
      }
    ]
  },
  {
    "title": "Cosine similarity based anomaly detection methodology for the CAN bus",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114066",
    "abstract": "In recent years, vehicular technology has rapidly evolved in terms of the driver’s convenience and safety, along with the convergence of vehicle communication and the expansion of external interfaces. However, the connectivity of the vehicle to the external environment poses a considerable driving risk because of the pre-existing vulnerabilities in the vehicle. Furthermore, most of the in-vehicle networks, such as controller area network (CAN), local interconnect network (LIN), and FlexRay network, are not ready to cope with malicious attacks from the outside. For that reason, various studies have addressed the security issues of the automobiles, as protecting the life and safety of the drivers and passengers is one of the core values of the in-vehicle technology. In the present study, in order to address these critical security issues, we propose an anomaly detection method based on cosine similarity for in-vehicle network through the analysis of self-similarity of the CAN bus. Our main goal is to detect three types of injection attacks without having additional information about the attacks. To this end, we evaluated the performance of the proposed method by measuring the accuracy and detection time using a dataset extracted from two real vehicles in driving and stationary conditions. More specifically, we designed a light-weight feature vector that can accomplish real-time detection and then analyzed the performance in terms of accuracy, recall, and detection time by the time window. In the performance evaluation, we achieved high detection accuracy–namely, 98.93% and 99.18% for KIA Soul in the driving condition and in the stationary condition, respectively, 99.43% and 99.49% for the HYUNDAI YF Sonata in the driving condition and in the stationary condition, respectively. Finally, we also showed that the cosine similarity in the CAN bus is a meaningful feature to identify and classify the types of attacks on target CAN IDs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308289",
    "keywords": [
      "Agronomy",
      "Anomaly detection",
      "Artificial intelligence",
      "Biology",
      "CAN bus",
      "Computer network",
      "Computer science",
      "Controller (irrigation)",
      "Cosine similarity",
      "Data mining",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Real-time computing",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Kwak",
        "given_name": "Byung Il"
      },
      {
        "surname": "Han",
        "given_name": "Mee Lan"
      },
      {
        "surname": "Kim",
        "given_name": "Huy Kang"
      }
    ]
  },
  {
    "title": "A two-layer feature selection method using Genetic Algorithm and Elastic Net",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114072",
    "abstract": "Feature selection, as a critical pre-processing step for machine learning, aims at determining representative predictors from a high-dimensional feature space dataset to improve the prediction accuracy. However, the increase in feature space dimensionality, comparing to the number of observations, poses a severe challenge to many existing feature selection methods considering computational efficiency and prediction performance. This paper presents a new two-layer feature selection approach that combines a wrapper and an embedded method in constructing an appropriate subset of predictors. In the first layer of the proposed method, Genetic Algorithm (GA) has been adopted as a wrapper to search for the optimal subset of predictors, which aims to reduce the number of predictors and the prediction error. As one of the meta-heuristic approaches, GA is selected due to its computational efficiency; however, GAs do not guarantee the optimality. To address this issue, a second layer is added to the proposed method to eliminate any remaining redundant/irrelevant predictors to improve the prediction accuracy. Elastic Net (EN) has been selected as the embedded method in the second layer because of its flexibility in adjusting the penalty terms in the regularization process and time efficiency. This two-layer approach has been applied on a Maize genetic dataset from NAM population, which consists of multiple subsets of datasets with different ratios of the number of predictors to the number of observations. The numerical results confirm the superiority of the proposed model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308320",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Curse of dimensionality",
      "Data mining",
      "Demography",
      "Elastic net regularization",
      "Feature (linguistics)",
      "Feature selection",
      "Genetic algorithm",
      "Heuristic",
      "Linguistics",
      "Machine learning",
      "Philosophy",
      "Population",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Amini",
        "given_name": "Fatemeh"
      },
      {
        "surname": "Hu",
        "given_name": "Guiping"
      }
    ]
  },
  {
    "title": "Improved salient object detection using hybrid Convolution Recurrent Neural Network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114064",
    "abstract": "Salient object detection is a critical and active field that aims at the detection of objects in a video, however, it draws increased attention among researchers. With increasing dynamic video data, the performance of saliency object detection method has been degrading with conventional object detection methods. The challenges lie with blurry moving targets, rapid movement of objects and background occlusion or dynamic background change on foreground regions in video frames. Such challenges result in poor saliency detection. In this paper, we design a deep learning model to address the issues, which uses a novel framework by combining the idea of Convolutional Neural Network (CNN) with Recurrent Neural Network (RNN) for video saliency detection. The proposed method aims at developing a spatiotemporal model that exploits temporal, spatial and local constraint cues to achieve global optimization. The task of finding the salient objects in benchmark dynamic video datasets is then carried out by capturing the temporal, spatial and local constraint features with the Convolution Recurrent Neural Network (CRNN). The CRNN is evaluated on benchmark datasets against conventional video salient object detection methods in terms of precision, F-measure, mean absolute error (MAE) and computational load. The experiments reveal that the CRNN model achieves improved performance than other state-of-the-art saliency models in terms of increased speed and reduced computational load.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308265",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Constraint (computer-aided design)",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Deep learning",
      "Engineering",
      "Geodesy",
      "Geography",
      "Mechanical engineering",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Recurrent neural network"
    ],
    "authors": [
      {
        "surname": "Kousik",
        "given_name": "NalliyannaV."
      },
      {
        "surname": "Natarajan",
        "given_name": "Yuvaraj"
      },
      {
        "surname": "Arshath Raja",
        "given_name": "R."
      },
      {
        "surname": "Kallam",
        "given_name": "Suresh"
      },
      {
        "surname": "Patan",
        "given_name": "Rizwan"
      },
      {
        "surname": "Gandomi",
        "given_name": "Amir H."
      }
    ]
  },
  {
    "title": "NOHAR - NOvelty discrete data stream for Human Activity Recognition based on smartphones with inertial sensors",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114093",
    "abstract": "Smartphone sensing capabilities have enabled human activity recognition (HAR) solutions to better understand human behavior through computational techniques. However, such solutions have suffered from scalability problems due to the high consumption of computational resources (e.g. memory and processing) and the difficulty of acting in real time due to not observing data evolution over time. These problems occur because the HAR solutions for smartphones have been solved through offline learning with models limited by a data history. The disadvantage of this approach is that human activities constantly change over time and are strongly influenced by the physical environment and user profile. To overcome such problem, this paper proposes a novel low-cost learning algorithm called NOHAR (NOvelty discrete data stream for Human Activity Recognition), focused on continuous flow of data analysis. NOHAR is an online classification algorithm based on symbolic data generated by a discretization process using algorithms as SAX and SFA. The advantages of these algorithms are their abilities to compress and reduce the dimensionality of data. In addition, this paper proposes a new framework called DISTAR (DIscrete STream learning for Activity Recognition) focused on the data streaming analysis. Its goals include standardizing the development of online algorithms for symbolic data. Experimental results using three databases show that NOHAR is on average 33 times faster than the state of the art and can reduce memory consumption by an average of 99.97%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308484",
    "keywords": [
      "Activity recognition",
      "Artificial intelligence",
      "Computer science",
      "Curse of dimensionality",
      "Data mining",
      "Data stream",
      "Data stream mining",
      "Database",
      "Discretization",
      "Flexibility (engineering)",
      "Inertial measurement unit",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Novelty",
      "Novelty detection",
      "Operating system",
      "Philosophy",
      "Process (computing)",
      "Scalability",
      "Statistics",
      "Telecommunications",
      "Theology"
    ],
    "authors": [
      {
        "surname": "Lima",
        "given_name": "Wesllen Sousa"
      },
      {
        "surname": "Bragança",
        "given_name": "Hendrio L.S."
      },
      {
        "surname": "Souto",
        "given_name": "Eduardo J.P."
      }
    ]
  },
  {
    "title": "Hierarchical and lateral multiple timescales gated recurrent units with pre-trained encoder for long text classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113898",
    "abstract": "Text classification, using deep learning techniques, has become a research challenge in natural language processing. Most of the existing deep learning models for text classification face difficulties when the length of the input text increases. Most models work well on shorter text inputs, however, their performance degrades with the increase in the input length. In this work, we introduce a model for text classification that can alleviate this problem. We present the hierarchical and lateral multiple timescales gated recurrent units (HL-MTGRU), in combination with pre-trained encoders to address the long text classification problem. HL-MTGRU can represent multiple temporal scale dependencies for the discrimination task. By combining the slow and fast units of the HL-MTGRU, our model effectively classifies long multi-sentence texts into the desired classes. We also show that the HL-MTGRU structure helps the model to prevent degradation of performance on longer text inputs. We demonstrate that the proposed network with the help of the latest pre-trained encoders for feature extraction outperforms the conventional models on various long text classification benchmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030693X",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Economics",
      "Encoder",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Machine learning",
      "Management",
      "Natural language processing",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Sentence",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Moirangthem",
        "given_name": "Dennis Singh"
      },
      {
        "surname": "Lee",
        "given_name": "Minho"
      }
    ]
  },
  {
    "title": "Structural representation learning for network alignment with self-supervised anchor links",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113857",
    "abstract": "Network alignment, the problem of identifying similar nodes across networks, is an emerging research topic due to its ubiquitous applications in many data domains such as social-network reconciliation and protein-network analysis. While traditional alignment methods struggle to scale to large graphs, the state-of-the-art representation-based methods often rely on pre-defined anchor links, which are unavailable or expensive to compute in many applications. In this paper, we propose NAWAL, a novel, end-to-end unsupervised embedding-based network alignment framework emphasizing on structural information. The model first embeds network nodes into a low-dimension space where the structural neighborhoodship on original network is captured by the distance on the space. As the space for the input networks are learnt independently, we further leverage a generative adversarial deep neural network to reconcile the spaces without relying on hand-crafted features or domain-specific supervision. The empirical results on three real-world datasets show that NAWAL significantly outperforms state-of-the-art baselines, by over 13% of accuracy against unsupervised methods and on par or better than supervised methods. Our technique also demonstrate the robustness against adversarial conditions, such as structural noises and graph size imbalance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306680",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Embedding",
      "Feature learning",
      "Gene",
      "Graph",
      "Law",
      "Leverage (statistics)",
      "Machine learning",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Robustness (evolution)",
      "Theoretical computer science",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Nguyen",
        "given_name": "Thanh Toan"
      },
      {
        "surname": "Pham",
        "given_name": "Minh Tam"
      },
      {
        "surname": "Nguyen",
        "given_name": "Thanh Tam"
      },
      {
        "surname": "Huynh",
        "given_name": "Thanh Trung"
      },
      {
        "surname": "Tong",
        "given_name": "Van Vinh"
      },
      {
        "surname": "Nguyen",
        "given_name": "Quoc Viet Hung"
      },
      {
        "surname": "Quan",
        "given_name": "Thanh Tho"
      }
    ]
  },
  {
    "title": "Post-hoc explanation of black-box classifiers using confident itemsets",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113941",
    "abstract": "Black-box Artificial Intelligence (AI) methods, e.g. deep neural networks, have been widely utilized to build predictive models that can extract complex relationships in a dataset and make predictions for new unseen data records. However, it is difficult to trust decisions made by such methods since their inner working and decision logic is hidden from the user. Explainable Artificial Intelligence (XAI) refers to systems that try to explain how a black-box AI model produces its outcomes. Post-hoc XAI methods approximate the behavior of a black-box by extracting relationships between feature values and the predictions. Perturbation-based and decision set methods are among commonly used post-hoc XAI systems. The former explanators rely on random perturbations of data records to build local or global linear models that explain individual predictions or the whole model. The latter explanators use those feature values that appear more frequently to construct a set of decision rules that produces the same outcomes as the target black-box. However, these two classes of XAI methods have some limitations. Random perturbations do not take into account the distribution of feature values in different subspaces, leading to misleading approximations. Decision sets only pay attention to frequent feature values and miss many important correlations between features and class labels that appear less frequently but accurately represent decision boundaries of the model. In this paper, we address the above challenges by proposing an explanation method named Confident Itemsets Explanation (CIE). We introduce confident itemsets, a set of feature values that are highly correlated to a specific class label. CIE utilizes confident itemsets to discretize the whole decision space of a model to smaller subspaces. Extracting important correlations between the features and the outcomes of the classifier in different subspaces, CIE produces instance-wise and class-wise explanations that accurately approximate the behavior of the target black-box. Conducting a set of experiments on various black-box classifiers, and different tabular and textual data classification tasks, we show that our CIE method performs better than the previous perturbation-based and rule-based explanators in terms of the descriptive accuracy (an improvement of 9.3%) and interpretability (an improvement of 8.8%) of the explanations. Subjective evaluations demonstrate that the users find the explanations of CIE more understandable and interpretable than those of the other comparison methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307302",
    "keywords": [
      "Artificial intelligence",
      "Black box",
      "Computer science",
      "Data mining",
      "Dentistry",
      "Machine learning",
      "Mathematics",
      "Medicine",
      "Post hoc",
      "Post-hoc analysis",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Moradi",
        "given_name": "Milad"
      },
      {
        "surname": "Samwald",
        "given_name": "Matthias"
      }
    ]
  },
  {
    "title": "Development of a consumer financial goals ontology for use with FinTech applications for improving financial capability",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113843",
    "abstract": "In this research, we communicate the design and evaluation of a consumer financial goals ontology to be utilized as a knowledgebase within recommender systems applications designed to provide decision support in the domain of financial planning. The goal of this research is to provide a domain conceptualization and knowledge classification of a comprehensive set of financial capability enhancing objectives contextually appropriate for a wide range of socio-economically situated consumers. Currently, to the best of our knowledge, within the domain of consumer financial planning no formal conceptual model or knowledge classification of financial goals exists which might be utilized as a knowledgebase for applications such as a FinTech recommender system. Achieving financial goals is a key behavior associated with consumer financial capability, a topic of national economic importance. A holistic representation of domain concepts is critical for advancement of solutions to problems pertinent to a domain. One primary reason for the dearth of applications designed to assist consumers with financial goal setting is the absence of a common domain ontology of financial goals. This study addresses a gap in the literature by contributing to the research knowledgebase an ontology for a domain of consumer financial goals. In doing so, it advances scholarly research through novel domain knowledge classification while providing researchers and practitioners with an ontological knowledgebase for indexing and retrieval within applications designed to improve consumer financial capability through identification and recommendation of specific, context-aware financial goals. The ontology could be used, for example, as a knowledgebase for a Personal Financial Recommender System (PFRS), or other financial technology (FinTech) application, designed to assist users with identification, setting, and tracking of financial goals.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306527",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Business",
      "Computer science",
      "Conceptualization",
      "Context (archaeology)",
      "Domain (mathematical analysis)",
      "Epistemology",
      "Finance",
      "Financial services",
      "Identification (biology)",
      "Knowledge management",
      "Mathematical analysis",
      "Mathematics",
      "Ontology",
      "Paleontology",
      "Philosophy",
      "Recommender system",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Bunnell",
        "given_name": "Lawrence"
      },
      {
        "surname": "Osei-Bryson",
        "given_name": "Kweku-Muata"
      },
      {
        "surname": "Yoon",
        "given_name": "Victoria Y."
      }
    ]
  },
  {
    "title": "Fuzzy C-Means clustering algorithm for data with unequal cluster sizes and contaminated with noise and outliers: Review and development",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113856",
    "abstract": "Clustering algorithms aim at finding dense regions of data based on similarities and dissimilarities of data points. Noise and outliers contribute to the computational procedure of the algorithms as well as the actual data points that leads to inaccurate and misplaced cluster centers. This problem also arises when sizes of the clusters are different that moves centers of small clusters towards large clusters. Mass of the data points is important as well as their location in engineering and physics where non-uniform mass distribution results displacement of the cluster centers towards heavier clusters even if sizes of the clusters are identical and the data are noise-free. Fuzzy C-Means (FCM) algorithm that suffers from these problems is the most popular fuzzy clustering algorithm and has been subject of numerous researches and developments though improvements are still marginal. This work revises the FCM algorithm to make it applicable to data with unequal cluster sizes, noise and outliers, and non-uniform mass distribution. Revised FCM (RFCM) algorithm employs adaptive exponential functions to eliminate impacts of noise and outliers on the cluster centers and modifies constraint of the FCM algorithm to prevent large or heavier clusters from attracting centers of small clusters. Several algorithms are reviewed and their mathematical structures are discussed in the paper including Possibilistic Fuzzy C-Means (PFCM), Possibilistic C-Means (PCM), Robust Fuzzy C-Means (FCM-σ), Noise Clustering (NC), Kernel Fuzzy C-Means (KFCM), Intuitionistic Fuzzy C-Means (IFCM), Robust Kernel Fuzzy C-Mean (KFCM-σ), Robust Intuitionistic Fuzzy C-Means (IFCM-σ), Kernel Intuitionistic Fuzzy C-Means (KIFCM), Robust Kernel Intuitionistic Fuzzy C-Means (KIFCM-σ), Credibilistic Fuzzy C-Means (CFCM), Size-insensitive integrity-based Fuzzy C-Means (siibFCM), Size-insensitive Fuzzy C-Means (csiFCM), Subtractive Clustering (SC), Density Based Spatial Clustering of Applications with Noise (DBSCAN), Gaussian Mixture Models (GMM), Spectral clustering, and Outlier Removal Clustering (ORC). Some of these algorithms are suitable for noisy data and some others are designed for data with unequal clusters. The study shows that the RFCM algorithm works for both cases and outperforms the both categories of the algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306679",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "CURE data clustering algorithm",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Data point",
      "FLAME clustering",
      "Fuzzy clustering",
      "Fuzzy logic",
      "Image (mathematics)",
      "Kernel (algebra)",
      "Mathematics",
      "Noise (video)",
      "Outlier",
      "Pattern recognition (psychology)",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Askari",
        "given_name": "Salar"
      }
    ]
  },
  {
    "title": "Multi-objective symbiotic organism search algorithm for optimal feature selection in brain computer interfaces",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113907",
    "abstract": "Feature selection is crucial to develop a brain computer interface (BCI) system which has high classification accuracy and less computational complexity in especially a large feature space. Feature selection (FS) problem has been solved by many various methods. Among these methods, especially evolutionary computation (EC) techniques have gained a lot of attention in recent years. However, there are very few studies in the literature that consider FS problem as a multi-objective problem to find the optimal trade-off between classification accuracy and the number of selected features. Therefore, in this paper, a non-dominated sorting multi-objective symbiotic organism search (NSMOSOS) algorithm is proposed to generate the optimal feature subset in BCI. The efficiency and robustness of the proposed algorithm as a feature selection method is investigated in two datasets based on motor imagery. The highest classification accuracies of NSMOSOS for dataset 1 and dataset 2 are obtained 97.86% with 11 features and 96.57% with average 19 features, respectively. The obtained results demonstrate that the proposed method achieves satisfying results with regard to both the classification accuracy improvement and feature reduction rates for both datasets. The superiority of the proposed method is verified compared with the existing methods for both datasets. Besides, three different versions of symbiotic search organism (SOS) algorithm are improved, and pros and cons of these algorithms are evaluated compared with each other. In conclusion, the paper indicates that the proposed NSMOSOS algorithm is an efficient and practicable technique for FS problem and could be helpful in developing the BCI applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307028",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Brain–computer interface",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Electroencephalography",
      "Feature (linguistics)",
      "Feature selection",
      "Gene",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Psychiatry",
      "Psychology",
      "Robustness (evolution)",
      "Sorting"
    ],
    "authors": [
      {
        "surname": "Baysal",
        "given_name": "Yesim A."
      },
      {
        "surname": "Ketenci",
        "given_name": "Seniha"
      },
      {
        "surname": "Altas",
        "given_name": "Ismail H."
      },
      {
        "surname": "Kayikcioglu",
        "given_name": "Temel"
      }
    ]
  },
  {
    "title": "A survey of the state-of-the-art of optimisation methodologies in school timetabling problems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113943",
    "abstract": "Educational timetabling is an ongoing challenging administrative task that is required in most academic institutions. This is mainly due to a large number of constraints and requirements that have to be satisfied. Educational timetabling problems have been classified as NP-hard problems and can be divided into three types: exam timetabling, course timetabling and high school timetabling. The domain of high school timetabling is not well developed when compared to other fields of educational timetabling such as university exam timetabling and course timetabling. As the evolution of the educational systems are continuous, new challenges often arise, requiring new models and solution methodologies. Over the years, a number of methodologies have been developed to address high school timetabling problems. However, there are no comparative studies or rigorous analysis of these methodologies. This survey paper aims to provide a scientific review of high school timetabling. The paper presents a categorisation of the methodologies conducted in recent years based on chronology, category and application (dataset). We first present comparative studies on the success of proposed methodologies. The components and mechanisms of different methodologies are analysed and compared. We also discuss their performance, advantages, disadvantages and potential for improvement. Methodology wise, a shift of popularity from meta-heuristic to mathematical optimisation is observed in recent years. Another observation is that more researchers are opting for XHSTT formatted datasets as a testbed for their algorithms. Finally, we outline the industrial perspective, trends and future direction in high school timetabling optimisation problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307314",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data science",
      "Domain (mathematical analysis)",
      "Economics",
      "Engineering",
      "Heuristic",
      "Machine learning",
      "Management science",
      "Mathematical analysis",
      "Mathematics",
      "Operations research",
      "Perspective (graphical)",
      "Popularity",
      "Psychology",
      "Social psychology",
      "Systems engineering",
      "Task (project management)",
      "Testbed",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Tan",
        "given_name": "Joo Siang"
      },
      {
        "surname": "Goh",
        "given_name": "Say Leng"
      },
      {
        "surname": "Kendall",
        "given_name": "Graham"
      },
      {
        "surname": "Sabar",
        "given_name": "Nasser R."
      }
    ]
  },
  {
    "title": "Weakly supervised power line detection algorithm using a recursive noisy label update with refined broken line segments",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113895",
    "abstract": "Detection of power lines in aerial images is an important problem to prevent accidents of unmanned aerial vehicles operating at low altitudes in the electrical industry. Recently, pixel-level power line detection using deep learning has been studied but production of the pixel-level annotations for massive dataset is difficult. In this study, we propose a power line detection algorithm using weakly supervised learning method to reduce the labeling cost for dataset generation. The algorithm is divided into two stages. First, an approximately localized mask was generated based on a convolutional neural network which was trained with only patch-level labels. Second, recursive training of segmentation network with refined broken line segments was executed. A refinement algorithm, line segment connecting (LSC) is a power-line-specialized refinement module that connects broken lines by approximating the segments as partially straight. In proposed algorithm, predicted image at each recursive step was updated as a label of the next training and the label was developed by itself with LSC. The comprehensive experimental results of our algorithm showed state-of-art F 1 -score of 94.3% in weakly supervised learning approaches on public dataset. This result suggests that the proposed algorithm is useful for low labeling cost with high performance in line detection application.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306953",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Geometry",
      "Line (geometry)",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel",
      "Power (physics)",
      "Quantum mechanics",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Choi",
        "given_name": "Hyeyeon"
      },
      {
        "surname": "Koo",
        "given_name": "Gyogwon"
      },
      {
        "surname": "Kim",
        "given_name": "Bum Jun"
      },
      {
        "surname": "Kim",
        "given_name": "Sang Woo"
      }
    ]
  },
  {
    "title": "Analysis and experimental evaluation of the Needleman-Wunsch algorithm for trajectory comparison",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114068",
    "abstract": "We evaluate whether the Needleman-Wunsch algorithm is suitable for user trajectory comparison. The problem that we aim to solve is pair-wise user trajectory comparison. Similar user trajectories are then clustered with respect to their similarity, where clusters emerge in a non-supervised way. We assume that user position, provided by GPS (Global positioning system), is normally distributed around user actual position. This assumption allows us to derive a model for setting score for match, penalty for gap and penalty for mismatch, which are an input to the Needleman-Wunsch algorithm. Our model implies that, in scenarios where actual user position is unknown and must be thus estimated from measured positions, the Needleman-Wunsch algorithm may be prevented from applying mismatches. In an experimental evaluation, we apply two data sets that contain recorded user positions and we show that our approach based on the Needleman-Wunsch algorithm is capable of correct classification of user trajectories into groups. Unlike in existing literature, we show that in GPS based user trajectory comparison, it is indeed not necessary to consider mismatches when applying the Needleman-Wunsch algorithms. This leads to a simplified string editing problem known as Longest Common Subsequence (LCS). We compare our approach with Edit Distance on Real sequence (EDR) in order to provide an insight into the performance of our approach. Applying the Needleman-Wunsch algorithm has helped to solve several problems that emerge in GPS based user trajectory comparison such as interrupted GPS service due to satellite occlusion and various signal propagation phenomena such as signal reflection, fading etc. In order to improve the efficiency of the Needleman-Wunsch algorithm, we apply Move ability to identify when such detrimental conditions could occur. We also apply linear approximation in order to enhance user GPS trajectories with missing points, what further improves the efficiency of user trajectory comparison.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308307",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Astronomy",
      "Computer science",
      "Data mining",
      "Economics",
      "Edit distance",
      "Finance",
      "Global Positioning System",
      "Longest common subsequence problem",
      "Physics",
      "Position (finance)",
      "Telecommunications",
      "Trajectory"
    ],
    "authors": [
      {
        "surname": "Čavojský",
        "given_name": "Maroš"
      },
      {
        "surname": "Drozda",
        "given_name": "Martin"
      },
      {
        "surname": "Balogh",
        "given_name": "Zoltán"
      }
    ]
  },
  {
    "title": "Discovery and diagnosis of wrong SPARQL queries with ontology and constraint reasoning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113772",
    "abstract": "The discovery and diagnosis of wrong queries in database query languages have gained more attention in recent years. While for imperative languages well-known and mature debugging tools exist, the case of database query languages has traditionally attracted less attention. SPARQL is a database query language proposed for the retrieval of information in Semantic Web resources. RDF and OWL are standardized formats for representing Semantic Web information, and SPARQL acts on RDF/OWL resources allowing to retrieve answers of user’s queries. In spite of the SPARQL apparent simplicity, the number of mistakes a user can make in queries can be high and their detection, localization, and correction can be difficult to carry out. Wrong queries have as consequence most of the times empty answers, but also wrong and missing (expected but not found) answers. In this paper we present two ontology and constraint reasoning based methods for the discovery and diagnosis of wrong queries in SPARQL. The first method is used for detecting wrongly typed and unsatisfiable queries. The second method is used for detecting mismatching between user intention and queries, reporting incomplete, faulty queries as well as counterexamples. We formally define the above concepts and a list of examples to illustrate the methods is shown. A Web online tool has been developed to analyze SPARQL queries according to the proposed methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305960",
    "keywords": [
      "Computer science",
      "Constraint (computer-aided design)",
      "Engineering",
      "Epistemology",
      "Information retrieval",
      "Linked data",
      "Mechanical engineering",
      "Named graph",
      "Ontology",
      "Ontology language",
      "Philosophy",
      "Query language",
      "RDF",
      "SPARQL",
      "Semantic Web",
      "Web Ontology Language"
    ],
    "authors": [
      {
        "surname": "Almendros-Jiménez",
        "given_name": "Jesús M."
      },
      {
        "surname": "Becerra-Terón",
        "given_name": "Antonio"
      }
    ]
  },
  {
    "title": "Multi-objective truss structural optimization considering natural frequencies of vibration and global stability",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113777",
    "abstract": "Conflicting objectives such as minimizing weight and minimizing the maximum nodal displacement, with constraints on normal stresses in the bars, is a common multi-objective structural optimization problem widely found in the literature. This paper proposes multi-objective structural optimization problems with the combination of new conflicting objectives functions and constraints, such as the natural frequencies of vibration and the load factors concerning the global stability of the structure. The solution for these problems may be of great interest in the field of structural engineering, not yet discussed in the literature. The problems analyzed in this paper deal with both discrete and continuous sizing, shape, and layout design variables. The search algorithm adopted here is a modified version of the Differential Evolution called the Third Evolution Step Differential Evolution (GDE3). Several experiments are analyzed with their Pareto-fronts showing the non-dominated solutions. The solutions are defined after obtaining the Pareto curve, which is one of the most important steps and a task that may not be trivial for the Decision Maker. This paper involves a strategy that establishes criteria defining weights (importance) for each objective function and, through these values, enables comparison scenarios. The numerical experiments include plane and spatial benchmark trusses.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306011",
    "keywords": [
      "Benchmark (surveying)",
      "Computer science",
      "Differential evolution",
      "Engineering",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Multi-objective optimization",
      "Pareto principle",
      "Physics",
      "Quantum mechanics",
      "Stability (learning theory)",
      "Structural engineering",
      "Truss",
      "Vibration"
    ],
    "authors": [
      {
        "surname": "Lemonge",
        "given_name": "Afonso C.C."
      },
      {
        "surname": "Carvalho",
        "given_name": "José P.G."
      },
      {
        "surname": "Hallak",
        "given_name": "Patrícia H."
      },
      {
        "surname": "Vargas",
        "given_name": "Dênis.E.C."
      }
    ]
  },
  {
    "title": "Coronavirus disease (COVID-19) detection in Chest X-Ray images using majority voting based classifier ensemble",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113909",
    "abstract": "Novel coronavirus disease (nCOVID-19) is the most challenging problem for the world. The disease is caused by severe acute respiratory syndrome coronavirus-2 (SARS-COV-2), leading to high morbidity and mortality worldwide. The study reveals that infected patients exhibit distinct radiographic visual characteristics along with fever, dry cough, fatigue, dyspnea, etc. Chest X-Ray (CXR) is one of the important, non-invasive clinical adjuncts that play an essential role in the detection of such visual responses associated with SARS-COV-2 infection. However, the limited availability of expert radiologists to interpret the CXR images and subtle appearance of disease radiographic responses remains the biggest bottlenecks in manual diagnosis. In this study, we present an automatic COVID screening (ACoS) system that uses radiomic texture descriptors extracted from CXR images to identify the normal, suspected, and nCOVID-19 infected patients. The proposed system uses two-phase classification approach (normal vs. abnormal and nCOVID-19 vs. pneumonia) using majority vote based classifier ensemble of five benchmark supervised classification algorithms. The training-testing and validation of the ACoS system are performed using 2088 (696 normal, 696 pneumonia and 696 nCOVID-19) and 258 (86 images of each category) CXR images, respectively. The obtained validation results for phase-I (accuracy (ACC) = 98.062%, area under curve (AUC) = 0.956) and phase-II (ACC = 91.329% and AUC = 0.831) show the promising performance of the proposed system. Further, the Friedman post-hoc multiple comparisons and z-test statistics reveals that the results of ACoS system are statistically significant. Finally, the obtained performance is compared with the existing state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307041",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Coronavirus",
      "Coronavirus disease 2019 (COVID-19)",
      "Disease",
      "Infectious disease (medical specialty)",
      "Internal medicine",
      "Machine learning",
      "Medicine",
      "Pathology",
      "Pneumonia",
      "Radiography",
      "Radiology"
    ],
    "authors": [
      {
        "surname": "Chandra",
        "given_name": "Tej Bahadur"
      },
      {
        "surname": "Verma",
        "given_name": "Kesari"
      },
      {
        "surname": "Singh",
        "given_name": "Bikesh Kumar"
      },
      {
        "surname": "Jain",
        "given_name": "Deepak"
      },
      {
        "surname": "Netam",
        "given_name": "Satyabhuwan Singh"
      }
    ]
  },
  {
    "title": "Multi-objective evolutionary clustering with complex networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113916",
    "abstract": "Evolutionary clustering (EC) refers to the applications of evolutionary optimization algorithms such as genetic algorithm to data clustering. Although multi-objective evolutionary clustering algorithms were proposed to simultaneously consider different cluster properties such as compactness and separation, these techniques usually suffer from a reasonable initial population and a pre-defined number of clusters. Besides, the effectiveness of evolutionary operators is decreased in dealing with the clustering problem. On the other side, complex networks play an essential role in different fields of machine learning. In a complex network, points are considered as nodes, and the dataset is shown as a connected weighted graph. Also, complex networks tend to present a modular structure. This paper applies two concepts of complex networks including node centrality and community modularity to introduce a novel multi-objective evolutionary clustering. The proposed centrality modularity-based multi-objective evolutionary clustering (CMMOEC) takes the advantage of nodes similarity to find the best initial population of clustering solutions and provide new structural-based modularity to determine the optimal number of clusters automatically. Moreover, the proposed modularity is used to design a new recombination and mutation operator so that it generates offspring solutions that satisfy more diversity. Experiments carried out on several artificial and real-world datasets with different structures. The performance of the proposed algorithm is evaluated by the Adjusted Rand Index (ARI). Simulation results indicate that the proposed algorithm satisfies better performance in comparison to traditional methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307090",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Centrality",
      "Cluster analysis",
      "Combinatorics",
      "Complex network",
      "Computer science",
      "Data mining",
      "Demography",
      "Engineering",
      "Evolutionary algorithm",
      "Genetics",
      "Machine learning",
      "Mathematics",
      "Modularity (biology)",
      "Node (physics)",
      "Population",
      "Rand index",
      "Sociology",
      "Structural engineering",
      "Theoretical computer science",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Orouskhani",
        "given_name": "Maysam"
      },
      {
        "surname": "Shi",
        "given_name": "Daming"
      },
      {
        "surname": "Orouskhani",
        "given_name": "Yasin"
      }
    ]
  },
  {
    "title": "DENS-ECG: A deep learning approach for ECG signal delineation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113911",
    "abstract": "Objectives With the technological advancements in the field of tele-health monitoring, it is now possible to gather huge amount of electro-physiological signals such as the electrocardiogram (ECG). It is therefore necessary to develop models/algorithms that are capable of analysing these massive amount of data in real-time. This paper proposes a deep learning model for real-time segmentation of heartbeats. Methods The proposed DENS-ECG algorithm, combines convolutional neural network (CNN) and long short-term memory (LSTM) model to detect onset, peak, and offset of different heartbeat waveforms such as the P-waves, QRS complexes, T-waves, and No waves (NW). Using ECG as the inputs, the model learns to extract high level features through the training process, which, unlike other classical machine learning based methods, eliminates the feature engineering step. Results The proposed DENS-ECG model was trained and validated on a dataset with 105 ECG records of length 15 min each and achieved an average sensitivity and precision of 97.95% and 95.68%, respectively, using a stratified 5-fold cross validation. Additionally, the model was evaluated on an unseen dataset to examine its robustness in QRS detection, which resulted in a sensitivity of 99.61% and precision of 99.52%. Conclusion The empirical results show the flexibility and accuracy of the combined CNN-LSTM model for ECG signal delineation. Significance This paper proposes an efficient and easy to use approach using deep learning for heartbeat segmentation, which could potentially be used in real-time tele-health monitoring systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307065",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Cardiology",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Convolutional neural network",
      "Deep learning",
      "Electronic engineering",
      "Engineering",
      "Feature engineering",
      "Gene",
      "Heartbeat",
      "Machine learning",
      "Medicine",
      "Offset (computer science)",
      "Pattern recognition (psychology)",
      "Programming language",
      "QRS complex",
      "Radar",
      "Robustness (evolution)",
      "Segmentation",
      "Sensitivity (control systems)",
      "Telecommunications",
      "Waveform"
    ],
    "authors": [
      {
        "surname": "Peimankar",
        "given_name": "Abdolrahman"
      },
      {
        "surname": "Puthusserypady",
        "given_name": "Sadasivan"
      }
    ]
  },
  {
    "title": "A novel multi-stage ensemble model with enhanced outlier adaptation for credit scoring",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113872",
    "abstract": "Credit and credit-based transactions underlie the financial system. After decades of development, artificial intelligence and machine learning have brought new momentum to the credit scoring model. In this study, a novel multi-stage ensemble model with enhanced outlier adaptation is proposed to achieve good predictive power for credit scoring. To reduce the adverse effects of outliers existing in the noise-filled credit datasets, a local outlier factor algorithm is enhanced with the bagging strategy to effectively identify outliers and subsequently boost them back into the training set to construct an outlier-adapted training set that enhances the outlier adaptability of base classifiers. To improve the feature interpretability, a new dimension-reduced feature transformation method is proposed to hierarchically evolve features and extract salient features. To further strengthen the predictive power of the proposed model, a stacking-based ensemble learning method with self-adaptive parameter optimization is proposed to optimize the parameters of selected base classifiers automatically and then to construct a stacking-based multi-stage ensemble model. Ten datasets are tested with six evaluation indicators to evaluate the performance of the proposed model. The experimental results including statistical test results indicate the superior performance of the proposed model and prove its significance and effectiveness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306795",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Computer science",
      "Construct (python library)",
      "Data mining",
      "Ensemble forecasting",
      "Ensemble learning",
      "Interpretability",
      "Machine learning",
      "Outlier",
      "Pattern recognition (psychology)",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Wenyu"
      },
      {
        "surname": "Yang",
        "given_name": "Dongqi"
      },
      {
        "surname": "Zhang",
        "given_name": "Shuai"
      },
      {
        "surname": "Ablanedo-Rosas",
        "given_name": "Jose H."
      },
      {
        "surname": "Wu",
        "given_name": "Xin"
      },
      {
        "surname": "Lou",
        "given_name": "Yu"
      }
    ]
  },
  {
    "title": "A cross-disciplinary comparison of multimodal data fusion approaches and applications: Accelerating learning through trans-disciplinary information sharing",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113885",
    "abstract": "Multimodal data fusion (MMDF) is the process of combining disparate data streams (of different dimensionality, resolution, type, etc.) to generate information in a form that is more understandable or usable. Despite the explosion of data availability in recent decades, as yet there is no well-developed theoretical basis for multimodal data fusion, i.e., no way to determine a priori which approach is best suited to combine an arbitrary set of available data to achieve a stated goal for a given application. This has resulted in exploration of a wide variety of approaches across numerous domains but as yet very little integration of conclusions at a meta (cross-disciplinary) level. In response, this manuscript poses the following questions: (1) How convergent (or divergent) are approaches within single disciplines? (2) How similar are the challenges posed across different disciplines, i.e., might there be opportunity for successes in MMDF achieved in one field to inform progress in other areas as well? and (3) Where are the outstanding gaps in MMDF research, and what does this imply as targets for high impact research in the coming years? To begin to answer these questions, an apples-to-apples comparison of the literature of nine stakeholder-centric engineering domains (civil engineering, transportation, energy, environmental engineering, food engineering, critical care (healthcare), neuroscience, manufacturing/automation, and robotics) was created by quantifying the numbers and dimensionalities of modalities and sensors in each published project and classifying the algorithms used and purposes for which they are used. Within disciplines, it is shown there is often a tendency for use of similar methodologies, both in choice of level of fusion and data algorithm class. Yet this analysis also reveals that many problem types (defined by data dimensionality, modality number and type, and fusion purpose) are shared across different domains and are approached differently in those domains, e.g., transportation problems have similar characteristics to critical care, food science, robotics, and civil engineering. Of the disciplines studied, most ( > 75 %) share problem characteristics with 3–5 others; to support leveraging these resources, lookup tables indexed by data dimensions, number of modalities, etc. are provided as a starting point for cross-disciplinary MMDF literature searches for new applications. Critical gaps identified are (1) a drop off of the number of published studies with increasing number of distinct modalities and (2) a dearth of publications tackling challenges with high dimensionality inputs, especially time-series 2D and 3D data. These gaps may point to topics where algorithm development will be fruitful to enable future solutions as video and other high-dimensionality sensors decrease in price. Finally, the lack of a shared vocabulary across disciplines makes analyses like the one conducted here challenging, as does the often implicit incorporation of expert knowledge into design; therefore progress toward a better leveraging of the current state of knowledge and toward a theoretical MMDF framework depends critically on improved cross-disciplinary communication and coordination on this topic.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306886",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data science",
      "Discipline",
      "Engineering",
      "Field (mathematics)",
      "Management science",
      "Mathematics",
      "Operating system",
      "Process (computing)",
      "Pure mathematics",
      "Sensor fusion",
      "Social science",
      "Sociology",
      "Variety (cybernetics)"
    ],
    "authors": [
      {
        "surname": "Bokade",
        "given_name": "Rohit"
      },
      {
        "surname": "Navato",
        "given_name": "Alfred"
      },
      {
        "surname": "Ouyang",
        "given_name": "Ruilin"
      },
      {
        "surname": "Jin",
        "given_name": "Xiaoning"
      },
      {
        "surname": "Chou",
        "given_name": "Chun-An"
      },
      {
        "surname": "Ostadabbas",
        "given_name": "Sarah"
      },
      {
        "surname": "Mueller",
        "given_name": "Amy V."
      }
    ]
  },
  {
    "title": "Portfolio optimization with return prediction using deep learning and machine learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113973",
    "abstract": "Integrating return prediction of traditional time series models in portfolio formation can improve the performance of original portfolio optimization model. Since machine learning and deep learning models have shown overwhelming superiority than time series models, this paper combines return prediction in portfolio formation with two machine learning models, i.e., random forest (RF) and support vector regression (SVR), and three deep learning models, i.e., LSTM neural network, deep multilayer perceptron (DMLP) and convolutional neural network. To be specific, this paper first applies these prediction models for stock preselection before portfolio formation. Then, this paper incorporates their predictive results in advancing mean–variance (MV) and omega portfolio optimization models. In order to present the superiority of these models, portfolio models with autoregressive integrated moving average’s return prediction are used as benchmarks. Evaluation is based on historical data of 9 years from 2007 to 2015 of component stocks of China securities 100 index. Experimental results show that MV and omega models with RF return prediction, i.e., RF+MVF and RF+OF, outperform the other models. Further, RF+MVF is superior to RF+OF. Due to the high turnover of these two models, this paper discusses their performance after deducting the transaction fee cased by turnover. Experiments present that RF+MVF still performs the best among MVF models and omega model with SVR prediction (SVR+OF) performs the best among OF models. Moreover, RF+MVF performs better than SVR+OF and high turnover erodes nearly half of their total returns especially for RF+OF and RF+MVF. Therefore, this paper recommends investors to build MVF with RF return prediction for daily trading investment.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307521",
    "keywords": [
      "Active learning (machine learning)",
      "Artificial intelligence",
      "Computer science",
      "Economics",
      "Finance",
      "Machine learning",
      "Online machine learning",
      "Portfolio",
      "Portfolio optimization"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Yilin"
      },
      {
        "surname": "Han",
        "given_name": "Ruizhu"
      },
      {
        "surname": "Wang",
        "given_name": "Weizhong"
      }
    ]
  },
  {
    "title": "Portfolio optimization with return prediction using deep learning and machine learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113973",
    "abstract": "Integrating return prediction of traditional time series models in portfolio formation can improve the performance of original portfolio optimization model. Since machine learning and deep learning models have shown overwhelming superiority than time series models, this paper combines return prediction in portfolio formation with two machine learning models, i.e., random forest (RF) and support vector regression (SVR), and three deep learning models, i.e., LSTM neural network, deep multilayer perceptron (DMLP) and convolutional neural network. To be specific, this paper first applies these prediction models for stock preselection before portfolio formation. Then, this paper incorporates their predictive results in advancing mean–variance (MV) and omega portfolio optimization models. In order to present the superiority of these models, portfolio models with autoregressive integrated moving average’s return prediction are used as benchmarks. Evaluation is based on historical data of 9 years from 2007 to 2015 of component stocks of China securities 100 index. Experimental results show that MV and omega models with RF return prediction, i.e., RF+MVF and RF+OF, outperform the other models. Further, RF+MVF is superior to RF+OF. Due to the high turnover of these two models, this paper discusses their performance after deducting the transaction fee cased by turnover. Experiments present that RF+MVF still performs the best among MVF models and omega model with SVR prediction (SVR+OF) performs the best among OF models. Moreover, RF+MVF performs better than SVR+OF and high turnover erodes nearly half of their total returns especially for RF+OF and RF+MVF. Therefore, this paper recommends investors to build MVF with RF return prediction for daily trading investment.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307521",
    "keywords": [
      "Active learning (machine learning)",
      "Artificial intelligence",
      "Computer science",
      "Economics",
      "Finance",
      "Machine learning",
      "Online machine learning",
      "Portfolio",
      "Portfolio optimization"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Yilin"
      },
      {
        "surname": "Han",
        "given_name": "Ruizhu"
      },
      {
        "surname": "Wang",
        "given_name": "Weizhong"
      }
    ]
  },
  {
    "title": "Novel Artificial Immune Networks-based optimization of shallow machine learning (ML) classifiers",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113834",
    "abstract": "Artificial Immune Networks (AIN) is a population-based evolutionary algorithm that is inspired by theoretical immunology. It applies ideas and metaphors from the biological immune system to solve multi-disciplinary problems. This paper presents a novel application of the AIN for optimizing shallow machine learning (ML) classification algorithms. AIN accomplishes this task by searching the best hyper-parameter set for a specific classification algorithm (also termed model selection), which minimizes training error and enhances the generalization capability of the algorithm. We present a convergence analysis of the proposed algorithm and employ it in conjunction with selected, well-known ML classifiers, namely, an extreme learning machine (ELM), a support vector machine (SVM) and an echo state network (ESN). The performance is evaluated in terms of classification accuracy and learning time, using a range of benchmark datasets, and compared against grid search as well as evolutionary strategy (ES)-based optimization techniques. An empirical study with different datasets demonstrates improved classification accuracy of SVM, from 2% to 5%, for ESN from 3% to 6%, whereas in the case of ELM from 3% to 9%. Comparative simulation results demonstrate the potential of AIN as an alternative optimizer for shallow ML algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306448",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Evolutionary algorithm",
      "Extreme learning machine",
      "Generalization",
      "Geodesy",
      "Geography",
      "Hyperparameter optimization",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Meta learning (computer science)",
      "Support vector machine",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Kanwal",
        "given_name": "Summrina"
      },
      {
        "surname": "Hussain",
        "given_name": "Amir"
      },
      {
        "surname": "Huang",
        "given_name": "Kaizhu"
      }
    ]
  },
  {
    "title": "Analysis of factors that influence the performance of biometric systems based on EEG signals",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113967",
    "abstract": "Searching for new biometric traits is currently a necessity because traditional ones such as fingerprint, voice, or face are highly prone to forgery. For this reason, the study of bioelectric signals has the potential to develop new biometric systems. A motivation for using electroencephalogram signals is that they are unique to each person and are much more difficult to replicate than conventional biometrics. The objective of this study is to analyze the factors that influence the performance of a biometric system based on electroencephalogram signals. This work uses six different classifiers to compare several decomposition levels of the discrete wavelet transform as a preprocessing technique and also explores the importance of the recording time. These classifiers are Gaussian Naïve Bayes Classifier, K-Nearest Neighbors, Random Forest, AdaBoost, Support Vector Machine, and Multilayer Perceptron. This work proves that the decomposition level does not have a high impact on the overall result of the system. On the other hand, the recording time of electroencephalograms has a significant impact on the performance of the classifiers. It is worth mentioning that this study used two different datasets to validate the results. Finally, our experiments show that Support Vector Machine and AdaBoost are the best classifiers for this particular problem since they achieved a sensitivity, specificity, and accuracy of 85 . 94 ± 1 . 8 , 99 . 55 ± 0 . 06 , 99 . 12 ± 0 . 11 and 95 . 54 ± 0 . 53 , 99 . 91 ± 0 . 01 , and 99 . 83 ± 0 . 02 respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030748X",
    "keywords": [
      "AdaBoost",
      "Artificial intelligence",
      "Artificial neural network",
      "Biometrics",
      "Classifier (UML)",
      "Computer science",
      "Electroencephalography",
      "Machine learning",
      "Multilayer perceptron",
      "Naive Bayes classifier",
      "Pattern recognition (psychology)",
      "Perceptron",
      "Preprocessor",
      "Psychiatry",
      "Psychology",
      "Random forest",
      "Speech recognition",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Carrión-Ojeda",
        "given_name": "Dustin"
      },
      {
        "surname": "Fonseca-Delgado",
        "given_name": "Rigoberto"
      },
      {
        "surname": "Pineda",
        "given_name": "Israel"
      }
    ]
  },
  {
    "title": "3D automatic levels propagation approach to breast MRI tumor segmentation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113965",
    "abstract": "Magnetic Resonance Imaging MRI is a relevant tool for breast cancer screening. Moreover, an accurate 3D segmentation of breast tumors from MRI scans plays a key role in the analysis of the disease. In this manuscript, we propose a novel 3D automatic method for segmenting MRI breast tumors, called 3D Automatic Levels Propagation Approach (3D-ALPA). The proposed method performs the segmentation automatically in two steps: in the first step, the entire MRI volume to process is segmented slice by slice. Specifically, using a new automatic approach called 2D Automatic Levels Propagation Approach (2D-ALPA) which is an improved version of a previous semi-automatic approach, named 2D Levels Propagation Approach (2D-LPA). In the second step, the partial segmentations obtained after the application of 2D-ALPA are recombined to rebuild the complete volume(s) of tumor(s). 3D-ALPA has many characteristics, mainly: it is an automatic method which can take into consideration multi-tumor segmentation, and it has the property to be easily applicable according to the Axial, Coronal, as well as Sagittal planes. Therefore, it offers a multi-view representation of the segmented tumor(s). To validate the new 3D-ALPA method, we have firstly performed tests on a 2D private dataset composed of eighteen patients to estimate the accuracy of the new 2D-ALPA in comparison to the previous 2D-LPA. The obtained results have been in favor of the proposed 2D-ALPA, showing hence an improvement in accuracy after integrating the automatization in the 2D-ALPA approach. Then, we have evaluated the complete 3D-ALPA method on a 3D private dataset constituted of MRI exams of twenty-two patients having real breast tumors of different types, and on the public RIDER dataset. Essentially, 3D-ALPA has been evaluated regarding two main features: segmentation accuracy and running time, by considering two kinds of breast tumors: non-enhanced and enhanced tumors. The experimental studies have shown that 3D-ALPA has produced better results for the both kinds of tumors than a recent and concurrent method in the literature that addresses the same problematic.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307478",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Coronal plane",
      "Engineering",
      "Fully automatic",
      "Magnetic resonance imaging",
      "Mechanical engineering",
      "Medicine",
      "Pattern recognition (psychology)",
      "Radiology",
      "Sagittal plane",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Bouchebbah",
        "given_name": "Fatah"
      },
      {
        "surname": "Slimani",
        "given_name": "Hachem"
      }
    ]
  },
  {
    "title": "Sequence-based clustering applied to long-term credit risk assessment",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113940",
    "abstract": "This paper studies the effectiveness of estimating credit rating transition matrices using sequence-based clustering on historical credit rating sequences. The data set used in this study consisted of monthly credit rating sequences from Korean companies from 1986 to 2018. The credit rating sequences were converted to sequence matrices and was clustered using PCA-guided K-means. Representative transition matrices of the resulting clusters were then generated to be used in the classification process. The proposed clustering model is evaluated under the 3 different long-term classification scenarios; 7 class credit rating prediction, credit rating transition direction (upgrade, stay, or downgrade) prediction, and default behaviour prediction. All three classification scenarios produced promising results suggesting that the representative transition matrix of the K clusters better describes future credit rating behaviour than a single transition matrix.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307296",
    "keywords": [
      "Actuarial science",
      "Artificial intelligence",
      "Biology",
      "Business",
      "Cluster analysis",
      "Computer science",
      "Computer security",
      "Credit rating",
      "Credit risk",
      "Data mining",
      "Data set",
      "Downgrade",
      "Genetics",
      "Machine learning",
      "Markov chain",
      "Physics",
      "Quantum mechanics",
      "Sequence (biology)",
      "Stochastic matrix",
      "Term (time)"
    ],
    "authors": [
      {
        "surname": "Le",
        "given_name": "Richard"
      },
      {
        "surname": "Ku",
        "given_name": "Hyejin"
      },
      {
        "surname": "Jun",
        "given_name": "Doobae"
      }
    ]
  },
  {
    "title": "Utilizing historical data for corporate credit rating assessment",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113925",
    "abstract": "Corporate credit rating assessment is one of the crucial problems of credit risk management; it will help the financial institutions and government decide whether to issue debts. Recent studies focusing on the prediction of credit rating by using artificial intelligence (AI) techniques have shown impressive results compared to traditional statistical methods. Although the AI techniques can be used to assess credit risk, the prediction accuracy is still worth improving further, as even a small improvement in credit rating prediction accuracy leads to significant loss reduction in the industry. In this paper, we propose new learning analytic methods to enhance the prediction accuracy of credit rating. First, we devise the metrics based on the credit rating history of the firms, and expand the feature space with new input variables. This approach can be applied to any conventional AI methods for improvement of prediction accuracy. Second, we develop a novel learning algorithm that is designed to take into account historical financial data. We propose the parallel artificial neural networks (PANNs) ensemble model that creates several independent artificial neural networks (ANNs); each ANN deals with financial performance of the firms for each year, and the final output of PANNs is aggregated by ensemble learning. In our experiment, three different real-world datasets are used to validate the performance of our proposed approach. Consequently, the experimental results show that our proposed approach achieved competitive results compared to conventional AI techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307156",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Business",
      "Computer science",
      "Credit rating",
      "Credit risk",
      "Finance",
      "Machine learning"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Mingfu"
      },
      {
        "surname": "Ku",
        "given_name": "Hyejin"
      }
    ]
  },
  {
    "title": "#stayhome to contain Covid-19: Neuro-SIR – Neurodynamical epidemic modeling of infection patterns in social networks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113970",
    "abstract": "An innovative neurodynamical model of epidemics in social networks – the Neuro-SIR – is introduced. Susceptible–Infected–Removed (SIR) epidemic processes are mechanistically modeled as analogous to the activity propagation in neuronal populations. The workings of infection transmission from individual to individual through a network of social contacts, is driven by the dynamics of the threshold mechanism of leaky integrate-and-fire neurons. Through this approach a dynamically evolving landscape of the susceptibility of a population to a disease is formed. In this context, epidemics with varying velocities and scales are triggered by a small fraction of infected individuals according to the configuration of various endogenous and exogenous factors representing the individuals’ vulnerability, the infectiousness of a pathogen, the density of a contact network, and environmental conditions. Adjustments in the length of immunity (if any) after recovery, enable the modeling of the Susceptible–Infected–Recovered–Susceptible (SIRS) process of recurrent epidemics. Neuro-SIR by supporting an impressive level of heterogeneities in the description of a population, contagiousness of a disease, and external factors, allows a more insightful investigation of epidemic spreading in comparison with existing approaches. Through simulation experiments with Neuro-SIR, we demonstrate the effectiveness of the #stayhome strategy for containing Covid-19, and successfully validate the simulation results against the classical epidemiological theory. Neuro-SIR is applicable in designing and assessing prevention and control strategies for spreading diseases, as well as in predicting the evolution pattern of epidemics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307508",
    "keywords": [
      "Biology",
      "Computer science",
      "Computer security",
      "Context (archaeology)",
      "Coronavirus disease 2019 (COVID-19)",
      "Disease",
      "Environmental health",
      "Epidemic disease",
      "Epidemic model",
      "Infectious disease (medical specialty)",
      "Mechanism (biology)",
      "Medicine",
      "Paleontology",
      "Pandemic",
      "Pathology",
      "Physics",
      "Population",
      "Quantum mechanics",
      "Susceptible individual",
      "Telecommunications",
      "Transmission (telecommunications)",
      "Virology",
      "Vulnerability (computing)"
    ],
    "authors": [
      {
        "surname": "Lymperopoulos",
        "given_name": "Ilias N."
      }
    ]
  },
  {
    "title": "A survey of research hotspots and frontier trends of recommendation systems from the perspective of knowledge graph",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113764",
    "abstract": "With the advent of the era of big data, the recommendation system has become an effective solution to the problem of information overload. This paper takes the literature data related to the recommendation system theme from 2009 to 2018 and included in the core collection of Web of Science database as the research object, and utilizes bibliometric methods to analyze the theme of recommendation system. To this end, firstly, classify statistics and feature analysis of valid literature data. Secondly, use VOSviewer software to construct various different scientific knowledge graph to discover valuable knowledge. Thirdly, according to keyword co-concurrence graph conclude five main hotspots of current research about recommendation system and discover five main directions that have potential value in research field of recommendation system. Finally, deeply explore five main key issues and propose corresponding solutions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305881",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Big data",
      "Computer science",
      "Construct (python library)",
      "Data mining",
      "Data science",
      "Field (mathematics)",
      "Frontier",
      "Graph",
      "History",
      "Information retrieval",
      "Mathematics",
      "Perspective (graphical)",
      "Programming language",
      "Pure mathematics",
      "Recommender system",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Shao",
        "given_name": "Bilin"
      },
      {
        "surname": "Li",
        "given_name": "Xiaojun"
      },
      {
        "surname": "Bian",
        "given_name": "Genqing"
      }
    ]
  },
  {
    "title": "Preventing the generation of inconsistent sets of crisp classification rules",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113811",
    "abstract": "In recent years, the interest in interpretable classification models has grown. One of the proposed ways to improve the interpretability of a classification model based on collections of crisp rules is to use sets (unordered collections) instead of lists (ordered collections). One of the problems associated with sets is that multiple rules may cover a single instance but predict different classes for it, thus requiring a conflict resolution strategy. In this work, we propose two algorithms capable of finding feature-space regions inside which any created rule would be consistent with the already existing rules, preventing inconsistencies from arising. Our algorithms, named CFSGS and CFSBE, do not generate classification rules nor classification models but are instead meant to enhance algorithms that do so, such as Learning Classifier Systems. We analyzed both algorithms from a theoretical perspective and conducted experiments with a proof of concept evolutionary algorithm that employs CFSBE. The experiments suggest that using CFSBE as an embedded tool does incur a computational overhead, but such cost is not prohibitive.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306254",
    "keywords": [
      "Artificial intelligence",
      "Classification rule",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Interpretability",
      "Machine learning",
      "Perspective (graphical)"
    ],
    "authors": [
      {
        "surname": "Miranda",
        "given_name": "Thiago Zafalon"
      },
      {
        "surname": "Sardinha",
        "given_name": "Diorge Brognara"
      },
      {
        "surname": "Cerri",
        "given_name": "Ricardo"
      }
    ]
  },
  {
    "title": "Beetle antenna strategy based grey wolf optimization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113882",
    "abstract": "Finding feasible solutions to real-world problems is a crucial task. Metaheuristic algorithms are widely used in many fields due to the variety of solutions they can produce. The grey wolf optimizer (GWO) is a relatively novel population-based metaheuristic algorithm that has been shown to have good optimization performance. However, due to the insufficient diversity of wolves in some cases, this approach can lead to locally optimal situations. Therefore, this paper proposes a grey wolf optimization method based on a beetle antenna strategy (BGWO) that gives the leader wolf a sense of hearing to improve the global search ability and reduce unnecessary searches. In addition, to balance exploration and exploitation, a nonlinear dynamic control parameter update strategy based on the cosine function is proposed. To evaluate the performance of the proposed BGWO, this paper uses 23 standard benchmark functions to test the method in different dimensions. Moreover, four well-known engineering problems are used to evaluate the ability of the proposed algorithm to obtain real-world problem solutions. The experimental results show that BGWO has superior performance and is competitive with many state-of-the-art algorithms in terms of solution accuracy, convergence rate, and stability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306862",
    "keywords": [
      "Algorithm",
      "Benchmark (surveying)",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Metaheuristic",
      "Optimization problem",
      "Stability (learning theory)"
    ],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "Qingsong"
      },
      {
        "surname": "Huang",
        "given_name": "Haisong"
      },
      {
        "surname": "Li",
        "given_name": "Yiting"
      },
      {
        "surname": "Han",
        "given_name": "Zhenggong"
      },
      {
        "surname": "Hu",
        "given_name": "Yao"
      },
      {
        "surname": "Huang",
        "given_name": "Dong"
      }
    ]
  },
  {
    "title": "In-air signature verification system using Leap Motion",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113797",
    "abstract": "Signature verification is a widely explored field due to its high acceptance and its compromise between security and comfort. Recently, different techniques have appeared to improve the capture, processing, and classification of signatures. In this work, authors present a novel and robust in-air signature verification system, which applies the use of Leap Motion controller to characterize in-air strokes, due to its stability and good performance for this task, as it will be demonstrated. Therefore, a database has been built for developing the experiments, which is composed of 100 users, with 10 genuine and 10 forgery samples per user. The implemented system is tested against two tests of impostor samples, zero effort attacks and active impostors. The second type of attacks are developed by different users, who showed very good abilities with the sensor. The classification is done by a Least Square Support Vector Machine. The equal error rate was 0.25% and 1.20%, respectively. The proposed system achieves very good results in comparison with the state-of-the-art one, which suggests that in-air signature processing gives an opportunity to increase systems’ security.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306163",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Geometry",
      "Mathematics",
      "Motion (physics)",
      "Signature (topology)"
    ],
    "authors": [
      {
        "surname": "Guerra-Segura",
        "given_name": "Elyoenai"
      },
      {
        "surname": "Ortega-Pérez",
        "given_name": "Aysse"
      },
      {
        "surname": "Travieso",
        "given_name": "Carlos M."
      }
    ]
  },
  {
    "title": "A benchmark of machine learning approaches for credit score prediction",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113986",
    "abstract": "Credit risk assessment plays a key role for correctly supporting financial institutes in defining their bank policies and commercial strategies. Over the last decade, the emerging of social lending platforms has disrupted traditional services for credit risk assessment. Through these platforms, lenders and borrowers can easily interact among them without any involvement of financial institutes. In particular, they support borrowers in the fundraising process, enabling the participation of any number and size of lenders. However, the lack of lenders’ experience and missing or uncertain information about borrower’s credit history can increase risks in social lending platforms, requiring an accurate credit risk scoring. To overcome such issues, the credit risk assessment problem of financial operations is usually modeled as a binary problem on the basis of debt’s repayment and proper machine learning techniques can be consequently exploited. In this paper, we propose a benchmarking study of some of the most used credit risk scoring models to predict if a loan will be repaid in a P2P platform. We deal with a class imbalance problem and leverage several classifiers among the most used in the literature, which are based on different sampling techniques. A real social lending platform (Lending Club) data-set, composed by 877,956 samples, has been used to perform the experimental analysis considering different evaluation metrics (i.e. AUC, Sensitivity, Specificity), also comparing the obtained outcomes with respect to the state-of-the-art approaches. Finally, the three best approaches have also been evaluated in terms of their explainability by means of different eXplainable Artificial Intelligence (XAI) tools.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307636",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Benchmarking",
      "Business",
      "Computer science",
      "Credit rating",
      "Credit risk",
      "Credit score",
      "Debt",
      "Finance",
      "Geodesy",
      "Geography",
      "Leverage (statistics)",
      "Loan",
      "Machine learning",
      "Marketing"
    ],
    "authors": [
      {
        "surname": "Moscato",
        "given_name": "Vincenzo"
      },
      {
        "surname": "Picariello",
        "given_name": "Antonio"
      },
      {
        "surname": "Sperlí",
        "given_name": "Giancarlo"
      }
    ]
  },
  {
    "title": "An improved ELM-based and data preprocessing integrated approach for phishing detection considering comprehensive features",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113863",
    "abstract": "In this paper, a novel approach based on non-inverse matrix online sequence extreme learning machine (NIOSELM) for phishing detection is presented, which takes into account three types of features to comprehensively characterize a website. For the NIOSELM algorithm, we use Sherman Morriso Woodbury equation to avoid the matrix inversion operation, and introduce the idea of online sequence extreme learning machine (OSELM) to update the training model. In order to reduce the dependence of the detection model on the majority class, we use Adaptive Synthetic Sampling (ADASYN) algorithm to generate the synthetic minority class samples to balance the distribution between the samples of the majority and minority classes. Furthermore, an improved denoising auto-encoder (SDAE) is designed to reduce the dimension of the experimental dataset. The experimental results show the efficiency and feasibility of the proposed detection mechanism. Moreover, the overall detection performance of NIOSELM is better than that of other existing methods, especially in training speed and the detection accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306734",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Data mining",
      "Data pre-processing",
      "Extreme learning machine",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Preprocessor"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Liqun"
      },
      {
        "surname": "Zhang",
        "given_name": "Jiawei"
      },
      {
        "surname": "Wang",
        "given_name": "Xiaozhe"
      },
      {
        "surname": "Li",
        "given_name": "Zhi"
      },
      {
        "surname": "Li",
        "given_name": "Zhoujun"
      },
      {
        "surname": "He",
        "given_name": "Yueying"
      }
    ]
  },
  {
    "title": "Bezier Search Differential Evolution Algorithm for numerical function optimization A comparative study with CRMLSP, MVO, WA, SHADE and LSHADE",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113875",
    "abstract": "Differential Evolution Algorithm (DE) is a commonly used stochastic search method for solving real-valued numerical optimization problems. Unfortunately, DE's problem solving success is very sensitive to the internal parameters of the artificial numerical genetic operators (i.e., mutation and crossover operators) used. Although several mutation and crossover methods have been developed for DE, there is not still an analytical method that can be used to select the most efficient mutation and crossover method while solving a problem with DE. Therefore, selection and parameter tuning processes of artificial numerical genetic operators used by DE are based on a trial-and-error process which is time consuming. The development of modern DE versions has been focused on developing fast, structurally simple and efficient genetic operators that are not sensitive to the initial values of their internal parameters. Problem solving successes of the Universal Differential Algorithms (uDE) are not sensitive to the structure and internal parameters of the related artificial numerical genetic operators used, unlike DE. In this paper a new uDE, Bezier Search Differential Evolution Algorithm, BeSD, has been proposed. BeSD’s mutation and crossover operators are structurally simple, fast, unique and produce highly efficient trial patterns. BeSD utilizes a partially elitist unique mutation operator and a unique crossover operator. In this paper, the experiments were performed by using the 30 benchmark problems of CEC2014 with Dim=30, and one 3D viewshed problem as a real world application. The problem solving success of BeSD was statistically compared with five top-methods of CEC2014, i.e., CRMLSP, MVO, WA, SHADE and LSHADE by using Wilcoxon Signed Rank test. Statistical results exposed that BeSD’s problem solving success is better than those of the comparison methods in general.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306813",
    "keywords": [
      "Algorithm",
      "Biology",
      "Computer science",
      "Differential evolution",
      "Evolutionary biology",
      "Function (biology)",
      "Mathematical optimization",
      "Mathematics"
    ],
    "authors": [
      {
        "surname": "Civicioglu",
        "given_name": "Pinar"
      },
      {
        "surname": "Besdok",
        "given_name": "Erkan"
      }
    ]
  },
  {
    "title": "High-order Markov-switching portfolio selection with capital gain tax",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113915",
    "abstract": "The uncertainties of market state and returns of risky assets both affect the investors’ decisions significantly. It is necessary and prudent to consider the regime-switching mechanism of market states in portfolio selection. Different from the traditional first-order Markov-switching portfolio selection studies, we consider a high-order Markov transition process of market state, which can better depict the market state changes and incorporate more market information into portfolio selection due to the financial market has the long memory property. The capital gain tax is treated as the trading cost of which the tax rate not only depends on the holding periods of risky assets but also on the trading volume. In addition, the capital gain–loss offsetting is studied explicitly where the gain–loss offsetting in the same period and capital loss carry-over effect in different periods are considered simultaneously. A high-order Markov-switching portfolio selection model (HOMSPSM) is proposed. The Monte Carlo simulation is employed to approximate the expected values and variances of the complicated random returns, and the Monte Carlo simulation based particle swarm optimization algorithm (MCPSO) is designed to obtain the optimal investment strategy. Finally, simulated and practical numerical experiments are provided to verify the effectiveness and practicability of HOMSPSM and MCPSO.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307089",
    "keywords": [
      "Biology",
      "Capital gain",
      "Capital market line",
      "Computer science",
      "Economics",
      "Finance",
      "Financial economics",
      "Horse",
      "Market depth",
      "Mathematical optimization",
      "Mathematics",
      "Order (exchange)",
      "Paleontology",
      "Portfolio",
      "Portfolio optimization",
      "Stock market"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Sini"
      },
      {
        "surname": "Ching",
        "given_name": "Wai-Ki"
      }
    ]
  },
  {
    "title": "Data augmentation for skin lesion using self-attention based progressive generative adversarial network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113922",
    "abstract": "While recent years have witnessed the remarkable success of deep learning methods in automated skin lesion detection systems, there still exists a gap between manual assessment of experts and automated evaluation of computers. The reason behind such a gap is the deep learning models demand considerable amounts of data, while the availability of annotated images is often limited. Data Augmentation (DA) is one way to mitigate the lack of labeled data; however, the augmented images intrinsically have a similar distribution to the original ones, leading to limited performance improvement. To satisfy the data lack in the real image distribution, we synthesize skin lesion images – realistic but completely different from the original ones – using Generative Adversarial Networks (GANs). In this paper, we propose the Self-attention Progressive Growing of GANs (SPGGANs) to generate fine-grained 256 × 256 skin lesion images for Convolutional Neural Network-based melanoma detection, which is challenging via conventional GANs; difficulties arise due to unstable GAN training with high resolution and a variety of skin lesions in size, shape, and location. In SPGGAN, details can be generated using aggregated information from all feature locations. Moreover, the discriminator can monitor that highly detailed features in distant portions of the image are consistent with each other. Furthermore, the Two-Timescale Update Rule (TTUR) is applied to SPGGAN (SPGGAN-TTUR) to improve stability while generating 256 × 256 skin lesion images. SPGGAN-TTUR is evaluated on data generation and classification tasks using the HAM10000 dataset. Our results confirm the importance of our proposed GAN-based DA approach for training skin lesion classifiers and indicate that it can lead to statistically significant improvements ( p -value < 0 . 05 ) in the sensitivity (recall) over non-augmented and augmented, with classical DA, counterparts. In general, in the case of all classes, The sensitivity improvements were 5.6% and 2.5% over non-augmented and augmented (with the best DA scheme) counterparts, respectively. Specifically, in the case of melanoma class, the sensitivity improvements were 13.8% and 8.6%. We believe that the proposed approach can be adopted in clinical practice to improve the sensitivity of automated skin lesion detection in dermoscopic images and thus support dermatologists’ efforts to improve melanoma diagnosis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307132",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Detector",
      "Discriminator",
      "Feature (linguistics)",
      "Generative adversarial network",
      "Linguistics",
      "Machine learning",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Skin lesion",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Abdelhalim",
        "given_name": "Ibrahim Saad Aly"
      },
      {
        "surname": "Mohamed",
        "given_name": "Mamdouh Farouk"
      },
      {
        "surname": "Mahdy",
        "given_name": "Yousef Bassyouni"
      }
    ]
  },
  {
    "title": "An empirical analysis of constraint handling on evolutionary multi-objective algorithms for the Environmental/Economic Load Dispatch problem",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113774",
    "abstract": "This paper analyses different multi-objective evolutionary algorithms to deal with the Environmental/Economic Load Dispatch (EELD). EELD is formulated as a multi-objective optimization problem in which two competing objectives (fuel cost and pollutants emission) should be optimized simultaneously while fulfilling constraints. Due to the typical process of an evolutionary algorithm (EA), the use of operators applied to individuals of the population might violate constraint rules of the problem. The way in which EAs deal with such constraint rules is an important point and it is directly related to the quality of the generated solutions. One of the contributions of this paper is the analysis of the impact of a repair procedure in four multi-objective EAs. The analyzed approaches are evaluated in eight known instances (with 3, 6, 10, 20 and 40 generators) of the multi-objective EELD. Furthermore, two new instances (with 80 and 120 generators) are proposed and evaluated in this work. Experiments were applied using Dominance Ranking, hypervolume and unary-epsilon indicators, empirical attainment functions and statistical tests, in order to evaluate the algorithms performances. The results point to the consistency of the NSGA-II with repair procedure compared to the literature algorithms, and it outperforms other approaches in most of the considered instances.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305984",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Constraint (computer-aided design)",
      "Demography",
      "Evolutionary algorithm",
      "Geometry",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Population",
      "Ranking (information retrieval)",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Kuk",
        "given_name": "Josiel Neumann"
      },
      {
        "surname": "Gonçalves",
        "given_name": "Richard Aderbal"
      },
      {
        "surname": "Pavelski",
        "given_name": "Lucas Marcondes"
      },
      {
        "surname": "Guse Scós Venske",
        "given_name": "Sandra Mara"
      },
      {
        "surname": "de Almeida",
        "given_name": "Carolina Paula"
      },
      {
        "surname": "Ramirez Pozo",
        "given_name": "Aurora Trinidad"
      }
    ]
  },
  {
    "title": "A comprehensive comparison of end-to-end approaches for handwritten digit string recognition",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114196",
    "abstract": "Over the last decades, most approaches proposed for handwritten digit string recognition (HDSR) have resorted to digit segmentation, which is dominated by heuristics, thereby imposing substantial constraints on the final performance. Few of them have been based on segmentation-free strategies where each pixel column has a potential cut location. Recently, segmentation-free strategies has added another perspective to the problem, leading to promising results. However, these strategies still show some limitations when dealing with a large number of touching digits. To bridge the resulting gap, in this paper, we hypothesize that a string of digits can be approached as a sequence of objects. We thus evaluate different end-to-end approaches to solve the HDSR problem, particularly in two verticals: those based on object-detection (e.g., Yolo and RetinaNet) and those based on sequence-to-sequence representation (CRNN). The main contribution of this work lies in its provision of a comprehensive comparison with a critical analysis of the above mentioned strategies on five benchmarks commonly used to assess HDSR, including the challenging Touching Pair dataset, NIST SD19, and two real-world datasets (CAR and CVL) proposed for the ICFHR 2014 competition on HDSR. Our results show that the Yolo model compares favorably against segmentation-free models with the advantage of having a shorter pipeline that minimizes the presence of heuristics-based models. It achieved a 97%, 96%, and 84% recognition rate on the NIST-SD19, CAR, and CVL datasets, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420309271",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "End-to-end principle",
      "Genetics",
      "Geodesy",
      "Geography",
      "Heuristics",
      "Law",
      "Machine learning",
      "Mathematical physics",
      "Mathematics",
      "NIST",
      "Operating system",
      "Pattern recognition (psychology)",
      "Pipeline (software)",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Segmentation",
      "Sequence (biology)",
      "Speech recognition",
      "String (physics)"
    ],
    "authors": [
      {
        "surname": "Hochuli",
        "given_name": "Andre G."
      },
      {
        "surname": "Britto Jr",
        "given_name": "Alceu S."
      },
      {
        "surname": "Saji",
        "given_name": "David A."
      },
      {
        "surname": "Saavedra",
        "given_name": "José M."
      },
      {
        "surname": "Sabourin",
        "given_name": "Robert"
      },
      {
        "surname": "Oliveira",
        "given_name": "Luiz S."
      }
    ]
  },
  {
    "title": "Collaborative multiple centers fresh logistics distribution network optimization with resource sharing and temperature control constraints",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113838",
    "abstract": "Collaboration such as resource sharing among logistics participants (LPs) can effectively increase the efficiency and sustainability of logistics operations, especially in the transportation and distribution of fresh and perishable products that require special infrastructure (e.g., refrigerated trucks/vehicles). This study tackles a collaborative multi-center vehicle routing problem with resource sharing and temperature control constraints (CMCVRP-RSTC). Solving the CMCVRP-RSTC by minimizing the total cost and the number of refrigerated vehicles returns a fresh logistics operational strategy that pinpoints how a multi-center fresh logistics distribution network can be reorganized to highlight potential collaboration opportunities. To find the solution to the CMCVRP-RSTC, we develop a hybrid heuristic algorithm that combines the extended k-means clustering and tabu search non-dominated sorting genetic algorithm-II (TS-NSGA-II) to search a large solution space. This hybrid heuristic algorithm ensures that the optimal solution is found efficiently through initial solution filtering and the combination of local and global searches. Furthermore, we explore how to motivate individual LPs to collaborate by analyzing the benefits of collaboration to each LP. Using the minimum costs remaining savings method and the strictly monotonic path rule, a cost saving calculation model is proposed to find the best profit allocation scheme where each collaborating LP keeps benefiting from long-term collaboration. An empirical case study of Chongqing City, China indicates the efficiency of our proposed collaborative mechanism and optimization algorithms. Our study will help improve the efficiency of logistics operation significantly and contribute to the development of more intelligent logistics systems and smart cities.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306485",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Business",
      "Cluster analysis",
      "Commerce",
      "Computer network",
      "Computer science",
      "Control (management)",
      "Distribution center",
      "Heuristic",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Operations research",
      "Routing (electronic design automation)",
      "Sorting",
      "Tabu search",
      "Vehicle routing problem"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yong"
      },
      {
        "surname": "Zhang",
        "given_name": "Jie"
      },
      {
        "surname": "Guan",
        "given_name": "Xiangyang"
      },
      {
        "surname": "Xu",
        "given_name": "Maozeng"
      },
      {
        "surname": "Wang",
        "given_name": "Zheng"
      },
      {
        "surname": "Wang",
        "given_name": "Haizhong"
      }
    ]
  },
  {
    "title": "Total carbon emissions minimization in connected and automated vehicle routing problem with speed variables",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113910",
    "abstract": "Environmental protection and intelligence have become the inevitable development trend of future transportation. Connected and automated vehicles (CAVs) are expected to be applied in the near future. In this context, how to schedule CAVs to meet customer demands with carbon emissions minimization has become a new green vehicle routing problem (VRP). Due to the fact that carbon emissions are tremendously influenced by vehicle speed, this paper considers vehicle speed as a decision variable in the above low-carbon VRP for CAVs. In addition, the differentiation on speed limits in each time period and each type of road are also taken into account. This study formulates a nonlinear mixed-integer programming model for this problem. The outer-approximate method is used to linearize the proposed model. Moreover, a hybrid particle swarm optimization (HPSO) algorithm is developed to solve this problem. Extensive numerical experiments are conducted to validate the effectiveness of the proposed model and the efficiency of the proposed solution method. Some implications are also drawn out for reducing carbon emissions in logistics activities.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307053",
    "keywords": [
      "Algorithm",
      "Biology",
      "Computer network",
      "Computer science",
      "Context (archaeology)",
      "Ecology",
      "Economics",
      "Environmental economics",
      "Green logistics",
      "Greenhouse gas",
      "Integer programming",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Minification",
      "Operating system",
      "Paleontology",
      "Particle swarm optimization",
      "Routing (electronic design automation)",
      "Schedule",
      "Variable (mathematics)",
      "Vehicle routing problem"
    ],
    "authors": [
      {
        "surname": "Cai",
        "given_name": "Lecai"
      },
      {
        "surname": "Lv",
        "given_name": "Wenya"
      },
      {
        "surname": "Xiao",
        "given_name": "Liyang"
      },
      {
        "surname": "Xu",
        "given_name": "Ziheng"
      }
    ]
  },
  {
    "title": "Using positional sequence patterns to estimate the selectivity of SQL LIKE queries",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113762",
    "abstract": "Sequence patterns are frequently employed in many expert system applications in a wide range of domains from bioinformatics to smart homes and stock market analysis. Regular sequence patterns fail to express whether two consecutive items in a pattern are occurring right after each other in all pattern occurrences in an item database or not. Such a differentiation may be important for many intelligent system applications, for instance, to better address business questions like “should two frequently-bought-together items be located right next to each other on a retail store shelf, or is it ok to place them at some distance as long as they are in the same aisle?”. In this paper, we propose a novel type of sequence pattern, called “positional sequence patterns”, and illustrate its application on a special expert system, i.e., the query planner/optimizer of a database management system. Positional sequence patterns allow to accommodate extra information regarding whether a frequent ordered item pair always occurs next to each other without any gap in between in all pattern occurrences. Since positional sequence patterns are not considered by the existing sequence pattern mining algorithms, we also propose an algorithm to mine them. Next, we integrate the positional sequence patterns into the selectivity estimation component of the query optimizer as an expert system application. More specifically, in the knowledgebase of the query optimizer, a histogram-like structure of positional sequence patterns are created and stored. Then, during query optimization time, these histograms are utilized to infer the selectivity of flexible text queries that are enabled by the SQL LIKE operator. In particular, the proposed selectivity estimation method employs redundant pattern elimination based on pattern information content during histogram construction, and a partitioning-based matching scheme. The experimental results on a real dataset from DBLP show that the proposed approach outperforms the state of the art by around 20% improvement in error rates.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305868",
    "keywords": [],
    "authors": [
      {
        "surname": "Aytimur",
        "given_name": "Mehmet"
      },
      {
        "surname": "Cakmak",
        "given_name": "Ali"
      }
    ]
  },
  {
    "title": "Player Identification in Hockey Broadcast Videos",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113891",
    "abstract": "We present a deep recurrent convolutional neural network (CNN) approach to solve the problem of hockey player identification in NHL broadcast videos. Player identification is a difficult computer vision problem mainly because of the players’ similar appearance, occlusion, and blurry facial and physical features. However, we can observe players’ jersey numbers over time by processing variable length image sequences of players (aka ‘tracklets’). We propose an end-to-end trainable ResNet+LSTM network, with a residual network (ResNet) base and a long short-term memory (LSTM) layer, to discover spatio-temporal features of jersey numbers over time and learn long-term dependencies. Additionally, we employ a secondary 1-dimensional convolutional neural network classifier as a late score-level fusion method to classify the output of the ResNet+LSTM network. For this work, we created a new hockey player tracklet dataset that contains sequences of hockey player bounding boxes. This achieves an overall player identification accuracy score over 87% on the test split of our new dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306916",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Bounding overwatch",
      "Classifier (UML)",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Identification (biology)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Residual",
      "Residual neural network",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Chan",
        "given_name": "Alvin"
      },
      {
        "surname": "Levine",
        "given_name": "Martin D."
      },
      {
        "surname": "Javan",
        "given_name": "Mehrsan"
      }
    ]
  },
  {
    "title": "A novel F-SVM based on FOA for improving SVM performance",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113713",
    "abstract": "Parameter setting is critical for the solution efficiency and accuracy of support vector machine (SVM). The general methods for setting parameters include Grid search method (GS) and some typical swarm intelligence algorithms. However, these SVM variants only consider the margin but ignore the radius. This paper develops a new radius-margin-based SVM model with fruit fly optimization algorithm (FOA) called FOA-F-SVM, which considers the maximization of margin and the minimization of radius information. The FOA is adopted to optimize the penalty factor and parameter of RBF in F-SVM. The established model is solved in three steps, including initialization of matrix, decision of hyperplane and solution of transformation matrix. The effectiveness of the proposed FOA-F-SVM is evaluated against eight UCI datasets and eight comparison algorithms. The performance of the FOA-F-SVM is validated using the experimental results, and it is observed that FOA-F-SVM algorithm can produce more appropriate model parameters and significantly reduce the computational cost, which generates a high classification accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305376",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Geometry",
      "Hyperparameter optimization",
      "Hyperplane",
      "Initialization",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Pattern recognition (psychology)",
      "Programming language",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Gu",
        "given_name": "Qinghua"
      },
      {
        "surname": "Chang",
        "given_name": "Yinxin"
      },
      {
        "surname": "Li",
        "given_name": "Xinhong"
      },
      {
        "surname": "Chang",
        "given_name": "Zhaozhao"
      },
      {
        "surname": "Feng",
        "given_name": "Zhidong"
      }
    ]
  },
  {
    "title": "Extending latent semantic analysis to manage its syntactic blindness",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114130",
    "abstract": "Natural Language Processing (NLP) is the sub-field of Artificial Intelligence that represents and analyses human language automatically. NLP has been employed in many applications, such as information retrieval, information processing and automated answer ranking. Semantic analysis focuses on understanding the meaning of text. Among other proposed approaches, Latent Semantic Analysis (LSA) is a widely used corpus-based approach that evaluates similarity of text based on the semantic relations among words. LSA has been applied successfully in diverse language systems for calculating the semantic similarity of texts. LSA ignores the structure of sentences, i.e., it suffers from a syntactic blindness problem. LSA fails to distinguish between sentences that contain semantically similar words but have opposite meanings. Disregarding sentence structure, LSA cannot differentiate between a sentence and a list of keywords. If the list and the sentence contain similar words, comparing them using LSA would lead to a high similarity score. In this paper, we propose xLSA, an extension of LSA that focuses on the syntactic structure of sentences to overcome the syntactic blindness problem of the original LSA approach. xLSA was tested on sentence pairs that contain similar words but have significantly different meaning. Our results showed that xLSA alleviates the syntactic blindness problem, providing more realistic semantic similarity scores.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308782",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Image (mathematics)",
      "Information retrieval",
      "Latent semantic analysis",
      "Meaning (existential)",
      "Natural language processing",
      "Psychology",
      "Psychotherapist",
      "Ranking (information retrieval)",
      "Semantic role labeling",
      "Semantic similarity",
      "Sentence",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Suleman",
        "given_name": "Raja Muhammad"
      },
      {
        "surname": "Korkontzelos",
        "given_name": "Ioannis"
      }
    ]
  },
  {
    "title": "A hybrid method of link prediction in directed graphs",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113896",
    "abstract": "Link prediction is an important issue in complex network analysis and mining. Given the structure of a network, a link prediction algorithm obtains the probability that a link is established between two non-adjacent nodes in the future snapshots of the network. Many of the available link prediction methods are based on common neighborhood. A problem with these methods is that if two nodes do not have any common neighbors, they always predict a chance of zero for establishment of a link between them; however, such nodes have been shown to establish links in some real systems. Another issue with these measures is that they often disregard the connection direction. Here, we propose a novel measure based on common neighborhood that resolves the above issues. The proposed measures are applied on three benchmark networks in both unsupervised and supervised learning modes. Our experiments show the superior performance of the proposed measures over that of the state-of-the-art link prediction methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306965",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Complex network",
      "Computer network",
      "Computer science",
      "Data mining",
      "Geodesy",
      "Geography",
      "Link (geometry)",
      "Machine learning",
      "Measure (data warehouse)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Ghorbanzadeh",
        "given_name": "Hossien"
      },
      {
        "surname": "Sheikhahmadi",
        "given_name": "Amir"
      },
      {
        "surname": "Jalili",
        "given_name": "Mahdi"
      },
      {
        "surname": "Sulaimany",
        "given_name": "Sadegh"
      }
    ]
  },
  {
    "title": "CNN-based multilingual handwritten numeral recognition: A fusion-free approach",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113784",
    "abstract": "Numeral recognition plays a vital role in making automated systems like postal address sorting and license plate recognition. In a multilingual country like India, more often, the multiple languages are mixed while writing. Numeral recognizer systems which can handle more than one language are very much useful to recognize numerals of various scripts. Handwritten numeral recognition is much more complicated than the printed one because of the different writing styles. Few multilingual works have been reported earlier, but these systems are either trained for each language individually or fusion of similar shaped classes of performed. In either case, multiple classes exist for a single digit. In this work, we have developed a script independent numeral recognition system for multilingual handwritten digits which is independent of fusion and has only 10 classes corresponding to every single numeric digit. This work is first of its kind in which we have addressed the problems of multilingual numeral recognition systems. Exhaustive experiments are done with numeral datasets of 8 Indic and non-Indic scripts. We have obtained the accuracy of 96.23% collectively for all the eight scripts. This obtained high accuracy is promising and demonstrates the hypothesis that multilingual handwritten numeral recognition is void with CNN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306084",
    "keywords": [
      "Arabic numerals",
      "Arithmetic",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Digit recognition",
      "Mathematics",
      "Natural language processing",
      "Numeral system",
      "Numerical digit",
      "Pattern recognition (psychology)",
      "Programming language",
      "Scripting language",
      "Sorting",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Gupta",
        "given_name": "Deepika"
      },
      {
        "surname": "Bag",
        "given_name": "Soumen"
      }
    ]
  },
  {
    "title": "Multi-temperature simulated annealing for optimizing mixed-blocking permutation flowshop scheduling problems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113837",
    "abstract": "Scheduling problems play an increasingly significant role in the design and optimization of highly computerized and automated production systems. Given the importance of just-in-time production in advanced manufacturing, scheduling methods should enable the users to consider various blocking situations in a zero work-in-process scheme. In this situation, Permutation Flowshop Scheduling Problem with Mixed-Blocking Constraints (MBPFSP) is a much-needed scheduling extension that allows for heterogeneous blocking criteria between successive machines. Considering the scale of integrated production systems, and the inherent complexities involved in this type of scheduling problems, efficient and robust solution algorithms are necessary to facilitate industry applications of this emerging scheduling problem. This study extends to develop an improved metaheuristic, the Multiple Temperature Simulated Annealing (MTSA) algorithm, to provide high-quality solutions to MBPFSPs, considering makespan. Using extensive benchmark experiments, it is shown that the developed algorithm outperforms the state-of-the-art existing approaches applied to solve the MBPFSP. Overall, this research sets the stage for MBPFSP’s industry scale applications, narrowing the gap between the scheduling theory and practice.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306473",
    "keywords": [
      "Algorithm",
      "Computer network",
      "Computer science",
      "Distributed computing",
      "Dynamic priority scheduling",
      "Embedded system",
      "Fair-share scheduling",
      "Flow shop scheduling",
      "Job shop scheduling",
      "Mathematical optimization",
      "Mathematics",
      "Quality of service",
      "Rate-monotonic scheduling",
      "Routing (electronic design automation)",
      "Scheduling (production processes)",
      "Simulated annealing",
      "Two-level scheduling"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Shih-Wei"
      },
      {
        "surname": "Cheng",
        "given_name": "Chen-Yang"
      },
      {
        "surname": "Pourhejazy",
        "given_name": "Pourya"
      },
      {
        "surname": "Ying",
        "given_name": "Kuo-Ching"
      }
    ]
  },
  {
    "title": "Player Identification in Hockey Broadcast Videos",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113891",
    "abstract": "We present a deep recurrent convolutional neural network (CNN) approach to solve the problem of hockey player identification in NHL broadcast videos. Player identification is a difficult computer vision problem mainly because of the players’ similar appearance, occlusion, and blurry facial and physical features. However, we can observe players’ jersey numbers over time by processing variable length image sequences of players (aka ‘tracklets’). We propose an end-to-end trainable ResNet+LSTM network, with a residual network (ResNet) base and a long short-term memory (LSTM) layer, to discover spatio-temporal features of jersey numbers over time and learn long-term dependencies. Additionally, we employ a secondary 1-dimensional convolutional neural network classifier as a late score-level fusion method to classify the output of the ResNet+LSTM network. For this work, we created a new hockey player tracklet dataset that contains sequences of hockey player bounding boxes. This achieves an overall player identification accuracy score over 87% on the test split of our new dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306916",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Bounding overwatch",
      "Classifier (UML)",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Identification (biology)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Residual",
      "Residual neural network",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Chan",
        "given_name": "Alvin"
      },
      {
        "surname": "Levine",
        "given_name": "Martin D."
      },
      {
        "surname": "Javan",
        "given_name": "Mehrsan"
      }
    ]
  },
  {
    "title": "A fuzzy inference based scenario building in two-stage optimization framework for sustainable recycling supply chain redesign",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113906",
    "abstract": "This paper aims to develop a scenario-based optimization framework to deals with several issues related to redesign sustainable reverse supply chain; focused particularly in the epistemic uncertainty of supply and demand input parameters and their relationship with supply chain performance, both important for the redesign problem. The two-step optimization framework starts with a Fuzzy Inference System methodology for scenario generation that faces the lack of information, and the necessity of estimate the expected operation cost and environmental impact. Then we use generated scenarios into the epsilon-constraint method which solves a multi-objective model to obtain a relevant set of solutions. After solving the second optimization model, we propose to analyze the robustness of the achieved redesign solutions considering a customer satisfaction approach. The computational experiments show that our proposed framework supports better the inclusion of scenarios for redesigning the plastic recycling supply chain. Furthermore, we study a real-life plastic recycling problem in Cuba which demonstrates that the framework is able to support the redesign decision making with robust solutions sensitive to the changes of the studied uncertain parameters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307016",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Constraint (computer-aided design)",
      "Engineering",
      "Fuzzy logic",
      "Gene",
      "Inference",
      "Law",
      "Mathematical optimization",
      "Mathematics",
      "Mechanical engineering",
      "Medicine",
      "Political science",
      "Risk analysis (engineering)",
      "Robust optimization",
      "Robustness (evolution)",
      "Service management",
      "Supply chain",
      "Supply chain management",
      "Supply chain risk management"
    ],
    "authors": [
      {
        "surname": "Feitó-Cespón",
        "given_name": "Michael"
      },
      {
        "surname": "Costa",
        "given_name": "Yasel"
      },
      {
        "surname": "Pishvaee",
        "given_name": "Mir Saman"
      },
      {
        "surname": "Cespón-Castro",
        "given_name": "Roberto"
      }
    ]
  },
  {
    "title": "Cross-lingual learning for text processing: A survey",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113765",
    "abstract": "Many intelligent systems in business, government or academy process natural language as an input during inference or they might even communicate with users in natural language. The natural language processing is currently often done with machine learning models. However, machine learning needs training data and such data are often scarce for low-resource languages. The lack of data and resulting poor performance of natural language processing can be solved with cross-lingual learning. Cross-lingual learning is a paradigm for transferring knowledge from one natural language to another. The transfer of knowledge can help us overcome the lack of data in the target languages and create intelligent systems and machine learning models for languages, where it was not possible previously. Despite its increasing popularity and potential, no comprehensive survey on cross-lingual learning was conducted so far. We survey 173 text processing cross-lingual learning papers and examine tasks, datasets and languages that were used. The most important contribution of our work is that we identify and analyze four types of cross-lingual transfer based on “what” is being transferred. Such insight might help other NLP researchers and practitioners to understand how to use cross-lingual learning for wide range of problems. In addition, we identify what we consider to be the most important research directions that might help the community to focus their future work in cross-lingual learning. We present a comprehensive table of all the surveyed papers with various data related to the cross-lingual learning techniques they use. The table can be used to find relevant papers and compare the approaches to cross-lingual learning. To the best of our knowledge, no survey of cross-lingual text processing techniques was done in this scope before.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305893",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Information retrieval",
      "Natural language processing"
    ],
    "authors": [
      {
        "surname": "Pikuliak",
        "given_name": "Matúš"
      },
      {
        "surname": "Šimko",
        "given_name": "Marián"
      },
      {
        "surname": "Bieliková",
        "given_name": "Mária"
      }
    ]
  },
  {
    "title": "A cooperative system for metaheuristic algorithms",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113976",
    "abstract": "Optimization problems are defined as the functions whereby the target is to find the optimum state depending on the parameters that have certain limitations. In the field of optimization, the aim is to find from among multiple alternative solutions the optimal solution or approximate solution that provides all the restrictions. Metaheuristic is an extremely effective method to find approximate solutions to optimization problems. However, when metaheuristics are used, there occurs an algorithm selection problem. This problem involves decision-making about which algorithm is to be used to solve the existing optimization problem with maximum performance. The objective of this study is to use a cooperative system that combines different metaheuristics to successfully deal with algorithm selection problems. An intelligent combination of different metaheuristics is expected to provide more flexible, more efficient and more robust approaches. However, such a combination requires less precision. The combination is generated through a methodology designed with soft computing. In addition to the algorithm selection problem, the adjustment of algorithm parameters has significant importance in obtaining good results. For this reason, the cooperative system proposed in this study offers fine-tuning of parameters based on soft computing techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307545",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Field (mathematics)",
      "Mathematical optimization",
      "Mathematics",
      "Meta-optimization",
      "Metaheuristic",
      "Optimization problem",
      "Parallel metaheuristic",
      "Pure mathematics",
      "Selection (genetic algorithm)",
      "Soft computing"
    ],
    "authors": [
      {
        "surname": "Tezel",
        "given_name": "Baris Tekin"
      },
      {
        "surname": "Mert",
        "given_name": "Ali"
      }
    ]
  },
  {
    "title": "Self-driving cars: A survey",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113816",
    "abstract": "We survey research on self-driving cars published in the literature focusing on autonomous cars developed since the DARPA challenges, which are equipped with an autonomy system that can be categorized as SAE level 3 or higher. The architecture of the autonomy system of self-driving cars is typically organized into the perception system and the decision-making system. The perception system is generally divided into many subsystems responsible for tasks such as self-driving-car localization, static obstacles mapping, moving obstacles detection and tracking, road mapping, traffic signalization detection and recognition, among others. The decision-making system is commonly partitioned as well into many subsystems responsible for tasks such as route planning, path planning, behavior selection, motion planning, and control. In this survey, we present the typical architecture of the autonomy system of self-driving cars. We also review research on relevant methods for perception and decision making. Furthermore, we present a detailed description of the architecture of the autonomy system of the self-driving car developed at the Universidade Federal do Espírito Santo (UFES), named Intelligent Autonomous Robotics Automobile (IARA). Finally, we list prominent self-driving car research platforms developed by academia and technology companies, and reported in the media.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030628X",
    "keywords": [
      "Archaeology",
      "Architecture",
      "Artificial intelligence",
      "Autonomy",
      "Computer science",
      "Control (management)",
      "Engineering",
      "Geography",
      "Law",
      "Motion planning",
      "Neuroscience",
      "Perception",
      "Political science",
      "Psychology",
      "Robot",
      "Robotics",
      "Self driving",
      "Systems architecture",
      "Transport engineering"
    ],
    "authors": [
      {
        "surname": "Badue",
        "given_name": "Claudine"
      },
      {
        "surname": "Guidolini",
        "given_name": "Rânik"
      },
      {
        "surname": "Carneiro",
        "given_name": "Raphael Vivacqua"
      },
      {
        "surname": "Azevedo",
        "given_name": "Pedro"
      },
      {
        "surname": "Cardoso",
        "given_name": "Vinicius B."
      },
      {
        "surname": "Forechi",
        "given_name": "Avelino"
      },
      {
        "surname": "Jesus",
        "given_name": "Luan"
      },
      {
        "surname": "Berriel",
        "given_name": "Rodrigo"
      },
      {
        "surname": "Paixão",
        "given_name": "Thiago M."
      },
      {
        "surname": "Mutz",
        "given_name": "Filipe"
      },
      {
        "surname": "de Paula Veronese",
        "given_name": "Lucas"
      },
      {
        "surname": "Oliveira-Santos",
        "given_name": "Thiago"
      },
      {
        "surname": "De Souza",
        "given_name": "Alberto F."
      }
    ]
  },
  {
    "title": "An improved grammatical evolution approach for generating perturbative heuristics to solve combinatorial optimization problems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113853",
    "abstract": "Search methodologies such as hyper-heuristics have been successfully used to automate the generation of perturbative heuristics to solve combinatorial optimization problems. However, the domain of automated generation of perturbative heuristics has generally not been well researched and very few works have actually been conducted in the area. In addition, most of the proposed hyper-heuristic methods in the literature simply recombine already existing and human-derived low-level perturbative heuristics (or primitive heuristic components) with various move acceptance criteria to generate the new perturbative heuristics instead of producing the heuristics from scratch. As a result, these methods cannot be applied to problem domains where the human-derived low-level heuristics are not available. The study presented in this paper addresses this issue by proposing an improved approach, based on our previous work, that generates good quality reusable perturbative heuristics from scratch. While this new approach also uses grammatical evolution to generate the heuristics from a set of basic actions and components of the solution (as in our previous work), the grammar has been substantially extended and includes new methods for selecting solution components, conditional constructs, utilizes some information from the solution space as well as extends the syntax of the basic actions in order to cover a wider range of heuristics. Furthermore, the new approach has been applied to a new problem domain, i.e. the boolean satisfiability problem, in addition to the examination timetabling and vehicle routing problems investigated in the earlier work. The experimental results show that the new approach not only generates better perturbative heuristics than those produced in our earlier work, it also produces results that are competitive with those obtained by currently existing generation perturbative hyper-heuristics that have been applied to the benchmark sets in the three domains.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306643",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Domain (mathematical analysis)",
      "Grammar",
      "Heuristic",
      "Heuristics",
      "Linguistics",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Philosophy",
      "Programming language",
      "Set (abstract data type)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Mweshi",
        "given_name": "George"
      },
      {
        "surname": "Pillay",
        "given_name": "Nelishia"
      }
    ]
  },
  {
    "title": "The user-knowledge crowdsourcing task allocation integrated decision model and genetic matrix factorization algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113798",
    "abstract": "Given the pattern matching problem between the knowledge in the Expert Knowledge Recommendation System (EKRS) established by the author of this article and user’s expertise knowledge, based on the full consideration of some constraint conditions such as user characteristics, task characteristics and matching eigenvalues, the user-knowledge integrated decision-making mathematical model (UKID) is established by using the crowdsourcing task allocation method and matrix decomposition algorithm. Aiming at the characteristics of slow convergence speed and inaccurate user-knowledge (task) matching of the UKID model in a high-dimensional space, the improved genetic matrix factorization algorithm (GMF) is designed. The genetic matrix algorithm is used to map the high-dimensional spatial data to the low-dimensional characteristic space which can improve the accuracy of task assignment. Through analysis to ‘hereditary property’ under high complexity condition, with the maximum preserved crossover operator and adaptive crossover mutation, GMF not only furthest retains the excellent characteristics of parents and improves the algorithm ‘premature properties’, but also builds up the quality of the simple genetic algorithm’s optimization capabilities and enhances the solution’s accuracy. Finally, the algorithm was experimentally verified with Amazon Public dataset and the real data extracted with EKRS. The experimental results showed that the UKID model and algorithm were applicable and could provide the decision-making guidance for the knowledge crowdsourcing task assignment.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306175",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Convergence (economics)",
      "Crossover",
      "Crowdsourcing",
      "Data mining",
      "Economic growth",
      "Economics",
      "Eigenvalues and eigenvectors",
      "Genetic algorithm",
      "Machine learning",
      "Management",
      "Matching (statistics)",
      "Mathematics",
      "Matrix decomposition",
      "Physics",
      "Quantum mechanics",
      "Statistics",
      "Task (project management)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Li"
      },
      {
        "surname": "Gan",
        "given_name": "Yi"
      },
      {
        "surname": "Sun",
        "given_name": "Mingda"
      },
      {
        "surname": "Gao",
        "given_name": "Liping"
      }
    ]
  },
  {
    "title": "Kernel based tabu search for the Set-union Knapsack Problem",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113802",
    "abstract": "Given a set of profitable items where each item is a set of weighted elements, the Set-union Knapsack Problem is to pack a subset of items into a capacity constrained knapsack to maximize the total profit of the selected items. This problem appears in many practical applications; however, it is computationally challenging. To advance the state-of-the-art for solving this relevant problem, we introduce a competitive heuristic algorithm, which features original kernel-based search components and an effective local search procedure. Extensive computational assessments on 60 benchmark instances demonstrate the high performance of the algorithm. We show different analyses to get insights into the influences of its algorithmic components. We make the code of the algorithm publicly available to facilitate its use in practice.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306199",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Combinatorics",
      "Computer science",
      "Continuous knapsack problem",
      "Geodesy",
      "Geography",
      "Heuristic",
      "Kernel (algebra)",
      "Knapsack problem",
      "Mathematical optimization",
      "Mathematics",
      "Programming language",
      "Set (abstract data type)",
      "Tabu search"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Zequn"
      },
      {
        "surname": "Hao",
        "given_name": "Jin-Kao"
      }
    ]
  },
  {
    "title": "DeepSigns: A predictive model based on Deep Learning for the early detection of patient health deterioration",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113905",
    "abstract": "Early diagnosis of critically ill patients depends on the attention and observation of medical staff about different variables, as vital signs, results of laboratory tests, among other. Seriously ill patients usually have changes in their vital signs before worsening. Monitoring these changes is important to anticipate the diagnosis in order to initiate patients’ care. Prognostic indexes play a fundamental role in this context since they allow to estimate the patients’ health status. Besides, the adoption of electronic health records improved the availability of data, which can be processed by machine learning techniques for information extraction to support clinical decisions. In this context, this work aims to create a computational model able to predict the deterioration of patients’ health status in such a way that it is possible to start the appropriate treatment as soon as possible. The model was developed based on Deep Learning technique, a Recurrent Neural Networks, the Long Short-Term Memory, for the prediction of patient’s vital signs and subsequent evaluation of the patient’s health status severity through Prognostic Indexes commonly used in the health area. Experiments showed that it is possible to predict vital signs with good precision (accuracy > 80%) and, consequently, predict the Prognostic Indexes in advance to treat the patients before deterioration. Predicting the patient’s vital signs for the future and use them for the Prognostic Index’ calculation allows clinical times to predict future severe diagnoses that would not be possible applying the current patient’s vital signs (50%–60% of cases would not be identified).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307004",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Machine learning"
    ],
    "authors": [
      {
        "surname": "da Silva",
        "given_name": "Denise Bandeira"
      },
      {
        "surname": "Schmidt",
        "given_name": "Diogo"
      },
      {
        "surname": "da Costa",
        "given_name": "Cristiano André"
      },
      {
        "surname": "da Rosa Righi",
        "given_name": "Rodrigo"
      },
      {
        "surname": "Eskofier",
        "given_name": "Björn"
      }
    ]
  },
  {
    "title": "Hierarchical visual localization for visually impaired people using multimodal images",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113743",
    "abstract": "Localization is one of the crucial issues in assistive technology for visually impaired people. In this paper, we propose a novel hierarchical visual localization pipeline based on the wearable assistive navigation device for visually impaired people. The proposed pipeline involves the deep descriptor network, 2D-3D geometric verification and online sequence matching. Images in different modalities (RGB, Infrared and Depth) are fed into Dual Desc network to generate robust attentive global descriptors and local features. The global descriptors are leveraged to retrieve the coarse candidates of query images. The 2D local features, as well as 3D sparse point cloud, are used in geometric verification to select the optimal results from the retrieved candidates. Finally, sequence matching robustifies the localization results by synthesizing the verified results of successive frames. The proposed unified descriptor network Dual Desc surpasses the state-of-the-art NetVLAD and its variant on the task of image description. Validated on the real-world dataset captured by the wearable assistive device, the proposed visual localization utilizes multimodal images to overcome the disadvantages of RGB images and robustifies the localization performance by deep descriptor network and hierarchical pipeline. In the challenging scenarios of the Yuquan dataset, the proposed method achieves the F 1 score of 0.77 and the mean localization error of 2.75, which is satisfactory in practical use.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305674",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Embedded system",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pipeline (software)",
      "Point cloud",
      "Programming language",
      "RGB color model",
      "Statistics",
      "Wearable computer"
    ],
    "authors": [
      {
        "surname": "Cheng",
        "given_name": "Ruiqi"
      },
      {
        "surname": "Hu",
        "given_name": "Weijian"
      },
      {
        "surname": "Chen",
        "given_name": "Hao"
      },
      {
        "surname": "Fang",
        "given_name": "Yicheng"
      },
      {
        "surname": "Wang",
        "given_name": "Kaiwei"
      },
      {
        "surname": "Xu",
        "given_name": "Zhijie"
      },
      {
        "surname": "Bai",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "Trust modeling based on probabilistic linguistic term sets and the MULTIMOORA method",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113817",
    "abstract": "Trust modeling has attracted wide attention in different domains and served as the basis for decision-making under different contexts. Building a robust and effective trust model that considers various trust-related characteristics remains anenormouschallenge. This report proposes a multi-criteria group decision-making based (MCGDM) trust evaluation model. We generalize our work from three aspects, which are trust metric, trust evaluation, and decision-making. First, we utilize the extension of the hesitant fuzzy linguistic term sets (HFLTS), called probabilistic linguistic term sets (PLTS), as our trust scaling method, which is a very suitable tool to describe the decision maker’s opinions when they are hesitant about their judgments and intend to depict their evaluation information using several linguistic terms with corresponding probability. Second, In the process of trust evaluation, trust is decomposed into multiple trustworthiness facets with different importance degrees defined by the trustor, and a structural evaluation framework is established to evaluate the trustworthiness of each alternative. The unique properties of trust are also considered comprehensively. Specifically, the properties involve the subjectivity and context-sensitivity of trust in particular application scenarios, the hesitancy and uncertainty of decision-makers in expressing their assessment opinions, similarity between the trustor and the recommenders, and the dynamic reliability of the provided opinions. Finally, in the decision-making process, we adopt the Multi-Objective Optimization by Ratio Analysis (MULTIMOORA) method, which is a robust decision-making method that simultaneously fuses three subordinate orders to derive the final ranking. The experimental results demonstrate the effectiveness and accuracy compared with the other method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306291",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Economics",
      "Group decision-making",
      "Law",
      "Metric (unit)",
      "Operating system",
      "Operations management",
      "Paleontology",
      "Physics",
      "Political science",
      "Probabilistic logic",
      "Process (computing)",
      "Quantum mechanics",
      "Ranking (information retrieval)",
      "Term (time)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yan"
      },
      {
        "surname": "Tian",
        "given_name": "Liqin"
      },
      {
        "surname": "Wu",
        "given_name": "Zenan"
      }
    ]
  },
  {
    "title": "Estimation of disparity maps through an evolutionary algorithm and global image features as descriptors",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113900",
    "abstract": "In several approaches that include analysis processes – the most well-known being object tracking, video understanding, automatic surveillance systems, and image reconstruction – there are basic tasks to be performed. One of these tasks is related to how to select an image feature window in a frame and then compute its displacement in another frame. In the literature, the last two tasks represent an open research topic because (1) estimation of the similitude for a region window involves a set of invariants that are scene-dependent; (2) a general method for detecting the best-fitting region criterion to compute the displacement is dependent on the similarity criterion and numerical approaches for estimating the displacement; and (3) the type of conditions must be warranted so that an image feature has a high probability of estimating the displacement and numerically reaching a convergence state. In this paper, we propose a framework to estimate the displacement of an image feature from a reference image to another image. The proposal uses a generalization of the optimization concept, that is, a random search process in the dissimilarity metric space. This approach is carried out in a discrete space by mapping the variable domain to be estimated to a symbolic space with a set of operators, where a random search method is described through a uniform sampling process and genetic operators. The approach searches for the best suboptimal solution of the locality under a predefined metric criterion, avoiding divergence for the worst suboptimal solutions. The proposal is based on the formalization of the Lucas and Kanade approach. It considers as a metric-space solution the proposal of the Shi and Tomasi approach, but instead of a Taylor series expansion and step-descendant approach to solve the system, an evolutionary algorithm is used. The reference approach is well accepted as one of the most important approaches in motion displacement. To test our approach, we take the task of building a disparity map for 3D geometry extraction given the number of times the displacement computation is performed (once for each pixel). Finally, the results demonstrate that the evolutionary approach increases the repeatability and robustness of the distance estimation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306941",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Displacement (psychology)",
      "Divergence (linguistics)",
      "Economics",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Metric (unit)",
      "Metric space",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Psychology",
      "Psychotherapist",
      "Set (abstract data type)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Reynosa-Guerrero",
        "given_name": "J."
      },
      {
        "surname": "Garcia-Huerta",
        "given_name": "J.-M."
      },
      {
        "surname": "Vazquez-Cervantes",
        "given_name": "A."
      },
      {
        "surname": "Reyes-Santos",
        "given_name": "E."
      },
      {
        "surname": "Perez-Ramos",
        "given_name": "J.-L."
      },
      {
        "surname": "Jimenez-Hernandez",
        "given_name": "H."
      }
    ]
  },
  {
    "title": "Ensemble mutation-driven salp swarm algorithm with restart mechanism: Framework and fundamental analysis",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113897",
    "abstract": "This research proposes a reinforced salp swarm algorithm (SSA) variant with an ensemble mutation strategy and a restart mechanism, which is named CMSRSSSA for short, to enhance exploration and exploitation capacity of SSA and conquer the restriction of a single search mechanism of the SSA in tackling continuous optimization problems. In this variant, an ensemble/composite mutation strategy (CMS) can boost the exploitation and exploration trends of SSA, as well as restart strategy (RS) is capable of assisting salps in getting away from local optimum. To investigate the performance of the proposed optimizer, firstly, IEEE CEC2017 benchmark problems are used to estimate the capability of the presented CMSRSSSA in solving continuous optimization problems in comparison to other advanced algorithms; furthermore, IEEE CEC2011 real-world benchmark problems and constrained engineering optimization problems are also utilized to assess the performance of CMSRSSSA for practical ideas. Experimental and statistical results reveal that the CMSRSSSA outperforms all the competitors, including winners of the related IEEE CEC competition; therefore, it will be able to be treated as a promising method in resolving both constrained and unconstrained optimization problems. For post-publication supports and guides on the idea of the paper, please be in touch with the hosting website: http://aliasgharheidari.com.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306977",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Competitor analysis",
      "Computer science",
      "Economics",
      "Epistemology",
      "Gene",
      "Geodesy",
      "Geography",
      "Management",
      "Mathematical optimization",
      "Mathematics",
      "Mechanism (biology)",
      "Mutation",
      "Philosophy",
      "Swarm behaviour"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Hongliang"
      },
      {
        "surname": "Wang",
        "given_name": "Zhiyan"
      },
      {
        "surname": "Chen",
        "given_name": "Weibin"
      },
      {
        "surname": "Heidari",
        "given_name": "Ali Asghar"
      },
      {
        "surname": "Wang",
        "given_name": "Mingjing"
      },
      {
        "surname": "Zhao",
        "given_name": "Xuehua"
      },
      {
        "surname": "Liang",
        "given_name": "Guoxi"
      },
      {
        "surname": "Chen",
        "given_name": "Huiling"
      },
      {
        "surname": "Zhang",
        "given_name": "Xin"
      }
    ]
  },
  {
    "title": "An effective Progressive Hedging algorithm for the two-layers time window assignment vehicle routing problem in a stochastic environment",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113877",
    "abstract": "This paper presents an effective Progressive Hedging algorithm for vehicle routing problem with two-layers time window assignment and stochastic service times (2L-TWAVRPSST). Based on a predefined exogenous time window determined by the customers, an endogenous time window is assigned to each customer. Endogenous time windows have flexible width and composed of two-layers. The outer layer is wider than inner layer and is determined by violation variable. This approach aims to giving more flexibility to career companies for serving more customers using less vehicles. Customers could be visited even after the end of their assigned time windows by paying proportional penalty, while extra violation from the outer layer is not permitted. This problem is formulated as a two-stage stochastic model with the first-stage decisions of assigning inner and outer layers time window. Then in the second stage, routes are planned for each scenario combination of stochastic demand and service time. The validity and effectiveness of the proposed model was examined by various numerical examples. The problem was solved by a Progressive Hedging (PH) algorithm for large-scale instances. The results confirm efficiency of the considered solution approach in different instances.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306837",
    "keywords": [
      "Algorithm",
      "Chemistry",
      "Computer network",
      "Computer science",
      "Economics",
      "Economy",
      "Flexibility (engineering)",
      "Layer (electronics)",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Organic chemistry",
      "Routing (electronic design automation)",
      "Service (business)",
      "Statistics",
      "Variable (mathematics)",
      "Vehicle routing problem",
      "Window (computing)"
    ],
    "authors": [
      {
        "surname": "Jalilvand",
        "given_name": "Mahdi"
      },
      {
        "surname": "Bashiri",
        "given_name": "Mahdi"
      },
      {
        "surname": "Nikzad",
        "given_name": "Erfaneh"
      }
    ]
  },
  {
    "title": "A new multi-objective optimization algorithm combined with opposition-based learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113844",
    "abstract": "The optimization problems are divided into a single objective and multi-objective. Single objective optimization has only one objective function; whereas, multi-objective optimization has multiple objective functions that generate the Pareto set; therefore, solving a multi-objective problem is a challenging problem. This paper presents a new multi-objective optimization method (called MWDEO) based on improved whale optimization algorithm (WOA) by combining the differential evolution (DE) algorithm and the opposition-based learning (OBL). The MWDEO uses the WOA to perform a global exploration, whereas DE is used to exploit the search space; while the OBL is applied to improve the exploration and exploitation by generating the opposite values. The proposed algorithm is evaluated using 32 multi-objective test problems besides a set of benchmark problems of CEC’2017. The experimental results are compared with nine state-of-the-art multi-objective methods. The analysis of the results showed that the proposed MWDEO outperformed all other algorithms in most of the test problems which indicates that the proposed MWDEO is competitive and effective in solving different types of multi-objective problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306539",
    "keywords": [
      "Algorithm",
      "Benchmark (surveying)",
      "Computer science",
      "Computer security",
      "Differential evolution",
      "Evolutionary algorithm",
      "Exploit",
      "Geodesy",
      "Geography",
      "Global optimization",
      "Mathematical optimization",
      "Mathematics",
      "Multi-objective optimization",
      "Multi-swarm optimization",
      "Optimization algorithm",
      "Optimization problem",
      "Pareto principle",
      "Test functions for optimization"
    ],
    "authors": [
      {
        "surname": "Ewees",
        "given_name": "Ahmed A."
      },
      {
        "surname": "Abd Elaziz",
        "given_name": "Mohamed"
      },
      {
        "surname": "Oliva",
        "given_name": "Diego"
      }
    ]
  },
  {
    "title": "Automatic text summarization: A comprehensive survey",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113679",
    "abstract": "Automatic Text Summarization (ATS) is becoming much more important because of the huge amount of textual content that grows exponentially on the Internet and the various archives of news articles, scientific papers, legal documents, etc. Manual text summarization consumes a lot of time, effort, cost, and even becomes impractical with the gigantic amount of textual content. Researchers have been trying to improve ATS techniques since the 1950s. ATS approaches are either extractive, abstractive, or hybrid. The extractive approach selects the most important sentences in the input document(s) then concatenates them to form the summary. The abstractive approach represents the input document(s) in an intermediate representation then generates the summary with sentences that are different than the original sentences. The hybrid approach combines both the extractive and abstractive approaches. Despite all the proposed methods, the generated summaries are still far away from the human-generated summaries. Most researches focus on the extractive approach. It is required to focus more on the abstractive and hybrid approaches. This research provides a comprehensive survey for the researchers by presenting the different aspects of ATS: approaches, methods, building blocks, techniques, datasets, evaluation methods, and future research directions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305030",
    "keywords": [
      "Artificial intelligence",
      "Automatic summarization",
      "Computer science",
      "Data science",
      "Focus (optics)",
      "Information retrieval",
      "Law",
      "Natural language processing",
      "Optics",
      "Physics",
      "Political science",
      "Politics",
      "Representation (politics)",
      "The Internet",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "El-Kassas",
        "given_name": "Wafaa S."
      },
      {
        "surname": "Salama",
        "given_name": "Cherif R."
      },
      {
        "surname": "Rafea",
        "given_name": "Ahmed A."
      },
      {
        "surname": "Mohamed",
        "given_name": "Hoda K."
      }
    ]
  },
  {
    "title": "Waiting strategy for the vehicle routing problem with simultaneous pickup and delivery using genetic algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113959",
    "abstract": "With the development of information and telecommunication technology and the wide adoption of smartphones, consumers gradually change their purchase pattern toward online shopping. They can order products from their smartphones at any moment from any place, and the volume and variety of products delivered to consumers are increasing explosively. Companies in this industry need to set up the operational strategies to accommodate the increasing demand for delivery and return of products, and their focus should be the real-world vehicle routing problems with an additional consideration of the dynamic orders placed over time. This study proposes a waiting strategy for the vehicle routing problem with simultaneous pickup and delivery. This strategy implements an index called the rerouting indicator, which functions as a decision-making threshold to determine the rerouting point for real-time demands. For the most real-world-cases with complex problems, this study proposes a genetic algorithm to solve and validate its accuracy and performance by comparing the computational results. The significance and application of the waiting strategy are validated through several experiments, and the appropriate discretion by a decision maker can demonstrate the value of the proposed strategy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307429",
    "keywords": [
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Decision maker",
      "Delivery Performance",
      "Engineering",
      "Genetic algorithm",
      "Geometry",
      "Image (mathematics)",
      "Industrial engineering",
      "Machine learning",
      "Mathematics",
      "Operations research",
      "Pickup",
      "Point (geometry)",
      "Programming language",
      "Routing (electronic design automation)",
      "Set (abstract data type)",
      "Vehicle routing problem"
    ],
    "authors": [
      {
        "surname": "Park",
        "given_name": "Hyungbin"
      },
      {
        "surname": "Son",
        "given_name": "Dongmin"
      },
      {
        "surname": "Koo",
        "given_name": "Bonwoo"
      },
      {
        "surname": "Jeong",
        "given_name": "Bongju"
      }
    ]
  },
  {
    "title": "Learning ladder neural networks for semi-supervised node classification in social network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113957",
    "abstract": "Graph convolutional networks (GCNs) and network embedding are the two main categories of popular methods for Semi-Supervised Node Classification (SSNC) in social network. However, the former is commonly oriented to attributed networks with efficient auxiliary information in nodes. The latter is usually not geared towards specific graph mining tasks. Therefore, these methods often perform poorly for specific tasks in non-attributed networks. To solve the above problems, in this paper, we propose a novel semi-supervised Node Classification method with Ladder Neural Networks named NCLNN for non-attributed network. We first preprocess the graph for capturing the structural information. Then we present and learn a deep ladder neural network for SSNC. Our trained ladder neural networks could combine supervised learning with unsupervised learning in deep neural networks via simultaneously minimizing the sum of supervised and unsupervised loss functions. Extensive experiments on three real-world network datasets demonstrate that the proposed NCLNN substantially outperforms the state-of-the-art methods on SSNC task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307405",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Economics",
      "Embedding",
      "Engineering",
      "Graph",
      "Graph embedding",
      "Machine learning",
      "Management",
      "Node (physics)",
      "Structural engineering",
      "Supervised learning",
      "Task (project management)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Bentian"
      },
      {
        "surname": "Pi",
        "given_name": "Dechang"
      },
      {
        "surname": "Lin",
        "given_name": "Yunxia"
      }
    ]
  },
  {
    "title": "An improved Jaya optimization algorithm with Lévy flight",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113902",
    "abstract": "Recent advances in metaheuristics have shown the advantages of using the Lévy distribution, which models a kind of random walk (named “Lévy flight”) with occasional “big” steps. This characteristic makes Lévy flight especially useful for performing large “jumps” that allow the search to escape from a local optimum and restart in a different region of the search space. In this paper, we investigate this idea by applying Lévy flight to Jaya, a simple yet effective Swarm Intelligence optimization algorithm recently proposed in the literature. We perform experiments on the CEC 2014 benchmark as well as five industrial optimization problems taken from the CEC 2011 benchmark, and compare the performance of the proposed Lévy flight Jaya Algorithm (LJA) against several state-of-the-art algorithms for continuous optimization. Our numerical results show that, although both Jaya and LJA are in general less efficient than the most advanced algorithms on the CEC 2014 benchmark, LJA largely outperforms the original Jaya algorithm in most cases, and is also highly competitive on the tested industrial problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306989",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Mathematical optimization",
      "Mathematics",
      "Optimization algorithm"
    ],
    "authors": [
      {
        "surname": "Iacca",
        "given_name": "Giovanni"
      },
      {
        "surname": "dos Santos Junior",
        "given_name": "Vlademir Celso"
      },
      {
        "surname": "Veloso de Melo",
        "given_name": "Vinícius"
      }
    ]
  },
  {
    "title": "BIDI: A classification algorithm with instance difficulty invariance",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113920",
    "abstract": "In artificial intelligence, an expert/intelligent systems can emulate the decision-making ability of human experts. A good classification algorithm can provide significant assistance to expert/intelligent systems in solving a variety of practical problems. In classification, the “hard” instances may be outliers or noisy instances that are difficult to learn, which may confuse the classifier and induce the overfitting problem in the case of placing much emphasis on them. In fact, the difficulty of instances is crucial for improving the generalization and credibility of classification. Unfortunately, nearly all the existing classifiers ignore this important information. In this paper, the classification difficulty of each instance is introduced from a statistical perspective, which is an inherent characteristic of the instance itself. Then, a new classification algorithm named “boosting with instance difficulty invariance (BIDI)” is proposed by incorporating the classification difficulty of instances. The BIDI conforms to the human cognition that easy instances are misclassified with a lower probability than difficult ones, and performs better with respect to generalization. The key insight of BIDI can provide relevant guidance for researchers to improve the generalization and credibility of classifiers in the expert systems of decision support systems. Experimental results demonstrate the effectiveness of BIDI in real-world data sets, indicating that it has great potential for solving many classification tasks of expert systems such as disease diagnosis and credit card fraud detection. Although the classification difficulty has strong statistical significance, its implementation remains computationally expensive. A fast method demonstrating rationality and feasibility is also proposed to approximate instances’ classification difficulty.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307120",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Shuang"
      },
      {
        "surname": "Li",
        "given_name": "Xiongfei"
      },
      {
        "surname": "Wang",
        "given_name": "Hancheng"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaoli"
      },
      {
        "surname": "Chen",
        "given_name": "Shiping"
      }
    ]
  },
  {
    "title": "A novel multi-task linear mixed model for smartphone-based telemonitoring",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113809",
    "abstract": "Telemonitoring is the use of electronic devices including smartphones to remotely monitor patients. Using predictive models, telemonitoring data can be translated into a clinical indicator of disease severity, thus allowing physicians to assess patient conditions more frequently and adjust treatment. This greatly complements conventional in-clinic medical examination that requires patients’ physical presence in a specialized clinic, which is logistically inconvenient, costly, and far less frequent. The challenge, however, is the need for a patient-specific predictive model to address patient heterogeneity. Training a patient-specific model suffers from small sample sizes. This can be potentially tackled by multi-task learning (MTL), which builds models for multiple related tasks (e.g., patients with the same disease) jointly to allow the models to borrow strength from each other. Existing MTL models do not suffice because they typically assume sample independence, but the samples in a telemonitoring application correspond to repeated measurements over time for the same patient that have an inherent correlation. This special data characteristic requires a linear mixed model (LMM), which has not been integrated with MTL in the existing literature. We propose a new Multi-task Linear Mixed Model (MultiLMM) model that integrates MTL and LMM in a single framework. Our methodological contributions include a mathematical formulation for MultiLMM, an efficient and converging algorithm for parameter estimation, and a theoretical analysis to reveal the reason why MultiLMM outperforms LMM by integrating the multi-task learning capability. Our simulation studies demonstrate better performance of MultiLMM than LMM under various scenarios. Finally, we present an application of using MultiLMM to predict the Unified Parkinson Disease Rating Scale (UPDRS), a clinical instrument for measuring PD severity, based on tapping signals collected by the mPower app installed on patients’ smartphones. MultiLMM shows higher prediction accuracy than a collection of competing approaches under different training sample sizes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306242",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Data mining",
      "Economics",
      "Independence (probability theory)",
      "Linear model",
      "Machine learning",
      "Management",
      "Mathematics",
      "Sample (material)",
      "Statistics",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Yoon",
        "given_name": "Hyunsoo"
      },
      {
        "surname": "Gaw",
        "given_name": "Nathan"
      }
    ]
  },
  {
    "title": "A feature selection algorithm of decision tree based on feature weight",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113842",
    "abstract": "In order to improve the classification accuracy, a preprocessing step is used to pre-filter some redundant or irrelevant features before decision tree construction. And a new feature selection algorithm FWDT is proposed based on this. Experimental results show that FWDT our proposed method performs better for the measures of accuracy, recall and F1-score. Furthermore, it can reduce the required time in constructing the decision tree.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306515",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Data pre-processing",
      "Decision tree",
      "Decision tree learning",
      "Feature (linguistics)",
      "Feature selection",
      "Filter (signal processing)",
      "ID3 algorithm",
      "Incremental decision tree",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Minimum redundancy feature selection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Precision and recall",
      "Preprocessor",
      "Selection (genetic algorithm)",
      "Tree (set theory)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "HongFang"
      },
      {
        "surname": "Zhang",
        "given_name": "JiaWei"
      },
      {
        "surname": "Zhou",
        "given_name": "YueQing"
      },
      {
        "surname": "Guo",
        "given_name": "XiaoJie"
      },
      {
        "surname": "Ma",
        "given_name": "YiMing"
      }
    ]
  },
  {
    "title": "Quaternion statistics applied to the classification of motion capture data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113813",
    "abstract": "Unit quaternions give quite new possibilities in an analysis of motion capture data. They provide a compact, holistic axis-angle representation of 3D rotations. An application of descriptive statistics – measures of location and dispersion – is common in numerous problems related to an assessment of joint movements. For those reasons, the paper proposes new approaches to the extraction of motion descriptors on the basis of descriptive statistics of 3D rotations represented by unit quaternions as well as the appropriate classification schemes operating on motion descriptors. Mean and median values and standard deviations are calculated for time series with raw rotational data as well as angular velocities and accelerations. The problem of human gait identification is addressed in the numerical validation of the introduced methods and highly precise marker-based motion capture data are utilized. The results obtained – the accuracy of gait recognition – are compared to the ones achieved by descriptive statistics calculated for time series of Euler angles. The general conclusion is that unit quaternions are effective in the calculation of descriptive statistics. They preserve robust discriminative features of joint movements and they can be applied in numerous challenges of expert and intelligent systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306266",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Descriptive statistics",
      "Discriminative model",
      "Euler angles",
      "Geometry",
      "Mathematics",
      "Motion (physics)",
      "Pattern recognition (psychology)",
      "Quaternion",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Świtoński",
        "given_name": "Adam"
      },
      {
        "surname": "Josiński",
        "given_name": "Henryk"
      },
      {
        "surname": "Michalczuk",
        "given_name": "Agnieszka"
      },
      {
        "surname": "Wojciechowski",
        "given_name": "Konrad"
      }
    ]
  },
  {
    "title": "Quaternion statistics applied to the classification of motion capture data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113813",
    "abstract": "Unit quaternions give quite new possibilities in an analysis of motion capture data. They provide a compact, holistic axis-angle representation of 3D rotations. An application of descriptive statistics – measures of location and dispersion – is common in numerous problems related to an assessment of joint movements. For those reasons, the paper proposes new approaches to the extraction of motion descriptors on the basis of descriptive statistics of 3D rotations represented by unit quaternions as well as the appropriate classification schemes operating on motion descriptors. Mean and median values and standard deviations are calculated for time series with raw rotational data as well as angular velocities and accelerations. The problem of human gait identification is addressed in the numerical validation of the introduced methods and highly precise marker-based motion capture data are utilized. The results obtained – the accuracy of gait recognition – are compared to the ones achieved by descriptive statistics calculated for time series of Euler angles. The general conclusion is that unit quaternions are effective in the calculation of descriptive statistics. They preserve robust discriminative features of joint movements and they can be applied in numerous challenges of expert and intelligent systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306266",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Descriptive statistics",
      "Discriminative model",
      "Euler angles",
      "Geometry",
      "Mathematics",
      "Motion (physics)",
      "Pattern recognition (psychology)",
      "Quaternion",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Świtoński",
        "given_name": "Adam"
      },
      {
        "surname": "Josiński",
        "given_name": "Henryk"
      },
      {
        "surname": "Michalczuk",
        "given_name": "Agnieszka"
      },
      {
        "surname": "Wojciechowski",
        "given_name": "Konrad"
      }
    ]
  },
  {
    "title": "Sign Language Recognition: A Deep Survey",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113794",
    "abstract": "Sign language, as a different form of the communication language, is important to large groups of people in society. There are different signs in each sign language with variability in hand shape, motion profile, and position of the hand, face, and body parts contributing to each sign. So, visual sign language recognition is a complex research area in computer vision. Many models have been proposed by different researchers with significant improvement by deep learning approaches in recent years. In this survey, we review the vision-based proposed models of sign language recognition using deep learning approaches from the last five years. While the overall trend of the proposed models indicates a significant improvement in recognition accuracy in sign language recognition, there are some challenges yet that need to be solved. We present a taxonomy to categorize the proposed models for isolated and continuous sign language recognition, discussing applications, datasets, hybrid models, complexity, and future lines of research in the field.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030614X",
    "keywords": [
      "Artificial intelligence",
      "Categorization",
      "Computer science",
      "Deep learning",
      "Field (mathematics)",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Philosophy",
      "Pure mathematics",
      "Sign (mathematics)",
      "Sign language"
    ],
    "authors": [
      {
        "surname": "Rastgoo",
        "given_name": "Razieh"
      },
      {
        "surname": "Kiani",
        "given_name": "Kourosh"
      },
      {
        "surname": "Escalera",
        "given_name": "Sergio"
      }
    ]
  },
  {
    "title": "An alternative practical public-key cryptosystems based on the Dependent RSA Discrete Logarithm Problems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114047",
    "abstract": "In this paper, an alternative public-key cryptosystems (PKCs) are proposed based on the new algebraic problems namely “Dependent RSA Discrete Logarithm Problems” derived from the RSA and Discrete Logarithm (DLog) assumptions together. These PKCs are provably secure for the notions of security: indistinguishable encryptions under chosen-plaintext attacks (IND-CPA), and adaptive chosen-ciphertext attacks (IND-CCA2). Initially, a new algebraic “Computational-Dependent RSA Discrete Logarithm Problem” is presented. Then, its variant named “Decisional-Dependent RSA Discrete Logarithm Problem” is presented. Thereafter, a specific discussion has been done about their hardness and their relations to each other. Also, some arguments are given to validate the cryptographic purpose of these problems. Next, using this decisional variant an efficient PKC: “Dependent RSA Discrete Logarithm” (DRDL) cryptosystem that has indistinguishable encryptions under chosen-plaintext attacks, in the standard model is presented. Further, a PKC variant: DRDL-1 cryptosystem with improved security properties that has indistinguishable encryptions under adaptive chosen-ciphertext attacks using this decisional variant in the random oracle model, with a low computational cost is presented. These new algebraic problems constructed by using the apparent hardness of RSA and Discrete Logarithm (DLog) problems are helpful in combining both efficiency and security. Hence, it becomes more efficient than all the cryptosystems specially designed for the ElGamal cryptosystem to make it indistinguishable encryptions under adaptive chosen-ciphertext attacks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308149",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Computer science",
      "Computer security",
      "Cryptography",
      "Cryptosystem",
      "Discrete logarithm",
      "Encryption",
      "Key (lock)",
      "Logarithm",
      "Mathematical analysis",
      "Mathematics",
      "Public key cryptosystem",
      "Public-key cryptography",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Tripathi",
        "given_name": "Shailendra Kumar"
      },
      {
        "surname": "Gupta",
        "given_name": "Bhupendra"
      },
      {
        "surname": "Pandian",
        "given_name": "K.K. Soundra"
      }
    ]
  },
  {
    "title": "New activation functions for single layer feedforward neural network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113977",
    "abstract": "Artificial Neural Network (ANN) is a subfield of machine learning and it has been widely used by the researchers. The attractiveness of ANNs comes from their remarkable information processing capability. An important key parameter of ANN is activation function (AF). Different AFs can significantly affect the performance of an ANN, and therefore choosing a good AF is important. This study aims to introduce some new AFs that combine the advantages of predefined AFs and perform better than them. For this purpose, we propose some new AFs, which we called generalized swish, mean-swish, ReLU-swish, triple-state swish, sigmoid-algebraic, triple-state sigmoid, exponential swish, sinc-sigmoid and derivative of sigmoid AFs. Then, we compare the proposed AFs with some well-known and recently proposed AFs. To investigate the performance of these AFs, we use four different data sets, which are simulated data, optical interconnection network data, specifications of cars, and average house costs data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307557",
    "keywords": [
      "Activation function",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Machine learning",
      "Sigmoid function"
    ],
    "authors": [
      {
        "surname": "Koçak",
        "given_name": "Yılmaz"
      },
      {
        "surname": "Üstündağ Şiray",
        "given_name": "Gülesen"
      }
    ]
  },
  {
    "title": "Deep neural system for supporting tumor recognition of mammograms using modified GAN",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113968",
    "abstract": "This paper presents the autoencoder-generative adversarial network (AGAN) in the analysis of mammograms. The AGAN architecture is used to augment the data by generating additional representations of the mammogram images, enhancing this way the information of the analyzed problem. The images generated by this this deep network are appended to the original set of mammograms and fed to the input of convolutional neural network, which plays the role of the final classifier. The proposed system was used to recognize the mammograms belonging to two classes: normal and abnormal. The investigations were performed using a large database consisting of 11,218 regions of interest of mammographic images from the DDSM base. The results demonstrate the advantage of this proposed deep learning system over other known approaches to mammogram recognition. Our average accuracy in detecting abnormal cases (malignant plus benign versus healthy) was 89.71%, sensitivity 93.54%, specificity 80.58% and AUC = 0.9410. These results are among the best for this large database.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307491",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Breast cancer",
      "Cancer",
      "Classifier (UML)",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Generative adversarial network",
      "Internal medicine",
      "Mammography",
      "Medicine",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Swiderski",
        "given_name": "B."
      },
      {
        "surname": "Gielata",
        "given_name": "L."
      },
      {
        "surname": "Olszewski",
        "given_name": "P."
      },
      {
        "surname": "Osowski",
        "given_name": "S."
      },
      {
        "surname": "Kołodziej",
        "given_name": "M."
      }
    ]
  },
  {
    "title": "Optimizing nonlinear charging times of electric vehicle routing with genetic algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114039",
    "abstract": "With the rising share of electric vehicles used in the service industry, the optimization of their specific constraints is gaining importance. Lowering energy consumption, time of charging and the strain on the electric grid are just some of the issues that must be tackled, to ensure a cleaner and more efficient industry. This paper presents a Two-Layer Genetic Algorithm (TLGA) for solving the capacitated Multi-Depot Vehicle Routing Problem with Time Windows (MDVRPTW) and Electric Vehicles (EV) with partial nonlinear recharging times (NL) – E-MDVRPTW-NL. Here, the optimization goal is to minimize driving times, number of stops at electric charging stations and time of recharging while taking the nonlinear recharging times into account. This routing problem closes the gap between electric vehicle routing problem research on the one hand and its applications to several problems with numerous real-world constraints of electric vehicles on the other. Next to the definition and the formulation of the E-MDVRPTW-NL, this paper presents the evolutionary method for solving this problem using the Genetic Algorithm (GA), where a novel two-layer genotype with multiple crossover operators is considered. This allows the GA to not only solve the order of the routes but also the visits to electric charging stations and the electric battery recharging times. Various settings of the proposed method are presented, tested and compared to competing meta-heuristics using well-known benchmarks with the addition of charging stations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308083",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Battery (electricity)",
      "Computer network",
      "Computer science",
      "Crossover",
      "Electric vehicle",
      "Genetic algorithm",
      "Heuristics",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Nonlinear system",
      "Operating system",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Routing (electronic design automation)",
      "Vehicle routing problem"
    ],
    "authors": [
      {
        "surname": "Karakatič",
        "given_name": "Sašo"
      }
    ]
  },
  {
    "title": "An efficient and accurate recommendation strategy using degree classification criteria for item-based collaborative filtering",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113756",
    "abstract": "An efficient and accurate recommender system provides online users with a variety of personalized recommendation services, thus effectively improving the satisfaction and experience of users. However, there is a trade-off between the accuracy and efficiency in recommender systems. Accordingly, this study introduces a recommendation strategy to address this limitation. The extended degree classification criteria are first proposed to assign items to more fine-grained classes. Later, item similarity measure is deployed to quickly evaluate similarities between items within the same class, which greatly reduces the runtime of similarity calculation. To obtain a better recommendation result, a Hellinger distance (HD) based item similarity is presented to calculate item similarity from the perspective of rating probability distribution. Additionally, a sigmoid function is considered in the HD similarity to emphasize the importance of the co-rated items and effectively distinguish differences between a pair of items. The experimental results on two benchmark datasets show that the proposed similarity method using the classification criteria has better performance in both accuracy and efficiency compared to other methods. Also, the results verify the effectiveness of the proposed classification criteria, especially the runtime of item-based CF method is reduced by at least 61% while maintaining a relatively stable or higher accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305807",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Collaborative filtering",
      "Computer science",
      "Data mining",
      "Geodesy",
      "Geography",
      "Hellinger distance",
      "Image (mathematics)",
      "Information retrieval",
      "Machine learning",
      "Mathematics",
      "Recommender system",
      "Similarity (geometry)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Junpeng"
      },
      {
        "surname": "Deng",
        "given_name": "Jiangzhou"
      },
      {
        "surname": "Ran",
        "given_name": "Xun"
      },
      {
        "surname": "Wang",
        "given_name": "Yong"
      },
      {
        "surname": "Jin",
        "given_name": "Hang"
      }
    ]
  },
  {
    "title": "DCT-phase statistics for forged IMEI numbers and air ticket detection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114014",
    "abstract": "New tools have been developing with the intention of having more flexibility and greater user-friendliness for editing the images and documents in digital technologies, but, unfortunately, they are also being used for manipulating and tampering information. Examples of such crimes include creating forged International Mobile Equipment Identity (IMEI) numbers which are embedded on mobile packages and inside smart mobile cases for illicit activities. Another example of such crimes is altering the name or date on air tickets for breaching security at the airport. This paper presents a new expert system for detecting forged IMEI numbers as well as altered air ticket images. The proposed method derives the phase spectrum using the Discrete Cosine Transform (DCT) to highlight the suspicious regions; it is unlike the phase spectrum from a Fourier transform, which is ineffective due to power spectrum noise. From the phase spectrum, our method extracts phase statistics to study the effect of distortions introduced by forgery operations. This results in feature vectors, which are fed to a Support Vector Machine (SVM) classifier for detection of forged IMEI numbers and air ticket images. Experimental results on our dataset of forged IMEI numbers (which is created by us for this work), on altered air tickets, on benchmark datasets of video caption text (which is tampered text), and on altered receipts of the ICPR 2018 FDC dataset, show that the proposed method is robust across different datasets. Furthermore, comparative studies of the proposed method with the existing methods on the same datasets show that the proposed method outperforms the existing methods. The dataset created will be available freely on request to the authors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307879",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Classifier (UML)",
      "Computer science",
      "Computer security",
      "Data mining",
      "Discrete cosine transform",
      "Feature vector",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Support vector machine",
      "Ticket"
    ],
    "authors": [
      {
        "surname": "Nandanwar",
        "given_name": "Lokesh"
      },
      {
        "surname": "Shivakumara",
        "given_name": "Palaiahnakote"
      },
      {
        "surname": "Kanchan",
        "given_name": "Swati"
      },
      {
        "surname": "Basavaraja",
        "given_name": "V."
      },
      {
        "surname": "Guru",
        "given_name": "D.S."
      },
      {
        "surname": "Pal",
        "given_name": "Umapada"
      },
      {
        "surname": "Lu",
        "given_name": "Tong"
      },
      {
        "surname": "Blumenstein",
        "given_name": "Michael"
      }
    ]
  },
  {
    "title": "Extreme point bias compensation: A similarity method of functional clustering and its application to the stock market",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113949",
    "abstract": "Functional clustering is based on functional similarity measures that are adapted to functional data. However, the existing functional similarity measures account either for the position (value) or temporal deviation (bias) of extreme points of the functional curves. This may lead to erroneous conclusions on the similarities of the curves. In this case, most functional clustering measures underperform in (for example) the analysis of stock market data. To address this methodological limitation, a new similarity measure that is based on extreme point bias compensation is proposed in this paper. By penalizing the curves with the temporal deviation of extreme points and rewarding the curves that are close to each other, the new similarity measure better reflects the shape of the curve. In addition, the proposed method overcomes the difficulty of unifying the dimensions of the horizontal and vertical axes (i.e., time and function value) when calculating the distance between two adjacent extreme points. Finally, an empirical example of stock return analysis verifies the validity of this new measure.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307363",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Econometrics",
      "Extreme point",
      "Extreme value theory",
      "Functional data analysis",
      "Horse",
      "Image (mathematics)",
      "Mathematics",
      "Measure (data warehouse)",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Similarity (geometry)",
      "Similarity measure",
      "Statistics",
      "Stock market"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Lirong"
      },
      {
        "surname": "Wang",
        "given_name": "Kaili"
      },
      {
        "surname": "Balezentis",
        "given_name": "Tomas"
      },
      {
        "surname": "Streimikiene",
        "given_name": "Dalia"
      },
      {
        "surname": "Zhang",
        "given_name": "Chonghui"
      }
    ]
  },
  {
    "title": "Dynamic Salp swarm algorithm for feature selection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113873",
    "abstract": "Recently, many optimization algorithms have been applied for Feature selection (FS) problems and show a clear outperformance in comparison with traditional FS methods. Therefore, this has motivated our study to apply the new Salp swarm algorithm (SSA) on the FS problem. However, SSA, like other optimizations algorithms, suffer from the problem of population diversity and fall into local optima. To solve these problems, this study presents an enhanced version of SSA which is known as the Dynamic Salp swarm algorithm (DSSA). Two main improvements were included in SSA to solve its problems. The first improvement includes the development of a new equation for salps’ position update. The use of this new equation is controlled by using Singer's chaotic map. The purpose of the first improvement is to enhance SSA solutions' diversity. The second improvement includes the development of a new local search algorithm (LSA) to improve SSA exploitation. The proposed DSSA was combined with the K-nearest neighbor (KNN) classifier in a wrapper mode. 20 benchmark datasets were selected from the UCI repository and 3 Hadith datasets to test and evaluate the effectiveness of the proposed DSSA algorithm. The DSSA results were compared with the original SSA and four well-known optimization algorithms including Particle Swarm Optimization (PSO), Genetic Algorithm (GA), Ant Lion Optimizer (ALO), and Grasshopper Optimization Algorithm (GOA). From the obtained results, DSSA outperformed the original SSA and the other well-known optimization algorithms over the 23 datasets in terms of classification accuracy, fitness function values, the number of selected features, and convergence speed. Also, DSSA accuracy results were compared with the most recent variants of the SSA algorithm. DSSA showed a significant improvement over the competing algorithms in statistical analysis. These results confirm the capability of the proposed DSSA to simultaneously improve the classification accuracy while selecting the minimal number of the most informative features.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306801",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Feature (linguistics)",
      "Feature selection",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Selection (genetic algorithm)",
      "Swarm behaviour"
    ],
    "authors": [
      {
        "surname": "Tubishat",
        "given_name": "Mohammad"
      },
      {
        "surname": "Ja'afar",
        "given_name": "Salinah"
      },
      {
        "surname": "Alswaitti",
        "given_name": "Mohammed"
      },
      {
        "surname": "Mirjalili",
        "given_name": "Seyedali"
      },
      {
        "surname": "Idris",
        "given_name": "Norisma"
      },
      {
        "surname": "Ismail",
        "given_name": "Maizatul Akmar"
      },
      {
        "surname": "Omar",
        "given_name": "Mardian Shah"
      }
    ]
  },
  {
    "title": "GEP-based classifier for mining imbalanced data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114058",
    "abstract": "The paper proposes an incremental Gene Expression Programming classifier for mining imbalanced datasets. Imbalanced datasets are commonly encountered in real-life applications. There exist numerous algorithms, techniques, and tools which are proposed as suitable for dealing with imbalanced class distribution. Yet, none of them seems to be able to outperform all others in all possible applications. We believe that our approach can extend the available range of learners that have proven good performance in mining imbalanced data and imbalanced streams. The idea is to adapt the GEP classifier to requirements of the imbalanced data environment with reuse of the minority class instances, and application of the incremental learning paradigm. The paper offers an overview of the related work and a detailed description of the proposed incremental learner. An extensive computational experiment, based on data from the KEEL dataset repository, proves that in numerous cases the approach is competitive to other state-of-the-art learners.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308204",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Data stream mining",
      "Ecology",
      "Machine learning",
      "Reuse"
    ],
    "authors": [
      {
        "surname": "Jedrzejowicz",
        "given_name": "Joanna"
      },
      {
        "surname": "Jedrzejowicz",
        "given_name": "Piotr"
      }
    ]
  },
  {
    "title": "Deep learning approaches for COVID-19 detection based on chest X-ray images",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114054",
    "abstract": "COVID-19 is a novel virus that causes infection in both the upper respiratory tract and the lungs. The numbers of cases and deaths have increased on a daily basis on the scale of a global pandemic. Chest X-ray images have proven useful for monitoring various lung diseases and have recently been used to monitor the COVID-19 disease. In this paper, deep-learning-based approaches, namely deep feature extraction, fine-tuning of pretrained convolutional neural networks (CNN), and end-to-end training of a developed CNN model, have been used in order to classify COVID-19 and normal (healthy) chest X-ray images. For deep feature extraction, pretrained deep CNN models (ResNet18, ResNet50, ResNet101, VGG16, and VGG19) were used. For classification of the deep features, the Support Vector Machines (SVM) classifier was used with various kernel functions, namely Linear, Quadratic, Cubic, and Gaussian. The aforementioned pretrained deep CNN models were also used for the fine-tuning procedure. A new CNN model is proposed in this study with end-to-end training. A dataset containing 180 COVID-19 and 200 normal (healthy) chest X-ray images was used in the study’s experimentation. Classification accuracy was used as the performance measurement of the study. The experimental works reveal that deep learning shows potential in the detection of COVID-19 based on chest X-ray images. The deep features extracted from the ResNet50 model and SVM classifier with the Linear kernel function produced a 94.7% accuracy score, which was the highest among all the obtained results. The achievement of the fine-tuned ResNet50 model was found to be 92.6%, whilst end-to-end training of the developed CNN model produced a 91.6% result. Various local texture descriptors and SVM classifications were also used for performance comparison with alternative deep approaches; the results of which showed the deep approaches to be quite efficient when compared to the local texture descriptors in the detection of COVID-19 based on chest X-ray images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308198",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Combinatorics",
      "Computer science",
      "Convolutional neural network",
      "Coronavirus disease 2019 (COVID-19)",
      "Deep learning",
      "Disease",
      "Feature extraction",
      "Gaussian",
      "Gaussian function",
      "Infectious disease (medical specialty)",
      "Kernel (algebra)",
      "Machine learning",
      "Mathematics",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Ismael",
        "given_name": "Aras M."
      },
      {
        "surname": "Şengür",
        "given_name": "Abdulkadir"
      }
    ]
  },
  {
    "title": "Point of interest recommendations based on the anchoring effect in location-based social network services",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114018",
    "abstract": "A point of interest (POI) recommender system (RS) is one of the representative research areas based on the location-based social network (LBSN). Most POI RS studies utilized various implicit information or social information to improve recommendation accuracy. However, majority of these studies overlooked the importance of users’ initial check-in information. Users are affected by their first input data in online services, and this phenomenon is called the anchoring effect. In POI RSs, few studies have analyzed the association with the anchoring effect while other RS domains already verified this effect. In particular, a research area, including POI RS, that focuses on the importance of the initial input does not exist. In this paper, we propose a latent Dirichlet allocation (LDA) model based on the anchoring effect for POI RS. This model emphasizes the importance of initial check-in data and is called the anchor-LDA. Experimental results showed that the anchor-LDA outperformed existing LDA-based POI recommender algorithms. Furthermore, we validated the importance of initial check-in information on the LBSN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307909",
    "keywords": [
      "Anchoring",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Engineering",
      "Geometry",
      "Information retrieval",
      "Latent Dirichlet allocation",
      "Mathematics",
      "Point (geometry)",
      "Point of interest",
      "RSS",
      "Recommender system",
      "Social media",
      "Social network (sociolinguistics)",
      "Structural engineering",
      "Topic model",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Seo",
        "given_name": "Young-Duk"
      },
      {
        "surname": "Cho",
        "given_name": "Yoon-Sik"
      }
    ]
  },
  {
    "title": "Knowledge graph enhanced neural collaborative recommendation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113992",
    "abstract": "Existing neural collaborative filtering (NCF) recommendation methods suffer from severe sparsity problem. Knowledge Graph (KG), which commonly consists of fruitful connected facts about items, presents an unprecedented opportunity to alleviate the sparsity problem. However, pure NCF models can hardly model the high-order connectivity in KG, and ignores complex pairwise correlations between user/item embedding dimensions. To address these problems, we propose a novel Knowledge graph enhanced Neural Collaborative Recommendation (K-NCR) framework, which effectively combines user–item interaction information and auxiliary knowledge information for recommendation task into three parts: (1) For items, the proposed propagating model learns the representation of item entity. It recursively aggregates information from its multi-hop neighbours in KG, and employs an attention mechanism to discriminate the importance of the relation type to mine users’ potential preferences. (2) For users, another heterogeneous attention weights are leveraged to strengthen the embedding learning of users. (3) The user and item embeddings are then fed into a newly designed two-dimensional interaction map with convolutional hidden layers to model the complex pairwise correlations between their embedding dimensions explicitly. Extensive experimental results on three benchmark datasets demonstrate the effectiveness of our K-NCR framework.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307685",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Graph",
      "Information retrieval",
      "Knowledge graph",
      "Machine learning",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Sang",
        "given_name": "Lei"
      },
      {
        "surname": "Xu",
        "given_name": "Min"
      },
      {
        "surname": "Qian",
        "given_name": "Shengsheng"
      },
      {
        "surname": "Wu",
        "given_name": "Xindong"
      }
    ]
  },
  {
    "title": "Pythagorean fuzzy linear programming technique for multidimensional analysis of preference using a squared-distance-based approach for multiple criteria decision analysis",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113908",
    "abstract": "Pythagorean fuzzy (PF) sets involving Pythagorean membership grades can befittingly manipulate inexact and equivocal information in real-life problems involving multiple criteria decision analysis (MCDA). The linear programming technique for multidimensional analysis of preference (LINMAP) is a prototypical compromising model, and it is widely used to carry on decision-making problems in many down-to-earth applications. In LINMAP, the employment of squares of Euclidean distances is a significant technique that is an effective approach to fit measurements. Taking the advantages of a newly developed Euclidean distance model on the grounds of PF sets, this paper initiates a beneficial concept of squared PF Euclidean distances and studies its valuable and desirable properties. This paper aims to establish a squared Euclidean distance (SED)-based outranking approach and develop a novel PF LINMAP methodology for handling an MCDA problem under PF uncertainty. In the SED-based outranking approach, a novel SED-based dominance index is proposed to reflect an overall balance of a PF evaluative rating between the connection and remotest connection with positive- and negative-ideal ratings, respectively. The properties of the proposed index are also analyzed to exhibit its efficaciousness in determining the dominance relations for intracriterion comparisons. Moreover, this paper derives the comprehensive dominance index to determine the overall dominance relation and defines measurements of rank consistency for goodness of fit and rank inconsistency for poorness of fit. The PF LINMAP model is formulated to seek to ascertain the optimal weight vector that maximizes the total comprehensive dominance index and minimizes the poorness of fit under consideration of the lowest acceptable level and specialized degenerate weighting issues. The practical application concerning bridge-superstructure construction methods is conducted to test the feasibility and practicability of the PF LINMAP model. Over and above that, a generalization of the proposed methodology, along with applications to green supplier selection and railway project investment, is investigated to deal with group decision-making issues. Several comparative studies are implemented to further validate its usefulness and advantages. The application and comparison results display the effectuality and flexibility of the developed PF LINMAP methodology. In the end, the directions for future research of this work are represented in the conclusion.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030703X",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Euclidean distance",
      "Euclidean geometry",
      "Fuzzy logic",
      "Geometry",
      "Mathematical optimization",
      "Mathematics",
      "Multiple-criteria decision analysis",
      "Pythagorean theorem",
      "Rank (graph theory)",
      "Ranking (information retrieval)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Ting-Yu"
      }
    ]
  },
  {
    "title": "Learning latent representations of bank customers with the Variational Autoencoder",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114020",
    "abstract": "Learning data representations that reflect the customers’ creditworthiness can improve marketing campaigns, customer relationship management, data and process management or the credit risk assessment in retail banks. In this research, we show that it is possible to steer data representations in the latent space of the Variational Autoencoder (VAE) using a semi-supervised learning framework and a specific grouping of the input data called Weight of Evidence (WoE). Our proposed method learns a latent representation of the data showing a well-defied clustering structure. The clustering structure captures the customers’ creditworthiness, which is unknown a priori and cannot be identified in the input space. The main advantages of our proposed method are that it captures the natural clustering of the data, suggests the number of clusters, captures the spatial coherence of customers’ creditworthiness, generates data representations of unseen customers and assign them to one of the existing clusters. Our empirical results, based on real data sets reflecting different market and economic conditions, show that none of the well-known data representation models in the benchmark analysis are able to obtain well-defined clustering structures like our proposed method. Further, we show how banks can use our proposed methodology to improve marketing campaigns and credit risk assessment.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307910",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Business",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Data science",
      "Deep learning",
      "Feature learning",
      "Latent variable",
      "Law",
      "Machine learning",
      "Marketing",
      "Operating system",
      "Political science",
      "Politics",
      "Process (computing)",
      "Representation (politics)",
      "Space (punctuation)"
    ],
    "authors": [
      {
        "surname": "Mancisidor",
        "given_name": "Rogelio A."
      },
      {
        "surname": "Kampffmeyer",
        "given_name": "Michael"
      },
      {
        "surname": "Aas",
        "given_name": "Kjersti"
      },
      {
        "surname": "Jenssen",
        "given_name": "Robert"
      }
    ]
  },
  {
    "title": "Recognizing activities of daily living from UWB radars and deep learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113994",
    "abstract": "Since years, the number of seniors increases while, at the same time, we observe a diminution of the potential support ratio. In order to overcome this limitation, solutions emerged, such as smart homes and wearable devices. Smart homes integrate sensors, actuators, and artificial intelligence to assist seniors in their everyday life. One of the objectives is to recognize the activities of everyday life. This recognition aims to provide the right assistance at the right moment and gives some autonomy to seniors. However, it is a complex task (a significant quantity of different sensors, hardware implementation), and the number of solutions (combinations between approaches, for example, video-based HAR and wearable sensors-based HAR) that exist is important. In this paper, we propose to perform the activity recognition from three ultra-wideband (UWB) radars, deep learning models, and a voting system. Also, all the experiments have been conducted in a real apartment and are composed of 15 different activities. The presented solution is simple compared to the literature since we exploit only one type of sensor. Finally, we obtained promising results with our approach. Indeed, the classification rate reaches 90% and more in some cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307703",
    "keywords": [
      "Activities of daily living",
      "Activity recognition",
      "Artificial intelligence",
      "Assisted living",
      "Autonomy",
      "Computer science",
      "Computer security",
      "Deep learning",
      "Embedded system",
      "Engineering",
      "Everyday life",
      "Exploit",
      "Human–computer interaction",
      "Law",
      "Machine learning",
      "Medicine",
      "Nursing",
      "Political science",
      "Psychiatry",
      "Psychology",
      "Real-time computing",
      "Systems engineering",
      "Task (project management)",
      "Wearable computer"
    ],
    "authors": [
      {
        "surname": "Maitre",
        "given_name": "Julien"
      },
      {
        "surname": "Bouchard",
        "given_name": "Kévin"
      },
      {
        "surname": "Bertuglia",
        "given_name": "Camille"
      },
      {
        "surname": "Gaboury",
        "given_name": "Sébastien"
      }
    ]
  },
  {
    "title": "An efficient algorithm for unique class association rule mining",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113978",
    "abstract": "Association rule mining is one of the main means in Knowledge discovery and Machine learning. Such kind of rules present knowledge of interrelations among items in a dataset. Class Association Rules (CARs) are a subset of association rules which are always mined using labeled datasets. Simply, a typical CAR has an itemset that is associated to a class label. Mining CARs is vital for construction of pattern or rule-based classification models and has received recently increasing research interest. In this work, a complete efficient but not exhaustive CAR mining algorithm (UniqAR) is introduced. UniqAR generates always and only 100 % accurate CARs which are called unique association rules using two rule search hypothesis of Subsumption and Nonsense to find unique itemsets in order to generate the Unique CARs. Unlike alternatives of CAR mining algorithms, UniqAR mined association rules aren’t based on itemset frequency or item selectivity. It can generate both frequent and rare association rules. No preferences of support, coverage, or item participant in itemsets are required to be provided for the proposed mining process. The main contribution of this work to CARs’ state of the art is describing unique itemsets and class association rules and providing an efficient mining process for them. Unlike the other unique rule mining alternatives in the literature, the proposed novel mining process depends on a complete but not exhaustive search that employs rules inter-relations. UniqAR has been modeled with computational analysis and extended evaluation. It is shown that UniqAR can extract all unique itemsets for unique association mining with no need to setup any user preferences, template or any constraints. Moreover, it describes accurately the effects of different dataset criteria like number of attributes/features, feature values, cases, and class labels on UniqAR unique itemset extraction mining process in an efficient way that avoids a huge number of itemsets/cases comparisons. Results have shown that the proposed UniqAR algorithm is feasible and promising.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307569",
    "keywords": [
      "Artificial intelligence",
      "Association (psychology)",
      "Association rule learning",
      "Class (philosophy)",
      "Computer science",
      "Data mining",
      "Epistemology",
      "K-optimal pattern discovery",
      "Knowledge extraction",
      "Machine learning",
      "Operating system",
      "Philosophy",
      "Process (computing)"
    ],
    "authors": [
      {
        "surname": "Nasr",
        "given_name": "Mahmoud"
      },
      {
        "surname": "Hamdy",
        "given_name": "Mohamed"
      },
      {
        "surname": "Hegazy",
        "given_name": "Doaa"
      },
      {
        "surname": "Bahnasy",
        "given_name": "Khaled"
      }
    ]
  },
  {
    "title": "An improved ant colony optimization with an automatic updating mechanism for constraint satisfaction problems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114021",
    "abstract": "Constraint satisfaction problem (CSP) is defined as a set of variables whose values need to satisfies a set of constraints. Ant colony optimization (ACO) has been proved to be a promising algorithm for solving the CSP, but the solution quality and convergence speed of existing ACO-based algorithms are not satisfactory. To overcome these drawbacks, this paper proposes an improved ant colony optimization with an automatic updating mechanism (AU-ACO). The idea of the automatic updating mechanism is to optimize an assignment without giving up the excellent variable-value pairs of the assignment. Under the impact of this mechanism, AU-ACO can only optimize the non-excellent variable-value pairs of a selected assignment, which results in the algorithm having a greater chance of finding better solutions. Furthermore, by optimizing only some variable-value pairs rather than all variable-value pairs, the convergence speed of the proposed algorithm is improved. AU-ACO is compared with eight other state-of-the-art algorithms on a wide range of binary problems, and experimental results demonstrate that AU-ACO a more effective and efficient for solving the CSP.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307922",
    "keywords": [
      "Algorithm",
      "Ant colony",
      "Ant colony optimization algorithms",
      "Artificial intelligence",
      "Computer science",
      "Constraint (computer-aided design)",
      "Constraint satisfaction problem",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Geometry",
      "Machine learning",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Probabilistic logic",
      "Programming language",
      "Set (abstract data type)",
      "Value (mathematics)",
      "Variable (mathematics)"
    ],
    "authors": [
      {
        "surname": "Guan",
        "given_name": "Boxin"
      },
      {
        "surname": "Zhao",
        "given_name": "Yuhai"
      },
      {
        "surname": "Li",
        "given_name": "Yuan"
      }
    ]
  },
  {
    "title": "Multi-objective periodic cash transportation problem with path dissimilarity and arrival time variation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114015",
    "abstract": "This paper introduces a multi-objective periodic routing problem in the context of cash transportation, which attempts to increase security by generating unpredictable alternative paths and spreading arrival times at each demand node. The current study covers the shortcomings of previous models on dissimilar routing and cash transportation problems from several aspects. The studied problem has three objectives, including completion times, risk of robbery, and customers’ satisfaction level considering the effects of traffic congestion as a daily phenomenon. On top of these, we extend the studied routing problem in multigraph setting, which can keep a set of efficient paths with multiple attributes (e.g., risk, time). Such representation enables us to evoke dissimilar route plans not only by reordering the sequence of nodes but also by employing alternative links even in a fix sequence of nodes. To handle the computational challenges arising from these properties, we propose a new evolutionary algorithm based on NSGA-II. The proposed method is embedded with a fuzzy logic technique to guide the applied operators and benefits from caching memory to accelerate and diversify the searching process. The results of implementing the proposed algorithm on test instances confirm the effectiveness of our method in compression to standard NSGA-II. In addition, our performed sensitivity analyses show that the multigraph setting can substantially improve the quality of solutions with respect to all studied objectives.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307880",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer network",
      "Computer science",
      "Context (archaeology)",
      "Engineering",
      "Fuzzy logic",
      "Genetics",
      "Graph",
      "Mathematical optimization",
      "Mathematics",
      "Multigraph",
      "Node (physics)",
      "Paleontology",
      "Path (computing)",
      "Programming language",
      "Routing (electronic design automation)",
      "Sequence (biology)",
      "Set (abstract data type)",
      "Structural engineering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Tikani",
        "given_name": "Hamid"
      },
      {
        "surname": "Setak",
        "given_name": "Mostafa"
      },
      {
        "surname": "Demir",
        "given_name": "Emrah"
      }
    ]
  },
  {
    "title": "Multi-DQN: An ensemble of Deep Q-learning agents for stock market forecasting",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113820",
    "abstract": "The stock market forecasting is one of the most challenging application of machine learning, as its historical data are naturally noisy and unstable. Most of the successful approaches act in a supervised manner, labeling training data as being of positive or negative moments of the market. However, training machine learning classifiers in such a way may suffer from over-fitting, since the market behavior depends on several external factors like other markets trends, political events, etc. In this paper, we aim at minimizing such problems by proposing an ensemble of reinforcement learning approaches which do not use annotations (i.e. market goes up or down) to learn, but rather learn how to maximize a return function over the training stage. In order to achieve this goal, we exploit a Q-learning agent trained several times with the same training data and investigate its ensemble behavior in important real-world stock markets. Experimental results in intraday trading indicate better performance than the conventional Buy-and-Hold strategy, which still behaves well in our setups. We also discuss qualitative and quantitative analyses of these results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306321",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Computer security",
      "Econometrics",
      "Economics",
      "Engineering",
      "Ensemble learning",
      "Exploit",
      "Horse",
      "Machine learning",
      "Mechanical engineering",
      "Paleontology",
      "Reinforcement learning",
      "Stock (firearms)",
      "Stock market",
      "Supervised learning",
      "Trading strategy",
      "Training set"
    ],
    "authors": [
      {
        "surname": "Carta",
        "given_name": "Salvatore"
      },
      {
        "surname": "Ferreira",
        "given_name": "Anselmo"
      },
      {
        "surname": "Podda",
        "given_name": "Alessandro Sebastian"
      },
      {
        "surname": "Reforgiato Recupero",
        "given_name": "Diego"
      },
      {
        "surname": "Sanna",
        "given_name": "Antonio"
      }
    ]
  },
  {
    "title": "CDBH: A clustering and density-based hybrid approach for imbalanced data classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114035",
    "abstract": "The problem of imbalanced data set classification is prevalent in the studies of machine learning and data mining. In these kinds of data sets, the number of samples in classes is unequal so that one class has a lot more samples (the majority or negative class) than the other (the minority or positive class). The classical classifiers are ineffective in these conditions because they are biased toward the majority class and ignore the minority class, which is more important. Preprocessing the data distribution before training the classifier is one of the most effective methods to resolve this problem. These methods, balance the data distribution by decreasing the majority class size (under-sampling methods) or increasing the minority class size (over-sampling methods) or combining both of them (hybrid methods). In this paper, we propose an effective and simple hybrid approach based on the density concept and clustering, which is called Clustering and Density-Based Hybrid (CDBH). First, the minority class samples are clustered by the well-known k-means algorithm and their densities in each cluster are obtained. Then, the denser minority samples are selected with more likely to generate the new minority samples. To decrease the majority class size, the k-means algorithm is applied again on the majority class samples to cluster them and compute their densities, like the previous stage. Finally, the denser majority samples will have more chance to choose from the training set, and other samples are removed to balance the data samples distribution between classes. In the experiments, the Support Vector Machine (SVM) classifier is used as the classifier, and F-measure and AUC criteria are employed for evaluation. Also, preprocessing methods are compared in terms of the complexity of the classification model and the over-sampling rate. The results of comparing CDBH and other state of the art methods over 44 imbalanced data sets show the superiority of the proposed CDBH method based on the F-measure criterion.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308058",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Classifier (UML)",
      "Cluster analysis",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Data pre-processing",
      "Data set",
      "Filter (signal processing)",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Preprocessor",
      "Sampling (signal processing)"
    ],
    "authors": [
      {
        "surname": "Mirzaei",
        "given_name": "Behzad"
      },
      {
        "surname": "Nikpour",
        "given_name": "Bahareh"
      },
      {
        "surname": "Nezamabadi-pour",
        "given_name": "Hossein"
      }
    ]
  },
  {
    "title": "Predicting stock movements based on financial news with segmentation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113988",
    "abstract": "With the development of machine learning technologies, predicting stock movements by analyzing news articles has been studied actively. Most of the existing studies utilize only the datasets of target companies, and some studies use datasets of the relevant companies in the Global Industry Classification Standard (GICS) sectors. However, we show that GICS has a limitation in finding relevance regarding stock prediction because heterogeneity exists in the GICS sectors. To solve this limitation, we suggest a methodology that reflects heterogeneity and searches for homogeneous groups of companies which have high relevance. Stock price movements are predicted using the K-means clustering and multiple kernel learning technique which integrates information from the target company and its homogeneous cluster. We experiment using three-year data from the Republic of Korea and compare the results of the proposed method with those of existing methods. The results show that the proposed method shows higher predictability than existing methods in the majority of cases. The results also imply that the necessity of cluster analysis depends on the heterogeneity in the sector, and it is essential to perform cluster analysis with a larger number of clusters as the heterogeneity increases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030765X",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Biology",
      "Cluster (spacecraft)",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Econometrics",
      "Economics",
      "Geography",
      "Homogeneous",
      "Law",
      "Machine learning",
      "Mathematics",
      "Paleontology",
      "Political science",
      "Predictability",
      "Programming language",
      "Relevance (law)",
      "Segmentation",
      "Series (stratigraphy)",
      "Statistics",
      "Stock (firearms)",
      "Stock price"
    ],
    "authors": [
      {
        "surname": "Seong",
        "given_name": "Nohyoon"
      },
      {
        "surname": "Nam",
        "given_name": "Kihwan"
      }
    ]
  },
  {
    "title": "A k-NN method for lung cancer prognosis with the use of a genetic algorithm for feature selection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113981",
    "abstract": "Lung cancer is one of the most common diseases for human beings everywhere throughout the world. Early identification of this disease is the main conceivable approach to enhance the possibility of patients’ survival. In this paper, a k-Nearest-Neighbors technique, for which a genetic algorithm is applied for the efficient feature selection to reduce the dataset dimensions and enhance the classifier pace, is employed for diagnosing the stage of patients’ disease. To improve the accuracy of the proposed algorithm, the best value for k is determined using an experimental procedure. The implementation of the proposed approach on a lung cancer database reveals 100% accuracy. This implies that one could use the algorithm to find a correlation between the clinical information and data mining techniques to support lung cancer staging diagnosis efficiently.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307594",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Feature selection",
      "Genetic algorithm",
      "Geodesy",
      "Geography",
      "Lung cancer",
      "Machine learning",
      "Medicine",
      "Pace",
      "Pathology",
      "Pattern recognition (psychology)",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Maleki",
        "given_name": "Negar"
      },
      {
        "surname": "Zeinali",
        "given_name": "Yasser"
      },
      {
        "surname": "Niaki",
        "given_name": "Seyed Taghi Akhavan"
      }
    ]
  },
  {
    "title": "Network analytics and machine learning for predictive risk modelling of cardiovascular disease in patients with type 2 diabetes",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113918",
    "abstract": "A high proportion of older adults with type 2 diabetes (T2D) often develop cardiovascular diseases (CVD). Diagnosis and regular monitoring of their multimorbidity is clinically and economically resource intensive. The interconnectedness of their health data and disease progression pathways can potentially reveal the multimorbidity risk if carefully analysed by data mining and network analysis techniques. This study proposed a risk prediction model utilising administrative data that uses network-based features and machine learning techniques to assess the risk of CVD in T2D patients. For this, two cohorts (i.e., patients with both T2D and CVD and patients with only T2D) were identified from an administrative dataset collected from the private healthcare funds based in Australia. Two baseline disease networks were generated from two study cohorts. A final disease network was then generated from two baseline disease networks through normalisation. This study extracted some social network-based features (i.e., the prevalence of comorbidities, transition patterns and clustering membership) from the final disease network and some demographic characteristics directly from the dataset. These risk factors were then used to develop six machine learning prediction models to assess the risk of CVD in patients with T2D. The classifiers accuracy ranged from 79% to 88% shows the potential of the network- and machine learning-based risk prediction model utilising administrative data. The proposed risk prediction model could be useful for medical practice as well as stakeholders to develop health management programs for patients at a high risk of developing chronic diseases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307119",
    "keywords": [
      "Artificial intelligence",
      "Baseline (sea)",
      "Big data",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Diabetes mellitus",
      "Disease",
      "Endocrinology",
      "Geology",
      "Internal medicine",
      "Machine learning",
      "Medicine",
      "Oceanography",
      "Predictive analytics",
      "Type 2 diabetes"
    ],
    "authors": [
      {
        "surname": "Hossain",
        "given_name": "Md Ekramul"
      },
      {
        "surname": "Uddin",
        "given_name": "Shahadat"
      },
      {
        "surname": "Khan",
        "given_name": "Arif"
      }
    ]
  },
  {
    "title": "Deep reinforcement learning for portfolio management of markets with a dynamic number of assets",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114002",
    "abstract": "This work proposes a novel portfolio management method using deep reinforcement learning on markets with a dynamic number of assets. This problem is especially important in cryptocurrency markets, which already support the trading of hundreds of assets with new ones being added every month. A novel neural network architecture is proposed, which is trained using deep reinforcement learning. Our architecture considers all assets in the market, and automatically adapts when new ones are suddenly introduced, making our method more general and sample-efficient than previous methods. Further, transaction cost minimization is considered when formulating the problem. For this purpose, a novel algorithm to compute optimal transactions given a desired portfolio is integrated into the architecture. The proposed method was tested on a dataset of one of the largest cryptocurrency markets in the world, outperforming state-of-the-art methods, achieving average daily returns of over 24%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307776",
    "keywords": [
      "Algorithm",
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer security",
      "Cryptocurrency",
      "Database",
      "Database transaction",
      "Deep learning",
      "Dynamic programming",
      "Economics",
      "Finance",
      "Financial market",
      "Machine learning",
      "Management",
      "Portfolio",
      "Portfolio optimization",
      "Project management",
      "Project portfolio management",
      "Reinforcement learning",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Betancourt",
        "given_name": "Carlos"
      },
      {
        "surname": "Chen",
        "given_name": "Wen-Hui"
      }
    ]
  },
  {
    "title": "Hierarchical deep multi-modal network for medical visual question answering",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113993",
    "abstract": "Visual Question Answering in Medical domain (VQA-Med) plays an important role in providing medical assistance to the end-users. These users are expected to raise either a straightforward question with a Yes/No answer or a challenging question that requires a detailed and descriptive answer. The existing techniques in VQA-Med fail to distinguish between the different question types sometimes complicates the simpler problems, or over-simplifies the complicated ones. It is certainly true that for different question types, several distinct systems can lead to confusion and discomfort for the end-users. To address this issue, we propose a hierarchical deep multi-modal network that analyzes and classifies end-user questions/queries and then incorporates a query-specific approach for answer prediction. We refer our proposed approach as Hierarchical Question Segregation based Visual Question Answering, in short HQS-VQA. Our contributions are three-fold, viz. firstly, we propose a question segregation (QS) technique for VQA-Med; secondly, we integrate the QS model to the hierarchical deep multi-modal neural network to generate proper answers to the queries related to medical images; and thirdly, we study the impact of QS in Medical-VQA by comparing the performance of the proposed model with QS and a model without QS. We evaluate the performance of our proposed model on two benchmark datasets, viz. RAD and CLEF18. Experimental results show that our proposed HQS-VQA technique outperforms the baseline models with significant margins. We also conduct a detailed quantitative and qualitative analysis of the obtained results and discover potential causes of errors and their solutions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307697",
    "keywords": [],
    "authors": [
      {
        "surname": "Gupta",
        "given_name": "Deepak"
      },
      {
        "surname": "Suman",
        "given_name": "Swati"
      },
      {
        "surname": "Ekbal",
        "given_name": "Asif"
      }
    ]
  },
  {
    "title": "An improved ant colony optimization with an automatic updating mechanism for constraint satisfaction problems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114021",
    "abstract": "Constraint satisfaction problem (CSP) is defined as a set of variables whose values need to satisfies a set of constraints. Ant colony optimization (ACO) has been proved to be a promising algorithm for solving the CSP, but the solution quality and convergence speed of existing ACO-based algorithms are not satisfactory. To overcome these drawbacks, this paper proposes an improved ant colony optimization with an automatic updating mechanism (AU-ACO). The idea of the automatic updating mechanism is to optimize an assignment without giving up the excellent variable-value pairs of the assignment. Under the impact of this mechanism, AU-ACO can only optimize the non-excellent variable-value pairs of a selected assignment, which results in the algorithm having a greater chance of finding better solutions. Furthermore, by optimizing only some variable-value pairs rather than all variable-value pairs, the convergence speed of the proposed algorithm is improved. AU-ACO is compared with eight other state-of-the-art algorithms on a wide range of binary problems, and experimental results demonstrate that AU-ACO a more effective and efficient for solving the CSP.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307922",
    "keywords": [
      "Algorithm",
      "Ant colony",
      "Ant colony optimization algorithms",
      "Artificial intelligence",
      "Computer science",
      "Constraint (computer-aided design)",
      "Constraint satisfaction problem",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Geometry",
      "Machine learning",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Probabilistic logic",
      "Programming language",
      "Set (abstract data type)",
      "Value (mathematics)",
      "Variable (mathematics)"
    ],
    "authors": [
      {
        "surname": "Guan",
        "given_name": "Boxin"
      },
      {
        "surname": "Zhao",
        "given_name": "Yuhai"
      },
      {
        "surname": "Li",
        "given_name": "Yuan"
      }
    ]
  },
  {
    "title": "Fast online computation of the Q n estimator with applications to the detection of outliers in data streams",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113831",
    "abstract": "We present fqn (Fast Q n ), a novel algorithm for online computation of the Q n scale estimator. The algorithm works in the sliding window model, cleverly computing the Q n scale estimator in the current window. We thoroughly compare our algorithm for online Q n with the state of the art competing algorithm by Nunkesser et al., and show that fqn (i) is faster, requiring only O ( s ) time in the worst case where s is the length of the window (ii) its computational complexity does not depend on the input distribution and (iii) it requires less space. To the best of our knowledge, our algorithm is the first that allows online computation of the Q n scale estimator in worst case time linear in the size of the window. As an example of a possible application, besides its use as a robust measure of statistical dispersion, we show how to use the Q n estimator for fast detection of outliers in data streams. Extensive experimental results on both synthetic and real datasets confirm the validity of our approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306424",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computation",
      "Computer science",
      "Estimator",
      "Mathematics",
      "Operating system",
      "Outlier",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)",
      "Sliding window protocol",
      "Statistics",
      "Window (computing)"
    ],
    "authors": [
      {
        "surname": "Cafaro",
        "given_name": "Massimo"
      },
      {
        "surname": "Melle",
        "given_name": "Catiuscia"
      },
      {
        "surname": "Pulimeno",
        "given_name": "Marco"
      },
      {
        "surname": "Epicoco",
        "given_name": "Italo"
      }
    ]
  },
  {
    "title": "Exploiting dimensionality reduction and neural network techniques for the development of expert brain–computer interfaces",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114031",
    "abstract": "Background: Analysis and classification of extensive medical data (e.g. electroencephalography (EEG) signals) is a significant challenge to develop effective brain–computer interface (BCI) system. Therefore, it is necessary to build automated classification framework to decode different brain signals. Methods: In the present study, two-step filtering approach is utilize to achieve resilience towards cognitive and external noises. Then, empirical wavelet transform (EWT) and four data reduction techniques; principal component analysis (PCA), independent component analysis (ICA), linear discriminant analysis (LDA) and neighborhood component analysis (NCA) are first time integrated together to explore dynamic nature and pattern mining of motor imagery (MI) EEG signals. Specifically, EWT helped to explore the hidden patterns of MI tasks by decomposing EEG data into different modes where every mode was consider as a feature vector in this study and each data reduction technique have been applied to all these modes to reduce the dimension of huge feature matrix. Moreover, an automated correlation-based components/coefficients selection criteria and parameter tuning were implemented for PCA, ICA, LDA, and NCA respectively. For the comparison purposes, all the experiments were performed on two publicly available datasets (BCI competition III dataset IVa and IVb). The performance of the experiments was verified by decoding three different channel combination strategies along with several neural networks. The regularization parameter tuning of NCA guaranteed to improve classification performance with significant features for each subject. Results: The experimental results revealed that NCA provides an average sensitivity, specificity, accuracy, precision, F1 score and kappa-coefficient of 100% for subject dependent case whereas 93%, 93%, 92.9%, 93%, 96.4% and 90% for subject independent case respectively. All the results were obtained with artificial neural networks, cascade-forward neural networks and multilayer perceptron neural networks (MLP) for subject dependent case while with MLP for subject independent case by utilizing 7 channels out of total 118. Such an increase in results can alleviate users to explain more clearly their MI activities. For instance, physically impaired person will be able to manage their wheelchair quite effectively, and rehabilitated persons may be able to improve their activities.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308010",
    "keywords": [
      "Artificial intelligence",
      "Brain–computer interface",
      "Computer science",
      "Data mining",
      "Dimensionality reduction",
      "Electroencephalography",
      "Feature selection",
      "Independent component analysis",
      "Linear discriminant analysis",
      "Motor imagery",
      "Pattern recognition (psychology)",
      "Principal component analysis",
      "Psychiatry",
      "Psychology"
    ],
    "authors": [
      {
        "surname": "Sadiq",
        "given_name": "Muhammad Tariq"
      },
      {
        "surname": "Yu",
        "given_name": "Xiaojun"
      },
      {
        "surname": "Yuan",
        "given_name": "Zhaohui"
      }
    ]
  },
  {
    "title": "Clustering of large scale QoS time series data in federated clouds using improved variable Chromosome Length Genetic Algorithm (CQGA)",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113840",
    "abstract": "Service monitoring in federated clouds generates large scale QoS time series data with various unknown, frequent and abnormal patterns. This could be associated with inaccurate resource provisioning and avoid violations through predictive and preventive actions. A sufficient intelligence in the form of expert system for decision support is needed in such situations. Therefore, the main challenge here is to efficiently discover unknown frequent and abnormal patterns from QoS time series data of federated clouds. On the other hand, QoS time series data in federated clouds is unlabeled and consists of frequent and abnormal structures. Studies showed that clustering is the most common and efficient method to discover interesting patterns and structures from unlabeled data. But, clustering is normally associated with time overhead that should be optimized as well as accuracy issues mainly in connection with convergence and finding an optimum number of clusters. This work proposes a new genetic based clustering algorithm that shows better accuracy and speed in comparison to state-of-the-art methods. Furthermore, the proposed algorithm can find the optimum number of clusters concurrently with the clustering itself. Achieved accuracy and convergence of the proposed method in the experimental results assure its use in expert systems, mainly for resource provisioning and further autonomous decision making situations in federated clouds. In addition to the scientific impact of this paper, the proposed method can be used by federated cloud service providers in practice.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306497",
    "keywords": [
      "Algorithm",
      "Cloud computing",
      "Cluster analysis",
      "Computer network",
      "Computer science",
      "Convergence (economics)",
      "Data mining",
      "Distributed computing",
      "Economic growth",
      "Economics",
      "Economy",
      "Genetic algorithm",
      "Machine learning",
      "Operating system",
      "Overhead (engineering)",
      "Physics",
      "Provisioning",
      "Quality of service",
      "Quantum mechanics",
      "Resource (disambiguation)",
      "Scale (ratio)",
      "Service (business)"
    ],
    "authors": [
      {
        "surname": "Keshavarzi",
        "given_name": "Amin"
      },
      {
        "surname": "Toroghi Haghighat",
        "given_name": "Abolfazl"
      },
      {
        "surname": "Bohlouli",
        "given_name": "Mahdi"
      }
    ]
  },
  {
    "title": "Detecting the shuttlecock for a badminton robot: A YOLO based approach",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113833",
    "abstract": "The ability to identify objects of interest from digital visual signals is critical for many applications of intelligent systems. For such object detection task, accuracy and computational efficiency are two important aspects, especially for applications with real-time requirement. In this paper, we study shuttlecock detection problem of a badminton robot, which is very challenging since the shuttlecock often moves fast in complex contexts, and must be detected precisely in real time so that the robot can plan and execute its following movements. To this end, we propose two novel variants of Tiny YOLOv2, a well-known deep learning based detector. We first modify the loss function to adaptively improve the detection speed for small objects such as shuttlecock. We then modify the architecture of Tiny YOLOv2 to retain more semantic information of small objects, so as to further improve the performance. Experimental results show that the proposed networks can achieve high detection accuracy with the fastest speed, compared with state-of-the-art deep detectors such as Faster R-CNN, SSD, Tiny YOLOv2, and YOLOv3. Our methods could be potentially applied to other tasks of detecting high-speed small objects.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306436",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Detector",
      "Economics",
      "Evolutionary biology",
      "Function (biology)",
      "Management",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Robot",
      "Task (project management)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Cao",
        "given_name": "Zhiguang"
      },
      {
        "surname": "Liao",
        "given_name": "Tingbo"
      },
      {
        "surname": "Song",
        "given_name": "Wen"
      },
      {
        "surname": "Chen",
        "given_name": "Zhenghua"
      },
      {
        "surname": "Li",
        "given_name": "Chongshou"
      }
    ]
  },
  {
    "title": "Towards greener petrochemical production: Two-stage network data envelopment analysis in a fully fuzzy environment in the presence of undesirable outputs",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113903",
    "abstract": "Traditional Data Envelopment Analysis (DEA) models usually consider decision-making units (DMU) as black boxes that consume a set of inputs to produce a set of outputs and ignore the units’ internal structure. In literature, two-stage DEA models have been introduced and developed to deal with the inner workings of such DMUs; however, models in the extant literature can only be used to assess technologies with precise inputs and outputs. In this study we present an innovative two-stage DEA model for DMU evaluation in the presence of imprecise data, which are modeled using triangular fuzzy values in this study. We also innovate by including the treatment of undesirable outputs. We propose some alternative two-stage DEA network models with undesirable outputs in a full-fuzzy mode that are either directional or radial and are embedded within a multi-objective linear planning structure based on fuzzy triangular numbers. We also propose a method to solve fuzzy programming models in the presence of undesirable pollutant outputs from production processes such as greenhouse gas emissions, harmful effluents, among others. Our proposed model is then applied to a real case of 15 ammonia-manufacturing units in the Iranian petrochemical sector. The promising results that we obtained in our research and their implications for theory and practice are then presented and discussed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306990",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Data envelopment analysis",
      "Data mining",
      "Economics",
      "Engineering",
      "Envelopment",
      "Environmental engineering",
      "Fuzzy logic",
      "Fuzzy number",
      "Fuzzy set",
      "Geodesy",
      "Geography",
      "Macroeconomics",
      "Mathematical optimization",
      "Mathematics",
      "Operations research",
      "Petrochemical",
      "Production (economics)",
      "Programming language",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Mozaffari",
        "given_name": "Mohammad Reza"
      },
      {
        "surname": "Mohammadi",
        "given_name": "Samira"
      },
      {
        "surname": "Wanke",
        "given_name": "Peter F."
      },
      {
        "surname": "Correa",
        "given_name": "Henrique L."
      }
    ]
  },
  {
    "title": "Supervised discriminative dimensionality reduction by learning multiple transformation operators",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113958",
    "abstract": "Analyzing and learning from high dimensional data have always been challenging in machine learning tasks, causing serious computational complexities and poor learning performances. Supervised dimensionality reduction is a popular technique to address such challenges in supervised learning tasks, where data are accompanied with labels. Traditionally, such techniques mostly learn one single transformation to project data into a low-dimensional discriminative subspace. However, learning only one transformation for the whole data could be dominated by one or several classes, and the rest of classes receive less discrimination in the reduced space. That is to say, learning one transformation is insufficient to properly discriminate classes of data in the reduced space because they may have complex and completely dissimilar distributions. This insufficiency becomes even more serious if the number of classes increases, leading to poor discrimination and lessening the learning performance in the reduced space. To overcome this limitation, we propose a novel supervised dimensionality reduction method, which learns per-class transformations by optimizing a newly designed and efficient objective function. The proposed method captures more discriminative information from each single class of data compared to the case of one single transformation. Moreover, the proposed objective function enjoys several desirable properties: (1) maximizing margins between the transformed classes of data, (2) having a closed-form solution, (3) being easily kernelized in the case of nonlinear data,(4) preventing overfitting, and (5) ensuring the transformations are sparse in rows so that discriminative features are learned in the reduced space. Experimental results verify that the proposed method is superior to the related state-of-the-art methods and promising in generating discriminative embeddings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307417",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Class (philosophy)",
      "Classical mechanics",
      "Computer science",
      "Dimensionality reduction",
      "Discriminative model",
      "Gene",
      "Geometry",
      "Kinematics",
      "Machine learning",
      "Mathematics",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Physics",
      "Reduction (mathematics)",
      "Semi-supervised learning",
      "Subspace topology",
      "Transformation (genetics)",
      "Transformation matrix"
    ],
    "authors": [
      {
        "surname": "Rajabzadeh",
        "given_name": "Hossein"
      },
      {
        "surname": "Jahromi",
        "given_name": "Mansoor Zolghadri"
      },
      {
        "surname": "Ghodsi",
        "given_name": "Ali"
      }
    ]
  },
  {
    "title": "Ensemble Belief Rule-Based Model for complex system classification and prediction",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113952",
    "abstract": "Belief Rule-Based (BRB) model has been widely used for complex system classification and prediction. However, excessive antecedent attributes will cause the combinatorial explosion problem, which restricts the applicability of the BRB model to high-dimensional problems. In this paper, we propose an Ensemble-BRB model with the use of the bagging framework to downsize the belief rule base and avoid the combinatorial explosion problem. The kernel of the Ensemble-BRB model is to generate several weak BRBs orderly, each of which only consists of a subset of antecedent attributes. Different combination methods can be used to integrate these weak BRBs coherently for classification and prediction respectively. Four benchmark problems are tested to validate the efficiency of the proposed Ensemble-BRB model in classification, and a real case on the health index prediction of engines proves the feasibility of the Ensemble-BRB model in prediction. The results on both classification and prediction show that the Ensemble-BRB model can effectively downsize the BRB as well as reach a high modeling accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307387",
    "keywords": [
      "Antecedent (behavioral psychology)",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Combinatorial explosion",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Developmental psychology",
      "Ensemble forecasting",
      "Ensemble learning",
      "Geodesy",
      "Geography",
      "Kernel (algebra)",
      "Machine learning",
      "Mathematics",
      "Psychology"
    ],
    "authors": [
      {
        "surname": "You",
        "given_name": "Yaqian"
      },
      {
        "surname": "Sun",
        "given_name": "Jianbin"
      },
      {
        "surname": "Chen",
        "given_name": "Yu-wang"
      },
      {
        "surname": "Niu",
        "given_name": "Caiyun"
      },
      {
        "surname": "Jiang",
        "given_name": "Jiang"
      }
    ]
  },
  {
    "title": "A whale optimization system for energy-efficient container placement in data centers",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113719",
    "abstract": "The recent popularity of the container-as-a-service (CaaS) paradigm in data centers and with cloud providers increases the significance of the process of container deployment modeling in cloud environments. Modern data centers face the significant challenge of optimizing two objectives, power consumption and resource utilization. Thus, the task of initial placement has a new dimension, placing the containers on virtual machines (VMs) and placing these host VMs on physical machines (PMs) such that the power consumption is minimized and the resource utilization is maximized. From another perspective, the complexity of this problem increases when the heterogeneity of the containers, VMs and PMs, is considered. Therefore, in this paper, we address the problem of container and VM placement in CaaS environments with consideration of optimizing both power consumption and resource utilization. Existing solutions have addressed this problem by applying simple heuristics to the container placement problem and then applying a more sophisticated approach to the VM placement problem. In other words, the existing methods separate the two search spaces. In this work, we propose an algorithm based on the Whale Optimization Algorithm (WOA) to solve these two stages of placement as one optimization problem. The proposed algorithm searches for the optimal numbers of VMs and PMs in one search space. The proposed method is evaluated over different levels of heterogeneous environments against recent methods. Experimental results show the superiority of the proposed method over the methods of comparison on the suite of test environments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305431",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Cloud computing",
      "Computer network",
      "Computer science",
      "Container (type theory)",
      "Distributed computing",
      "Ecology",
      "Energy consumption",
      "Engineering",
      "Heuristic",
      "Heuristics",
      "Mathematical optimization",
      "Mathematics",
      "Mechanical engineering",
      "Operating system",
      "Resource (disambiguation)",
      "Software deployment",
      "Virtual machine"
    ],
    "authors": [
      {
        "surname": "Al-Moalmi",
        "given_name": "Ammar"
      },
      {
        "surname": "Luo",
        "given_name": "Juan"
      },
      {
        "surname": "Salah",
        "given_name": "Ahmad"
      },
      {
        "surname": "Li",
        "given_name": "Kenli"
      },
      {
        "surname": "Yin",
        "given_name": "Luxiu"
      }
    ]
  },
  {
    "title": "Multi-core sine cosine optimization: Methods and inclusive analysis",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113974",
    "abstract": "The Sine Cosine Algorithm (SCA) is a popular population-based optimization method, which has shown competitive results compared to other algorithms, and it has been utilized to tackle optimization cases in various domains. Despite popularity, the initial SCA suffers from minimalistic originality, mediocre performance, and shallow mathematical model. In fact, there is undoubtedly room for improvement in the structure of original SCA because it may face problems of lazy convergence and inertia to local optima. To relieve these drawbacks, this paper develops a new multi-core SCA named SGLSCA, which is combined with three strategies based on the patterns of Salp Swarm Algorithm (SSA), Grey Wolf Optimizer (GWO), and Levy flight (LF). Based on introducing the updating strategy of SSA and GWO, it is proposed to strengthen the exploration aptitude of the conventional SCA. Also, the SSA updating strategy aims to further update the population based on the best solution of SCA, while the GWO updating plan helps using the top three solutions of SCA. Also, the LF strategy is embedded to achieve the random individual walk during the history of the exploration and further augment the competence of SCA to avoid local optimal solutions. To substantiate the structure and results of the proposed multi-core SCA, which is entitled SGLSCA, it is compared against nine state-of-art algorithms, six improved SCA variants, and nine successful advanced algorithms on 34 benchmark functions selected from 23 benchmark functions and 30 IEEE CEC 2014 benchmark problems. Additionally, three practical, real-world engineering problems are considered. The final experimental results expose that the multi-core SGLSCA outperforms other optimizers including LSHADE-cnEpSin and LSHADE methods in terms of both convergence and optimality of solutions. A public repository will support this research at http://aliasgharheidari.com for future works and possible guidance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307533",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Demography",
      "Geodesy",
      "Geography",
      "Geometry",
      "Local optimum",
      "Lévy flight",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Population",
      "Random walk",
      "Sine",
      "Sociology",
      "Statistics",
      "Trigonometric functions"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Wei"
      },
      {
        "surname": "Wang",
        "given_name": "Pengjun"
      },
      {
        "surname": "Heidari",
        "given_name": "Ali Asghar"
      },
      {
        "surname": "Wang",
        "given_name": "Mingjing"
      },
      {
        "surname": "Zhao",
        "given_name": "Xuehua"
      },
      {
        "surname": "Chen",
        "given_name": "Huiling"
      }
    ]
  },
  {
    "title": "Towards efficient unconstrained handwriting recognition using Dilated Temporal Convolution Network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114004",
    "abstract": "Recognition of cursive handwritten images has advanced well with recent recurrent architectures and attention mechanism. Most of the works focus on improving transcription performance in terms of Character Error Rate (CER) and Word Error Rate (WER). Existing models are too slow to train and test networks. Furthermore, recent studies have recommended models be not only efficient in terms of task performance but also environmentally friendly in terms of model carbon footprint. Reviewing the recent state-of-the-art models, it recommends considering model training and retraining time while designing. High training time increases costs not only in terms of resources but also in carbon footprint. This becomes challenging for handwriting recognition model with popular recurrent architectures. It is truly critical since line images usually have a very long width resulting in a longer sequence to decode. In this work, we present a fully convolution based deep network architecture for cursive handwriting recognition from line level images. The architecture is a combination of 2-D convolutions and 1-D dilated non causal convolutions with Connectionist Temporal Classification (CTC) output layer. This offers a high parallelism with a smaller number of parameters. We further demonstrate experiments with various re-scaling factors of the images and how it affects the performance of the proposed model. A data augmentation pipeline is further analyzed while model training. The experiments show our model, has comparable performance on CER and WER measures with recurrent architectures. A comparison is done with state-of-the-art models with different architectures based on Recurrent Neural Networks (RNN) and its variants. The analysis shows training performance and network details of three different dataset of English and French handwriting. This shows our model has fewer parameters and takes less training and testing time, making it suitable for low-resource and environment-friendly deployment.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307788",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Cursive",
      "Deep learning",
      "Handwriting",
      "Pattern recognition (psychology)",
      "Recurrent neural network",
      "Speech recognition",
      "Word error rate"
    ],
    "authors": [
      {
        "surname": "Sharma",
        "given_name": "Annapurna"
      },
      {
        "surname": "Jayagopi",
        "given_name": "Dinesh Babu"
      }
    ]
  },
  {
    "title": "A design failure pre-alarming system using score- and vote-based associative classification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113950",
    "abstract": "Design failures often incur substantial cost overruns in the shipbuilding industry. Precautions against possible design failures facilitate on-time delivery and improved productivity. However, few studies have investigated the use of accumulated knowledge to prevent ship design failure. In addition, existing associative classification (AC) methods pay little attention to the rule consolidation process whereby discriminative association rules can be aggregated. In this study, we propose a new AC method that considers both support and confidence, while the number of matching features is taken into account not only to identify specific rules that capture useful associations, but also to enhance predictive performance by effectively aggregating relevant rules. We present an empirical case for the Korean shipbuilding industry by applying the proposed method to help reduce design failures by providing a designer with the most relevant revision history for a given design task so that unnecessary rectification can be avoided. The comparative results showed that the proposed method returns the best prediction accuracy among competing AC processes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307375",
    "keywords": [
      "Accounting",
      "Archaeology",
      "Artificial intelligence",
      "Association rule learning",
      "Associative property",
      "Business",
      "Computer science",
      "Consolidation (business)",
      "Discriminative model",
      "History",
      "Machine learning",
      "Matching (statistics)",
      "Mathematics",
      "Medicine",
      "Operating system",
      "Process (computing)",
      "Pure mathematics",
      "Risk analysis (engineering)",
      "Shipbuilding",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Park",
        "given_name": "Hee-Young"
      },
      {
        "surname": "Lim",
        "given_name": "Dong-Joon"
      }
    ]
  },
  {
    "title": "Adaptive Segmentation and Sequence Learning of human activities from skeleton data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113836",
    "abstract": "Discovering underlying patterns for predicting future actions from spatio-temporal human activity information is a fundamental component of research related to the development of expert systems in human activity recognition and assistive robotics. Current research focuses on classification or learning representations of activities for various applications. However, not much attention is given to the pattern discovery of activities which have a major role in the prediction of unseen actions. This paper proposes a novel Adaptive Segmentation and Sequence Learning (ASSL) framework which aims at segmenting unlabelled observations of human activities from extracted 3D joint information. Learning from these obtained segments provides information about the underlying patterns of activity sequences needed in predicting subsequent actions. In the proposed method, the temporal accumulated motion energy of body parts in an activity is utilised in the segmentation process to obtain key actions from unlabelled activity sequences since body parts show changes in acceleration and deceleration during an activity. Based on the segments obtained, the temporal sequence of transitions across activity segments are learned by employing a Long Short-Term Memory Recurrent Neural Network. This ASSL technique has been evaluated using both an experimental human activity dataset and a public activity dataset, and achieved a better performance when compared with other techniques including an Auto-regressive Integrated Moving Average, Support Vector Regression and Gaussian Mixture Regression Models in learning to predict patterns of activity sequences.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306461",
    "keywords": [
      "Activity recognition",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Business",
      "Computer science",
      "Genetics",
      "Machine learning",
      "Market segmentation",
      "Marketing",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Regression",
      "Segmentation",
      "Sequence (biology)",
      "Sequence learning",
      "Statistics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Adama",
        "given_name": "David Ada"
      },
      {
        "surname": "Lotfi",
        "given_name": "Ahmad"
      },
      {
        "surname": "Ranson",
        "given_name": "Robert"
      }
    ]
  },
  {
    "title": "A novel method for solving data envelopment analysis problems with weak ordinal data using robust measures",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113835",
    "abstract": "The efficiency measurement with traditional data envelopment analysis (DEA) requires precise input and output data. However, the input and output data is imprecise in many real-world problems. Various imprecise methods are developed to estimate the lower and upper bound efficiency scores in the presence of interval and weak ordinal data. We show the existing methods provide an estimation of the lower or upper bound efficiency score and cannot find their exact values in the presence of weak ordinal data. As a result, the ranking of decision making units (DMUs) may not be reliable since they are based on estimated efficiency scores. We propose a pair of DEA models to find the lower and upper bound efficiency scores. We prove the proposed models can find the exact values of the lower and upper bound efficiency scores in the presence of weak ordinal data. However, these exact values depend on a set of parameters in the presence of ordinal data. Therefore, we further propose a novel measure, fathoming the robustness of the efficiency function for the parameter space, to select the best practice DMUs in the presence of imprecise data. We present a real-world application in the space industry to demonstrate the applicability and superiority of the proposed method over the existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030645X",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Combinatorics",
      "Computer science",
      "Data envelopment analysis",
      "Data mining",
      "Data set",
      "Efficiency",
      "Estimator",
      "Gene",
      "Interval (graph theory)",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Measure (data warehouse)",
      "Ordinal data",
      "Ordinal optimization",
      "Ordinal regression",
      "Ranking (information retrieval)",
      "Robustness (evolution)",
      "Statistics",
      "Upper and lower bounds"
    ],
    "authors": [
      {
        "surname": "Ebrahimi",
        "given_name": "Bohlool"
      },
      {
        "surname": "Dellnitz",
        "given_name": "Andreas"
      },
      {
        "surname": "Kleine",
        "given_name": "Andreas"
      },
      {
        "surname": "Tavana",
        "given_name": "Madjid"
      }
    ]
  },
  {
    "title": "A survey of Twitter research: Data model, graph structure, sentiment analysis and attacks",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114006",
    "abstract": "Twitter is the third most popular worldwide Online Social Network (OSN) after Facebook and Instagram. Compared to other OSNs, it has a simple data model and a straightforward data access API. This makes it ideal for social network studies attempting to analyze the patterns of online behavior, the structure of the social graph, the sentiment towards various entities and the nature of malicious attacks in a vivid network with hundreds of millions of users. Indeed, Twitter has been established as a major research platform, utilized in more than ten thousands research articles over the last ten years. Although there are excellent review and comparison studies for most of the research that utilizes Twitter, there are limited efforts to map this research terrain as a whole. Here we present an effort to map the current research topics in Twitter focusing on three major areas: the structure and properties of the social graph, sentiment analysis and threats such as spam, bots, fake news and hate speech. We also present Twitter’s basic data model and best practices for sampling and data access. This survey also lays the ground of computational techniques used in these areas such as Graph Sampling, Natural Language Processing and Machine Learning. Along with existing reviews and comparison studies, we also discuss the key findings and the state of the art in these methods. Overall, we hope that this survey will help researchers create a clear conceptual model of Twitter and act as a guide to expand further the topics presented.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030779X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Data science",
      "Graph",
      "Information retrieval",
      "Sentiment analysis",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Antonakaki",
        "given_name": "Despoina"
      },
      {
        "surname": "Fragopoulou",
        "given_name": "Paraskevi"
      },
      {
        "surname": "Ioannidis",
        "given_name": "Sotiris"
      }
    ]
  },
  {
    "title": "A clustering based ensemble of weighted kernelized extreme learning machine for class imbalance learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114041",
    "abstract": "Most of the classification problems in real word suffer from the problem of class skewness. Various methods are required to handle the classification of problems having class skewness. Several methods have been proposed for this purpose which includes some ensemble methods. Most of such ensemble methods targets on creating balanced sub-problems like UnderBagging based kernelized Extreme Learning Machine (UBKELM), UnderBagging based Reduced Kernel Extreme Learning Machine (UBRKELM), Random Undersampling Boost (RUSBoost), EasyEnsemble and BalanceCascade. This work suggests that apart from class skewness, there are many other factors like class overlapping, length of decision boundary and the number of probability distributions present in the problem which are responsible for performance degradation of a classifier. This work proposes an ensemble method which decomposes a complex imbalanced problem into simpler sub-problems, solves these sub-problems using cost sensitive classifiers and then combines the results of each classifier using voting methods. A classification problem can have mixture distribution, the complexity of the problem increases with increase in the number of probability distributions present in it. The proposed problem decomposition method try to create less complex sub-problems by decreasing the number of distributions present in the sub-problems than the original problem. The proposed method uses a clustering evaluation algorithm to find the optimal number of sub-problems. For decomposing an imbalanced classification problem into sub-problems this work uses fuzzy clustering algorithm (FCM) which is a soft clustering algorithm, i.e. the overlapping between the sub-problems depends on the selected value of threshold (TH) parameter in the FCM algorithm. The sub-problems created in this work may or may not be balanced, so Weighted Kernelized Extreme Learning Machine (WKELM) is used to create the classifiers for these sub-problems. The final prediction of the ensemble of these classifiers is determined using soft voting and majority voting. The proposed method is evaluated on 38 benchmark binary class imbalanced datasets downloaded from KEEL dataset repository. The obtained results show that the proposed method outperforms other state of the art methods of imbalance classification. The Wilcoxon signed rank test is performed to show the significant improvement in results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308095",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Classifier (UML)",
      "Cluster analysis",
      "Computer science",
      "Decision boundary",
      "Ensemble learning",
      "Extreme learning machine",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Choudhary",
        "given_name": "Roshani"
      },
      {
        "surname": "Shukla",
        "given_name": "Sanyam"
      }
    ]
  },
  {
    "title": "Learning comprehensible and accurate hybrid trees",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113980",
    "abstract": "Finding the best classifiers according to different criteria is often performed by a multi-objective machine learning algorithm. This study considers two criteria that are usually treated as the most important when deciding which classifier to apply in practice: comprehensibility and accuracy. A model that offers a broad range of trade-offs between the two criteria is introduced because they conflict; i.e., increasing one decreases the other. The choice of the model is motivated by the fact that domain experts often formalize decisions based on knowledge that can be represented by comprehensible rules and some tacit knowledge. This approach is mimicked by a hybrid tree that consists of comprehensible parts that originate from a regular classification tree and incomprehensible parts that originate from an accurate black-box classifier. An empirical evaluation on 23 UCI datasets shows that the hybrid trees provide trade-offs between the accuracy and comprehensibility that are not possible using traditional machine learning models. A corresponding hybrid-tree comprehensibility metric is also proposed. Furthermore, the paper presents a novel algorithm for learning MAchine LeArning Classifiers with HybrId TrEes (MALACHITE), and it proves that the algorithm finds a complete set of nondominated hybrid trees with regard to their accuracy and comprehensibility. The algorithm is shown to be faster than the well-known multi-objective evolutionary optimization algorithm NSGA-II for trees with moderate size, which is a prerequisite for comprehensibility. On the other hand, the MALACHITE algorithm can generate considerably larger hybrid-trees than a naïve exhaustive search algorithm in a reasonable amount of time. In addition, an interactive iterative data mining process based on the algorithm is proposed that enables inspection of the Pareto set of hybrid trees. In each iteration, the domain expert analyzes the current set of nondominated hybrid trees, infers domain relations, and sets the parameters for the next machine learning step accordingly.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307582",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Decision tree",
      "Decision tree learning",
      "ID3 algorithm",
      "Incremental decision tree",
      "Machine learning"
    ],
    "authors": [
      {
        "surname": "Piltaver",
        "given_name": "Rok"
      },
      {
        "surname": "Luštrek",
        "given_name": "Mitja"
      },
      {
        "surname": "Džeroski",
        "given_name": "Sašo"
      },
      {
        "surname": "Gjoreski",
        "given_name": "Martin"
      },
      {
        "surname": "Gams",
        "given_name": "Matjaž"
      }
    ]
  },
  {
    "title": "An unsupervised orthogonal rotation invariant moment based fuzzy C-means approach for the segmentation of brain magnetic resonance images",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113989",
    "abstract": "Brain magnetic resonance images (MRI) suffer from many artifacts such as noise and intensity inhomogeneity. Moreover, they contain an abundant amount of fine image structures, edges, and corners in various areas of the image. These anomalies and structural complexities affect the segmentation process of the brain MRI which is required by physicians for the diagnosis purpose. Recently, we have proposed a local Zernike moment (LZM)-based unbiased nonlocal means fuzzy C-means (LZM-UNLM-FCM) approach that has dealt with the noise artifact in the moment domain using the LZM approach. The method provides high segmentation results for the MR images corrupted with Rician noise. However, the method does not deal with the intensity inhomogeneity artifact effectively. Moreover, the method uses a regularization parameter that needs to be adjusted to obtain effective segmentation results. This paper presents an unsupervised local Zernike moment and unbiased nonlocal means-based bias corrected fuzzy C-means (LZM-UNLM-BCFCM) approach that deals with both noise and intensity inhomogeneity artifacts. The main concept behind the proposed method is to use the attractive properties of the LZMs to effectively filter the image by determining a large number of similar regions in an MR image which is mostly corrupted by Rician noise and intensity inhomogeneity. The ability of the LZMs to determine such regions in MR images consisting of fine tissue structures in any orientation is well utilized for dealing with the high levels of noise. The intensity inhomogeneity is removed by estimating the bias field pixel-by-pixel during the segmentation process using the filtered image without the use of regularization parameter. The bias field is estimated as a linear combination of the orthogonal polynomials in which the weights are obtained by minimizing the fuzzy objective function. Experimental results on both simulated and real MR images show the superiority of the proposed method as compared to other unsupervised state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307661",
    "keywords": [
      "Algorithm",
      "Artifact (error)",
      "Artificial intelligence",
      "Classical mechanics",
      "Computer science",
      "Computer vision",
      "Decoding methods",
      "Fading",
      "Fuzzy logic",
      "Image (mathematics)",
      "Mathematics",
      "Moment (physics)",
      "Noise (video)",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel",
      "Rician fading",
      "Segmentation",
      "Wavefront",
      "Zernike polynomials"
    ],
    "authors": [
      {
        "surname": "Singh",
        "given_name": "Chandan"
      },
      {
        "surname": "Bala",
        "given_name": "Anu"
      }
    ]
  },
  {
    "title": "deepsing: Generating sentiment-aware visual stories using cross-modal music translation",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114059",
    "abstract": "In this paper we propose a deep learning method for performing attributed-based music-to-image translation. The proposed method is applied for synthesizing visual stories according to the sentiment expressed by songs. The generated images aim to induce the same feelings to the viewers, as the original song does, reinforcing the primary aim of music, i.e., communicating feelings. The process of music-to-image translation poses unique challenges, mainly due to the unstable mapping between the different modalities involved in this process. In this paper, we employ a trainable cross-modal translation method to overcome this limitation, leading to the first, to the best of our knowledge, deep learning method for generating sentiment-aware visual stories. The proposed method was evaluated both quantitatively and qualitatively using a collection of songs that belong to 10 different genres, demonstrating that it is indeed possible to generate visual content that can match the sentiment expressed in songs. A user study was also conducted further validating the ability of the proposed method to provide sentiment-enriched visualizations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308228",
    "keywords": [],
    "authors": [
      {
        "surname": "Passalis",
        "given_name": "Nikolaos"
      },
      {
        "surname": "Doropoulos",
        "given_name": "Stavros"
      }
    ]
  },
  {
    "title": "Twitter trends: A ranking algorithm analysis on real time data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113990",
    "abstract": "Social media has recently become popular due to its vast applications. The common people all over the world uses its diverse channels to express personal views, experiences and opinions regarding diverse topics. Social media has revolutionized the way people interact and communicate with each other and overall, it has changed the methods and approaches in about all the aspects of life such as social issue, business, education, health, etc. Thus, sales and marketing departments of multinational industries are focusing on social media trends to analyze current trends and predict future trends by analyzing user generated content on Facebook, Flickr, Twitter, etc. However, the prediction process becomes challenging as the multiplicity of factors affect the popular elements in the social media content. This research paper aims to work on Twitter trend analysis and proposes a trend detection process over streams of tweets. The proposed approach detects the trending topics of the real-time Twitter trends along with ranking the top terms and hashtags. The paper further discusses the motivation for trend prediction over the social media; In addition to exploratory data analysis, the research paper explores the Term Frequency-Inverse Document Frequency (Tf-IDF), Combined Component Approach (CCA) and Biterm Topic Model (BTM) approaches for finding the topics and terms within given topics. In modern competitive world, this research provides investors, advertisers, industries and all the stakeholders. A detailed and comprehensive data analysis which may help them to focus their investment, area of work, marketing, and product.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420307673",
    "keywords": [
      "Business",
      "Computer science",
      "Data mining",
      "Data science",
      "Exploratory data analysis",
      "Finance",
      "Information retrieval",
      "Multinational corporation",
      "Operating system",
      "Process (computing)",
      "Ranking (information retrieval)",
      "Social media",
      "Social media analytics",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Khan",
        "given_name": "Hikmat Ullah"
      },
      {
        "surname": "Nasir",
        "given_name": "Shumaila"
      },
      {
        "surname": "Nasim",
        "given_name": "Kishwar"
      },
      {
        "surname": "Shabbir",
        "given_name": "Danial"
      },
      {
        "surname": "Mahmood",
        "given_name": "Ahsan"
      }
    ]
  },
  {
    "title": "Redefining fuzzy entropy with a general framework",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113671",
    "abstract": "A general entropy framework is proposed, which could lead to intuitive, interpretable and comparable entropy functions in both probabilistic or fuzzy domain, alike. Based on the proposed general entropy framework, the existing fuzzy entropy functions are redefined. The proposed entropy functions are also extended to the probabilistic-fuzzy domain. The usefulness of the work is shown in a real world case-study.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420304954",
    "keywords": [
      "Artificial intelligence",
      "Binary entropy function",
      "Computer science",
      "Data mining",
      "Entropy (arrow of time)",
      "Fuzzy logic",
      "Joint entropy",
      "Mathematics",
      "Physics",
      "Principle of maximum entropy",
      "Probabilistic logic",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Aggarwal",
        "given_name": "Manish"
      }
    ]
  },
  {
    "title": "Novel weighted ensemble classifier for smartphone based indoor localization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113758",
    "abstract": "Indoor localization systems have the capability to change the way of providing location-based services in a closed environment. Though there is no agreed-upon technology that works best in indoor, WiFi signal is an important alternative as most of such places are covered by WiFi Access Points (APs). In this paper, the problem of indoor localization is investigated from the perspective of expert systems through applying machine learning techniques. The significant variation of WiFi signal strength with ambient conditions as well as device configuration badly affects the localization accuracy. Thus, the fingerprinting effort required to train a localization system subject to context heterogeneity is huge. The uncertainty in localization performance due to varying contexts is hardly investigated in the literature. Consequently, the main contribution of this paper is to propose a weighted ensemble classifier based on Dempster–Shafer belief theory to efficiently handle context heterogeneity. Here, the context is defined in terms of different smartphone configurations used for training and testing the system as well as temporal variation of signals. The method presented here utilizes the Dempster–Shafer theory of belief functions to calculate the weights of the base learners in the decision of the ensemble. Belief theory is applied here to handle the inherent uncertainty in WiFi signal variations due to heterogeneous context. Real life experiments are conducted for two datasets, JUIndoorLoc and UJIIndoorLoc at different granularity levels. For JUIndoorLoc, with state-of-the-art classifiers, 86–97% accuracy can be achieved for 10-fold cross-validation. However, when the training context differs from the test conditions, accuracy drops to 62–87%. In such a scenario, the proposed weighted ensemble technique is found to achieve almost 98% localization accuracy when RSSIs, mean and variance of RSSIs are considered as features. The technique can lead to an effective expert system for indoor localization at varying granularity levels. Such systems would be beneficial for pervasive indoor positioning applications as no dedicated infrastructure is needed for positioning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305820",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Classifier (UML)",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Dempster–Shafer theory",
      "Ensemble learning",
      "Granularity",
      "Machine learning",
      "Operating system",
      "Paleontology",
      "Signal strength",
      "Telecommunications",
      "Wireless"
    ],
    "authors": [
      {
        "surname": "Roy",
        "given_name": "Priya"
      },
      {
        "surname": "Chowdhury",
        "given_name": "Chandreyee"
      },
      {
        "surname": "Kundu",
        "given_name": "Mausam"
      },
      {
        "surname": "Ghosh",
        "given_name": "Dip"
      },
      {
        "surname": "Bandyopadhyay",
        "given_name": "Sanghamitra"
      }
    ]
  },
  {
    "title": "TechWord: Development of a technology lexical database for structuring textual technology information based on natural language processing",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.114042",
    "abstract": "The role of text mining based on technological documents such as patents is important in the research field of technology intelligence for technology R&D planning. In addition, WordNet, an English-based lexical database, is widely used for pre-processing text data such as word lemmatization and synonym search. However, technological vocabulary information is complex and specific, and WordNet’s ability to analyze technological information is limited in its reflecting technological features. Thus, to improve the text mining performance of technological information, this study proposes a methodology for designing a TechWord-based lexical database that is based on the lexical characteristics of technological words that are differentiated from general words. To do this, we define TechWord, a technology lexical information, and construct a TechSynset, a synonym set between TechWords. First, through dependency parsing between words, TechWord, a unit word that describes a technology, is structured and identifies nouns and verbs. The importance of connectivity is investigated by a network centrality index analysis based on the dependency relations of words. Subsequently, to search for synonyms suitable for the target technology domain, a TechSynset is constructed through synset information, with an additional analysis that calculates cosine similarity based on a word embedding vector. Applying the proposed methodology to the actual technology-related information analysis, we collect patent data on the technological fields of the automotive field, and present the results of the TechWord and TechSynset. This study improves technological information-based text mining by structuring the word-to-word link information in technological documents based on an automated process.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420308101",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Database",
      "Development (topology)",
      "Economics",
      "Finance",
      "Information retrieval",
      "Lexical database",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Structuring",
      "WordNet"
    ],
    "authors": [
      {
        "surname": "Jang",
        "given_name": "Hyejin"
      },
      {
        "surname": "Jeong",
        "given_name": "Yujin"
      },
      {
        "surname": "Yoon",
        "given_name": "Byungun"
      }
    ]
  },
  {
    "title": "The importance of temporal information in Bayesian network structure learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113814",
    "abstract": "Several algorithms have been proposed towards discovering the graphical structure of Bayesian networks. Most of these algorithms are restricted to observational data and some enable us to incorporate knowledge as constraints in terms of what can and cannot be discovered by an algorithm. A common type of such knowledge involves the temporal order of the variables in the data. For example, knowledge that event B occurs after observing A and hence, the constraint that B cannot cause A . This paper investigates real-world case studies that incorporate interesting properties of objective temporal variable order, and the impact these temporal constraints have on the learnt graph. The results show that most of the learnt graphs are subject to major modifications after incorporating incomplete temporal objective information. Because temporal information is widely viewed as a form of knowledge that is subjective, rather than as a form of data that tends to be objective, it is generally disregarded and reduced to an optional piece of information that only few of the structure learning algorithms may consider. The paper argues that objective temporal information should form part of observational data, to reduce the risk of disregarding such information when available and to encourage its reusability across related studies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306278",
    "keywords": [
      "Artificial intelligence",
      "Bayesian network",
      "Bayesian probability",
      "Computer science",
      "Constraint (computer-aided design)",
      "Data mining",
      "Geometry",
      "Graph",
      "Graphical model",
      "Machine learning",
      "Mathematics",
      "Temporal database",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Constantinou",
        "given_name": "Anthony C."
      }
    ]
  },
  {
    "title": "Incremental frequent itemsets mining based on frequent pattern tree and multi-scale",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113805",
    "abstract": "Multi-scale can reveal the structure and hierarchical characteristics of the data objects to reflect their essence from different perspectives and levels. An incremental frequent itemsets mining algorithm based on frequent pattern tree is proposed by incorporating multi-scale theory(simplified to FP-tree and Multi-Scale based Incremental Mining, FPMSIM). FPMSIM uses the classic FP-Growth to construct a pattern tree and generate frequent itemsets for more fine-grained dataset which is called benchmark scale dataset. The newly added dataset is also independently mined as a benchmark scale dataset. The ultimate frequent itemsets for the target scales are derived by means of the scale-up process. In which, some unknown itemsets counts need to be estimated by comparing the similarity among benchmark scale datasets. In this way, severe dataset rescanning and tree structure adjustment overhead are avoided during the maintenance process. The experimental results show that although the support estimation error will lead to incomplete frequent itemsets mining, it can be offset by the performance gains in the mining efficiency and I/O cost, especially in the field of big data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306217",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Binary tree",
      "Computer science",
      "Data mining",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Overhead (engineering)",
      "Pattern recognition (psychology)",
      "Physics",
      "Process (computing)",
      "Quantum mechanics",
      "Scale (ratio)",
      "Similarity (geometry)",
      "Tree (set theory)",
      "Tree structure"
    ],
    "authors": [
      {
        "surname": "Xun",
        "given_name": "Yaling"
      },
      {
        "surname": "Cui",
        "given_name": "Xiaohui"
      },
      {
        "surname": "Zhang",
        "given_name": "Jifu"
      },
      {
        "surname": "Yin",
        "given_name": "Qingxia"
      }
    ]
  },
  {
    "title": "Improvement of evolution process of dandelion algorithm with extreme learning machine for global optimization problems",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113803",
    "abstract": "Dandelion Algorithm (DA) is a novel swarm intelligent optimization algorithm. In evolutionary process of DA, the quality of the seeds generated by dandelions is uneven, and the excellent seeds are expected to be retained and evaluated, while the poor seeds should be discarded without evaluation. In order to determine whether a seed is excellent or not, an improvement of evolution process of dandelion algorithm with extreme learning machine (ELMDA) is proposed in this paper. In ELMDA, firstly, the dandelion population can be partitioned into excellent dandelions and poor dandelions based on fitness values. Then, the excellent dandelions and poor dandelions are assigned corresponding labels (i.e. +1 if excellent or -1 if poor), which can be regarded as a training set, and the training model is built based on ELM. Finally, the model is applied to classify the seeds as excellent or poor, and the excellent seeds are chosen to participate in evolution process. Meanwhile, the robustness of the proposed algorithm is analyzed in this paper. Experimental results performed on test functions show that the proposed algorithm is competitive to its peers. Moreover, the proposed algorithm is demonstrated on three engineering designed problems, and the results indicate that the proposed algorithm has better performance in solving them.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306205",
    "keywords": [
      "Algorithm",
      "Alternative medicine",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Dandelion",
      "Demography",
      "Evolutionary algorithm",
      "Gene",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Medicine",
      "Operating system",
      "Pathology",
      "Population",
      "Process (computing)",
      "Robustness (evolution)",
      "Sociology",
      "Swarm behaviour",
      "Traditional Chinese medicine"
    ],
    "authors": [
      {
        "surname": "Han",
        "given_name": "Shoufei"
      },
      {
        "surname": "Zhu",
        "given_name": "Kun"
      },
      {
        "surname": "Wang",
        "given_name": "Ran"
      }
    ]
  },
  {
    "title": "Principal component analysis based construction and evaluation of cryptocurrency index",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113796",
    "abstract": "Decentralized nature of cryptocurrencies, irrational cryptocurrency valuations and severe price volatility in cryptocurrency market makes it a formidable task for investors to pick individual coins, and rather investors would prefer to invest money on the entire cryptocurrency market accurately represented by a cryptocurrency index. This paper proposes the design of a Principal Component Analysis based methodology to construct a dynamic cryptocurrency index that accurately tracks the movement of the entire cryptocurrency market. Our analysis on real market data shows why first component derived from PCA is sufficient to construct the cryptocurrency index and how to determine the number of constituents while building the index. Proposed PCA based tool, tested on actual historical cryptocurrency data, takes into account the changing dynamics of the cryptocurrency market by regularly shifting the number of constituents as well as the weights and is able to capture the evolving pattern in the cryptocurrency market. The proposed index has been validated by three-factor pricing model, consisting of market, size and momentum factors. Subsequently, the proposed index has been compared with other existing indexes based on the results of three-factor model. In summary, the paper presents a robust mathematical model for construction of a dynamic cryptocurrency index that can be used as a tool to analyze the return on investments as well as to study the fluctuations present in the cryptocurrency market.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306151",
    "keywords": [
      "Artificial intelligence",
      "Component (thermodynamics)",
      "Computer science",
      "Computer security",
      "Cryptocurrency",
      "Econometrics",
      "Economics",
      "Index (typography)",
      "Physics",
      "Principal component analysis",
      "Thermodynamics",
      "Volatility (finance)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Shah",
        "given_name": "Agam"
      },
      {
        "surname": "Chauhan",
        "given_name": "Yagnesh"
      },
      {
        "surname": "Chaudhury",
        "given_name": "Bhaskar"
      }
    ]
  },
  {
    "title": "Auto loan fraud detection using dominance-based rough set approach versus machine learning methods",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113740",
    "abstract": "Financial fraud is escalating as financial services and operations grow. Despite preventive actions and security measures deployed to mitigate financial fraud, fraudsters are learning and finding new ways to get around fraud prevention systems, thereby, challenging quantitative techniques and predictive models. Thus, new techniques must be explored and tested so the insights obtained from the analysis may be used to support more accurate fraud prediction and the development of fraud prevention systems which have additional checks to mitigate suspicious events. Auto loan is a significant financial product not yet explored in the literature, unlike the misuse of credit cards, for instance. Given the recent increase in fraudulent transactions concerning auto loan applications, this paper tests a new data set for auto loan applications using a technique not yet explored for financial fraud prediction, namely the Dominance-based Rough Set Balanced Rule Ensemble (DRSA-BRE), and after comparing it with other techniques traditionally used for predicting financial fraud, finds that the proposed approach has several advantages over the traditional ones.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305649",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Business",
      "Chemistry",
      "Computer science",
      "Credit card",
      "Credit card fraud",
      "Dominance (genetics)",
      "Finance",
      "Financial services",
      "Gene",
      "Loan",
      "Machine learning",
      "Payment",
      "Programming language",
      "Rough set",
      "Set (abstract data type)",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Błaszczyński",
        "given_name": "Jerzy"
      },
      {
        "surname": "de Almeida Filho",
        "given_name": "Adiel T."
      },
      {
        "surname": "Matuszyk",
        "given_name": "Anna"
      },
      {
        "surname": "Szeląg",
        "given_name": "Marcin"
      },
      {
        "surname": "Słowiński",
        "given_name": "Roman"
      }
    ]
  },
  {
    "title": "Appearance feature enhancement for person re-identification",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113771",
    "abstract": "Person re-identification (Re-ID) has important practical application value in intelligent video analysis. Due to the illumination, occlusion, and pose variation, person Re-ID is still a challenging problem. Some recent Re-ID methods based on ResNet-50 have achieved high accuracy, but performance degradation is caused by pose variation. To address this issue, Pose-Invariant Convolutional Baseline (PICB) embed with the proposed Pooling Fusion Block (PFB) is put forward as a new baseline for person Re-ID task. On the basis of PICB, an end-to-end network named Appearance-Enhanced Feature Learning Network (AEFLN) is proposed to simultaneously learn diversity body features and discriminative part features. Specially, a novel (DBFL) strategy is presented to learn diversity body features, which could alleviate the potential local minima problem generated by optimizing model with randomly initialized parameters in PFB. In addition, uniform part-level feature extractors are applied to learn part features, which compensates for body features’ lack of distinguishable local information. In testing phase, body features and part features are integrated to represent the enhanced appearance feature for each person image. Comprehensive experiments have demonstrated that our method can outperform the sate-of-the-art results on several public available datasets, including Market-1501, CUHK03 and DukeMTMC-reID. For instance, we achieve 74.8% (+11.1%) and 76.5% (+19.0%) in Rank-1 accuracy and mAP on CUHK03 dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305959",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Discriminative model",
      "Feature (linguistics)",
      "Geometry",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Maxima and minima",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pooling",
      "Pyramid (geometry)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Wenfeng"
      },
      {
        "surname": "Huang",
        "given_name": "Lei"
      },
      {
        "surname": "Wei",
        "given_name": "Zhiqiang"
      },
      {
        "surname": "Nie",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "Detection of common risk factors for diagnosis of cardiac arrhythmia using machine learning algorithm",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113807",
    "abstract": "This article aims to establish an accurate and innovative objective framework for classification of cardiac arrhythmia patients by trying to measure the importance of specific factors that are potentially relevant to its diagnosis. Cardiac arrhythmia (CA) is a group of condition related to the irregular heartbeats. It is very essential to prevent a CAs, as they are the most common cause of natural death in all over the world. According to the health reports, more than 4.5 lakh cardiac patients fatalities annually in the United States alone. To diagnose cardiac diseases, patient’s reported qualitative symptoms can be useful. However, this strategy may fail sometimes due to less accuracy and false positive cases. Therefore in this work, we strive to find a quantitative basis for more reliable and accurate diagnosis of cardiac arrhythmias. This research used the openly available MIMIC-III database to obtain large quantities of clinical monitoring data from patients over the age of sixteen admitted to intensive care units (ICUs). The database was processed on the Health Sciences and Technology (HEST) Cluster, filtered with in a specified time frame(24hrs, 12hrs and 6hrs) and organized into a multi-class and a single-class and finally split into train, validation, and test sets with respective weights of 0.7, 0.2, and 0.1. We used random forest classifier model for the diagnosis of cardiac arrhythmia and measure the importance of different features like respiratory rate, blood pressure, sodium, potassium, calcium, among the other features. Hyperparameter optimization techniques like grid search and genetic algorithms are compared to find the maximum number and depth of trees in the forest. The model achieved, at its best, an Area Under the Receiver Operator Curve (AUC) score of 0.9787 and, thus, confirmed the importance of several previously suggested factors in the diagnosis of cardiac arrhythmias. We substantiated claims that each of sodium, calcium, potassium, respiratory rates and blood pressure can be used for the early diagnosis of cardiac arrhythmias.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306229",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Atrial fibrillation",
      "Cardiac arrhythmia",
      "Computer science",
      "Internal medicine",
      "Machine learning",
      "Medicine"
    ],
    "authors": [
      {
        "surname": "Yadav",
        "given_name": "Samir S."
      },
      {
        "surname": "Jadhav",
        "given_name": "Shivajirao M."
      }
    ]
  },
  {
    "title": "Collaborative and efficient privacy-preserving critical incident management system",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113727",
    "abstract": "When a critical incident occurs, timely location-based status messages (known as alerts) conveyed by a witness present at the incident site to the competent authority constitute a key part of an effective approach to handle the situation. Details provided by witnesses are of extreme significance, but their collaboration with the authority may make them susceptible to various threats such as loss of anonymity, false implication, unwarranted surveillance, etc. As a solution, an anonymous reporting mechanism can be provided to encourage the witnesses to report the incident to the concerned authority. However, there is a possibility of system collapse in case the anonymous witnesses inundate the authority with multiple fake reports, resulting in delayed response to the critical incident. In this paper, we present a critical incident reporting system that provides privacy to honest witnesses but can disclose the identity and punish the malicious ones. The proposed system facilitates the indistinguishability of the witness among a group of users such that reports are linked to groups of k (or more) users instead of a single user. We use a game-theoretic approach based on the co-utility principle to encourage the users of the system to engage in mutually beneficial collaboration, which leads to a “rewards and punishment” mechanism to encourage legitimate information and discourage false information. The incident management authority rewards honest witnesses with a blockchain-based cryptocurrency, which can be redeemed anonymously by the awardees from the city council. We have analyzed the security and privacy properties of the system, and carried out real-device testing to evaluate the system performance and its feasibility.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305510",
    "keywords": [
      "Acoustics",
      "Anonymity",
      "Business",
      "Computer science",
      "Computer security",
      "Identity (music)",
      "Identity theft",
      "Internet privacy",
      "Physics",
      "Programming language",
      "Psychology",
      "Punishment (psychology)",
      "Social psychology",
      "Witness"
    ],
    "authors": [
      {
        "surname": "Qureshi",
        "given_name": "Amna"
      },
      {
        "surname": "Garcia-Font",
        "given_name": "Victor"
      },
      {
        "surname": "Rifà-Pous",
        "given_name": "Helena"
      },
      {
        "surname": "Megías",
        "given_name": "David"
      }
    ]
  },
  {
    "title": "Option pricing using Machine Learning",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113799",
    "abstract": "This paper examines the option pricing performance of the most popular Machine Learning algorithms. The classic parametrical models suffer from several limitations in term of computational power required for parametric calibration and unrealistic economical and statistical assumptions. Therefore, a data driven approach based on non-parametric models is are well justified. Most of the previous researchers focus especially on the neural networks method (NN), the other algorithms being unexplored. Beside NN, this paper also analyses the performance of the Support Vector Regressions and Genetic Algorithms and propose three other Decision Tree methods, respectively Random Forest, XGBoost and LightGMB. In order to emphasize the power of this algorithms, a comparison with classical methods like Black-Scholes and Corrado-Su with both historical and implied parameters have been conducted. The analyzes were performed on European call options who have as underlying asset the WTI crude oil future contracts. Machine Learning algorithms outperform by a great margin the classical approaches regardless of the moneyness and the maturity of the contracts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306187",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Black–Scholes model",
      "Calibration",
      "Computer science",
      "Decision tree",
      "Econometrics",
      "Economics",
      "Focus (optics)",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematics",
      "Optics",
      "Physics",
      "Statistics",
      "Support vector machine",
      "Valuation of options",
      "Volatility (finance)",
      "West Texas Intermediate"
    ],
    "authors": [
      {
        "surname": "Ivașcu",
        "given_name": "Codruț-Florin"
      }
    ]
  },
  {
    "title": "Explicit song lyrics detection with subword-enriched word embeddings",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113749",
    "abstract": "In this paper, we investigate the problem of automatically detecting explicit song lyrics, i.e., determining if the lyrics of a given song could be offensive or unsuitable for children. The problem can be framed as a binary classification task, and in this work we propose to tackle it with the fastText classifier, an efficient linear classification model leveraging a peculiar distributional text representation that, by exploiting subword information in building the embeddings of the words, enables to cope with words not seen at training time. We assess the performance of the fastText classifier and word representations with a lyrics dataset of over 800K songs, annotated with explicit information, that we assembled from publicly available resources. The evaluation shows that the fastText classifier is effective for explicit lyrics detection, substantially outperforming a reference approach for the task, and that the subword information effectively contributes to this result.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030573X",
    "keywords": [
      "Arithmetic",
      "Art",
      "Artificial intelligence",
      "Binary classification",
      "Binary number",
      "Classifier (UML)",
      "Computer science",
      "Economics",
      "Linguistics",
      "Literature",
      "Lyrics",
      "Management",
      "Mathematics",
      "Natural language processing",
      "Offensive",
      "Operations research",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Speech recognition",
      "Support vector machine",
      "Task (project management)",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Rospocher",
        "given_name": "Marco"
      }
    ]
  },
  {
    "title": "Novel hybrid pair recommendations based on a large-scale comparative study of concept drift detection",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113786",
    "abstract": "During the classification of streaming data, changes in the underlying distribution make formerly learned models insecure and imprecise, which is known as the concept drift phenomenon. Online learning derives information from a vast volume of stream data, which are usually affected by these changes in unforeseen ways and are currently generated primarily by the Internet of Things, social media applications, and the stock market. There is abundant literature focused on addressing concept drift using detectors, which essentially attempt to forecast the position of the change to improve the overall accuracy by altering the base learner. This paper presents novel hybrid pairs (classifier and detector) collected from a large-scale comparison of 15 drift detectors; drift detection method (DDM), early drift detection method (EDDM), EWMA for concept drift detection (ECDD), adaptive sliding window (ADWIN), geometrical moving average (GMA), drift detection methods based on Hoeffding’s bound (HDDMA and HDDMW), Fisher exact test drift detector (FTDD), fast Hoeffding drift detection method (FHDDM), Page–Hinkley test (PH), reactive drift detection method (RDDM), SEED, statistical test of equal proportions (STEPD), SeqDrift2, and Wilcoxon rank-sum test drift detector (WSTD) and six classifiers; Naïve Bayes (NB), Hoeffding tree (HT), Hoeffding option tree (HOT), Perceptron (P), decision stump (DS), and k-nearest neighbour (KNN), to determine and recommend the best pair in accordance with the properties of the dataset. The objective of this study is to assess the contribution of a detector to a classifier and obtain the most efficient matched pairs. Through these pairwise comparison experiments, the accuracy rates and evaluation times of the pairs, as well as their false positives, true negatives, false negatives, true positives, drift detection delay, and the MCC. Additionally, the Nemenyi test is employed to compare the pairs against other methods to identify the method(s) for which there is a statistical difference. The results of the experiments indicate that the most efficient pairs—which differed for each dataset type and size—primarily include the HDDMA, RDDM, WSTD, and FHDDM detectors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306102",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Classifier (UML)",
      "Computer science",
      "Concept drift",
      "Data mining",
      "Data stream",
      "Data stream mining",
      "Decision tree",
      "Detector",
      "Machine learning",
      "Mathematics",
      "Naive Bayes classifier",
      "Pattern recognition (psychology)",
      "Perceptron",
      "Statistical hypothesis testing",
      "Statistics",
      "Support vector machine",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Babüroğlu",
        "given_name": "Elif Selen"
      },
      {
        "surname": "Durmuşoğlu",
        "given_name": "Alptekin"
      },
      {
        "surname": "Dereli",
        "given_name": "Türkay"
      }
    ]
  },
  {
    "title": "RC-Tweet: Modeling and predicting the popularity of tweets through the dynamics of a capacitor",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113785",
    "abstract": "A novel model for the popularity evolution of tweets – the RC-Tweet – is introduced. Its originality stems from the revelation of the astounding similarity between the popularity growth of tweets and the charging dynamics of a capacitor in an RC-Circuit. Fitting the model to empirical retweet patterns pertaining to cascades of all sizes, accurate goodness-of-fit statistics are obtained. These findings illustrate that the RC-Tweet precisely captures the dynamics of retweets, through a macroscopic mechanistic model comprising just two independent parameters. Exploring the predictive power of the model on retweet cascades of various sizes, it was found that accurate popularity forecasts, with a 6.95% average error, are produced using only the posting time of retweets, mostly occurring within an extremely short observation period ( ~ 3 min ). The RC-Tweet model outperforms existing state-of-the-art popularity models in precision, speed and simplicity. It is suitable for real time predictions with minimal publicly available information, without training on existing data. The novelty and performance of the RC-Tweet model open up new opportunities for theoretical and applied research in the popularity evolution mechanisms in social media. Also, the RC-Tweet model is potentially applicable to the explanation and prediction of the popularity dynamics in other fields, such as commerce, financial markets, fashions and trends.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306096",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Data science",
      "Dynamics (music)",
      "Machine learning",
      "Novelty",
      "Philosophy",
      "Physics",
      "Popularity",
      "Psychology",
      "Social media",
      "Social psychology",
      "Theology",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Lymperopoulos",
        "given_name": "Ilias N."
      }
    ]
  },
  {
    "title": "Enhancing polymer electrolyte membrane fuel cell system diagnostics through semantic modelling",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113550",
    "abstract": "Polymer electrolyte membrane fuel cells (PEMFC) are a promising technology for economic and environmentally friendly energy production. However, they haven’t reached their full potential in the market yet as only few reliable PEMFC systems have successfully passed the prototyping face. A drawback of the current diagnostic tools is that only a select few are of high genericity, reliability and can perform efficiently on-line at the same time. Furthermore, there is only limited research identifying both PEMFC stack faults and ancillary system faults simultaneously. While none of the existing tools can be interrogated by the end-user. In this research, we develop novel artificial intelligence-based technologies to overcome these existing barriers, i.e., (i) a semantically enriched integrating schema (ontology) of the overall operation and structure of the PEMFC that allows automatic inference engines to automatically deduce fault detection; (ii) a knowledge-based, light-weight, on-line fuel cell system diagnosis (FuCSyDi) platform. FuCSyDi detects and provides the location of failures by considering only the data from the reliable sensors. Additionally, it provides the reasons underpinning any forthcoming failures and enables the end-user to interrogate the platform for further information regarding its operation and structure. Our platform is validated by performing tests against common automotive stress conditions. This innovative approach enhances the reliability of the fuel cell system diagnosis and, hence, its lifetime performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420303742",
    "keywords": [
      "Aerospace engineering",
      "Artificial intelligence",
      "Automotive engineering",
      "Automotive industry",
      "Chemical engineering",
      "Computer science",
      "Engineering",
      "Epistemology",
      "Fuel cells",
      "Inference",
      "Ontology",
      "Philosophy",
      "Physics",
      "Power (physics)",
      "Proton exchange membrane fuel cell",
      "Quantum mechanics",
      "Reliability (semiconductor)",
      "Reliability engineering"
    ],
    "authors": [
      {
        "surname": "Tsalapati",
        "given_name": "E."
      },
      {
        "surname": "Johnson",
        "given_name": "C.W.D."
      },
      {
        "surname": "Jackson",
        "given_name": "T.W."
      },
      {
        "surname": "Jackson",
        "given_name": "L."
      },
      {
        "surname": "Low",
        "given_name": "D."
      },
      {
        "surname": "Davies",
        "given_name": "B."
      },
      {
        "surname": "Mao",
        "given_name": "L."
      },
      {
        "surname": "West",
        "given_name": "A."
      }
    ]
  },
  {
    "title": "A novel dynamic asset allocation system using Feature Saliency Hidden Markov models for smart beta investing",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113720",
    "abstract": "The financial crisis of 2008 generated interest in more transparent, rules-based strategies for portfolio construction, with smart beta strategies emerging as a trend among institutional investors. Whilst they perform well in the long run, these strategies often suffer from severe short-term drawdown (peak-to-trough decline) with fluctuating performance across cycles. To manage short term risk (cyclicality and underperformance), we build a dynamic asset allocation system using Hidden Markov Models (HMMs). We use a variety of portfolio construction techniques to test our smart beta strategies and the resulting portfolios show an improvement in risk-adjusted returns, especially on more return-oriented portfolios (up to 50% of return in excess of market adjusted by relative risk annually). In addition, we propose a novel smart beta allocation system based on the Feature Saliency HMM (FSHMM) algorithm that performs feature selection simultaneously with the training of the HMM, to improve regime identification. We evaluate our systematic trading system with real life assets using MSCI indices; further, the results (up to 60% of return in excess of market adjusted by relative risk annually) show model performance improvement with respect to portfolios built using full feature HMMs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305443",
    "keywords": [],
    "authors": [
      {
        "surname": "Fons",
        "given_name": "Elizabeth"
      },
      {
        "surname": "Dawson",
        "given_name": "Paula"
      },
      {
        "surname": "Yau",
        "given_name": "Jeffrey"
      },
      {
        "surname": "Zeng",
        "given_name": "Xiao-jun"
      },
      {
        "surname": "Keane",
        "given_name": "John"
      }
    ]
  },
  {
    "title": "Improving Bayesian inference efficiency for sensory anomaly detection and recovery in mobile robots",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113755",
    "abstract": "For mobile robots to operate in real environments, it is essential that basic tasks such as localization, mapping and navigation are performed properly. These tasks strongly rely on an adequate perception of the environment, which may be challenging in some cases due to the nature of the scene itself, the limited operation of some sensors, or even both. A mobile robot should be able to intelligently identify and overcome abnormal situations efficiently in order to avoid sensory malfunctioning. We propose in this work a novel methodology based on Bayesian networks, which enables to naturally represent complex relationships among sensors, to integrate heterogeneous sources of knowledge, to deduct the presence of sensory anomalies, and finally to recover from them by using the available information. The high computational cost of inference is addressed by a new algorithm that takes advantage of our model structure. Our proposal has been assessed in several simulations and has also been tested in a real environment with a mobile robot. The obtained results show that it achieves better performance and accuracy compared to other existing methods, while enhancing the robustness of the whole sensory system.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305790",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Bayesian inference",
      "Bayesian probability",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Cognitive psychology",
      "Computer science",
      "Gene",
      "Inference",
      "Machine learning",
      "Mobile robot",
      "Neuroscience",
      "Perception",
      "Psychology",
      "Robot",
      "Robustness (evolution)",
      "Sensory system"
    ],
    "authors": [
      {
        "surname": "Castellano-Quero",
        "given_name": "Manuel"
      },
      {
        "surname": "Fernández-Madrigal",
        "given_name": "Juan-Antonio"
      },
      {
        "surname": "García-Cerezo",
        "given_name": "Alfonso"
      }
    ]
  },
  {
    "title": "A Q-learning agent for automated trading in equity stock markets",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113761",
    "abstract": "Trading strategies play a vital role in Algorithmic trading, a computer program that takes and executes automated trading decisions in the stock market. The conventional wisdom is that the same trading strategy is not profitable for all stocks all the time. The selection of a trading strategy for the stock at a particular time instant is the major research problem in the stock market trading. An optimal dynamic trading strategy generated from the current pattern of the stock price trend can attempt to solve this problem. Reinforcement Learning can find this optimal dynamic trading strategy by interacting with the actual stock market as its environment. The representation of the state of the environment is crucial for performance. We have proposed two different ways to represent the discrete states of the environment. In this work, we trained the trading agent using the Q-learning algorithm of Reinforcement Learning to find optimal dynamic trading strategies. We experimented with the two proposed models on real stock market data from the Indian and American stock markets. The proposed models outperformed the Buy-and-Hold and Decision-Tree based trading strategy in terms of profitability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305856",
    "keywords": [
      "Algorithm",
      "Algorithmic trading",
      "Alternative trading system",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Economics",
      "Electronic trading",
      "Engineering",
      "Equity (law)",
      "Finance",
      "Financial economics",
      "High-frequency trading",
      "Horse",
      "Law",
      "Market maker",
      "Mechanical engineering",
      "Open outcry",
      "Pairs trade",
      "Paleontology",
      "Political science",
      "Profitability index",
      "Reinforcement learning",
      "Stock (firearms)",
      "Stock market",
      "Trading strategy"
    ],
    "authors": [
      {
        "surname": "Chakole",
        "given_name": "Jagdish Bhagwan"
      },
      {
        "surname": "Kolhe",
        "given_name": "Mugdha S."
      },
      {
        "surname": "Mahapurush",
        "given_name": "Grishma D."
      },
      {
        "surname": "Yadav",
        "given_name": "Anushka"
      },
      {
        "surname": "Kurhekar",
        "given_name": "Manish P."
      }
    ]
  },
  {
    "title": "Enhancing credit scoring with alternative data",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113766",
    "abstract": "Hundreds of millions of people in low-income economies do not have a credit or bank account because they have insufficient credit history for a credit score to be ascribed to them. In this paper we evaluate the predictive accuracy of models using alternative data, that may be used instead of credit history, to predict the credit risk of a new account. Without alternative data, the type of data that is typically available is demographic data. We show that a model that contains email usage and psychometric variables, as well as demographic variables, can give greater predictive accuracy than a model that uses demographic data only and that the predictive accuracy is sufficiently high for the demographic and email data to be used when conventional credit history data is unavailable. The same applies if merely psychometric data is included together with demographic data. However, we show that different randomly selected training: test sample splits give a wide range of predictive accuracies. In the second part of the paper, using two datasets that include only email usage as a predictor, we compare the predictive performances of a wide range of machine learning and statistical classifiers. We find that some classifiers applied to these alternative predictors give sufficiently accurate predictions for these variables to be used when no other data is available.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030590X",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Composite material",
      "Computer science",
      "Data mining",
      "Econometrics",
      "Epistemology",
      "Machine learning",
      "Materials science",
      "Mathematics",
      "Philosophy",
      "Predictive modelling",
      "Predictive power",
      "Range (aeronautics)",
      "Sample (material)"
    ],
    "authors": [
      {
        "surname": "Djeundje",
        "given_name": "Viani B."
      },
      {
        "surname": "Crook",
        "given_name": "Jonathan"
      },
      {
        "surname": "Calabrese",
        "given_name": "Raffaella"
      },
      {
        "surname": "Hamid",
        "given_name": "Mona"
      }
    ]
  },
  {
    "title": "Stochastic reserving with a stacked model based on a hybridized Artificial Neural Network",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113782",
    "abstract": "Currently, legal requirements demand that insurance companies increase their emphasis on monitoring the risks linked to the underwriting and asset management activities. Regarding underwriting risks, the main uncertainties that insurers must manage are related to the premium sufficiency to cover future claims and the adequacy of the current reserves to pay outstanding claims. Both risks are calibrated using stochastic models due to their nature. This paper introduces a reserving model based on a set of machine learning techniques such as Gradient Boosting, Random Forest and Artificial Neural Networks. These algorithms and other widely used reserving models are stacked to predict the shape of the runoff. To compute the deviation around a former prediction, a log-normal approach is combined with the suggested model. The empirical results demonstrate that the proposed methodology can be used to improve the performance of the traditional reserving techniques based on Bayesian statistics and a Chain Ladder, leading to a more accurate assessment of the reserving risk.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306060",
    "keywords": [
      "Actuarial science",
      "Artificial intelligence",
      "Artificial neural network",
      "Business",
      "Computer science",
      "Machine learning",
      "Random forest",
      "Underwriting"
    ],
    "authors": [
      {
        "surname": "Ramos-Pérez",
        "given_name": "Eduardo"
      },
      {
        "surname": "Alonso-González",
        "given_name": "Pablo J."
      },
      {
        "surname": "Núñez-Velázquez",
        "given_name": "José Javier"
      }
    ]
  },
  {
    "title": "Distance Metric Learning for Radio Fingerprinting Localization",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113747",
    "abstract": "Metric is not only a function to mesure the distance between data points, but also a main tool to evaluate the error of data analysis, so it is one of the important factors to be considered by expert systems and intelligent systems. A general metric function is difficult to adapt to all application scenarios. Thanks to the development of big data and machine learning technology, metric learning can be used to obtain an optimal metric for a specific application scenario. Considering the fingerprinting localization (FL) as an intelligent system for processing radio signals, this paper proposes two novel novel location fingerprint (LF) metric learning algorithms to improve the accuracy and adaptability of the system. The two proposed algorithms are named LMNN-LF and NCA-LF respectively, and they are based on two famous metric learning algorithms, LMNN and NCA. Considering the distribution characteristics of position fingerprints, we modified the original cost function in both schemes. In order to accommodate the low resolution of the fingerprint, an error radius is defined in our method. The distance relation of the LFs in high dimensional space is better described by the learned metric function, which improves the accuracy of the FL system compared with the general metric function or metric learning method. Moreover, the proposed methods can also effectively extract the features or reduce the dimension of LFs, improving the accuracy of other feature-based localization algorithms. Experiments on different data sets show that the metric obtained by the proposed method and several existing methods performs good in kNN localization, and performs good in LF feature extraction and dimensionality reduction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305716",
    "keywords": [
      "Adaptability",
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Data mining",
      "Ecology",
      "Economics",
      "Evolutionary biology",
      "Feature (linguistics)",
      "Fingerprint (computing)",
      "Function (biology)",
      "Linguistics",
      "Machine learning",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Bai",
        "given_name": "Siqi"
      },
      {
        "surname": "Luo",
        "given_name": "Yongjie"
      },
      {
        "surname": "Yan",
        "given_name": "Mingjiang"
      },
      {
        "surname": "Wan",
        "given_name": "Qun"
      }
    ]
  },
  {
    "title": "A recommendation algorithm based on fine-grained feature analysis",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113759",
    "abstract": "Most existing recommender methods have insufficient power to capture or recover fine-grained features of products and user preferences. Knowledge graphs contain considerable information about products and mutual relationships in the world. To analyze the fine-grained features during the process of recommendation, we propose a knowledge-aware collaborative learning framework (KACL), which is the first work to combine users’ historical reviews with knowledge graphs. First, we use a named-entity recognition (NER) system to recognize the named entities that correspond to item features in the unstructured reviews. After that, we use an entity-linking (EL) system to map entities which identified in the first step to the corresponding entity in Wikipedia. Next, we constructed a sub-graph that depends on the extracted entity and the related ones and embedded the sub-graph into a low latent vector space as pretrain item embedding through the knowledge graph embedding model TransR. In the next step, we will fine-tune item embedding through deep learning built by 4 levels of relu and combine collaborative information with pretrain item embedding. And in accordance with the user review behavior, the item embedding will be averagely poled to express the user embedding, we will put the item embedding and the user embedding into the classification model. Finally, we use the KACL to integrate collaborative filtering with entity representation and make recommendations. Experiments are performed on real-world datasets to evaluate the effectiveness of our new method. The results show that the analysis of fine-grained features based on a knowledge graph helps our KACL improve recommender accuracy compared with the selected state-of-the-art models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420305832",
    "keywords": [
      "Artificial intelligence",
      "Collaborative filtering",
      "Computer science",
      "Embedding",
      "Feature learning",
      "Graph",
      "Information retrieval",
      "Knowledge graph",
      "Machine learning",
      "Recommender system",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Wenjie"
      },
      {
        "surname": "Altenbek",
        "given_name": "Gulila"
      }
    ]
  },
  {
    "title": "Generative Adversarial Networks and Markov Random Fields for oversampling very small training sets",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113819",
    "abstract": "In this work, we propose a new method for oversampling the training set of a classifier, in a scenario of extreme scarcity of training data. It is based on two concepts: Generative Adversarial Networks (GAN) and vector Markov Random Field (vMRF). Thus, the generative block of GAN uses the vMRF model to synthesize surrogates by the Graph Fourier Transform. Then, the discriminative block implements a linear discriminant on features measuring clique similarities between the synthesized and the original instances. Both blocks iterate until the linear discriminant cannot discriminate the synthetic from the original instances. We have assessed the new method, called Generative Adversarial Network Synthesis for Oversampling (GANSO), with both simulated and real data in experiments where the classifier is to be trained with just 3 or 5 instances. The applications consisted of classification of stages of neuropsychological tests using electroencephalographic (EEG) and functional magnetic resonance imaging (fMRI) data and classification of sleep stages using electrocardiographic (ECG) data. We have verified that GANSO can effectively improve the classifier performance, while the benchmark method SMOTE is not appropriate to deal with such a small size of the training set.",
    "link": "https://www.sciencedirect.com/science/article/pii/S095741742030631X",
    "keywords": [
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Classifier (UML)",
      "Computer network",
      "Computer science",
      "Discriminative model",
      "Linear discriminant analysis",
      "Machine learning",
      "Oversampling",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Salazar",
        "given_name": "Addisson"
      },
      {
        "surname": "Vergara",
        "given_name": "Luis"
      },
      {
        "surname": "Safont",
        "given_name": "Gonzalo"
      }
    ]
  },
  {
    "title": "A comprehensive comparison of handcrafted features and convolutional autoencoders for epileptic seizures detection in EEG signals",
    "journal": "Expert Systems with Applications",
    "year": "2021",
    "doi": "10.1016/j.eswa.2020.113788",
    "abstract": "Epilepsy, a brain disease generally associated with seizures, has tremendous effects on people’s quality of life. Diagnosis of epileptic seizures is commonly performed on electroencephalography (EEG) signals, and by using computer-aided diagnosis systems (CADS), neurologists can diagnose epileptic seizure stages more accurately. In these systems, a mandatory stage is feature extraction, performed by handcrafting features or learning them, ordinarily by a deep neural net. While researches in this field commonly show the value of a group of limited features, yet an accurate comparison between different suggested features is essential. In this article, first, a comparison between the importance of 50 different handcrafted features for seizure detection is presented. Additionally, the computational complexity of features is investigated as well. Then the best features based on Fisher scores are picked to classify signals on a benchmark dataset for evaluation. Additionally, a convolutional autoencoder with five layers is applied to learn features in order to have a complete comparison among feature extraction approaches. Finally, a hybrid method is employed, which combines handcrafted features and encoding of autoencoder to reach high performance in seizure detection in EEG signals.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0957417420306114",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Benchmark (surveying)",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Electroencephalography",
      "Epilepsy",
      "Epileptic seizure",
      "Feature (linguistics)",
      "Feature extraction",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Machine learning",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Psychology"
    ],
    "authors": [
      {
        "surname": "Shoeibi",
        "given_name": "Afshin"
      },
      {
        "surname": "Ghassemi",
        "given_name": "Navid"
      },
      {
        "surname": "Alizadehsani",
        "given_name": "Roohallah"
      },
      {
        "surname": "Rouhani",
        "given_name": "Modjtaba"
      },
      {
        "surname": "Hosseini-Nejad",
        "given_name": "Hossein"
      },
      {
        "surname": "Khosravi",
        "given_name": "Abbas"
      },
      {
        "surname": "Panahiazar",
        "given_name": "Maryam"
      },
      {
        "surname": "Nahavandi",
        "given_name": "Saeid"
      }
    ]
  }
]