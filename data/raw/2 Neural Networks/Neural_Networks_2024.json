[
  {
    "title": "A new hybrid learning control system for robots based on spiking neural networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106656",
    "abstract": "This paper presents a new hybrid learning and control method that can tune their parameters based on reinforcement learning. In the new proposed method, nonlinear controllers are considered multi-input multi-output functions and then the functions are replaced with SNNs with reinforcement learning algorithms. Dopamine-modulated spike-timing-dependent plasticity (STDP) is used for reinforcement learning and manipulating the synaptic weights between the input and output of neuronal groups (for parameter adjustment). Details of the method are presented and some case studies are done on nonlinear controllers such as Fractional Order PID (FOPID) and Feedback Linearization. The structure and the dynamic equations for learning are presented, and the proposed algorithm is tested on robots and results are compared with other works. Moreover, to demonstrate the effectiveness of SNNFOPID, we conducted rigorous testing on a variety of systems including a two-wheel mobile robot, a double inverted pendulum, and a four-link manipulator robot. The results revealed impressively low errors of 0.01 m, 0.03 rad, and 0.03 rad for each system, respectively. The method is tested on another controller named Feedback Linearization, which provides acceptable results. Results show that the new method has better performance in terms of Integral Absolute Error (IAE) and is highly useful in hardware implementation due to its low energy consumption, high speed, and accuracy. The duration necessary for achieving full and stable proficiency in the control of various robotic systems using SNNFOPD, and SNNFL on an Asus Core i5 system within Simulink’s Simscape environment is as follows: – Two-link robot manipulator with SNNFOPID: 19.85656 hours – Two-link robot manipulator with SNNFL: 0.45828 hours – Double inverted pendulum with SNNFOPID: 3.455 hours – Mobile robot with SNNFOPID: 3.71948 hours – Four-link robot manipulator with SNNFOPID: 16.6789 hours. This method can be generalized to other controllers and systems like robots.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400580X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Control (management)",
      "Machine learning",
      "Robot",
      "Spiking neural network"
    ],
    "authors": [
      {
        "surname": "Azimirad",
        "given_name": "Vahid"
      },
      {
        "surname": "Khodkam",
        "given_name": "S. Yaser"
      },
      {
        "surname": "Bolouri",
        "given_name": "Amir"
      }
    ]
  },
  {
    "title": "Adaptive ambiguity-aware weighting for multi-label recognition with limited annotations",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106642",
    "abstract": "In multi-label recognition, effectively addressing the challenge of partial labels is crucial for reducing annotation costs and enhancing model generalization. Existing methods exhibit limitations by relying on unrealistic simulations with uniformly dropped labels, overlooking how ambiguous instances and instance-level factors impacts label ambiguity in real-world datasets. To address this deficiency, our paper introduces a realistic partial label setting grounded in instance ambiguity, complemented by Reliable Ambiguity-Aware Instance Weighting (R-AAIW)—a strategy that utilizes importance weighting to adapt dynamically to the inherent ambiguity of multi-label instances. The strategy leverages an ambiguity score to prioritize learning from clearer instances. As proficiency of the model improves, the weights are dynamically modulated to gradually shift focus towards more ambiguous instances. By employing an adaptive re-weighting method that adjusts to the complexity of each instance, our approach not only enhances the model’s capability to detect subtle variations among labels but also ensures comprehensive learning without excluding difficult instances. Extensive experimentation across various benchmarks highlights our approach’s superiority over existing methods, showcasing its ability to provide a more accurate and adaptable framework for multi-label recognition tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005665",
    "keywords": [
      "Ambiguity",
      "Annotation",
      "Artificial intelligence",
      "Computer science",
      "Generalization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Programming language",
      "Radiology",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Shrewsbury",
        "given_name": "Daniel"
      },
      {
        "surname": "Kim",
        "given_name": "Suneung"
      },
      {
        "surname": "Lee",
        "given_name": "Seong-Whan"
      }
    ]
  },
  {
    "title": "Addressing unreliable local models in federated learning through unlearning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106688",
    "abstract": "Federated unlearning (FUL) is a promising solution for removing negative influences from the global model. However, ensuring the reliability of local models in FL systems remains challenging. Existing FUL studies mainly focus on eliminating bad data influences and neglecting scenarios where other factors, such as adversarial attacks and communication constraints, also contribute to negative influences that require mitigation. In this paper, we introduce Local Model Refining (LMR), a FUL method designed to address the negative impacts of bad data as well as other factors on the global model. LMR consists of three components: (i) Identifying and categorizing unreliable local models into two classes based on their influence source: bad data or other factors. (ii) Bad Data Influence Unlearning (BDIU): BDIU is a client-side algorithm that identifies affected layers in unreliable models and employs gradient ascent to mitigate bad data influences. Boosting training is applied when necessary under specific conditions. (iii) Other Influence Unlearning (OIU): OIU is a server-side algorithm that identifies unaffected parameters in the unreliable local model and combines them with corresponding parameters of the previous global model to construct the updated local model. Finally, LMR aggregates updated local models with remaining local models to produce the unlearned global model. Extensive evaluation shows LMR enhances accuracy and accelerates average unlearning speed by 5x compared to comparison methods on MNIST, FMNIST, CIFAR-10, and CelebA datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006129",
    "keywords": [
      "Artificial intelligence",
      "Boosting (machine learning)",
      "Computer science",
      "Deep learning",
      "Federated learning",
      "MNIST database",
      "Machine learning",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Reliability (semiconductor)"
    ],
    "authors": [
      {
        "surname": "Ameen",
        "given_name": "Muhammad"
      },
      {
        "surname": "Khan",
        "given_name": "Riaz Ullah"
      },
      {
        "surname": "Wang",
        "given_name": "Pengfei"
      },
      {
        "surname": "Batool",
        "given_name": "Sidra"
      },
      {
        "surname": "Alajmi",
        "given_name": "Masoud"
      }
    ]
  },
  {
    "title": "OS-SSVEP: One-shot SSVEP classification",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106734",
    "abstract": "It is extremely challenging to classify steady-state visual evoked potentials (SSVEPs) in scenarios characterized by a huge scarcity of calibration data where only one calibration trial is available for each stimulus target. To address this challenge, we introduce a novel approach named OS-SSVEP, which combines a dual domain cross-subject fusion network (CSDuDoFN) with the task-related and task-discriminant component analysis (TRCA and TDCA) based on data augmentation. The CSDuDoFN framework is designed to comprehensively transfer information from source subjects, while TRCA and TDCA are employed to exploit the information from the single available calibration trial of the target subject. Specifically, CSDuDoFN uses multi-reference least-squares transformation (MLST) to map data from both the source subjects and the target subject into the domain of sine-cosine templates, thereby reducing cross-subject domain gap and benefiting transfer learning. In addition, CSDuDoFN is fed with both transformed and original data, with an adequate fusion of their features occurring at different network layers. To capitalize on the calibration trial of the target subject, OS-SSVEP utilizes source aliasing matrix estimation (SAME)-based data augmentation to incorporate into the training process of the ensemble TRCA (eTRCA) and TDCA models. Ultimately, the outputs of CSDuDoFN, eTRCA, and TDCA are combined for the SSVEP classification. The effectiveness of our proposed approach is comprehensively evaluated on three publicly available SSVEP datasets, achieving the best performance on two datasets and competitive performance on the third. Further, it is worth noting that our method follows a different technical route from the current state-of-the-art (SOTA) method and the two are complementary. The performance is significantly improved when our method is combined with the SOTA method. This study underscores the potential to integrate the SSVEP-based brain-computer interface (BCI) into daily life. The corresponding source code is accessible at https://github.com/Sungden/One-shot-SSVEP-classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006580",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Shot (pellet)"
    ],
    "authors": [
      {
        "surname": "Deng",
        "given_name": "Yang"
      },
      {
        "surname": "Ji",
        "given_name": "Zhiwei"
      },
      {
        "surname": "Wang",
        "given_name": "Yijun"
      },
      {
        "surname": "Zhou",
        "given_name": "S. Kevin"
      }
    ]
  },
  {
    "title": "BGAT-CCRF: A novel end-to-end model for knowledge graph noise correction",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106715",
    "abstract": "Knowledge graph (KG) noise correction aims to select suitable candidates to correct the noises in KGs. Most of the existing studies have limited performance in repairing the noisy triple that contains more than one incorrect entity or relation, which significantly constrains their implementation in real-world KGs. To overcome this challenge, we propose a novel end-to-end model (BGAT-CCRF) that achieves better noise correction results. Specifically, we construct a balanced-based graph attention model (BGAT) to learn the features of nodes in triples’ neighborhoods and capture the correlation between nodes based on their position and frequency. Additionally, we design a constrained conditional random field model (CCRF) to select suitable candidates guided by three constraints for correcting one or more noises in the triple. In this way, BGAT-CCRF can select multiple candidates from a smaller domain to repair multiple noises in triples simultaneously, rather than selecting candidates from the whole KG to repair noisy triples as traditional methods do, which can only repair one noise in the triple at a time. The effectiveness of BGAT-CCRF is validated by the KG noise correction experiment. Compared with the state-of-the-art models, BGAT-CCRF improves the fMRR metric by 3.58% on the FB15K dataset. Hence, it has the potential to facilitate the implementation of KGs in the real world.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006397",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Conditional random field",
      "Economics",
      "End-to-end principle",
      "Graph",
      "Image (mathematics)",
      "Machine learning",
      "Metric (unit)",
      "Noise (video)",
      "Operations management",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Jiangtao"
      },
      {
        "surname": "Li",
        "given_name": "Kunlin"
      },
      {
        "surname": "Zhang",
        "given_name": "Fan"
      },
      {
        "surname": "Wang",
        "given_name": "Yanjun"
      },
      {
        "surname": "Luo",
        "given_name": "Xiangyang"
      },
      {
        "surname": "Li",
        "given_name": "Chenliang"
      },
      {
        "surname": "Qiao",
        "given_name": "Yaqiong"
      }
    ]
  },
  {
    "title": "Beyond smoothness: A general optimization framework for graph neural networks with negative Laplacian regularization",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106704",
    "abstract": "Graph Neural Networks (GNNs) have drawn great attention in handling graph-structured data. To characterize the message-passing mechanism of GNNs, recent studies have established a unified framework that models the graph convolution operation as a graph signal denoising problem. While increasing interpretability, this framework often performs poorly on heterophilic graphs and also leads to shallow and fragile GNNs in practice. The key reason is that it encourages feature smoothness, but ignores the high-frequency information of node features. To address this issue, we propose a general framework for GNNs via relaxation of the smoothness regularization. In particular, it employs an information aggregation mechanism to learn the low- and high-frequency components adaptively from data, offering more flexible graph convolution operators compared to the smoothness-promoted framework. Theoretical analyses demonstrate that our framework can capture both low- and high-frequency information of node features, effectively. Experiments on nine benchmark datasets show that our framework achieves the state-of-the-art performance in most cases. Furthermore, it can be used to handle deep models and adversarial attacks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006282",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Graph",
      "Laplace operator",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Regularization (linguistics)",
      "Smoothness",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Zhengpin"
      },
      {
        "surname": "Jia",
        "given_name": "Mengzhe"
      },
      {
        "surname": "Wei",
        "given_name": "Zheng"
      },
      {
        "surname": "Wang",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "Enhancing SNN-based spatio-temporal learning: A benchmark dataset and Cross-Modality Attention model",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106677",
    "abstract": "Spiking Neural Networks (SNNs), renowned for their low power consumption, brain-inspired architecture, and spatio-temporal representation capabilities, have garnered considerable attention in recent years. Similar to Artificial Neural Networks (ANNs), high-quality benchmark datasets are of great importance to the advances of SNNs. However, our analysis indicates that many prevalent neuromorphic datasets lack strong temporal correlation, preventing SNNs from fully exploiting their spatio-temporal representation capabilities. Meanwhile, the integration of event and frame modalities offers more comprehensive visual spatio-temporal information. Yet, the SNN-based cross-modality fusion remains underexplored. In this work, we present a neuromorphic dataset called DVS-SLR that can better exploit the inherent spatio-temporal properties of SNNs. Compared to existing datasets, it offers advantages in terms of higher temporal correlation, larger scale, and more varied scenarios. In addition, our neuromorphic dataset contains corresponding frame data, which can be used for developing SNN-based fusion methods. By virtue of the dual-modal feature of the dataset, we propose a Cross-Modality Attention (CMA) based fusion method. The CMA model efficiently utilizes the unique advantages of each modality, allowing for SNNs to learn both temporal and spatial attention scores from the spatio-temporal features of event and frame modalities, subsequently allocating these scores across modalities to enhance their synergy. Experimental results demonstrate that our method not only improves recognition accuracy but also ensures robustness across diverse scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006014",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Modality (human–computer interaction)",
      "Neuromorphic engineering",
      "Pattern recognition (psychology)",
      "Spiking neural network"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Shibo"
      },
      {
        "surname": "Yang",
        "given_name": "Bo"
      },
      {
        "surname": "Yuan",
        "given_name": "Mengwen"
      },
      {
        "surname": "Jiang",
        "given_name": "Runhao"
      },
      {
        "surname": "Yan",
        "given_name": "Rui"
      },
      {
        "surname": "Pan",
        "given_name": "Gang"
      },
      {
        "surname": "Tang",
        "given_name": "Huajin"
      }
    ]
  },
  {
    "title": "Intermediate-grained kernel elements pruning with structured sparsity",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106708",
    "abstract": "Neural network pruning provides a promising prospect for the deployment of neural networks on embedded or mobile devices with limited resources. Although current structured strategies are unconstrained by specific hardware architecture in the phase of forward inference, the decline in classification accuracy of structured methods is beyond the tolerance at the level of general pruning rate. This inspires us to develop a technique that satisfies high pruning rate with a small decline in accuracy and has the general nature of structured pruning. In this paper, we propose a new pruning method, namely KEP (Kernel Elements Pruning), to compress deep convolutional neural networks by exploring the significance of elements in each kernel plane and removing unimportant elements. In this method, we apply a controllable regularization penalty to constrain unimportant elements by adding a prior knowledge mask and obtain a compact model. In the calculation procedure of forward inference, we introduce a sparse convolution operation which is different from the sliding window to eliminate invalid zero calculations and verify the effectiveness of the operation for further deployment on FPGA. A massive variety of experiments demonstrate the effectiveness of KEP on two datasets: CIFAR-10 and ImageNet. Specially, with few indexes of non-zero weights introduced, KEP has a significant improvement over the latest structured methods in terms of parameter and float-point operation (FLOPs) reduction, and performs well on large datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006324",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Combinatorics",
      "Computer science",
      "Convolutional neural network",
      "FLOPS",
      "Inference",
      "Kernel (algebra)",
      "Machine learning",
      "Mathematics",
      "Parallel computing",
      "Pruning",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Peng"
      },
      {
        "surname": "Zhao",
        "given_name": "Liang"
      },
      {
        "surname": "Tian",
        "given_name": "Cong"
      },
      {
        "surname": "Duan",
        "given_name": "Zhenhua"
      }
    ]
  },
  {
    "title": "Deep graph representation learning for influence maximization with accelerated inference",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106649",
    "abstract": "Selecting a set of initial users from a social network in order to maximize the envisaged number of influenced users is known as influence maximization (IM). Researchers have achieved significant advancements in the theoretical design and performance gain of several classical approaches, but these advances are almost reaching their pinnacle. Learning-based IM approaches have emerged recently with a higher generalization to unknown graphs than conventional methods. The development of learning-based IM methods is still constrained by a number of fundamental hardships, including (1) solving the objective function efficiently, (2) struggling to characterize the diverse underlying diffusion patterns, and (3) adapting the solution to different node-centrality-constrained IM variants. To address the aforementioned issues, we design a novel framework DeepIM for generatively characterizing the latent representation of seed sets, as well as learning the diversified information diffusion pattern in a data-driven and end-to-end way. Subsequently, we design a novel objective function to infer optimal seed sets under flexible node-centrality-based budget constraints. Extensive analyses are conducted over both synthetic and real-world datasets to demonstrate the overall performance of DeepIM.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005732",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Centrality",
      "Combinatorics",
      "Computer science",
      "Engineering",
      "Evolutionary biology",
      "Feature learning",
      "Function (biology)",
      "Generalization",
      "Graph",
      "Inference",
      "Law",
      "Machine learning",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Node (physics)",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Set (abstract data type)",
      "Social graph",
      "Social media",
      "Structural engineering",
      "Theoretical computer science",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Chowdhury",
        "given_name": "Tanmoy"
      },
      {
        "surname": "Ling",
        "given_name": "Chen"
      },
      {
        "surname": "Jiang",
        "given_name": "Junji"
      },
      {
        "surname": "Wang",
        "given_name": "Junxiang"
      },
      {
        "surname": "Thai",
        "given_name": "My T."
      },
      {
        "surname": "Zhao",
        "given_name": "Liang"
      }
    ]
  },
  {
    "title": "GEPAF: A non-monotonic generalized activation function in neural network for improving prediction with diverse data distributions characteristics",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106738",
    "abstract": "The world today has made prescriptive analytics that uses data-driven insights to guide future actions. The distribution of data, however, differs depending on the scenario, making it difficult to interpret and comprehend the data efficiently. Different neural network models are used to solve this, taking inspiration from the complex network architecture in the human brain. The activation function is crucial in introducing non-linearity to process data gradients effectively. Although popular activation functions such as ReLU, Sigmoid, Swish, and Tanh have advantages and disadvantages, they may struggle to adapt to diverse data characteristics. A generalized activation function named the Generalized Exponential Parametric Activation Function (GEPAF) is proposed to address this issue. This function consists of three parameters expressed: α , which stands for a differencing factor similar to the mean; σ , which stands for a variance to control distribution spread; and p , which is a power factor that improves flexibility; all these parameters are present in the exponent. When p = 2 , the activation function resembles a Gaussian function. Initially, this paper describes the mathematical derivation and validation of the properties of this function mathematically and graphically. After this, the GEPAF function is practically implemented in real-world supply chain datasets. One dataset features a small sample size but exhibits high variance, while the other shows significant variance with a moderate amount of data. An LSTM network processes the dataset for sales and profit prediction. The suggested function performs better than popular activation functions when a comparative analysis of the activation function is performed, showing at least 30% improvement in regression evaluation metrics and better loss decay characteristics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006622",
    "keywords": [
      "Activation function",
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Evolutionary biology",
      "Function (biology)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Monotonic function"
    ],
    "authors": [
      {
        "surname": "Attarde",
        "given_name": "Khush"
      },
      {
        "surname": "Sayyad",
        "given_name": "Javed"
      }
    ]
  },
  {
    "title": "Illumination-aware divide-and-conquer network for improperly-exposed image enhancement",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106733",
    "abstract": "Improperly-exposed images often have unsatisfactory visual characteristics like inadequate illumination, low contrast, and the loss of small structures and details. The mapping relationship from an improperly-exposed condition to a well-exposed one may vary significantly due to the presence of multiple exposure conditions. Consequently, the enhancement methods that do not pay specific attention to this issue tend to yield inconsistent results when applied to the same scene under different exposure conditions. In order to obtain consistent enhancement results for various exposures while restoring rich details, we propose an illumination-aware divide-and-conquer network (IDNet). Specifically, to address the challenge of directly learning a sophisticated nonlinear mapping from an improperly-exposed condition to a well-exposed one, we utilize the discrete wavelet transform (DWT) to decompose the image into the low-frequency (LF) component, which primarily captures brightness and contrast, and the high-frequency (HF) components that depict fine-scale structures. To mitigate the inconsistency in correction across various exposures, we extract a conditional feature from the input that represents illumination-related global information. This feature is then utilized to modulate the dynamic convolution weights, enabling precise correction of the LF component. Furthermore, as the co-located positions of LF and HF components are highly correlated, we create a mask to distill useful knowledge from the corrected LF component, and integrate it into the HF component to support the restoration of fine-scale details. Extensive experimental results demonstrate that the proposed IDNet is superior to several state-of-the-art enhancement methods on two datasets with multiple exposures.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006579",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Divide and conquer algorithms",
      "Image (mathematics)",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Han",
        "given_name": "Fenggang"
      },
      {
        "surname": "Chang",
        "given_name": "Kan"
      },
      {
        "surname": "Li",
        "given_name": "Guiqing"
      },
      {
        "surname": "Ling",
        "given_name": "Mingyang"
      },
      {
        "surname": "Huang",
        "given_name": "Mengyuan"
      },
      {
        "surname": "Gao",
        "given_name": "Zan"
      }
    ]
  },
  {
    "title": "Generalized M -sparse algorithms for constructing fault tolerant RBF networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106633",
    "abstract": "In the construction process of radial basis function (RBF) networks, two common crucial issues arise: the selection of RBF centers and the effective utilization of the given source without encountering the overfitting problem. Another important issue is the fault tolerant capability. That is, when noise or faults exist in a trained network, it is crucial that the network’s performance does not undergo significant deterioration or decrease. However, without employing a fault tolerant procedure, a trained RBF network may exhibit significantly poor performance. Unfortunately, most existing algorithms are unable to simultaneously address all of the aforementioned issues. This paper proposes fault tolerant training algorithms that can simultaneously select RBF nodes and train RBF output weights. Additionally, our algorithms can directly control the number of RBF nodes in an explicit manner, eliminating the need for a time-consuming procedure to tune the regularization parameter and achieve the target RBF network size. Based on simulation results, our algorithms demonstrate improved test set performance when more RBF nodes are used, effectively utilizing the given source without encountering the overfitting problem. This paper first defines a fault tolerant objective function, which includes a term to suppress the effects of weight faults and weight noise. This term also prevents the issue of overfitting, resulting in better test set performance when more RBF nodes are utilized. With the defined objective function, the training process is designed to solve a generalized M -sparse problem by incorporating an ℓ 0 -norm constraint. The ℓ 0 -norm constraint allows us to directly and explicitly control the number of RBF nodes. To address the generalized M -sparse problem, we introduce the noise-resistant iterative hard thresholding (NR-IHT) algorithm. The convergence properties of the NR-IHT algorithm are subsequently discussed theoretically. To further enhance performance, we incorporate the momentum concept into the NR-IHT algorithm, referring to the modified version as “NR-IHT-Mom”. Simulation results show that both the NR-IHT algorithm and the NR-IHT-Mom algorithm outperform several state-of-the-art comparison algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005574",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Distributed computing",
      "Fault tolerance",
      "Machine learning",
      "Overfitting",
      "Radial basis function"
    ],
    "authors": [
      {
        "surname": "Wong",
        "given_name": "Hiu-Tung"
      },
      {
        "surname": "Mai",
        "given_name": "Jiajie"
      },
      {
        "surname": "Wang",
        "given_name": "Zhenni"
      },
      {
        "surname": "Leung",
        "given_name": "Chi-Sing"
      }
    ]
  },
  {
    "title": "A surrogate-assisted extended generative adversarial network for parameter optimization in free-form metasurface design",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106654",
    "abstract": "Metasurfaces have widespread applications in fifth-generation (5G) microwave communication. Among the metasurface family, free-form metasurfaces excel in achieving intricate spectral responses compared to regular-shape counterparts. However, conventional numerical methods for free-form metasurfaces are time-consuming and demand specialized expertise. Alternatively, recent studies demonstrate that deep learning has great potential to accelerate and refine metasurface designs. Here, we present XGAN, an extended generative adversarial network (GAN) with a surrogate for high-quality free-form metasurface designs. The proposed surrogate provides a physical constraint to XGAN so that XGAN can accurately generate metasurfaces monolithically from input spectral responses. In comparative experiments involving 20000 free-form metasurface designs, XGAN achieves 0.9734 average accuracy and is 500 times faster than the conventional methodology. This method facilitates the metasurface library building for specific spectral responses and can be extended to various inverse design problems, including optical metamaterials, nanophotonic devices, and drug discovery.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005781",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Constraint (computer-aided design)",
      "Deep learning",
      "Electronic engineering",
      "Engineering",
      "Geometry",
      "Mathematics",
      "Metamaterial",
      "Optics",
      "Physics"
    ],
    "authors": [
      {
        "surname": "Dai",
        "given_name": "Manna"
      },
      {
        "surname": "Jiang",
        "given_name": "Yang"
      },
      {
        "surname": "Yang",
        "given_name": "Feng"
      },
      {
        "surname": "Chattoraj",
        "given_name": "Joyjit"
      },
      {
        "surname": "Xia",
        "given_name": "Yingzhi"
      },
      {
        "surname": "Xu",
        "given_name": "Xinxing"
      },
      {
        "surname": "Zhao",
        "given_name": "Weijiang"
      },
      {
        "surname": "Dao",
        "given_name": "My Ha"
      },
      {
        "surname": "Liu",
        "given_name": "Yong"
      }
    ]
  },
  {
    "title": "Improved region proposal network for enhanced few-shot object detection",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106699",
    "abstract": "Despite significant success of deep learning in object detection tasks, the standard training of deep neural networks requires access to a substantial quantity of annotated images across all classes. Data annotation is an arduous and time-consuming endeavor, particularly when dealing with infrequent objects. Few-shot object detection (FSOD) methods have emerged as a solution to the limitations of classic object detection approaches based on deep learning. FSOD methods demonstrate remarkable performance by achieving robust object detection using a significantly smaller amount of training data. A challenge for FSOD is that instances from novel classes that do not belong to the fixed set of training classes appear in the background and the base model may pick them up as potential objects. These objects behave similarly to label noise because they are classified as one of the training dataset classes, leading to FSOD performance degradation. We develop a semi-supervised algorithm to detect and then utilize these unlabeled novel objects as positive samples during the FSOD training stage to improve FSOD performance. Specifically, we develop a hierarchical ternary classification region proposal network (HTRPN) to localize the potential unlabeled novel objects and assign them new objectness labels to distinguish these objects from the base training dataset classes. Our improved hierarchical sampling strategy for the region proposal network (RPN) also boosts the perception ability of the object detection model for large objects. We test our approach and COCO and PASCAL VOC baselines that are commonly used in FSOD literature. Our experimental results indicate that our method is effective and outperforms the existing state-of-the-art (SOTA) FSOD methods. Our implementation is provided as a supplement to support reproducibility of the results https://github.com/zshanggu/HTRPN. 1 1 Early partial results of this work is presented in the 2023 ICCV Workshop on Visual Continual Learning (Shangguan and Rostami, 2023).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006233",
    "keywords": [
      "Annotation",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Deep learning",
      "Machine learning",
      "Object (grammar)",
      "Object detection",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Test set",
      "Training set"
    ],
    "authors": [
      {
        "surname": "Shangguan",
        "given_name": "Zeyu"
      },
      {
        "surname": "Rostami",
        "given_name": "Mohammad"
      }
    ]
  },
  {
    "title": "Towards a configurable and non-hierarchical search space for NAS",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106700",
    "abstract": "Neural Architecture Search (NAS) outperforms handcrafted Neural Network (NN) design. However, current NAS methods generally use hard-coded search spaces, and predefined hierarchical architectures. As a consequence, adapting them to a new problem can be cumbersome, and it is hard to know which of the NAS algorithm or the predefined hierarchical structure impacts performance the most. To improve flexibility, and be less reliant on expert knowledge, this paper proposes a NAS methodology in which the search space is easily customizable, and allows for full network search. NAS is performed with Gaussian Process (GP)-based Bayesian Optimization (BO) in a continuous architecture embedding space. This embedding is built upon a Wasserstein Autoencoder, regularized by both a Maximum Mean Discrepancy (MMD) penalization and a Fully Input Convex Neural Network (FICNN) latent predictor, trained to infer the parameter count of architectures. This paper first assesses the embedding’s suitability for optimization by solving 2 computationally inexpensive problems: minimizing the number of parameters, and maximizing a zero-shot accuracy proxy. Then, two variants of complexity-aware NAS are performed on CIFAR-10 and STL-10, based on two different search spaces, providing competitive NN architectures with limited model sizes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006245",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Bayesian optimization",
      "Computer science",
      "Data mining",
      "Embedding",
      "Flexibility (engineering)",
      "Gaussian",
      "Gaussian process",
      "Machine learning",
      "Mathematics",
      "Physics",
      "Quantum mechanics",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Perrin",
        "given_name": "Mathieu"
      },
      {
        "surname": "Guicquero",
        "given_name": "William"
      },
      {
        "surname": "Paille",
        "given_name": "Bruno"
      },
      {
        "surname": "Sicard",
        "given_name": "Gilles"
      }
    ]
  },
  {
    "title": "The vitals for steady nucleation maps of spontaneous spiking coherence in autonomous two-dimensional neuronal networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106589",
    "abstract": "Thin pancake-like neuronal networks cultured on top of a planar microelectrode array have been extensively tried out in neuroengineering, as a substrate for the mobile robot’s control unit, i.e., as a cyborg’s brain. Most of these attempts failed due to intricate self-organizing dynamics in the neuronal systems. In particular, the networks may exhibit an emergent spatial map of steady nucleation sites (“n-sites”) of spontaneous population spikes. Being unpredictable and independent of the surface electrode locations, the n-sites drastically change local ability of the network to generate spikes. Here, using a spiking neuronal network model with generative spatially-embedded connectome, we systematically show in simulations that the number, location, and relative activity of spontaneously formed n-sites (“the vitals”) crucially depend on the samplings of three distributions: (1) the network distribution of neuronal excitability, (2) the distribution of connections between neurons of the network, and (3) the distribution of maximal amplitudes of a single synaptic current pulse. Moreover, blocking the dynamics of a small fraction (about 4%) of non-pacemaker neurons having the highest excitability was enough to completely suppress the occurrence of population spikes and their n-sites. This key result is explained theoretically. Remarkably, the n-sites occur taking into account only short-term synaptic plasticity, i.e., without a Hebbian-type plasticity. As the spiking network model used in this study is strictly deterministic, all simulation results can be accurately reproduced. The model, which has already demonstrated a very high richness-to-complexity ratio, can also be directly extended into the three-dimensional case, e.g., for targeting peculiarities of spiking dynamics in cerebral (or brain) organoids. We recommend the model as an excellent illustrative tool for teaching network-level computational neuroscience, complementing a few benchmark models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005136",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biological neural network",
      "Biological system",
      "Biology",
      "Computer science",
      "Demography",
      "Electrode",
      "Hebbian theory",
      "Machine learning",
      "Microelectrode",
      "Multielectrode array",
      "Neuroscience",
      "Physics",
      "Population",
      "Quantum mechanics",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Zendrikov",
        "given_name": "Dmitrii"
      },
      {
        "surname": "Paraskevov",
        "given_name": "Alexander"
      }
    ]
  },
  {
    "title": "Universal approximation theorem for vector- and hypercomplex-valued neural networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106632",
    "abstract": "The universal approximation theorem states that a neural network with one hidden layer can approximate continuous functions on compact sets with any desired precision. This theorem supports using neural networks for various applications, including regression and classification tasks. Furthermore, it is valid for real-valued neural networks and some hypercomplex-valued neural networks such as complex-, quaternion-, tessarine-, and Clifford-valued neural networks. However, hypercomplex-valued neural networks are a type of vector-valued neural network defined on an algebra with additional algebraic or geometric properties. This paper extends the universal approximation theorem for a wide range of vector-valued neural networks, including hypercomplex-valued models as particular instances. Precisely, we introduce the concept of non-degenerate algebra and state the universal approximation theorem for neural networks defined on such algebras.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005562",
    "keywords": [
      "Algebra over a field",
      "Algebraic number",
      "Algebraic operation",
      "Artificial intelligence",
      "Artificial neural network",
      "Clifford algebra",
      "Composite material",
      "Computer science",
      "Geometry",
      "Hypercomplex number",
      "Materials science",
      "Mathematical analysis",
      "Mathematics",
      "Pure mathematics",
      "Quaternion",
      "Range (aeronautics)"
    ],
    "authors": [
      {
        "surname": "Valle",
        "given_name": "Marcos Eduardo"
      },
      {
        "surname": "Vital",
        "given_name": "Wington L."
      },
      {
        "surname": "Vieira",
        "given_name": "Guilherme"
      }
    ]
  },
  {
    "title": "Moving sampling physics-informed neural networks induced by moving mesh PDE",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106706",
    "abstract": "In this work, we propose an end-to-end adaptive sampling framework based on deep neural networks and the moving mesh method (MMPDE-Net), which can adaptively generate new sampling points by solving the moving mesh PDE. This model focuses on improving the quality of sampling points generation. Moreover, we develop an iterative algorithm based on MMPDE-Net, which makes sampling points distribute more precisely and controllably. Since MMPDE-Net is independent of the deep learning solver, we combine it with physics-informed neural networks (PINN) to propose moving sampling PINN (MS-PINN) and show the error estimate of our method under some assumptions. Finally, we demonstrate the performance improvement of MS-PINN compared to PINN through numerical experiments of four typical examples, which numerically verify the effectiveness of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006300",
    "keywords": [
      "Adaptive sampling",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Filter (signal processing)",
      "Mathematical optimization",
      "Mathematics",
      "Monte Carlo method",
      "Programming language",
      "Sampling (signal processing)",
      "Solver",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Yu"
      },
      {
        "surname": "Yang",
        "given_name": "Qihong"
      },
      {
        "surname": "Deng",
        "given_name": "Yangtao"
      },
      {
        "surname": "He",
        "given_name": "Qiaolin"
      }
    ]
  },
  {
    "title": "How adversarial attacks can disrupt seemingly stable accurate classifiers",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106711",
    "abstract": "Adversarial attacks dramatically change the output of an otherwise accurate learning system using a seemingly inconsequential modification to a piece of input data. Paradoxically, empirical evidence indicates that even systems which are robust to large random perturbations of the input data remain susceptible to small, easily constructed, adversarial perturbations of their inputs. Here, we show that this may be seen as a fundamental feature of classifiers working with high dimensional input data. We introduce a simple generic and generalisable framework for which key behaviours observed in practical systems arise with high probability—notably the simultaneous susceptibility of the (otherwise accurate) model to easily constructed adversarial attacks, and robustness to random perturbations of the input data. We confirm that the same phenomena are directly observed in practical neural networks trained on standard image classification problems, where even large additive random noise fails to trigger the adversarial instability of the network. A surprising takeaway is that even small margins separating a classifier’s decision surface from training and testing data can hide adversarial susceptibility from being detected using randomly sampled perturbations. Counter-intuitively, using additive noise during training or testing is therefore inefficient for eradicating or detecting adversarial examples, and more demanding adversarial training is required.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400635X",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Machine learning",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Sutton",
        "given_name": "Oliver J."
      },
      {
        "surname": "Zhou",
        "given_name": "Qinghua"
      },
      {
        "surname": "Tyukin",
        "given_name": "Ivan Y."
      },
      {
        "surname": "Gorban",
        "given_name": "Alexander N."
      },
      {
        "surname": "Bastounis",
        "given_name": "Alexander"
      },
      {
        "surname": "Higham",
        "given_name": "Desmond J."
      }
    ]
  },
  {
    "title": "DiagSWin: A multi-scale vision transformer with diagonal-shaped windows for object detection and segmentation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106653",
    "abstract": "Recently, Vision Transformer and its variants have demonstrated remarkable performance on various computer vision tasks, thanks to its competence in capturing global visual dependencies through self-attention. However, global self-attention suffers from high computational cost due to quadratic computational overhead, especially for the high-resolution vision tasks (e.g., object detection and semantic segmentation). Many recent works have attempted to reduce the cost by applying fine-grained local attention, but these approaches cripple the long-range modeling power of the original self-attention mechanism. Furthermore, these approaches usually have similar receptive fields within each layer, thus limiting the ability of each self-attention layer to capture multi-scale features, resulting in performance degradation when handling images with objects of different scales. To address these issues, we develop the Diagonal-shaped Window (DiagSWin) attention mechanism for modeling attentions in diagonal regions at hybrid scales per attention layer. The key idea of DiagSWin attention is to inject multi-scale receptive field sizes into tokens: before computing the self-attention matrix, each token attends its closest surrounding tokens at fine granularity and the tokens far away at coarse granularity. This mechanism is able to effectively capture multi-scale context information while reducing computational complexity. With DiagSwin attention, we present a new variant of Vision Transformer models, called DiagSWin Transformers, and demonstrate their superiority in extensive experiments across various tasks. Specifically, the DiagSwin Transformer with a large size achieves 84.4% Top-1 accuracy and outperforms the SOTA CSWin Transformer on ImageNet with 40% fewer model size and computation cost. When employed as backbones, DiagSWin Transformers achieve significant improvements over the current SOTA modules. In addition, our DiagSWin-Base model yields 51.1 box mAP and 45.8 mask mAP on COCO for object detection and segmentation, and 52.3 mIoU on the ADE20K for semantic segmentation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400577X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computation",
      "Computer science",
      "Computer vision",
      "Granularity",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Rendering (computer graphics)",
      "Segmentation",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Ke"
      },
      {
        "surname": "Wang",
        "given_name": "Di"
      },
      {
        "surname": "Liu",
        "given_name": "Gang"
      },
      {
        "surname": "Zhu",
        "given_name": "Wenxuan"
      },
      {
        "surname": "Zhong",
        "given_name": "Haodi"
      },
      {
        "surname": "Wang",
        "given_name": "Quan"
      }
    ]
  },
  {
    "title": "Attention-based stackable graph convolutional network for multi-view learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106648",
    "abstract": "In multi-view learning, graph-based methods like Graph Convolutional Network (GCN) are extensively researched due to effective graph processing capabilities. However, most GCN-based methods often require complex preliminary operations such as sparsification, which may bring additional computation costs and training difficulties. Additionally, as the number of stacking layers increases in most GCN, over-smoothing problem arises, resulting in ineffective utilization of GCN capabilities. In this paper, we propose an attention-based stackable graph convolutional network that captures consistency across views and combines attention mechanism to exploit the powerful aggregation capability of GCN to effectively mitigate over-smoothing. Specifically, we introduce node self-attention to establish dynamic connections between nodes and generate view-specific representations. To maintain cross-view consistency, a data-driven approach is devised to assign attention weights to views, forming a common representation. Finally, based on residual connectivity, we apply an attention mechanism to the original projection features to generate layer-specific complementarity, which compensates for the information loss during graph convolution. Comprehensive experimental results demonstrate that the proposed method outperforms other state-of-the-art methods in multi-view semi-supervised tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005720",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Exploit",
      "Graph",
      "Machine learning",
      "Smoothing",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Zhiyong"
      },
      {
        "surname": "Chen",
        "given_name": "Weibin"
      },
      {
        "surname": "Zou",
        "given_name": "Ying"
      },
      {
        "surname": "Fang",
        "given_name": "Zihan"
      },
      {
        "surname": "Wang",
        "given_name": "Shiping"
      }
    ]
  },
  {
    "title": "GlobalSR: Global context network for single image super-resolution via deformable convolution attention and fast Fourier convolution",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106686",
    "abstract": "Vision Transformer have achieved impressive performance in image super-resolution. However, they suffer from low inference speed mainly because of the quadratic complexity of multi-head self-attention (MHSA), which is the key to learning long-range dependencies. On the contrary, most CNN-based methods neglect the important effect of global contextual information, resulting in inaccurate and blurring details. If one can make the best of both Transformers and CNNs, it will achieve a better trade-off between image quality and inference speed. Based on this observation, firstly assume that the main factor affecting the performance in the Transformer-based SR models is the general architecture design, not the specific MHSA component. To verify this, some ablation studies are made by replacing MHSA with large kernel convolutions, alongside other essential module replacements. Surprisingly, the derived models achieve competitive performance. Therefore, a general architecture design GlobalSR is extracted by not specifying the core modules including blocks and domain embeddings of Transformer-based SR models. It also contains three practical guidelines for designing a lightweight SR network utilizing image-level global contextual information to reconstruct SR images. Following the guidelines, the blocks and domain embeddings of GlobalSR are instantiated via Deformable Convolution Attention Block (DCAB) and Fast Fourier Convolution Domain Embedding (FCDE), respectively. The instantiation of general architecture, termed GlobalSR-DF, proposes a DCA to extract the global contextual feature by utilizing Deformable Convolution and a Hadamard product as the attention map at the block level. Meanwhile, the FCDE utilizes the Fast Fourier to transform the input spatial feature into frequency space and then extract image-level global information from it by convolutions. Extensive experiments demonstrate that GlobalSR is the key part in achieving a superior trade-off between SR quality and efficiency. Specifically, our proposed GlobalSR-DF outperforms state-of-the-art CNN-based and ViT-based SISR models regarding accuracy-speed trade-offs with sharp and natural details.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006105",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Convolution (computer science)",
      "Embedding",
      "Fourier transform",
      "Inference",
      "Kernel (algebra)",
      "Mathematical analysis",
      "Mathematics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Qiangpu"
      },
      {
        "surname": "Wen",
        "given_name": "Wushao"
      },
      {
        "surname": "Qin",
        "given_name": "Jinghui"
      }
    ]
  },
  {
    "title": "ECCT: Efficient Contrastive Clustering via Pseudo-Siamese Vision Transformer and Multi-view Augmentation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106684",
    "abstract": "Image clustering aims to divide a set of unlabeled images into multiple clusters. Recently, clustering methods based on contrastive learning have attracted much attention due to their ability to learn discriminative feature representations. Nevertheless, existing clustering algorithms face challenges in capturing global information and preserving semantic continuity. Additionally, these methods often exhibit relatively singular feature distributions, limiting the full potential of contrastive learning in clustering. These problems can have a negative impact on the performance of image clustering. To address the above problems, we propose a deep clustering framework termed Efficient Contrastive Clustering via Pseudo-Siamese Vision Transformer and Multi-view Augmentation (ECCT). The core idea is to introduce Vision Transformer (ViT) to provide the global view, and improve it with Hilbert Patch Embedding (HPE) module to construct a new ViT branch. Finally, we fuse the features extracted from the two ViT branches to obtain both global view and semantic coherence. In addition, we employ multi-view random aggressive augmentation to broaden the feature distribution, enabling the model to learn more comprehensive and richer contrastive features. Our results on five datasets demonstrate that ECCT outperforms previous clustering methods. In particular, the ARI metric of ECCT on the STL-10 (ImageNet-Dogs) dataset is 0.852 (0.424), which is 10.3% (4.8%) higher than the best baseline.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006087",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Correlation clustering",
      "Discriminative model",
      "Embedding",
      "Feature learning",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Xing"
      },
      {
        "surname": "Hu",
        "given_name": "Taizhang"
      },
      {
        "surname": "Wu",
        "given_name": "Di"
      },
      {
        "surname": "Yang",
        "given_name": "Fan"
      },
      {
        "surname": "Zhao",
        "given_name": "Chong"
      },
      {
        "surname": "Lu",
        "given_name": "Yang"
      }
    ]
  },
  {
    "title": "Backdoor attacks on unsupervised graph representation learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106668",
    "abstract": "Unsupervised graph learning techniques have garnered increasing interest among researchers. These methods employ the technique of maximizing mutual information to generate representations of nodes and graphs. We show that these methods are susceptible to backdoor attacks, wherein the adversary can poison a small portion of unlabeled graph data (e.g., node features and graph structure) by introducing triggers into the graph. This tampering disrupts the representations and increases the risk to various downstream applications. Previous backdoor attacks in supervised learning primarily operate directly on the label space and may not be suitable for unlabeled graph data. To tackle this challenge, we introduce GRBA, 1 1 https://github.com/fbd3/GRBA.git. a gradient-based first-order backdoor attack method. To the best of our knowledge, this constitutes a pioneering endeavor in investigating backdoor attacks within the domain of unsupervised graph learning. The initiation of this method does not necessitate prior knowledge of downstream tasks, as it directly focuses on representations. Furthermore, it is versatile and can be applied to various downstream tasks, including node classification, node clustering and graph classification. We evaluate GRBA on state-of-the-art unsupervised learning models, and the experimental results substantiate the effectiveness and evasiveness of GRBA in both node-level and graph-level tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005926",
    "keywords": [
      "Adversary",
      "Artificial intelligence",
      "Backdoor",
      "Computer science",
      "Computer security",
      "Graph",
      "Machine learning",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Feng",
        "given_name": "Bingdao"
      },
      {
        "surname": "Jin",
        "given_name": "Di"
      },
      {
        "surname": "Wang",
        "given_name": "Xiaobao"
      },
      {
        "surname": "Cheng",
        "given_name": "Fangyu"
      },
      {
        "surname": "Guo",
        "given_name": "Siqi"
      }
    ]
  },
  {
    "title": "Learning clustering-friendly representations via partial information discrimination and cross-level interaction",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106696",
    "abstract": "Despite significant advances in the deep clustering research, there remain three critical limitations to most of the existing approaches. First, they often derive the clustering result by associating some distribution-based loss to specific network layers, neglecting the potential benefits of leveraging the contrastive sample-wise relationships. Second, they frequently focus on representation learning at the full-image scale, overlooking the discriminative information latent in partial image regions. Third, although some prior studies perform the learning process at multiple levels, they mostly lack the ability to exploit the interaction between different learning levels. To overcome these limitations, this paper presents a novel deep image clustering approach via Partial Information discrimination and Cross-level Interaction (PICI). Specifically, we utilize a Transformer encoder as the backbone, coupled with two types of augmentations to formulate two parallel views. The augmented samples, integrated with masked patches, are processed through the Transformer encoder to produce the class tokens. Subsequently, three partial information learning modules are jointly enforced, namely, the partial information self-discrimination (PISD) module for masked image reconstruction, the partial information contrastive discrimination (PICD) module for the simultaneous instance- and cluster-level contrastive learning, and the cross-level interaction (CLI) module to ensure the consistency across different learning levels. Through this unified formulation, our PICI approach for the first time, to our knowledge, bridges the gap between the masked image modeling and the deep contrastive clustering, offering a novel pathway for enhanced representation learning and clustering. Experimental results across six image datasets demonstrate the superiority of our PICI approach over the state-of-the-art. In particular, our approach achieves an ACC of 0.772 (0.634) on the RSOD (UC-Merced) dataset, which shows an improvement of 29.7% (24.8%) over the best baseline. The source code is available at https://github.com/Regan-Zhang/PICI.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006208",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Cluster analysis",
      "Computer science",
      "Computer security",
      "Consistency (knowledge bases)",
      "Deep learning",
      "Discriminative model",
      "Encoder",
      "Exploit",
      "Feature learning",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Hai-Xin"
      },
      {
        "surname": "Huang",
        "given_name": "Dong"
      },
      {
        "surname": "Ling",
        "given_name": "Hua-Bao"
      },
      {
        "surname": "Sun",
        "given_name": "Weijun"
      },
      {
        "surname": "Wen",
        "given_name": "Zihao"
      }
    ]
  },
  {
    "title": "Near-optimal deep neural network approximation for Korobov functions with respect to L p and H 1 norms",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106702",
    "abstract": "This paper derives the optimal rate of approximation for Korobov functions with deep neural networks in the high dimensional hypercube with respect to L p -norms and H 1 -norm. Our approximation bounds are non-asymptotic in both the width and depth of the networks. The obtained approximation rates demonstrate a remarkable super-convergence feature, improving the existing convergence rates of neural networks that are continuous function approximators. Finally, using a VC-dimension argument, we show that the established rates are near-optimal.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006269",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Discrete mathematics",
      "Hypercube",
      "Mathematics"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Yahong"
      },
      {
        "surname": "Lu",
        "given_name": "Yulong"
      }
    ]
  },
  {
    "title": "Sampled-data synchronization for fuzzy inertial cellular neural networks and its application in secure communication",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106671",
    "abstract": "This paper designs the sampled-data control (SDC) scheme to delve into the synchronization problem of fuzzy inertial cellular neural networks (FICNNs). Technically, the rate at which the information or activation of cellular neuronal transmission made can be described in a first-order differential model, but the network response concerning the received information may be dependent on time that can be modeled as a second-order (inertial) cellular neural network (ICNN) model. Generally, a fuzzy cellular neural network (FCNN) is a combination of fuzzy logic and a cellular neural network. Fuzzy logic models are composed of input and output templates which are in the form of a sum of product operations that help to evaluate the information transmission on a rule-basis. Hence, this study proposes a user-controlled FICNNs model with the same dynamic properties as FICNN model. In this regard, the synchronization approach is considerably effective in ensuring the dynamical properties of the drive (without control input) and response (with external control input). Theoretically, the synchronization between the drive-response can be ensured by analyzing the error model derived from the drive-response but due to nonlinearities, the Lyapunov stability theory can be utilized to derive sufficient stability conditions in terms of linear matrix inequalities (LMIs) that will guarantee the convergence of the error model onto the origin. Distinct from the existing stability conditions, this paper derives the stability conditions by involving the delay information in the form of a quadratic function with lower and upper bounds, which are evaluated through the negative determination lemma (NDL). Besides, numerical simulations that support the validation of proposed theoretical frameworks are discussed. As a direct application, the FICNN model is considered as a cryptosystem in image encryption and decryption algorithm, and the corresponding outcomes are illustrated along with security measures.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005951",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Cellular neural network",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Fuzzy logic",
      "Machine learning",
      "Stability (learning theory)",
      "Synchronization (alternating current)"
    ],
    "authors": [
      {
        "surname": "Subramaniam",
        "given_name": "Sasikala"
      },
      {
        "surname": "Mani",
        "given_name": "Prakash"
      }
    ]
  },
  {
    "title": "Multi-scale locality preserving projection for partial multi-view incomplete multi-label learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106748",
    "abstract": "Amidst advancements in feature extraction techniques, research on multi-view multi-label classifications has attracted widespread interest in recent years. However, real-world scenarios often pose a challenge where the completeness of multiple views and labels cannot be ensured. At present, only a handful of techniques have attempted to address the complex issue of partial multi-view incomplete multi-label classification, and the majority of these approaches overlook the significance of manifold structures between instances. To tackle these challenges, we propose a novel partial multi-view incomplete multi-label learning model, termed MSLPP. Differing from existing studies, MSLPP emphasizes retaining the effective inherent structure of data during the feature extraction process, thereby facilitating a richer semantic information extraction. Specifically, MSLPP captures and integrates four types of information: the distance and similarity information in the original feature space, and the distance and similarity information in the extracted feature space. Further, by adopting the graph embedding technique, it simultaneously preserves the intrinsic structure with multi-scale information through a constraint term. Moreover, taking into account the negative impact of the missing views on the model and the possible impact of missing views on the data inherent structure, we further propose a shielding strategy for missing views, which not only eliminates the negative effects of missing views on the model but also more accurately captures the inherent data structure. The experimental results on five widely recognized datasets indicate that the model performs better than many excellent methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006725",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Linguistics",
      "Locality",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Projection (relational algebra)",
      "Quantum mechanics",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Long",
        "given_name": "Jiang"
      },
      {
        "surname": "Zhang",
        "given_name": "Qi"
      },
      {
        "surname": "Lu",
        "given_name": "Xiaohuan"
      },
      {
        "surname": "Wen",
        "given_name": "Jie"
      },
      {
        "surname": "Zhao",
        "given_name": "Lian"
      },
      {
        "surname": "Xie",
        "given_name": "Wulin"
      }
    ]
  },
  {
    "title": "GFANC-RL: Reinforcement Learning-based Generative Fixed-filter Active Noise Control",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106687",
    "abstract": "The recent Generative Fixed-filter Active Noise Control (GFANC) method achieves a good trade-off between noise reduction performance and system stability. However, labelling noise data for training the Convolutional Neural Network (CNN) in GFANC is typically resource-consuming. Even worse, labelling errors will degrade the CNN’s filter-generation accuracy. Therefore, this paper proposes a novel Reinforcement Learning-based GFANC (GFANC-RL) approach that omits the labelling process by leveraging the exploring property of Reinforcement Learning (RL). The CNN’s parameters are automatically updated through the interaction between the RL agent and the environment. Moreover, the RL algorithm solves the non-differentiability issue caused by using binary combination weights in GFANC. Simulation results demonstrate the effectiveness and transferability of the GFANC-RL method in handling real-recorded noises across different acoustic paths. 2 2 The code will be accessible at https://github.com/Luo-Zhengding/GFANC-RL.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006117",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Filter (signal processing)",
      "Image (mathematics)",
      "Machine learning",
      "Noise (video)",
      "Noise reduction",
      "Reinforcement learning"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Zhengding"
      },
      {
        "surname": "Ma",
        "given_name": "Haozhe"
      },
      {
        "surname": "Shi",
        "given_name": "Dongyuan"
      },
      {
        "surname": "Gan",
        "given_name": "Woon-Seng"
      }
    ]
  },
  {
    "title": "Bidirectional consistency with temporal-aware for semi-supervised time series classification",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106709",
    "abstract": "Semi-supervised learning (SSL) has achieved significant success due to its capacity to alleviate annotation dependencies. Most existing SSL methods utilize pseudo-labeling to propagate useful supervised information for training unlabeled data. However, these methods ignore learning temporal representations, making it challenging to obtain a well-separable feature space for modeling explicit class boundaries. In this work, we propose a semi-supervised Time Series classification framework via Bidirectional Consistency with Temporal-aware (TS-BCT), which regularizes the feature space distribution by learning temporal representations through pseudo-label-guided contrastive learning. Specifically, TS-BCT utilizes time-specific augmentation to transform the entire raw time series into two distinct views, avoiding sampling bias. The pseudo-labels for each view, generated through confidence estimation in the feature space, are then employed to propagate class-related information into unlabeled samples. Subsequently, we introduce a temporal-aware contrastive learning module that learns discriminative temporal-invariant representations. Finally, we design a bidirectional consistency strategy by incorporating pseudo-labels from two distinct views into temporal-aware contrastive learning to construct a class-related contrastive pattern. This strategy enables the model to learn well-separated feature spaces, making class boundaries more discriminative. Extensive experimental results on real-world datasets demonstrate the effectiveness of TS-BCT compared to baselines.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006336",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Consistency (knowledge bases)",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature learning",
      "Feature vector",
      "Law",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Han"
      },
      {
        "surname": "Zhang",
        "given_name": "Fengbin"
      },
      {
        "surname": "Huang",
        "given_name": "Xunhua"
      },
      {
        "surname": "Wang",
        "given_name": "Ruidong"
      },
      {
        "surname": "Xi",
        "given_name": "Liang"
      }
    ]
  },
  {
    "title": "Multistability and fixed-time multisynchronization of switched neural networks with state-dependent switching rules",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106713",
    "abstract": "This paper presents theoretical results on the multistability and fixed-time synchronization of switched neural networks with multiple almost-periodic solutions and state-dependent switching rules. It is shown herein that the number, location, and stability of the almost-periodic solutions of the switched neural networks can be characterized by making use of the state-space partition. Two sets of sufficient conditions are derived to ascertain the existence of 3 n exponentially stable almost-periodic solutions. Subsequently, this paper introduces the novel concept of fixed-time multisynchronization in switched neural networks associated with a range of almost-periodic parameters within multiple stable equilibrium states for the first time. Based on the multistability results, it is demonstrated that there are 3 n synchronization manifolds, wherein n is the number of neurons. Additionally, an estimation for the settling time required for drive–response switched neural networks to achieve synchronization is provided. It should be noted that this paper considers stable equilibrium points (static multisynchronization), stable almost-periodic orbits (dynamical multisynchronization), and hybrid stable equilibrium states (hybrid multisynchronization) as special cases of multistability (multisynchronization). Two numerical examples are elaborated to substantiate the theoretical results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006373",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Machine learning",
      "Mathematics",
      "Multistability",
      "Nonlinear system",
      "Physics",
      "Quantum mechanics",
      "Stability (learning theory)",
      "State (computer science)",
      "State space",
      "Statistics",
      "Synchronization (alternating current)",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Ou",
        "given_name": "Shiqin"
      },
      {
        "surname": "Guo",
        "given_name": "Zhenyuan"
      },
      {
        "surname": "Wen",
        "given_name": "Shiping"
      },
      {
        "surname": "Huang",
        "given_name": "Tingwen"
      }
    ]
  },
  {
    "title": "Complete synchronization of discrete-time fractional-order BAM neural networks with leakage and discrete delays",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106705",
    "abstract": "This paper concerns complete synchronization (CS) problem of discrete-time fractional-order BAM neural networks (BAMNNs) with leakage and discrete delays. Firstly, on the basis of Caputo fractional difference theory and nabla l -Laplace transform, two equations about the nabla sum are strictly proved. Secondly, two extended Halanay inequalities that are suitable for discrete-time fractional difference inequations with arbitrary initial time and multiple types of delays are introduced. In addition, through applying Caputo fractional difference theory and combining with inequalities gained from this paper, some sufficient CS criteria of discrete-time fractional-order BAMNNs with leakage and discrete delays are established under adaptive controller. Finally, one numerical simulation is utilized to certify the effectiveness of the obtained theoretical results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006294",
    "keywords": [
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Discrete time and continuous time",
      "Economics",
      "Fractional calculus",
      "Laplace transform",
      "Leakage (economics)",
      "Machine learning",
      "Macroeconomics",
      "Mathematical analysis",
      "Mathematics",
      "Statistics",
      "Synchronization (alternating current)",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Jianfei"
      },
      {
        "surname": "Li",
        "given_name": "Hong-Li"
      },
      {
        "surname": "Hu",
        "given_name": "Cheng"
      },
      {
        "surname": "Jiang",
        "given_name": "Haijun"
      },
      {
        "surname": "Cao",
        "given_name": "Jinde"
      }
    ]
  },
  {
    "title": "SFT-SGAT: A semi-supervised fine-tuning self-supervised graph attention network for emotion recognition and consciousness detection",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106643",
    "abstract": "Emotional recognition is highly important in the field of brain-computer interfaces (BCIs). However, due to the individual variability in electroencephalogram (EEG) signals and the challenges in obtaining accurate emotional labels, traditional methods have shown poor performance in cross-subject emotion recognition. In this study, we propose a cross-subject EEG emotion recognition method based on a semi-supervised fine-tuning self-supervised graph attention network (SFT-SGAT). First, we model multi-channel EEG signals by constructing a graph structure that dynamically captures the spatiotemporal topological features of EEG signals. Second, we employ a self-supervised graph attention neural network to facilitate model training, mitigating the impact of signal noise on the model. Finally, a semi-supervised approach is used to fine-tune the model, enhancing its generalization ability in cross-subject classification. By combining supervised and unsupervised learning techniques, the SFT-SGAT maximizes the utility of limited labeled data in EEG emotion recognition tasks, thereby enhancing the model’s performance. Experiments based on leave-one-subject-out cross-validation demonstrate that SFT-SGAT achieves state-of-the-art cross-subject emotion recognition performance on the SEED and SEED-IV datasets, with accuracies of 92.04% and 82.76%, respectively. Furthermore, experiments conducted on a self-collected dataset comprising ten healthy subjects and eight patients with disorders of consciousness (DOCs) revealed that the SFT-SGAT attains high classification performance in healthy subjects (maximum accuracy of 95.84%) and was successfully applied to DOC patients, with four patients achieving emotion recognition accuracies exceeding 60%. The experiments demonstrate the effectiveness of the proposed SFT-SGAT model in cross-subject EEG emotion recognition and its potential for assessing levels of consciousness in patients with DOC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005677",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Consciousness",
      "Emotion recognition",
      "Graph",
      "Machine learning",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Psychology",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Qiu",
        "given_name": "Lina"
      },
      {
        "surname": "Zhong",
        "given_name": "Liangquan"
      },
      {
        "surname": "Li",
        "given_name": "Jianping"
      },
      {
        "surname": "Feng",
        "given_name": "Weisen"
      },
      {
        "surname": "Zhou",
        "given_name": "Chengju"
      },
      {
        "surname": "Pan",
        "given_name": "Jiahui"
      }
    ]
  },
  {
    "title": "Time-series domain adaptation via sparse associative structure alignment: Learning invariance and variance",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106659",
    "abstract": "Domain adaptation on time-series data, which is often encountered in the field of industry, like anomaly detection and sensor data forecasting, but received limited attention in academia, is an important but challenging task in real-world scenarios. Most of the existing methods for time-series data use the covariate shift assumption for non-time-series data to extract the domain-invariant representation, but this assumption is hard to meet in practice due to the complex dependence among variables and a small change of the time lags may lead to a huge change of future values. To address this challenge, we leverage the stableness of causal structures among different domains. To further avoid the strong assumptions in causal discovery like linear non-Gaussian assumption, we relax it to mine the stable sparse associative structures instead of discovering the causal structures directly. Besides the domain-invariant structures, we also find that some domain-specific information like the strengths of the structures is important for prediction. Based on the aforementioned intuition, we extend the sparse associative structure alignment model in the conference version to the Sparse Associative Structure Alignment model with domain-specific information enhancement (SASA2 in short), which aligns the invariant unweighted spare associative structures and considers the variant information for time-series unsupervised domain adaptation. Specifically, we first generate the segment set to exclude the obstacle of offsets. Second, we extract the unweighted sparse associative structures via sparse attention mechanisms. Third, we extract the domain-specific information via an autoregressive module. Finally, we employ a unidirectional alignment restriction to guide the transformation from the source to the target. Moreover, we further provide a generalization analysis to show the theoretical superiority of our method. Compared with existing methods, our method yields state-of-the-art performance, with a 5% relative improvement in three real-world datasets, covering different applications: air quality, in-hospital healthcare, and anomaly detection. Furthermore, visualization results of sparse associative structures illustrate what knowledge can be transferred, boosting the transparency and interpretability of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005835",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Associative property",
      "Computer science",
      "Invariant (physics)",
      "Leverage (statistics)",
      "Machine learning",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Zijian"
      },
      {
        "surname": "Cai",
        "given_name": "Ruichu"
      },
      {
        "surname": "Chen",
        "given_name": "Jiawei"
      },
      {
        "surname": "Yan",
        "given_name": "Yuguang"
      },
      {
        "surname": "Chen",
        "given_name": "Wei"
      },
      {
        "surname": "Zhang",
        "given_name": "Keli"
      },
      {
        "surname": "Ye",
        "given_name": "Junjian"
      }
    ]
  },
  {
    "title": "Finding core labels for maximizing generalization of graph neural networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106635",
    "abstract": "Graph neural networks (GNNs) have become a popular approach for semi-supervised graph representation learning. GNNs research has generally focused on improving methodological details, whereas less attention has been paid to exploring the importance of labeling the data. However, for semi-supervised learning, the quality of training data is vital. In this paper, we first introduce and elaborate on the problem of training data selection for GNNs. More specifically, focusing on node classification, we aim to select representative nodes from a graph used to train GNNs to achieve the best performance. To solve this problem, we are inspired by the popular lottery ticket hypothesis, typically used for sparse architectures, and we propose the following subset hypothesis for graph data: “There exists a core subset when selecting a fixed-size dataset from the dense training dataset, that can represent the properties of the dataset, and GNNs trained on this core subset can achieve a better graph representation”. Equipped with this subset hypothesis, we present an efficient algorithm to identify the core data in the graph for GNNs. Extensive experiments demonstrate that the selected data (as a training set) can obtain performance improvements across various datasets and GNNs architectures.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005598",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Core (optical fiber)",
      "Data mining",
      "Generalization",
      "Graph",
      "Labeled data",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Telecommunications",
      "Theoretical computer science",
      "Training set"
    ],
    "authors": [
      {
        "surname": "Fu",
        "given_name": "Sichao"
      },
      {
        "surname": "Ma",
        "given_name": "Xueqi"
      },
      {
        "surname": "Zhan",
        "given_name": "Yibing"
      },
      {
        "surname": "You",
        "given_name": "Fanyu"
      },
      {
        "surname": "Peng",
        "given_name": "Qinmu"
      },
      {
        "surname": "Liu",
        "given_name": "Tongliang"
      },
      {
        "surname": "Bailey",
        "given_name": "James"
      },
      {
        "surname": "Mandic",
        "given_name": "Danilo"
      }
    ]
  },
  {
    "title": "Bipartite secure synchronization criteria for coupled quaternion-valued neural networks with signed graph",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106717",
    "abstract": "This study explores the bipartite secure synchronization problem of coupled quaternion-valued neural networks (QVNNs), in which variable sampled communications and random deception attacks are considered. Firstly, by employing the signed graph theory, the mathematical model of coupled QVNNs with structurally-balanced cooperative–competitive interactions is established. Secondly, by adopting non-decomposition method and constructing a suitable unitary Lyapunov functional, the bipartite secure synchronization (BSS) criteria for coupled QVNNs are obtained in the form of quaternion-valued LMIs. It is essential to mention that the structurally-balanced topology is relatively strong, hence, the coupled QVNNs with structurally-unbalanced graph are further studied. The structurally-unbalanced graph is treated as an interruption of the structurally-balanced graph, the bipartite secure quasi-synchronization (BSQS) criteria for coupled QVNNs with structurally-unbalanced graph are derived. Finally, two simulations are given to illustrate the feasibility of the suggested BSS and BSQS approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006415",
    "keywords": [
      "Artificial intelligence",
      "Bipartite graph",
      "Combinatorics",
      "Computer science",
      "Geometry",
      "Graph",
      "Law",
      "Mathematics",
      "Political science",
      "Quaternion",
      "Signed graph",
      "Subspace topology",
      "Synchronization (alternating current)",
      "Theoretical computer science",
      "Topology (electrical circuits)",
      "Unitary state"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Ning"
      },
      {
        "surname": "Cao",
        "given_name": "Jinde"
      },
      {
        "surname": "Wang",
        "given_name": "Fei"
      }
    ]
  },
  {
    "title": "SNN-BERT: Training-efficient Spiking Neural Networks for energy-efficient BERT",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106630",
    "abstract": "Spiking Neural Networks (SNNs) are naturally suited to process sequence tasks such as NLP with low power, due to its brain-inspired spatio-temporal dynamics and spike-driven nature. Current SNNs employ ”repeat coding” that re-enter all input tokens at each timestep, which fails to fully exploit temporal relationships between the tokens and introduces memory overhead. In this work, we align the number of input tokens with the timestep and refer to this input coding as ”individual coding”. To cope with the increase in training time for individual encoded SNNs due to the dramatic increase in timesteps, we design a Bidirectional Parallel Spiking Neuron (BPSN) with following features: First, BPSN supports spike parallel computing and effectively avoids the issue of uninterrupted firing; Second, BPSN excels in handling adaptive sequence length tasks, which is a capability that existing work does not have; Third, the fusion of bidirectional information enhances the temporal information modeling capabilities of SNNs; To validate the effectiveness of our BPSN, we present the SNN-BERT, a deep direct training SNN architecture based on the BERT model in NLP. Compared to prior repeat 4-timestep coding baseline, our method achieves a 6.46 × reduction in energy consumption and a significant 16.1% improvement, raising the performance upper bound of the SNN domain on the GLUE dataset to 74.4%. Additionally, our method achieves 3.5 × training acceleration and 3.8 × training memory optimization. Compared with artificial neural networks of similar architecture, we obtain comparable performance but up to 22.5 × energy efficiency. We would provide the codes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005549",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Machine learning",
      "Meteorology",
      "Physics",
      "Spiking neural network",
      "Training (meteorology)"
    ],
    "authors": [
      {
        "surname": "Su",
        "given_name": "Qiaoyi"
      },
      {
        "surname": "Mei",
        "given_name": "Shijie"
      },
      {
        "surname": "Xing",
        "given_name": "Xingrun"
      },
      {
        "surname": "Yao",
        "given_name": "Man"
      },
      {
        "surname": "Zhang",
        "given_name": "Jiajun"
      },
      {
        "surname": "Xu",
        "given_name": "Bo"
      },
      {
        "surname": "Li",
        "given_name": "Guoqi"
      }
    ]
  },
  {
    "title": "Few-shot learning with representative global prototype",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106600",
    "abstract": "Few-shot learning is often challenged by low generalization performance due to the model is mostly learned with the base classes only. To mitigate the above issues, a few-shot learning method with representative global prototype is proposed in this paper. Specifically, to enhance generalization to novel class, we propose a strategy for jointly training base and novel classes. This process produces prototypes characterizing the class information called representative global prototypes. Additionally, to avoid the problem of data imbalance and prototype bias caused by newly added categories of sparse samples, a novel sample synthesis method is proposed for augmenting more representative samples of novel class. Finally, representative samples and non-representative samples with high uncertainty are selected to enhance the representational and discriminative abilities of the global prototype. Intensive experiments have been conducted on two popular benchmark datasets, and the experimental results show that this method significantly improves the classification ability of few-shot learning tasks and achieves state-of-the-art performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005240",
    "keywords": [
      "Artificial intelligence",
      "Base (topology)",
      "Benchmark (surveying)",
      "Chemistry",
      "Chromatography",
      "Class (philosophy)",
      "Computer science",
      "Data mining",
      "Discriminative model",
      "Generalization",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Sample (material)",
      "Shot (pellet)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yukun"
      },
      {
        "surname": "Shi",
        "given_name": "Daming"
      },
      {
        "surname": "Lin",
        "given_name": "Hexiu"
      }
    ]
  },
  {
    "title": "Neural dynamics and seizure correlations: Insights from neural mass models in a Tetanus Toxin rat model of epilepsy",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106746",
    "abstract": "This study focuses on the use of a neural mass model to investigate potential relationships between functional connectivity and seizure frequency in epilepsy. We fitted a three-layer neural mass model of a cortical column to intracranial EEG (iEEG) data from a Tetanus Toxin rat model of epilepsy, which also included responses to periodic electrical stimulation. Our results show that some of the connectivity weights between different neural populations correlate significantly with the number of seizures each day, offering valuable insights into the dynamics of neural circuits during epileptogenesis. We also simulated single-pulse electrical stimulation of the neuronal populations to observe their responses after the connectivity weights were optimized to fit background (non-seizure) EEG data. The recovery time, defined as the time from stimulation until the membrane potential returns to baseline, was measured as a representation of the critical slowing down phenomenon observed in nonlinear systems operating near a bifurcation boundary. The results revealed that recovery times in the responses of the computational model fitted to the EEG data were longer during 5 min periods preceding seizures compared to 1 hr before seizures in four out of six rats. Analysis of the iEEG recorded in response to electrical stimulation revealed results similar to the computational model in four out of six rats. This study supports the potential use of this computational model as a model-based biomarker for seizure prediction when direct electrical stimulation to the brain is not feasible.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006701",
    "keywords": [
      "Computer science",
      "Dynamics (music)",
      "Epilepsy",
      "Medicine",
      "Neuroscience",
      "Pathology",
      "Pedagogy",
      "Psychology",
      "Tetanus",
      "Vaccination"
    ],
    "authors": [
      {
        "surname": "Zarei Eskikand",
        "given_name": "Parvin"
      },
      {
        "surname": "Soto-Breceda",
        "given_name": "Artemio"
      },
      {
        "surname": "Cook",
        "given_name": "Mark J."
      },
      {
        "surname": "Burkitt",
        "given_name": "Anthony N."
      },
      {
        "surname": "Grayden",
        "given_name": "David B."
      }
    ]
  },
  {
    "title": "No-reference stereoscopic image quality assessment based on binocular collaboration",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106752",
    "abstract": "Stereoscopic images typically consist of left and right views along with depth information. Assessing the quality of stereoscopic/3D images (SIQA) is often more complex than that of 2D images due to scene disparities between the left and right views and the intricate process of fusion in binocular vision. To address the problem of quality prediction bias of multi-distortion images, we investigated the visual physiology and the processing of visual information by the primary visual cortex of the human brain and proposed a no-reference stereoscopic image quality evaluation method. The method mainly includes an innovative end-to-end NR-SIQA neural network with a picture patch generation algorithm. The algorithm generates a saliency map by fusing the left and right views and then guides the image cropping in the database based on the saliency map. The proposed models are validated and compared based on publicly available databases. The results show that the model and algorithm together outperform the state-of-the-art NR-SIQA metric in the LIVE 3D database and the WIVC 3D database, and have excellent results in the specific noise metric. The model generalization experiments demonstrate a certain degree of generality of our proposed model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006762",
    "keywords": [
      "Artificial intelligence",
      "Binocular disparity",
      "Computer science",
      "Computer vision",
      "Epistemology",
      "Image (mathematics)",
      "Image quality",
      "Philosophy",
      "Quality (philosophy)",
      "Stereoscopy"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Hanling"
      },
      {
        "surname": "Ke",
        "given_name": "Xiao"
      },
      {
        "surname": "Guo",
        "given_name": "Wenzhong"
      },
      {
        "surname": "Zheng",
        "given_name": "Wukun"
      }
    ]
  },
  {
    "title": "Object and spatial discrimination makes weakly supervised local feature better",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106697",
    "abstract": "Local feature extraction plays a crucial role in numerous critical visual tasks. However, there remains room for improvement in both descriptors and keypoints, particularly regarding the discriminative power of descriptors and the localization precision of keypoints. To address these challenges, this study introduces a novel local feature extraction pipeline named OSDFeat (Object and Spatial Discrimination Feature). OSDFeat employs a decoupling strategy, training descriptor and detection networks independently. Inspired by semantic correspondence, we propose an Object and Spatial Discrimination ResUNet (OSD-ResUNet). OSD-ResUNet captures features from the feature map that differentiate object appearance and spatial context, thus enhancing descriptor performance. To further improve the discriminative capability of descriptors, we propose a Discrimination Information Retained Normalization module (DIRN). DIRN complementarily integrates spatial-wise normalization and channel-wise normalization, yielding descriptors that are more distinguishable and informative. In the detection network, we propose a Cross Saliency Pooling module (CSP). CSP employs a cross-shaped kernel to aggregate long-range context in both vertical and horizontal dimensions. By enhancing the saliency of keypoints, CSP enables the detection network to effectively utilize descriptor information and achieve more precise localization of keypoints. Compared to the previous best local feature extraction methods, OSDFeat achieves Mean Matching Accuracy of 79.4% in local feature matching task, improving by 1.9% and achieving state-of-the-art results. Additionally, OSDFeat achieves competitive results in Visual Localization and 3D Reconstruction. The results of this study indicate that object and spatial discrimination can improve the accuracy and robustness of local feature, even in challenging environments. The code is available at https://github.com/pandaandyy/OSDFeat.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400621X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Linguistics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Yin",
        "given_name": "Yifan"
      },
      {
        "surname": "Yin",
        "given_name": "Mengxiao"
      },
      {
        "surname": "Xiong",
        "given_name": "Yunhui"
      },
      {
        "surname": "Lai",
        "given_name": "Pengfei"
      },
      {
        "surname": "Chang",
        "given_name": "Kan"
      },
      {
        "surname": "Yang",
        "given_name": "Feng"
      }
    ]
  },
  {
    "title": "G 2 BFNN: Generalized geodesic basis function neural network",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106701",
    "abstract": "Real-world data is typically distributed on low-dimensional manifolds embedded in high-dimensional Euclidean spaces. Accurately extracting spatial distribution features on general manifolds that reflect the intrinsic characteristics of data is crucial for effective feature representation. Therefore, we propose a generalized geodesic basis function neural network (G 2 BFNN) architecture. The generalized geodesic basis functions (G 2 BF) are defined based on generalized geodesic distances. The generalized geodesic distance metric (G 2 DM) is obtained by learning the manifold structure. To implement this architecture, a specific G 2 BFNN, named discriminative local preserving projection-based G 2 BFNN (DLPP-G 2 BFNN) is proposed. DLPP-G 2 BFNN mainly contains two modules, namely the manifold structure learning module (MSLM) and the network mapping module (NMM). In the MSLM module, a supervised adjacency graph matrix is constructed to constrain the learning of the manifold structure. This enables the learned features in the embedding subspace to maintain the manifold structure while enhancing the discriminability. The features and G 2 DM learned in the MSLM are fed into the NMM. Through the G 2 BF in the NMM, the spatial distribution features on manifold are obtained. Finally, the output of the network is obtained through the fully connected layer. Compared with the local response neural network based on Euclidean distance, the proposed network can reveal more essential spatial structure characteristics of the data. Meanwhile, the proposed G 2 BFNN is a generalized network architecture that can be combined with any manifold learning method, showcasing high scalability. The experimental results demonstrate that the proposed DLPP-G 2 BFNN outperforms existing methods by utilizing fewer kernels while achieving higher recognition performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006257",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Basis (linear algebra)",
      "Computer science",
      "Euclidean distance",
      "Euclidean geometry",
      "Geodesic",
      "Geometry",
      "Law",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Yang"
      },
      {
        "surname": "Xu",
        "given_name": "Jiayi"
      },
      {
        "surname": "Pei",
        "given_name": "Jihong"
      },
      {
        "surname": "Yang",
        "given_name": "Xuan"
      }
    ]
  },
  {
    "title": "Coagulo-Net: Enhancing the mathematical modeling of blood coagulation using physics-informed neural networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106732",
    "abstract": "Blood coagulation, which involves a group of complex biochemical reactions, is a crucial step in hemostasis to stop bleeding at the injury site of a blood vessel. Coagulation abnormalities, such as hypercoagulation and hypocoagulation, could either cause thrombosis or hemorrhage, resulting in severe clinical consequences. Mathematical models of blood coagulation have been widely used to improve the understanding of the pathophysiology of coagulation disorders, guide the design and testing of new anticoagulants or other therapeutic agents, and promote precision medicine. However, estimating the parameters in these coagulation models has been challenging as not all reaction rate constants and new parameters derived from model assumptions are measurable. Although various conventional methods have been employed for parameter estimation for coagulation models, the existing approaches have several shortcomings. Inspired by the physics-informed neural networks, we propose Coagulo-Net, which synergizes the strengths of deep neural networks with the mechanistic understanding of the blood coagulation processes to enhance the mathematical models of the blood coagulation cascade. We assess the performance of the Coagulo-Net using two existing coagulation models with different extents of complexity. Our simulation results illustrate that Coagulo-Net can efficiently infer the unknown model parameters and dynamics of species based on sparse measurement data and data contaminated with noise. In addition, we show that Coagulo-Net can process a mixture of synthetic and experimental data and refine the predictions of existing mathematical models of coagulation. These results demonstrate the promise of Coagulo-Net in enhancing current coagulation models and aiding the creation of novel models for physiological and pathological research. These results showcase the potential of Coagulo-Net to advance computational modeling in the study of blood coagulation, improving both research methodologies and the development of new therapies for treating patients with coagulation disorders.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006567",
    "keywords": [
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Coagulation",
      "Computer science",
      "Internal medicine",
      "Mathematics",
      "Medicine",
      "Physics",
      "Statistical physics"
    ],
    "authors": [
      {
        "surname": "Qian",
        "given_name": "Ying"
      },
      {
        "surname": "Zhu",
        "given_name": "Ge"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhen"
      },
      {
        "surname": "Modepalli",
        "given_name": "Susree"
      },
      {
        "surname": "Zheng",
        "given_name": "Yihao"
      },
      {
        "surname": "Zheng",
        "given_name": "Xiaoning"
      },
      {
        "surname": "Frydman",
        "given_name": "Galit"
      },
      {
        "surname": "Li",
        "given_name": "He"
      }
    ]
  },
  {
    "title": "Is artificial consciousness achievable? Lessons from the human brain",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106714",
    "abstract": "We here analyse the question of developing artificial consciousness from an evolutionary perspective, taking the evolution of the human brain and its relation with consciousness as a reference model or as a benchmark. This kind of analysis reveals several structural and functional features of the human brain that appear to be key for reaching human-like complex conscious experience and that current research on Artificial Intelligence (AI) should take into account in its attempt to develop systems capable of human-like conscious processing. We argue that, even if AI is limited in its ability to emulate human consciousness for both intrinsic (i.e., structural and architectural) and extrinsic (i.e., related to the current stage of scientific and technological knowledge) reasons, taking inspiration from those characteristics of the brain that make human-like conscious processing possible and/or modulate it, is a potentially promising strategy towards developing conscious AI. Also, it cannot be theoretically excluded that AI research can develop partial or potentially alternative forms of consciousness that are qualitatively different from the human form, and that may be either more or less sophisticated depending on the perspectives. Therefore, we recommend neuroscience-inspired caution in talking about artificial consciousness: since the use of the same word “consciousness” for humans and AI becomes ambiguous and potentially misleading, we propose to clearly specify which level and/or type of consciousness AI research aims to develop, as well as what would be common versus differ in AI conscious processing compared to human conscious experience.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006385",
    "keywords": [
      "Artificial consciousness",
      "Artificial intelligence",
      "Cognitive psychology",
      "Cognitive science",
      "Computer science",
      "Consciousness",
      "Neuroscience",
      "Perspective (graphical)",
      "Psychology"
    ],
    "authors": [
      {
        "surname": "Farisco",
        "given_name": "Michele"
      },
      {
        "surname": "Evers",
        "given_name": "Kathinka"
      },
      {
        "surname": "Changeux",
        "given_name": "Jean-Pierre"
      }
    ]
  },
  {
    "title": "Knowledge graph confidence-aware embedding for recommendation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106601",
    "abstract": "Knowledge graphs (KG) are vital for extracting and storing knowledge from large datasets. Current research favors knowledge graph-based recommendation methods, but they often overlook the features learning of relations between entities and focus excessively on entity-level details. Moreover, they ignore a crucial fact: the aggregation process of entity and relation features in KG is complex, diverse, and imbalanced. To address this, we propose a recommendation-oriented KG confidence-aware embedding technique. It introduces an information aggregation graph and a confidence feature aggregation mechanism to overcome these challenges. Additionally, we quantify entity confidence at the feature and category levels, improving the precision of embeddings during information propagation and aggregation. Our approach achieves significant improvements over state-of-the-art KG embedding-based recommendation methods, with up to 6.20% increase in AUC and 8.46% increase in GAUC, as demonstrated on four public KG datasets 2 2 Our code in this work will be made available upon reasonable request to the authors. .",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005252",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Embedding",
      "Feature (linguistics)",
      "Focus (optics)",
      "Graph",
      "Information retrieval",
      "Knowledge graph",
      "Linguistics",
      "Machine learning",
      "Optics",
      "Philosophy",
      "Physics",
      "Recommender system",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Chen"
      },
      {
        "surname": "Yu",
        "given_name": "Fei"
      },
      {
        "surname": "Wan",
        "given_name": "Zhiguo"
      },
      {
        "surname": "Li",
        "given_name": "Fengying"
      },
      {
        "surname": "Ji",
        "given_name": "Hui"
      },
      {
        "surname": "Li",
        "given_name": "Yuandi"
      }
    ]
  },
  {
    "title": "Kolmogorov n-widths for multitask physics-informed machine learning (PIML) methods: Towards robust metrics",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106703",
    "abstract": "Physics-informed machine learning (PIML) as a means of solving partial differential equations (PDEs) has garnered much attention in the Computational Science and Engineering (CS&E) world. This topic encompasses a broad array of methods and models aimed at solving a single or a collection of PDE problems, called multitask learning. PIML is characterized by the incorporation of physical laws into the training process of machine learning models in lieu of large data when solving PDE problems. Despite the overall success of this collection of methods, it remains incredibly difficult to analyze, benchmark, and generally compare one approach to another. Using Kolmogorov n-widths as a measure of effectiveness of approximating functions, we judiciously apply this metric in the comparison of various multitask PIML architectures. We compute lower accuracy bounds and analyze the model’s learned basis functions on various PDE problems. This is the first objective metric for comparing multitask PIML architectures and helps remove uncertainty in model validation from selective sampling and overfitting. We also identify avenues of improvement for model architectures, such as the choice of activation function, which can drastically affect model generalization to “worst-case” scenarios, which is not observed when reporting task-specific errors. We also incorporate this metric into the optimization process through regularization, which improves the models’ generalizability over the multitask PDE problem.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006270",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Economics",
      "Generalizability theory",
      "Generalization",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Metric (unit)",
      "Multi-task learning",
      "Operations management",
      "Overfitting",
      "Regularization (linguistics)",
      "Statistics",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Penwarden",
        "given_name": "Michael"
      },
      {
        "surname": "Owhadi",
        "given_name": "Houman"
      },
      {
        "surname": "Kirby",
        "given_name": "Robert M."
      }
    ]
  },
  {
    "title": "Deep brain stimulation and lag synchronization in a memristive two-neuron network",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106728",
    "abstract": "In the pursuit of potential treatments for neurological disorders and the alleviation of patient suffering, deep brain stimulation (DBS) has been utilized to intervene or investigate pathological neural activities. To explore the exact mechanism of how DBS works, a memristive two-neuron network considering DBS is newly proposed in this work. This network is implemented by coupling two-dimensional Morris–Lecar neuron models and using a memristor synaptic synapse to mimic synaptic plasticity. The complex bursting activities and dynamical effects are revealed numerically through dynamical analysis. By examining the synchronous behavior, the desynchronization mechanism of the memristor synapse is uncovered. The study demonstrates that synaptic connections lead to the appearance of time-lagged or asynchrony in completely synchronized firing activities. Additionally, the memristive two-neuron network is implemented in hardware based on FPGA, and experimental results confirm the abundant neuronal electrical activities and chaotic dynamical behaviors. This work offers insights into the potential mechanisms of DBS intervention in neural networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400652X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biological neuron model",
      "Bursting",
      "Channel (broadcasting)",
      "Chaotic",
      "Computer network",
      "Computer science",
      "Deep brain stimulation",
      "Disease",
      "Mechanism (biology)",
      "Medicine",
      "Memristor",
      "Neuron",
      "Neuroscience",
      "Parkinson's disease",
      "Pathology",
      "Physics",
      "Psychology",
      "Quantum mechanics",
      "Synapse",
      "Synchronization (alternating current)"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Xihong"
      },
      {
        "surname": "Bao",
        "given_name": "Han"
      },
      {
        "surname": "Xu",
        "given_name": "Quan"
      },
      {
        "surname": "Chen",
        "given_name": "Mo"
      },
      {
        "surname": "Bao",
        "given_name": "Bocheng"
      }
    ]
  },
  {
    "title": "RBP-DIP: Residual back projection with deep image prior for ill-posed CT reconstruction",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106740",
    "abstract": "The success of deep image prior (DIP) in a number of image processing tasks has motivated their application in image reconstruction problems in computed tomography (CT). In this paper, we introduce a residual back projection technique (RBP) that improves the performance of deep image prior framework in iterative CT reconstruction, especially when the reconstruction problem is highly ill-posed. The RBP-DIP framework uses an untrained U-net in conjunction with a novel residual back projection connection to minimize the objective function while improving reconstruction accuracy. In each iteration, the weights of the untrained U-net are optimized, and the output of the U-net in the current iteration is used to update the input of the U-net in the next iteration through the proposed RBP connection. The introduction of the RBP connection strengthens the regularization effects of the DIP framework in the context of iterative CT reconstruction leading to improvements in accuracy. Our experiments demonstrate that the RBP-DIP framework offers improvements over other state-of-the-art conventional IR methods, as well as pre-trained and untrained models with similar network structures under multiple conditions. These improvements are particularly significant in the few-view and limited-angle CT reconstructions, where the corresponding inverse problems are highly ill-posed and the training data is limited. Furthermore, RBP-DIP has the potential for further improvement. Most existing IR algorithms, pre-trained models, and enhancements applicable to the original DIP algorithm can also be integrated into the RBP-DIP framework.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006646",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Iterative reconstruction",
      "Mathematics",
      "Projection (relational algebra)",
      "Radon transform",
      "Residual",
      "Well-posed problem"
    ],
    "authors": [
      {
        "surname": "Shu",
        "given_name": "Ziyu"
      },
      {
        "surname": "Entezari",
        "given_name": "Alireza"
      }
    ]
  },
  {
    "title": "Knowledge-reinforced explainable next basket recommendation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106675",
    "abstract": "The next basket recommendation task aims to predict the items in the user’s next basket by modeling the user’s basket sequence. Existing next basket recommendations focus on improving recommendation performance, and most of these methods are black-box models, ignoring the importance of providing explanations to improve user satisfaction. Furthermore, most next basket recommendation methods are designed for consumer users, and few methods are proposed for business user characteristics. To address the above problems, we propose a Knowledge Reinforced Explainable Next Basket Recommendation (KRE-NBR). Specifically, we construct a basket-based knowledge graph and obtain pretrained embeddings of entities that contain rich information of the knowledge graph. To obtain high-quality user predictive vectors, we fuse user pretrained embeddings, user basket sequence level embeddings, and user repurchase embeddings. One highlight of the user repurchase embeddings is that they are able to model business user repurchase behavior. To make the results of next basket recommendations explainable, we use reinforcement learning for path reasoning to find the items recommended in the next basket and generate recommendation explanations at the same time. To the best of our knowledge, this is the first work to provide recommendation explanations for next basket recommendations. Extensive experiments on real datasets show that the recommendation performance of our proposed approach outperforms several state-of-the-art baselines.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005999",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Economics",
      "Epistemology",
      "Graph",
      "Information retrieval",
      "Knowledge graph",
      "Machine learning",
      "Management",
      "Philosophy",
      "Quality (philosophy)",
      "Recommender system",
      "Reinforcement learning",
      "Task (project management)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Ling"
      },
      {
        "surname": "Zou",
        "given_name": "Han"
      },
      {
        "surname": "Huang",
        "given_name": "Xiao-Dong"
      },
      {
        "surname": "Gao",
        "given_name": "Yuefang"
      },
      {
        "surname": "Kuang",
        "given_name": "Yingjie"
      },
      {
        "surname": "Wang",
        "given_name": "Chang-Dong"
      }
    ]
  },
  {
    "title": "Multi-source Selective Graph Domain Adaptation Network for cross-subject EEG emotion recognition",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106742",
    "abstract": "Affective brain-computer interface is an important part of realizing emotional human–computer interaction. However, existing objective individual differences among subjects significantly hinder the application of electroencephalography (EEG) emotion recognition. Existing methods still lack the complete extraction of subject-invariant representations for EEG and the ability to fuse valuable information from multiple subjects to facilitate the emotion recognition of the target subject. To address the above challenges, we propose a Multi-source Selective Graph Domain Adaptation Network (MSGDAN), which can better utilize data from different source subjects and perform more robust emotion recognition on the target subject. The proposed network extracts and selects the individual information specific to each subject, where public information refers to subject-invariant components from multi-source subjects. Moreover, the graph domain adaptation network captures both functional connectivity and regional states of the brain via a dynamic graph network and then integrates graph domain adaptation to ensure the invariance of both functional connectivity and regional states. To evaluate our method, we conduct cross-subject emotion recognition experiments on the SEED, SEED-IV, and DEAP datasets. The results demonstrate that the MSGDAN has superior classification performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400666X",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Domain adaptation",
      "Electroencephalography",
      "Emotion recognition",
      "Graph",
      "Library science",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Psychology",
      "Speech recognition",
      "Subject (documents)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jing"
      },
      {
        "surname": "Ning",
        "given_name": "Xiaojun"
      },
      {
        "surname": "Xu",
        "given_name": "Wei"
      },
      {
        "surname": "Li",
        "given_name": "Yunze"
      },
      {
        "surname": "Jia",
        "given_name": "Ziyu"
      },
      {
        "surname": "Lin",
        "given_name": "Youfang"
      }
    ]
  },
  {
    "title": "PathMLP: Smooth path towards high-order homophily",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106650",
    "abstract": "Real-world graphs exhibit increasing heterophily, where nodes no longer tend to be connected to nodes with the same label, challenging the homophily assumption of classical graph neural networks (GNNs) and impeding their performance. Intriguingly, from the observation of heterophilous data, we notice that certain high-order information exhibits higher homophily, which motivates us to involve high-order information in node representation learning. However, common practices in GNNs to acquire high-order information mainly through increasing model depth and altering message-passing mechanisms, which, albeit effective to a certain extent, suffer from three shortcomings: (1) over-smoothing due to excessive model depth and propagation times; (2) high-order information is not fully utilized; (3) low computational efficiency. In this regard, we design a similarity-based path sampling strategy to capture smooth paths containing high-order homophily. Then we propose a lightweight model based on multi-layer perceptrons (MLP), named PathMLP, which can encode messages carried by paths via simple transformation and concatenation operations, and effectively learn node representations in heterophilous graphs through adaptive path aggregation. Extensive experiments demonstrate that our method outperforms baselines on 16 out of 20 datasets, underlining its effectiveness and superiority in alleviating the heterophily problem. In addition, our method is immune to over-smoothing and has high computational efficiency. The source code will be available in https://github.com/Graph4Sec-Team/PathMLP.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005744",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer network",
      "Computer science",
      "Economics",
      "Finance",
      "Homophily",
      "Mathematics",
      "Order (exchange)",
      "Path (computing)",
      "Physics",
      "Statistical physics"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Jiajun"
      },
      {
        "surname": "Xie",
        "given_name": "Chenxuan"
      },
      {
        "surname": "Gong",
        "given_name": "Shengbo"
      },
      {
        "surname": "Qian",
        "given_name": "Jiaxu"
      },
      {
        "surname": "Yu",
        "given_name": "Shanqing"
      },
      {
        "surname": "Xuan",
        "given_name": "Qi"
      },
      {
        "surname": "Yang",
        "given_name": "Xiaoniu"
      }
    ]
  },
  {
    "title": "Multi-task heterogeneous graph learning on electronic health records",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106644",
    "abstract": "Learning electronic health records (EHRs) has received emerging attention because of its capability to facilitate accurate medical diagnosis. Since the EHRs contain enriched information specifying complex interactions between entities, modeling EHRs with graphs is shown to be effective in practice. The EHRs, however, present a great degree of heterogeneity, sparsity, and complexity, which hamper the performance of most of the models applied to them. Moreover, existing approaches modeling EHRs often focus on learning the representations for a single task, overlooking the multi-task nature of EHR analysis problems and resulting in limited generalizability across different tasks. In view of these limitations, we propose a novel framework for EHR modeling, namely MulT-EHR (Multi-Task EHR), which leverages a heterogeneous graph to mine the complex relations and model the heterogeneity in the EHRs. To mitigate the large degree of noise, we introduce a denoising module based on the causal inference framework to adjust for severe confounding effects and reduce noise in the EHR data. Additionally, since our model adopts a single graph neural network for simultaneous multi-task prediction, we design a multi-task learning module to leverage the inter-task knowledge to regularize the training process. Extensive empirical studies on MIMIC-III and MIMIC-IV datasets validate that the proposed method consistently outperforms the state-of-the-art designs in four popular EHR analysis tasks — drug recommendation, and predictions of the length of stay, mortality, and readmission. Thorough ablation studies demonstrate the robustness of our method upon variations to key components and hyperparameters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005689",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Economic growth",
      "Economics",
      "Gene",
      "Generalizability theory",
      "Graph",
      "Graphical model",
      "Health care",
      "Health records",
      "Inference",
      "Leverage (statistics)",
      "Machine learning",
      "Management",
      "Mathematics",
      "Multi-task learning",
      "Robustness (evolution)",
      "Statistics",
      "Task (project management)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Chan",
        "given_name": "Tsai Hor"
      },
      {
        "surname": "Yin",
        "given_name": "Guosheng"
      },
      {
        "surname": "Bae",
        "given_name": "Kyongtae"
      },
      {
        "surname": "Yu",
        "given_name": "Lequan"
      }
    ]
  },
  {
    "title": "Learning explainable task-relevant state representation for model-free deep reinforcement learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106741",
    "abstract": "State representations considerably accelerate learning speed and improve data efficiency for deep reinforcement learning (DRL), especially for visual tasks. Task-relevant state representations could focus on features relevant to the task, filter out irrelevant elements, and thus further improve performance. However, task-relevant representations are typically obtained through model-based DRL methods, which involves the challenging task of learning a transition function. Moreover, inaccuracies in the learned transition function can potentially lead to performance degradation and negatively impact the learning of the policy. In this paper, to address the above issue, we propose a novel method of explainable task-relevant state representation (ETrSR) for model-free DRL that is direct, robust, and without any requirement of learning of a transition model. More specifically, the proposed ETrSR first disentangles the features from the states based on the beta variational autoencoder ( β -VAE). Then, a reward prediction model is employed to bootstrap these features to be relevant to the task, and the explainable states can be obtained by decoding the task-related features. Finally, we validate our proposed method on the CarRacing environment and various tasks in the DeepMind control suite (DMC), which demonstrates the explainability for better understanding of the decision-making process and the outstanding performance of the proposed method even in environments with strong distractions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006658",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Engineering",
      "Law",
      "Machine learning",
      "Multi-task learning",
      "Political science",
      "Politics",
      "Reinforcement learning",
      "Representation (politics)",
      "State (computer science)",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Tingting"
      },
      {
        "surname": "Li",
        "given_name": "Guixi"
      },
      {
        "surname": "Zhao",
        "given_name": "Tuo"
      },
      {
        "surname": "Chen",
        "given_name": "Yarui"
      },
      {
        "surname": "Xie",
        "given_name": "Ning"
      },
      {
        "surname": "Niu",
        "given_name": "Gang"
      },
      {
        "surname": "Sugiyama",
        "given_name": "Masashi"
      }
    ]
  },
  {
    "title": "Asynchronous iterative Q-learning based tracking control for nonlinear discrete-time multi-agent systems",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106667",
    "abstract": "This paper addresses the tracking control problem of nonlinear discrete-time multi-agent systems (MASs). First, a local neighborhood error system (LNES) is constructed. Then, a novel tracking algorithm based on asynchronous iterative Q-learning (AIQL) is developed, which can transform the tracking problem into the optimal regulation of LNES. The AIQL-based algorithm has two Q values Q i A and Q i B for each agent i , where Q i A is used for improving the control policy and Q i B is used for evaluating the value of the control policy. Moreover, the convergence of LNES is given. It is shown that the LNES converges to 0 and the tracking problem is solved. A neural network-based actor-critic framework is used to implement AIQL. The critic network of AIQL is composed of two neural networks, which are used for approximating Q i A and Q i B respectively. Finally, simulation results are given to verify the performance of the developed algorithm. It is shown that the AIQL-based tracking algorithm has a lower cost value and faster convergence speed than the IQL-based tracking algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005914",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Asynchronous communication",
      "Computer network",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Discrete time and continuous time",
      "Iterative learning control",
      "Iterative method",
      "Mathematical optimization",
      "Mathematics",
      "Multi-agent system",
      "Nonlinear system",
      "Pedagogy",
      "Physics",
      "Psychology",
      "Quantum mechanics",
      "Statistics",
      "Tracking (education)",
      "Tracking error"
    ],
    "authors": [
      {
        "surname": "Shen",
        "given_name": "Ziwen"
      },
      {
        "surname": "Dong",
        "given_name": "Tao"
      },
      {
        "surname": "Huang",
        "given_name": "Tingwen"
      }
    ]
  },
  {
    "title": "Relaxed stability criteria of delayed neural networks using delay-parameters-dependent slack matrices",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106676",
    "abstract": "This note aims to reduce the conservatism of stability criteria for neural networks with time-varying delay. To this goal, on the one hand, we construct an augmented Lyapunov–Krasovskii functional (LKF), incorporating some delay-product terms that capture more information about neural states. On the other hand, when dealing with the derivative of the LKF, we introduce several parameter-dependent slack matrices into an affine integral inequality, zero equations, and the S -procedure. As a result, more relaxed stability criteria are obtained by employing the so-called Lyapunov–Krasovskii Theorem. Two numerical examples show that the proposed stability criteria are of less conservatism compared with some existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006002",
    "keywords": [
      "Affine transformation",
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Conservatism",
      "Control (management)",
      "Control theory (sociology)",
      "Derivative (finance)",
      "Discrete time and continuous time",
      "Economics",
      "Financial economics",
      "Geometry",
      "Law",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Philosophy",
      "Political science",
      "Politics",
      "Product (mathematics)",
      "Pure mathematics",
      "Stability (learning theory)",
      "Stability conditions",
      "Stability criterion",
      "Statistics",
      "Zero (linguistics)"
    ],
    "authors": [
      {
        "surname": "Zeng",
        "given_name": "Hong-Bing"
      },
      {
        "surname": "Zhu",
        "given_name": "Zong-Jun"
      },
      {
        "surname": "Wang",
        "given_name": "Wei"
      },
      {
        "surname": "Zhang",
        "given_name": "Xian-Ming"
      }
    ]
  },
  {
    "title": "A class-incremental learning approach for learning feature-compatible embeddings",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106685",
    "abstract": "Humans have the ability to constantly learn new knowledge. However, for artificial intelligence, trying to continuously learn new knowledge usually results in catastrophic forgetting, the existing regularization-based and dynamic structure-based approaches have shown great potential for alleviating. Nevertheless, these approaches have certain limitations. They usually do not fully consider the problem of incompatible feature embeddings. Instead, they tend to focus only on the features of new or previous classes and fail to comprehensively consider the entire model. Therefore, we propose a two-stage learning paradigm to solve feature embedding incompatibility problems. Specifically, we retain the previous model and freeze all its parameters in the first stage while dynamically expanding a new module to alleviate feature embedding incompatibility questions. In the second stage, a fusion knowledge distillation approach is used to compress the redundant feature dimensions. Moreover, we propose weight pruning and consolidation approaches to improve the efficiency of the model. Our experimental results obtained on the CIFAR-100, ImageNet-100 and ImageNet-1000 benchmark datasets show that the proposed approaches achieve the best performance among all the compared approaches. For example, on the ImageNet-100 dataset, the maximal accuracy improvement is 5.08%. Code is available at https://github.com/ybyangjing/CIL-FCE.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006099",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Class (philosophy)",
      "Computer science",
      "Embedding",
      "Feature (linguistics)",
      "Feature learning",
      "Focus (optics)",
      "Forgetting",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Machine learning",
      "Optics",
      "Philosophy",
      "Physics",
      "Pruning",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "An",
        "given_name": "Hongchao"
      },
      {
        "surname": "Yang",
        "given_name": "Jing"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiuhua"
      },
      {
        "surname": "Ruan",
        "given_name": "Xiaoli"
      },
      {
        "surname": "Wu",
        "given_name": "Yuankai"
      },
      {
        "surname": "Li",
        "given_name": "Shaobo"
      },
      {
        "surname": "Hu",
        "given_name": "Jianjun"
      }
    ]
  },
  {
    "title": "Delay learning based on temporal coding in Spiking Neural Networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106678",
    "abstract": "Spiking Neural Networks (SNNs) hold great potential for mimicking the brain’s efficient processing of information. Although biological evidence suggests that precise spike timing is crucial for effective information encoding, contemporary SNN research mainly concentrates on adjusting connection weights. In this work, we introduce Delay Learning based on Temporal Coding (DLTC), an innovative approach that integrates delay learning with a temporal coding strategy to optimize spike timing in SNNs. DLTC utilizes a learnable delay shift, which assigns varying levels of importance to different informational elements. This is complemented by an adjustable threshold that regulates firing times, allowing for earlier or later neuron activation as needed. We have tested DLTC’s effectiveness in various contexts, including vision and auditory classification tasks, where it consistently outperformed traditional weight-only SNNs. The results indicate that DLTC achieves remarkable improvements in accuracy and computational efficiency, marking a step forward in advancing SNNs towards real-world applications. Our codes are accessible at https://github.com/sunpengfei1122/DLTC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006026",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Coding (social sciences)",
      "Computer science",
      "Encoding (memory)",
      "Machine learning",
      "Mathematics",
      "Neural coding",
      "Pattern recognition (psychology)",
      "Predictive coding",
      "Software engineering",
      "Speech recognition",
      "Spike (software development)",
      "Spiking neural network",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Pengfei"
      },
      {
        "surname": "Wu",
        "given_name": "Jibin"
      },
      {
        "surname": "Zhang",
        "given_name": "Malu"
      },
      {
        "surname": "Devos",
        "given_name": "Paul"
      },
      {
        "surname": "Botteldooren",
        "given_name": "Dick"
      }
    ]
  },
  {
    "title": "Multiview learning with twin parametric margin SVM",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106598",
    "abstract": "Multiview learning (MVL) seeks to leverage the benefits of diverse perspectives to complement each other, effectively extracting and utilizing the latent information within the dataset. Several twin support vector machine-based MVL (MvTSVM) models have been introduced and demonstrated outstanding performance in various learning tasks. However, MvTSVM-based models face significant challenges in the form of computational complexity due to four matrix inversions, the need to reformulate optimization problems in order to employ kernel-generated surfaces for handling non-linear cases, and the constraint of uniform noise assumption in the training data. Particularly in cases where the data possesses a heteroscedastic error structure, these challenges become even more pronounced. In view of the aforementioned challenges, we propose multiview twin parametric margin support vector machine (MvTPMSVM). MvTPMSVM constructs parametric margin hyperplanes corresponding to both classes, aiming to regulate and manage the impact of the heteroscedastic noise structure existing within the data. The proposed MvTPMSVM model avoids the explicit computation of matrix inversions in the dual formulation, leading to enhanced computational efficiency. We perform an extensive assessment of the MvTPMSVM model using benchmark datasets such as UCI, KEEL, synthetic, and Animals with Attributes (AwA). Our experimental results, coupled with rigorous statistical analyses, confirm the superior generalization capabilities of the proposed MvTPMSVM model compared to the baseline models. The source code of the proposed MvTPMSVM model is available at https://github.com/mtanveer1/MvTPMSVM.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005227",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematics",
      "Parametric statistics",
      "Pattern recognition (psychology)",
      "Statistics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Quadir",
        "given_name": "A."
      },
      {
        "surname": "Tanveer",
        "given_name": "M."
      }
    ]
  },
  {
    "title": "Adaptive pessimism via target Q-value for offline reinforcement learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106588",
    "abstract": "Offline reinforcement learning (RL) methods learn from datasets without further environment interaction, facing errors due to out-of-distribution (OOD) actions. Although effective methods have been proposed to conservatively estimate the Q-values of those OOD actions to mitigate this problem, insufficient or excessive pessimism under constant constraints often harms the policy learning process. Moreover, since the distribution of each task on the dataset varies among different environments and behavior policies, it is desirable to learn an adaptive weight for balancing constraints on the conservative estimation of Q-value and the standard RL objectives depending on each task. To achieve this, in this paper, we point out that the quantile of the Q-value is an effective metric to refer to the Q-value distribution of the fixed data set. Based on this observation, we design Adaptive Pessimism via a Target Q-value (APTQ) algorithm that balances between the pessimism constraint and the RL objective; this leads the expectation of Q-value to stably converge to a given target Q-value from a reasonable quantile of the Q-value distribution of the dataset. Experiments show that our method remarkably improves the performance of the state-of-the-art method CQL by 6.20% on the D4RL-v0 and 1.89% on the D4RL-v2.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005124",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Constraint (computer-aided design)",
      "Economics",
      "Epistemology",
      "Geometry",
      "Machine learning",
      "Management",
      "Mathematical optimization",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pessimism",
      "Philosophy",
      "Q-learning",
      "Quantile",
      "Reinforcement learning",
      "Statistics",
      "Task (project management)",
      "Value (mathematics)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Jie"
      },
      {
        "surname": "Zhang",
        "given_name": "Yinmin"
      },
      {
        "surname": "Li",
        "given_name": "Chuming"
      },
      {
        "surname": "Yang",
        "given_name": "Yaodong"
      },
      {
        "surname": "Liu",
        "given_name": "Yu"
      },
      {
        "surname": "Ouyang",
        "given_name": "Wanli"
      }
    ]
  },
  {
    "title": "Reconstruct incomplete relation for incomplete modality brain tumor segmentation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106657",
    "abstract": "Different brain tumor magnetic resonance imaging (MRI) modalities provide diverse tumor-specific information. Previous works have enhanced brain tumor segmentation performance by integrating multiple MRI modalities. However, multi-modal MRI data are often unavailable in clinical practice. An incomplete modality leads to missing tumor-specific information, which degrades the performance of existing models. Various strategies have been proposed to transfer knowledge from a full modality network (teacher) to an incomplete modality one (student) to address this issue. However, they neglect the fact that brain tumor segmentation is a structural prediction problem that requires voxel semantic relations. In this paper, we propose a Reconstruct Incomplete Relation Network (RIRN) that transfers voxel semantic relational knowledge from the teacher to the student. Specifically, we propose two types of voxel relations to incorporate structural knowledge: Class-relative relations (CRR) and Class-agnostic relations (CAR). The CRR groups voxels into different tumor regions and constructs a relation between them. The CAR builds a global relation between all voxel features, complementing the local inter-region relation. Moreover, we use adversarial learning to align the holistic structural prediction between the teacher and the student. Extensive experimentation on both the BraTS 2018 and BraTS 2020 datasets establishes that our method outperforms all state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005811",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Data mining",
      "Machine learning",
      "Modalities",
      "Modality (human–computer interaction)",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Relation (database)",
      "Segmentation",
      "Social science",
      "Sociology",
      "Voxel"
    ],
    "authors": [
      {
        "surname": "Su",
        "given_name": "Jiawei"
      },
      {
        "surname": "Luo",
        "given_name": "Zhiming"
      },
      {
        "surname": "Wang",
        "given_name": "Chengji"
      },
      {
        "surname": "Lian",
        "given_name": "Sheng"
      },
      {
        "surname": "Lin",
        "given_name": "Xuejuan"
      },
      {
        "surname": "Li",
        "given_name": "Shaozi"
      }
    ]
  },
  {
    "title": "Localized estimation of event-related neural source activity from simultaneous MEG-EEG with a recurrent neural network",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106731",
    "abstract": "Estimating intracranial current sources underlying the electromagnetic signals observed from extracranial sensors is a perennial challenge in non-invasive neuroimaging. Established solutions to this inverse problem treat time samples independently without considering the temporal dynamics of event-related brain processes. This paper describes current source estimation from simultaneously recorded magneto- and electro-encephalography (MEEG) using a recurrent neural network (RNN) that learns sequential relationships from neural data. The RNN was trained in two phases: (1) pre-training and (2) transfer learning with L1 regularization applied to the source estimation layer. Performance of using scaled labels derived from MEEG, magnetoencephalography (MEG), or electroencephalography (EEG) were compared, as were results from volumetric source space with free dipole orientation and surface source space with fixed dipole orientation. Exact low-resolution electromagnetic tomography (eLORETA) and mixed-norm L1/L2 (MxNE) source estimation methods were also applied to these data for comparison with the RNN method. The RNN approach outperformed other methods in terms of output signal-to-noise ratio, correlation and mean-squared error metrics evaluated against reference event-related field (ERF) and event-related potential (ERP) waveforms. Using MEEG labels with fixed-orientation surface sources produced the most consistent estimates. To estimate sources of ERF and ERP waveforms, the RNN generates temporal dynamics within its internal computational units, driven by sequential structure in neural data used as training labels. It thus provides a data-driven model of computational transformations from psychophysiological events into corresponding event-related neural signals, which is unique among MEEG source reconstruction solutions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006555",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Electroencephalography",
      "Geometry",
      "Inverse problem",
      "Magnetoencephalography",
      "Mathematical analysis",
      "Mathematics",
      "Orientation (vector space)",
      "Pattern recognition (psychology)",
      "Psychiatry",
      "Psychology",
      "Radar",
      "Recurrent neural network",
      "Telecommunications",
      "Waveform"
    ],
    "authors": [
      {
        "surname": "O'Reilly",
        "given_name": "Jamie A."
      },
      {
        "surname": "Zhu",
        "given_name": "Judy D."
      },
      {
        "surname": "Sowman",
        "given_name": "Paul F."
      }
    ]
  },
  {
    "title": "Improving classification performance of motor imagery BCI through EEG data augmentation with conditional generative adversarial networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106665",
    "abstract": "In brain-computer interface (BCI), building accurate electroencephalogram (EEG) classifiers for specific mental tasks is critical for BCI performance. The classifiers are developed by machine learning (ML) and deep learning (DL) techniques, requiring a large dataset for training to build reliable and accurate models. However, collecting large enough EEG datasets is difficult due to intra-/inter-subject variabilities and experimental costs. This leads to the data scarcity problem, which causes overfitting issues to training samples, resulting in reducing generalization performance. To solve the EEG data scarcity problem and improve the performance of the EEG classifiers, we propose a novel EEG data augmentation (DA) framework using conditional generative adversarial networks (cGANs). An experimental study is implemented with two public EEG datasets, including motor imagery (MI) tasks (BCI competition IV IIa and III IVa), to validate the effectiveness of the proposed EEG DA method for the EEG classifiers. To evaluate the proposed cGAN-based DA method, we tested eight EEG classifiers for the experiment, including traditional MLs and state-of-the-art DLs with three existing EEG DA methods. Experimental results showed that most DA methods with proper DA proportion in the training dataset had higher classification performances than without DA. Moreover, applying the proposed DA method showed superior classification performance improvement than the other DA methods. This shows that the proposed method is a promising EEG DA method for enhancing the performances of the EEG classifiers in MI-based BCIs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005896",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Brain–computer interface",
      "Computer science",
      "Convolutional neural network",
      "Electroencephalography",
      "Generalization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Motor imagery",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Psychiatry",
      "Psychology"
    ],
    "authors": [
      {
        "surname": "Choo",
        "given_name": "Sanghyun"
      },
      {
        "surname": "Park",
        "given_name": "Hoonseok"
      },
      {
        "surname": "Jung",
        "given_name": "Jae-Yoon"
      },
      {
        "surname": "Flores",
        "given_name": "Kevin"
      },
      {
        "surname": "Nam",
        "given_name": "Chang S."
      }
    ]
  },
  {
    "title": "PSD-ELGAN: A pseudo self-distillation based CycleGAN with enhanced local adversarial interaction for single image dehazing",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106689",
    "abstract": "Compared to pixel-level content loss, domain-level style loss in CycleGAN-based dehazing algorithms just imposes relatively soft constraints on the intermediate translated images, resulting in struggling to accurately model haze-free features from real hazy scenes. Furthermore, globally perceptual discriminator may misclassify real hazy images with significant scene depth variations as clean style, thereby resulting in severe haze residue. To address these issues, we propose a pseudo self-distillation based CycleGAN with enhanced local adversarial interaction for image dehazing, termed as PSD-ELGAN. On the one hand, we leverage the characteristic of CycleGAN to generate pseudo image pairs during training. Knowledge distillation is employed in this unsupervised framework to transfer the informative high-quality features from the self-reconstruction network of real clean images to the dehazing generator of paired pseudo hazy images, which effectively improves its haze-free feature representation ability without increasing network parameters. On the other hand, in the output of dehazing generator, four non-uniform image patches severely affected by residual haze are adaptively selected as input samples. The local discriminator could easily distinguish their hazy style, thereby further compelling the dehazing generator to suppress haze residues in such regions, thus enhancing its dehazing performance. Extensive experiments show that our PSD-ELGAN can achieve promising results and better generality across various datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006130",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Detector",
      "Discriminator",
      "Distillation",
      "Generator (circuit theory)",
      "Haze",
      "Image (mathematics)",
      "Leverage (statistics)",
      "Meteorology",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Residual",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Kangle"
      },
      {
        "surname": "Huang",
        "given_name": "Jun"
      },
      {
        "surname": "Ma",
        "given_name": "Yong"
      },
      {
        "surname": "Fan",
        "given_name": "Fan"
      },
      {
        "surname": "Ma",
        "given_name": "Jiayi"
      }
    ]
  },
  {
    "title": "Sample selection of adversarial attacks against traffic signs",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106698",
    "abstract": "In the real world, the correct recognition of traffic signs plays a crucial role in vehicle autonomous driving and traffic monitoring. The research on its adversarial attack can test the security of vehicle autonomous driving system and provide enlightenment for improving the recognition algorithm. However, with the development of transportation infrastructure, new traffic signs may be introduced. The adversarial attack model for traffic signs needs to adapt to the addition of new types. Based on this, class incremental learning for traffic sign adversarial attacks has become an interesting research field. We propose a class incremental learning method for adversarial attacks on traffic signs. First, this method uses Pinpoint Region Probability Estimation Network (PRPEN) to predict the probability of each pixel being attacked in old samples. It helps to identify the high attack probability regions of the samples. Subsequently, based on the size of high probability pixel concentration area, the replay sample set is constructed. Old samples with smaller concentration areas receive higher priority and are prioritized for incremental learning. The experimental results show that compared with other sample selection methods, our method selects more representative samples and can train PRPEN more effectively to generate probability maps, thereby better generating adversarial attacks on traffic signs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006221",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Class (philosophy)",
      "Computer science",
      "Computer security",
      "Machine learning",
      "Programming language",
      "Sample (material)",
      "Selection (genetic algorithm)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yiwen"
      },
      {
        "surname": "Wang",
        "given_name": "Yue"
      },
      {
        "surname": "Feng",
        "given_name": "Guorui"
      }
    ]
  },
  {
    "title": "Multi-scale convolution enhanced transformer for multivariate long-term time series forecasting",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106745",
    "abstract": "In data analysis and forecasting, particularly for multivariate long-term time series, challenges persist. The Transformer model in deep learning methods has shown significant potential in time series forecasting. The Transformer model’s dot-product attention mechanism, however, due to its quadratic computational complexity, impairs training and forecasting efficiency. In addition, the Transformer architecture has limitations in modeling local features and dealing with multivariate cross-dimensional dependency relationship. In this article, a Multi-Scale Convolution Enhanced Transformer model (MSCformer) is proposed for multivariate long-term time series forecasting. As an alternative to modeling the time series in its entirety, a segmentation strategy is designed to convert the input original series into segmented forms with different lengths, then process time series segments using a new constructed multi-Dependency Aggregation module. This multi-Scale segmentation approach reduces the computational complexity of the attention mechanism part in subsequent models, and for each segment of length corresponds to a specific time scale, it also ensures that each segment retains the semantic information of the data sequence level, thereby comprehensively utilizing the multi-scale information of the data while more accurately capturing the real dependency of the time series. The Multi-Dependence Aggregate module captures both cross-temporal and cross-dimensional dependencies of multivariate long-term time series and compensates for local dependencies within the segments thereby captures local series features comprehensively and addressing the issue of insufficient information utilization. MSCformer synthesizes dependency information extracted from various temporal segments at different scales and reconstructs future series using linear layers. MSCformer exhibits higher forecasting accuracy, outperforming existing methods in multiple domains including energy, transportation, weather, electricity, disease and finance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006695",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Cartography",
      "Computer science",
      "Convolution (computer science)",
      "Econometrics",
      "Geography",
      "Geology",
      "Machine learning",
      "Mathematics",
      "Multivariate statistics",
      "Paleontology",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)",
      "Series (stratigraphy)",
      "Term (time)",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Ao"
      },
      {
        "surname": "Li",
        "given_name": "Ying"
      },
      {
        "surname": "Xu",
        "given_name": "Yunyang"
      },
      {
        "surname": "Li",
        "given_name": "Xuemei"
      },
      {
        "surname": "Zhang",
        "given_name": "Caiming"
      }
    ]
  },
  {
    "title": "Learning the feature distribution similarities for online time series anomaly detection",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106638",
    "abstract": "Identifying anomalies in multi-dimensional sequential data is crucial for ensuring optimal performance across various domains and in large-scale systems. Traditional contrastive methods utilize feature similarity between different features extracted from multidimensional raw inputs as an indicator of anomaly severity. However, the complex objective functions and meticulously designed modules of these methods often lead to efficiency issues and a lack of interpretability. Our study introduces a structural framework called SimDetector, which is a Local–Global Multi-Scale Similarity Contrast network. Specifically, the restructured and enhanced GRU module extracts more generalized local features, including long-term cyclical trends. The multi-scale sparse attention module efficiently extracts multi-scale global features with pattern information. Additionally, we modified the KL divergence to suit the characteristics of time series anomaly detection, proposing a symmetric absolute KL divergence that focuses more on overall distribution differences. The proposed method achieves results that surpass or approach the State-of-the-Art (SOTA) on multiple real-world datasets and synthetic datasets, while also significantly reducing Multiply-Accumulate Operations (MACs) and memory usage.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005628",
    "keywords": [
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Condensed matter physics",
      "Data mining",
      "Divergence (linguistics)",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Interpretability",
      "Linguistics",
      "Machine learning",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)",
      "Series (stratigraphy)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "Jin"
      },
      {
        "surname": "Ge",
        "given_name": "Yan"
      },
      {
        "surname": "Zhang",
        "given_name": "Xinyi"
      },
      {
        "surname": "Wang",
        "given_name": "ZheYu"
      },
      {
        "surname": "Wu",
        "given_name": "Huifeng"
      },
      {
        "surname": "Wu",
        "given_name": "Jia"
      }
    ]
  },
  {
    "title": "Aperiodically intermittent quantized control-based exponential synchronization of quaternion-valued inertial neural networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106669",
    "abstract": "Inertial neural networks are proposed via introducing an inertia term into the Hopfield models, which make their dynamic behavior more complex compared to the traditional first-order models. Besides, the aperiodically intermittent quantized control over conventional feedback control has its potential advantages on reducing communication blocking and saving control cost. Based on these facts, we are mainly devoted to exploring of exponential synchronization of quaternion-valued inertial neural networks under aperiodically intermittent quantized control. Firstly, a compact quaternion-valued aperiodically intermittent quantized control protocol is developed, which can mitigate significantly the complexity of theoretical derivation. Subsequently, several concise criteria involving matrix inequalities are formulated through constructing a type of Lyapunov functional and employing a direct analysis approach. The correctness of the obtained results eventually is verified by a typical example.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005938",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Classical mechanics",
      "Combinatorics",
      "Computer science",
      "Control (management)",
      "Control engineering",
      "Control theory (sociology)",
      "Engineering",
      "Exponential function",
      "Geometry",
      "Inertial frame of reference",
      "Intermittent control",
      "Mathematical analysis",
      "Mathematics",
      "Physics",
      "Quaternion",
      "Synchronization (alternating current)",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Fei",
        "given_name": "Jingnan"
      },
      {
        "surname": "Ren",
        "given_name": "Sijie"
      },
      {
        "surname": "Zheng",
        "given_name": "Caicai"
      },
      {
        "surname": "Yu",
        "given_name": "Juan"
      },
      {
        "surname": "Hu",
        "given_name": "Cheng"
      }
    ]
  },
  {
    "title": "Learning functional brain networks with heterogeneous connectivities for brain disease identification",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106660",
    "abstract": "Functional brain networks (FBNs), which are used to portray interactions between different brain regions, have been widely used to identify potential biomarkers of neurological and mental disorders. The FBNs estimated using current methods tend to be homogeneous, indicating that different brain regions exhibit the same type of correlation. This homogeneity limits our ability to accurately encode complex interactions within the brain. Therefore, to the best of our knowledge, in the present study, for the first time, we propose the existence of heterogeneous FBNs and introduce a novel FBN estimation model that adaptively assigns heterogeneous connections to different pairs of brain regions, thereby effectively encoding the complex interaction patterns in the brain. Specifically, we first construct multiple types of candidate correlations from different views or based on different methods and then develop an improved orthogonal matching pursuit algorithm to select at most one correlation for each brain region pair under the guidance of label information. These adaptively estimated heterogeneous FBNs were then used to distinguish subjects with neurological/mental disorders from healthy controls and identify potential biomarkers related to these disorders. Experimental results on real datasets show that the proposed scheme improves classification performance by 7.07% and 7.58% at the two sites, respectively, compared with the baseline approaches. This emphasizes the plausibility of the heterogeneity hypothesis and effectiveness of the heterogeneous connection assignment algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005847",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Brain disease",
      "Computer science",
      "Disease",
      "Identification (biology)",
      "Medicine",
      "Neuroscience",
      "Pathology",
      "Psychology"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Chaojun"
      },
      {
        "surname": "Ma",
        "given_name": "Yunling"
      },
      {
        "surname": "Qiao",
        "given_name": "Lishan"
      },
      {
        "surname": "Zhang",
        "given_name": "Limei"
      },
      {
        "surname": "Liu",
        "given_name": "Mingxia"
      }
    ]
  },
  {
    "title": "The combined Lyapunov functionals method for stability analysis of neutral Cohen–Grossberg neural networks with multiple delays",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106641",
    "abstract": "This research article will employ the combined Lyapunov functionals method to deal with stability analysis of a more general type of Cohen–Grossberg neural networks which simultaneously involve constant time and neutral delay parameters. By utilizing some combinations of various Lyapunov functionals, we determine novel criteria ensuring global stability of such a model of neural systems that employ Lipschitz continuous activation functions. These proposed results are totally stated independently of delay terms and they can be completely characterized by the constants parameters involved in the neural system. By making some detailed analytical comparisons between the stability results derived in this research article and the existing corresponding stability criteria obtained in the past literature, we prove that our proposed stability results lead to establishing some sets of stability conditions and these conditions may be evaluated as different alternative results to the previously reported corresponding stability criteria. A numerical example is also presented to show the applicability of the proposed stability results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005653",
    "keywords": [
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Constant (computer programming)",
      "Control (management)",
      "Control theory (sociology)",
      "Discrete time and continuous time",
      "Lipschitz continuity",
      "Lyapunov function",
      "Lyapunov stability",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Nonlinear system",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Stability (learning theory)",
      "Stability conditions",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Faydasicok",
        "given_name": "Ozlem"
      },
      {
        "surname": "Arik",
        "given_name": "Sabri"
      }
    ]
  },
  {
    "title": "Local artifacts amplification for deepfakes augmentation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106692",
    "abstract": "With the rapid and continuous development of AIGC, It is becoming increasingly difficult to distinguish between real and forged facial images, which calls for efficient forgery detection systems. Although many detection methods have noticed the importance of local artifacts, there has been a lack of in-depth discussion regarding the selection of locations and their effective utilization. Besides, the traditional image augmentation methods that are widely used have limited improvements for forgery detection tasks and require more specialized augmentation methods specifically designed for forgery detection tasks. In this paper, this study proposes Local Artifacts Amplification for Deepfakes Augmentation, which amplifies the local artifacts on the forged faces. Furthermore, this study incorporates prior knowledge about similar facial features into the model. This means that within the facial regions defined in this work, forged features exhibit similar patterns. By aggregating the results from all facial regions, the study can enhance the overall performance of the model. The evaluation experiments conducted in this research, achieving an AUC of 93.40% and an Acc of 87.03% in the challenging WildDeepfake dataset, demonstrate a promising improvement in accuracy compared to traditional image augmentation methods and achieve superior performance on intra-dataset evaluation. The cross-dataset evaluation also showed that the method presented in this study has strong generalization abilities.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006166",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Face (sociological concept)",
      "Generalization",
      "Image (mathematics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Chunlei"
      },
      {
        "surname": "Sun",
        "given_name": "Feiyang"
      },
      {
        "surname": "Liu",
        "given_name": "Decheng"
      },
      {
        "surname": "Wang",
        "given_name": "Nannan"
      },
      {
        "surname": "Gao",
        "given_name": "Xinbo"
      }
    ]
  },
  {
    "title": "Time-optimal open-loop set stabilization of Boolean control networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106694",
    "abstract": "We show that for stabilization of Boolean control networks (BCNs) with unobservable initial states, open-loop control and close-loop control are not equivalent. An example is given to illustrate the nonequivalence. Enlightened by the nonequivalence, we explore open-loop set stabilization of BCNs with unobservable initial states. More specifically, this issue is to investigate that for a given BCN, whether there exists a unified free control sequence that is effective for all initial states of the system to stabilize the system states to a given set. The criteria for open-loop set stabilization is derived and for any open-loop set stabilizable BCN, every time-optimal open-loop set stabilizer is proposed. Besides, we obtain the least upper bounds of two integers, which are respectively related to the global stabilization and partial stabilization of BCNs in the results of two literature articles. Using the methods in the two literature articles, the least upper bounds of the two integers cannot be obtained.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400618X",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Econometrics",
      "Loop (graph theory)",
      "Mathematical analysis",
      "Mathematics",
      "Programming language",
      "Set (abstract data type)",
      "Unobservable",
      "Upper and lower bounds"
    ],
    "authors": [
      {
        "surname": "Dai",
        "given_name": "Shaoyu"
      },
      {
        "surname": "Li",
        "given_name": "Bowen"
      },
      {
        "surname": "Lu",
        "given_name": "Jianquan"
      }
    ]
  },
  {
    "title": "The Artificial Neural Twin — Process optimization and continual learning in distributed process chains",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106647",
    "abstract": "Industrial process optimization and control is crucial to increase economic and ecologic efficiency. However, data sovereignty, differing goals, or the required expert knowledge for implementation impede holistic implementation. Further, the increasing use of data-driven AI-methods in process models and industrial sensory often requires regular fine-tuning to accommodate distribution drifts. We propose the Artificial Neural Twin, which combines concepts from model predictive control, deep learning, and sensor networks to address these issues. Our approach introduces decentral, differentiable data fusion to estimate the state of distributed process steps and their dependence on input data. By treating the interconnected process steps as a quasi neural-network, we can backpropagate loss gradients for process optimization or model fine-tuning to process parameters or AI models respectively. The concept is demonstrated on a virtual machine park simulated in Unity, consisting of bulk material processes in plastic recycling.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005719",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Chemical engineering",
      "Computer science",
      "Engineering",
      "Operating system",
      "Process (computing)",
      "Process optimization"
    ],
    "authors": [
      {
        "surname": "Emmert",
        "given_name": "Johannes"
      },
      {
        "surname": "Mendez",
        "given_name": "Ronald"
      },
      {
        "surname": "Dastjerdi",
        "given_name": "Houman Mirzaalian"
      },
      {
        "surname": "Syben",
        "given_name": "Christopher"
      },
      {
        "surname": "Maier",
        "given_name": "Andreas"
      }
    ]
  },
  {
    "title": "Aperiodic intermittent dynamic event-triggered synchronization control for stochastic delayed multi-links complex networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106658",
    "abstract": "In this work, the exponential synchronization issue of stochastic complex networks with time delays and time-varying multi-links (SCNTM) is discussed via a novel aperiodic intermittent dynamic event-triggered control (AIDE-TC). The AIDE-TC is designed by combining intermittent control with an exponential function and dynamic event-triggered control, aiming to minimize the number of the required triggers. Then, based on the proposed control strategy, the sufficient conditions for exponential synchronization in mean square of SCNTM are obtained by adopting graph theoretic approach and Lyapunov function method. In the meanwhile, it is proven that the Zeno behavior can be excluded under the AIDE-TC, which ensures the feasibility of the control mechanism to realize the synchronization of SCNTM. Finally, we provide a numerical simulation on islanded microgrid systems to validate the effectiveness of main results and the simulation comparison results show that the AIDE-TC can reduce the number of event triggers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005823",
    "keywords": [
      "Aperiodic graph",
      "Artificial intelligence",
      "Biology",
      "Channel (broadcasting)",
      "Combinatorics",
      "Computer network",
      "Computer science",
      "Control (management)",
      "Control engineering",
      "Control theory (sociology)",
      "Engineering",
      "Evolutionary biology",
      "Exponential function",
      "Function (biology)",
      "Intermittent control",
      "Lyapunov function",
      "Mathematical analysis",
      "Mathematics",
      "Nonlinear system",
      "Physics",
      "Quantum mechanics",
      "Synchronization (alternating current)"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Yanfeng"
      },
      {
        "surname": "Sun",
        "given_name": "Lixia"
      },
      {
        "surname": "Chen",
        "given_name": "Lili"
      },
      {
        "surname": "Wang",
        "given_name": "Zhen"
      }
    ]
  },
  {
    "title": "Spectral integrated neural networks (SINNs) for solving forward and inverse dynamic problems",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106756",
    "abstract": "This study introduces an innovative neural network framework named spectral integrated neural networks (SINNs) to address both forward and inverse dynamic problems in three-dimensional space. In the SINNs, the spectral integration technique is utilized for temporal discretization, followed by the application of a fully connected neural network to solve the resulting partial differential equations in the spatial domain. Furthermore, the polynomial basis functions are employed to expand the unknown function, with the goal of improving the performance of SINNs in tackling inverse problems. The performance of the developed framework is evaluated through several dynamic benchmark examples encompassing linear and nonlinear heat conduction problems, linear and nonlinear wave propagation problems, inverse problem of heat conduction, and long-time heat conduction problem. The numerical results demonstrate that the SINNs can effectively and accurately solve forward and inverse problems involving heat conduction and wave propagation. Additionally, the SINNs provide precise and stable solutions for dynamic problems with extended time durations. Compared to commonly used physics-informed neural networks, the SINNs exhibit superior performance with enhanced convergence speed, computational accuracy, and efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006804",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Geometry",
      "Inverse",
      "Inverse problem",
      "Mathematical analysis",
      "Mathematics"
    ],
    "authors": [
      {
        "surname": "Qiu",
        "given_name": "Lin"
      },
      {
        "surname": "Wang",
        "given_name": "Fajie"
      },
      {
        "surname": "Qu",
        "given_name": "Wenzhen"
      },
      {
        "surname": "Gu",
        "given_name": "Yan"
      },
      {
        "surname": "Qin",
        "given_name": "Qing-Hua"
      }
    ]
  },
  {
    "title": "Stability and passivity analysis of delayed neural networks via an improved matrix-valued polynomial inequality",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106637",
    "abstract": "The stability and passivity of delayed neural networks are addressed in this paper. A novel Lyapunov–Krasovskii functional (LKF) without multiple integrals is constructed. By using an improved matrix-valued polynomial inequality (MVPI), the previous constraint involving skew-symmetric matrices within the MVPI is removed. Then, the stability and passivity criteria for delayed neural networks that are less conservative than the existing ones are proposed. Finally, three examples are employed to demonstrate the meliority and feasibility of the obtained results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005616",
    "keywords": [
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Composite material",
      "Computer science",
      "Constraint (computer-aided design)",
      "Control (management)",
      "Control theory (sociology)",
      "Electrical engineering",
      "Engineering",
      "Geometry",
      "Linear matrix inequality",
      "Machine learning",
      "Materials science",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Passivity",
      "Polynomial",
      "Skew",
      "Stability (learning theory)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Kong",
        "given_name": "Guo-Qiang"
      },
      {
        "surname": "Guo",
        "given_name": "Liang-Dong"
      }
    ]
  },
  {
    "title": "Deformation depth decoupling network for point cloud domain adaptation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106626",
    "abstract": "Recently, point cloud domain adaptation (DA) practices have been implemented to improve the generalization ability of deep learning models on point cloud data. However, variations across domains often result in decreased performance of models trained on different distributed data sources. Previous studies have focused on output-level domain alignment to address this challenge. But this approach may increase the amount of errors experienced when aligning different domains, particularly for targets that would otherwise be predicted incorrectly. Therefore, in this study, we propose an input-level discretization-based matching to enhance the generalization ability of DA. Specifically, an efficient geometric deformation depth decoupling network (3DeNet) is implemented to learn the knowledge from the source domain and embed it into an implicit feature space, which facilitates the effective constraint of unsupervised predictions for downstream tasks. Secondly, we demonstrate that the sparsity within the implicit feature space varies between domains, rendering domain differences difficult to support. Consequently, we match sets of neighboring points with different densities and biases by differentiating the adaptive densities. Finally, inter-domain differences are aligned by constraining the loss originating from and between the target domains. We conduct experiments on point cloud DA datasets PointDA-10 and PointSegDA, achieving advanced results (over 1.2% and 1% on average).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005501",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Classifier (UML)",
      "Cloud computing",
      "Computer science",
      "Constraint (computer-aided design)",
      "Control engineering",
      "Data mining",
      "Decoupling (probability)",
      "Discretization",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Engineering",
      "Geometry",
      "Matching (statistics)",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Point cloud",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Huang"
      },
      {
        "surname": "Ning",
        "given_name": "Xin"
      },
      {
        "surname": "Wang",
        "given_name": "Changshuo"
      },
      {
        "surname": "Ning",
        "given_name": "Enhao"
      },
      {
        "surname": "Li",
        "given_name": "Lusi"
      }
    ]
  },
  {
    "title": "Delay learning based on temporal coding in Spiking Neural Networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106678",
    "abstract": "Spiking Neural Networks (SNNs) hold great potential for mimicking the brain’s efficient processing of information. Although biological evidence suggests that precise spike timing is crucial for effective information encoding, contemporary SNN research mainly concentrates on adjusting connection weights. In this work, we introduce Delay Learning based on Temporal Coding (DLTC), an innovative approach that integrates delay learning with a temporal coding strategy to optimize spike timing in SNNs. DLTC utilizes a learnable delay shift, which assigns varying levels of importance to different informational elements. This is complemented by an adjustable threshold that regulates firing times, allowing for earlier or later neuron activation as needed. We have tested DLTC’s effectiveness in various contexts, including vision and auditory classification tasks, where it consistently outperformed traditional weight-only SNNs. The results indicate that DLTC achieves remarkable improvements in accuracy and computational efficiency, marking a step forward in advancing SNNs towards real-world applications. Our codes are accessible at https://github.com/sunpengfei1122/DLTC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006026",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Coding (social sciences)",
      "Computer science",
      "Encoding (memory)",
      "Machine learning",
      "Mathematics",
      "Neural coding",
      "Pattern recognition (psychology)",
      "Predictive coding",
      "Software engineering",
      "Speech recognition",
      "Spike (software development)",
      "Spiking neural network",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Pengfei"
      },
      {
        "surname": "Wu",
        "given_name": "Jibin"
      },
      {
        "surname": "Zhang",
        "given_name": "Malu"
      },
      {
        "surname": "Devos",
        "given_name": "Paul"
      },
      {
        "surname": "Botteldooren",
        "given_name": "Dick"
      }
    ]
  },
  {
    "title": "Fixed-time synchronization of delayed multiple inertial neural network with reaction-diffusion terms under cyber–physical attacks using distributed control and its application to multi-image encryption",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106743",
    "abstract": "This study examines the fixed-time synchronization (FXTS) problem of delayed multiple inertial neural networks (MINNs) against cyber–physical attacks (CPA) execute an uncertain impulse, using reaction–diffusion (RD) terms. Using fixed-time stability theory, the paper derives innovative and practical criteria for FXTS. It also introduces a MINNs to counteract CPA by executing uncertain impulses with RD terms. Designing security control laws for MINNS with RD terms poses significant challenges, particularly when these networks are tasked with cooperative functions in the presence of failures or attacks. A distributed control strategy is introduced to attain FXTS for the delayed MINNs incorporating RD terms. To examine the consequences of CPA, we will build a Lyapunov function and combine it with some M-matrix properties. Additionally, a security control law is provided to guarantee the FXTS of the consider NN system. The demonstrated settling time (ST) of the designated MINNs is provided. From an algorithmic perspective, it is notable that the security framework and control algorithm are designed to select parameters for the feedback gain matrix and coupling strength to achieve synchronization. A numerical model is provided to support the obtained theoretical findings. Finally, our proposition of a multi-image encryption algorithm, utilizing MINNs and secured by robust security protocols, serves to uphold the integrity of electronic healthcare systems, ensuring the safeguarding of sensitive medical data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006671",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Encryption",
      "Image (mathematics)",
      "Inertial frame of reference",
      "Mathematical analysis",
      "Mathematics",
      "Physics",
      "Quantum mechanics",
      "Reaction–diffusion system",
      "Synchronization (alternating current)"
    ],
    "authors": [
      {
        "surname": "Kowsalya",
        "given_name": "P."
      },
      {
        "surname": "Kathiresan",
        "given_name": "S."
      },
      {
        "surname": "Kashkynbayev",
        "given_name": "Ardak"
      },
      {
        "surname": "Rakkiyappan",
        "given_name": "R."
      }
    ]
  },
  {
    "title": "Exploring refined dual visual features cross-combination for image captioning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106710",
    "abstract": "For current image caption tasks used to encode region features and grid features Transformer-based encoders have become commonplace, because of their multi-head self-attention mechanism, the encoder can better capture the relationship between different regions in the image and contextual information. However, stacking Transformer blocks necessitates quadratic computation through self-attention to visual features, not only resulting in the computation of numerous redundant features but also significantly increasing computational overhead. This paper presents a novel Distilled Cross-Combination Transformer (DCCT) network. Technically, we first introduce a distillation cascade fusion encoder (DCFE), where a probabilistic sparse self-attention layer is used to filter out some redundant and distracting features that affect attention focus, aiming to obtain more refined visual features and enhance encoding efficiency. Next, we develop a parallel cross-fusion attention module (PCFA) that fully exploits the complementarity and correlation between grid and region features to better fuse the encoded dual visual features. Extensive experiments conducted on the MSCOCO dataset demonstrate that our proposed DCCT method achieves outstanding performance, rivaling current state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006348",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computation",
      "Computer science",
      "Computer security",
      "Computer vision",
      "ENCODE",
      "Encoder",
      "Exploit",
      "Gene",
      "Geometry",
      "Grid",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Redundancy (engineering)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Junbo"
      },
      {
        "surname": "Li",
        "given_name": "Zhixin"
      },
      {
        "surname": "Su",
        "given_name": "Qiang"
      },
      {
        "surname": "Tang",
        "given_name": "Zhenjun"
      },
      {
        "surname": "Ma",
        "given_name": "Huifang"
      }
    ]
  },
  {
    "title": "GazeForensics: DeepFake detection via gaze-guided spatial inconsistency learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106636",
    "abstract": "DeepFake detection is pivotal in personal privacy and public safety. With the iterative advancement of DeepFake techniques, high-quality forged videos and images are becoming increasingly deceptive. Prior research has seen numerous attempts by scholars to incorporate biometric features into the field of DeepFake detection. However, traditional biometric-based approaches tend to segregate biometric features from general ones and freeze the biometric feature extractor. These approaches resulted in the exclusion of valuable general features, potentially leading to a performance decline and, consequently, a failure to fully exploit the potential of biometric information in assisting DeepFake detection. Moreover, insufficient attention has been dedicated to scrutinizing gaze authenticity within the realm of DeepFake detection in recent years. In this paper, we introduce GazeForensics, an innovative DeepFake detection method that utilizes gaze representation obtained from a 3D gaze estimation model to regularize the corresponding representation within our DeepFake detection model, while concurrently integrating general features to further enhance the performance of our model. Experimental results demonstrate that our proposed GazeForensics method performs admirably in terms of performance and exhibits excellent interpretability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005604",
    "keywords": [
      "Artificial intelligence",
      "Biometrics",
      "Computer science",
      "Feature (linguistics)",
      "Gaze",
      "Interpretability",
      "Law",
      "Linguistics",
      "Machine learning",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Qinlin"
      },
      {
        "surname": "Peng",
        "given_name": "Chunlei"
      },
      {
        "surname": "Liu",
        "given_name": "Decheng"
      },
      {
        "surname": "Wang",
        "given_name": "Nannan"
      },
      {
        "surname": "Gao",
        "given_name": "Xinbo"
      }
    ]
  },
  {
    "title": "Finite-time cluster synchronization of multi-weighted fractional-order coupled neural networks with and without impulsive effects",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106646",
    "abstract": "In this paper, finite-time cluster synchronization (FTCS) of multi-weighted fractional-order neural networks is studied. Firstly, a FTCS criterion of the considered neural networks is obtained by designing a new delayed state feedback controller. Secondly, a FTCS criterion for the considered neural networks with mixed impulsive effects is given by constructing a new piecewise controller, where both synchronizing and desynchronizing impulses are taken into account. It should be noted that it is the first time that finite-time cluster synchronization of multi-weighted neural networks has been investigated. Finally, numerical simulations are given to show the validity of the theoretical results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005707",
    "keywords": [
      "Agronomy",
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Cluster (spacecraft)",
      "Combinatorics",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Controller (irrigation)",
      "Mathematical analysis",
      "Mathematics",
      "Piecewise",
      "Programming language",
      "Synchronization (alternating current)",
      "Synchronizing",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Nie",
        "given_name": "Huining"
      },
      {
        "surname": "Zhang",
        "given_name": "Yu"
      }
    ]
  },
  {
    "title": "Generative commonsense knowledge subgraph retrieval for open-domain dialogue response generation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106666",
    "abstract": "Grounding on a commonsense knowledge subgraph can help the model generate more informative and diverse dialogue responses. Prior Traverse-based works explicitly retrieve a subgraph from the external knowledge base (eKB). Notably, the available knowledge is strictly restricted by the eKB. To break this restriction, Generative Retrieval methods externalize knowledge from the language model. However, they always generate boring knowledge due to their one-pass externalization procedure. This work proposes a novel TiLM Traverse in Language Model (TiLM), which uses three ‘Chain-of-Thought’ sub-tasks, i.e., Query Entity Production, Topic Entity Prediction, and Knowledge Subgraph Completion, to build a high-quality knowledge subgraph to ground the next Response Generation without explicitly accessing the eKB in inference. Experimental results on both Chinese and English datasets demonstrate TiLM’s outstanding performance even only with a small scale of parameters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005902",
    "keywords": [
      "Artificial intelligence",
      "Commonsense knowledge",
      "Computer science",
      "Domain (mathematical analysis)",
      "Domain knowledge",
      "Generative grammar",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Sixing"
      },
      {
        "surname": "Yu",
        "given_name": "Jiong"
      },
      {
        "surname": "Chen",
        "given_name": "Jiahao"
      },
      {
        "surname": "Zhou",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Harnessing collective structure knowledge in data augmentation for graph neural networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106651",
    "abstract": "Graph neural networks (GNNs) have achieved state-of-the-art performance in graph representation learning. Message passing neural networks, which learn representations through recursively aggregating information from each node and its neighbors, are among the most commonly-used GNNs. However, a wealth of structural information of individual nodes and full graphs is often ignored in such process, which restricts the expressive power of GNNs. Various graph data augmentation methods that enable the message passing with richer structure knowledge have been introduced as one main way to tackle this issue, but they are often focused on individual structure features and difficult to scale up with more structure features. In this work we propose a novel approach, namely collective structure knowledge-augmented graph neural network (CoS-GNN), in which a new message passing method is introduced to allow GNNs to harness a diverse set of node- and graph-level structure features, together with original node features/attributes, in augmented graphs. In doing so, our approach largely improves the structural knowledge modeling of GNNs in both node and graph levels, resulting in substantially improved graph representations. This is justified by extensive empirical results where CoS-GNN outperforms state-of-the-art models in various graph-level learning tasks, including graph classification, anomaly detection, and out-of-distribution generalization.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005756",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Distributed computing",
      "Engineering",
      "Expressive power",
      "Graph",
      "Machine learning",
      "Message passing",
      "Node (physics)",
      "Structural engineering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Rongrong"
      },
      {
        "surname": "Pang",
        "given_name": "Guansong"
      },
      {
        "surname": "Chen",
        "given_name": "Ling"
      }
    ]
  },
  {
    "title": "Euclidean-Distance-Preserved Feature Reduction for efficient person re-identification",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106572",
    "abstract": "Person Re-identification (Re-ID) aims to match person images across non-overlapping cameras. The existing approaches formulate this task as fine-grained representation learning with deep neural networks, which involves extracting image features using a deep convolutional network, followed by mapping the features into a discriminative space through another smaller network, in order to make full use of all possible cues. However, recent Re-ID methods that strive to capture every cue and make the space more discriminative have resulted in longer features, ranging from 1024 to 14336, leading to higher time (distance computation) and space (feature storage) complexities. There are two potential solutions: reduction-after-training methods (such as Principal Component Analysis and Linear Discriminant Analysis) and reduction-during-training methods (such as 1 × 1 Convolution). The former utilizes a statistical approach aiming for a global optimum but lacking end-to-end optimization of large data and deep neural networks. The latter lacks theoretical guarantees and may be vulnerable to training noise such as dataset noise or initialization seed. To address these limitations, we propose a method called Euclidean-Distance-Preserving Feature Reduction (EDPFR) that combines the strengths of both reduction-after-training and reduction-during-training methods. EDPFR first formulates the feature reduction process as a matrix decomposition and derives a condition to preserve the Euclidean distance between features, thus ensuring accuracy in theory. Furthermore, the method integrates the matrix decomposition process into a deep neural network to enable end-to-end optimization and batch training, while maintaining the theoretical guarantee. The result of the EDPFR is a reduction of the feature dimensions from f a and f b to f a ′ and f b ′ , while preserving their Euclidean distance, i.e. L 2 ( f a , f b ) = L 2 ( f a ′ , f b ′ ) . In addition to its Euclidean-Distance-Preserving capability, EDPFR also features a novel feature-level distillation loss. One of the main challenges in knowledge distillation is dimension mismatch. While previous distillation losses, usually project the mismatched features to matched class-level, spatial-level, or similarity-level spaces, this can result in a loss of information and decrease the flexibility and efficiency of distillation. Our proposed feature-level distillation leverages the benefits of the Euclidean-Distance-Preserving property and performs distillation directly in the feature space, resulting in a more flexible and efficient approach. Extensive on three Re-ID datasets, Market-1501, DukeMTMC-reID and MSMT demonstrate the effectiveness of our proposed Euclidean-Distance-Preserving Feature Reduction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004969",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolutional neural network",
      "Dimensionality reduction",
      "Discriminative model",
      "Euclidean distance",
      "Feature (linguistics)",
      "Feature vector",
      "Geometry",
      "Image (mathematics)",
      "Initialization",
      "Linear discriminant analysis",
      "Linguistics",
      "Mathematics",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Principal component analysis",
      "Programming language",
      "Reduction (mathematics)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Guan’an"
      },
      {
        "surname": "Huang",
        "given_name": "Xiaowen"
      },
      {
        "surname": "Yang",
        "given_name": "Yang"
      },
      {
        "surname": "Tiwari",
        "given_name": "Prayag"
      },
      {
        "surname": "Zhang",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "JFDI: Joint Feature Differentiation and Interaction for domain adaptive object detection",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106682",
    "abstract": "In unsupervised domain adaptive object detection, learning target-specific features is pivotal in enhancing detector performance. However, previous methods mostly concentrated on aligning domain-invariant features across domains and neglected integrating the specific features. To tackle this issue, we introduce a novel feature learning method called Joint Feature Differentiation and Interaction (JFDI), which significantly boosts the adaptability of the object detector. We construct a dual-path architecture based on we proposed feature differentiate modules: One path, guided by the source domain data, utilizes multiple discriminators to confuse and align domain-invariant features. The other path, specifically tailored to the target domain, learns its distinctive characteristics based on pseudo-labeled target data. Subsequently, we implement an interactive enhanced mechanism between these paths to ensure stable learning of features and mitigate interference from pseudo-label noise during the iterative optimization. Additionally, we devise a hierarchical pseudo-label fusion module that consolidates more comprehensive and reliable results. In addition, we analyze the generalization error bound of JFDI, which provides a theoretical basis for the effectiveness of JFDI. Extensive empirical evaluations across diverse benchmark scenarios demonstrate that our method is advanced and efficient.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006063",
    "keywords": [
      "Adaptability",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Domain (mathematical analysis)",
      "Ecology",
      "Feature (linguistics)",
      "Feature learning",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Qiao",
        "given_name": "Ziteng"
      },
      {
        "surname": "Shi",
        "given_name": "Dianxi"
      },
      {
        "surname": "Jin",
        "given_name": "Songchang"
      },
      {
        "surname": "Shi",
        "given_name": "Yanyan"
      },
      {
        "surname": "Wang",
        "given_name": "Zhen"
      },
      {
        "surname": "Qiu",
        "given_name": "Chunping"
      }
    ]
  },
  {
    "title": "Image harmonization with Simple Hybrid CNN-Transformer Network",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106673",
    "abstract": "Image harmonization seeks to transfer the illumination distribution of the background to that of the foreground within a composite image. Existing methods lack the ability of establishing global–local pixel illumination dependencies between foreground and background of composite images, which is indispensable for sharp and color-consistent harmonized image generation. To overcome this challenge, we design a novel Simple Hybrid CNN-Transformer Network (SHT-Net), which is formulated into an efficient symmetrical hierarchical architecture. It is composed of two newly designed light-weight Transformer blocks. Firstly, the scale-aware gated block is designed to capture multi-scale features through different heads and expand the receptive fields, which facilitates to generate images with fine-grained details. Secondly, we introduce a simple parallel attention block, which integrates the window-based self-attention and gated channel attention in parallel, resulting in simultaneously global–local pixel illumination relationship modeling capability. Besides, we propose an efficient simple feed forward network to filter out less informative features and allow the features to contribute to generating photo-realistic harmonized results passing through. Extensive experiments on image harmonization benchmarks indicate that our method achieve promising quantitative and qualitative results. The code and pre-trained models are available at https://github.com/guanguanboy/SHT-Net.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005975",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Composite image filter",
      "Computer science",
      "Computer vision",
      "Geometry",
      "Harmonization",
      "Image (mathematics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel",
      "Quantum mechanics",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Guanlin"
      },
      {
        "surname": "Zhao",
        "given_name": "Bin"
      },
      {
        "surname": "Li",
        "given_name": "Xuelong"
      }
    ]
  },
  {
    "title": "Face Omron Ring: Proactive defense against face forgery with identity awareness",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106639",
    "abstract": "In the era of Artificial Intelligence Generated Content (AIGC), face forgery models pose significant security threats. These models have caused widespread negative impacts through the creation of forged products targeting public figures, national leaders, and other Persons-of-interest (POI). To address this, we propose the Face Omron Ring (FOR) to proactively protect the POI from face forgery. Specifically, by introducing FOR into a target face forgery model, the model will proactively refuse to forge any face image of protected identities without compromising the forgery capability for unprotected ones. We conduct extensive experiments on 4 face forgery models, StarGAN, AGGAN, AttGAN, and HiSD on the widely used large-scale face image datasets CelebA, CelebA-HQ, and PubFig83. Our results demonstrate that the proposed method can effectively protect 5000 different identities with a 100% protection success rate, for each of which only about 100 face images are needed. Our method also shows great robustness against multiple image processing attacks, such as JPEG, cropping, noise addition, and blurring. Compared to existing proactive defense methods, our method offers identity-centric protection for any image of the protected identity without requiring any special preprocessing, resulting in improved scalability and security. We hope that this work can provide a solution for responsible AIGC companies in regulating the use of face forgery models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400563X",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Biochemistry",
      "Biometrics",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Database",
      "Face (sociological concept)",
      "Gene",
      "Identity (music)",
      "Image (mathematics)",
      "Physics",
      "Preprocessor",
      "Robustness (evolution)",
      "Scalability",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Dai",
        "given_name": "Yunshu"
      },
      {
        "surname": "Fei",
        "given_name": "Jianwei"
      },
      {
        "surname": "Huang",
        "given_name": "Fangjun"
      },
      {
        "surname": "Xia",
        "given_name": "Zhihua"
      }
    ]
  },
  {
    "title": "Secure impulsive tracking of multi-agent systems with directed hypergraph topologies against hybrid deception attacks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106691",
    "abstract": "This research delves into the challenges of achieving secure consensus tracking within multi-agent systems characterized by directed hypergraph topologies, in the face of hybrid deception attacks. The hybrid discrete and continuous deception attacks are targeted at the controller communication channels and the hyperedges, respectively. To overcome these threats, an impulsive control mechanism based on hypergraph theory are introduced, and sufficient conditions are established, under which consensus can be maintained in a mean-square bounded sense, supported by rigorous mathematical proofs. Furthermore, the investigation quantifies the relationship between the mean-square bounded consensus of the multi-agent system and the intensity of the deception attacks, delineating a specific range for this error metric. The robustness and effectiveness of the proposed control method are verified through comprehensive simulation experiments, demonstrating its applicability in varied scenarios influenced by these sophisticated attacks. This study underscores the potential of hypergraph-based strategies in enhancing system resilience against complex hybrid attacks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006154",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Bounded function",
      "Chemistry",
      "Computer network",
      "Computer science",
      "Deception",
      "Discrete mathematics",
      "Gene",
      "Geometry",
      "Hypergraph",
      "Law",
      "Mathematical analysis",
      "Mathematical proof",
      "Mathematics",
      "Multi-agent system",
      "Network topology",
      "Physics",
      "Political science",
      "Resilience (materials science)",
      "Robustness (evolution)",
      "Theoretical computer science",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Zonglin"
      },
      {
        "surname": "Ling",
        "given_name": "Guang"
      },
      {
        "surname": "Ge",
        "given_name": "Ming-Feng"
      }
    ]
  },
  {
    "title": "Deep dual incomplete multi-view multi-label classification via label semantic-guided contrastive learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106674",
    "abstract": "Multi-view multi-label learning (MVML) aims to train a model that can explore the multi-view information of the input sample to obtain its accurate predictions of multiple labels. Unfortunately, a majority of existing MVML methods are based on the assumption of data completeness, making them useless in practical applications with partially missing views or some uncertain labels. Recently, many approaches have been proposed for incomplete data, but few of them can handle the case of both missing views and labels. Moreover, these few existing works commonly ignore potentially valuable information about unknown labels or do not sufficiently explore latent label information. Therefore, in this paper, we propose a label semantic-guided contrastive learning method named LSGC for the dual incomplete multi-view multi-label classification problem. Concretely, LSGC employs deep neural networks to extract high-level features of samples. Inspired by the observation of exploiting label correlations to improve the feature discriminability, we introduce a graph convolutional network to effectively capture label semantics. Furthermore, we introduce a new sample-label contrastive loss to explore the label semantic information and enhance the feature representation learning. For missing labels, we adopt a pseudo-label filling strategy and develop a weighting mechanism to explore the confidently recovered label information. We validate the framework on five standard datasets and the experimental results show that our method achieves superior performance in comparison with the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005987",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Completeness (order theory)",
      "Computer science",
      "Convolutional neural network",
      "Dual (grammatical number)",
      "Feature (linguistics)",
      "Feature learning",
      "Graph",
      "Linguistics",
      "Literature",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Multi-label classification",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Radiology",
      "Semantics (computer science)",
      "Theoretical computer science",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Cui",
        "given_name": "Jinrong"
      },
      {
        "surname": "Xie",
        "given_name": "Yazi"
      },
      {
        "surname": "Liu",
        "given_name": "Chengliang"
      },
      {
        "surname": "Huang",
        "given_name": "Qiong"
      },
      {
        "surname": "Li",
        "given_name": "Mu"
      },
      {
        "surname": "Wen",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "DMGNet: Depth mask guiding network for RGB-D salient object detection",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106751",
    "abstract": "Though depth images can provide supplementary spatial structural cues for salient object detection (SOD) task, inappropriate utilization of depth features may introduce noisy or misleading features, which may greatly destroy SOD performance. To address this issue, we propose a depth mask guiding network (DMGNet) for RGB-D SOD. In this network, a depth mask guidance module (DMGM) is designed to pre-segment the salient objects from depth images and then create masks using pre-segmented objects to guide the RGB subnetwork to extract more discriminative features. Furthermore, a feature fusion pyramid module (FFPM) is employed to acquire more informative fused features using multi-branch convolutional channels with varying receptive fields, further enhancing the fusion of cross-modal features. Extensive experiments on nine benchmark datasets demonstrate the effectiveness of the proposed network.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006750",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "RGB color model",
      "Salient"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Yinggan"
      },
      {
        "surname": "Li",
        "given_name": "Mengyao"
      }
    ]
  },
  {
    "title": "PROSE: Predicting Multiple Operators and Symbolic Expressions using multimodal transformers",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106707",
    "abstract": "Approximating nonlinear differential equations using a neural network provides a robust and efficient tool for various scientific computing tasks, including real-time predictions, inverse problems, optimal controls, and surrogate modeling. Previous works have focused on embedding dynamical systems into networks through two approaches: learning a single operator (i.e., the mapping from input parameterised functions to solutions) or learning the governing system of equations (i.e., the constitutive model relative to the state variables). Both of these approaches yield different representations for the same underlying data or function. Observing that families of differential equations often share key characteristics, we seek one network representation across a wide range of equations. Our multimodality approach, called Predicting Multiple Operators and Symbolic Expressions (PROSE), is capable of constructing multi-operators and governing equations simultaneously through a novel fusion structure. In particular, PROSE solves differential equations, predicts future states, and generates the underlying equations of motion by incorporating symbolic “words” through a language model. Experiments with 25600 distinct equations show that PROSE benefits from its multimodal nature, resulting in robust generalization (e.g. noisy observations, equation misspecification, and data imbalance) supported by comparison and ablation studies. PROSE provides a new operator learning framework that incorporates multimodal input/output and language models for solving forward and inverse problems related to differential equations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006312",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Electrical engineering",
      "Engineering",
      "Natural language processing",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yuxuan"
      },
      {
        "surname": "Zhang",
        "given_name": "Zecheng"
      },
      {
        "surname": "Schaeffer",
        "given_name": "Hayden"
      }
    ]
  },
  {
    "title": "Complex-valued soft-log threshold reweighting for sparsity of complex-valued convolutional neural networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106664",
    "abstract": "Complex-valued convolutional neural networks (CVCNNs) have been demonstrated effectiveness in classifying complex signals and synthetic aperture radar (SAR) images. However, due to the introduction of complex-valued parameters, CVCNNs tend to become redundant with heavy floating-point operations. Model sparsity is emerged as an efficient method of removing the redundancy without much loss of performance. Currently, there are few studies on the sparsity problem of CVCNNs. Therefore, a complex-valued soft-log threshold reweighting (CV-SLTR) algorithm is proposed for the design of sparse CVCNN to reduce the number of weight parameters and simplify the structure of CVCNN. On one hand, considering the difference between complex and real numbers, we redefine and derive the complex-valued log-sum threshold method. On the other hand, by considering the distinctive characteristics of complex-valued convolutional (CConv) layers and complex-valued fully connected (CFC) layers of CVCNNs, the complex-valued soft and log-sum threshold methods are respectively developed to prune the weights of different layers during the forward propagation, and the sparsity thresholds are optimized during the backward propagation by inducing a sparsity budget. Furthermore, different optimizers can be integrated with CV-SLTR. When stochastic gradient descent (SGD) is used, the convergence of CV-SLTR is proved if Lipschitzian continuity is satisfied. Experiments on the RadioML 2016.10A and S1SLC-CVDL datasets show that the proposed algorithm is efficient for the sparsity of CVCNNs. It is worth noting that the proposed algorithm has fast sparsity speed while maintaining high classification accuracy. These demonstrate the feasibility and potential of the CV-SLTR algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005884",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convergence (economics)",
      "Convolutional neural network",
      "Economic growth",
      "Economics",
      "Gradient descent",
      "Mathematics",
      "Operating system",
      "Redundancy (engineering)",
      "Synthetic aperture radar"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Jingwei"
      },
      {
        "surname": "Huang",
        "given_name": "He"
      }
    ]
  },
  {
    "title": "Boosting cross-modal retrieval in remote sensing via a novel unified attention network",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106718",
    "abstract": "With the rapid advent and abundance of remote sensing data in different modalities, cross-modal retrieval tasks have gained importance in the research community. Cross-modal retrieval belongs to the research paradigm in which the query is of one modality and the retrieved output is of the other modality. In this paper, the remote sensing (RS) data modalities considered are the earth observation optical data (aerial photos) and the corresponding hand-drawn sketches. The main challenge of the cross-modal retrieval research objective for optical remote sensing images and the corresponding sketches is the distribution gap between the shared embedding space of the modalities. Prior attempts to resolve this issue have not yielded satisfactory outcomes regarding accurately retrieving cross-modal sketch-image RS data. The state-of-the-art architectures used conventional convolutional architectures, which focused on local pixel-wise information about the modalities to be retrieved. This limits the interaction between the sketch texture and the corresponding image, making these models susceptible to overfitting datasets with particular scenarios. To circumvent this limitation, we suggest establishing multi-modal correspondence using a novel architecture of the combined self and cross-attention algorithms, SPCA-Net to minimize the modality gap by employing attention mechanisms for the query and other modalities. Efficient cross-modal retrieval is achieved through the suggested attention architecture, which empirically emphasizes the global information of the relevant query modality and bridges the domain gap through a unique pairwise cross-attention network. In addition to the novel architecture, this paper introduces a unique loss function, label-specific supervised contrastive loss, tailored to the intricacies of the task and to enhance the discriminative power of the learned embeddings. Extensive evaluations are conducted on two sketch-image remote sensing datasets, Earth-on-Canvas and RSketch. Under the same experimental conditions, the performance metrics of our proposed model beat the state-of-the-art architectures by significant margins of 16.7%, 18.9%, 33.7%, and 40.9% correspondingly.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006427",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Boosting (machine learning)",
      "Chemistry",
      "Computer science",
      "Convolutional neural network",
      "Information retrieval",
      "Machine learning",
      "Modal",
      "Modalities",
      "Modality (human–computer interaction)",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Polymer chemistry",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Choudhury",
        "given_name": "Shabnam"
      },
      {
        "surname": "Saini",
        "given_name": "Devansh"
      },
      {
        "surname": "Banerjee",
        "given_name": "Biplab"
      }
    ]
  },
  {
    "title": "Manifold-based Shapley explanations for high dimensional correlated features",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106634",
    "abstract": "Explainable artificial intelligence (XAI) holds significant importance in enhancing the reliability and transparency of network decision-making. SHapley Additive exPlanations (SHAP) is a game-theoretic approach for network interpretation, attributing confidence to inputs features to measure their importance. However, SHAP often relies on a flawed assumption that the model’s features are independent, leading to incorrect results when dealing with correlated features. In this paper, we introduce a novel manifold-based Shapley explanation method, termed Latent SHAP. Latent SHAP transforms high-dimensional data into low-dimensional manifolds to capture correlations among features. We compute Shapley values on the data manifold and devise three distinct gradient-based mapping methods to transfer them back to the high-dimensional space. Our primary objectives include: (1) correcting misinterpretations by SHAP in certain samples; (2) addressing the challenge of feature correlations in high-dimensional data interpretation; and (3) reducing algorithmic complexity through Manifold SHAP for application in complex network interpretations. Code is available at https://github.com/Teriri1999/Latent-SHAP.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005586",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Engineering",
      "Game theory",
      "Interpretation (philosophy)",
      "Machine learning",
      "Manifold (fluid mechanics)",
      "Mathematical economics",
      "Mathematics",
      "Mechanical engineering",
      "Operating system",
      "Programming language",
      "Shapley value",
      "Space (punctuation)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Xuran"
      },
      {
        "surname": "Zhu",
        "given_name": "Mingzhe"
      },
      {
        "surname": "Feng",
        "given_name": "Zhenpeng"
      },
      {
        "surname": "Stanković",
        "given_name": "Ljubiša"
      }
    ]
  },
  {
    "title": "Center-enhanced video captioning model with multimodal semantic alignment",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106744",
    "abstract": "Video captioning aims at automatically generating descriptive sentences based on the given video, establishing an association between the visual contents and textual languages, has attracted great attention and plays a significant role in many practical applications. Previous researches focus more on the aspect of caption generation, ignoring the alignment of multimodal feature and just simply concatenating them. Besides, video feature extraction is usually done in an off-line manner, which leads to the fact that the extracted feature may not adapted to the subsequent caption generation task. To improve the applicability of extracted features for downstream caption generation and to address the issue of multimodal semantic alignment fusion, we propose an end-to-end center-enhanced video captioning model with multimodal semantic alignment, which integrates feature extraction and caption generation task into a unified framework. In order to enhance the completeness of semantic features, we design a center enhancement strategy where the visual–textual deep joint semantic feature can be captured via incremental clustering, then the cluster centers can serve as the guidance for better caption generation. Moreover, we propose to promote the visual–textual multimodal alignment fusion by learning the visual and textual representation in a shared latent semantic space, so as to alleviate the multimodal misalignment problem. Experimental results on two popular datasets MSVD and MSR-VTT demonstrate that the proposed model could outperform the state-of-the-art methods, obtaining higher-quality caption results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006683",
    "keywords": [
      "Artificial intelligence",
      "Center (category theory)",
      "Chemistry",
      "Closed captioning",
      "Computer science",
      "Computer vision",
      "Crystallography",
      "Image (mathematics)",
      "Natural language processing",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Benhui"
      },
      {
        "surname": "Gao",
        "given_name": "Junyu"
      },
      {
        "surname": "Yuan",
        "given_name": "Yuan"
      }
    ]
  },
  {
    "title": "Multi-view scene matching with relation aware feature perception",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106662",
    "abstract": "For scene matching, the extraction of metric features is a challenging task in the face of multi-source and multi-view scenes. Aiming at the requirements of multi-source and multi-view scene matching, a siamese network model for Spatial Relation Aware feature perception and fusion is proposed. The key contributions of this work are as follows: (1) Seeking to enhance the coherence of multi-view image features, we investigate the relation aware feature perception. With the help of spatial relation vector decomposition, the distribution consistency perception of image features in the horizontal H → and vertical W → directions is realized. (2) In order to establish the metric consistency relationship, the large-scale local information perception strategy is studied to realize the relative trade-off scale selection under the size of mainstream aerial images and satellite images. (3) After obtaining the multi-scale metric features, in order to improve the metric confidence, the feature selection and fusion strategy is proposed. The significance of distinct feature levels in the backbone network is systematically assessed prior to fusion, leading to an enhancement in the representation of pivotal components within the metric features during the fusion process. The experimental results obtained from the University-1652 dataset and the collected real scene data affirm the efficacy of the proposed method in enhancing the reliability of the metric model. The demonstrated effectiveness of this method suggests its applicability to diverse scene matching tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005860",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Data mining",
      "Economics",
      "Feature (linguistics)",
      "Feature extraction",
      "Geography",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Metric (unit)",
      "Neuroscience",
      "Operations management",
      "Pattern recognition (psychology)",
      "Perception",
      "Philosophy",
      "Relation (database)",
      "Scale (ratio)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Bo"
      },
      {
        "surname": "Liu",
        "given_name": "Ganchao"
      },
      {
        "surname": "Yuan",
        "given_name": "Yuan"
      }
    ]
  },
  {
    "title": "Cross-domain zero-shot learning for enhanced fault diagnosis in high-voltage circuit breakers",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106681",
    "abstract": "Ensuring the stability of high-voltage circuit breakers (HVCBs) is crucial for maintaining an uninterrupted supply of electricity. Existing fault diagnosis methods typically rely on extensive labeled datasets, which are challenging to obtain due to the unique operational contexts and complex mechanical structures of HVCBs. Additionally, these methods often cater to specific HVCB models and lack generalizability across different types, limiting their practical applicability. To address these challenges, we propose a novel cross-domain zero-shot learning (CDZSL) approach specifically designed for HVCB fault diagnosis. This approach incorporates an adaptive weighted fusion strategy that combines vibration and current signals. To bypass the constraints of manual fault semantics, we develop an automatic semantic construction method. Furthermore, a multi-channel residual convolutional neural network is engineered to distill deep, low-level features, ensuring robust cross-domain diagnostic capabilities. Our model is further enhanced with a local subspace embedding technique that effectively aligns semantic features within the embedding space. Comprehensive experimental evaluations demonstrate the superior performance of our CDZSL approach in diagnosing faults across various HVCB types.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006051",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Circuit breaker",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Domain (mathematical analysis)",
      "Electrical engineering",
      "Embedding",
      "Engineering",
      "Fault (geology)",
      "Generalizability theory",
      "Geology",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Residual",
      "Seismology",
      "Statistics",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Qiuyu"
      },
      {
        "surname": "Liao",
        "given_name": "Yuxiang"
      },
      {
        "surname": "Li",
        "given_name": "Jianxing"
      },
      {
        "surname": "Xie",
        "given_name": "Jingyi"
      },
      {
        "surname": "Ruan",
        "given_name": "Jiangjun"
      }
    ]
  },
  {
    "title": "StochCA: A novel approach for exploiting pretrained models with cross-attention",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106663",
    "abstract": "Utilizing large-scale pretrained models is a well-known strategy to enhance performance on various target tasks. It is typically achieved through fine-tuning pretrained models on target tasks. However, naï ve fine-tuning may not fully leverage knowledge embedded in pretrained models. In this study, we introduce a novel fine-tuning method, called stochastic cross-attention (StochCA), specific to Transformer architectures. This method modifies the Transformer’s self-attention mechanism to selectively utilize knowledge from pretrained models during fine-tuning. Specifically, in each block, instead of self-attention, cross-attention is performed stochastically according to the predefined probability, where keys and values are extracted from the corresponding block of a pretrained model. By doing so, queries and channel-mixing multi-layer perceptron layers of a target model are fine-tuned to target tasks to learn how to effectively exploit rich representations of pretrained models. To verify the effectiveness of StochCA, extensive experiments are conducted on benchmarks in the areas of transfer learning and domain generalization, where the exploitation of pretrained models is critical. Our experimental results show the superiority of StochCA over state-of-the-art approaches in both areas. Furthermore, we demonstrate that StochCA is complementary to existing approaches, i.e., it can be combined with them to further improve performance. We release the code at https://github.com/daintlab/stochastic_cross_attention.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005872",
    "keywords": [
      "Artificial intelligence",
      "Computer science"
    ],
    "authors": [
      {
        "surname": "Seo",
        "given_name": "Seungwon"
      },
      {
        "surname": "Lee",
        "given_name": "Suho"
      },
      {
        "surname": "Hwang",
        "given_name": "Sangheum"
      }
    ]
  },
  {
    "title": "ADP-based fault-tolerant consensus control for multiagent systems with irregular state constraints",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106737",
    "abstract": "This paper investigates the consensus control issue for nonlinear multiagent systems (MASs) subject to irregular state constraints and actuator faults using an adaptive dynamic programming (ADP) algorithm. Unlike the regular state constraints considered in previous studies, this paper addresses irregular state constraints that may exhibit asymmetry, time variation, and can emerge or disappear during operation. By developing a system transformation method based on one-to-one state mapping, equivalent unconstrained MASs can be obtained. Subsequently, a finite-time distributed observer is designed to estimate the state information of the leader, and the consensus control problem is transformed into the tracking control problem for each agent to ensure that actuator faults of any agent cannot affect its neighboring agents. Then, a critic-only ADP-based fault tolerant control strategy, which consists of the optimal control policy for nominal system and online fault compensation for time-varying addictive faults, is proposed to achieve optimal tracking control. To enhance the learning efficiency of critic neural networks (NNs), an improved weight learning law utilizing stored historical data is employed, ensuring the convergence of critic NN weights towards ideal values under a finite excitation condition. Finally, a practical example of multiple manipulator systems is presented to demonstrate the effectiveness of the developed control method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006610",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Consensus",
      "Control (management)",
      "Control theory (sociology)",
      "Distributed computing",
      "Fault tolerance",
      "Multi-agent system",
      "State (computer science)"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Zijie"
      },
      {
        "surname": "Zhou",
        "given_name": "Qi"
      },
      {
        "surname": "Ren",
        "given_name": "Hongru"
      },
      {
        "surname": "Ma",
        "given_name": "Hui"
      },
      {
        "surname": "Li",
        "given_name": "Hongyi"
      }
    ]
  },
  {
    "title": "Inter-participant transfer learning with attention based domain adversarial training for P300 detection",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106655",
    "abstract": "A Brain-computer interface (BCI) system establishes a novel communication channel between the human brain and a computer. Most event related potential-based BCI applications make use of decoding models, which requires training. This training process is often time-consuming and inconvenient for new users. In recent years, deep learning models, especially participant-independent models, have garnered significant attention in the domain of ERP classification. However, individual differences in EEG signals hamper model generalization, as the ERP component and other aspects of the EEG signal vary across participants, even when they are exposed to the same stimuli. This paper proposes a novel One-source domain transfer learning method based Attention Domain Adversarial Neural Network (OADANN) to mitigate data distribution discrepancies for cross-participant classification tasks. We train and validate our proposed model on both a publicly available OpenBMI dataset and a Self-collected dataset, employing a leave one participant out cross validation scheme. Experimental results demonstrate that the proposed OADANN method achieves the highest and most robust classification performance and exhibits significant improvements when compared to baseline methods (CNN, EEGNet, ShallowNet, DeepCovNet) and domain generalization methods (ERM, Mixup, and Groupdro). These findings underscore the efficacy of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005793",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Artificial neural network",
      "Brain–computer interface",
      "Bubble",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Domain (mathematical analysis)",
      "Electroencephalography",
      "Generalization",
      "Interface (matter)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Maximum bubble pressure method",
      "Parallel computing",
      "Pattern recognition (psychology)",
      "Psychiatry",
      "Psychology",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Shurui"
      },
      {
        "surname": "Daly",
        "given_name": "Ian"
      },
      {
        "surname": "Guan",
        "given_name": "Cuntai"
      },
      {
        "surname": "Cichocki",
        "given_name": "Andrzej"
      },
      {
        "surname": "Jin",
        "given_name": "Jing"
      }
    ]
  },
  {
    "title": "Complementary information mutual learning for multimodality medical image segmentation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106670",
    "abstract": "Radiologists must utilize medical images of multiple modalities for tumor segmentation and diagnosis due to the limitations of medical imaging technology and the diversity of tumor signals. This has led to the development of multimodal learning in medical image segmentation. However, the redundancy among modalities creates challenges for existing subtraction-based joint learning methods, such as misjudging the importance of modalities, ignoring specific modal information, and increasing cognitive load. These thorny issues ultimately decrease segmentation accuracy and increase the risk of overfitting. This paper presents the complementary information mutual learning (CIML) framework, which can mathematically model and address the negative impact of inter-modal redundant information. CIML adopts the idea of addition and removes inter-modal redundant information through inductive bias-driven task decomposition and message passing-based redundancy filtering. CIML first decomposes the multimodal segmentation task into multiple subtasks based on expert prior knowledge, minimizing the information dependence between modalities. Furthermore, CIML introduces a scheme in which each modality can extract information from other modalities additively through message passing. To achieve non-redundancy of extracted information, the redundant filtering is transformed into complementary information learning inspired by the variational information bottleneck. The complementary information learning procedure can be efficiently solved by variational inference and cross-modal spatial attention. Numerical results from the verification task and standard benchmarks indicate that CIML efficiently removes redundant information between modalities, outperforming SOTA methods regarding validation accuracy and segmentation effect. To emphasize, message-passing-based redundancy filtering allows neural network visualization techniques to visualize the knowledge relationship among different modalities, which reflects interpretability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400594X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Economics",
      "Image segmentation",
      "Inference",
      "Information bottleneck method",
      "Machine learning",
      "Management",
      "Medical imaging",
      "Modalities",
      "Multi-task learning",
      "Mutual information",
      "Operating system",
      "Pattern recognition (psychology)",
      "Redundancy (engineering)",
      "Segmentation",
      "Social science",
      "Sociology",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Shen",
        "given_name": "Chuyun"
      },
      {
        "surname": "Li",
        "given_name": "Wenhao"
      },
      {
        "surname": "Chen",
        "given_name": "Haoqing"
      },
      {
        "surname": "Wang",
        "given_name": "Xiaoling"
      },
      {
        "surname": "Zhu",
        "given_name": "Fengping"
      },
      {
        "surname": "Li",
        "given_name": "Yuxin"
      },
      {
        "surname": "Wang",
        "given_name": "Xiangfeng"
      },
      {
        "surname": "Jin",
        "given_name": "Bo"
      }
    ]
  },
  {
    "title": "Joint weight optimization for partial domain adaptation via kernel statistical distance estimation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106739",
    "abstract": "The goal of Partial Domain Adaptation (PDA) is to transfer a neural network from a source domain (joint source distribution) to a distinct target domain (joint target distribution), where the source label space subsumes the target label space. To address the PDA problem, existing works have proposed to learn the marginal source weights to match the weighted marginal source distribution to the marginal target distribution. However, this is sub-optimal, since the neural network’s target performance is concerned with the joint distribution disparity, not the marginal distribution disparity. In this paper, we propose a Joint Weight Optimization (JWO) approach that optimizes the joint source weights to match the weighted joint source distribution to the joint target distribution in the neural network’s feature space. To measure the joint distribution disparity, we exploit two statistical distances: the distribution-difference-based L 2 -distance and the distribution-ratio-based χ 2 -divergence. Since these two distances are unknown in practice, we propose a Kernel Statistical Distance Estimation (KSDE) method to estimate them from the weighted source data and the target data. Our KSDE method explicitly expresses the two estimated statistical distances as functions of the joint source weights. Therefore, we can optimize the joint weights to minimize the estimated distance functions and reduce the joint distribution disparity. Finally, we achieve the PDA goal by training the neural network on the weighted source data. Experiments on several popular datasets are conducted to demonstrate the effectiveness of our approach. Intro video and Pytorch code are available at https://github.com/sentaochen/Joint-Weight-Optimation. Interested readers can also visit https://github.com/sentaochen for more source codes of the related domain adaptation, multi-source domain adaptation, and domain generalization approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006634",
    "keywords": [
      "Adaptation (eye)",
      "Architectural engineering",
      "Artificial intelligence",
      "Biology",
      "Classifier (UML)",
      "Combinatorics",
      "Computer science",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Economics",
      "Engineering",
      "Estimation",
      "Estimator",
      "Joint (building)",
      "Kernel (algebra)",
      "Kernel density estimation",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Sentao"
      }
    ]
  },
  {
    "title": "DualAttlog: Context aware dual attention networks for log-based anomaly detection",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106680",
    "abstract": "Most existing log-driven anomaly detection methods assume that logs are static and unchanged, which is often impractical. To address this, we propose a log anomaly detection model called DualAttlog. This model includes word-level and sequence-level semantic encoding modules, as well as a context-aware dual attention module. Specifically, The word-level semantic encoding module utilizes a self-matching attention mechanism to explore the interactive properties between words in log sequences. By performing word embedding and semantic encoding, it captures the associations and evolution processes between words, extracting local-level semantic information. while The sequence-level semantic encoding module encoding the entire log sequence using a pre-trained model. This extracts global semantic information, capturing overall patterns and trends in the logs. The context-aware dual attention module integrates these two levels of encoding, utilizing contextual information to reduce redundancy and enhance detection accuracy. Experimental results show that the DualAttlog model achieves an F1-Score of over 95% on 7 public datasets. Impressively, it achieves an F1-Score of 82.35% on the Real-Industrial W dataset and 83.54% on the Real-Industrial Q dataset. It outperforms existing baseline techniques on 9 datasets, demonstrating its significant advantages.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400604X",
    "keywords": [
      "Anomaly detection",
      "Art",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Dual (grammatical number)",
      "Embedding",
      "Encoding (memory)",
      "Genetics",
      "Geometry",
      "Literature",
      "Mathematics",
      "Natural language processing",
      "Operating system",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Redundancy (engineering)",
      "Semantic similarity",
      "Sequence (biology)",
      "Word (group theory)",
      "Word embedding"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Haitian"
      },
      {
        "surname": "Sun",
        "given_name": "Degang"
      },
      {
        "surname": "Huang",
        "given_name": "Weiqing"
      }
    ]
  },
  {
    "title": "State transition learning with limited data for safe control of switched nonlinear systems",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106695",
    "abstract": "Switching dynamics are prevalent in real-world systems, arising from either intrinsic changes or responses to external influences, which can be appropriately modeled by switched systems. Control synthesis for switched systems, especially integrating safety constraints, is recognized as a significant and challenging topic. This study focuses on devising a learning-based control strategy for switched nonlinear systems operating under arbitrary switching law. It aims to maintain stability and uphold safety constraints despite limited system data. To achieve these goals, we employ the control barrier function method and Lyapunov theory to synthesize a controller that delivers both safety and stability performance. To overcome the difficulties associated with constructing the specific control barrier and Lyapunov function and take advantage of switching characteristics, we create a neural control barrier function and a neural Lyapunov function separately for control policies through a state transition learning approach. These neural barrier and Lyapunov functions facilitate the design of the safe controller. The corresponding control policy is governed by learning from two components: policy loss and forward state estimation. The effectiveness of the developing scheme is verified through simulation examples.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006191",
    "keywords": [],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "Chenchen"
      },
      {
        "surname": "Chu",
        "given_name": "Kai-Fung"
      },
      {
        "surname": "Wang",
        "given_name": "Xiaomei"
      },
      {
        "surname": "Kwok",
        "given_name": "Ka-Wai"
      },
      {
        "surname": "Iida",
        "given_name": "Fumiya"
      }
    ]
  },
  {
    "title": "Interplay between depth and width for interpolation in neural ODEs",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106640",
    "abstract": "Neural ordinary differential equations have emerged as a natural tool for supervised learning from a control perspective, yet a complete understanding of the role played by their architecture remains elusive. In this work, we examine the interplay between the width p and the number of transitions between layers L (corresponding to a depth of L + 1 ). Specifically, we construct explicit controls interpolating either a finite dataset D , comprising N pairs of points in R d , or two probability measures within a Wasserstein error margin ɛ > 0 . Our findings reveal a balancing trade-off between p and L , with L scaling as 1 + O ( N / p ) for data interpolation, and as 1 + O p − 1 + ( 1 + p ) − 1 ɛ − d for measures. In the high-dimensional and wide setting where d , p > N , our result can be refined to achieve L = 0 . This naturally raises the problem of data interpolation in the autonomous regime, characterized by L = 0 . We adopt two alternative approaches: either controlling in a probabilistic sense, or by relaxing the target condition. In the first case, when p = N we develop an inductive control strategy based on a separability assumption whose probability increases with d . In the second one, we establish an explicit error decay rate with respect to p which results from applying a universal approximation theorem to a custom-built Lipschitz vector field interpolating D .",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005641",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Construct (python library)",
      "Differential equation",
      "Interpolation (computer graphics)",
      "Mathematical analysis",
      "Mathematics",
      "Motion (physics)",
      "Ode",
      "Ordinary differential equation",
      "Perspective (graphical)",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Álvarez-López",
        "given_name": "Antonio"
      },
      {
        "surname": "Slimane",
        "given_name": "Arselane Hadj"
      },
      {
        "surname": "Zuazua",
        "given_name": "Enrique"
      }
    ]
  },
  {
    "title": "Decoupling visual and identity features for adversarial palm-vein image attack",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106693",
    "abstract": "Palm-vein has been widely used for biometric recognition due to its resistance to theft and forgery. However, with the emergence of adversarial attacks, most existing palm-vein recognition methods are vulnerable to adversarial image attacks, and to the best of our knowledge, there is still no study specifically focusing on palm-vein image attacks. In this paper, we propose an adversarial palm-vein image attack network that generates highly similar adversarial palm-vein images to the original samples, but with altered palm-identities. Unlike most existing generator-oriented methods that directly learn image features via concatenated convolutional layers, our proposed network first maps palm-vein images into multi-scale high-dimensional shallow representation, and then develops attention-based dual-path feature learning modules to extensively exploit diverse palm-vein-specific features. After that, we design visual-consistency and identity-aware loss functions to specially decouple the visual and identity features to reconstruct the adversarial palm-vein images. By doing this, the visual characteristics of palm-vein images can be largely preserved while the identity information is removed in the adversarial palm-vein images, such that high-aggressive adversarial palm-vein samples can be obtained. Extensive white-box and black-box attack experiments conducted on three widely used databases clearly show the effectiveness of the proposed network.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006178",
    "keywords": [
      "Adversarial system",
      "Aesthetics",
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Control engineering",
      "Decoupling (probability)",
      "Engineering",
      "Identity (music)",
      "Image (mathematics)",
      "Palm",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Jiacheng"
      },
      {
        "surname": "Wong",
        "given_name": "Wai Keung"
      },
      {
        "surname": "Fei",
        "given_name": "Lunke"
      },
      {
        "surname": "Zhao",
        "given_name": "Shuping"
      },
      {
        "surname": "Wen",
        "given_name": "Jie"
      },
      {
        "surname": "Teng",
        "given_name": "Shaohua"
      }
    ]
  },
  {
    "title": "A dual-region speech enhancement method based on voiceprint segmentation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106683",
    "abstract": "Single-channel speech enhancement primarily relies on deep learning models to recover clean speech signals from noise-contaminated speech. These models establish a mapping relationship between noisy and clean speech. However, considering the sparse distribution characteristics of speech energy across the entire time–frequency spectrogram, constructing the mapping relationship from noisy to clean speech exhibits significant differences in regions where speech energy is concentrated and non-concentrated. Utilizing one deep model to simultaneously address these two distinct regression tasks increases the complexity of the mapping relationships, consequently restricting the model’s performance. To validate our hypothesis, we propose a dual-region speech enhancement model based on voiceprint region segmentation. Specifically, we first train a voiceprint segmentation model to classify noisy speech into two regions. Subsequently, we establish dedicated speech enhancement models for each region, with the dual-region models concurrently constructing mapping relationships for noise-corrupted speech to clean speech in distinct regions. Finally, by merging the results, the complete restored speech can be obtained. Experimental results on public datasets demonstrate that our method achieves competitive speech enhancement performance, outperforming the state-of-the-art. Ablation study results confirm the effectiveness of the proposed approach in enhancing model performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006075",
    "keywords": [
      "Artificial intelligence",
      "Background noise",
      "Computer science",
      "Energy (signal processing)",
      "Image (mathematics)",
      "Mathematics",
      "Noise (video)",
      "Noise reduction",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Spectrogram",
      "Speech enhancement",
      "Speech processing",
      "Speech recognition",
      "Statistics",
      "Telecommunications",
      "Voice activity detection"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yang"
      },
      {
        "surname": "Zhang",
        "given_name": "Wei-Tao"
      },
      {
        "surname": "Lou",
        "given_name": "Shun-Tian"
      }
    ]
  },
  {
    "title": "A multiscale distributed neural computing model database (NCMD) for neuromorphic architecture",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106727",
    "abstract": "Distributed neuromorphic architecture is a promising technique for on-chip processing of multiple tasks. Deploying the constructed model in a distributed neuromorphic system, however, remains time-consuming and challenging due to considerations such as network topology, connection rules, and compatibility with multiple programming languages. We proposed a multiscale distributed neural computing model database (NCMD), which is a framework designed for ARM-based multi-core hardware. Various neural computing components, including ion channels, synapses, and neurons, are encompassed in NCMD. We demonstrated how NCMD constructs and deploys multi-compartmental detailed neuron models as well as spiking neural networks (SNNs) in BrainS, a distributed multi-ARM neuromorphic system. We demonstrated that the electrodiffusive Pinsky–Rinzel (edPR) model developed by NCMD is well-suited for BrainS. All dynamic properties, such as changes in membrane potential and ion concentrations, can be easily explored. In addition, SNNs constructed by NCMD can achieve an accuracy of 86.67% on the test set of the Iris dataset. The proposed NCMD offers an innovative approach to applying BrainS in neuroscience, cognitive decision-making, and artificial intelligence research.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024006518",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computational neuroscience",
      "Computer architecture",
      "Computer science",
      "Distributed computing",
      "Neuromorphic engineering",
      "Spiking neural network"
    ],
    "authors": [
      {
        "surname": "Gong",
        "given_name": "Bo"
      },
      {
        "surname": "Wang",
        "given_name": "Jiang"
      },
      {
        "surname": "Chang",
        "given_name": "Siyuan"
      },
      {
        "surname": "Xue",
        "given_name": "Gang"
      },
      {
        "surname": "Wei",
        "given_name": "Xile"
      }
    ]
  },
  {
    "title": "Mask-Shift-Inference: A novel paradigm for domain generalization",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106629",
    "abstract": "Domain Generalization (DG) focuses on the Out-Of-Distribution (OOD) generalization, which is able to learn a robust model that generalizes the knowledge acquired from the source domain to the unseen target domain. However, due to the existence of the domain shift, domain-invariant representation learning is challenging. Guided by fine-grained knowledge, we propose a novel paradigm Mask-Shift-Inference (MSI) for DG based on the architecture of Convolutional Neural Networks (CNN). Different from relying on a series of constraints and assumptions for model optimization, this paradigm novelly shifts the focus to feature channels in the latent space for domain-invariant representation learning. We put forward a two-branch working mode of a main module and multiple domain-specific sub-modules. The latter can only achieve good prediction performance in its own specific domain but poor predictions in other source domains, which provides the main module with the fine-grained knowledge guidance and contributes to the improvement of the cognitive ability of MSI. Firstly, during the forward propagation of the main module, the proposed MSI accurately discards unstable channels based on spurious classifications varying across domains, which have domain-specific prediction limitations and are not conducive to generalization. In this process, a progressive scheme is adopted to adaptively increase the masking ratio according to the training progress to further reduce the risk of overfitting. Subsequently, our paradigm enters the compatible shifting stage before the formal prediction. Based on maximizing semantic retention, we implement the domain style matching and shifting through the simple transformation through Fourier transform, which can explicitly and safely shift the target domain back to the source domain whose style is closest to it, requiring no additional model updates and reducing the domain gap. Eventually, the paradigm MSI enters the formal inference stage. The updated target domain is predicted in the main module trained in the previous stage with the benefit of familiar knowledge from the nearest source domain masking scheme. Our paradigm is logically progressive, which can intuitively exclude the confounding influence of domain-specific spurious information along with mitigating domain shifts and implicitly perform semantically invariant representation learning, achieving robust OOD generalization. Extensive experimental results on PACS, VLCS, Office-Home and DomainNet datasets verify the superiority and effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005537",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Domain (mathematical analysis)",
      "Epistemology",
      "Generalization",
      "Inference",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Paradigm shift",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Shao",
        "given_name": "Youjia"
      },
      {
        "surname": "Tian",
        "given_name": "Na"
      },
      {
        "surname": "Li",
        "given_name": "Xinyi"
      },
      {
        "surname": "Zhang",
        "given_name": "Qinghao"
      },
      {
        "surname": "Zhao",
        "given_name": "Wencang"
      }
    ]
  },
  {
    "title": "Deep network embedding with dimension selection",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106512",
    "abstract": "Network embedding is a general-purpose machine learning technique that converts network data from non-Euclidean space to Euclidean space, facilitating downstream analyses for the networks. However, existing embedding methods are often optimization-based, with the embedding dimension determined in a heuristic or ad hoc way, which can cause potential bias in downstream statistical inference. Additionally, existing deep embedding methods can suffer from a nonidentifiability issue due to the universal approximation power of deep neural networks. We address these issues within a rigorous statistical framework. We treat the embedding vectors as missing data, reconstruct the network features using a sparse decoder, and simultaneously impute the embedding vectors and train the sparse decoder using an adaptive stochastic gradient Markov chain Monte Carlo (MCMC) algorithm. Under mild conditions, we show that the sparse decoder provides a parsimonious mapping from the embedding space to network features, enabling effective selection of the embedding dimension and overcoming the nonidentifiability issue encountered by existing deep embedding methods. Furthermore, we show that the embedding vectors converge weakly to a desired posterior distribution in the 2-Wasserstein distance, addressing the potential bias issue experienced by existing embedding methods. This work lays down the first theoretical foundation for network embedding within the framework of missing data imputation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004362",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Dimension (graph theory)",
      "Embedding",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Dong",
        "given_name": "Tianning"
      },
      {
        "surname": "Sun",
        "given_name": "Yan"
      },
      {
        "surname": "Liang",
        "given_name": "Faming"
      }
    ]
  },
  {
    "title": "A novel fractional-order memristive Hopfield neural network for traveling salesman problem and its FPGA implementation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106548",
    "abstract": "This paper proposes a novel fractional-order memristive Hopfield neural network (HNN) to address traveling salesman problem (TSP). Fractional-order memristive HNN can efficiently converge to a globally optimal solution, while conventional HNN tends to become stuck at a local minimum in solving TSP. Incorporating fractional-order calculus and memristors gives the system long-term memory properties and complex chaotic characteristics, resulting in faster convergence speeds and shorter average distances in solving TSP. Moreover, a novel chaotic optimization algorithm based on fractional-order memristive HNN is designed for the calculation process to deal with mutual constraint between convergence accuracy and convergence speed, which circumvents random search and diminishes the rate of invalid solutions. Numerical simulations demonstrate the effectiveness and merits of the proposed algorithm. Furthermore, Field Programmable Gate Array (FPGA) technology is utilized to implement the proposed neural network.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004726",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Economics",
      "Embedded system",
      "Field-programmable gate array",
      "Finance",
      "Hopfield network",
      "Mathematics",
      "Order (exchange)",
      "Topology (electrical circuits)",
      "Travelling salesman problem"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xiangping"
      },
      {
        "surname": "Yang",
        "given_name": "Xinsong"
      },
      {
        "surname": "Ju",
        "given_name": "Xingxing"
      }
    ]
  },
  {
    "title": "View-shuffled clustering via the modified Hungarian algorithm",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106602",
    "abstract": "In the majority of existing multi-view clustering methods, the prerequisite is that the data have the correct cross-view correspondence. However, this strong assumption may not always hold in real-world applications, giving rise to the so-called View-shuffled Problem (VsP). To address this challenge, we propose a novel multi-view clustering method, namely View-shuffled Clustering via the Modified Hungarian Algorithm (VsC-mH). Specifically, we first establish the cross-view correspondence of the shuffled data utilizing strategies of the global alignment and modified Hungarian algorithm (mH) based intra-category alignment. Subsequently, we generate the partition of the aligned data employing matrix factorization. The fusion of these two processes facilitates the interaction of information, resulting in improved quality of both data alignment and partition. VsC-mH is capable of handling the data with alignment ratios ranging from 0 to 100%. Both experimental and theoretical evidence guarantees the convergence of the proposed optimization algorithm. Extensive experimental results obtained on six practical datasets demonstrate the effectiveness and merits of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005264",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Convergence (economics)",
      "Data mining",
      "Economic growth",
      "Economics",
      "Mathematics",
      "Partition (number theory)"
    ],
    "authors": [
      {
        "surname": "Dong",
        "given_name": "Wenhua"
      },
      {
        "surname": "Wu",
        "given_name": "Xiao-Jun"
      },
      {
        "surname": "Xu",
        "given_name": "Tianyang"
      },
      {
        "surname": "Feng",
        "given_name": "Zhenhua"
      },
      {
        "surname": "Ahmed",
        "given_name": "Sara Atito Ali"
      },
      {
        "surname": "Awais",
        "given_name": "Muhammad"
      },
      {
        "surname": "Kittler",
        "given_name": "Josef"
      }
    ]
  },
  {
    "title": "Directly training temporal Spiking Neural Network with sparse surrogate gradient",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106499",
    "abstract": "Brain-inspired Spiking Neural Networks (SNNs) have attracted much attention due to their event-based computing and energy-efficient features. However, the spiking all-or-none nature has prevented direct training of SNNs for various applications. The surrogate gradient (SG) algorithm has recently enabled spiking neural networks to shine in neuromorphic hardware. However, introducing surrogate gradients has caused SNNs to lose their original sparsity, thus leading to the potential performance loss. In this paper, we first analyze the current problem of direct training using SGs and then propose Masked Surrogate Gradients (MSGs) to balance the effectiveness of training and the sparseness of the gradient, thereby improving the generalization ability of SNNs. Moreover, we introduce a temporally weighted output (TWO) method to decode the network output, reinforcing the importance of correct timesteps. Extensive experiments on diverse network structures and datasets show that training with MSG and TWO surpasses the SOTA technique.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004234",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Machine learning",
      "Meteorology",
      "Pattern recognition (psychology)",
      "Physics",
      "Spiking neural network",
      "Training (meteorology)",
      "Training set"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yang"
      },
      {
        "surname": "Zhao",
        "given_name": "Feifei"
      },
      {
        "surname": "Zhao",
        "given_name": "Dongcheng"
      },
      {
        "surname": "Zeng",
        "given_name": "Yi"
      }
    ]
  },
  {
    "title": "Mining core information by evaluating semantic importance for unpaired image captioning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106519",
    "abstract": "Recently, exciting progress has been made in the research of supervised image captioning. However, manually annotated image-annotation pair data is difficult and expensive to obtain. Therefore, unpaired image captioning becomes an emerging challenge. This paper proposes a method called Mining Core Information by Evaluating Semantic Importance (MCIESI) for Unpaired Image Captioning, which is a method for image captioning using unpaired images and sentences. The main difference from the existing methods is that MCIESI focuses on mining the information that should be described in the image and embodies them in the generated natural language that conforms to human thinking. To achieve this goal, we use scene graphs to represent the semantics of images and evaluates the importance of objects and interaction relationships to mine core information in images, which are then encouraged to be embodied in generated sentences through semantic constraint. Combined with grammatical constraint using adversarial training with real sentence corpus and relative constraint using a triplet loss, the generator is trained to generate semantically plausible and grammatically correct sentences. Extensive experiments verify the effectiveness of MCIESI.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400443X",
    "keywords": [
      "Artificial intelligence",
      "Closed captioning",
      "Computer science",
      "Core (optical fiber)",
      "Image (mathematics)",
      "Information retrieval",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Jiahui"
      },
      {
        "surname": "Li",
        "given_name": "Zhixin"
      },
      {
        "surname": "Zhang",
        "given_name": "Canlong"
      },
      {
        "surname": "Ma",
        "given_name": "Huifang"
      }
    ]
  },
  {
    "title": "Distilling mathematical reasoning capabilities into Small Language Models",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106594",
    "abstract": "This work addresses the challenge of democratizing advanced Large Language Models (LLMs) by compressing their mathematical reasoning capabilities into sub-billion parameter Small Language Models (SLMs) without compromising performance. We introduce Equation-of-Thought Distillation (EoTD), a novel technique that encapsulates the reasoning process into equation-based representations to construct an EoTD dataset for fine-tuning SLMs. Additionally, we propose the Ensemble Thoughts Distillation (ETD) framework to enhance the reasoning performance of SLMs. This involves creating a reasoning dataset with multiple thought processes, including Chain-of-Thought (CoT), Program-of-Thought (PoT), and Equation-of-Thought (EoT), and using it for fine-tuning. Our experimental performance demonstrates that EoTD significantly boosts the reasoning abilities of SLMs, while ETD enables these models to achieve state-of-the-art reasoning performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005185",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Construct (python library)",
      "Distillation",
      "Language model",
      "Organic chemistry",
      "Process (computing)",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Xunyu"
      },
      {
        "surname": "Li",
        "given_name": "Jian"
      },
      {
        "surname": "Liu",
        "given_name": "Yong"
      },
      {
        "surname": "Ma",
        "given_name": "Can"
      },
      {
        "surname": "Wang",
        "given_name": "Weiping"
      }
    ]
  },
  {
    "title": "Multi-focus image fusion with parameter adaptive dual channel dynamic threshold neural P systems",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106603",
    "abstract": "Multi-focus image fusion (MFIF) is an important technique that aims to combine the focused regions of multiple source images into a fully clear image. Decision-map methods are widely used in MFIF to maximize the preservation of information from the source images. While many decision-map methods have been proposed, they often struggle with difficulties in determining focus and non-focus boundaries, further affecting the quality of the fused images. Dynamic threshold neural P (DTNP) systems are computational models inspired by biological spiking neurons, featuring dynamic threshold and spiking mechanisms to better distinguish focused and unfocused regions for decision map generation. However, original DTNP systems require manual parameter configuration and have only one stimulus. Therefore, they are not suitable to be used directly for generating high-precision decision maps. To overcome these limitations, we propose a variant called parameter adaptive dual channel DTNP (PADCDTNP) systems. Inspired by the spiking mechanisms of PADCDTNP systems, we further develop a new MFIF method. As a new neural model, PADCDTNP systems adaptively estimate parameters according to multiple external inputs to produce decision maps with robust boundaries, resulting in high-quality fusion results. Comprehensive experiments on the Lytro/MFFW/MFI-WHU dataset show that our method achieves advanced performance and yields comparable results to the fourteen representative MFIF methods. In addition, compared to the standard DTNP systems, PADCDTNP systems improve the fusion performance and fusion efficiency on the three datasets by 5.69% and 86.03%, respectively. The codes for both the proposed method and the comparison methods are released at https://github.com/MorvanLi/MFIF-PADCDTNP.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005276",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Data mining",
      "Dual (grammatical number)",
      "Focus (optics)",
      "Fusion",
      "Image (mathematics)",
      "Image fusion",
      "Linguistics",
      "Literature",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Bo"
      },
      {
        "surname": "Zhang",
        "given_name": "Lingling"
      },
      {
        "surname": "Liu",
        "given_name": "Jun"
      },
      {
        "surname": "Peng",
        "given_name": "Hong"
      },
      {
        "surname": "Wang",
        "given_name": "Qianying"
      },
      {
        "surname": "Liu",
        "given_name": "Jiaqi"
      }
    ]
  },
  {
    "title": "Deep self-supervised spatial-variant image deblurring",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106591",
    "abstract": "Most existing model-based and learning-based image deblurring methods usually use synthetic blur-sharp training pairs to remove blur. However, these approaches do not perform well in real-world applications as the blur-sharp training pairs are difficult to be obtained and the blur in real-world scenarios is spatial-variant. In this paper, we propose a self-supervised learning-based image deblurring method that can deal with both uniform and spatial-variant blur distributions. Moreover, our method does not need for blur-sharp pairs for training. In our proposed method, we design the Deblurring Network (D-Net) and the Spatial Degradation Network (SD-Net). Specifically, the D-Net is designed for image deblurring while the SD-Net is used to simulate the spatial-variant degradation. Furthermore, the off-the-shelf pre-trained model is employed as the prior of our model, which facilitates image deblurring. Meanwhile, we design a recursive optimization strategy to accelerate the convergence of the model. Extensive experiments demonstrate that our proposed model achieves favorable performance against existing image deblurring methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400515X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deblurring",
      "Deep learning",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yaowei"
      },
      {
        "surname": "Jiang",
        "given_name": "Bo"
      },
      {
        "surname": "Shi",
        "given_name": "Zhenghao"
      },
      {
        "surname": "Chen",
        "given_name": "Xiaoxuan"
      },
      {
        "surname": "Pan",
        "given_name": "Jinshan"
      }
    ]
  },
  {
    "title": "Class-incremental learning with Balanced Embedding Discrimination Maximization",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106487",
    "abstract": "Class incremental learning is committed to solving representation learning and classification assignments while avoiding catastrophic forgetting in scenarios where categories are increasing. In this work, a unified method named Balanced Embedding Discrimination Maximization (BEDM) is developed to make the intermediate embedding more distinctive. Specifically, we utilize an orthogonality constraint based on doubly-blocked Toeplitz matrix to minimize the correlation of convolution kernels, and an algorithm for similarity visualization is introduced. Furthermore, uneven samples and distribution shift among old and new tasks eventuate strongly biased classifiers. To mitigate the imbalance, we propose an adaptive balance weighting in softmax to compensate insufficient categories dynamically. In addition, hybrid embedding learning is introduced to preserve knowledge from old models, which involves less hyper-parameters than conventional knowledge distillation. Our proposed method outperforms the existing approaches on three mainstream benchmark datasets. Moreover, we technically visualize that our method can produce a more uniform similarity histogram and more stable spectrum. Grad-CAM and t-SNE visualizations further confirm its effectiveness. Code is available at https://github.com/wqzh/BEDM.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004118",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Class (philosophy)",
      "Computer science",
      "Constraint (computer-aided design)",
      "Deep learning",
      "Embedding",
      "Geodesy",
      "Geography",
      "Geometry",
      "Image (mathematics)",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Pattern recognition (psychology)",
      "Similarity (geometry)",
      "Softmax function"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Qinglai"
      },
      {
        "surname": "Zhang",
        "given_name": "Weiqin"
      }
    ]
  },
  {
    "title": "A weighted prior tensor train decomposition method for community detection in multi-layer networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106523",
    "abstract": "Community detection in multi-layer networks stands as a prominent subject within network analysis research. However, the majority of existing techniques for identifying communities encounter two primary constraints: they lack suitability for high-dimensional data within multi-layer networks and fail to fully leverage additional auxiliary information among communities to enhance detection accuracy. To address these limitations, a novel approach named weighted prior tensor training decomposition (WPTTD) is proposed for multi-layer network community detection. Specifically, the WPTTD method harnesses the tensor feature optimization techniques to effectively manage high-dimensional data in multi-layer networks. Additionally, it employs a weighted flattened network to construct prior information for each dimension of the multi-layer network, thereby continuously exploring inter-community connections. To preserve the cohesive structure of communities and to harness comprehensive information within the multi-layer network for more effective community detection, the common community manifold learning (CCML) is integrated into the WPTTD framework for enhancing the performance. Experimental evaluations conducted on both artificial and real-world networks have verified that this algorithm outperforms several mainstream multi-layer network community detection algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004477",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Biology",
      "Composite material",
      "Computer science",
      "Decomposition",
      "Ecology",
      "Layer (electronics)",
      "Materials science",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pure mathematics",
      "Tensor (intrinsic definition)",
      "Tensor decomposition"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Siyuan"
      },
      {
        "surname": "Yang",
        "given_name": "Mingliang"
      },
      {
        "surname": "Yang",
        "given_name": "Zhijing"
      },
      {
        "surname": "Chen",
        "given_name": "Tianshui"
      },
      {
        "surname": "Xie",
        "given_name": "Jieming"
      },
      {
        "surname": "Ma",
        "given_name": "Guang"
      }
    ]
  },
  {
    "title": "Probability graph complementation contrastive learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106522",
    "abstract": "Graph Neural Network (GNN) has achieved remarkable progress in the field of graph representation learning. The most prominent characteristic, propagating features along the edges, degrades its performance in most heterophilic graphs. Certain researches make attempts to construct KNN graph to improve the graph homophily. However, there is no prior knowledge to choose proper K and they may suffer from the problem of Inconsistent Similarity Distribution (ISD). To accommodate this issue, we propose Probability Graph Complementation Contrastive Learning (PGCCL) which adaptively constructs the complementation graph. We employ Beta Mixture Model (BMM) to distinguish intra-class similarity and inter-class similarity. Based on the posterior probability, we construct Probability Complementation Graphs to form contrastive views. The contrastive learning prompts the model to preserve complementary information for each node from different views. By combining original graph embedding and complementary graph embedding, the final embedding is able to capture rich semantics in the finetuning stage. At last, comprehensive experimental results on 20 datasets including homophilic and heterophilic graphs firmly verify the effectiveness of our algorithm as well as the quality of probability complementation graph compared with other state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004465",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Complementation",
      "Computer science",
      "Gene",
      "Graph",
      "Mathematics",
      "Natural language processing",
      "Phenotype",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Wenhao"
      },
      {
        "surname": "Bai",
        "given_name": "Yuebin"
      }
    ]
  },
  {
    "title": "Optimistic sequential multi-agent reinforcement learning with motivational communication",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106547",
    "abstract": "Centralized Training with Decentralized Execution (CTDE) is a prevalent paradigm in the field of fully cooperative Multi-Agent Reinforcement Learning (MARL). Existing algorithms often encounter two major problems: independent strategies tend to underestimate the potential value of actions, leading to the convergence on sub-optimal Nash Equilibria (NE); some communication paradigms introduce added complexity to the learning process, complicating the focus on the essential elements of the messages. To address these challenges, we propose a novel method called Optimistic Sequential Soft Actor Critic with Motivational Communication (OSSMC). The key idea of OSSMC is to utilize a greedy-driven approach to explore the potential value of individual policies, named optimistic Q-values, which serve as an upper bound for the Q-value of the current policy. We then integrate a sequential update mechanism with optimistic Q-value for agents, aiming to ensure monotonic improvement in the joint policy optimization process. Moreover, we establish motivational communication modules for each agent to disseminate motivational messages to promote cooperative behaviors. Finally, we employ a value regularization strategy from the Soft Actor Critic (SAC) method to maximize entropy and improve exploration capabilities. The performance of OSSMC was rigorously evaluated against a series of challenging benchmark sets. Empirical results demonstrate that OSSMC not only surpasses current baseline algorithms but also exhibits a more rapid convergence rate.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004714",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Geodesy",
      "Geography",
      "Heuristics",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Operating system",
      "Q-learning",
      "Regularization (linguistics)",
      "Reinforcement learning"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Anqi"
      },
      {
        "surname": "Wang",
        "given_name": "Yongli"
      },
      {
        "surname": "Zhou",
        "given_name": "Xiaoliang"
      },
      {
        "surname": "Zou",
        "given_name": "Haochen"
      },
      {
        "surname": "Dong",
        "given_name": "Xu"
      },
      {
        "surname": "Che",
        "given_name": "Xun"
      }
    ]
  },
  {
    "title": "Joint computation offloading and resource allocation for end-edge collaboration in internet of vehicles via multi-agent reinforcement learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106621",
    "abstract": "Vehicular edge computing (VEC), a promising paradigm for the development of emerging intelligent transportation systems, can provide lower service latency for vehicular applications. However, it is still a challenge to fulfill the requirements of such applications with stringent latency requirements in the VEC system with limited resources. In addition, existing methods focus on handling the offloading task in a certain time slot with statically allocated resources, but ignore the heterogeneous tasks’ different resource requirements, resulting in resource wastage. To solve the real-time task offloading and heterogeneous resource allocation problem in VEC system, we propose a decentralized solution based on the attention mechanism and recurrent neural networks (RNN) with a multi-agent distributed deep deterministic policy gradient (AR-MAD4PG). First, to address the partial observability of agents, we construct a shared agent graph and propose a periodic communication mechanism that enables edge nodes to aggregate information from other edge nodes. Second, to help agents better understand the current system state, we design an RNN-based feature extraction network to capture the historical state and resource allocation information of the VEC system. Thirdly, to tackle the challenges of excessive joint observation-action space and ineffective information interference, we adopt the multi-head attention mechanism to compress the dimension of the observation-action space of agents. Finally, we build a simulation model based on the actual vehicle trajectories, and the experimental results show that our proposed method outperforms the existing approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005458",
    "keywords": [
      "Artificial intelligence",
      "Computer network",
      "Computer science",
      "Distributed computing",
      "Edge computing",
      "Enhanced Data Rates for GSM Evolution",
      "Reinforcement learning",
      "Resource allocation"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Cong"
      },
      {
        "surname": "Wang",
        "given_name": "Yaoming"
      },
      {
        "surname": "Yuan",
        "given_name": "Ying"
      },
      {
        "surname": "Peng",
        "given_name": "Sancheng"
      },
      {
        "surname": "Li",
        "given_name": "Guorui"
      },
      {
        "surname": "Yin",
        "given_name": "Pengfei"
      }
    ]
  },
  {
    "title": "Video domain adaptation for semantic segmentation using perceptual consistency matching",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106505",
    "abstract": "Unsupervised domain adaptation (UDA) aims to transfer knowledge in previous and related labeled datasets (sources) to a new unlabeled dataset (target). Despite the impressive performance, existing approaches have largely focused on image-based UDA only, and video-based UDA has been relatively understudied and received less attention due to the difficulty of adapting diverse modal video features and modeling temporal associations efficiently. To address this, existing studies use optical flow to capture motion cues between in-domain consecutive frames, but is limited by heavy compute requirements and modeling flow patterns across diverse domains is equally challenging. In this work, we propose an adversarial domain adaptation approach for video semantic segmentation that aims to align temporally associated pixels in successive source and target domain frames without relying on optical flow. Specifically, we introduce a Perceptual Consistency Matching (PCM) strategy that leverages perceptual similarity to identify pixels with high correlation across consecutive frames, and infer that such pixels should correspond to the same class. Therefore, we can enhance prediction accuracy for video-UDA by enforcing consistency not only between in-domain frames, but across domains using PCM objectives during model training. Extensive experiments on public datasets show the benefit of our approach over existing state-of-the-art UDA methods. Our approach not only addresses a crucial task in video domain adaptation but also offers notable improvements in performance with faster inference times.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004295",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Categorization",
      "Classifier (UML)",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Image (mathematics)",
      "Inference",
      "Machine learning",
      "Matching (statistics)",
      "Mathematical analysis",
      "Mathematics",
      "Optical flow",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel",
      "Segmentation",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Ullah",
        "given_name": "Ihsan"
      },
      {
        "surname": "An",
        "given_name": "Sion"
      },
      {
        "surname": "Kang",
        "given_name": "Myeongkyun"
      },
      {
        "surname": "Chikontwe",
        "given_name": "Philip"
      },
      {
        "surname": "Lee",
        "given_name": "Hyunki"
      },
      {
        "surname": "Choi",
        "given_name": "Jinwoo"
      },
      {
        "surname": "Park",
        "given_name": "Sang Hyun"
      }
    ]
  },
  {
    "title": "Cross-modal knowledge distillation for continuous sign language recognition",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106587",
    "abstract": "Continuous Sign Language Recognition (CSLR) is a task which converts a sign language video into a gloss sequence. The existing deep learning based sign language recognition methods usually rely on large-scale training data and rich supervised information. However, current sign language datasets are limited, and they are only annotated at sentence-level rather than frame-level. Inadequate supervision of sign language data poses a serious challenge for sign language recognition, which may result in insufficient training of sign language recognition models. To address above problems, we propose a cross-modal knowledge distillation method for continuous sign language recognition, which contains two teacher models and one student model. One of the teacher models is the Sign2Text dialogue teacher model, which takes a sign language video and a dialogue sentence as input and outputs the sign language recognition result. The other teacher model is the Text2Gloss translation teacher model, which targets to translate a text sentence into a gloss sequence. Both teacher models can provide information-rich soft labels to assist the training of the student model, which is a general sign language recognition model. We conduct extensive experiments on multiple commonly used sign language datasets, i.e., PHOENIX 2014T, CSL-Daily and QSL, the results show that the proposed cross-modal knowledge distillation method can effectively improve the sign language recognition accuracy by transferring multi-modal information from teacher models to the student model. Code is available at https://github.com/glq-1992/cross-modal-knowledge-distillation_new.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005112",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Classifier (UML)",
      "Computer science",
      "Language identification",
      "Language model",
      "Linguistics",
      "Modal",
      "Natural language",
      "Natural language processing",
      "Philosophy",
      "Polymer chemistry",
      "Sentence",
      "Sign language",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Liqing"
      },
      {
        "surname": "Shi",
        "given_name": "Peng"
      },
      {
        "surname": "Hu",
        "given_name": "Lianyu"
      },
      {
        "surname": "Feng",
        "given_name": "Jichao"
      },
      {
        "surname": "Zhu",
        "given_name": "Lei"
      },
      {
        "surname": "Wan",
        "given_name": "Liang"
      },
      {
        "surname": "Feng",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "DPGCL: Dual pass filtering based graph contrastive learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106517",
    "abstract": "Graph Contrastive Learning (GCL), which learns node or graph representation from supervision signals derived from the graph data itself, has recently attracted extensive research attention and achieved great success. Remarkably, most of the existing GCL encoders essentially perform low-frequency filtering on graph, which however limits their expressive power on heterophilous graphs where dissimilar nodes tend to be connected. This raises an interesting question: can high frequency be informative for GCL? In this work, we experimentally study the influence of high-frequency signals on GCL and find that adding some high-frequency signals in contrasting is beneficial for improving GCL performance. That motivates us to design a more general GCL framework beyond low-pass filtering, which simultaneously performs low-pass and high-pass signal contrasts, so as to capture both low and high-frequency information in general graphs. Furthermore, to enable the representation learning to be aware of neighbor diversity in heterophilic graphs, we propose a novel graph contrastive loss, termed Adap-infoNCE, which can automatically decide the weights of negative samples based on feature representations of neighboring nodes. Here two types of neighbors are considered, i.e., spatial neighbors and featural neighbors, whose effectiveness is verified using empirical study on synthetic datasets. Extensive experiments demonstrate that our method brings significant and consistent improvements over the base GCL approach and exceeds multiple state-of-the-art results on several unsupervised benchmarks, even surpassing the performance of supervised benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004416",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Encoder",
      "Feature learning",
      "Graph",
      "Machine learning",
      "Operating system",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Rui"
      },
      {
        "surname": "Li",
        "given_name": "Ping"
      },
      {
        "surname": "Zhang",
        "given_name": "Kai"
      }
    ]
  },
  {
    "title": "Adaptive Decision Spatio-temporal neural ODE for traffic flow forecasting with Multi-Kernel Temporal Dynamic Dilation Convolution",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106549",
    "abstract": "Traffic flow prediction is crucial for efficient traffic management. It involves predicting vehicle movement patterns to reduce congestion and enhance traffic flow. However, the highly non-linear and complex patterns commonly observed in traffic flow pose significant challenges for this task. Current Graph Neural Network (GNN) models often construct shallow networks, which limits their ability to extract deeper spatio-temporal representations. Neural ordinary differential equations for traffic prediction address over-smoothing but require significant computational resources, leading to inefficiencies, and sometimes deeper networks may lead to poorer predictions for complex traffic information. In this study, we propose an Adaptive Decision spatio-temporal Neural Ordinary Differential Network, which can adaptively determine the number of layers of ODE according to the complexity of traffic information. It can solve the over-smoothing problem better, improving overall efficiency and prediction accuracy. In addition, traditional temporal convolution methods make it difficult to deal with complex and variable traffic time information with a large time span. Therefore, we introduce a multi-kernel temporal dynamic expansive convolution to handle the traffic time information. Multi-kernel temporal dynamic expansive convolution employs a dynamic dilation strategy, dynamically adjusting the network’s receptive field across levels, effectively capturing temporal dependencies, and can better adapt to the changing time data of traffic information. Additionally, multi-kernel temporal dynamic expansive convolution integrates multi-scale convolution kernels, enabling the model to learn features across diverse temporal scales. We evaluated our proposed method on several real-world traffic datasets. Experimental results show that our method outperformed state-of-the-art benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004738",
    "keywords": [
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Convolution (computer science)",
      "Data mining",
      "Kernel (algebra)",
      "Machine learning",
      "Mathematics",
      "Ode",
      "Smoothing",
      "Traffic flow (computer networking)"
    ],
    "authors": [
      {
        "surname": "Chu",
        "given_name": "Zihao"
      },
      {
        "surname": "Ma",
        "given_name": "Wenming"
      },
      {
        "surname": "Li",
        "given_name": "Mingqi"
      },
      {
        "surname": "Chen",
        "given_name": "Hao"
      }
    ]
  },
  {
    "title": "MotionTrack: Learning motion predictor for multiple object tracking",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106539",
    "abstract": "Significant progress has been achieved in multi-object tracking (MOT) through the evolution of detection and re-identification (ReID) techniques. Despite these advancements, accurately tracking objects in scenarios with homogeneous appearance and heterogeneous motion remains a challenge. This challenge arises from two main factors: the insufficient discriminability of ReID features and the predominant utilization of linear motion models in MOT. In this context, we introduce a novel motion-based tracker, MotionTrack, centered around a learnable motion predictor that relies solely on object trajectory information. This predictor comprehensively integrates two levels of granularity in motion features to enhance the modeling of temporal dynamics and facilitate precise future motion prediction for individual objects. Specifically, the proposed approach adopts a self-attention mechanism to capture token-level information and a Dynamic MLP layer to model channel-level features. MotionTrack is a simple, online tracking approach. Our experimental results demonstrate that MotionTrack yields state-of-the-art performance on datasets such as Dancetrack and SportsMOT, characterized by highly complex object motion.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004635",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Match moving",
      "Motion (physics)",
      "Object (grammar)",
      "Pedagogy",
      "Psychology",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Xiao",
        "given_name": "Changcheng"
      },
      {
        "surname": "Cao",
        "given_name": "Qiong"
      },
      {
        "surname": "Zhong",
        "given_name": "Yujie"
      },
      {
        "surname": "Lan",
        "given_name": "Long"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiang"
      },
      {
        "surname": "Luo",
        "given_name": "Zhigang"
      },
      {
        "surname": "Tao",
        "given_name": "Dacheng"
      }
    ]
  },
  {
    "title": "Cross-layer importance evaluation for neural network pruning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106496",
    "abstract": "Filter pruning has achieved remarkable success in reducing memory consumption and speeding up inference for convolutional neural networks (CNNs). Some prior works, such as heuristic methods, attempted to search for suitable sparse structures during the pruning process, which may be expensive and time-consuming. In this paper, an efficient cross-layer importance evaluation (CIE) method is proposed to automatically calculate proportional relationships among convolutional layers. Firstly, every layer is pruned separately by grid sampling way to obtain the accuracy of the model for all sampling points. And then, contribution matrices are built to describe the importance of each layer to model accuracy. Finally, the binary search algorithm is used to search the optimal sparse structure under a target pruned value. Extensive experiments on multiple representative image classification tasks demonstrate that proposed method acquires better compression performance under a little time cost compared to existing pruning algorithms. For instance, it reduces more than 50% FLOPs with only a small loss of 0.93% and 0.43% in the top-1 and top-5 accuracy for ResNet50, respectively. At the cost of only 0.24% accuracy loss, the pruned VGG19 model parameters are successfully compressed by 27.23 × and the throughput has increased by 2.46 × . On the whole, CIE has an excellent effect on the deployment and application of the CNNs model in edge device in terms of efficiency and accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004209",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "FLOPS",
      "Filter (signal processing)",
      "Heuristic",
      "Inference",
      "Layer (electronics)",
      "Organic chemistry",
      "Parallel computing",
      "Pattern recognition (psychology)",
      "Pruning",
      "Sampling (signal processing)",
      "Telecommunications",
      "Throughput",
      "Wireless"
    ],
    "authors": [
      {
        "surname": "Lian",
        "given_name": "Youzao"
      },
      {
        "surname": "Peng",
        "given_name": "Peng"
      },
      {
        "surname": "Jiang",
        "given_name": "Kai"
      },
      {
        "surname": "Xu",
        "given_name": "Weisheng"
      }
    ]
  },
  {
    "title": "Stability and synchronization of fractional-order reaction–diffusion inertial time-delayed neural networks with parameters perturbation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106564",
    "abstract": "This study is centered around the dynamic behaviors observed in a class of fractional-order generalized reaction–diffusion inertial neural networks (FGRDINNs) with time delays. These networks are characterized by differential equations involving two distinct fractional derivatives of the state. The global uniform stability of FGRDINNs with time delays is explored utilizing Lyapunov comparison principles. Furthermore, global synchronization conditions for FGRDINNs with time delays are derived through the Lyapunov direct method, with consideration given to various feedback control strategies and parameter perturbations. The effectiveness of the theoretical findings is demonstrated through three numerical examples, and the impact of controller parameters on the error system is further investigated.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400488X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Inertial frame of reference",
      "Lyapunov function",
      "Lyapunov stability",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Nonlinear system",
      "Perturbation (astronomy)",
      "Physics",
      "Quantum mechanics",
      "Reaction–diffusion system",
      "Stability (learning theory)",
      "Synchronization (alternating current)",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Hu"
      },
      {
        "surname": "Gu",
        "given_name": "Yajuan"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaoli"
      },
      {
        "surname": "Yu",
        "given_name": "Yongguang"
      }
    ]
  },
  {
    "title": "Reweighted Alternating Direction Method of Multipliers for DNN weight pruning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106534",
    "abstract": "As Deep Neural Networks (DNNs) continue to grow in complexity and size, leading to a substantial computational burden, weight pruning techniques have emerged as an effective solution. This paper presents a novel method for dynamic regularization-based pruning, which incorporates the Alternating Direction Method of Multipliers (ADMM). Unlike conventional methods that employ simple and abrupt threshold processing, the proposed method introduces a reweighting mechanism to assign importance to the weights in DNNs. Compared to other ADMM-based methods, the new method not only achieves higher accuracy but also saves considerable time thanks to the reduced number of necessary hyperparameters. The method is evaluated on multiple architectures, including LeNet-5, ResNet-32, ResNet-56, and ResNet-50, using the MNIST, CIFAR-10, and ImageNet datasets, respectively. Experimental results demonstrate its superior performance in terms of compression ratios and accuracy compared to state-of-the-art pruning methods. In particular, on the LeNet-5 model for the MNIST dataset, it achieves compression ratios of 355.9 × with a slight improvement in accuracy; on the ResNet-50 model trained with the ImageNet dataset, it achieves compression ratios of 4.24 × without sacrificing accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004581",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Deep neural networks",
      "Hyperparameter",
      "MNIST database",
      "Pattern recognition (psychology)",
      "Pruning",
      "Regularization (linguistics)",
      "Residual neural network"
    ],
    "authors": [
      {
        "surname": "Yuan",
        "given_name": "Ming"
      },
      {
        "surname": "Du",
        "given_name": "Lin"
      },
      {
        "surname": "Jiang",
        "given_name": "Feng"
      },
      {
        "surname": "Bai",
        "given_name": "Jianchao"
      },
      {
        "surname": "Chen",
        "given_name": "Guanrong"
      }
    ]
  },
  {
    "title": "Generative subgoal oriented multi-agent reinforcement learning through potential field",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106552",
    "abstract": "Multi-agent reinforcement learning (MARL) effectively improves the learning speed of agents in sparse reward tasks with the guide of subgoals. However, existing works sever the consistency of the learning objectives of the subgoal generation and subgoal reached stages, thereby significantly inhibiting the effectiveness of subgoal learning. To address this problem, we propose a novel Potential field Subgoal-based Multi-Agent reinforcement learning (PSMA) method, which introduces the potential field (PF) to unify the two-stage learning objectives. Specifically, we design a state-to-PF representation model that describes agents’ states as potential fields, allowing easy measurement of the interaction effect for both allied and enemy agents. With the PF representation, a subgoal selector is designed to automatically generate multiple subgoals for each agent, drawn from the experience replay buffer that contains both individual and total PF values. Based on the determined subgoals, we define an intrinsic reward function to guide the agent to reach their respective subgoals while maximizing the joint action-value. Experimental results show that our method outperforms the state-of-the-art MARL method on both StarCraft II micro-management (SMAC) and Google Research Football (GRF) tasks with sparse reward settings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004763",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Consistency (knowledge bases)",
      "Evolutionary biology",
      "Field (mathematics)",
      "Function (biology)",
      "Law",
      "Machine learning",
      "Mathematics",
      "Political science",
      "Politics",
      "Pure mathematics",
      "Reinforcement learning",
      "Representation (politics)",
      "State (computer science)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Shengze"
      },
      {
        "surname": "Jiang",
        "given_name": "Hao"
      },
      {
        "surname": "Liu",
        "given_name": "Yuntao"
      },
      {
        "surname": "Zhang",
        "given_name": "Jieyuan"
      },
      {
        "surname": "Xu",
        "given_name": "Xinhai"
      },
      {
        "surname": "Liu",
        "given_name": "Donghong"
      }
    ]
  },
  {
    "title": "Differentiable self-supervised clustering with intrinsic interpretability",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106542",
    "abstract": "Self-supervised clustering has garnered widespread attention due to its ability to discover latent clustering structures without the need for external labels. However, most existing approaches on self-supervised clustering lack of inherent interpretability in the data clustering process. In this paper, we propose a differentiable self-supervised clustering method with intrinsic interpretability (DSC2I), which provides an interpretable data clustering mechanism by reformulating clustering process based on differentiable programming. To be specific, we first design a differentiable mutual information measurement to explicitly train a neural network with analytical gradients, which avoids variational inference and learns a discriminative and compact representation. Then, an interpretable clustering mechanism based on differentiable programming is devised to transform fundamental clustering process (i.e., minimum intra-cluster distance, maximum inter-cluster distance) into neural networks and convert cluster centers to learnable neural parameters, which allows us to obtain a transparent and interpretable clustering layer. Finally, a unified optimization method is designed, in which the differentiable representation learning and interpretable clustering can be optimized simultaneously in a self-supervised manner. Extensive experiments demonstrate the effectiveness of the proposed DSC2I method compared with 16 clustering approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004660",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "CURE data clustering algorithm",
      "Cluster analysis",
      "Computer science",
      "Conceptual clustering",
      "Constrained clustering",
      "Correlation clustering",
      "Data mining",
      "Differentiable function",
      "Fuzzy clustering",
      "Interpretability",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Xiaoqiang"
      },
      {
        "surname": "Jin",
        "given_name": "Zhixiang"
      },
      {
        "surname": "Mao",
        "given_name": "Yiqiao"
      },
      {
        "surname": "Ye",
        "given_name": "Yangdong"
      },
      {
        "surname": "Yu",
        "given_name": "Hui"
      }
    ]
  },
  {
    "title": "Protocol-based control for semi-Markov reaction-diffusion neural networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106556",
    "abstract": "This paper addresses the asynchronous control problem for semi-Markov reaction–diffusion neural networks (SMRDNNs) under probabilistic event-triggered protocol (PETP) scheduling. A semi-Markov process with a deterministic switching rule is introduced to characterize the stochastic behavior of these networks, effectively mitigating the impacts of arbitrary switching. Leveraging statistical data on communication-induced delays, a novel PETP is proposed that adjusts transmission frequencies through a probabilistic delay division method. The dynamic adjustment of event trigger conditions based on real-time neural network is realized, and the responsiveness of the system is enhanced, which is of great significance for improving the performance and reliability of the communication system. Additionally, a dynamic asynchronous model is introduced that more accurately captures the variations between system modes and controller modes in the network environment. Ultimately, the efficacy and superiority of the developed strategies are validated through a simulation example.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004805",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Asynchronous communication",
      "Computer network",
      "Computer science",
      "Machine learning",
      "Markov chain",
      "Markov process",
      "Mathematics",
      "Physics",
      "Power (physics)",
      "Probabilistic logic",
      "Quantum mechanics",
      "Real-time computing",
      "Reliability (semiconductor)",
      "Statistics",
      "Telecommunications network"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Na"
      },
      {
        "surname": "Qin",
        "given_name": "Wenjie"
      },
      {
        "surname": "Cheng",
        "given_name": "Jun"
      },
      {
        "surname": "Cao",
        "given_name": "Jinde"
      },
      {
        "surname": "Zhang",
        "given_name": "Dan"
      }
    ]
  },
  {
    "title": "Cube is a good form: Hyperspectral band selection via multi-dimensional and high-order structure preserved clustering",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106531",
    "abstract": "As an effective strategy for reducing the noisy and redundant information for hyperspectral imagery (HSI), hyperspectral band selection intends to select a subset of original hyperspectral bands, which boosts the subsequent different tasks. In this paper, we introduce a multi-dimensional high-order structure preserved clustering method for hyperspectral band selection, referred to as MHSPC briefly. By regarding original hyperspectral images as a tensor cube, we apply the tensor CP (CANDECOMP/PARAFAC) decomposition on it to exploit the multi-dimensional structural information as well as generate a low-dimensional latent feature representation. In order to capture the local geometrical structure along the spectral dimension, a graph regularizer is imposed on the new feature representation in the lower dimensional space. In addition, since the low rankness of HSIs is an important global property, we utilize a nuclear norm constraint on the latent feature representation matrix to capture the global data structure information. Different to most of previous clustering based hyperspectral band selection methods which vectorize each band as a vector without considering the 2-D spatial information, the proposed MHSPC can effectively capture the spatial structure as well as the spectral correlation of original hyperspectral cube in both local and global perspectives. An efficient alternatively updating algorithm with theoretical convergence guarantee is designed to solve the resultant optimization problem, and extensive experimental results on four benchmark datasets validate the effectiveness of the proposed MHSPC over other state-of-the-arts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004556",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Combinatorics",
      "Computer science",
      "Cube (algebra)",
      "Economics",
      "Finance",
      "Hyperspectral imaging",
      "Mathematics",
      "Order (exchange)",
      "Pattern recognition (psychology)",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Xiaogao"
      },
      {
        "surname": "Ding",
        "given_name": "Deqiong"
      },
      {
        "surname": "Xia",
        "given_name": "Fei"
      },
      {
        "surname": "Zhuang",
        "given_name": "Dan"
      },
      {
        "surname": "Tang",
        "given_name": "Chang"
      }
    ]
  },
  {
    "title": "Triplet-aware graph neural networks for factorized multi-modal knowledge graph entity alignment",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106479",
    "abstract": "Multi-Modal Entity Alignment (MMEA), aiming to discover matching entity pairs on two multi-modal knowledge graphs (MMKGs), is an essential task in knowledge graph fusion. Through mining feature information of MMKGs, entities are aligned to tackle the issue that an MMKG is incapable of effective integration. The recent attempt at neighbors and attribute fusion mainly focuses on aggregating multi-modal attributes, neglecting the structure effect with multi-modal attributes for entity alignment. This paper proposes an innovative approach, namely TriFac, to exploit embedding refinement for factorizing the original multi-modal knowledge graphs through a two-stage MMKG factorization. Notably, we propose triplet-aware graph neural networks to aggregate multi-relational features. We propose multi-modal fusion for aggregating multiple features and design three novel metrics to measure knowledge graph factorization performance on the unified factorized latent space. Empirical results indicate the effectiveness of TriFac, surpassing previous state-of-the-art models on two MMEA datasets and a power system dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004039",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Data mining",
      "Embedding",
      "Exploit",
      "Graph",
      "Machine learning",
      "Modal",
      "Polymer chemistry",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Qian"
      },
      {
        "surname": "Li",
        "given_name": "Jianxin"
      },
      {
        "surname": "Wu",
        "given_name": "Jia"
      },
      {
        "surname": "Peng",
        "given_name": "Xutan"
      },
      {
        "surname": "Ji",
        "given_name": "Cheng"
      },
      {
        "surname": "Peng",
        "given_name": "Hao"
      },
      {
        "surname": "Wang",
        "given_name": "Lihong"
      },
      {
        "surname": "Yu",
        "given_name": "Philip S."
      }
    ]
  },
  {
    "title": "Subgraph-level federated graph neural network for privacy-preserving recommendation with meta-learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106574",
    "abstract": "Graph neural networks (GNN) are widely used in recommendation systems, but traditional centralized methods raise privacy concerns. To address this, we introduce a federated framework for privacy-preserving GNN-based recommendations. This framework allows distributed training of GNN models using local user data. Each client trains a GNN using its own user–item graph and uploads gradients to a central server for aggregation. To overcome limited data, we propose expanding local graphs using Software Guard Extension (SGX) and Local Differential Privacy (LDP). SGX computes node intersections for subgraph exchange and expansion, while local differential privacy ensures privacy. Additionally, we introduce a personalized approach with Prototype Networks (PN) and Model-Agnostic Meta-Learning (MAML) to handle data heterogeneity. This enhances the encoding abilities of the federated meta-learner, enabling precise fine-tuning and quick adaptation to diverse client graph data. We leverage SGX and local differential privacy for secure parameter sharing and defense against malicious servers. Comprehensive experiments across six datasets demonstrate our method’s superiority over centralized GNN-based recommendations, while preserving user privacy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004982",
    "keywords": [
      "Computer network",
      "Computer science",
      "Data mining",
      "Differential privacy",
      "Distributed computing",
      "Graph",
      "Leverage (statistics)",
      "Machine learning",
      "Server",
      "Theoretical computer science",
      "Upload",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Han",
        "given_name": "Zhaoxing"
      },
      {
        "surname": "Hu",
        "given_name": "Chengyu"
      },
      {
        "surname": "Li",
        "given_name": "Tongyaqi"
      },
      {
        "surname": "Qi",
        "given_name": "Qingqiang"
      },
      {
        "surname": "Tang",
        "given_name": "Peng"
      },
      {
        "surname": "Guo",
        "given_name": "Shanqing"
      }
    ]
  },
  {
    "title": "Real-time hardware emulation of neural cultures: A comparative study of in vitro, in silico and in duris silico models",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106593",
    "abstract": "Biological neural networks are well known for their capacity to process information with extremely low power consumption. Fields such as Artificial Intelligence, with high computational costs, are seeking for alternatives inspired in biological systems. An inspiring alternative is to implement hardware architectures that replicate the behavior of biological neurons but with the flexibility in programming capabilities of an electronic device, all combined with a relatively low operational cost. To advance in this quest, here we analyze the capacity of the HEENS hardware architecture to operate in a similar manner as an in vitro neuronal network grown in the laboratory. For that, we considered data of spontaneous activity in living neuronal cultures of about 400 neurons and compared their collective dynamics and functional behavior with those obtained from direct numerical simulations (in silico) and hardware implementations (in duris silico). The results show that HEENS is capable to mimic both the in vitro and in silico systems with high efficient-cost ratio, and on different network topological designs. Our work shows that compact low-cost hardware implementations are feasible, opening new avenues for future, highly efficient neuromorphic devices and advanced human–machine interfacing.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005173",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Biology",
      "Computer architecture",
      "Computer engineering",
      "Computer hardware",
      "Computer science",
      "Distributed computing",
      "Economic growth",
      "Economics",
      "Embedded system",
      "Emulation",
      "Flexibility (engineering)",
      "Gene",
      "Implementation",
      "In silico",
      "Interfacing",
      "Mathematics",
      "Neuromorphic engineering",
      "Operating system",
      "Process (computing)",
      "Programming language",
      "Spiking neural network",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Vallejo-Mancero",
        "given_name": "Bernardo"
      },
      {
        "surname": "Faci-Lázaro",
        "given_name": "Sergio"
      },
      {
        "surname": "Zapata",
        "given_name": "Mireya"
      },
      {
        "surname": "Soriano",
        "given_name": "Jordi"
      },
      {
        "surname": "Madrenas",
        "given_name": "Jordi"
      }
    ]
  },
  {
    "title": "An attribution graph-based interpretable method for CNNs",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106597",
    "abstract": "Convolutional Neural Networks (CNNs) have demonstrated outstanding performance in various domains, such as face recognition, object detection, and image segmentation. However, the lack of transparency and limited interpretability inherent in CNNs pose challenges in fields such as medical diagnosis, autonomous driving, finance, and military applications. Several studies have explored the interpretability of CNNs and proposed various post-hoc interpretable methods. The majority of these methods are feature-based, focusing on the influence of input variables on outputs. Few methods undertake the analysis of parameters in CNNs and their overall structure. To explore the structure of CNNs and intuitively comprehend the role of their internal parameters, we propose an Attribution Graph-based Interpretable method for CNNs (AGIC) which models the overall structure of CNNs as graphs and provides interpretability from global and local perspectives. The runtime parameters of CNNs and feature maps of each image sample are applied to construct attribution graphs (At-GCs), where the convolutional kernels are represented as nodes and the SHAP values between kernel outputs are assigned as edges. These At-GCs are then employed to pretrain a newly designed heterogeneous graph encoder based on Deep Graph Infomax (DGI). To comprehensively delve into the overall structure of CNNs, the pretrained encoder is used for two types of interpretable tasks: (1) a classifier is attached to the pretrained encoder for the classification of At-GCs, revealing the dependency of At-GC’s topological characteristics on the image sample categories, and (2) a scoring aggregation (SA) network is constructed to assess the importance of each node in At-GCs, thus reflecting the relative importance of kernels in CNNs. The experimental results indicate that the topological characteristics of At-GC exhibit a dependency on the sample category used in its construction, which reveals that kernels in CNNs show distinct combined activation patterns for processing different image categories, meanwhile, the kernels that receive high scores from SA network are crucial for feature extraction, whereas low-scoring kernels can be pruned without affecting model performance, thereby enhancing the interpretability of CNNs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005215",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Classifier (UML)",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Graph",
      "Interpretability",
      "Machine learning",
      "Named graph",
      "Pattern recognition (psychology)",
      "RDF",
      "SPARQL",
      "Segmentation",
      "Semantic Web",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Xiangwei"
      },
      {
        "surname": "Zhang",
        "given_name": "Lifeng"
      },
      {
        "surname": "Xu",
        "given_name": "Chunyan"
      },
      {
        "surname": "Chen",
        "given_name": "Xuanchi"
      },
      {
        "surname": "Cui",
        "given_name": "Zhen"
      }
    ]
  },
  {
    "title": "Sequential action-induced invariant representation for reinforcement learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106579",
    "abstract": "How to accurately learn task-relevant state representations from high-dimensional observations with visual distractions is a realistic and challenging problem in visual reinforcement learning. Recently, unsupervised representation learning methods based on bisimulation metrics, contrast, prediction, and reconstruction have shown the ability for task-relevant information extraction. However, due to the lack of appropriate mechanisms for the extraction of task information in the prediction, contrast, and reconstruction-related approaches and the limitations of bisimulation-related methods in domains with sparse rewards, it is still difficult for these methods to be effectively extended to environments with distractions. To alleviate these problems, in the paper, the action sequences, which contain task-intensive signals, are incorporated into representation learning. Specifically, we propose a Sequential Action–induced invariant Representation (SAR) method, which decouples the controlled part (i.e., task-relevant information) and the uncontrolled part (i.e., task-irrelevant information) in noisy observations through sequential actions, thereby extracting effective representations related to decision tasks. To achieve it, the characteristic function of the action sequence’s probability distribution is modeled to specifically optimize the state encoder. We conduct extensive experiments on the distracting DeepMind Control suite while achieving the best performance over strong baselines. We also demonstrate the effectiveness of our method at disregarding task-irrelevant information by applying SAR to real-world CARLA-based autonomous driving with natural distractions. Finally, we provide the analysis results of generalization drawn from the generalization decay and t-SNE visualization. Code and demo videos are available at https://github.com/DMU-XMU/SAR.git.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005033",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Computer science",
      "Deep learning",
      "Economics",
      "Feature learning",
      "Generalization",
      "Invariant (physics)",
      "Law",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematical physics",
      "Mathematics",
      "Physics",
      "Political science",
      "Politics",
      "Reinforcement learning",
      "Representation (politics)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Liang",
        "given_name": "Dayang"
      },
      {
        "surname": "Chen",
        "given_name": "Qihang"
      },
      {
        "surname": "Liu",
        "given_name": "Yunlong"
      }
    ]
  },
  {
    "title": "Pre-gating and contextual attention gate — A new fusion method for multi-modal data tasks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106553",
    "abstract": "Multi-modal representation learning has received significant attention across diverse research domains due to its ability to model a scenario comprehensively. Learning the cross-modal interactions is essential to combining multi-modal data into a joint representation. However, conventional cross-attention mechanisms can produce noisy and non-meaningful values in the absence of useful cross-modal interactions among input features, thereby introducing uncertainty into the feature representation. These factors have the potential to degrade the performance of downstream tasks. This paper introduces a novel Pre-gating and Contextual Attention Gate (PCAG) module for multi-modal learning comprising two gating mechanisms that operate at distinct information processing levels within the deep learning model. The first gate filters out interactions that lack informativeness for the downstream task, while the second gate reduces the uncertainty introduced by the cross-attention module. Experimental results on eight multi-modal classification tasks spanning various domains show that the multi-modal fusion model with PCAG outperforms state-of-the-art multi-modal fusion models. Additionally, we elucidate how PCAG effectively processes cross-modality interactions",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004775",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Chemistry",
      "Computer science",
      "Epistemology",
      "Feature (linguistics)",
      "Feature learning",
      "Gating",
      "Law",
      "Linguistics",
      "Machine learning",
      "Modal",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physiology",
      "Political science",
      "Politics",
      "Polymer chemistry",
      "Property (philosophy)",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Duoyi"
      },
      {
        "surname": "Nayak",
        "given_name": "Richi"
      },
      {
        "surname": "Bashar",
        "given_name": "Md Abul"
      }
    ]
  },
  {
    "title": "Unsupervised domain adaptive building semantic segmentation network by edge-enhanced contrastive learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106581",
    "abstract": "Unsupervised domain adaptation (UDA) is a weakly supervised learning technique that classifies images in the target domain when the source domain has labeled samples, and the target domain has unlabeled samples. Due to the complexity of imaging conditions and the content of remote sensing images, the use of UDA to accurately extract artificial features such as buildings from high-spatial-resolution (HSR) imagery is still challenging. In this study, we propose a new UDA method for building extraction, the contrastive domain adaptation network (CDANet), by utilizing adversarial learning and contrastive learning techniques. CDANet consists of a single multitask generator and dual discriminators. The generator employs a region and edge dual-branch structure that strengthens its edge extraction ability and is beneficial for the extraction of small and densely distributed buildings. The dual discriminators receive the region and edge prediction outputs and achieve multilevel adversarial learning. During adversarial training processing, CDANet aligns the cross-domain of similar pixel features in the embedding space by constructing the regional pixelwise contrastive loss. A self-training (ST) strategy based on pseudolabel generation is further utilized to address the target intradomain discrepancy. Comprehensive experiments are conducted to validate CDANet on three publicly accessible datasets, namely the WHU, Austin, and Massachusetts. Ablation experiments show that the generator network structure, contrastive loss and ST strategy all improve the building extraction accuracy. Method comparisons validate that CDANet achieves superior performance to several state-of-the-art methods, including AdaptSegNet, AdvEnt, IntraDA, FDANet and ADRS, in terms of F1 score and mIoU.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005057",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Domain (mathematical analysis)",
      "Enhanced Data Rates for GSM Evolution",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Mengyuan"
      },
      {
        "surname": "Yang",
        "given_name": "Rui"
      },
      {
        "surname": "Tao",
        "given_name": "Shikang"
      },
      {
        "surname": "Zhang",
        "given_name": "Xin"
      },
      {
        "surname": "Wang",
        "given_name": "Min"
      }
    ]
  },
  {
    "title": "Subgraph-level federated graph neural network for privacy-preserving recommendation with meta-learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106574",
    "abstract": "Graph neural networks (GNN) are widely used in recommendation systems, but traditional centralized methods raise privacy concerns. To address this, we introduce a federated framework for privacy-preserving GNN-based recommendations. This framework allows distributed training of GNN models using local user data. Each client trains a GNN using its own user–item graph and uploads gradients to a central server for aggregation. To overcome limited data, we propose expanding local graphs using Software Guard Extension (SGX) and Local Differential Privacy (LDP). SGX computes node intersections for subgraph exchange and expansion, while local differential privacy ensures privacy. Additionally, we introduce a personalized approach with Prototype Networks (PN) and Model-Agnostic Meta-Learning (MAML) to handle data heterogeneity. This enhances the encoding abilities of the federated meta-learner, enabling precise fine-tuning and quick adaptation to diverse client graph data. We leverage SGX and local differential privacy for secure parameter sharing and defense against malicious servers. Comprehensive experiments across six datasets demonstrate our method’s superiority over centralized GNN-based recommendations, while preserving user privacy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004982",
    "keywords": [
      "Computer network",
      "Computer science",
      "Data mining",
      "Differential privacy",
      "Distributed computing",
      "Graph",
      "Leverage (statistics)",
      "Machine learning",
      "Server",
      "Theoretical computer science",
      "Upload",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Han",
        "given_name": "Zhaoxing"
      },
      {
        "surname": "Hu",
        "given_name": "Chengyu"
      },
      {
        "surname": "Li",
        "given_name": "Tongyaqi"
      },
      {
        "surname": "Qi",
        "given_name": "Qingqiang"
      },
      {
        "surname": "Tang",
        "given_name": "Peng"
      },
      {
        "surname": "Guo",
        "given_name": "Shanqing"
      }
    ]
  },
  {
    "title": "Real-time hardware emulation of neural cultures: A comparative study of in vitro, in silico and in duris silico models",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106593",
    "abstract": "Biological neural networks are well known for their capacity to process information with extremely low power consumption. Fields such as Artificial Intelligence, with high computational costs, are seeking for alternatives inspired in biological systems. An inspiring alternative is to implement hardware architectures that replicate the behavior of biological neurons but with the flexibility in programming capabilities of an electronic device, all combined with a relatively low operational cost. To advance in this quest, here we analyze the capacity of the HEENS hardware architecture to operate in a similar manner as an in vitro neuronal network grown in the laboratory. For that, we considered data of spontaneous activity in living neuronal cultures of about 400 neurons and compared their collective dynamics and functional behavior with those obtained from direct numerical simulations (in silico) and hardware implementations (in duris silico). The results show that HEENS is capable to mimic both the in vitro and in silico systems with high efficient-cost ratio, and on different network topological designs. Our work shows that compact low-cost hardware implementations are feasible, opening new avenues for future, highly efficient neuromorphic devices and advanced human–machine interfacing.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005173",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Biology",
      "Computer architecture",
      "Computer engineering",
      "Computer hardware",
      "Computer science",
      "Distributed computing",
      "Economic growth",
      "Economics",
      "Embedded system",
      "Emulation",
      "Flexibility (engineering)",
      "Gene",
      "Implementation",
      "In silico",
      "Interfacing",
      "Mathematics",
      "Neuromorphic engineering",
      "Operating system",
      "Process (computing)",
      "Programming language",
      "Spiking neural network",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Vallejo-Mancero",
        "given_name": "Bernardo"
      },
      {
        "surname": "Faci-Lázaro",
        "given_name": "Sergio"
      },
      {
        "surname": "Zapata",
        "given_name": "Mireya"
      },
      {
        "surname": "Soriano",
        "given_name": "Jordi"
      },
      {
        "surname": "Madrenas",
        "given_name": "Jordi"
      }
    ]
  },
  {
    "title": "Cross-view motion consistent self-supervised video inter-intra contrastive for action representation understanding",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106578",
    "abstract": "Self-supervised contrastive learning draws on power representational models to acquire generic semantic features from unlabeled data, and the key to training such models lies in how accurately to track motion features. Previous video contrastive learning methods have extensively used spatially or temporally augmentation as similar instances, resulting in models that are more likely to learn static backgrounds than motion features. To alleviate the background shortcuts, in this paper, we propose a cross-view motion consistent (CVMC) self-supervised video inter-intra contrastive model to focus on the learning of local details and long-term temporal relationships. Specifically, we first extract the dynamic features of consecutive video snippets and then align these features based on multi-view motion consistency. Meanwhile, we compare the optimized dynamic features for instance comparison of different videos and local spatial fine-grained with temporal order in the same video, respectively. Ultimately, the joint optimization of spatio-temporal alignment and motion discrimination effectively fills the challenges of the missing components of instance recognition, spatial compactness, and temporal perception in self-supervised learning. Experimental results show that our proposed self-supervised model can effectively learn visual representation information and achieve highly competitive performance compared to other state-of-the-art methods in both action recognition and video retrieval tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005021",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Feature (linguistics)",
      "Feature learning",
      "Focus (optics)",
      "Law",
      "Linguistics",
      "Machine learning",
      "Motion (physics)",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Bi",
        "given_name": "Shuai"
      },
      {
        "surname": "Hu",
        "given_name": "Zhengping"
      },
      {
        "surname": "Zhang",
        "given_name": "Hehao"
      },
      {
        "surname": "Di",
        "given_name": "Jirui"
      },
      {
        "surname": "Sun",
        "given_name": "Zhe"
      }
    ]
  },
  {
    "title": "DGSD: Dynamical graph self-distillation for EEG-based auditory spatial attention detection",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106580",
    "abstract": "Auditory Attention Detection (AAD) aims to detect the target speaker from brain signals in a multi-speaker environment. Although EEG-based AAD methods have shown promising results in recent years, current approaches primarily rely on traditional convolutional neural networks designed for processing Euclidean data like images. This makes it challenging to handle EEG signals, which possess non-Euclidean characteristics. In order to address this problem, this paper proposes a dynamical graph self-distillation (DGSD) approach for AAD, which does not require speech stimuli as input. Specifically, to effectively represent the non-Euclidean properties of EEG signals, dynamical graph convolutional networks are applied to represent the graph structure of EEG signals, which can also extract crucial features related to auditory spatial attention in EEG signals. In addition, to further improve AAD detection performance, self-distillation, consisting of feature distillation and hierarchical distillation strategies at each layer, is integrated. These strategies leverage features and classification results from the deepest network layers to guide the learning of shallow layers. Our experiments are conducted on two publicly available datasets, KUL and DTU. Under a 1-second time window, we achieve results of 90.0% and 79.6% accuracy on KUL and DTU, respectively. We compare our DGSD method with competitive baselines, and the experimental results indicate that the detection performance of our proposed DGSD method is not only superior to the best reproducible baseline but also significantly reduces the number of trainable parameters by approximately 100 times.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005045",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Convolutional neural network",
      "Distillation",
      "Electroencephalography",
      "Euclidean distance",
      "Graph",
      "Leverage (statistics)",
      "Machine learning",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Psychiatry",
      "Psychology",
      "Speech recognition",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "Cunhang"
      },
      {
        "surname": "Zhang",
        "given_name": "Hongyu"
      },
      {
        "surname": "Huang",
        "given_name": "Wei"
      },
      {
        "surname": "Xue",
        "given_name": "Jun"
      },
      {
        "surname": "Tao",
        "given_name": "Jianhua"
      },
      {
        "surname": "Yi",
        "given_name": "Jiangyan"
      },
      {
        "surname": "Lv",
        "given_name": "Zhao"
      },
      {
        "surname": "Wu",
        "given_name": "Xiaopei"
      }
    ]
  },
  {
    "title": "Protocol-based control for semi-Markov reaction-diffusion neural networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106556",
    "abstract": "This paper addresses the asynchronous control problem for semi-Markov reaction–diffusion neural networks (SMRDNNs) under probabilistic event-triggered protocol (PETP) scheduling. A semi-Markov process with a deterministic switching rule is introduced to characterize the stochastic behavior of these networks, effectively mitigating the impacts of arbitrary switching. Leveraging statistical data on communication-induced delays, a novel PETP is proposed that adjusts transmission frequencies through a probabilistic delay division method. The dynamic adjustment of event trigger conditions based on real-time neural network is realized, and the responsiveness of the system is enhanced, which is of great significance for improving the performance and reliability of the communication system. Additionally, a dynamic asynchronous model is introduced that more accurately captures the variations between system modes and controller modes in the network environment. Ultimately, the efficacy and superiority of the developed strategies are validated through a simulation example.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004805",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Asynchronous communication",
      "Computer network",
      "Computer science",
      "Machine learning",
      "Markov chain",
      "Markov process",
      "Mathematics",
      "Physics",
      "Power (physics)",
      "Probabilistic logic",
      "Quantum mechanics",
      "Real-time computing",
      "Reliability (semiconductor)",
      "Statistics",
      "Telecommunications network"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Na"
      },
      {
        "surname": "Qin",
        "given_name": "Wenjie"
      },
      {
        "surname": "Cheng",
        "given_name": "Jun"
      },
      {
        "surname": "Cao",
        "given_name": "Jinde"
      },
      {
        "surname": "Zhang",
        "given_name": "Dan"
      }
    ]
  },
  {
    "title": "Cube is a good form: Hyperspectral band selection via multi-dimensional and high-order structure preserved clustering",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106531",
    "abstract": "As an effective strategy for reducing the noisy and redundant information for hyperspectral imagery (HSI), hyperspectral band selection intends to select a subset of original hyperspectral bands, which boosts the subsequent different tasks. In this paper, we introduce a multi-dimensional high-order structure preserved clustering method for hyperspectral band selection, referred to as MHSPC briefly. By regarding original hyperspectral images as a tensor cube, we apply the tensor CP (CANDECOMP/PARAFAC) decomposition on it to exploit the multi-dimensional structural information as well as generate a low-dimensional latent feature representation. In order to capture the local geometrical structure along the spectral dimension, a graph regularizer is imposed on the new feature representation in the lower dimensional space. In addition, since the low rankness of HSIs is an important global property, we utilize a nuclear norm constraint on the latent feature representation matrix to capture the global data structure information. Different to most of previous clustering based hyperspectral band selection methods which vectorize each band as a vector without considering the 2-D spatial information, the proposed MHSPC can effectively capture the spatial structure as well as the spectral correlation of original hyperspectral cube in both local and global perspectives. An efficient alternatively updating algorithm with theoretical convergence guarantee is designed to solve the resultant optimization problem, and extensive experimental results on four benchmark datasets validate the effectiveness of the proposed MHSPC over other state-of-the-arts.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004556",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Clustering high-dimensional data",
      "Combinatorics",
      "Computer science",
      "Cube (algebra)",
      "Economics",
      "Finance",
      "Hyperspectral imaging",
      "Mathematics",
      "Order (exchange)",
      "Pattern recognition (psychology)",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Xiaogao"
      },
      {
        "surname": "Ding",
        "given_name": "Deqiong"
      },
      {
        "surname": "Xia",
        "given_name": "Fei"
      },
      {
        "surname": "Zhuang",
        "given_name": "Dan"
      },
      {
        "surname": "Tang",
        "given_name": "Chang"
      }
    ]
  },
  {
    "title": "Less confidence, less forgetting: Learning with a humbler teacher in exemplar-free Class-Incremental learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106513",
    "abstract": "Class-Incremental learning (CIL) is challenging due to catastrophic forgetting (CF), which escalates in exemplar-free scenarios. To mitigate CF, Knowledge Distillation (KD), which leverages old models as teacher models, has been widely employed in CIL. However, based on a case study, our investigation reveals that the teacher model exhibits over-confidence in unseen new samples. In this article, we conduct empirical experiments and provide theoretical analysis to investigate the over-confident phenomenon and the impact of KD in exemplar-free CIL, where access to old samples is unavailable. Building on our analysis, we propose a novel approach, Learning with Humbler Teacher, by systematically selecting an appropriate checkpoint model as a humbler teacher to mitigate CF. Furthermore, we explore utilizing the nuclear norm to obtain an appropriate temporal ensemble to enhance model stability. Notably, LwHT outperforms the state-of-the-art approach by a significant margin of 10.41%, 6.56%, and 4.31% in various settings while demonstrating superior model plasticity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004374",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Cognitive psychology",
      "Computer science",
      "Forgetting",
      "Incremental learning",
      "Law",
      "Machine learning",
      "Margin (machine learning)",
      "Norm (philosophy)",
      "Political science",
      "Psychology",
      "Stability (learning theory)"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Zijian"
      },
      {
        "surname": "Xu",
        "given_name": "Kele"
      },
      {
        "surname": "Zhuang",
        "given_name": "Huiping"
      },
      {
        "surname": "Liu",
        "given_name": "Li"
      },
      {
        "surname": "Mao",
        "given_name": "Xinjun"
      },
      {
        "surname": "Ding",
        "given_name": "Bo"
      },
      {
        "surname": "Feng",
        "given_name": "Dawei"
      },
      {
        "surname": "Wang",
        "given_name": "Huaimin"
      }
    ]
  },
  {
    "title": "A collaborative neurodynamic approach with two-timescale projection neural networks designed via majorization-minimization for global optimization and distributed global optimization",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106525",
    "abstract": "In this paper, two two-timescale projection neural networks are proposed based on the majorization-minimization principle for nonconvex optimization and distributed nonconvex optimization. They are proved to be globally convergent to Karush–Kuhn–Tucker points. A collaborative neurodynamic approach leverages multiple two-timescale projection neural networks repeatedly re-initialized using a meta-heuristic rule for global optimization and distributed global optimization. Two numerical examples are elaborated to demonstrate the efficacy of the proposed approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004490",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Deep neural networks",
      "Global optimization",
      "Machine learning",
      "Majorization",
      "Mathematical optimization",
      "Mathematics",
      "Minification",
      "Programming language",
      "Projection (relational algebra)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Yangxia"
      },
      {
        "surname": "Xia",
        "given_name": "Zicong"
      },
      {
        "surname": "Liu",
        "given_name": "Yang"
      },
      {
        "surname": "Wang",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Lag projective synchronization of discrete-time fractional-order quaternion-valued neural networks with time delays",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106532",
    "abstract": "This paper deals with the lag projective synchronization (LPS) problem for a class of discrete-time fractional-order quaternion-valued neural networks(DTFO QVNNs) systems with time delays. Firstly, a DTFOQVNNs system with time delay is constructed. Secondly, linear and adaptive feedback controllers with sign function are designed respectively. Furthermore, through Lyapunov direct method, DTFO inequality technique and Razumikhin theorem, some sufficiency criteria are obtained to ensure that the system in this article can achieve LPS. At last, the significance of the theoretical part of this paper is verified through numerical simulation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004568",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Combinatorics",
      "Computer network",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Discrete time and continuous time",
      "Evolutionary biology",
      "Function (biology)",
      "Geometry",
      "Lag",
      "Lyapunov function",
      "Mathematical analysis",
      "Mathematics",
      "Nonlinear system",
      "Physics",
      "Projective test",
      "Pure mathematics",
      "Quantum mechanics",
      "Quaternion",
      "Sign (mathematics)",
      "Sign function",
      "Statistics",
      "Synchronization (alternating current)",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Yan"
      },
      {
        "surname": "Zhang",
        "given_name": "Weiwei"
      },
      {
        "surname": "Zhang",
        "given_name": "Hai"
      },
      {
        "surname": "Chen",
        "given_name": "Dingyuan"
      },
      {
        "surname": "Cao",
        "given_name": "Jinde"
      }
    ]
  },
  {
    "title": "Adaptive Decision Spatio-temporal neural ODE for traffic flow forecasting with Multi-Kernel Temporal Dynamic Dilation Convolution",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106549",
    "abstract": "Traffic flow prediction is crucial for efficient traffic management. It involves predicting vehicle movement patterns to reduce congestion and enhance traffic flow. However, the highly non-linear and complex patterns commonly observed in traffic flow pose significant challenges for this task. Current Graph Neural Network (GNN) models often construct shallow networks, which limits their ability to extract deeper spatio-temporal representations. Neural ordinary differential equations for traffic prediction address over-smoothing but require significant computational resources, leading to inefficiencies, and sometimes deeper networks may lead to poorer predictions for complex traffic information. In this study, we propose an Adaptive Decision spatio-temporal Neural Ordinary Differential Network, which can adaptively determine the number of layers of ODE according to the complexity of traffic information. It can solve the over-smoothing problem better, improving overall efficiency and prediction accuracy. In addition, traditional temporal convolution methods make it difficult to deal with complex and variable traffic time information with a large time span. Therefore, we introduce a multi-kernel temporal dynamic expansive convolution to handle the traffic time information. Multi-kernel temporal dynamic expansive convolution employs a dynamic dilation strategy, dynamically adjusting the network’s receptive field across levels, effectively capturing temporal dependencies, and can better adapt to the changing time data of traffic information. Additionally, multi-kernel temporal dynamic expansive convolution integrates multi-scale convolution kernels, enabling the model to learn features across diverse temporal scales. We evaluated our proposed method on several real-world traffic datasets. Experimental results show that our method outperformed state-of-the-art benchmarks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004738",
    "keywords": [
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Convolution (computer science)",
      "Data mining",
      "Kernel (algebra)",
      "Machine learning",
      "Mathematics",
      "Ode",
      "Smoothing",
      "Traffic flow (computer networking)"
    ],
    "authors": [
      {
        "surname": "Chu",
        "given_name": "Zihao"
      },
      {
        "surname": "Ma",
        "given_name": "Wenming"
      },
      {
        "surname": "Li",
        "given_name": "Mingqi"
      },
      {
        "surname": "Chen",
        "given_name": "Hao"
      }
    ]
  },
  {
    "title": "MotionTrack: Learning motion predictor for multiple object tracking",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106539",
    "abstract": "Significant progress has been achieved in multi-object tracking (MOT) through the evolution of detection and re-identification (ReID) techniques. Despite these advancements, accurately tracking objects in scenarios with homogeneous appearance and heterogeneous motion remains a challenge. This challenge arises from two main factors: the insufficient discriminability of ReID features and the predominant utilization of linear motion models in MOT. In this context, we introduce a novel motion-based tracker, MotionTrack, centered around a learnable motion predictor that relies solely on object trajectory information. This predictor comprehensively integrates two levels of granularity in motion features to enhance the modeling of temporal dynamics and facilitate precise future motion prediction for individual objects. Specifically, the proposed approach adopts a self-attention mechanism to capture token-level information and a Dynamic MLP layer to model channel-level features. MotionTrack is a simple, online tracking approach. Our experimental results demonstrate that MotionTrack yields state-of-the-art performance on datasets such as Dancetrack and SportsMOT, characterized by highly complex object motion.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004635",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Match moving",
      "Motion (physics)",
      "Object (grammar)",
      "Pedagogy",
      "Psychology",
      "Tracking (education)",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Xiao",
        "given_name": "Changcheng"
      },
      {
        "surname": "Cao",
        "given_name": "Qiong"
      },
      {
        "surname": "Zhong",
        "given_name": "Yujie"
      },
      {
        "surname": "Lan",
        "given_name": "Long"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiang"
      },
      {
        "surname": "Luo",
        "given_name": "Zhigang"
      },
      {
        "surname": "Tao",
        "given_name": "Dacheng"
      }
    ]
  },
  {
    "title": "Cross-layer importance evaluation for neural network pruning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106496",
    "abstract": "Filter pruning has achieved remarkable success in reducing memory consumption and speeding up inference for convolutional neural networks (CNNs). Some prior works, such as heuristic methods, attempted to search for suitable sparse structures during the pruning process, which may be expensive and time-consuming. In this paper, an efficient cross-layer importance evaluation (CIE) method is proposed to automatically calculate proportional relationships among convolutional layers. Firstly, every layer is pruned separately by grid sampling way to obtain the accuracy of the model for all sampling points. And then, contribution matrices are built to describe the importance of each layer to model accuracy. Finally, the binary search algorithm is used to search the optimal sparse structure under a target pruned value. Extensive experiments on multiple representative image classification tasks demonstrate that proposed method acquires better compression performance under a little time cost compared to existing pruning algorithms. For instance, it reduces more than 50% FLOPs with only a small loss of 0.93% and 0.43% in the top-1 and top-5 accuracy for ResNet50, respectively. At the cost of only 0.24% accuracy loss, the pruned VGG19 model parameters are successfully compressed by 27.23 × and the throughput has increased by 2.46 × . On the whole, CIE has an excellent effect on the deployment and application of the CNNs model in edge device in terms of efficiency and accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004209",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "FLOPS",
      "Filter (signal processing)",
      "Heuristic",
      "Inference",
      "Layer (electronics)",
      "Organic chemistry",
      "Parallel computing",
      "Pattern recognition (psychology)",
      "Pruning",
      "Sampling (signal processing)",
      "Telecommunications",
      "Throughput",
      "Wireless"
    ],
    "authors": [
      {
        "surname": "Lian",
        "given_name": "Youzao"
      },
      {
        "surname": "Peng",
        "given_name": "Peng"
      },
      {
        "surname": "Jiang",
        "given_name": "Kai"
      },
      {
        "surname": "Xu",
        "given_name": "Weisheng"
      }
    ]
  },
  {
    "title": "An informative dual ForkNet for video anomaly detection",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106509",
    "abstract": "An autoencoder for video anomaly detection task is a type of algorithm with the primary purpose of learning an “informative” representation of the normal data that can be used for identifying the abnormal data by learning to reconstruct a set of input observations. Based on the encoding–decoding structure, we explore a novel dual ForkNet architecture that can dissociate and process the spatio-temporal representation. It is well-known in the information theory community that most autoencoders coding processes are inevitably accompanied by a certain loss of information. In this dual ForkNet, we focus on mitigating the information loss problem and propose a novel architectural recalibration approach, which we term the “Informetrics Recalibration” (IR). It can adaptively recalibrate latent feature representation by explicitly modeling the similarity between the corresponding feature maps of encoder and decoder, and retain more useful semantic information to generate greater differentiation between normal and abnormal events. Additionally, because the structure of the autoencoder itself determines the difficulty to obtain deep semantic information, we introduce a Secondary Encoder (SE) in each ForkNet, so as to recalibrate target features responses of latent feature representation. Our model is easy to be trained and robust to be applied, because it basically consists of some ResNet blocks without using complicated modules. Extensive experiments on the five publicly available benchmarks show that our model outperforms the existing state-of-the-art architectures, demonstrating our framework’s effectiveness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004337",
    "keywords": [
      "Anomaly detection",
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Dual (grammatical number)",
      "Literature",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Hongjun"
      },
      {
        "surname": "Wang",
        "given_name": "Yunlong"
      },
      {
        "surname": "Wang",
        "given_name": "Yating"
      },
      {
        "surname": "Chen",
        "given_name": "Junjie"
      }
    ]
  },
  {
    "title": "Prescribed performance adaptive neural event-triggered control for switched nonlinear cyber–physical systems under deception attacks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106586",
    "abstract": "In this paper, the design of an adaptive neural event-triggered control scheme for a class of switched nonlinear systems affected by external disturbances and deception attacks is presented. In order to address the effects caused by unknown disturbances, a switched nonlinear disturbance observer is used, and the error between the estimated signals and actual disturbances is small. Meanwhile, a prescribed performance function is introduced, which aims to ensure system output reaches the performance bounds within a predefined finite time. In addition, a dynamic event-triggered mechanism is designed to reduce the communication load. Based on the theoretical analysis, all signals within the closed-loop system are bounded, while simultaneously ensuring the complete elimination of Zeno behavior. Finally, the validity and efficacy of the scheme are proven by an example of numerical simulation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005100",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Biology",
      "Bounded function",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Controller (irrigation)",
      "Mathematical analysis",
      "Mathematics",
      "Nonlinear system",
      "Observer (physics)",
      "Physics",
      "Quantum mechanics",
      "Scheme (mathematics)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Liang"
      },
      {
        "surname": "Zhao",
        "given_name": "Zixiang"
      },
      {
        "surname": "Ma",
        "given_name": "Zheng"
      },
      {
        "surname": "Zhao",
        "given_name": "Ning"
      }
    ]
  },
  {
    "title": "Towards a rigorous analysis of mutual information in contrastive learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106584",
    "abstract": "Contrastive learning has emerged as a cornerstone in unsupervised representation learning. Its primary paradigm involves an instance discrimination task utilizing InfoNCE loss where the loss has been proven to be a form of mutual information. Consequently, it has become a common practice to analyze contrastive learning using mutual information as a measure. Yet, this analysis approach presents difficulties due to the necessity of estimating mutual information for real-world applications. This creates a gap between the elegance of its mathematical foundation and the complexity of its estimation, thereby hampering the ability to derive solid and meaningful insights from mutual information analysis. In this study, we introduce three novel methods and a few related theorems, aimed at enhancing the rigor of mutual information analysis. Despite their simplicity, these methods can carry substantial utility. Leveraging these approaches, we reassess three instances of contrastive learning analysis, illustrating the capacity of the proposed methods to facilitate deeper comprehension or to rectify pre-existing misconceptions. The main results can be summarized as follows: (1) While small batch sizes influence the range of training loss, they do not inherently limit learned representation’s information content or affect downstream performance adversely; (2) Mutual information, with careful selection of positive pairings and post-training estimation, proves to be a superior measure for evaluating practical networks; and (3) Distinguishing between task-relevant and irrelevant information presents challenges, yet irrelevant information sources do not necessarily compromise the generalization of downstream tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005082",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Economics",
      "Generalization",
      "Information bottleneck method",
      "Information theory",
      "Law",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Mutual information",
      "Pointwise mutual information",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Statistics",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Kyungeun"
      },
      {
        "surname": "Kim",
        "given_name": "Jaeill"
      },
      {
        "surname": "Kang",
        "given_name": "Suhyun"
      },
      {
        "surname": "Rhee",
        "given_name": "Wonjong"
      }
    ]
  },
  {
    "title": "Asymmetric double-winged multi-view clustering network for exploring diverse and consistent information",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106563",
    "abstract": "In unsupervised scenarios, deep contrastive multi-view clustering (DCMVC) is becoming a hot research spot, which aims to mine the potential relationships between different views. Most existing DCMVC algorithms focus on exploring the consistency information for the deep semantic features, while ignoring the diverse information on shallow features. To fill this gap, we propose a novel multi-view clustering network termed CodingNet to explore the diverse and consistent information simultaneously in this paper. Specifically, instead of utilizing the conventional auto-encoder, we design an asymmetric structure network to extract shallow and deep features separately. Then, by approximating the similarity matrix on the shallow feature to the zero matrix, we ensure the diversity for the shallow features, thus offering a better description of multi-view data. Moreover, we propose a dual contrastive mechanism that maintains consistency for deep features at both view-feature and pseudo-label levels. Our framework’s efficacy is validated through extensive experiments on six widely used benchmark datasets, outperforming most state-of-the-art multi-view clustering algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004878",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cluster analysis",
      "Computer science",
      "Consistency (knowledge bases)",
      "Data mining",
      "Deep learning",
      "Feature (linguistics)",
      "Focus (optics)",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Machine learning",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Qun"
      },
      {
        "surname": "Yang",
        "given_name": "Xihong"
      },
      {
        "surname": "Wang",
        "given_name": "Siwei"
      },
      {
        "surname": "An",
        "given_name": "Xinru"
      },
      {
        "surname": "Liu",
        "given_name": "Qi"
      }
    ]
  },
  {
    "title": "Tensorized Incomplete Multi-view Kernel Subspace Clustering",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106529",
    "abstract": "Recently considerable advances have been achieved in the incomplete multi-view clustering (IMC) research. However, the current IMC works are often faced with three challenging issues. First, they mostly lack the ability to recover the nonlinear subspace structures in the multiple kernel spaces. Second, they usually neglect the high-order relationship in multiple representations. Third, they often have two or even more hyper-parameters and may not be practical for some real-world applications. To tackle these issues, we present a Tensorized Incomplete Multi-view Kernel Subspace Clustering (TIMKSC) approach. Specifically, by incorporating the kernel learning technique into an incomplete subspace clustering framework, our approach can robustly explore the latent subspace structure hidden in multiple views. Furthermore, we impute the incomplete kernel matrices and learn the low-rank tensor representations in a mutual enhancement manner. Notably, our approach can discover the underlying relationship among the observed and missing samples while capturing the high-order correlation to assist subspace clustering. To solve the proposed optimization model, we design a three-step algorithm to efficiently minimize the unified objective function, which only involves one hyper-parameter that requires tuning. Experiments on various benchmark datasets demonstrate the superiority of our approach. The source code and datasets are available at: https://www.researchgate.net/publication/381828300_TIMKSC_20240629.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004532",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Geodesy",
      "Geography",
      "Kernel (algebra)",
      "Kernel method",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Rank (graph theory)",
      "Subspace topology",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Guang-Yu"
      },
      {
        "surname": "Huang",
        "given_name": "Dong"
      },
      {
        "surname": "Wang",
        "given_name": "Chang-Dong"
      }
    ]
  },
  {
    "title": "Global and multi-partition local network analysis of scalp EEG in West syndrome before and after treatment",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106540",
    "abstract": "West syndrome is an epileptic disease that seriously affects the normal growth and development of infants in early childhood. Based on the methods of brain topological network and graph theory, this article focuses on three clinical states of patients before and after treatment. In addition to discussing bidirectional and unidirectional global networks from the perspective of computational principles, a more in-depth analysis of local intra-network and inter-network characteristics of multi-partitioned networks is also performed. The spatial feature distribution based on feature path length is introduced for the first time. The results show that the bidirectional network has better significant differentiation. The rhythmic feature change trend and spatial characteristic distribution of this network can be used as a measure of the impact on global information processing in the brain after treatment. And localized brain regions variability in features and differences in the ability to interact with information between brain regions have potential as biomarkers for medication assessment in WEST syndrome. The above shows specific conclusions on the interaction relationship and consistency of macro-network and micro-network, which may have a positive effect on patients’ treatment and prognosis management.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004647",
    "keywords": [
      "Anatomy",
      "Artificial intelligence",
      "Combinatorics",
      "Complex network",
      "Computer science",
      "Electroencephalography",
      "Feature (linguistics)",
      "Linguistics",
      "Mathematics",
      "Medicine",
      "Mutual information",
      "Network analysis",
      "Neuroscience",
      "Partition (number theory)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Psychology",
      "Quantum mechanics",
      "Scalp",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Lishan"
      },
      {
        "surname": "Zheng",
        "given_name": "Runze"
      },
      {
        "surname": "Wu",
        "given_name": "Duanpo"
      },
      {
        "surname": "Yuan",
        "given_name": "Yixuan"
      },
      {
        "surname": "Lin",
        "given_name": "Yi"
      },
      {
        "surname": "Wang",
        "given_name": "Danping"
      },
      {
        "surname": "Jiang",
        "given_name": "Tiejia"
      },
      {
        "surname": "Cao",
        "given_name": "Jiuwen"
      },
      {
        "surname": "Xu",
        "given_name": "Yuansheng"
      }
    ]
  },
  {
    "title": "MS23D: A 3D object detection method using multi-scale semantic feature points to construct 3D feature layer",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106623",
    "abstract": "LiDAR point clouds can effectively depict the motion and posture of objects in three-dimensional space. Many studies accomplish the 3D object detection by voxelizing point clouds. However, in autonomous driving scenarios, the sparsity and hollowness of point clouds create some difficulties for voxel-based methods. The sparsity of point clouds makes it challenging to describe the geometric features of objects. The hollowness of point clouds poses difficulties for the aggregation of 3D features. We propose a two-stage 3D object detection framework, called MS23D. (1) We propose a method using voxel feature points from multi-branch to construct the 3D feature layer. Using voxel feature points from different branches, we construct a relatively compact 3D feature layer with rich semantic features. Additionally, we propose a distance-weighted sampling method, reducing the loss of foreground points caused by downsampling and allowing the 3D feature layer to retain more foreground points. (2) In response to the hollowness of point clouds, we predict the offsets between deep-level feature points and the object’s centroid, making them as close as possible to the object’s centroid. This enables the aggregation of these feature points with abundant semantic features. For feature points from shallow-level, we retain them on the object’s surface to describe the geometric features of the object. To validate our approach, we evaluated its effectiveness on both the KITTI and ONCE datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005471",
    "keywords": [
      "Artificial intelligence",
      "Cartography",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Construct (python library)",
      "Feature (linguistics)",
      "Geography",
      "Geometry",
      "Image (mathematics)",
      "Image processing",
      "Layer (electronics)",
      "Lidar",
      "Linguistics",
      "Mathematics",
      "Object (grammar)",
      "Object detection",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point (geometry)",
      "Point cloud",
      "Programming language",
      "Remote sensing",
      "Scale (ratio)",
      "Scale space",
      "Voxel"
    ],
    "authors": [
      {
        "surname": "Shao",
        "given_name": "Yongxin"
      },
      {
        "surname": "Tan",
        "given_name": "Aihong"
      },
      {
        "surname": "Wang",
        "given_name": "Binrui"
      },
      {
        "surname": "Yan",
        "given_name": "Tianhong"
      },
      {
        "surname": "Sun",
        "given_name": "Zhetao"
      },
      {
        "surname": "Zhang",
        "given_name": "Yiyang"
      },
      {
        "surname": "Liu",
        "given_name": "Jiaxin"
      }
    ]
  },
  {
    "title": "MIGP: Metapath Integrated Graph Prompt Neural Network",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106595",
    "abstract": "Graph neural networks (GNNs) leveraging metapaths have garnered extensive utilization. Nevertheless, the escalating parameters and data corpus within graph pre-training models incur mounting training costs. Consequently, GNN models encounter hurdles including diminished generalization capacity and compromised performance amidst small sample datasets. Drawing inspiration from the efficacy demonstrated by self-supervised learning methodologies in natural language processing, we embark on an exploration. We endeavor to imbue graph data with augmentable, learnable prompt vectors targeting node representation enhancement to foster superior adaptability to downstream tasks. This paper proposes a novel approach, the Metapath Integrated Graph Prompt Neural Network (MIGP), which leverages learnable prompt vectors to enhance node representations within a pretrained model framework. By leveraging learnable prompt vectors, MIGP aims to address the limitations posed by mall sample datasets and improve GNNs’ model generalization. In the pretraining stage, we split symmetric metapaths in heterogeneous graphs into short metapaths and explicitly propagate information along the metapaths to update node representations. In the prompt-tuning stage, the parameters of the pretrained model are fixed, a set of independent basis vectors is introduced, and an attention mechanism is employed to generate task-specific learnable prompt vectors for each node. Another notable contribution of our work is the introduction of three patent datasets, which is a pioneering application in related fields. We will make these three patent datasets publicly available to facilitate further research on large-scale patent data analysis. Through comprehensive experiments conducted on three patent datasets and three other public datasets, i.e., ACM, IMDB, and DBLP, we demonstrate the superior performance of the MIGP model in enhancing model applicability and performance across a variety of downstream datasets. The source code and datasets are available in the website. 1 1 https://github.com/hzw-ai/MIGP, with password: MIGPNN2024.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005197",
    "keywords": [
      "Adaptability",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Data mining",
      "Ecology",
      "Engineering",
      "Generalization",
      "Graph",
      "Labeled data",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Node (physics)",
      "Programming language",
      "Set (abstract data type)",
      "Structural engineering",
      "Theoretical computer science",
      "Training set"
    ],
    "authors": [
      {
        "surname": "Lai",
        "given_name": "Pei-Yuan"
      },
      {
        "surname": "Dai",
        "given_name": "Qing-Yun"
      },
      {
        "surname": "Lu",
        "given_name": "Yi-Hong"
      },
      {
        "surname": "Wang",
        "given_name": "Zeng-Hui"
      },
      {
        "surname": "Chen",
        "given_name": "Man-Sheng"
      },
      {
        "surname": "Wang",
        "given_name": "Chang-Dong"
      }
    ]
  },
  {
    "title": "MuDE: Multi-agent decomposed reward-based exploration",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106565",
    "abstract": "In cooperative multi-agent reinforcement learning, agents jointly optimize a centralized value function based on the rewards shared by all agents and learn decentralized policies through value function decomposition. Although such a learning framework is considered effective, estimating individual contribution from the rewards, which is essential for learning highly cooperative behaviors, is difficult. In addition, it becomes more challenging when reinforcement and punishment, help in increasing or decreasing the specific behaviors of agents, coexist because the processes of maximizing reinforcement and minimizing punishment can often conflict in practice. This study proposes a novel exploration scheme called multi-agent decomposed reward-based exploration (MuDE), which preferably explores the action spaces associated with positive sub-rewards based on a modified reward decomposition scheme, thus effectively exploring action spaces not reachable by existing exploration schemes. We evaluate MuDE with a challenging set of StarCraft II micromanagement and modified predator–prey tasks extended to include reinforcement and punishment. The results show that MuDE accurately estimates sub-rewards and outperforms state-of-the-art approaches in both convergence speed and win rates.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004891",
    "keywords": [
      "Action (physics)",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Convergence (economics)",
      "Decomposition",
      "Ecology",
      "Economic growth",
      "Economics",
      "Evolutionary biology",
      "Function (biology)",
      "Mathematical analysis",
      "Mathematics",
      "Physics",
      "Programming language",
      "Psychology",
      "Punishment (psychology)",
      "Quantum mechanics",
      "Reinforcement",
      "Reinforcement learning",
      "Scheme (mathematics)",
      "Set (abstract data type)",
      "Social psychology"
    ],
    "authors": [
      {
        "surname": "Yoo",
        "given_name": "Byunghyun"
      },
      {
        "surname": "Yi",
        "given_name": "Sungwon"
      },
      {
        "surname": "Kim",
        "given_name": "Hyunwoo"
      },
      {
        "surname": "Shin",
        "given_name": "Younghwan"
      },
      {
        "surname": "Han",
        "given_name": "Ran"
      },
      {
        "surname": "Seo",
        "given_name": "Seungwoo"
      },
      {
        "surname": "Song",
        "given_name": "Hwa Jeon"
      },
      {
        "surname": "Chung",
        "given_name": "Euisok"
      },
      {
        "surname": "Yang",
        "given_name": "Jeongmin"
      }
    ]
  },
  {
    "title": "A computationally efficient and robust looming perception model based on dynamic neural field",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106502",
    "abstract": "There are primarily two classes of bio-inspired looming perception visual systems. The first class employs hierarchical neural networks inspired by well-acknowledged anatomical pathways responsible for looming perception, and the second maps nonlinear relationships between physical stimulus attributes and neuronal activity. However, even with multi-layered structures, the former class is sometimes fragile in looming selectivity, i.e., the ability to well discriminate between approaching and other categories of movements. While the latter class leaves qualms regarding how to encode visual movements to indicate physical attributes like angular velocity/size. Beyond those, we propose a novel looming perception model based on dynamic neural field (DNF). The DNF is a brain-inspired framework that incorporates both lateral excitation and inhibition within the field through instant feedback, it could be an easily-built model to fulfill the looming sensitivity observed in biological visual systems. To achieve our target of looming perception with computational efficiency, we introduce a single-field DNF with adaptive lateral interactions and dynamic activation threshold. The former mechanism creates antagonism to translating motion, and the latter suppresses excitation during receding. Accordingly, the proposed model exhibits the strongest response to moving objects signaling approaching over other types of external stimuli. The effectiveness of the proposed model is supported by relevant mathematical analysis and ablation study. The computational efficiency and robustness of the model are verified through systematic experiments including on-line collision-detection tasks in micro-mobile robots, at success rate of 93% compared with state-of-the-art methods. The results demonstrate its superiority over the model-based methods concerning looming perception.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400426X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Cognitive psychology",
      "Computational model",
      "Computer science",
      "Gene",
      "Looming",
      "Neuroscience",
      "Perception",
      "Psychology",
      "Robustness (evolution)",
      "Stimulus (psychology)",
      "Visual perception",
      "Visual processing"
    ],
    "authors": [
      {
        "surname": "Qin",
        "given_name": "Ziyan"
      },
      {
        "surname": "Fu",
        "given_name": "Qinbing"
      },
      {
        "surname": "Peng",
        "given_name": "Jigen"
      }
    ]
  },
  {
    "title": "Emotion recognition using hierarchical spatial–temporal learning transformer from regional to global brain",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106624",
    "abstract": "Emotion recognition is an essential but challenging task in human–computer interaction systems due to the distinctive spatial structures and dynamic temporal dependencies associated with each emotion. However, current approaches fail to accurately capture the intricate effects of electroencephalogram (EEG) signals across different brain regions on emotion recognition. Therefore, this paper designs a transformer-based method, denoted by R2G-STLT, which relies on a spatial–temporal transformer encoder with regional to global hierarchical learning that learns the representative spatiotemporal features from the electrode level to the brain-region level. The regional spatial–temporal transformer (RST-Trans) encoder is designed to obtain spatial information and context dependence at the electrode level aiming to learn the regional spatiotemporal features. Then, the global spatial–temporal transformer (GST-Trans) encoder is utilized to extract reliable global spatiotemporal features, reflecting the impact of various brain regions on emotion recognition tasks. Moreover, the multi-head attention mechanism is placed into the GST-Trans encoder to empower it to capture the long-range spatial–temporal information among the brain regions. Finally, subject-independent experiments are conducted on each frequency band of the DEAP, SEED, and SEED-IV datasets to assess the performance of the proposed model. Results indicate that the R2G-STLT model surpasses several state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005483",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Electrical engineering",
      "Engineering",
      "Pattern recognition (psychology)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Cheng",
        "given_name": "Cheng"
      },
      {
        "surname": "Liu",
        "given_name": "Wenzhe"
      },
      {
        "surname": "Feng",
        "given_name": "Lin"
      },
      {
        "surname": "Jia",
        "given_name": "Ziyu"
      }
    ]
  },
  {
    "title": "Attractors for Hopfield lattice model in weighted spaces",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106500",
    "abstract": "The investigation into the dynamic behavior of infinite lattice systems holds paramount significance in the realm of physical phenomena, particularly in mechanics. This intricate domain has captivated the attention of both mathematicians and physicists. In acknowledgment of the inherent noise prevalent in real-world environments, our study embraces this aspect by introducing a random term into our model. This deliberate inclusion of stochasticity engenders a novel perspective, giving rise to a stochastic lattice differential equation. This model proves to be a versatile tool for accurately characterizing spatial structures characterized by discrete components and the associated uncertainties that pervade them. This research elucidates the intricate interplay between lattice dynamics and environmental noise, shedding light on the complex behavior of such systems in a realistic context. Our result generalizes many results in three directions: extending the connections between the terms to non-linear, extending the connection neighborhood from 3 (as in most cases) to arbitrary value n , and extending the results that are in ℓ 2 to ℓ ρ 2 .",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004246",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Artificial neural network",
      "Attractor",
      "Computer science",
      "Hopfield network",
      "Lattice (music)",
      "Mathematical analysis",
      "Mathematics",
      "Physics",
      "Statistical physics"
    ],
    "authors": [
      {
        "surname": "Usman",
        "given_name": "Basiru"
      },
      {
        "surname": "Wang",
        "given_name": "Xiaoli"
      }
    ]
  },
  {
    "title": "Infinite-dimensional reservoir computing",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106486",
    "abstract": "Reservoir computing approximation and generalization bounds are proved for a new concept class of input/output systems that extends the so-called generalized Barron functionals to a dynamic context. This new class is characterized by the readouts with a certain integral representation built on infinite-dimensional state–space systems. It is shown that this class is very rich and possesses useful features and universal approximation properties. The reservoir architectures used for the approximation and estimation of elements in the new class are randomly generated echo state networks with either linear or ReLU activation functions. Their readouts are built using randomly generated neural networks in which only the output layer is trained (extreme learning machines or random feature neural networks). The results in the paper yield a recurrent neural network-based learning algorithm with provable convergence guarantees that do not suffer from the curse of dimensionality when learning input/output systems in the class of generalized Barron functionals and measuring the error in a mean-squared sense.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004106",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Class (philosophy)",
      "Computer science",
      "Context (archaeology)",
      "Convergence (economics)",
      "Curse of dimensionality",
      "Echo state network",
      "Economic growth",
      "Economics",
      "Feature (linguistics)",
      "Function approximation",
      "Generalization",
      "Law",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Paleontology",
      "Philosophy",
      "Political science",
      "Politics",
      "Recurrent neural network",
      "Representation (politics)",
      "Reservoir computing",
      "State (computer science)",
      "State space",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Gonon",
        "given_name": "Lukas"
      },
      {
        "surname": "Grigoryeva",
        "given_name": "Lyudmila"
      },
      {
        "surname": "Ortega",
        "given_name": "Juan-Pablo"
      }
    ]
  },
  {
    "title": "Unified analysis on multistablity of fraction-order multidimensional-valued memristive neural networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106498",
    "abstract": "This article provides a unified analysis of the multistability of fraction-order multidimensional-valued memristive neural networks (FOMVMNNs) with unbounded time-varying delays. Firstly, based on the knowledge of fractional differentiation and memristors, a unified model is established. This model is a unified form of real-valued, complex-valued, and quaternion-valued systems. Then, based on a unified method, the number of equilibrium points for FOMVMNNs is discussed. The sufficient conditions for determining the number of equilibrium points have been obtained. By using 1-norm to construct Lyapunov functions, the unified criteria for multistability of FOMVMNNs are obtained, these criteria are less conservative and easier to verify. Moreover, the attraction basins of the stable equilibrium points are estimated. Finally, two numerical simulation examples are provided to verify the correctness of the results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004222",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Computer science",
      "Economics",
      "Finance",
      "Fraction (chemistry)",
      "Mathematics",
      "Order (exchange)",
      "Organic chemistry"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jiarui"
      },
      {
        "surname": "Zhu",
        "given_name": "Song"
      },
      {
        "surname": "Mu",
        "given_name": "Chaoxu"
      },
      {
        "surname": "Liu",
        "given_name": "Xiaoyang"
      },
      {
        "surname": "Wen",
        "given_name": "Shiping"
      }
    ]
  },
  {
    "title": "Multi-objective molecular generation via clustered Pareto-based reinforcement learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106596",
    "abstract": "De novo molecular design is the process of learning knowledge from existing data to propose new chemical structures that satisfy the desired properties. By using de novo design to generate compounds in a directed manner, better solutions can be obtained in large chemical libraries with less comparison cost. But drug design needs to take multiple factors into consideration. For example, in polypharmacology, molecules that activate or inhibit multiple target proteins produce multiple pharmacological activities and are less susceptible to drug resistance. However, most existing molecular generation methods either focus only on affinity for a single target or fail to effectively balance the relationship between multiple targets, resulting in insufficient validity and desirability of the generated molecules. To address the problems, an approach called clustered Pareto-based reinforcement learning (CPRL) is proposed. In CPRL, a pre-trained model is constructed to grasp existing molecular knowledge in a supervised learning manner. In addition, the clustered Pareto optimization algorithm is presented to find the best solution between different objectives. The algorithm first extracts an update set from the sampled molecules through the designed aggregation-based molecular clustering. Then, the final reward is computed by constructing the Pareto frontier ranking of the molecules from the updated set. To explore the vast chemical space, a reinforcement learning agent is designed in CPRL that can be updated under the guidance of the final reward to balance multiple properties. Furthermore, to increase the internal diversity of the molecules, a fixed-parameter exploration model is used for sampling in conjunction with the agent. The experimental results demonstrate that CPRL is capable of balancing multiple properties of the molecule and has higher desirability and validity, reaching 0.9551 and 0.9923, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005203",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemical space",
      "Chemistry",
      "Cluster analysis",
      "Computer science",
      "Drug discovery",
      "GRASP",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Pareto principle",
      "Programming language",
      "Ranking (information retrieval)",
      "Reinforcement learning",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jing"
      },
      {
        "surname": "Zhu",
        "given_name": "Fei"
      }
    ]
  },
  {
    "title": "An unsupervised multi-view contrastive learning framework with attention-based reranking strategy for entity alignment",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106583",
    "abstract": "Entity alignment is a crucial task in knowledge graphs, aiming to match corresponding entities from different knowledge graphs. Due to the scarcity of pre-aligned entities in real-world scenarios, research focused on unsupervised entity alignment has become more popular. However, current unsupervised entity alignment methods suffer from a lack of informative entity guidance, hindering their ability to accurately predict challenging entities with similar names and structures. To solve these problems, we present an unsupervised multi-view contrastive learning framework with an attention-based reranking strategy for entity alignment, named AR-Align. In AR-Align, two kinds of data augmentation methods are employed to provide a complementary view for neighborhood and attribute, respectively. Next, a multi-view contrastive learning method is introduced to reduce the semantic gap between different views of the augmented entities. Moreover, an attention-based reranking strategy is proposed to rerank the hard entities through calculating their weighted sum of embedding similarities on different structures. Experimental results indicate that AR-Align outperforms most both supervised and unsupervised state-of-the-art methods on three benchmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005070",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Economics",
      "Embedding",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Management",
      "Natural language processing",
      "Task (project management)",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Liang",
        "given_name": "Yan"
      },
      {
        "surname": "Cai",
        "given_name": "Weishan"
      },
      {
        "surname": "Yang",
        "given_name": "Minghao"
      },
      {
        "surname": "Jiang",
        "given_name": "Yuncheng"
      }
    ]
  },
  {
    "title": "DECNet: Dense embedding contrast for unsupervised semantic segmentation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106557",
    "abstract": "Unsupervised semantic segmentation is important for understanding that each pixel belongs to known categories without annotation. Recent studies have demonstrated promising outcomes by employing a vision transformer backbone pre-trained on an image-level dataset in a self-supervised manner. However, those methods always depend on complex architectures or meticulously designed inputs. Naturally, we are attempting to explore the investment with a straightforward approach. To prevent over-complication, we introduce a simple Dense Embedding Contrast network (DECNet) for unsupervised semantic segmentation in this paper. Specifically, we propose a Nearest Neighbor Similarity strategy (NNS) to establish well-defined positive and negative pairs for dense contrastive learning. Meanwhile, we optimize a contrastive objective named Ortho-InfoNCE to alleviate the false negative problem inherent in contrastive learning for further enhancing dense representations. Finally, extensive experiments conducted on COCO-Stuff and Cityscapes datasets demonstrate that our approach outperforms state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004817",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Contrast (vision)",
      "Embedding",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Segmentation",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Xiaoqin"
      },
      {
        "surname": "Chen",
        "given_name": "Baiyu"
      },
      {
        "surname": "Zhou",
        "given_name": "Xiaolong"
      },
      {
        "surname": "Chan",
        "given_name": "Sixian"
      }
    ]
  },
  {
    "title": "Embodied sequential sampling models and dynamic neural fields for decision-making: Why hesitate between two when a continuum is the answer",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106526",
    "abstract": "As two alternative options in a forced choice task are separated by design, two classes of computational models of decision-making have thrived independently in the literature for nearly five decades. While sequential sampling models (SSM) focus on response times and keypresses in binary decisions in experimental paradigms, dynamic neural fields (DNF) focus on continuous sensorimotor dimensions and tasks found in perception and robotics. Recent attempts have been made to address limitations in their application to other domains, but strong similarities and compatibility between prominent models from both classes were hardly considered. This article is an attempt at bridging the gap between these classes of models, and simultaneously between disciplines and paradigms relying on binary or continuous responses. A unifying formulation of representative SSM and DNF equations is proposed, varying the number of units which interact and compete to reach a decision. The embodiment of decisions is also considered by coupling cognitive and sensorimotor processes, enabling the model to generate decision trajectories at trial level. The resulting mechanistic model is therefore able to target different paradigms (forced choices or continuous response scales) and measures (final responses or dynamics). The validity of the model is assessed statistically by fitting empirical distributions obtained from human participants in moral decision-making mouse-tracking tasks, for which both dichotomous and nuanced responses are meaningful. Comparing equations at the theoretical level, and model parametrizations at the empirical level, the implications for psychological decision-making processes, as well as the fundamental assumptions and limitations of models and paradigms are discussed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004507",
    "keywords": [
      "Artificial intelligence",
      "Binary classification",
      "Binary decision diagram",
      "Bridging (networking)",
      "Cognition",
      "Cognitive psychology",
      "Computational model",
      "Computer network",
      "Computer science",
      "Embodied cognition",
      "Focus (optics)",
      "Machine learning",
      "Neuroscience",
      "Optics",
      "Perception",
      "Physics",
      "Psychology",
      "Support vector machine",
      "Theoretical computer science",
      "Two-alternative forced choice"
    ],
    "authors": [
      {
        "surname": "Quinton",
        "given_name": "Jean-Charles"
      },
      {
        "surname": "Gautheron",
        "given_name": "Flora"
      },
      {
        "surname": "Smeding",
        "given_name": "Annique"
      }
    ]
  },
  {
    "title": "A Multi-Group Multi-Stream attribute Attention network for fine-grained zero-shot learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106558",
    "abstract": "Fine-grained visual categorization in zero-shot setting is a challenging problem in the computer vision community. It requires algorithms to accurately identify fine-grained categories that do not appear during the training phase and have high visual similarity to each other. Existing methods usually address this problem by using attribute information as intermediate knowledge, which provides sufficient fine-grained characteristics of categories and can be transferred from seen categories to unseen categories. However, the learning of attribute visual features is not trivial due to the following two reasons: (i) The visual information about attributes of different types may interfere with the visual feature learning of each other. (ii) The visual characteristics of the same attribute may vary in different categories. To solve these issues, we propose a Multi-Group Multi-Stream attribute Attention network (MGMSA), which not only separates the feature learning of attributes of different types, but also isolates the learning of attribute visual features for categories with big differences in attribute appearance. This avoids the interference between uncorrelated attributes and helps to learn category-specific attribute-related visual features. This is beneficial for distinguishing fine-grained categories with subtle visual differences. Extensive experiments on benchmark datasets show that MGMSA achieves state-of-the-art performance on attribute prediction and fine-grained zero-shot learning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004829",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Categorization",
      "Computer science",
      "Feature (linguistics)",
      "Feature learning",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Lingyun"
      },
      {
        "surname": "Shang",
        "given_name": "Xuequn"
      },
      {
        "surname": "Zhou",
        "given_name": "Ruizhi"
      },
      {
        "surname": "Liu",
        "given_name": "Jun"
      },
      {
        "surname": "Ma",
        "given_name": "Jie"
      },
      {
        "surname": "Li",
        "given_name": "Zhanhuai"
      },
      {
        "surname": "Sun",
        "given_name": "Mingxuan"
      }
    ]
  },
  {
    "title": "Arithmetic with language models: From memorization to computation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106550",
    "abstract": "A better understanding of the emergent computation and problem-solving capabilities of recent large language models is of paramount importance to further improve them and broaden their applicability. This work investigates how a language model, trained to predict the next token, can perform arithmetic computations generalizing beyond training data. Binary addition and multiplication constitute a good testbed for this purpose, since they require a very small vocabulary and exhibit relevant input/output discontinuities making smooth input interpolation ineffective for novel data. We successfully trained a light language model to learn these tasks and ran a number of experiments to investigate the extrapolation capabilities and internal information processing. Our findings support the hypothesis that the language model works as an Encoding–Regression–Decoding machine where the computation takes place in the value space once the input token representation is mapped to an appropriate internal representation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400474X",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Computation",
      "Computer science",
      "Mathematics",
      "Mathematics education",
      "Memorization",
      "Natural language processing",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Maltoni",
        "given_name": "Davide"
      },
      {
        "surname": "Ferrara",
        "given_name": "Matteo"
      }
    ]
  },
  {
    "title": "Contrastive fine-grained domain adaptation network for EEG-based vigilance estimation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106617",
    "abstract": "Vigilance state is crucial for the effective performance of users in brain-computer interface (BCI) systems. Most vigilance estimation methods rely on a large amount of labeled data to train a satisfactory model for the specific subject, which limits the practical application of the methods. This study aimed to build a reliable vigilance estimation method using a small amount of unlabeled calibration data. We conducted a vigilance experiment in the designed BCI-based cursor-control task. Electroencephalogram (EEG) signals of eighteen participants were recorded in two sessions on two different days. And, we proposed a contrastive fine-grained domain adaptation network (CFGDAN) for vigilance estimation. Here, an adaptive graph convolution network (GCN) was built to project the EEG data of different domains into a common space. The fine-grained feature alignment mechanism was designed to weight and align the feature distributions across domains at the EEG channel level, and the contrastive information preservation module was developed to preserve the useful target-specific information during the feature alignment. The experimental results show that the proposed CFGDAN outperforms the compared methods in our BCI vigilance dataset and SEED-VIG dataset. Moreover, the visualization results demonstrate the efficacy of the designed feature alignment mechanisms. These results indicate the effectiveness of our method for vigilance estimation. Our study is helpful for reducing calibration efforts and promoting the practical application potential of vigilance estimation methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005410",
    "keywords": [
      "Artificial intelligence",
      "Attention network",
      "Brain–computer interface",
      "Cognitive psychology",
      "Computer science",
      "Electroencephalography",
      "Pattern recognition (psychology)",
      "Psychiatry",
      "Psychology",
      "Vigilance (psychology)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Kangning"
      },
      {
        "surname": "Wei",
        "given_name": "Wei"
      },
      {
        "surname": "Yi",
        "given_name": "Weibo"
      },
      {
        "surname": "Qiu",
        "given_name": "Shuang"
      },
      {
        "surname": "He",
        "given_name": "Huiguang"
      },
      {
        "surname": "Xu",
        "given_name": "Minpeng"
      },
      {
        "surname": "Ming",
        "given_name": "Dong"
      }
    ]
  },
  {
    "title": "A Hyper-Transformer model for Controllable Pareto Front Learning with Split Feasibility Constraints",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106571",
    "abstract": "Controllable Pareto front learning (CPFL) approximates the Pareto optimal solution set and then locates a non-dominated point with respect to a given reference vector. However, decision-maker objectives were limited to a constraint region in practice, so instead of training on the entire decision space, we only trained on the constraint region. Controllable Pareto front learning with Split Feasibility Constraints (SFC) is a way to find the best Pareto solutions to a split multi-objective optimization problem that meets certain constraints. In the previous study, CPFL used a Hypernetwork model comprising multi-layer perceptron (Hyper-MLP) blocks. Transformer can be more effective than previous architectures on numerous modern deep learning tasks in certain situations due to their distinctive advantages. Therefore, we have developed a hyper-transformer (Hyper-Trans) model for CPFL with SFC. We use the theory of universal approximation for the sequence-to-sequence function to show that the Hyper-Trans model makes MED errors smaller in computational experiments than the Hyper-MLP model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004957",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Electrical engineering",
      "Engineering",
      "Genetics",
      "Mathematical optimization",
      "Mathematics",
      "Multi-objective optimization",
      "Pareto principle",
      "Perceptron",
      "Sequence (biology)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Tuan",
        "given_name": "Tran Anh"
      },
      {
        "surname": "Dung",
        "given_name": "Nguyen Viet"
      },
      {
        "surname": "Thang",
        "given_name": "Tran Ngoc"
      }
    ]
  },
  {
    "title": "Continual pre-training mitigates forgetting in language and vision",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106492",
    "abstract": "Pre-trained models are commonly used in Continual Learning to initialize the model before training on the stream of non-stationary data. However, pre-training is rarely applied during Continual Learning. We investigate the characteristics of the Continual Pre-Training scenario, where a model is continually pre-trained on a stream of incoming data and only later fine-tuned to different downstream tasks. We introduce an evaluation protocol for Continual Pre-Training which monitors forgetting against a Forgetting Control dataset not present in the continual stream. We disentangle the impact on forgetting of 3 main factors: the input modality (NLP, Vision), the architecture type (Transformer, ResNet) and the pre-training protocol (supervised, self-supervised). Moreover, we propose a Sample-Efficient Pre-training method (SEP) that speeds up the pre-training phase. We show that the pre-training protocol is the most important factor accounting for forgetting. Surprisingly, we discovered that self-supervised continual pre-training in both NLP and Vision is sufficient to mitigate forgetting without the use of any Continual Learning strategy. Other factors, like model depth, input modality and architecture type are not as crucial.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004167",
    "keywords": [
      "Artificial intelligence",
      "Cognitive psychology",
      "Computer science",
      "Forgetting",
      "Geography",
      "Machine learning",
      "Meteorology",
      "Natural language processing",
      "Psychology",
      "Training (meteorology)"
    ],
    "authors": [
      {
        "surname": "Cossu",
        "given_name": "Andrea"
      },
      {
        "surname": "Carta",
        "given_name": "Antonio"
      },
      {
        "surname": "Passaro",
        "given_name": "Lucia"
      },
      {
        "surname": "Lomonaco",
        "given_name": "Vincenzo"
      },
      {
        "surname": "Tuytelaars",
        "given_name": "Tinne"
      },
      {
        "surname": "Bacciu",
        "given_name": "Davide"
      }
    ]
  },
  {
    "title": "HDConv: Heterogeneous kernel-based dilated convolutions",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106568",
    "abstract": "Dilated convolution has been widely used in various computer vision tasks due to its ability to expand the receptive field while maintaining the resolution of feature maps. However, the critical challenge is the gridding problem caused by the isomorphic structure of the dilated convolution, where the holes filled in the dilated convolution destroy the integrity of the extracted information and cut off the relevance of neighboring pixels. In this work, a novel heterogeneous dilated convolution, called HDConv, is proposed to address this issue by setting independent dilation rates on grouped channels while keeping the general convolution operation. The heterogeneous structure can effectively avoid the gridding problem while introducing multi-scale kernels in the filters. Based on the heterogeneous structure of the proposed HDConv, we also explore the benefit of large receptive fields to feature extraction by comparing different combinations of dilated rates. Finally, a series of experiments are conducted to verify the effectiveness of some computer vision tasks, such as image segmentation and object detection. The results show the proposed HDConv can achieve a competitive performance on ADE20K, Cityscapes, COCO-Stuff10k, COCO, and a medical image dataset UESTC-COVID-19. The proposed module can readily replace conventional convolutions in existing convolutional neural networks (i.e., plug-and-play), and it is promising to further extend dilated convolution to wider scenarios in the field of image segmentation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004921",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Dilation (metric space)",
      "Feature (linguistics)",
      "Feature extraction",
      "Image segmentation",
      "Kernel (algebra)",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Haigen"
      },
      {
        "surname": "Yu",
        "given_name": "Chenghan"
      },
      {
        "surname": "Zhou",
        "given_name": "Qianwei"
      },
      {
        "surname": "Guan",
        "given_name": "Qiu"
      },
      {
        "surname": "Feng",
        "given_name": "Hailin"
      }
    ]
  },
  {
    "title": "Masked cross-domain self-supervised deep learning framework for photoacoustic computed tomography reconstruction",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106515",
    "abstract": "Accurate image reconstruction is crucial for photoacoustic (PA) computed tomography (PACT). Recently, deep learning has been used to reconstruct PA images with a supervised scheme, which requires high-quality images as ground truth labels. However, practical implementations encounter inevitable trade-offs between cost and performance due to the expensive nature of employing additional channels for accessing more measurements. Here, we propose a masked cross-domain self-supervised (CDSS) reconstruction strategy to overcome the lack of ground truth labels from limited PA measurements. We implement the self-supervised reconstruction in a model-based form. Simultaneously, we take advantage of self-supervision to enforce the consistency of measurements and images across three partitions of the measured PA data, achieved by randomly masking different channels. Our findings indicate that dynamically masking a substantial proportion of channels, such as 80%, yields meaningful self-supervisors in both the image and signal domains. Consequently, this approach reduces the multiplicity of pseudo solutions and enables efficient image reconstruction using fewer PA measurements, ultimately minimizing reconstruction error. Experimental results on in-vivo PACT dataset of mice demonstrate the potential of our self-supervised framework. Moreover, our method exhibits impressive performance, achieving a structural similarity index (SSIM) of 0.87 in an extreme sparse case utilizing only 13 channels, which outperforms the performance of the supervised scheme with 16 channels (0.77 SSIM). Adding to its advantages, our method can be deployed on different trainable models in an end-to-end manner, further enhancing its versatility and applicability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004398",
    "keywords": [
      "Artificial intelligence",
      "Computed tomography",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Domain (mathematical analysis)",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Optics",
      "Pattern recognition (psychology)",
      "Photoacoustic imaging in biomedicine",
      "Physics",
      "Radiology"
    ],
    "authors": [
      {
        "surname": "Lan",
        "given_name": "Hengrong"
      },
      {
        "surname": "Huang",
        "given_name": "Lijie"
      },
      {
        "surname": "Wei",
        "given_name": "Xingyue"
      },
      {
        "surname": "Li",
        "given_name": "Zhiqiang"
      },
      {
        "surname": "Lv",
        "given_name": "Jing"
      },
      {
        "surname": "Ma",
        "given_name": "Cheng"
      },
      {
        "surname": "Nie",
        "given_name": "Liming"
      },
      {
        "surname": "Luo",
        "given_name": "Jianwen"
      }
    ]
  },
  {
    "title": "Improved switching condition for reachable set estimation of discrete-time switched delayed neural networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106530",
    "abstract": "This research delves into the reachable set estimation (RSE) problem for general switched delayed neural networks (SDNNs) in the discrete-time context. Note that existing relevant research on SDNNs predominantly relies on either time-dependent or state-dependent switching approaches. The time-dependent versions necessitate the stability of each subnetwork beforehand, whereas the state-dependent switching strategies solely depend on the current state, thus disregarding the historical information of the neuron states. For fully harnessing the historical information pertaining to neuron states, a delicate combined switching strategy (CSS) is formulated with the explicit goal of furnishing a relaxed and less conservative design framework tailored for discrete-time SDNNs, where all subnetworks can also be unstable. By resorting to the established time-dependent multiple Lyapunov–Krasovskii functional (TDMLF) technique, the improved criteria are subsequently presented, ensuring that the reachable set encompassing all potential states of SDNNs is confined to an anticipated bounded set. Ultimately, the practicality and superiority of the presented RSE approach are thoroughly validated by two illustrative simulation examples.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004544",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Bounded function",
      "Computer science",
      "Computer security",
      "Context (archaeology)",
      "Control (management)",
      "Control theory (sociology)",
      "Discrete time and continuous time",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Paleontology",
      "Programming language",
      "Reachability",
      "Set (abstract data type)",
      "Stability (learning theory)",
      "State (computer science)",
      "Statistics",
      "Subnetwork"
    ],
    "authors": [
      {
        "surname": "Chang",
        "given_name": "Wenting"
      },
      {
        "surname": "Wu",
        "given_name": "Libing"
      },
      {
        "surname": "Zhu",
        "given_name": "Shuaibing"
      },
      {
        "surname": "Sang",
        "given_name": "Hong"
      },
      {
        "surname": "Guo",
        "given_name": "Liangdong"
      }
    ]
  },
  {
    "title": "Deep learning approach for unified recognition of driver speed and lateral intentions using naturalistic driving data",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106569",
    "abstract": "Driver intention recognition is a critical component of advanced driver assistance systems, with significant implications for improving vehicle safety, intelligence, and fuel economy. However, previous research on driver intention recognition has not fully considered the influence of the driving environment on speed intentions and has not exploited the temporal dependency inherent in the lateral intentions to prevent erroneous changes in recognition. Furthermore, the coupling of speed and lateral intentions was overlooked; they were generally considered separately. To address these limitations, a unified recognition approach for speed and lateral intentions based on deep learning is presented in this study. First, extensive naturalistic driving data are collected, and information related to road slope and driving trajectories is extracted. A comprehensive classification of driver intentions is then performed. Toeplitz inverse covariance-based clustering and trajectory clustering methods are applied separately to label speed and lateral intentions, so that the influence of driving environments and the coupling of speed and lateral intentions are integrated into intention recognition. Finally, a deep-learning-based unified recognition model for driver intention is developed. This model uses a hierarchical recognition approach for speed intentions and includes a double-layer networks architecture with long short-term memory for the recognition of lateral intention. The validation results show that the created driver intention recognition model can accurately and stably recognize both speed and lateral intentions in complex driving environments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004933",
    "keywords": [
      "Artificial intelligence",
      "Astronomy",
      "Cluster analysis",
      "Computer science",
      "Driving simulator",
      "Machine learning",
      "Physics",
      "Trajectory"
    ],
    "authors": [
      {
        "surname": "Cheng",
        "given_name": "Kun"
      },
      {
        "surname": "Sun",
        "given_name": "Dongye"
      },
      {
        "surname": "Qin",
        "given_name": "Datong"
      },
      {
        "surname": "Cai",
        "given_name": "Jing"
      },
      {
        "surname": "Chen",
        "given_name": "Chong"
      }
    ]
  },
  {
    "title": "Contrastive cross-domain sequential recommendation via emphasized intention features",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106488",
    "abstract": "The objective of cross-domain sequential recommendation is to forecast upcoming interactions by leveraging past interactions across diverse domains. Most methods aim to utilize single-domain and cross-domain information as much as possible for personalized preference extraction and effective integration. However, on one hand, most models ignore that cross-domain information is composed of multiple single-domains when generating representations. They still treat cross-domain information the same way as single-domain information, resulting in noisy representation generation. Only by imposing certain constraints on cross-domain information during representation generation can subsequent models minimize interference when considering user preferences. On the other hand, some methods neglect the joint consideration of users’ long-term and short-term preferences and reduce the weight of cross-domain user preferences to minimize noise interference. To better consider the mutual promotion of cross-domain and single-domains factors, we propose a novel model ( C 2 D R E I F ) that utilizes Gaussian graph encoders to handle information, effectively constraining the correlation of information and capturing useful contextual information more accurately. It also employs a Top-down transformer to accurately extract user intents within each domain, taking into account the user’s long-term and short-term preferences. Additionally, entropy regularized is applied to enhance contrastive learning and mitigate the impact of randomness caused by negative sample composition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400412X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Domain (mathematical analysis)",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Ni",
        "given_name": "Ruoxin"
      },
      {
        "surname": "Cai",
        "given_name": "Weishan"
      },
      {
        "surname": "Jiang",
        "given_name": "Yuncheng"
      }
    ]
  },
  {
    "title": "Bayesian learning of feature spaces for multitask regression",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106619",
    "abstract": "This paper introduces a novel approach to learn multi-task regression models with constrained architecture complexity. The proposed model, named RFF-BLR, consists of a randomised feedforward neural network with two fundamental characteristics: a single hidden layer whose units implement the random Fourier features that approximate an RBF kernel, and a Bayesian formulation that optimises the weights connecting the hidden and output layers. The RFF-based hidden layer inherits the robustness of kernel methods. The Bayesian formulation enables promoting multioutput sparsity: all tasks interplay during the optimisation to select a compact subset of the hidden layer units that serve as common non-linear mapping for every tasks. The experimental results show that the RFF-BLR framework can lead to significant performance improvements compared to the state-of-the-art methods in multitask nonlinear regression, especially in small-sized training dataset scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005434",
    "keywords": [
      "Artificial intelligence",
      "Bayesian probability",
      "Computer science",
      "Economics",
      "Feature (linguistics)",
      "Feature selection",
      "Linguistics",
      "Machine learning",
      "Management",
      "Mathematics",
      "Multi-task learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Regression",
      "Statistics",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Sevilla-Salcedo",
        "given_name": "Carlos"
      },
      {
        "surname": "Gallardo-Antolín",
        "given_name": "Ascensión"
      },
      {
        "surname": "Gómez-Verdejo",
        "given_name": "Vanessa"
      },
      {
        "surname": "Parrado-Hernández",
        "given_name": "Emilio"
      }
    ]
  },
  {
    "title": "Cross-modal group-relation optimization for visible–infrared person re-identification",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106576",
    "abstract": "Visible–infrared person re-identification (VIPR) plays an important role in intelligent transportation systems. Modal discrepancies between visible and infrared images seriously confuse person appearance discrimination, e.g., the similarity of the same class of different modalities is lower than the similarity between different classes of the same modality. Worse still, the modal discrepancies and appearance discrepancies are coupled with each other. The prevailing practice is to disentangle modal and appearance discrepancies, but it usually requires complex decoupling networks. In this paper, rather than disentanglement, we propose to measure and optimize modal discrepancies. We explore a cross-modal group-relation (CMGR) to describe the relationship between the same group of people in two different modalities. The CMGR has great potential in modal invariance because it considers more stable groups rather than individuals, so it is a good measurement for modal discrepancies. Furthermore, we design a group-relation correlation (GRC) loss function based on Pearson correlations to optimize CMGR, which can be easily integrated with the learning of VIPR’s appearance features. Consequently, our CMGR model serves as a pivotal constraint to minimize modal discrepancies, operating in a manner similar to a loss function. It is applied solely during the training phase, thereby obviating the need for any execution during the inference phase. Experimental results on two public datasets (i.e., RegDB and SYSU-MM01) demonstrate that our CMGR method is superior to state-of-the-art approaches. In particular, on the RegDB dataset, with the help of CMGR, the rank-1 identification rate has improved by more than 7% compared to the case of not using CMGR.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005008",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Chemistry",
      "Computer science",
      "Constraint (computer-aided design)",
      "Control engineering",
      "Data mining",
      "Decoupling (probability)",
      "Engineering",
      "Evolutionary biology",
      "Function (biology)",
      "Geometry",
      "Identification (biology)",
      "Image (mathematics)",
      "Machine learning",
      "Mathematics",
      "Modal",
      "Modalities",
      "Modality (human–computer interaction)",
      "Pattern recognition (psychology)",
      "Polymer chemistry",
      "Relation (database)",
      "Similarity (geometry)",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Jianqing"
      },
      {
        "surname": "Wu",
        "given_name": "Hanxiao"
      },
      {
        "surname": "Chen",
        "given_name": "Yutao"
      },
      {
        "surname": "Xu",
        "given_name": "Heng"
      },
      {
        "surname": "Fu",
        "given_name": "Yuqing"
      },
      {
        "surname": "Zeng",
        "given_name": "Huanqiang"
      },
      {
        "surname": "Liu",
        "given_name": "Liu"
      },
      {
        "surname": "Lei",
        "given_name": "Zhen"
      }
    ]
  },
  {
    "title": "A novel multivariate time series forecasting dendritic neuron model for COVID-19 pandemic transmission tendency",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106527",
    "abstract": "A novel coronavirus discovered in late 2019 (COVID-19) quickly spread into a global epidemic and, thankfully, was brought under control by 2022. Because of the virus’s unknown mutations and the vaccine’s waning potency, forecasting is still essential for resurgence prevention and medical resource management. Computational efficiency and long-term accuracy are two bottlenecks for national-level forecasting. This study develops a novel multivariate time series forecasting model, the densely connected highly flexible dendritic neuron model (DFDNM) to predict daily and weekly positive COVID-19 cases. DFDNM’s high flexibility mechanism improves its capacity to deal with nonlinear challenges. The dense introduction of shortcut connections alleviates the vanishing and exploding gradient problems, encourages feature reuse, and improves feature extraction. To deal with the rapidly growing parameters, an improved variation of the adaptive moment estimation (AdamW) algorithm is employed as the learning algorithm for the DFDNM because of its strong optimization ability. The experimental results and statistical analysis conducted across three Japanese prefectures confirm the efficacy and feasibility of the DFDNM while outperforming various state-of-the-art machine learning models. To the best of our knowledge, the proposed DFDNM is the first to restructure the dendritic neuron model’s neural architecture, demonstrating promising use in multivariate time series prediction. Because of its optimal performance, the DFDNM may serve as an important reference for national and regional government decision-makers aiming to optimize pandemic prevention and medical resource management. We also verify that DFDMN is efficiently applicable not only to COVID-19 transmission prediction, but also to more general multivariate prediction tasks. It leads us to believe that it might be applied as a promising prediction model in other fields.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004519",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Machine learning",
      "Multivariate statistics",
      "Telecommunications",
      "Time series",
      "Transmission (telecommunications)",
      "Univariate"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Cheng"
      },
      {
        "surname": "Todo",
        "given_name": "Yuki"
      },
      {
        "surname": "Kodera",
        "given_name": "Sachiko"
      },
      {
        "surname": "Sun",
        "given_name": "Rong"
      },
      {
        "surname": "Shimada",
        "given_name": "Atsushi"
      },
      {
        "surname": "Hirata",
        "given_name": "Akimasa"
      }
    ]
  },
  {
    "title": "PSAR-SR: Patches separation and artifacts removal for improving super-resolution networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106554",
    "abstract": "The success of the ClassSR has led to a strategy of decomposing images being used for large image SR. The decomposed image patches have different recovery difficulties. Therefore, in ClassSR, image patches are reconstructed by different networks to greatly reduce the computational cost. However, in ClassSR, the training of multiple sub-networks inevitably increases the training difficulty. Furthermore, decomposing images with overlapping not only increases the computational cost but also inevitably produces artifacts. To address these challenges, we propose an end-to-end general framework, named patches separation and artifacts removal SR (PSAR-SR). In PSAR-SR, we propose an image information complexity module (IICM) to efficiently determine the difficulty of recovering image patches. Then, we propose a patches classification and separation module (PCSM), which can dynamically select an appropriate SR path for image patches of different recovery difficulties. Moreover, we propose a multi-attention artifacts removal module (MARM) in the network backend, which can not only greatly reduce the computational cost but also solve the artifacts problem well under the overlapping-free decomposition. Further, we propose two loss functions — threshold penalty loss (TP-Loss) and artifacts removal loss (AR-Loss). TP-Loss can better select appropriate SR paths for image patches. AR-Loss can effectively guarantee the reconstruction quality between image patches. Experiments show that compared to the leading methods, PSAR-SR well eliminates artifacts under the overlapping-free decomposition and achieves superior performance on existing methods (e.g., FSRCNN, CARN, SRResNet, RCAN and CAMixerSR). Moreover, PSAR-SR saves 53%–65% FLOPs in computational cost far beyond the leading methods. The code will be made available: https://github.com/dywang95/PSAR-SR.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004787",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computational complexity theory",
      "Computer science",
      "Computer vision",
      "Decomposition",
      "Ecology",
      "FLOPS",
      "Image (mathematics)",
      "Image processing",
      "Image quality",
      "Image resolution",
      "Image restoration",
      "Parallel computing",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Daoyong"
      },
      {
        "surname": "Yang",
        "given_name": "Xiaomin"
      },
      {
        "surname": "Liu",
        "given_name": "Jingyi"
      },
      {
        "surname": "Li",
        "given_name": "Haoran"
      },
      {
        "surname": "Jeon",
        "given_name": "Gwanggil"
      }
    ]
  },
  {
    "title": "Adaptive self-supervised learning for sequential recommendation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106570",
    "abstract": "Sequential recommendation typically utilizes deep neural networks to mine rich information in interaction sequences. However, existing methods often face the issue of insufficient interaction data. To alleviate the sparsity issue, self-supervised learning is introduced into sequential recommendation. Despite its effectiveness, we argue that current self-supervised learning-based (i.e., SSL-based) sequential recommendation models have the following limitations: (1) using only a single self-supervised learning method, either contrastive self-supervised learning or generative self-supervised learning. (2) employing a simple data augmentation strategy in either the graph structure domain or the node feature domain. We believe that they have not fully utilized the capabilities of both self-supervised methods and have not sufficiently explored the advantages of combining graph augmentation schemes. As a result, they often fail to learn better item representations. In light of this, we propose a novel multi-task sequential recommendation framework named A daptive S elf-supervised L earning for sequential Rec ommendation (ASLRec) . Specifically, our framework combines contrastive and generative self-supervised learning methods adaptively, simultaneously applying different perturbations at both the graph topology and node feature levels. This approach constructs diverse augmented graph views and employs multiple loss functions (including contrastive loss, generative loss, mask loss, and prediction loss) for joint training. By encompassing the capabilities of various methods, our model learns item representations across different augmented graph views to achieve better performance and effectively mitigate interaction noise and sparsity. In addition, we add a small proportion of random uniform noise to item representations, making the item representations more uniform and mitigating the inherent popularity bias in interaction records. We conduct extensive experiments on three publicly available benchmark datasets to evaluate our model. The results demonstrate that our approach achieves state-of-the-art performance compared to 14 other competitive methods: the hit rate (HR) improved by over 14.39%, and the normalized discounted cumulative gain (NDCG) increased by over 18.67%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004945",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Feature (linguistics)",
      "Feature learning",
      "Generative grammar",
      "Generative model",
      "Graph",
      "Linguistics",
      "Machine learning",
      "Philosophy",
      "Semi-supervised learning",
      "Supervised learning",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Xiujuan"
      },
      {
        "surname": "Sun",
        "given_name": "Fuzhen"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhiwei"
      },
      {
        "surname": "Li",
        "given_name": "Pengcheng"
      },
      {
        "surname": "Wang",
        "given_name": "Shaoqing"
      }
    ]
  },
  {
    "title": "Data-free knowledge distillation via generator-free data generation for Non-IID federated learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106627",
    "abstract": "Data heterogeneity (Non-IID) on Federated Learning (FL) is currently a widely publicized problem, which leads to local model drift and performance degradation. Because of the advantage of knowledge distillation, it has been explored in some recent work to refine global models. However, these approaches rely on a proxy dataset or a data generator. First, in many FL scenarios, proxy dataset do not necessarily exist on the server. Second, the quality of data generated by the generator is unstable and the generator depends on the computing resources of the server. In this work, we propose a novel data-Free knowledge distillation approach via generator-Free Data Generation for Non-IID FL, dubbed as FedF2DG. Specifically, FedF2DG requires only local models to generate pseudo datasets for each client, and can generate hard samples by adding an additional regularization term that exploit disagreements between local model and global model. Meanwhile, FedF2DG enables flexible utilization of computational resources by generating pseudo dataset locally or on the server. And to address the label distribution shift in Non-IID FL, we propose a Data Generation Principle that can adaptively control the label distribution and number of pseudo dataset based on client current state, and this allows for the extraction of more client knowledge. Then knowledge distillation is performed to transfer the knowledge in local models to the global model. Extensive experiments demonstrate that our proposed method significantly outperforms the state-of-the-art FL methods and can serve as plugin for existing Federated Learning methds such as FedAvg, FedProx, etc, and improve their performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005513",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Distillation",
      "Generator (circuit theory)",
      "Machine learning",
      "Organic chemistry",
      "Physics",
      "Power (physics)",
      "Proxy (statistics)",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Siran"
      },
      {
        "surname": "Liao",
        "given_name": "Tianchi"
      },
      {
        "surname": "Fu",
        "given_name": "Lele"
      },
      {
        "surname": "Chen",
        "given_name": "Chuan"
      },
      {
        "surname": "Bian",
        "given_name": "Jing"
      },
      {
        "surname": "Zheng",
        "given_name": "Zibin"
      }
    ]
  },
  {
    "title": "Curriculum learning empowered reinforcement learning for graph-based portfolio management: Performance optimization and comprehensive analysis",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106537",
    "abstract": "Portfolio management (PM) is a popular financial process that concerns the occasional reallocation of a particular quantity of capital into a portfolio of assets, with the main aim of maximizing profitability conditioned to a certain level of risk. Given the inherent dynamicity of stock exchanges and development for long-term performance, reinforcement learning (RL) has become a dominating solution for solving the problem of portfolio management in an automated and efficient manner. Nevertheless, the present RL-based PM methods just take into account the variations in prices of portfolio assets and the implications of price variations, while overlooking the significant relationships among different assets in the market, which are extremely valuable for managerial decisions. To close this gap, this paper introduces a novel deep model that combines two subnetworks; one to learn a temporal representation of historical prices using a refined temporal learner, while the other learns the relationships between different stocks in the market using a relation graph learner (RGL). Then, the above learners are integrated into the curriculum RL scheme for formulating the PM as a curriculum Markov Decision Process, in which an adaptive curriculum policy is presented to enable the agent to adaptively minimize risk value and maximize cumulative return. Proof-of-concept experiments are performed on data from three public stock indices (namely S&P500, NYSE, and NASDAQ), and the results demonstrate the efficiency of the proposed framework in improving the portfolio management performance over the competing RL solutions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004611",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Curriculum",
      "Graph",
      "Machine learning",
      "Pedagogy",
      "Psychology",
      "Reinforcement learning",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Salamai",
        "given_name": "Abdullah Ali"
      }
    ]
  },
  {
    "title": "Boosting integral-based human pose estimation through implicit heatmap learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106524",
    "abstract": "Human pose estimation typically encompasses three categories: heatmap-, regression-, and integral-based methods. While integral-based methods possess advantages such as end-to-end learning, full-convolution learning, and being free from quantization errors, they have garnered comparatively less attention due to inferior performance. In this paper, we revisit integral-based approaches for human pose estimation and propose a novel implicit heatmap learning framework. The framework learns the true distribution of keypoints from the perspective of maximum likelihood estimation, aiming to mitigate inherent ambiguity in shape and variance associated with implicit heatmaps. Specifically, Simple Implicit Heatmap Normalization (SIHN) is first introduced to calculate implicit heatmaps as an efficient and effective representation for keypoint localization, which replaces the vanilla softmax normalization method. As implicit heatmaps may introduce potential challenges related to variance and shape ambiguity arising from the inherent nature of implicit heatmaps, we thus propose a Differentiable Spatial-to-Distributive Transform (DSDT) method to aptly map those implicit heatmaps onto the transformation coefficients of a deformed distribution. The deformed distribution is predicted by a likelihood-based generative model to unravel the shape ambiguity quandary effectively, and the transformation coefficients are learned by a regression model to resolve the variance ambiguity issue. Additionally, to expedite the acquisition of precise shape representations throughout the training process, we introduce a Wasserstein Distance-based Constraint (WDC) to ensure stable and reasonable supervision during the initial generation of implicit heatmaps. Experimental results on both the MSCOCO and MPII datasets demonstrate the effectiveness of our proposed method, achieving competitive performance against heatmap-based approaches while maintaining the advantages of integral-based approaches. Our source codes and pre-trained models are available at https://github.com/ducongju/IHL.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004489",
    "keywords": [
      "Artificial intelligence",
      "Boosting (machine learning)",
      "Computer science",
      "Economics",
      "Estimation",
      "Machine learning",
      "Management",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pose"
    ],
    "authors": [
      {
        "surname": "Du",
        "given_name": "Congju"
      },
      {
        "surname": "Yan",
        "given_name": "Zengqiang"
      },
      {
        "surname": "Xiong",
        "given_name": "Zixiang"
      },
      {
        "surname": "Yu",
        "given_name": "Li"
      }
    ]
  },
  {
    "title": "Structural prior-driven feature extraction with gradient-momentum combined optimization for convolutional neural network image classification",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106511",
    "abstract": "Recent image classification efforts have achieved certain success by incorporating prior information such as labels and logical rules to learn discriminative features. However, these methods overlook the variability of features, resulting in feature inconsistency and fluctuations in model parameter updates, which further contribute to decreased image classification accuracy and model instability. To address this issue, this paper proposes a novel method combining structural prior-driven feature extraction with gradient-momentum (SPGM), from the perspectives of consistent feature learning and precise parameter updates, to enhance the accuracy and stability of image classification. Specifically, SPGM leverages a structural prior-driven feature extraction (SPFE) approach to calculate gradients of multi-level features and original images to construct structural information, which is then transformed into prior knowledge to drive the network to learn features consistent with the original images. Additionally, an optimization strategy integrating gradients and momentum (GMO) is introduced, dynamically adjusting the direction and step size of parameter updates based on the angle and norm of the sum of gradients and momentum, enabling precise model parameter updates. Extensive experiments on CIFAR10 and CIFAR100 datasets demonstrate that the SPGM method significantly reduces the top-1 error rate in image classification, enhances the classification performance, and outperforms state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004350",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Convolutional neural network",
      "Extraction (chemistry)",
      "Feature (linguistics)",
      "Feature extraction",
      "Image (mathematics)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Yunyun"
      },
      {
        "surname": "Li",
        "given_name": "Peng"
      },
      {
        "surname": "Xu",
        "given_name": "He"
      },
      {
        "surname": "Wang",
        "given_name": "Ruchuan"
      }
    ]
  },
  {
    "title": "Improved fixed-time stability analysis and applications to synchronization of discontinuous complex-valued fuzzy cellular neural networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106585",
    "abstract": "This article mainly centers on proposing new fixed-time (FXT) stability lemmas of discontinuous systems, in which novel optimization approaches are utilized and more relaxed conditions are required. The conventional discussions about V t > 1 and 0 < V t ⩽ 1 are no longer required. For the purpose of verifying the new lemmas, complex-valued fuzzy cellular neural networks (CVFCNNs) with discontinuous activation functions are studied with a nontraditional non-separation method, which could reduce the conservatism of the obtained results greatly. Besides, FXT synchronization is discussed simultaneously. When contrasted with the results of other similar prominent pioneering works nowadays, the accuracy of settling times (STs) is quite enhanced. At last, numerical simulations are conducted to demonstrate the validity and superiority of our established theoretical results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005094",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Channel (broadcasting)",
      "Computer science",
      "Conservatism",
      "Control (management)",
      "Control engineering",
      "Control theory (sociology)",
      "Engineering",
      "Fuzzy logic",
      "Law",
      "Machine learning",
      "Mathematics",
      "Political science",
      "Politics",
      "Real-time computing",
      "Settling time",
      "Stability (learning theory)",
      "Step response",
      "Synchronization (alternating current)",
      "Telecommunications",
      "Time synchronization"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Jingsha"
      },
      {
        "surname": "Yang",
        "given_name": "Jing"
      },
      {
        "surname": "Gan",
        "given_name": "Qintao"
      },
      {
        "surname": "Wu",
        "given_name": "Huaiqin"
      },
      {
        "surname": "Cao",
        "given_name": "Jinde"
      }
    ]
  },
  {
    "title": "Multi-level sequence denoising with cross-signal contrastive learning for sequential recommendation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106480",
    "abstract": "Sequential recommender systems (SRSs) aim to suggest next item for a user based on her historical interaction sequences. Recently, many research efforts have been devoted to attenuate the influence of noisy items in sequences by either assigning them with lower attention weights or discarding them directly. The major limitation of these methods is that the former would still prone to overfit noisy items while the latter may overlook informative items. To the end, in this paper, we propose a novel model named Multi-level Sequence Denoising with Cross-signal Contrastive Learning (MSDCCL) for sequential recommendation. To be specific, we first introduce a target-aware user interest extractor to simultaneously capture users’ long and short term interest with the guidance of target items. Then, we develop a multi-level sequence denoising module to alleviate the impact of noisy items by employing both soft and hard signal denoising strategies. Additionally, we extend existing curriculum learning by simulating the learning pattern of human beings. It is worth noting that our proposed model can be seamlessly integrated with a majority of existing recommendation models and significantly boost their effectiveness. Experimental studies on five public datasets are conducted and the results demonstrate that the proposed MSDCCL is superior to the state-of-the-art baselines. The source code is publicly available at https://github.com/lalunex/MSDCCL/tree/main.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004040",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Machine learning",
      "Noise reduction",
      "Pattern recognition (psychology)",
      "Programming language",
      "SIGNAL (programming language)",
      "Sequence (biology)",
      "Sequence learning",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Xiaofei"
      },
      {
        "surname": "Li",
        "given_name": "Liang"
      },
      {
        "surname": "Liu",
        "given_name": "Weidong"
      },
      {
        "surname": "Luo",
        "given_name": "Xin"
      }
    ]
  },
  {
    "title": "An information-theoretic perspective of physical adversarial patches",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106590",
    "abstract": "Real-world adversarial patches were shown to be successful in compromising state-of-the-art models in various computer vision applications. Most existing defenses rely on analyzing input or feature level gradients to detect the patch. However, these methods have been compromised by recent GAN-based attacks that generate naturalistic patches. In this paper, we propose a new perspective to defend against adversarial patches based on the entropy carried by the input, rather than on its saliency. We present Jedi, a new defense against adversarial patches that tackles the patch localization problem from an information theory perspective; leveraging the high entropy of adversarial patches to identify potential patch zones, and using an autoencoder to complete patch regions from high entropy kernels. Jedi achieves high-precision adversarial patch localization and removal, detecting on average 90% of adversarial patches across different benchmarks, and recovering up to 94% of successful patch attacks. Since Jedi relies on an input entropy analysis, it is model-agnostic, and can be applied to off-the-shelf models without changes to the training or inference of the models. Moreover, we propose a comprehensive qualitative analysis that investigates the cases where Jedi fails, comparatively with related methods. Interestingly, we find a significant core failure cases among the different defenses share one common property: high entropy. We think that this work offers a new perspective to understand the adversarial effect under physical-world settings. We also leverage these findings to enhance Jedi’s handling of entropy outliers by introducing Adaptive Jedi, which boosts performance by up to 9% in challenging images.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005148",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Entropy (arrow of time)",
      "Information theory",
      "Leverage (statistics)",
      "Machine learning",
      "Mathematics",
      "Outlier",
      "Physics",
      "Quantum mechanics",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Tarchoun",
        "given_name": "Bilel"
      },
      {
        "surname": "Ben Khalifa",
        "given_name": "Anouar"
      },
      {
        "surname": "Mahjoub",
        "given_name": "Mohamed Ali"
      },
      {
        "surname": "Abu-Ghazaleh",
        "given_name": "Nael"
      },
      {
        "surname": "Alouani",
        "given_name": "Ihsen"
      }
    ]
  },
  {
    "title": "Broad learning system based on maximum multi-kernel correntropy criterion",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106521",
    "abstract": "The broad learning system (BLS) is an effective machine learning model that exhibits excellent feature extraction ability and fast training speed. However, the traditional BLS is derived from the minimum mean square error (MMSE) criterion, which is highly sensitive to non-Gaussian noise. In order to enhance the robustness of BLS, this paper reconstructs the objective function of BLS based on the maximum multi-kernel correntropy criterion (MMKCC), and obtains a new robust variant of BLS (MKC-BLS). For the multitude of parameters involved in MMKCC, an effective parameter optimization method is presented. The fixed-point iteration method is employed to further optimize the model, and a reliable convergence proof is provided. In comparison to the existing robust variants of BLS, MKC-BLS exhibits superior performance in the non-Gaussian noise environment, particularly in the multi-modal noise environment. Experiments on multiple public datasets and real application validate the efficacy of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004453",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Kernel (algebra)",
      "Kernel method",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Haiquan"
      },
      {
        "surname": "Lu",
        "given_name": "Xin"
      }
    ]
  },
  {
    "title": "Fusing multi-scale functional connectivity patterns via Multi-Branch Vision Transformer (MB-ViT) for macaque brain age prediction",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106592",
    "abstract": "Brain age (BA) is defined as a measure of brain maturity and could help characterize both the typical brain development and neuropsychiatric disorders in mammals. Various biological phenotypes have been successfully applied to predict BA of human using chronological age (CA) as label. However, whether the BA of macaque, one of the most important animal models, can also be reliably predicted is largely unknown. To address this question, we propose a novel deep learning model called Multi-Branch Vision Transformer (MB-ViT) to fuse multi-scale (i.e., from coarse-grained to fine-grained) brain functional connectivity (FC) patterns derived from resting state functional magnetic resonance imaging (rs-fMRI) data to predict BA of macaques. The discriminative functional connections and the related brain regions contributing to the prediction are further identified based on Gradient-weighted Class Activation Mapping (Grad-CAM) method. Our proposed model successfully predicts BA of 450 normal rhesus macaques from the publicly available PRIMatE Data Exchange (PRIME-DE) dataset with lower mean absolute error (MAE) and mean square error (MSE) as well as higher Pearson's correlation coefficient (PCC) and coefficient of determination ( R 2 ) compared to other baseline models. The correlation between the predicted BA and CA reaches as high as 0.82 of our proposed method. Furthermore, our analysis reveals that the functional connections predominantly contributing to the prediction results are situated in the primary motor cortex (M1), visual cortex, area v23 in the posterior cingulate cortex, and dysgranular temporal pole. In summary, our proposed deep learning model provides an effective tool to accurately predict BA of primates (macaque in this study), and lays a solid foundation for future studies of age-related brain diseases in those animal models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005161",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Correlation coefficient",
      "Discriminative model",
      "Functional magnetic resonance imaging",
      "Immunology",
      "Macaque",
      "Machine learning",
      "Mathematics",
      "Matthews correlation coefficient",
      "Mean absolute error",
      "Mean squared error",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Pearson product-moment correlation coefficient",
      "Primate",
      "Resting state fMRI",
      "Rhesus macaque",
      "Standard deviation",
      "Statistics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Jingchao"
      },
      {
        "surname": "Chen",
        "given_name": "Yuzhong"
      },
      {
        "surname": "Jin",
        "given_name": "Xuewei"
      },
      {
        "surname": "Mao",
        "given_name": "Wei"
      },
      {
        "surname": "Xiao",
        "given_name": "Zhenxiang"
      },
      {
        "surname": "Zhang",
        "given_name": "Songyao"
      },
      {
        "surname": "Zhang",
        "given_name": "Tuo"
      },
      {
        "surname": "Liu",
        "given_name": "Tianming"
      },
      {
        "surname": "Kendrick",
        "given_name": "Keith"
      },
      {
        "surname": "Jiang",
        "given_name": "Xi"
      }
    ]
  },
  {
    "title": "LD-CSNet: A latent diffusion-based architecture for perceptual Compressed Sensing",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106541",
    "abstract": "Compressed Sensing (CS) is a groundbreaking paradigm in image acquisition, challenging the constraints of the Nyquist–Shannon sampling theorem. This enables high-quality image reconstruction using a minimal number of measurements. Neural Networks’ potent feature induction capabilities enable advanced data-driven CS methods to achieve high-fidelity image reconstruction. However, achieving satisfactory reconstruction performance, particularly in terms of perceptual quality, remains challenging at extremely low sampling rates. To tackle this challenge, we introduce a novel two-stage image CS framework based on latent diffusion, named LD-CSNet. In the first stage, we utilize an autoencoder pre-trained on a large dataset to represent natural images as low-dimensional latent vectors, establishing prior knowledge distinct from sparsity and effectively reducing the dimensionality of the solution space. In the second stage, we employ a conditional diffusion model for maximum likelihood estimates in the latent space. This is supported by a measurement embedding module designed to encode measurements, making them suitable for a denoising network. This guides the generation process in reconstructing low-dimensional latent vectors. Finally, the image is reconstructed using a pre-trained decoder. Experimental results across multiple public datasets demonstrate LD-CSNet’s superior perceptual quality and robustness to noise. It maintains fidelity and visual quality at lower sampling rates. Research findings suggest the promising application of diffusion models in image CS. Future research can focus on developing more appropriate models for the first stage.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004659",
    "keywords": [
      "Archaeology",
      "Architecture",
      "Artificial intelligence",
      "Compressed sensing",
      "Computer science",
      "Computer vision",
      "Diffusion",
      "Geography",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Perception",
      "Physics",
      "Psychology",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Bowen"
      },
      {
        "surname": "Sun",
        "given_name": "Guiling"
      },
      {
        "surname": "Dong",
        "given_name": "Liang"
      },
      {
        "surname": "Wang",
        "given_name": "Sirui"
      }
    ]
  },
  {
    "title": "Convergence analysis of sparse TSK fuzzy systems based on spectral Dai-Yuan conjugate gradient and application to high-dimensional feature selection",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106599",
    "abstract": "Dealing with high-dimensional problems has always been a key and challenging issue in the field of fuzzy systems. Traditional Takagi–Sugeno–Kang (TSK) fuzzy systems face the challenges of the curse of dimensionality and computational complexity when applied to high-dimensional data. To overcome these challenges, this paper proposes a novel approach for optimizing TSK fuzzy systems by integrating the spectral Dai-Yuan conjugate gradient (SDYCG) algorithm and the smoothing group L 0 regularization technique. This method aims to address the challenges faced by TSK fuzzy systems in handling high-dimensional problems. The smoothing group L 0 regularization technique is employed to introduce sparsity, select relevant features, and improve the generalization ability of the model. The SDYCG algorithm effectively accelerates convergence and enhances the learning performance of the network. Furthermore, we prove the weak convergence and strong convergence of the new algorithm under the strong Wolfe criterion, which means that the gradient norm of the error function with respect to the weight vector converges to zero, and the weight sequence approaches a fixed point.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005239",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Conjugate gradient method",
      "Convergence (economics)",
      "Curse of dimensionality",
      "Economic growth",
      "Economics",
      "Face (sociological concept)",
      "Feature (linguistics)",
      "Feature selection",
      "Field (mathematics)",
      "Fuzzy logic",
      "Linguistics",
      "Mathematical optimization",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pure mathematics",
      "Smoothing",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Ji",
        "given_name": "Deqing"
      },
      {
        "surname": "Fan",
        "given_name": "Qinwei"
      },
      {
        "surname": "Dong",
        "given_name": "Qingmei"
      },
      {
        "surname": "Liu",
        "given_name": "Yunlong"
      }
    ]
  },
  {
    "title": "Dark-DSAR: Lightweight one-step pipeline for action recognition in dark videos",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106622",
    "abstract": "Dark video human action recognition has a wide range of applications in the real world. General action recognition methods focus on the actor or the action itself, ignoring the dark scene where the action happens, resulting in unsatisfied accuracy in recognition. For dark scenes, the existing two-step action recognition methods are stage complex due to introducing additional augmentation steps, and the one-step pipeline method is not lightweight enough. To address these issues, a one-step Transformer-based method named Dark Domain Shift for Action Recognition (Dark-DSAR) is proposed in this paper, which integrates the tasks of domain migration and classification into a single step and enhances the model’s functional coherence with respect to these two tasks, making our Dark-DSAR has low computation but high accuracy. Specifically, the domain shift module (DSM) achieves domain adaption from dark to bright to reduce the number of parameters and the computational cost. Besides, we explore the matching relationship between the input video size and the model, which can further optimize the inference efficiency by removing the redundant information in videos through spatial resolution dropping. Extensive experiments have been conducted on the datasets of ARID1.5, HMDB51-Dark, and UAV-human-night. Results show that the proposed Dark-DSAR obtains the best Top-1 accuracy on ARID1.5 with 89.49%, which is 2.56% higher than the state-of-the-art method, 67.13% and 61.9% on HMDB51-Dark and UAV-human-night, respectively. In addition, ablation experiments reveal that the action classifiers can gain ≥ 1% in accuracy compared to the original model when equipped with our DSM.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400546X",
    "keywords": [
      "Action recognition",
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Inference",
      "Pattern recognition (psychology)",
      "Pipeline (software)",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Yin",
        "given_name": "Yuwei"
      },
      {
        "surname": "Liu",
        "given_name": "Miao"
      },
      {
        "surname": "Yang",
        "given_name": "Renjie"
      },
      {
        "surname": "Liu",
        "given_name": "Yuanzhong"
      },
      {
        "surname": "Tu",
        "given_name": "Zhigang"
      }
    ]
  },
  {
    "title": "Photonic deep residual time-delay reservoir computing",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106575",
    "abstract": "Time-delay reservoir computing (TDRC) represents a simplified variant of recurrent neural networks, employing a nonlinear node with a feedback mechanism to construct virtual nodes. The capabilities of TDRC can be enhanced by transitioning to a deep architecture. In this work, we propose a novel photonic deep residual TDRC (DR-TDRC) with augmented capabilities. The additional time delay added to the residual structure enables DR-TDRC superior to traditional deep structures across various benchmark tasks, especially in memory capability and almost an order of magnitude improvement in nonlinear channel equalization. Additionally, a specifically designed clipping algorithm is utilized to counteract the damage of redundant layers in deep structures, enabling the extension of the deep TDRC to dozens rather than just a few layers, with higher performance. We experimentally demonstrate the proof-of-concept with a 4-layer DR-TDRC containing 960 interrelated neurons (240 neurons per layer), based on four injection-locked distributed feedback lasers. We confirm the potential for scalable deep RC with elevated performance. Our results provide a feasible approach for expanding deep photonic computing to satisfy the boosting demand for artificial intelligence.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004994",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Clipping (morphology)",
      "Computer engineering",
      "Computer science",
      "Database",
      "Deep learning",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Materials science",
      "Nonlinear system",
      "Optoelectronics",
      "Philosophy",
      "Photonics",
      "Physics",
      "Quantum mechanics",
      "Recurrent neural network",
      "Reservoir computing",
      "Residual",
      "Scalability"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Changdi"
      },
      {
        "surname": "Huang",
        "given_name": "Yu"
      },
      {
        "surname": "Yang",
        "given_name": "Yigong"
      },
      {
        "surname": "Cai",
        "given_name": "Deyu"
      },
      {
        "surname": "Zhou",
        "given_name": "Pei"
      },
      {
        "surname": "Li",
        "given_name": "Nianqiang"
      }
    ]
  },
  {
    "title": "Neural operators for robust output regulation of hyperbolic PDEs",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106620",
    "abstract": "The recently introduced neural operator (NO) has been employed as a gain approximator in the backstepping stabilization control of first-order hyperbolic and parabolic partial differential equation (PDE) systems. Due to the global approximation ability of the DeepONet, the NO provides approximate spatial gain function with arbitrary accuracy. The closed-loop system stability can be ensured by the backstepping controller involving the approximate gain with sufficiently small error. In this paper, the NO theory is leveraged to solve the robust output regulation problem for a class of uncertain hyperbolic PDE systems under the design framework of backstepping-based regulator. The NO is trained offline on a dataset containing a sufficient number of system parameters and corresponding prior solutions of the kernel equation, so as to generate feedback gain for the robust regulator. Once the NO is trained, the kernel equation does not need to be solved ever again, for any new system parameters that do not exceed the range of the training set. Based on the internal model principle, the regulator is inherently robust to a degree of parameter uncertainty and error in approximate gain. Therefore, the tracking error can still converge to 0 if the extended regulator equations are solvable and the parameter uncertainty leads to an asymptotically stable origin. We provide a series of theory proofs and a numerical test under the approximate control and observation gains to demonstrate the robust regulation problem. It is shown that the NO is almost three orders of magnitude faster than PDE solver in generating kernel function, and the loss remains on the order of 1 0 − 4 in the test. This provides an opportunity to use the NO methodology for accelerated gain scheduling regulation for PDEs with time-varying system parameters.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005446",
    "keywords": [
      "Adaptive control",
      "Agronomy",
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Backstepping",
      "Biology",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Controller (irrigation)",
      "Hyperbolic function",
      "Hyperbolic partial differential equation",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Nonlinear system",
      "Partial differential equation",
      "Physics",
      "Quantum mechanics",
      "Robust control"
    ],
    "authors": [
      {
        "surname": "Xiao",
        "given_name": "Yu"
      },
      {
        "surname": "Yuan",
        "given_name": "Yuan"
      },
      {
        "surname": "Luo",
        "given_name": "Biao"
      },
      {
        "surname": "Xu",
        "given_name": "Xiaodong"
      }
    ]
  },
  {
    "title": "Multi-view heterogeneous graph learning with compressed hypergraph neural networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106562",
    "abstract": "Multi-view learning is an emerging field of multi-modal fusion, which involves representing a single instance using multiple heterogeneous features to improve compatibility prediction. However, existing graph-based multi-view learning approaches are implemented on homogeneous assumptions and pairwise relationships, which may not adequately capture the complex interactions among real-world instances. In this paper, we design a compressed hypergraph neural network from the perspective of multi-view heterogeneous graph learning. This approach effectively captures rich multi-view heterogeneous semantic information, incorporating a hypergraph structure that simultaneously enables the exploration of higher-order correlations between samples in multi-view scenarios. Specifically, we introduce efficient hypergraph convolutional networks based on an explainable regularizer-centered optimization framework. Additionally, a low-rank approximation is adopted as hypergraphs to reformat the initial complex multi-view heterogeneous graph. Extensive experiments compared with several advanced node classification methods and multi-view classification methods have demonstrated the feasibility and effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004866",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Discrete mathematics",
      "Graph",
      "Hypergraph",
      "Machine learning",
      "Mathematics",
      "Pairwise comparison",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Aiping"
      },
      {
        "surname": "Fang",
        "given_name": "Zihan"
      },
      {
        "surname": "Wu",
        "given_name": "Zhihao"
      },
      {
        "surname": "Tan",
        "given_name": "Yanchao"
      },
      {
        "surname": "Han",
        "given_name": "Peng"
      },
      {
        "surname": "Wang",
        "given_name": "Shiping"
      },
      {
        "surname": "Zhang",
        "given_name": "Le"
      }
    ]
  },
  {
    "title": "Open-world electrocardiogram classification via domain knowledge-driven contrastive learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106551",
    "abstract": "Automatic electrocardiogram (ECG) classification provides valuable auxiliary information for assisting disease diagnosis and has received much attention in research. The success of existing classification models relies on fitting the labeled samples for every ECG type. However, in practice, well-annotated ECG datasets usually cover only limited ECG types. It thus raises an issue: conventional classification models trained with limited ECG types can only identify those ECG types that have already been observed in the training set, but fail to recognize unseen (or unknown) ECG types that exist in the wild and are not included in training data. In this work, we investigate an important problem called open-world ECG classification that can predict fine-grained observed ECG classes and identify unseen classes. Accordingly, we propose a customized method that first incorporates clinical knowledge into contrastive learning by generating “hard negative” samples to guide learning diagnostic ECG features (i.e., distinguishable representations), and then performs multi-hypersphere learning to learn compact ECG representations for classification. The experiment results on 12-lead ECG datasets (CPSC2018, PTB-XL, and Georgia) demonstrate that the proposed method outperforms the state-of-the-art methods. Specifically, our method achieves superior accuracy than the comparative methods on the unseen ECG class and certain seen classes. Overall, the investigated problem (i.e., open-world ECG classification) helps to draw attention to the reliability of automatic ECG diagnosis, and the proposed method is proven effective in tackling the challenges. The code and datasets are released at https://github.com/betterzhou/Open_World_ECG_Classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004751",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Domain (mathematical analysis)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Natural language processing",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Shuang"
      },
      {
        "surname": "Huang",
        "given_name": "Xiao"
      },
      {
        "surname": "Liu",
        "given_name": "Ninghao"
      },
      {
        "surname": "Zhang",
        "given_name": "Wen"
      },
      {
        "surname": "Zhang",
        "given_name": "Yuan-Ting"
      },
      {
        "surname": "Chung",
        "given_name": "Fu-Lai"
      }
    ]
  },
  {
    "title": "Towards consensual representation: Model-agnostic knowledge extraction for dual heterogeneous federated fault diagnosis",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106618",
    "abstract": "Federated fault diagnosis has attracted increasing attention in industrial cloud–edge collaboration scenarios, where a ubiquitous assumption is that client models have the same architecture. Practically, this assumption cannot always be fulfilled due to requirements for personalized models, thereby resulting in the problem of model heterogeneity. Many approaches dealing with heterogeneous models tend to neglect the issue of representation bias, particularly in the context of non-identically and independently distributed data. In this article, to address the representation bias problem, Federated Model-Agnostic Knowledge Extraction (FedMAKE) is proposed. To bridge the information gap among clients, different from methods with public datasets, we initially develop two novel architecture-independent knowledge carriers. These carriers are derived based on the importance of process variables, without the need for additional datasets. Subsequently, we introduce a bi-directional distillation algorithm utilizing the two knowledge carriers. This algorithm facilitates the mutual transfer of knowledge embedded in carriers between a generative network and client models, thereby enabling the generation of fault data that is unbiased and well-balanced across categories. Furthermore, to mitigate the impact of statistical heterogeneity, we formulate a local objective for each client using two global knowledge carriers to guide local knowledge extraction and constrain client drift. Extensive experiments conducted on two prevalent industry datasets (TE and CWRU) illustrate that our proposed FedMAKE outperforms baseline methods. Specifically, FedMAKE enhances fault diagnosis accuracy by up to 11.7% on the TE dataset and up to 3.31% on the CWRU dataset compared to the sub-optimal method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005422",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Cloud computing",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Domain knowledge",
      "Fault (geology)",
      "Geology",
      "Knowledge extraction",
      "Law",
      "Machine learning",
      "Operating system",
      "Paleontology",
      "Political science",
      "Politics",
      "Process (computing)",
      "Representation (politics)",
      "Seismology"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jiaye"
      },
      {
        "surname": "Song",
        "given_name": "Pengyu"
      },
      {
        "surname": "Zhao",
        "given_name": "Chunhui"
      }
    ]
  },
  {
    "title": "Shuffling-type gradient method with bandwidth-based step sizes for finite-sum optimization",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106514",
    "abstract": "Shuffling-type gradient method is a popular machine learning algorithm that solves finite-sum optimization problems by randomly shuffling samples during iterations. In this paper, we explore the convergence properties of shuffling-type gradient method under mild assumptions. Specifically, we employ the bandwidth-based step size strategy that covers both monotonic and non-monotonic step sizes, thereby providing a unified convergence guarantee in terms of step size. Additionally, we replace the lower bound assumption of the objective function with that of the loss function, thereby eliminating the restrictions on the variance and the second-order moment of stochastic gradient that are difficult to verify in practice. For non-convex objectives, we recover the last iteration convergence of shuffling-type gradient algorithm with a less cumbersome proof. Meanwhile, we also establish the convergence rate for the minimum iteration of gradient norms. Under the Polyak-Łojasiewicz (PL) condition, we prove that the function value of last iteration converges to the lower bound of the objective function. By selecting appropriate boundary functions, we further improve the previous sublinear convergence rate results. Overall, this paper contributes to the understanding of shuffling-type gradient method and its convergence properties, providing insights for optimizing finite-sum problems in machine learning. Finally, numerical experiments demonstrate the efficiency of shuffling-type gradient method with bandwidth-based step size and validate our theoretical results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004386",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Biology",
      "Computer science",
      "Ecology",
      "Gradient method",
      "Mathematical optimization",
      "Mathematics",
      "Programming language",
      "Shuffling",
      "Telecommunications",
      "Type (biology)"
    ],
    "authors": [
      {
        "surname": "Liang",
        "given_name": "Yuqing"
      },
      {
        "surname": "Yang",
        "given_name": "Yang"
      },
      {
        "surname": "Liu",
        "given_name": "Jinlan"
      },
      {
        "surname": "Xu",
        "given_name": "Dongpo"
      }
    ]
  },
  {
    "title": "Optimal synchronization with L 2 -gain performance: An adaptive dynamic programming approach",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106566",
    "abstract": "This paper studies an optimal synchronous control protocol design for nonlinear multi-agent systems under partially known dynamics and uncertain external disturbance. Under some mild assumptions, Hamilton–Jacobi–Isaacs equation is derived by the performance index function and system dynamics, which serves as an equivalent formulation. Distributed policy iteration adaptive dynamic programming is developed to obtain the numerical solution to the Hamilton–Jacobi–Isaacs equation. Three theoretical results are given about the proposed algorithm. First, the iterative variables is proved to converge to the solution to Hamilton–Jacobi–Isaacs equation. Second, the L 2 -gain performance of the closed loop system is achieved. As a special case, the origin of the nominal system is asymptotically stable. Third, the obtained control protocol constitutes an Nash equilibrium solution. Neural network-based implementation is designed following the main results. Finally, two numerical examples are provided to verify the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004908",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Biology",
      "Combinatorics",
      "Computer science",
      "Evolutionary biology",
      "Function (biology)",
      "Hamilton–Jacobi equation",
      "Iterative method",
      "Mathematics",
      "Synchronization (alternating current)",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Zitao"
      },
      {
        "surname": "Chen",
        "given_name": "Kairui"
      },
      {
        "surname": "Tang",
        "given_name": "Ruizhi"
      }
    ]
  },
  {
    "title": "Unsupervised and semi-supervised domain adaptation networks considering both global knowledge and prototype-based local class information for Motor Imagery Classification",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106497",
    "abstract": "The non-stationarity of EEG signals results in variability across sessions, impeding model building and data sharing. In this paper, we propose a domain adaptation method called GPL, which simultaneously considers global knowledge and prototype-based local class information to enhance the classification accuracy of motor imagery signals. Depending on the amount of labeled data available in the target domain, the method is implemented in both unsupervised and semi-supervised versions. Specifically, at the global level, we employ the maximum mean difference (MMD) loss to globally constrain the feature space, achieving comprehensive alignment. In the context of class-level operations, we propose two memory banks designed to accommodate class prototypes in each domain and constrain feature embeddings by applying two prototype-based contrastive losses. The source contrastive loss is used to organize source features spatially based on categories, thereby reconciling inter-class and intra-class relationships, while the interactive contrastive loss is employed to facilitate cross-domain information interaction. Simultaneously, in unsupervised scenarios, to mitigate the adverse effects of excessive pseudo-labels, we introduce an entropy-aware strategy that dynamically evaluates the confidence level of target data and personalized constraints on the participation of interactive contrastive loss. To validate our approach, extensive experiments were conducted on a highly regarded public EEG dataset, namely Dataset IIa of the BCI Competition IV, as well as a large-scale EEG dataset called GigaDB. The experiments yielded average classification accuracies of 86.03% and 84.22% respectively. These results demonstrate that our method is an effective EEG decoding model, conducive to advancing the development of motor imagery brain–computer interfaces. The architecture proposed in this study and the code for data partitioning can be found at https://github.com/zhangdx21/GPL.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004210",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Artificial neural network",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Psychology"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Dongxue"
      },
      {
        "surname": "Li",
        "given_name": "Huiying"
      },
      {
        "surname": "Xie",
        "given_name": "Jingmeng"
      }
    ]
  },
  {
    "title": "Progressive Neighbor-masked Contrastive Learning for Fusion-style Deep Multi-view Clustering",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106503",
    "abstract": "Fusion-style Deep Multi-view Clustering (FDMC) can efficiently integrate comprehensive feature information from latent embeddings of multiple views and has drawn much attention recently. However, existing FDMC methods suffer from the interference of view-specific information for fusion representation, affecting the learning of discriminative cluster structure. In this paper, we propose a new framework of Progressive Neighbor-masked Contrastive Learning for FDMC (PNCL-FDMC) to tackle the aforementioned issues. Specifically, by using neighbor-masked contrastive learning, PNCL-FDMC can explicitly maintain the local structure during the embedding aggregation, which is beneficial to the common semantics enhancement on the fusion view. Based on the consistent aggregation, the fusion view is further enhanced by diversity-aware cluster structure enhancement. In this process, the enhanced cluster assignments and cluster discrepancies are employed to guide the weighted neighbor-masked contrastive alignment of semantic structure between individual views and the fusion view. Extensive experiments validate the effectiveness of the proposed framework, revealing its ability in discriminative representation learning and improving clustering performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004271",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature learning",
      "Fusion",
      "Law",
      "Linguistics",
      "Machine learning",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Semantics (computer science)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Mingyang"
      },
      {
        "surname": "Yang",
        "given_name": "Zuyuan"
      },
      {
        "surname": "Han",
        "given_name": "Wei"
      },
      {
        "surname": "Xie",
        "given_name": "Shengli"
      }
    ]
  },
  {
    "title": "Robust visual question answering via polarity enhancement and contrast",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106560",
    "abstract": "The Visual Question Answering (VQA) task is an important research direction in the field of artificial intelligence, which requires a model that can simultaneously understand visual images and natural language questions, and answer questions related to images. Recent studies have shown that many Visual Question Answering models rely on statistically regular correlations between questions and answers, which in turn weakens the correlation between visual content and textual information. In this work, we propose an unbiased Visual Question Answering method to solve language priors from the perspective of strengthening the contrast between the correct answer and the positive and negative predictions. We design a new model consisting of two modules with different roles. We input the image and the question corresponding to it into the Answer Visual Attention Modules to generate positive prediction output, and then use a Dual Channels Joint Module to generate negative prediction output with great linguistic prior knowledge. Finally, we input the positive and negative predictions together with the correct answer to our newly designed loss function for training. Our method achieves high performance (61.24%) on the VQA-CP v2 dataset. In addition, most existing debiasing methods improve performance on VQA-CP v2 dataset at the cost of reducing performance on VQA v2 dataset, while our method not only does not reduce the accuracy on VQA v2 dataset. Instead, it improves performance on both datasets mentioned above.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004842",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Cognitive science",
      "Computer science",
      "Contrast (vision)",
      "Debiasing",
      "Dual (grammatical number)",
      "Field (mathematics)",
      "Literature",
      "Machine learning",
      "Mathematics",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Psychology",
      "Pure mathematics",
      "Question answering"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Dahe"
      },
      {
        "surname": "Li",
        "given_name": "Zhixin"
      }
    ]
  },
  {
    "title": "A two-stage importance-aware subgraph convolutional network based on multi-source sensors for cross-domain fault diagnosis",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106518",
    "abstract": "Graph convolutional networks (GCNs) as the emerging neural networks have shown great success in Prognostics and Health Management because they can not only extract node features but can also mine relationship between nodes in the graph data. However, the most existing GCNs-based methods are still limited by graph quality, variable working conditions, and limited data, making them difficult to obtain remarkable performance. Therefore, it is proposed in this paper a two stage importance-aware subgraph convolutional network based on multi-source sensors named I 2 SGCN to address the above-mentioned limitations. In the real-world scenarios, it is found that the diagnostic performance of the most existing GCNs is commonly bounded by the graph quality because it is hard to get high quality through a single sensor. Therefore, we leveraged multi-source sensors to construct graphs that contain more fault-based information of mechanical equipment. Then, we discovered that unsupervised domain adaptation (UDA) methods only use single stage to achieve cross-domain fault diagnosis and ignore more refined feature extraction, which can make the representations contained in the features inadequate. Hence, it is proposed the two-stage fault diagnosis in the whole framework to achieve UDA. In the first stage, the multiple-instance learning is adopted to obtain the importance factor of each sensor towards preliminary fault diagnosis. In the second stage, it is proposed I 2 SGCN to achieve refined cross-domain fault diagnosis. Moreover, we observed that deficient and limited data may cause label bias and biased training, leading to reduced generalization capacity of the proposed method. Therefore, we constructed the feature-based graph and importance-based graph to jointly mine more effective relationship and then presented a subgraph learning strategy, which not only enriches sufficient and complementary features but also regularizes the training. Comprehensive experiments conducted on four case studies demonstrate the effectiveness and superiority of the proposed method for cross-domain fault diagnosis, which outperforms the state-of-the art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004428",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Deep learning",
      "Domain (mathematical analysis)",
      "Fault (geology)",
      "Feature engineering",
      "Geology",
      "Graph",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Prognostics",
      "Seismology",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Yue"
      },
      {
        "surname": "He",
        "given_name": "Youqian"
      },
      {
        "surname": "Karimi",
        "given_name": "Hamid Reza"
      },
      {
        "surname": "Gelman",
        "given_name": "Len"
      },
      {
        "surname": "Cetin",
        "given_name": "Ahmet Enis"
      }
    ]
  },
  {
    "title": "Mittag-Leffler stability and application of delayed fractional-order competitive neural networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106501",
    "abstract": "In the article, the Mittag-Leffler stability and application of delayed fractional-order competitive neural networks (FOCNNs) are developed. By virtue of the operator pair, the conditions of the coexistence of equilibrium points (EPs) are discussed and analyzed for delayed FOCNNs, in which the derived conditions of coexistence improve the existing results. In particular, these conditions are simplified in FOCNNs with stepped activations. Furthermore, the Mittag-Leffler stability of delayed FOCNNs is established by using the principle of comparison, which enriches the methodologies of fractional-order neural networks. The results on the obtained stability can be used to design the horizontal line detection of images, which improves the practicability of image detection results. Two simulations are displayed to validate the superiority of the obtained results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004258",
    "keywords": [
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Economics",
      "Finance",
      "Machine learning",
      "Mathematics",
      "Order (exchange)",
      "Stability (learning theory)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Fanghai"
      },
      {
        "surname": "Huang",
        "given_name": "Tingwen"
      },
      {
        "surname": "Wu",
        "given_name": "Ailong"
      },
      {
        "surname": "Zeng",
        "given_name": "Zhigang"
      }
    ]
  },
  {
    "title": "Joint Dual Feature Distillation and Gradient Progressive Pruning for BERT compression",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106533",
    "abstract": "The increasing size of pre-trained language models has led to a growing interest in model compression. Pruning and distillation are the primary methods employed to compress these models. Existing pruning and distillation methods are effective in maintaining model accuracy and reducing its size. However, they come with limitations. For instance, pruning is often suboptimal and biased by transforming it into a continuous optimization problem. Distillation relies primarily on one-to-one layer mappings for knowledge transfer, which leads to underutilization of the rich knowledge in teacher. Therefore, we propose a method of joint pruning and distillation for automatic pruning of pre-trained language models. Specifically, we first propose Gradient Progressive Pruning (GPP), which achieves a smooth transition of indicator vector values from real to binary by progressively converging the values of unimportant units’ indicator vectors to zero before the end of the search phase. This effectively overcomes the limitations of traditional pruning methods while supporting compression with higher sparsity. In addition, we propose the Dual Feature Distillation (DFD). DFD adaptively globally fuses teacher features and locally fuses student features, and then uses the dual features of global teacher features and local student features for knowledge distillation. This realizes a “preview-review” mechanism that can better extract useful information from multi-level teacher information and transfer it to student. Comparative experiments on the GLUE benchmark dataset and ablation experiments indicate that our method outperforms other state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400457X",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Chemistry",
      "Chromatography",
      "Composite material",
      "Compression (physics)",
      "Computer science",
      "Distillation",
      "Dual (grammatical number)",
      "Engineering",
      "Feature (linguistics)",
      "Joint (building)",
      "Linguistics",
      "Literature",
      "Materials science",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pruning",
      "Structural engineering"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zhou"
      },
      {
        "surname": "Lu",
        "given_name": "Yang"
      },
      {
        "surname": "Wang",
        "given_name": "Tengfei"
      },
      {
        "surname": "Wei",
        "given_name": "Xing"
      },
      {
        "surname": "Wei",
        "given_name": "Zhen"
      }
    ]
  },
  {
    "title": "Dual-stage feedback network for lightweight color image compression artifact reduction",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106555",
    "abstract": "Lossy image coding techniques usually result in various undesirable compression artifacts. Recently, deep convolutional neural networks have seen encouraging advances in compression artifact reduction. However, most of them focus on the restoration of the luma channel without considering the chroma components. Besides, most deep convolutional neural networks are hard to deploy in practical applications because of their high model complexity. In this article, we propose a dual-stage feedback network (DSFN) for lightweight color image compression artifact reduction. Specifically, we propose a novel curriculum learning strategy to drive a DSFN to reduce color image compression artifacts in a luma-to-RGB manner. In the first stage, the DSFN is dedicated to reconstructing the luma channel, whose high-level features containing rich structural information are then rerouted to the second stage by a feedback connection to guide the RGB image restoration. Furthermore, we present a novel enhanced feedback block for efficient high-level feature extraction, in which an adaptive iterative self-refinement module is carefully designed to refine the low-level features progressively, and an enhanced separable convolution is advanced to exploit multiscale image information fully. Extensive experiments show the notable advantage of our DSFN over several state-of-the-art methods in both quantitative indices and visual effects with lower model complexity.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004799",
    "keywords": [
      "Artificial intelligence",
      "Compression artifact",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Image compression",
      "Image processing",
      "Linguistics",
      "Lossy compression",
      "Philosophy",
      "RGB color model"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Zhengxin"
      },
      {
        "surname": "He",
        "given_name": "Xiaohai"
      },
      {
        "surname": "Zhang",
        "given_name": "Tingrong"
      },
      {
        "surname": "Xiong",
        "given_name": "Shuhua"
      },
      {
        "surname": "Ren",
        "given_name": "Chao"
      }
    ]
  },
  {
    "title": "Spectral Decomposition and Transformation for Cross-domain Few-shot Learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106536",
    "abstract": "Cross-domain few-shot Learning (CDFSL) is proposed to first pre-train deep models on a source domain dataset where sufficient data is available, and then generalize models to target domains to learn from only limited data. However, the gap between the source and target domains greatly hampers the generalization and target-domain few-shot finetuning. To address this problem, we analyze the domain gap from the aspect of frequency-domain analysis. We find the domain gap could be reflected by the compositions of source-domain spectra, and the lack of compositions in the source datasets limits the generalization. Therefore, we aim to expand the coverage of spectra composition in the source datasets to help the source domain cover a larger range of possible target-domain information, to mitigate the domain gap. To achieve this goal, we propose the Spectral Decomposition and Transformation (SDT) method, which first randomly decomposes the spectrogram of the source datasets into orthogonal bases, and then randomly samples different coordinates in the space formed by these bases. We integrate the above process into a data augmentation module, and further design a two-stream network to handle augmented images and original images respectively. Experimental results show that our method achieves state-of-the-art performance in the CDFSL benchmark dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400460X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Domain (mathematical analysis)",
      "Frequency domain",
      "Gene",
      "Generalization",
      "Geodesy",
      "Geography",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Spectrogram",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Yicong"
      },
      {
        "surname": "Zou",
        "given_name": "Yixiong"
      },
      {
        "surname": "Li",
        "given_name": "Ruixuan"
      },
      {
        "surname": "Li",
        "given_name": "Yuhua"
      }
    ]
  },
  {
    "title": "CDGT: Constructing diverse graph transformers for emotion recognition from facial videos",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106573",
    "abstract": "Recognizing expressions from dynamic facial videos can find more natural affect states of humans, and it becomes a more challenging task in real-world scenes due to pose variations of face, partial occlusions and subtle dynamic changes of emotion sequences. Existing transformer-based methods often focus on self-attention to model the global relations among spatial features or temporal features, which cannot well focus on important expression-related locality structures from both spatial and temporal features for the in-the-wild expression videos. To this end, we incorporate diverse graph structures into transformers and propose a CDGT method to construct diverse graph transformers for efficient emotion recognition from in-the-wild videos. Specifically, our method contains a spatial dual-graphs transformer and a temporal hyperbolic-graph transformer. The former deploys a dual-graph constrained attention to capture latent emotion-related graph geometry structures among local spatial tokens for efficient feature representation, especially for the video frames with pose variations and partial occlusions. The latter adopts a hyperbolic-graph constrained self-attention that explores important temporal graph structure information under hyperbolic space to model more subtle changes of dynamic emotion. Extensive experimental results on in-the-wild video-based facial expression databases show that our proposed CDGT outperforms other state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004970",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Facial expression",
      "Graph",
      "Linguistics",
      "Locality",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Theoretical computer science",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Dongliang"
      },
      {
        "surname": "Wen",
        "given_name": "Guihua"
      },
      {
        "surname": "Li",
        "given_name": "Huihui"
      },
      {
        "surname": "Yang",
        "given_name": "Pei"
      },
      {
        "surname": "Chen",
        "given_name": "Chuyun"
      },
      {
        "surname": "Wang",
        "given_name": "Bao"
      }
    ]
  },
  {
    "title": "Gradient-based optimization for quantum architecture search",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106508",
    "abstract": "Quantum Architecture Search (QAS) has shown significant promise in designing quantum circuits for Variational Quantum Algorithms (VQAs). However, existing QAS algorithms primarily explore circuit architectures within a discrete space, which is inherently inefficient. In this paper, we propose a Gradient-based Optimization for Quantum Architecture Search (GQAS), which leverages a circuit encoder, decoder, and predictor. Initially, the encoder embeds circuit architectures into a continuous latent representation. Subsequently, a predictor utilizes this continuous latent representation as input and outputs an estimated performance for the given architecture. The latent representation is then optimized through gradient descent within the continuous latent space based on the predicted performance. The optimized latent representation is finally mapped back to a discrete architecture via the decoder. To enhance the quality of the latent representation, we pre-train the encoder on a substantial dataset of circuit architectures using Self-Supervised Learning (SSL). Our simulation results on the Variational Quantum Eigensolver (VQE) indicate that our method outperforms the current Differentiable Quantum Architecture Search (DQAS).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004325",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Encoder",
      "Gradient descent",
      "Law",
      "Operating system",
      "Physics",
      "Political science",
      "Politics",
      "Quantum",
      "Quantum circuit",
      "Quantum computer",
      "Quantum mechanics",
      "Quantum network",
      "Representation (politics)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "He",
        "given_name": "Zhimin"
      },
      {
        "surname": "Wei",
        "given_name": "Jiachun"
      },
      {
        "surname": "Chen",
        "given_name": "Chuangtao"
      },
      {
        "surname": "Huang",
        "given_name": "Zhiming"
      },
      {
        "surname": "Situ",
        "given_name": "Haozhen"
      },
      {
        "surname": "Li",
        "given_name": "Lvzhou"
      }
    ]
  },
  {
    "title": "A cross-temporal contrastive disentangled model for ancient Chinese understanding",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106559",
    "abstract": "Ancient Chinese is a crucial bridge for understanding Chinese history and culture. Most existing works utilize high-resource modern Chinese to understand low-resource ancient Chinese, but they fail to fully consider the semantic and syntactic gaps between them due to their changes over time, resulting in the misunderstanding of ancient Chinese. Hence, we propose a novel language pre-training framework for ancient Chinese understanding based on the Cross-temporal Contrastive Disentanglement Model (CCDM), which bridges the gap between modern and ancient Chinese with their parallel corpus. Specifically, we first explore a cross-temporal data augmentation method by disentangling and reconstructing the parallel ancient-modern corpus. It is noteworthy that the proposed decoupling strategy takes full account of the cross-temporal character between ancient and modern Chinese. Then, cross-temporal contrastive learning is exploited to train the model by fully leveraging the cross-temporal information. Finally, the trained language model is utilized for downstream tasks. We conduct extensive experiments on six ancient Chinese understanding tasks. Results demonstrate that our model outperforms the state-of-the-art baselines. Our framework also holds potential applicability to other languages that have undergone evolutionary changes, leading to shifts in syntax and semantics. 1 1 Our model is available at https://github.com/yuting-wei/CCDM.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004830",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Linguistics",
      "Natural language processing",
      "Philosophy",
      "Programming language",
      "Semantics (computer science)",
      "Syntax"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Yuting"
      },
      {
        "surname": "Zhu",
        "given_name": "Yangfu"
      },
      {
        "surname": "Bai",
        "given_name": "Ting"
      },
      {
        "surname": "Wu",
        "given_name": "Bin"
      }
    ]
  },
  {
    "title": "A smoothing approximation-based adaptive neurodynamic approach for nonsmooth resource allocation problem",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106625",
    "abstract": "In this paper, a smoothing approximation-based adaptive neurodynamic approach is proposed for a nonsmooth resource allocation problem (NRAP) with multiple constraints. The smoothing approximation method is combined with multi-agent systems to avoid the introduction of set-valued subgradient terms, thereby facilitating the practical implementation of the neurodynamic approach. In addition, using the adaptive penalty technique, private inequality constraints are processed, which eliminates the need for additional quantitative estimation of penalty parameters and significantly reduces the computational cost. Moreover, to reduce the impact of smoothing approximation on the convergence of the neurodynamic approach, time-varying control parameters are introduced. Due to the parallel computing characteristics of multi-agent systems, the neurodynamic approach proposed in this paper is completely distributed. Theoretical proof shows that the state solution of the neurodynamic approach converges to the optimal solution of NRAP. Finally, two application examples are used to validate the feasibility of the neurodynamic approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005495",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Mathematical optimization",
      "Mathematics",
      "Resource allocation",
      "Smoothing"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Haoze"
      },
      {
        "surname": "Luan",
        "given_name": "Linhua"
      },
      {
        "surname": "Qin",
        "given_name": "Sitian"
      }
    ]
  },
  {
    "title": "Multi-task neural networks by learned contextual inputs",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106528",
    "abstract": "This paper explores learned-context neural networks. It is a multi-task learning architecture based on a fully shared neural network and an augmented input vector containing trainable task parameters. The architecture is interesting due to its powerful task adaption mechanism, which facilitates a low-dimensional task parameter space. Theoretically, we show that a scalar task parameter is sufficient for universal approximation of all tasks, which is not necessarily the case for more common architectures. Empirically it is shown that, for homogeneous tasks, the dimension of the task parameter may vary with the complexity of the tasks, but a small task parameter space is generally viable. The task parameter space is found to be well-behaved, which simplifies workflows related to updating models as new data arrives, and learning new tasks with the shared parameters are frozen. Additionally, the architecture displays robustness towards datasets where tasks have few data points. The architecture’s performance is compared to similar neural network architectures on ten datasets, with competitive results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004520",
    "keywords": [
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Database",
      "Economics",
      "Gene",
      "Geometry",
      "Machine learning",
      "Management",
      "Mathematics",
      "Robustness (evolution)",
      "Scalar (mathematics)",
      "Task (project management)",
      "Visual arts",
      "Workflow"
    ],
    "authors": [
      {
        "surname": "Sandnes",
        "given_name": "Anders T."
      },
      {
        "surname": "Grimstad",
        "given_name": "Bjarne"
      },
      {
        "surname": "Kolbjørnsen",
        "given_name": "Odd"
      }
    ]
  },
  {
    "title": "Diffusion probabilistic model for bike-sharing demand recovery with factual knowledge fusion",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106538",
    "abstract": "The mining of diverse patterns from bike flow has attracted widespread interest from researchers and practitioners. Prior arts concentrate on forecasting the flow evolution from bike demand records. Nevertheless, a tricky reality is the frequent occurrence of missing bike flow, which hinders us from accurately understanding flow patterns. This study investigates an interesting task, i.e., Bike-sharing demand recovery (Biker). Biker is not a simple time-series imputation problem, rather, it confronts three concerns: observation uncertainty, complex dependencies, and environmental facts. To this end, we present a novel diffusion probabilistic solution with factual knowledge fusion, namely DBiker. Specifically, DBiker is the first attempt to extend the diffusion probabilistic models to the Biker task, along with a conditional Markov decision-making process. In contrast to existing probabilistic solutions, DBiker forecasts missing observations through progressive steps guided by an adaptive prior. Particularly, we introduce a Flow Conditioner with step embedding and a Factual Extractor to explore the complex dependencies and multiple environmental facts, respectively. Additionally, we devise a self-gated fusion layer that adaptively selects valuable knowledge to act as an adaptive prior, guiding the generation of missing observations. Finally, experiments conducted on three real-world bike systems demonstrate the superiority of DBiker against several baselines.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004623",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Engineering",
      "Machine learning",
      "Probabilistic logic",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Li"
      },
      {
        "surname": "Li",
        "given_name": "Pei"
      },
      {
        "surname": "Gao",
        "given_name": "Qiang"
      },
      {
        "surname": "Liu",
        "given_name": "Guisong"
      },
      {
        "surname": "Luo",
        "given_name": "Zhipeng"
      },
      {
        "surname": "Li",
        "given_name": "Tianrui"
      }
    ]
  },
  {
    "title": "FLAT: Fusing layer representations for more efficient transfer learning in NLP",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106631",
    "abstract": "Parameter efficient transfer learning (PETL) methods provide an efficient alternative for fine-tuning. However, typical PETL methods inject the same structures to all Pre-trained Language Model (PLM) layers and only use the final hidden states for downstream tasks, regardless of the knowledge diversity across PLM layers. Additionally, the backpropagation path of existing PETL methods still passes through the frozen PLM during training, which is computational and memory inefficient. In this paper, we propose FLAT, a generic PETL method that explicitly and individually combines knowledge across all PLM layers based on the tokens to perform a better transferring. FLAT considers the backbone PLM as a feature extractor and combines the features in a side-network, hence the backpropagation does not involve the PLM, which results in much less memory requirement than previous methods. The results on the GLUE benchmark show that FLAT outperforms other tuning techniques in the low-resource scenarios and achieves on-par performance in the high-resource scenarios with only 0.53% trainable parameters per task and 3 . 2 × less GPU memory usagewith BERT base . Besides, further ablation study is conducted to reveal that the proposed fusion layer effectively combines knowledge from PLM and helps the classifier to exploit the PLM knowledge to downstream tasks. We will release our code for better reproducibility.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005550",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Backpropagation",
      "Benchmark (surveying)",
      "Chemistry",
      "Computer network",
      "Computer science",
      "Economics",
      "Engineering",
      "Extractor",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Layer (electronics)",
      "Linguistics",
      "Machine learning",
      "Management",
      "Organic chemistry",
      "Path (computing)",
      "Philosophy",
      "Process engineering",
      "Programming language",
      "Resource (disambiguation)",
      "Task (project management)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Wenqiang"
      },
      {
        "surname": "Li",
        "given_name": "Yang"
      },
      {
        "surname": "Liu",
        "given_name": "Chunyan"
      },
      {
        "surname": "Zhao",
        "given_name": "Yunlong"
      }
    ]
  },
  {
    "title": "Quality-diversity based semi-autonomous teleoperation using reinforcement learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106543",
    "abstract": "Recent successes in robot learning have significantly enhanced autonomous systems across a wide range of tasks. However, they are prone to generate similar or the same solutions, limiting the controllability of the robot to behave according to user intentions. These limited robot behaviors may lead to collisions and potential harm to humans. To resolve these limitations, we introduce a semi-autonomous teleoperation framework that enables users to operate a robot by selecting a high-level command, referred to as option. Our approach aims to provide effective and diverse options by a learned policy, thereby enhancing the efficiency of the proposed framework. In this work, we propose a quality-diversity (QD) based sampling method that simultaneously optimizes both the quality and diversity of options using reinforcement learning (RL). Additionally, we present a mixture of latent variable models to learn multiple policy distributions defined as options. In experiments, we show that the proposed method achieves superior performance in terms of the success rate and diversity of the options in simulation environments. We further demonstrate that our method outperforms manual keyboard control for time duration over cluttered real-world environments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004672",
    "keywords": [
      "Anthropology",
      "Applied mathematics",
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Controllability",
      "Diversity (politics)",
      "Duration (music)",
      "Epistemology",
      "Literature",
      "Machine learning",
      "Mathematics",
      "Philosophy",
      "Quality (philosophy)",
      "Reinforcement learning",
      "Robot",
      "Sociology",
      "Teleoperation"
    ],
    "authors": [
      {
        "surname": "Park",
        "given_name": "Sangbeom"
      },
      {
        "surname": "Yoon",
        "given_name": "Taerim"
      },
      {
        "surname": "Lee",
        "given_name": "Joonhyung"
      },
      {
        "surname": "Park",
        "given_name": "Sunghyun"
      },
      {
        "surname": "Choi",
        "given_name": "Sungjoon"
      }
    ]
  },
  {
    "title": "T-distributed Stochastic Neighbor Network for unsupervised representation learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106520",
    "abstract": "Unsupervised representation learning (URL) is still lack of a reasonable operator (e.g. convolution kernel) for exploring meaningful structural information from generic data including vector, image and tabular data. In this paper, we propose a simple end-to-end T-distributed Stochastic Neighbor Network (TsNet) for URL with clustering downstream task. Concretely, our TsNet model has three major components: (1) an adaptive connectivity distribution learning module is presented to construct a pairwise graph for preserving the local structure of generic data; (2) a T-distributed stochastic neighbor embedding based loss function is designed to learn a transformation between embeddings and original data, which improves the discrimination of representations; (3) a nonlinear parametric mapping is learned via our TsNet on an unsupervised generalized manner, which can address the “out-of-sample” issue. By combining these components, our method is able to considerably outperform previous related unsupervised learning approaches on visualization and clustering of generic data. A simple deep neural network equipped on our model respectively achieves 74.90%, 76.56% ACC and NMI, which is 8% relative improvement over previous state-of-the-art on real single-cell RNA-sequencing (scRNA-seq) datasets clustering.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004441",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Feature learning",
      "Law",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zheng"
      },
      {
        "surname": "Xie",
        "given_name": "Jiaxi"
      },
      {
        "surname": "Nie",
        "given_name": "Feiping"
      },
      {
        "surname": "Wang",
        "given_name": "Rong"
      },
      {
        "surname": "Jia",
        "given_name": "Yanyan"
      },
      {
        "surname": "Liu",
        "given_name": "Shichang"
      }
    ]
  },
  {
    "title": "Robust stability of Boolean networks with data loss and disturbance inputs",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106504",
    "abstract": "This study discusses the robust stability problem of Boolean networks (BNs) with data loss and disturbances, where data loss is appropriately described by random Bernoulli distribution sequences. Firstly, a BN with data loss and disturbances is converted into an algebraic form via the semi-tensor product (STP) technique. Accordingly, the original system is constructed as a probabilistic augmented system, based on which the problem of stability with probability one for the original system becomes a set stability with probability one for the augmented system. Subsequently, certain criteria are proposed for the robust stability of the systems. Moreover, an algorithm is developed to verify the robust set stability of the augmented system based on truth matrices. Finally, the validity of the obtained results is demonstrated by an illustrative example.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004283",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Boolean function",
      "Boolean network",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Disturbance (geology)",
      "Machine learning",
      "Mathematics",
      "Paleontology",
      "Stability (learning theory)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xiao"
      },
      {
        "surname": "Xia",
        "given_name": "Jianwei"
      },
      {
        "surname": "Feng",
        "given_name": "Jun-e"
      },
      {
        "surname": "Fu",
        "given_name": "Shihua"
      }
    ]
  },
  {
    "title": "GCReID: Generalized continual person re-identification via meta learning and knowledge accumulation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106561",
    "abstract": "Person re-identification (ReID) has made good progress in stationary domains. The ReID model must be retrained to adapt to new scenarios (domains) as they emerge unexpectedly, which leads to catastrophic forgetting. Continual learning trains the model in the order of domain emergence to alleviate catastrophic forgetting. However, generalization ability of the model is still limited due to the distribution difference between training and testing domains. To address the above problem, we propose the generalized continual person re-Identification (GCReID) model to continuously train an anti-forgetting and generalizable model. We endeavor to increase the diversity of samples by prior to simulate unseen domains. Meta-train and meta-test are adopted to enhance generalization of the model. Universal knowledge extracted from all seen domains and the simulated domains is stored in a set of feature embeddings. The knowledge is continually updated and applied to guide meta-train and meta-test via a graph attention network. Extensive experiments on 12 benchmark datasets and comparisons with 6 representative models demonstrate the effectiveness of the proposed model GCReID in enhancing generalization performance on unseen domains and alleviating catastrophic forgetting of seen domains. The code will be available at https://github.com/DFLAG-NEU/GCReID if our work is accepted.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004854",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Botany",
      "Cartography",
      "Computer science",
      "Domain knowledge",
      "Economics",
      "Feature (linguistics)",
      "Forgetting",
      "Generalization",
      "Geodesy",
      "Geography",
      "Identification (biology)",
      "Linguistics",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Meta learning (computer science)",
      "Philosophy",
      "Programming language",
      "Set (abstract data type)",
      "Task (project management)",
      "Train"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Zhaoshuo"
      },
      {
        "surname": "Feng",
        "given_name": "Chaolu"
      },
      {
        "surname": "Yu",
        "given_name": "Kun"
      },
      {
        "surname": "Hu",
        "given_name": "Jun"
      },
      {
        "surname": "Yang",
        "given_name": "Jinzhu"
      }
    ]
  },
  {
    "title": "Randomized algorithms for large-scale dictionary learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106628",
    "abstract": "Dictionary learning is an important sparse representation algorithm which has been widely used in machine learning and artificial intelligence. However, for massive data in the big data era, classical dictionary learning algorithms are computationally expensive and even can be infeasible. To overcome this difficulty, we propose new dictionary learning methods based on randomized algorithms. The contributions of this work are as follows. First, we find that dictionary matrix is often numerically low-rank. Based on this property, we apply randomized singular value decomposition (RSVD) to the dictionary matrix, and propose a randomized algorithm for linear dictionary learning. Compared with the classical K-SVD algorithm, an advantage is that one can update all the elements of the dictionary matrix simultaneously. Second, to the best of our knowledge, there are few theoretical results on why one can solve the involved matrix computation problems inexactly in dictionary learning. To fill-in this gap, we show the rationality of this randomized algorithm with inexact solving, from a matrix perturbation analysis point of view. Third, based on the numerically low-rank property and Nyström approximation of the kernel matrix, we propose a randomized kernel dictionary learning algorithm, and establish the distance between the exact solution and the computed solution, to show the effectiveness of the proposed randomized kernel dictionary learning algorithm. Fourth, we propose an efficient scheme for the testing stage in kernel dictionary learning. By using this strategy, there is no need to form nor store kernel matrices explicitly both in the training and the testing stages. Comprehensive numerical experiments are performed on some real-world data sets. Numerical results demonstrate the rationality of our strategies, and show that the proposed algorithms are much efficient than some state-of-the-art dictionary learning algorithms. The MATLAB codes of the proposed algorithms are publicly available from https://github.com/Jiali-yang/RALDL_RAKDL.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024005525",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Composite material",
      "Computer science",
      "K-SVD",
      "Kernel (algebra)",
      "Materials science",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Singular value decomposition",
      "Sparse approximation"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Gang"
      },
      {
        "surname": "Yang",
        "given_name": "Jiali"
      }
    ]
  },
  {
    "title": "PLEASING: Exploring the historical and potential events for temporal knowledge graph reasoning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106516",
    "abstract": "Temporal Knowledge Graphs (TKGs) enable effective modeling of knowledge dynamics and event evolution, facilitating deeper insights and analysis into temporal information. Recently, extrapolation of TKG reasoning has attracted great significance due to its remarkable ability to capture historical correlations and predict future events. Existing studies of extrapolation aim mainly at encoding the structural and temporal semantics based on snapshot sequences, which contain graph aggregators for the association within snapshots and recurrent units for the evolution. However, these methods are limited to modeling long-distance history, as they primarily focus on capturing temporal correlations over shorter periods. Besides, a few approaches rely on compiling historical repetitive statistics of TKGs for predicting future facts. But they often overlook explicit interactions in the graph structure among concurrent events. To address these issues, we propose a PotentiaL concurrEnt Aggregation and contraStive learnING (PLEASING) method for TKG extrapolation. PLEASING is a two-step reasoning framework that effectively leverages the historical and potential features of TKGs. It includes two encoders for historical and global events with an adaptive gated mechanism, acquiring predictions with appropriate weight of the two aspects. Specifically, PLEASING constructs two auxiliary graphs to capture temporal interaction among timestamps and correlations among potential concurrent events, respectively, enabling a holistic investigation of temporal characteristics and future potential possibilities in TKGs. Furthermore, PLEASING incorporates contrastive learning to strengthen its capacity to identify whether queries are related to history. Extensive experiments on seven benchmark datasets demonstrate the state-of-the-art performances of PLEASING and its comprehensive ability to model TKG semantics.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004404",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Extrapolation",
      "Graph",
      "Knowledge graph",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Programming language",
      "Semantics (computer science)",
      "Snapshot (computer storage)",
      "Theoretical computer science",
      "Timestamp"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Jinchuan"
      },
      {
        "surname": "Sun",
        "given_name": "Ming"
      },
      {
        "surname": "Huang",
        "given_name": "Qian"
      },
      {
        "surname": "Tian",
        "given_name": "Ling"
      }
    ]
  },
  {
    "title": "Learning feature relationships in CNN model via relational embedding convolution layer",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106510",
    "abstract": "Establishing the relationships among hierarchical visual attributes of objects in the visual world is crucial for human cognition. The classic convolution neural network (CNN) can successfully extract hierarchical features but ignore the relationships among features, resulting in shortcomings compared to humans in areas like interpretability and domain generalization. Recently, algorithms have introduced feature relationships by external prior knowledge and special auxiliary modules, which have been proven to bring multiple improvements in many computer vision tasks. However, prior knowledge is often difficult to obtain, and auxiliary modules bring additional consumption of computing and storage resources, which limits the flexibility and practicality of the algorithm. In this paper, we aim to drive the CNN model to learn the relationships among hierarchical deep features without prior knowledge and consumption increasing, while enhancing the fundamental performance of some aspects. Firstly, the task of learning the relationships among hierarchical features in CNN is defined and three key problems related to this task are pointed out, including the quantitative metric of connection intensity, the threshold of useless connections, and the updating strategy of relation graph. Secondly, Relational Embedding Convolution (RE-Conv) layer is proposed for the representation of feature relationships in convolution layer, followed by a scheme called use & disuse strategy which aims to address the three problems of feature relation learning. Finally, the improvements brought by the proposed feature relation learning scheme have been demonstrated through numerous experiments, including interpretability, domain generalization, noise robustness, and inference efficiency. In particular, the proposed scheme outperforms many state-of-the-art methods in the domain generalization community and can be seamlessly integrated with existing methods for further improvement. Meanwhile, it maintains comparable precision to the original CNN model while reducing floating point operations (FLOPs) by approximately 50%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004349",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Data mining",
      "Domain knowledge",
      "Embedding",
      "Feature (linguistics)",
      "Feature learning",
      "Generalization",
      "Inference",
      "Interpretability",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Relation (database)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Xiong",
        "given_name": "Shengzhou"
      },
      {
        "surname": "Tan",
        "given_name": "Yihua"
      },
      {
        "surname": "Wang",
        "given_name": "Guoyou"
      },
      {
        "surname": "Yan",
        "given_name": "Pei"
      },
      {
        "surname": "Xiang",
        "given_name": "Xuanyu"
      }
    ]
  },
  {
    "title": "When an extra rejection class meets out-of-distribution detection in long-tailed image classification",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106485",
    "abstract": "Detecting Out-of-Distribution (OOD) inputs is essential for reliable deep learning in the open world. However, most existing OOD detection methods have been developed based on training sets that exhibit balanced class distributions, making them susceptible when confronted with training sets following a long-tailed distribution. To alleviate this problem, we propose an effective three-branch training framework, which demonstrates the efficacy of incorporating an extra rejection class along with auxiliary outlier training data for effective OOD detection in long-tailed image classification. In our proposed framework, all outlier training samples are assigned the label of the rejection class. We employ an inlier loss, an outlier loss, and a Tail-class prototype induced Supervised Contrastive Loss (TSCL) to train both the in-distribution classifier and OOD detector within one network. During inference, the OOD detector is constructed using the rejection class. Extensive experimental results demonstrate that the superior OOD detection performance of our proposed method in long-tailed image classification. For example, in the more challenging case where CIFAR100-LT is used as in-distribution, our method improves the average AUROC by 1.23% and reduces the average FPR95 by 3.18% compared to the baseline method utilizing Outlier Exposure (OE). Code is available at github.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400409X",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Contextual image classification",
      "Detector",
      "Image (mathematics)",
      "Inference",
      "Machine learning",
      "One-class classification",
      "Outlier",
      "Pattern recognition (psychology)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Feng",
        "given_name": "Shuai"
      },
      {
        "surname": "Wang",
        "given_name": "Chongjun"
      }
    ]
  },
  {
    "title": "Leveraging temporal dependency for cross-subject-MI BCIs by contrastive learning and self-attention",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106470",
    "abstract": "Brain-computer interfaces (BCIs) built based on motor imagery paradigm have found extensive utilization in motor rehabilitation and the control of assistive applications. However, traditional MI-BCI systems often exhibit suboptimal classification performance and require significant time for new users to collect subject-specific training data. This limitation diminishes the user-friendliness of BCIs and presents significant challenges in developing effective subject-independent models. In response to these challenges, we propose a novel subject-independent framework for learning temporal dependency for motor imagery BCIs by Contrastive Learning and Self-attention (CLS). In CLS model, we incorporate self-attention mechanism and supervised contrastive learning into a deep neural network to extract important information from electroencephalography (EEG) signals as features. We evaluate the CLS model using two large public datasets encompassing numerous subjects in a subject-independent experiment condition. The results demonstrate that CLS outperforms six baseline algorithms, achieving a mean classification accuracy improvement of 1.3 % and 4.71 % than the best algorithm on the Giga dataset and OpenBMI dataset, respectively. Our findings demonstrate that CLS can effectively learn invariant discriminative features from training data obtained from non-target subjects, thus showcasing its potential for building models for new users without the need for calibration.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003940",
    "keywords": [
      "Artificial intelligence",
      "Brain–computer interface",
      "Computer science",
      "Dependency (UML)",
      "Electroencephalography",
      "Library science",
      "Machine learning",
      "Natural language processing",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Psychology",
      "Speech recognition",
      "Subject (documents)"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Hao"
      },
      {
        "surname": "Ding",
        "given_name": "Yi"
      },
      {
        "surname": "Bao",
        "given_name": "Jianzhu"
      },
      {
        "surname": "Qin",
        "given_name": "Ke"
      },
      {
        "surname": "Tong",
        "given_name": "Chengxuan"
      },
      {
        "surname": "Jin",
        "given_name": "Jing"
      },
      {
        "surname": "Guan",
        "given_name": "Cuntai"
      }
    ]
  },
  {
    "title": "GRAM: An interpretable approach for graph anomaly detection using gradient attention maps",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106463",
    "abstract": "Detecting unusual patterns in graph data is a crucial task in data mining. However, existing methods face challenges in consistently achieving satisfactory performance and often lack interpretability, which hinders our understanding of anomaly detection decisions. In this paper, we propose a novel approach to graph anomaly detection that leverages the power of interpretability to enhance performance. Specifically, our method extracts an attention map derived from gradients of graph neural networks, which serves as a basis for scoring anomalies. Notably, our approach is flexible and can be used in various anomaly detection settings. In addition, we conduct theoretical analysis using synthetic data to validate our method and gain insights into its decision-making process. To demonstrate the effectiveness of our method, we extensively evaluate our approach against state-of-the-art graph anomaly detection techniques on real-world graph classification and wireless network datasets. The results consistently demonstrate the superior performance of our method compared to the baselines.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003873",
    "keywords": [
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Computer science",
      "Condensed matter physics",
      "Graph",
      "Pattern recognition (psychology)",
      "Physics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Yifei"
      },
      {
        "surname": "Wang",
        "given_name": "Peng"
      },
      {
        "surname": "He",
        "given_name": "Xiaofan"
      },
      {
        "surname": "Zou",
        "given_name": "Dongmian"
      }
    ]
  },
  {
    "title": "Federated learning using model projection for multi-center disease diagnosis with non-IID data",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106409",
    "abstract": "Multi-center disease diagnosis aims to build a global model for all involved medical centers. Due to privacy concerns, it is infeasible to collect data from multiple centers for training (i.e., centralized learning). Federated Learning (FL) is a decentralized framework that enables multiple clients (e.g., medical centers) to collaboratively train a global model while retaining patient data locally for privacy. However, in practice, the data across medical centers are not independently and identically distributed (Non-IID), causing two challenging issues: (1) catastrophic forgetting at clients, i.e., the local model at clients will forget the knowledge received from the global model after local training, causing reduced performance; and (2) invalid aggregation at the server, i.e., the global model at the server may not be favorable to some clients after model aggregation, resulting in a slow convergence rate. To mitigate these issues, an innovative Federated learning using Model Projection (FedMoP) is proposed, which guarantees: (1) the loss of local model on global data does not increase after local training without accessing the global data so that the performance will not be degenerated; and (2) the loss of global model on local data does not increase after aggregation without accessing local data so that convergence rate can be improved. Extensive experimental results show that our FedMoP outperforms state-of-the-art FL methods in terms of accuracy, convergence rate and communication cost. In particular, our FedMoP also achieves comparable or even higher accuracy than centralized learning. Thus, our FedMoP can ensure privacy protection while outperforming centralized learning in accuracy and communication cost.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003332",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Center (category theory)",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Crystallography",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Projection (relational algebra)"
    ],
    "authors": [
      {
        "surname": "Du",
        "given_name": "Jie"
      },
      {
        "surname": "Li",
        "given_name": "Wei"
      },
      {
        "surname": "Liu",
        "given_name": "Peng"
      },
      {
        "surname": "Vong",
        "given_name": "Chi-Man"
      },
      {
        "surname": "You",
        "given_name": "Yongke"
      },
      {
        "surname": "Lei",
        "given_name": "Baiying"
      },
      {
        "surname": "Wang",
        "given_name": "Tianfu"
      }
    ]
  },
  {
    "title": "Clothing-invariant contrastive learning for unsupervised person re-identification",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106477",
    "abstract": "Clothing change person re-identification (CC-ReID) aims to match images of the same person wearing different clothes across diverse scenes. Leveraging biological features or clothing labels, existing CC-ReID methods have demonstrated promising performance. However, current research primarily focuses on supervised CC-ReID methods, which require a substantial number of manually annotated labels. To tackle this challenge, we propose a novel clothing-invariant contrastive learning (CICL) framework for unsupervised CC-ReID task. Firstly, to obtain clothing change positive pairs at a low computational cost, we propose a random clothing augmentation (RCA) method. RCA initially partitions clothing regions based on parsing images, then applies random augmentation to different clothing regions, ultimately generating clothing change positive pairs to facilitate clothing-invariant learning. Secondly, to generate pseudo-labels strongly correlated with identity in an unsupervised manner, we design semantic fusion clustering (SFC), which enhances identity-related information through semantic fusion. Additionally, we develop a semantic alignment contrastive loss (SAC loss) to encourages the model to learn features strongly correlated with identity and enhances the model’s robustness to clothing changes. Unlike existing optimization methods that forcibly bring closer clusters with different pseudo-labels, SAC loss aligns the clustering results of real image features with those generated by SFC, forming a mutually reinforcing scheme with SFC. Experimental results on multiple CC-ReID datasets demonstrate that the proposed CICL not only outperforms existing unsupervised methods but can even achieves competitive performance with supervised CC-ReID methods. Code is made available at https://github.com/zqpang/CICL.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004015",
    "keywords": [
      "Archaeology",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Biology",
      "Botany",
      "Chemistry",
      "Clothing",
      "Cluster analysis",
      "Code (set theory)",
      "Computer science",
      "Gene",
      "History",
      "Identification (biology)",
      "Invariant (physics)",
      "Machine learning",
      "Mathematical physics",
      "Mathematics",
      "Natural language processing",
      "Parsing",
      "Pattern recognition (psychology)",
      "Programming language",
      "Robustness (evolution)",
      "Set (abstract data type)",
      "Supervised learning",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Pang",
        "given_name": "Zhiqi"
      },
      {
        "surname": "Zhao",
        "given_name": "Lingling"
      },
      {
        "surname": "Wang",
        "given_name": "Chunyu"
      }
    ]
  },
  {
    "title": "On convergence properties of the brain-state-in-a-convex-domain",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106481",
    "abstract": "Convergence in the presence of multiple equilibrium points is one of the most fundamental dynamical properties of a neural network (NN). Goal of the paper is to investigate convergence for the classic Brain-State-in-a-Box (BSB) NN model and some of its relevant generalizations named Brain-State-in-a-Convex-Body (BSCB). In particular, BSCB is a class of discrete-time NNs obtained by projecting a linear system onto a convex body of R n . The main result in the paper is that the BSCB is convergent when the matrix of the linear system is symmetric and positive semidefinite or, otherwise, it is symmetric and the step size does not exceed a given bound depending only on the minimum eigenvalue of the matrix. This result generalizes previous results in the literature for BSB and BSCB and it gives a solid foundation for the use of BSCB as a content addressable memory (CAM). The result is proved via Lyapunov method and LaSalle’s Invariance Principle for discrete-time systems and by using some fundamental inequalities enjoyed by the projection operator onto convex sets as Bourbaki–Cheney–Goldstein inequality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004052",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Convergence (economics)",
      "Convex combination",
      "Convex optimization",
      "Domain (mathematical analysis)",
      "Economic growth",
      "Economics",
      "Effective domain",
      "Geometry",
      "Mathematical analysis",
      "Mathematics",
      "Regular polygon",
      "State (computer science)"
    ],
    "authors": [
      {
        "surname": "Di Marco",
        "given_name": "Mauro"
      },
      {
        "surname": "Forti",
        "given_name": "Mauro"
      },
      {
        "surname": "Pancioni",
        "given_name": "Luca"
      },
      {
        "surname": "Tesi",
        "given_name": "Alberto"
      }
    ]
  },
  {
    "title": "Motion-Aware Video Frame Interpolation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106433",
    "abstract": "Video frame interpolation methodologies endeavor to create novel frames betwixt extant ones, with the intent of augmenting the video’s frame frequency. However, current methods are prone to image blurring and spurious artifacts in challenging scenarios involving occlusions and discontinuous motion. Moreover, they typically rely on optical flow estimation, which adds complexity to modeling and computational costs. To address these issues, we introduce a Motion-Aware Video Frame Interpolation (MA-VFI) network, which directly estimates intermediate optical flow from consecutive frames by introducing a novel hierarchical pyramid module. It not only extracts global semantic relationships and spatial details from input frames with different receptive fields, enabling the model to capture intricate motion patterns, but also effectively reduces the required computational cost and complexity. Subsequently, a cross-scale motion structure is presented to estimate and refine intermediate flow maps by the extracted features. This approach facilitates the interplay between input frame features and flow maps during the frame interpolation process and markedly heightens the precision of the intervening flow delineations. Finally, a discerningly fashioned loss centered around an intermediate flow is meticulously contrived, serving as a deft rudder to skillfully guide the prognostication of said intermediate flow, thereby substantially refining the precision of the intervening flow mappings. Experiments illustrate that MA-VFI surpasses several representative VFI methods across various datasets, and can enhance efficiency while maintaining commendable efficacy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003575",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Block-matching algorithm",
      "Computer science",
      "Computer vision",
      "Flow (mathematics)",
      "Frame (networking)",
      "Geometry",
      "Image (mathematics)",
      "Interpolation (computer graphics)",
      "Mathematics",
      "Motion (physics)",
      "Motion estimation",
      "Motion interpolation",
      "Optical flow",
      "Pyramid (geometry)",
      "Telecommunications",
      "Video processing",
      "Video tracking"
    ],
    "authors": [
      {
        "surname": "Han",
        "given_name": "Pengfei"
      },
      {
        "surname": "Zhang",
        "given_name": "Fuhua"
      },
      {
        "surname": "Zhao",
        "given_name": "Bin"
      },
      {
        "surname": "Li",
        "given_name": "Xuelong"
      }
    ]
  },
  {
    "title": "Multi-grained visual pivot-guided multi-modal neural machine translation with text-aware cross-modal contrastive disentangling",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106403",
    "abstract": "The goal of multi-modal neural machine translation (MNMT) is to incorporate language-agnostic visual information into text to enhance the performance of machine translation. However, due to the inherent differences between image and text, these two modalities inevitably suffer from semantic mismatch problems. To tackle this issue, this paper adopts a multi-grained visual pivot-guided multi-modal fusion strategy with cross-modal contrastive disentangling to eliminate the linguistic gaps between different languages. By using the disentangled multi-grained visual information as a cross-lingual pivot, we can enhance the alignment between different languages and improve the performance of MNMT. We first introduce text-guided stacked cross-modal disentangling modules to progressively disentangle image into two types of visual information: MT-related visual and background information. Then we effectively integrate these two kinds of multi-grained visual elements to assist target sentence generation. Extensive experiments on four benchmark MNMT datasets are conducted, and the results demonstrate that our proposed approach achieves significant improvement over the other state-of-the-art (SOTA) approaches on all test sets. The in-depth analysis highlights the benefits of text-guided cross-modal disentangling and visual pivot-based multi-modal fusion strategies in MNMT. We release the code at https://github.com/nlp-mnmt/ConVisPiv-MNMT.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003277",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Gene",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Machine translation",
      "Messenger RNA",
      "Modal",
      "Natural language processing",
      "Polymer chemistry",
      "Sentence",
      "Translation (biology)"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Junjun"
      },
      {
        "surname": "Su",
        "given_name": "Rui"
      },
      {
        "surname": "Ye",
        "given_name": "Junjie"
      }
    ]
  },
  {
    "title": "Meta-learning based blind image super-resolution approach to different degradations",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106429",
    "abstract": "Although recent studies on blind single image super-resolution (SISR) have achieved significant success, most of them typically require supervised training on synthetic low resolution (LR)-high resolution (HR) paired images. This leads to re-training necessity for different degradations and restricted applications in real-world scenarios with unfavorable inputs. In this paper, we propose an unsupervised blind SISR method with input underlying different degradations, named different degradations blind super-resolution (DDSR). It formulates a Gaussian modeling on blur degradation and employs a meta-learning framework for solving different image degradations. Specifically, a neural network-based kernel generator is optimized by learning from random kernel samples, referred to as random kernel learning. This operation provides effective initialization for blur degradation optimization. At the same time, a meta-learning framework is proposed to resolve multiple degradation modelings on the basis of alternative optimization between blur degradation and image restoration, respectively. Differing from the pre-trained deep-learning methods, the proposed DDSR is implemented in a plug-and-play manner, and is capable of restoring HR image from unfavorable LR input with degradations such as partial coverage, noise addition, and darkening. Extensive simulations illustrate the superior performance of the proposed DDSR approach compared to the state-of-the-arts on public datasets with comparable memory load and time consumption, yet exhibiting better application flexibility and convenience, and significantly better generalization ability towards multiple degradations. Our code is available at https://github.com/XYLGroup/DDSR.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003538",
    "keywords": [
      "Artificial intelligence",
      "Code (set theory)",
      "Combinatorics",
      "Computer science",
      "Degradation (telecommunications)",
      "Flexibility (engineering)",
      "Generalization",
      "Image (mathematics)",
      "Initialization",
      "Kernel (algebra)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Zhixiong"
      },
      {
        "surname": "Xia",
        "given_name": "Jingyuan"
      },
      {
        "surname": "Li",
        "given_name": "Shengxi"
      },
      {
        "surname": "Liu",
        "given_name": "Wende"
      },
      {
        "surname": "Zhi",
        "given_name": "Shuaifeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Shuanghui"
      },
      {
        "surname": "Liu",
        "given_name": "Li"
      },
      {
        "surname": "Fu",
        "given_name": "Yaowen"
      },
      {
        "surname": "Gündüz",
        "given_name": "Deniz"
      }
    ]
  },
  {
    "title": "UGEE-Net: Uncertainty-guided and edge-enhanced network for image splicing localization",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106430",
    "abstract": "Image splicing, a prevalent method for image tampering, has significantly undermined image authenticity. Existing methods for Image Splicing Localization (ISL) struggle with challenges like limited accuracy and subpar performance when dealing with imperceptible tampering and multiple tampered regions. We introduce an Uncertainty-Guided and Edge-Enhanced Network (UGEE-Net) for ISL to tackle these issues. UGEE-Net consists of two core tasks: uncertainty guidance and edge enhancement. We employ Bayesian learning to model uncertainty maps of tampered regions, directing the model's focus to challenging pixels. Simultaneously, we employ a frequency domain-auxiliary edge enhancement strategy to imbue localization features with global contour information and fine-grained local details. These mechanisms work in parallel, synergistically boosting performance. Additionally, we introduce a cross-level fusion and propagation mechanism that effectively utilizes contextual information for cross-layer feature integration and leverages channel-level correlations for cross-layer feature propagation, gradually enhancing the localization feature's details. Experiment results affirm UGEE-Net's superiority in terms of detection accuracy, robustness, and generalization capabilities. Furthermore, to meet the growing demand for high-quality datasets in image forensics, we present the HTSI12K dataset, which includes 12,000 spliced images with imperceptible tampering traces and diverse categories, rendering it suitable for real-world auxiliary model training.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400354X",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Boosting (machine learning)",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Enhanced Data Rates for GSM Evolution",
      "Feature (linguistics)",
      "Gene",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Rendering (computer graphics)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Hao",
        "given_name": "Qixian"
      },
      {
        "surname": "Ren",
        "given_name": "Ruyong"
      },
      {
        "surname": "Niu",
        "given_name": "Shaozhang"
      },
      {
        "surname": "Wang",
        "given_name": "Kai"
      },
      {
        "surname": "Wang",
        "given_name": "Maosen"
      },
      {
        "surname": "Zhang",
        "given_name": "Jiwei"
      }
    ]
  },
  {
    "title": "Sequence homology score-based deep fuzzy network for identifying therapeutic peptides",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106458",
    "abstract": "The detection of therapeutic peptides is a topic of immense interest in the biomedical field. Conventional biochemical experiment-based detection techniques are tedious and time-consuming. Computational biology has become a useful tool for improving the detection efficiency of therapeutic peptides. Most computational methods do not consider the deviation caused by noise. To improve the generalization performance of therapeutic peptide prediction methods, this work presents a sequence homology score-based deep fuzzy echo-state network with maximizing mixture correntropy (SHS-DFESN-MMC) model. Our method is compared with the existing methods on eight types of therapeutic peptide datasets. The model parameters are determined by 10 fold cross-validation on their training sets and verified by independent test sets. Across the 8 datasets, the average area under the receiver operating characteristic curve (AUC) values of SHS-DFESN-MMC are the highest on both the training (0.926) and independent sets (0.923).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003824",
    "keywords": [
      "Area under curve",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Bioinformatics",
      "Biology",
      "Computer science",
      "Data mining",
      "Fuzzy logic",
      "Gene",
      "Generalization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Peptide sequence",
      "Pharmacokinetics",
      "Receiver operating characteristic",
      "Sequence homology"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Xiaoyi"
      },
      {
        "surname": "Zheng",
        "given_name": "Ziyu"
      },
      {
        "surname": "Cheong",
        "given_name": "Kang Hao"
      },
      {
        "surname": "Zou",
        "given_name": "Quan"
      },
      {
        "surname": "Tiwari",
        "given_name": "Prayag"
      },
      {
        "surname": "Ding",
        "given_name": "Yijie"
      }
    ]
  },
  {
    "title": "KEMoS: A knowledge-enhanced multi-modal summarizing framework for Chinese online meetings",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106417",
    "abstract": "The demand for “online meetings” and “collaborative office work” keeps surging recently, producing an abundant amount of relevant data. How to provide participants with accurate and fast summarizing service has attracted extensive attention. Existing meeting summarizing models overlook the utilization of multi-modal information and the information offsetting during summarizing. In this paper, we develop a knowledge-enhanced multi-modal summarizing framework. Firstly, we construct a three-layer multi-modal meeting knowledge graph, including basic, knowledge, and multi-modal layer, to integrate meeting information thoroughly. Then, we raise a topic-based hierarchical clustering approach, which considers information entropy and difference simultaneously, to capture the semantic evolution of meetings. Next, we devise a multi-modal enhanced encoding strategy, including a sentence-level cross-modal encoder, a joint loss function, and a knowledge graph embedding module, to learn the meeting and topic-level presentations. Finally, when generating summaries, we design a topic-enhanced decoding strategy for the Transformer decoder which mitigates semantic offsetting with the aid of topic information. Extensive experiments show that our proposed work consistently outperforms state-of-the-art solutions on the Chinese meeting dataset, where the ROUGE-1, ROUGE-2, and ROUGE-L are 49.98%, 21.03%, and 32.03% respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003411",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Encoder",
      "Graph",
      "Information retrieval",
      "Knowledge graph",
      "Modal",
      "Operating system",
      "Polymer chemistry",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Qi",
        "given_name": "Peng"
      },
      {
        "surname": "Sun",
        "given_name": "Yan"
      },
      {
        "surname": "Yao",
        "given_name": "Muyan"
      },
      {
        "surname": "Tao",
        "given_name": "Dan"
      }
    ]
  },
  {
    "title": "STaRNet: A spatio-temporal and Riemannian network for high-performance motor imagery decoding",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106471",
    "abstract": "Brain–computer interfaces (BCIs), representing a transformative form of human–computer interaction, empower users to interact directly with external environments through brain signals. In response to the demands for high accuracy, robustness, and end-to-end capabilities within BCIs based on motor imagery (MI), this paper introduces STaRNet, a novel model that integrates multi-scale spatio-temporal convolutional neural networks (CNNs) with Riemannian geometry. Initially, STaRNet integrates a multi-scale spatio-temporal feature extraction module that captures both global and local features, facilitating the construction of Riemannian manifolds from these comprehensive spatio-temporal features. Subsequently, a matrix logarithm operation transforms the manifold-based features into the tangent space, followed by a dense layer for classification. Without preprocessing, STaRNet surpasses state-of-the-art (SOTA) models by achieving an average decoding accuracy of 83.29% and a kappa value of 0.777 on the BCI Competition IV 2a dataset, and 95.45% accuracy with a kappa value of 0.939 on the High Gamma Dataset. Additionally, a comparative analysis between STaRNet and several SOTA models, focusing on the most challenging subjects from both datasets, highlights exceptional robustness of STaRNet. Finally, the visualizations of learned frequency bands demonstrate that temporal convolutions have learned MI-related frequency bands, and the t-SNE analyses of features across multiple layers of STaRNet exhibit strong feature extraction capabilities. We believe that the accurate, robust, and end-to-end capabilities of the STaRNet will facilitate the advancement of BCIs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003952",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Brain–computer interface",
      "Chemistry",
      "Computer science",
      "Convolutional neural network",
      "Decoding methods",
      "Electroencephalography",
      "Feature extraction",
      "Gene",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Preprocessor",
      "Psychiatry",
      "Psychology",
      "Riemannian manifold",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xingfu"
      },
      {
        "surname": "Yang",
        "given_name": "Wenjie"
      },
      {
        "surname": "Qi",
        "given_name": "Wenxia"
      },
      {
        "surname": "Wang",
        "given_name": "Yu"
      },
      {
        "surname": "Ma",
        "given_name": "Xiaojun"
      },
      {
        "surname": "Wang",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Adaptive sampling artificial-actual control for non-zero-sum games of constrained systems",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106413",
    "abstract": "Considering physical constraints encountered by actuators, this paper addresses the non-zero-sum game of continuous nonlinear systems with symmetric and asymmetric input constraints through aperiodic sampling artificial-actual control. Initially, the artificial system built by the improved Elman dynamic neural networks (EDNNs) has artificial-actual interaction with the physical system, which provides a new perspective for predicting the system state. By constantly learning and adjusting parameters, EDNNs can gradually approximate the dynamic behavior of the real system to achieve more effective control. Aiming at accommodating diverse input constraints, the non-quadratic value function constructed from a smoothly bounded function is devised. Then, the polynomial parameterized adaptive dynamic programming (ADP) is employed to approximate the solution of the coupled Hamilton–Jacobi equation (HJE), deriving optimal control laws for two players. To improve the efficiency of data communication, three adaptive sampling mechanisms including event-triggered mechanism (ETM) with relative threshold, dynamic ETM (DETM) and self-triggered mechanism (STM) are introduced in turn during the iterative learning process of control sequences. DETM further extends sampling intervals by incorporating internal dynamic variables, while STM determines the next trigger time through soft calculation without hardware monitoring. All three trigger modes can ensure the system stability while avoiding the Zeno phenomenon, and relevant proofs are given. Finally, the simulation validates the effectiveness of the designed algorithm and highlights the unique characteristics of each trigger mode.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400337X",
    "keywords": [
      "Adaptive sampling",
      "Aperiodic graph",
      "Artificial intelligence",
      "Artificial neural network",
      "Bellman equation",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Control (management)",
      "Control theory (sociology)",
      "Filter (signal processing)",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Monte Carlo method",
      "Sampling (signal processing)",
      "Stability (learning theory)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Lu"
      },
      {
        "surname": "Song",
        "given_name": "Ruizhuo"
      }
    ]
  },
  {
    "title": "Unsupervised domain adaptation with weak source domain labels via bidirectional subdomain alignment",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106418",
    "abstract": "Unsupervised domain adaptation (UDA) enables knowledge transfer from a labeled source domain to an unlabeled target domain. However, UDA performance often relies heavily on the accuracy of source domain labels, which are frequently noisy or missing in real applications. To address unreliable source labels, we propose a novel framework for extracting robust, discriminative features via iterative pseudo-labeling, queue-based clustering, and bidirectional subdomain alignment (BSA). The proposed framework begins by generating pseudo-labels for unlabeled source data and constructing codebooks via iterative clustering to obtain label-independent class centroids. Then, the proposed framework performs two main tasks: rectifying features from both domains using BSA to match subdomain distributions and enhance features; and employing a two-stage adversarial process for global feature alignment. The feature rectification is done before feature enhancement, while the global alignment is done after feature enhancement. To optimize our framework, we formulate BSA and adversarial learning as maximizing a log-likelihood function, which is implemented via the Expectation–Maximization algorithm. The proposed framework shows significant improvements compared to state-of-the-art methods on Office-31, Office-Home, and VisDA-2017 datasets, achieving average accuracies of 91.5%, 76.6%, and 87.4%, respectively. Compared to existing methods, the proposed method shows consistent superiority in unsupervised domain adaptation tasks with both fully and weakly labeled source domains.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003423",
    "keywords": [
      "Artificial intelligence",
      "Centroid",
      "Cluster analysis",
      "Computer science",
      "Discriminative model",
      "Domain (mathematical analysis)",
      "Feature (linguistics)",
      "Feature learning",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Heng"
      },
      {
        "surname": "Zhong",
        "given_name": "Ping"
      },
      {
        "surname": "Li",
        "given_name": "Daoliang"
      },
      {
        "surname": "Shen",
        "given_name": "Zhencai"
      }
    ]
  },
  {
    "title": "A robust event-driven approach to always-on object recognition",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106415",
    "abstract": "We propose a neuromimetic architecture capable of always-on pattern recognition, i.e. at any time during processing. To achieve this, we have extended an existing event-based algorithm (Lagorce et al., 2017), which introduced novel spatio-temporal features as a Hierarchy Of Time-Surfaces (HOTS). Built from asynchronous events captured by a neuromorphic camera, these time surfaces allow to encode the local dynamics of a visual scene and to create an efficient event-based pattern recognition architecture. Inspired by neuroscience, we have extended this method to improve its performance. First, we add a homeostatic gain control on the activity of neurons to improve the learning of spatio-temporal patterns (Grimaldi et al., 2021). We also provide a new mathematical formalism that allows an analogy to be drawn between the HOTS algorithm and Spiking Neural Networks (SNN). Following this analogy, we transform the offline pattern categorization method into an online and event-driven layer. This classifier uses the spiking output of the network to define new time surfaces and we then perform the online classification with a neuromimetic implementation of a multinomial logistic regression. These improvements not only consistently increase the performance of the network, but also bring this event-driven pattern recognition algorithm fully online. The results have been validated on different datasets: Poker-DVS (Serrano-Gotarredona and Linares-Barranco, 2015), N-MNIST (Orchard, Jayawant et al., 2015) and DVS Gesture (Amir et al., 2017). This demonstrates the efficiency of this bio-realistic SNN for ultra-fast object recognition through an event-by-event categorization process.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003393",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Asynchronous communication",
      "Categorization",
      "Classifier (UML)",
      "Cognitive neuroscience of visual object recognition",
      "Computer network",
      "Computer science",
      "Event (particle physics)",
      "MNIST database",
      "Machine learning",
      "Neuromorphic engineering",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Spiking neural network"
    ],
    "authors": [
      {
        "surname": "Grimaldi",
        "given_name": "Antoine"
      },
      {
        "surname": "Boutin",
        "given_name": "Victor"
      },
      {
        "surname": "Ieng",
        "given_name": "Sio-Hoi"
      },
      {
        "surname": "Benosman",
        "given_name": "Ryad"
      },
      {
        "surname": "Perrinet",
        "given_name": "Laurent U."
      }
    ]
  },
  {
    "title": "Magnitude and angle dynamics in training single ReLU neurons",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106435",
    "abstract": "Understanding the training dynamics of deep ReLU networks is a significant area of interest in deep learning. However, there remains a lack of complete elucidation regarding the weight vector dynamics, even for single ReLU neurons. To bridge this gap, our study delves into the training dynamics of the gradient flow w ( t ) for single ReLU neurons under the square loss, dissecting it into its magnitude ‖ w ( t ) ‖ and angle φ ( t ) components. Through this decomposition, we establish upper and lower bounds on these components to elucidate the convergence dynamics. Furthermore, we demonstrate the empirical extension of our findings to general two-layer multi-neuron networks. All theoretical results are generalized to the gradient descent method and rigorously verified through experiments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003599",
    "keywords": [
      "Acoustics",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Balanced flow",
      "Computer science",
      "Convergence (economics)",
      "Dynamics (music)",
      "Economic growth",
      "Economics",
      "Extension (predicate logic)",
      "Gradient descent",
      "Mathematical analysis",
      "Mathematics",
      "Physics",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Sangmin"
      },
      {
        "surname": "Sim",
        "given_name": "Byeongsu"
      },
      {
        "surname": "Ye",
        "given_name": "Jong Chul"
      }
    ]
  },
  {
    "title": "Egoism, utilitarianism and egalitarianism in multi-agent reinforcement learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106544",
    "abstract": "In multi-agent partially observable sequential decision problems with general-sum rewards, it is necessary to account for the egoism (individual rewards), utilitarianism (social welfare), and egalitarianism (fairness) criteria simultaneously. However, achieving a balance between these criteria poses a challenge for current multi-agent reinforcement learning methods. Specifically, fully decentralized methods without global information of all agents’ rewards, observations and actions fail to learn a balanced policy, while agents in centralized training (with decentralized execution) methods are reluctant to share private information due to concerns of exploitation by others. To address these issues, this paper proposes a Decentralized and Federated (D&F) paradigm, where decentralized agents train egoistic policies utilizing solely local information to attain self-interest, and the federation controller primarily considers utilitarianism and egalitarianism. Meanwhile, the parameters of decentralized and federated policies are optimized with discrepancy constraints mutually, akin to a server and client pattern, which ensures the balance between egoism, utilitarianism, and egalitarianism. Furthermore, theoretical evidence demonstrates that the federated model, as well as the discrepancy between decentralized egoistic policies and federated utilitarian policies, obtains an O ( 1 / T ) convergence rate. Extensive experiments show that our D&F approach outperforms multiple baselines, in terms of both utilitarianism and egalitarianism.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004684",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Egalitarianism",
      "Epistemology",
      "Ethical egoism",
      "Law",
      "Philosophy",
      "Political science",
      "Politics",
      "Psychology",
      "Reinforcement",
      "Reinforcement learning",
      "Social psychology",
      "Sociology",
      "Utilitarianism"
    ],
    "authors": [
      {
        "surname": "Dong",
        "given_name": "Shaokang"
      },
      {
        "surname": "Li",
        "given_name": "Chao"
      },
      {
        "surname": "Yang",
        "given_name": "Shangdong"
      },
      {
        "surname": "An",
        "given_name": "Bo"
      },
      {
        "surname": "Li",
        "given_name": "Wenbin"
      },
      {
        "surname": "Gao",
        "given_name": "Yang"
      }
    ]
  },
  {
    "title": "Efficient tensor decomposition-based filter pruning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106393",
    "abstract": "In this paper, we present CORING, which is short for effiCient tensOr decomposition-based filteR prunING, a novel filter pruning methodology for neural networks. CORING is crafted to achieve efficient tensor decomposition-based pruning, a stark departure from conventional approaches that rely on vectorized or matricized filter representations. Our approach represents a significant leap forward in the field by introducing tensor decompositions, specifically the HOSVD, which preserves the multidimensional nature of filters while providing a low-rank approximation, thus substantially reducing complexity. Furthermore, we introduce a versatile method for calculating filter similarity by using the low-rank approximation offered by the HOSVD. This obviates the need for using full filters or reshaped versions and enhances the overall efficiency and effectiveness of our approach. Extensive experimentation across diverse architectures and datasets spanning various vision tasks, including image classification, object detection, instance segmentation, and keypoint detection, validates CORING’s prowess. Remarkably, it outperforms state-of-the-art methods in reducing MACs and parameters, consistently enhancing validation accuracy. Furthermore, we supplement our quantitative results with a comprehensive ablation study, providing substantial evidence of the efficiency of our tensor-based approach. Beyond quantitative outcomes, qualitative results vividly illustrate CORING’s ability to retain essential features within pruned neural networks. Our code is available for research purposes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003174",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Coring",
      "Drilling",
      "Engineering",
      "Filter (signal processing)",
      "Machine learning",
      "Mathematics",
      "Mechanical engineering",
      "Pattern recognition (psychology)",
      "Pruning",
      "Pure mathematics",
      "Rank (graph theory)",
      "Tensor (intrinsic definition)",
      "Tensor decomposition",
      "Tucker decomposition"
    ],
    "authors": [
      {
        "surname": "Pham",
        "given_name": "Van Tien"
      },
      {
        "surname": "Zniyed",
        "given_name": "Yassine"
      },
      {
        "surname": "Nguyen",
        "given_name": "Thanh Phuong"
      }
    ]
  },
  {
    "title": "Bio-inspired computational memory model of the Hippocampus: An approach to a neuromorphic spike-based Content-Addressable Memory",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106474",
    "abstract": "The brain has computational capabilities that surpass those of modern systems, being able to solve complex problems efficiently in a simple way. Neuromorphic engineering aims to mimic biology in order to develop new systems capable of incorporating such capabilities. Bio-inspired learning systems continue to be a challenge that must be solved, and much work needs to be done in this regard. Among all brain regions, the hippocampus stands out as an autoassociative short-term memory with the capacity to learn and recall memories from any fragment of them. These characteristics make the hippocampus an ideal candidate for developing bio-inspired learning systems that, in addition, resemble content-addressable memories. Therefore, in this work we propose a bio-inspired spiking content-addressable memory model based on the CA3 region of the hippocampus with the ability to learn, forget and recall memories, both orthogonal and non-orthogonal, from any fragment of them. The model was implemented on the SpiNNaker hardware platform using Spiking Neural Networks. A set of experiments based on functional, stress and applicability tests were performed to demonstrate its correct functioning. This work presents the first hardware implementation of a fully-functional bio-inspired spiking hippocampal content-addressable memory model, paving the way for the development of future more complex neuromorphic systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003988",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer architecture",
      "Computer science",
      "Content-addressable memory",
      "Hippocampus",
      "Linguistics",
      "Machine learning",
      "Neuromorphic engineering",
      "Neuroscience",
      "Philosophy",
      "Recall",
      "Software engineering",
      "Spike (software development)",
      "Spiking neural network"
    ],
    "authors": [
      {
        "surname": "Casanueva-Morato",
        "given_name": "Daniel"
      },
      {
        "surname": "Ayuso-Martinez",
        "given_name": "Alvaro"
      },
      {
        "surname": "Dominguez-Morales",
        "given_name": "Juan P."
      },
      {
        "surname": "Jimenez-Fernandez",
        "given_name": "Angel"
      },
      {
        "surname": "Jimenez-Moreno",
        "given_name": "Gabriel"
      }
    ]
  },
  {
    "title": "Graph Aggregating-Repelling Network: Do Not Trust All Neighbors in Heterophilic Graphs",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106484",
    "abstract": "Graph neural networks (GNNs) have demonstrated exceptional performance in processing various types of graph data, such as citation networks and social networks, etc. Although many of these GNNs prove their superiority in handling homophilic graphs, they often overlook the other kind of widespread heterophilic graphs, in which adjacent nodes tend to have different classes or dissimilar features. Recent methods attempt to address heterophilic graphs from the graph spatial domain, which try to aggregate more similar nodes or prevent dissimilar nodes with negative weights. However, they may neglect valuable heterophilic information or extract heterophilic information ineffectively, which could cause poor performance of downstream tasks on heterophilic graphs, including node classification and graph classification, etc. Hence, a novel framework named GARN is proposed to effectively extract both homophilic and heterophilic information. First, we analyze the shortcomings of most GNNs in tackling heterophilic graphs from the perspective of graph spectral and spatial theory. Then, motivated by these analyses, a Graph Aggregating-Repelling Convolution (GARC) mechanism is designed with the objective of fusing both low-pass and high-pass graph filters. Technically, it learns positive attention weights as a low-pass filter to aggregate similar adjacent nodes, and learns negative attention weights as a high-pass filter to repel dissimilar adjacent nodes. A learnable integration weight is used to adaptively fuse these two filters and balance the proportion of the learned positive and negative weights, which could control our GARC to evolve into different types of graph filters and prevent it from over-relying on high intra-class similarity. Finally, a framework named GARN is established by simply stacking several layers of GARC to evaluate its graph representation learning ability on both the node classification and image-converted graph classification tasks. Extensive experiments conducted on multiple homophilic and heterophilic graphs and complex real-world image-converted graphs indicate the effectiveness of our proposed framework and mechanism over several representative GNN baselines.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004088",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Graph",
      "Graph product",
      "Line graph",
      "Pathwidth",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yuhu"
      },
      {
        "surname": "Wen",
        "given_name": "Jinyong"
      },
      {
        "surname": "Zhang",
        "given_name": "Chunxia"
      },
      {
        "surname": "Xiang",
        "given_name": "Shiming"
      }
    ]
  },
  {
    "title": "A biologically inspired computational model of human ventral temporal cortex",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106437",
    "abstract": "Our minds represent miscellaneous objects in the physical world metaphorically in an abstract and complex high-dimensional object space, which is implemented in a two-dimensional surface of the ventral temporal cortex (VTC) with topologically organized object selectivity. Here we investigated principles guiding the topographical organization of object selectivities in the VTC by constructing a hybrid Self-Organizing Map (SOM) model that harnesses a biologically inspired algorithm of wiring cost minimization and adheres to the constraints of the lateral wiring span of human VTC neurons. In a series of in silico experiments with functional brain neuroimaging and neurophysiological single-unit data from humans and non-human primates, the VTC-SOM predicted the topographical structure of fine-scale category-selective regions (face-, tool-, body-, and place-selective regions) and the boundary in large-scale abstract functional maps (animate vs. inanimate, real-word small-size vs. big-size, central vs. peripheral), with no significant loss in functionality (e.g., categorical selectivity and view-invariant representations). In addition, when the same principle was applied to V1 orientation preferences, a pinwheel-like topology emerged, suggesting the model's broad applicability. In summary, our study illustrates that the simple principle of wiring cost minimization, coupled with the appropriate biological constraint of lateral wiring span, is able to implement the high-dimensional object space in a two-dimensional cortical surface.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003617",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Object (grammar)",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yiyuan"
      },
      {
        "surname": "Zhou",
        "given_name": "Ke"
      },
      {
        "surname": "Bao",
        "given_name": "Pinglei"
      },
      {
        "surname": "Liu",
        "given_name": "Jia"
      }
    ]
  },
  {
    "title": "Salient object detection in low-light RGB-T scene via spatial-frequency cues mining",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106406",
    "abstract": "Low-light conditions pose significant challenges to vision tasks, such as salient object detection (SOD), due to insufficient photons. Light-insensitive RGB-T SOD models mitigate the above problems to some extent, but they are limited in performance as they only focus on spatial feature fusion while ignoring the frequency discrepancy. To this end, we propose an RGB-T SOD model by mining spatial-frequency cues, called SFMNet, for low-light scenes. Our SFMNet consists of spatial-frequency feature exploration (SFFE) modules and spatial-frequency feature interaction (SFFI) modules. To be specific, the SFFE module aims to separate spatial-frequency features and adaptively extract high and low-frequency features. Moreover, the SFFI module integrates cross-modality and cross-domain information to capture effective feature representations. By deploying both modules in a top-down pathway, our method generates high-quality saliency predictions. Furthermore, we construct the first low-light RGB-T SOD dataset as a benchmark for evaluating performance. Extensive experiments demonstrate that our SFMNet can achieve higher accuracy than the existing models for low-light scenes.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003307",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Feature extraction",
      "Focus (optics)",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "RGB color model",
      "Salient",
      "Spatial frequency"
    ],
    "authors": [
      {
        "surname": "Yue",
        "given_name": "Huihui"
      },
      {
        "surname": "Guo",
        "given_name": "Jichang"
      },
      {
        "surname": "Yin",
        "given_name": "Xiangjun"
      },
      {
        "surname": "Zhang",
        "given_name": "Yi"
      },
      {
        "surname": "Zheng",
        "given_name": "Sida"
      }
    ]
  },
  {
    "title": "A Semi-supervised Gaussian Mixture Variational Autoencoder method for few-shot fine-grained fault diagnosis",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106482",
    "abstract": "In practical engineering, obtaining labeled high-quality fault samples poses challenges. Conventional fault diagnosis methods based on deep learning struggle to discern the underlying causes of mechanical faults from a fine-grained perspective, due to the scarcity of annotated data. To tackle those issue, we propose a novel semi-supervised Gaussian Mixed Variational Autoencoder method, SeGMVAE, aimed at acquiring unsupervised representations that can be transferred across fine-grained fault diagnostic tasks, enabling the identification of previously unseen faults using only the small number of labeled samples. Initially, Gaussian mixtures are introduced as a multimodal prior distribution for the Variational Autoencoder. This distribution is dynamically optimized for each task through an expectation–maximization (EM) algorithm, constructing a latent representation of the bridging task and unlabeled samples. Subsequently, a set variational posterior approach is presented to encode each task sample into the latent space, facilitating meta-learning. Finally, semi-supervised EM integrates the posterior of labeled data by acquiring task-specific parameters for diagnosing unseen faults. Results from two experiments demonstrate that SeGMVAE excels in identifying new fine-grained faults and exhibits outstanding performance in cross-domain fault diagnosis across different machines. Our code is available at https://github.com/zhiqan/SeGMVAE.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004064",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Computer science",
      "Fault (geology)",
      "Gaussian",
      "Geology",
      "Materials science",
      "Metallurgy",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Seismology",
      "Shot (pellet)"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Zhiqian"
      },
      {
        "surname": "Xu",
        "given_name": "Yeyin"
      },
      {
        "surname": "Zhang",
        "given_name": "Jiabin"
      },
      {
        "surname": "Zhao",
        "given_name": "Runchao"
      },
      {
        "surname": "Chen",
        "given_name": "Zhaobo"
      },
      {
        "surname": "Jiao",
        "given_name": "Yinghou"
      }
    ]
  },
  {
    "title": "Reliable object tracking by multimodal hybrid feature extraction and transformer-based fusion",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106493",
    "abstract": "Visual object tracking, which is primarily based on visible light image sequences, encounters numerous challenges in complicated scenarios, such as low light conditions, high dynamic ranges, and background clutter. To address these challenges, incorporating the advantages of multiple visual modalities is a promising solution for achieving reliable object tracking. However, the existing approaches usually integrate multimodal inputs through adaptive local feature interactions, which cannot leverage the full potential of visual cues, thus resulting in insufficient feature modeling. In this study, we propose a novel multimodal hybrid tracker (MMHT) that utilizes frame-event-based data for reliable single object tracking. The MMHT model employs a hybrid backbone consisting of an artificial neural network (ANN) and a spiking neural network (SNN) to extract dominant features from different visual modalities and then uses a unified encoder to align the features across different domains. Moreover, we propose an enhanced transformer-based module to fuse multimodal features using attention mechanisms. With these methods, the MMHT model can effectively construct a multiscale and multidimensional visual feature space and achieve discriminative feature modeling. Extensive experiments demonstrate that the MMHT model exhibits competitive performance in comparison with that of other state-of-the-art methods. Overall, our results highlight the effectiveness of the MMHT model in terms of addressing the challenges faced in visual object tracking tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004179",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Feature extraction",
      "Fusion",
      "Linguistics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Pedagogy",
      "Philosophy",
      "Psychology",
      "Tracking (education)",
      "Transformer",
      "Video tracking",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Hongze"
      },
      {
        "surname": "Liu",
        "given_name": "Rui"
      },
      {
        "surname": "Cai",
        "given_name": "Wuque"
      },
      {
        "surname": "Wang",
        "given_name": "Jun"
      },
      {
        "surname": "Wang",
        "given_name": "Yue"
      },
      {
        "surname": "Tang",
        "given_name": "Huajin"
      },
      {
        "surname": "Cui",
        "given_name": "Yan"
      },
      {
        "surname": "Yao",
        "given_name": "Dezhong"
      },
      {
        "surname": "Guo",
        "given_name": "Daqing"
      }
    ]
  },
  {
    "title": "Cycle contrastive adversarial learning with structural consistency for unsupervised high-quality image deraining transformer",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106428",
    "abstract": "In overcoming the challenges faced in adapting to paired real-world data, recent unsupervised single image deraining (SID) methods have proven capable of accomplishing notably acceptable deraining performance. However, the previous methods usually fail to produce a high quality rain-free image due to neglecting sufficient attention to semantic representation and the image content, which results in the inability to completely separate the content from the rain layer. In this paper, we develop a novel cycle contrastive adversarial framework for unsupervised SID, which mainly consists of cycle contrastive learning (CCL) and location contrastive learning (LCL). Specifically, CCL achieves high-quality image reconstruction and rain-layer stripping by pulling similar features together while pushing dissimilar features further in both semantic and discriminant latent spaces. Meanwhile, LCL implicitly constrains the mutual information of the same location of different exemplars to maintain the content information. In addition, recently inspired by the powerful Segment Anything Model (SAM) that can effectively extract widely applicable semantic structural details, we formulate a structural-consistency regularization to fine-tune our network using SAM. Apart from this, we attempt to introduce vision transformer (VIT) into our network architecture to further improve the performance. In our designed transformer-based GAN, to obtain a stronger representation, we propose a multi-layer channel compression attention module (MCCAM) to extract a richer feature. Equipped with the above techniques, our proposed unsupervised SID algorithm, called CCLformer, can show advantageous image deraining performance. Extensive experiments demonstrate both the superiority of our method and the effectiveness of each module in CCLformer. The code is available at https://github.com/zhihefang/CCLGAN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003526",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Feature learning",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Regularization (linguistics)",
      "Transformer",
      "Unsupervised learning",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Chen"
      },
      {
        "surname": "Cai",
        "given_name": "Weiling"
      },
      {
        "surname": "Hu",
        "given_name": "Chengwei"
      },
      {
        "surname": "Yuan",
        "given_name": "Zheng"
      }
    ]
  },
  {
    "title": "Analysis of medical images super-resolution via a wavelet pyramid recursive neural network constrained by wavelet energy entropy",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106460",
    "abstract": "Recently, multi-resolution pyramid-based techniques have emerged as the prevailing research approach for image super-resolution. However, these methods typically rely on a single mode of information transmission between levels. In our approach, a wavelet pyramid recursive neural network (WPRNN) based on wavelet energy entropy (WEE) constraint is proposed. This network transmits previous-level wavelet coefficients and additional shallow coefficient features to capture local details. Besides, the parameter of low- and high-frequency wavelet coefficients within each pyramid level and across pyramid levels is shared. A multi-resolution wavelet pyramid fusion (WPF) module is devised to facilitate information transfer across network pyramid levels. Additionally, a wavelet energy entropy loss is proposed to constrain the reconstruction of wavelet coefficients from the perspective of signal energy distribution. Finally, our method achieves the competitive reconstruction performance with the minimal parameters through an extensive series of experiments conducted on publicly available datasets, which demonstrates its practical utility.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003848",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cascade algorithm",
      "Computer science",
      "Computer vision",
      "Entropy (arrow of time)",
      "Geometry",
      "Lifting scheme",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Pyramid (geometry)",
      "Quantum mechanics",
      "Second-generation wavelet transform",
      "Stationary wavelet transform",
      "Wavelet",
      "Wavelet packet decomposition",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Yue"
      },
      {
        "surname": "She",
        "given_name": "Kun"
      },
      {
        "surname": "Shi",
        "given_name": "Kaibo"
      },
      {
        "surname": "Cai",
        "given_name": "Xiao"
      },
      {
        "surname": "Kwon",
        "given_name": "Oh-Min"
      },
      {
        "surname": "Soh",
        "given_name": "YengChai"
      }
    ]
  },
  {
    "title": "Inverse-free zeroing neural network for time-variant nonlinear optimization with manipulator applications",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106462",
    "abstract": "In this paper, the problem of time-variant optimization subject to nonlinear equation constraint is studied. To solve the challenging problem, methods based on the neural networks, such as zeroing neural network and gradient neural network, are commonly adopted due to their performance on handling nonlinear problems. However, the traditional zeroing neural network algorithm requires computing the matrix inverse during the solving process, which is a complicated and time-consuming operation. Although the gradient neural network algorithm does not require computing the matrix inverse, its accuracy is not high enough. Therefore, a novel inverse-free zeroing neural network algorithm without matrix inverse is proposed in this paper. The proposed algorithm not only avoids the matrix inverse, but also avoids matrix multiplication, greatly reducing the computational complexity. In addition, detailed theoretical analyses of the convergence performance of the proposed algorithm is provided to guarantee its excellent capability in solving time-variant optimization problems. Numerical simulations and comparative experiments with traditional zeroing neural network and gradient neural network algorithms substantiate the accuracy and superiority of the novel inverse-free zeroing neural network algorithm. To further validate the performance of the novel inverse-free zeroing neural network algorithm in practical applications, path tracking tasks of three manipulators (i.e., Universal Robot 5, Franka Emika Panda, and Kinova JACO2 manipulators) are conducted, and the results verify the applicability of the proposed algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003861",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Geometry",
      "Inverse",
      "Mathematical optimization",
      "Mathematics",
      "Nonlinear system",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Jielong"
      },
      {
        "surname": "Pan",
        "given_name": "Yan"
      },
      {
        "surname": "Zhang",
        "given_name": "Yunong"
      },
      {
        "surname": "Li",
        "given_name": "Shuai"
      },
      {
        "surname": "Tan",
        "given_name": "Ning"
      }
    ]
  },
  {
    "title": "Dynamics of heterogeneous Hopfield neural network with adaptive activation function based on memristor",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106408",
    "abstract": "Memristor and activation function are two important nonlinear factors of the memristive Hopfield neural network. The effects of different memristors on the dynamics of Hopfield neural networks have been studied by many researchers. However, less attention has been paid to the activation function. In this paper, we present a heterogeneous memristive Hopfield neural network with neurons using different activation functions. The activation functions include fixed activation functions and an adaptive activation function, where the adaptive activation function is based on a memristor. The theoretical and experimental study of the neural network’s dynamics has been conducted using phase portraits, bifurcation diagrams, and Lyapunov exponents spectras. Numerical results show that complex dynamical behaviors such as multi-scroll chaos, transient chaos, state jumps and multi-type coexisting attractors can be observed in the heterogeneous memristive Hopfield neural network. In addition, the hardware implementation of memristive Hopfield neural network with adaptive activation function is designed and verified. The experimental results are in good agreement with those obtained using numerical simulations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003320",
    "keywords": [
      "Acoustics",
      "Activation function",
      "Artificial intelligence",
      "Artificial neural network",
      "Biological system",
      "Biology",
      "Computer science",
      "Dynamics (music)",
      "Evolutionary biology",
      "Function (biology)",
      "Hopfield network",
      "Memristor",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Chunhua"
      },
      {
        "surname": "Liang",
        "given_name": "Junhui"
      },
      {
        "surname": "Deng",
        "given_name": "Quanli"
      }
    ]
  },
  {
    "title": "Capacity bounds for hyperbolic neural network representations of latent tree structures",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106420",
    "abstract": "We study the representation capacity of deep hyperbolic neural networks (HNNs) with a ReLU activation function. We establish the first proof that HNNs can ɛ -isometrically embed any finite weighted tree into a hyperbolic space of dimension d at least equal to 2 with prescribed sectional curvature κ < 0 , for any ɛ > 1 (where ɛ = 1 being optimal). We establish rigorous upper bounds for the network complexity on an HNN implementing the embedding. We find that the network complexity of HNN implementing the graph representation is independent of the representation fidelity/distortion. We contrast this result against our lower bounds on distortion which any ReLU multi-layer perceptron (MLP) must exert when embedding a tree with L > 2 d leaves into a d -dimensional Euclidean space, which we show at least Ω ( L 1 / d ) ; independently of the depth, width, and (possibly discontinuous) activation function defining the MLP.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003447",
    "keywords": [
      "Activation function",
      "Algorithm",
      "Amplifier",
      "Artificial intelligence",
      "Artificial neural network",
      "Bandwidth (computing)",
      "Biology",
      "Combinatorics",
      "Computer network",
      "Computer science",
      "Curvature",
      "Differential geometry",
      "Dimension (graph theory)",
      "Discrete mathematics",
      "Distortion (music)",
      "Embedding",
      "Euclidean geometry",
      "Euclidean space",
      "Evolutionary biology",
      "Function (biology)",
      "Geometry",
      "Graph",
      "Hyperbolic geometry",
      "Hyperbolic tree",
      "Law",
      "Mathematical analysis",
      "Mathematics",
      "Perceptron",
      "Political science",
      "Politics",
      "Pure mathematics",
      "Representation (politics)",
      "Tree (set theory)",
      "Upper and lower bounds"
    ],
    "authors": [
      {
        "surname": "Kratsios",
        "given_name": "Anastasis"
      },
      {
        "surname": "Hong",
        "given_name": "Ruiyang"
      },
      {
        "surname": "Sáez de Ocáriz Borde",
        "given_name": "Haitz"
      }
    ]
  },
  {
    "title": "Heterogeneous graph convolutional network for multi-view semi-supervised classification",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106438",
    "abstract": "This paper proposes a novel approach to semantic representation learning from multi-view datasets, distinct from most existing methodologies which typically handle single-view data individually, maintaining a shared semantic link across the multi-view data via a unified optimization process. Notably, even recent advancements, such as Co-GCN, continue to treat each view as an independent graph, subsequently aggregating the respective GCN representations to form output representations, which ignores the complex semantic interactions among heterogeneous data. To address the issue, we design a unified framework to connect multi-view data with heterogeneous graphs. Specifically, our study envisions multi-view data as a heterogeneous graph composed of shared isomorphic nodes and multi-type edges, wherein the same nodes are shared across different views, but each specific view possesses its own unique edge type. This perspective motivates us to utilize the heterogeneous graph convolutional network (HGCN) to extract semantic representations from multi-view data for semi-supervised classification tasks. To the best of our knowledge, this is an early attempt to transfigure multi-view data into a heterogeneous graph within the realm of multi-view semi-supervised learning. In our approach, the original input of the HGCN is composed of concatenated multi-view matrices, and its convolutional operator (the graph Laplacian matrix) is adaptively learned from multi-type edges in a data-driven fashion. After rigorous experimentation on eight public datasets, our proposed method, hereafter referred to as HGCN-MVSC, demonstrated encouraging superiority over several state-of-the-art competitors for semi-supervised classification tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003629",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "External Data Representation",
      "Graph",
      "Laplacian matrix",
      "Machine learning",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Shiping"
      },
      {
        "surname": "Huang",
        "given_name": "Sujia"
      },
      {
        "surname": "Wu",
        "given_name": "Zhihao"
      },
      {
        "surname": "Liu",
        "given_name": "Rui"
      },
      {
        "surname": "Chen",
        "given_name": "Yong"
      },
      {
        "surname": "Zhang",
        "given_name": "Dell"
      }
    ]
  },
  {
    "title": "A syntactic evidence network model for fact verification",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106424",
    "abstract": "In natural language processing, fact verification is a very challenging task, which requires retrieving multiple evidence sentences from a reliable corpus to verify the authenticity of a claim. Although most of the current deep learning methods use the attention mechanism for fact verification, they have not considered imposing attentional constraints on important related words in the claim and evidence sentences, resulting in inaccurate attention for some irrelevant words. In this paper, we propose a syntactic evidence network (SENet) model which incorporates entity keywords, syntactic information and sentence attention for fact verification. The SENet model extracts entity keywords from claim and evidence sentences, and uses a pre-trained syntactic dependency parser to extract the corresponding syntactic sentence structures and incorporates the extracted syntactic information into the attention mechanism for language-driven word representation. In addition, the sentence attention mechanism is applied to obtain a richer semantic representation. We have conducted experiments on the FEVER and UKP Snopes datasets for performance evaluation. Our SENet model has achieved 78.69% in Label Accuracy and 75.63% in FEVER Score on the FEVER dataset. In addition, our SENet model also has achieved 65.0% in precision and 61.2% in macro F1 on the UKP Snopes dataset. The experimental results have shown that our proposed SENet model has outperformed the baseline models and achieved the state-of-the-art performance for fact verification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003484",
    "keywords": [
      "Artificial intelligence",
      "Baseline (sea)",
      "Computer science",
      "Dependency (UML)",
      "Economics",
      "Geology",
      "Law",
      "Linguistics",
      "Management",
      "Natural language processing",
      "Oceanography",
      "Parsing",
      "Philosophy",
      "Political science",
      "Politics",
      "Question answering",
      "Representation (politics)",
      "Sentence",
      "Task (project management)",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Zhendong"
      },
      {
        "surname": "Hui",
        "given_name": "Siu Cheung"
      },
      {
        "surname": "Zhuang",
        "given_name": "Fuzhen"
      },
      {
        "surname": "Liao",
        "given_name": "Lejian"
      },
      {
        "surname": "Jia",
        "given_name": "Meihuizi"
      },
      {
        "surname": "Li",
        "given_name": "Jiaqi"
      },
      {
        "surname": "Huang",
        "given_name": "Heyan"
      }
    ]
  },
  {
    "title": "Adversarial Infrared Curves: An attack on infrared pedestrian detectors in the physical world",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106459",
    "abstract": "Deep neural network security is a persistent concern, with considerable research on visible light physical attacks but limited exploration in the infrared domain. Existing approaches, like white-box infrared attacks using bulb boards and QR suits, lack realism and stealthiness. Meanwhile, black-box methods with cold and hot patches often struggle to ensure robustness. To bridge these gaps, we propose Adversarial Infrared Curves (AdvIC). Using Particle Swarm Optimization, we optimize two Bezier curves and employ cold patches in the physical realm to introduce perturbations, creating infrared curve patterns for physical sample generation. Our extensive experiments confirm AdvIC’s effectiveness, achieving 94.8% and 67.2% attack success rates for digital and physical attacks, respectively. Stealthiness is demonstrated through a comparative analysis, and robustness assessments reveal AdvIC’s superiority over baseline methods. When deployed against diverse advanced detectors, AdvIC achieves an average attack success rate of 76.2%, emphasizing its robust nature. We conduct thorough experimental analyses, including ablation experiments, transfer attacks, adversarial defense investigations, etc. Given AdvIC’s substantial security implications for real-world vision-based applications, urgent attention and mitigation efforts are warranted.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003836",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Deep learning",
      "Gene",
      "Infrared",
      "Optics",
      "Physics",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Chengyin"
      },
      {
        "surname": "Shi",
        "given_name": "Weiwen"
      },
      {
        "surname": "Yao",
        "given_name": "Wen"
      },
      {
        "surname": "Jiang",
        "given_name": "Tingsong"
      },
      {
        "surname": "Tian",
        "given_name": "Ling"
      },
      {
        "surname": "Chen",
        "given_name": "Xiaoqian"
      },
      {
        "surname": "Li",
        "given_name": "Wen"
      }
    ]
  },
  {
    "title": "Sequential safe static and dynamic screening rule for accelerating support tensor machine",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106407",
    "abstract": "Support tensor machine (STM), as a higher-order extension of support vector machine, is adept at effectively addressing tensorial data classification problems, which maintains the inherent structure in tensors and mitigates the curse of dimensionality. However, it needs to resort to the alternating projection iterative technique, which is very time-consuming. To overcome this shortcoming, we propose an efficient sequential safe static and dynamic screening rule (SS-SDSR) for accelerating STM in this paper. Its main idea is to reduce every projection iterative sub-model by identifying and deleting the redundant variables before and during the training process without sacrificing accuracy. Its construction mainly consists of two parts: (1) The static screening rule and dynamic screening rule are first built based on the variational inequality and duality gap, respectively. (2) The sequential screening process is achieved by using the static screening rule with the different adjacent parameters and applying the dynamic screening rule under the same parameter. In the experiment, on the one hand, to verify the influence of different parameter intervals, screening frequencies, and forms of data on the effectiveness of our method, three experiments on artificial datasets are conducted, which indicate that our method is effective for any forms of data when the parameter interval is small and the screening frequency is appropriate. On the other hand, to demonstrate the feasibility and validity of our SS-SDSR, numerical experiments on eleven vector-based datasets, and six tensor-based datasets are conducted and compared with the other five algorithms. Experimental results illustrate the effectiveness and safety of our SS-SDSR.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003319",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Curse of dimensionality",
      "Data mining",
      "Discrete mathematics",
      "Duality (order theory)",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Process (computing)",
      "Projection (relational algebra)",
      "Pure mathematics",
      "Support vector machine",
      "Tensor (intrinsic definition)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Hongmei"
      },
      {
        "surname": "Jiang",
        "given_name": "Kun"
      },
      {
        "surname": "Li",
        "given_name": "Xiao"
      },
      {
        "surname": "Xu",
        "given_name": "Yitian"
      }
    ]
  },
  {
    "title": "Towards safer robot-assisted surgery: A markerless augmented reality framework",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106469",
    "abstract": "Robot-assisted surgery is rapidly developing in the medical field, and the integration of augmented reality shows the potential to improve the operation performance of surgeons by providing more visual information. In this paper, we proposed a markerless augmented reality framework to enhance safety by avoiding intra-operative bleeding, which is a high risk caused by collision between surgical instruments and delicate blood vessels (arteries or veins). Advanced stereo reconstruction and segmentation networks are compared to find the best combination to reconstruct the intra-operative blood vessel in 3D space for registration with the pre-operative model, and the minimum distance detection between the instruments and the blood vessel is implemented. A robot-assisted lymphadenectomy is emulated on the da Vinci Research Kit in a dry lab, and ten human subjects perform this operation to explore the usability of the proposed framework. The result shows that the augmented reality framework can help the users to avoid the dangerous collision between the instruments and the delicate blood vessel while not introducing an extra load. It provides a flexible framework that integrates augmented reality into the medical robotic platform to enhance safety during surgery.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003939",
    "keywords": [
      "Artificial intelligence",
      "Augmented reality",
      "Collision",
      "Collision detection",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Human–computer interaction",
      "Mixed reality",
      "Robot",
      "Robotic surgery",
      "SAFER",
      "Segmentation",
      "Usability"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Ziyang"
      },
      {
        "surname": "Cruciani",
        "given_name": "Laura"
      },
      {
        "surname": "Fan",
        "given_name": "Ke"
      },
      {
        "surname": "Fontana",
        "given_name": "Matteo"
      },
      {
        "surname": "Lievore",
        "given_name": "Elena"
      },
      {
        "surname": "De Cobelli",
        "given_name": "Ottavio"
      },
      {
        "surname": "Musi",
        "given_name": "Gennaro"
      },
      {
        "surname": "Ferrigno",
        "given_name": "Giancarlo"
      },
      {
        "surname": "De Momi",
        "given_name": "Elena"
      }
    ]
  },
  {
    "title": "A novel two-layer fuzzy neural network for solving inequality-constrained ℓ 1 -minimization problem with applications",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106491",
    "abstract": "In this paper, we propose a novel two-layer fuzzy neural network model (TLFNN) for solving the inequality-constrained ℓ 1 -minimization problem. The stability and global convergence of the proposed TLFNN model are detailedly analyzed using the Lyapunov theory. Compared with the existing three-layer neural network model (TLNN) recently designed by Yang et al., the proposed TLFNN model possesses less storage, stronger robustness, faster convergence rate and higher convergence accuracy. These advantages are illustrated by some numerical experiments, where it is shown that the TLFNN model can achieve a convergence accuracy of 1 0 − 13 within 5 s while the TLNN model can only acquire 1 0 − 6 in 1 0 5 s when some random coefficient matrices are applied. Since the linear equality-constrained conditions can be equivalently transformed into double inequality-constrained ones, some simulation experiments for sparse signal reconstruction show that the proposed TLFNN model also has less convergence time and stronger robustness than the existing state-of-the-art neural network models for the equality-constrained ℓ 1 -minimization problem.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004155",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Computer science",
      "Discrete mathematics",
      "Layer (electronics)",
      "Mathematics",
      "Organic chemistry",
      "Scalable Vector Graphics",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Qing"
      },
      {
        "surname": "Zheng",
        "given_name": "Bing"
      }
    ]
  },
  {
    "title": "Model-agnostic counterfactual reasoning for identifying and mitigating answer bias in knowledge tracing",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106495",
    "abstract": "Knowledge tracing (KT) aims to monitor students’ evolving knowledge states through their learning interactions with concept-related questions, and can be indirectly evaluated by predicting how students will perform on future questions. In this paper, we observe that there is a common phenomenon of answer bias, i.e., a highly unbalanced distribution of correct and incorrect answers for each question. Existing models tend to memorize the answer bias as a shortcut for achieving high prediction performance in KT, thereby failing to fully understand students’ knowledge states. To address this issue, we approach the KT task from a causality perspective. A causal graph of KT is first established, from which we identify that the impact of answer bias lies in the direct causal effect of questions on students’ responses. A novel COunterfactual REasoning (CORE) framework for KT is further proposed, which separately captures the total causal effect and direct causal effect during training, and mitigates answer bias by subtracting the latter from the former in testing. The CORE framework is applicable to various existing KT models, and we implement it based on the prevailing DKT, DKVMN, and AKT models, respectively. Extensive experiments on three benchmark datasets demonstrate the effectiveness of CORE in making the debiased inference for KT. We have released our code at https://github.com/lucky7-code/CORE.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004192",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Counterfactual thinking",
      "Machine learning",
      "Operating system",
      "Psychology",
      "Social psychology",
      "Tracing"
    ],
    "authors": [
      {
        "surname": "Cui",
        "given_name": "Chaoran"
      },
      {
        "surname": "Ma",
        "given_name": "Hebo"
      },
      {
        "surname": "Dong",
        "given_name": "Xiaolin"
      },
      {
        "surname": "Zhang",
        "given_name": "Chen"
      },
      {
        "surname": "Zhang",
        "given_name": "Chunyun"
      },
      {
        "surname": "Yao",
        "given_name": "Yumo"
      },
      {
        "surname": "Chen",
        "given_name": "Meng"
      },
      {
        "surname": "Ma",
        "given_name": "Yuling"
      }
    ]
  },
  {
    "title": "InA: Inhibition Adaption on pre-trained language models",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106410",
    "abstract": "Fine-tuning pre-trained language models (LMs) may not always be the most practical approach for downstream tasks. While adaptation fine-tuning methods have shown promising results, a clearer explanation of their mechanisms and further inhibition of the transmission of information is needed. To address this, we propose an Inhibition Adaptation (InA) fine-tuning method that aims to reduce the number of added tunable weights and appropriately reweight knowledge derived from pre-trained LMs. The InA method involves (1) inserting a small trainable vector into each Transformer attention architecture and (2) setting a threshold to directly eliminate irrelevant knowledge. This approach draws inspiration from the shunting inhibition, which allows the inhibition of specific neurons to gate other functional neurons. With the inhibition mechanism, InA achieves competitive or even superior performance compared to other fine-tuning methods on B E R T − l a r g e , R o B E R T a − l a r g e , and D e B E R T a − l a r g e for text classification and question-answering tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003344",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Epistemology",
      "Fine-tuning",
      "Language model",
      "Machine learning",
      "Mechanism (biology)",
      "Neuroscience",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Question answering",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Kang",
        "given_name": "Cheng"
      },
      {
        "surname": "Prokop",
        "given_name": "Jindrich"
      },
      {
        "surname": "Tong",
        "given_name": "Lei"
      },
      {
        "surname": "Zhou",
        "given_name": "Huiyu"
      },
      {
        "surname": "Hu",
        "given_name": "Yong"
      },
      {
        "surname": "Novak",
        "given_name": "Daniel"
      }
    ]
  },
  {
    "title": "Inductive reasoning with type-constrained encoding for emerging entities",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106468",
    "abstract": "Knowledge graph reasoning, vital for addressing incompleteness and supporting applications, faces challenges with the continuous growth of graphs. To address this challenge, several inductive reasoning models for encoding emerging entities have been proposed. However, they do not consider the multi-batch emergence scenario, where new entities and new facts are usually added to knowledge graphs (KGs) in multiple batches in the order of their emergence. To simulate the continuous growth of knowledge graphs, a novel multi-batch emergence (MBE) scenario has recently been proposed. We propose a path-based inductive model to handle multi-batch entity growth, enhancing entity encoding with type information. Specifically, we observe a noteworthy pattern in which entity types at the head and tail of the same relation exhibit relative regularity. To utilize this regularity, we introduce a pair of learnable parameters for each relation, representing entity type features linked to the relation. The type features are dedicated to encoding and updating the features of entities. Meanwhile, our model incorporates a novel attention mechanism, combining statistical co-occurrence and semantic similarity of relations effectively for contextual information capture. After generating embeddings, we employ reinforcement learning for path reasoning. To reduce sparsity and expand the action space, our model generates soft candidate facts by grounding a set of soft path rules. Meanwhile, we incorporate the confidence scores of these facts in the action space to facilitate the agent to better distinguish between original facts and rule-generated soft facts. Performances on three multi-batch entity growth datasets demonstrate robust performance, consistently outperforming state-of-the-art models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003927",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Encoding (memory)",
      "Machine learning",
      "Path (computing)",
      "Programming language",
      "Relation (database)",
      "Set (abstract data type)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Mu",
        "given_name": "Chong"
      },
      {
        "surname": "Zhang",
        "given_name": "Lizong"
      },
      {
        "surname": "Wang",
        "given_name": "Zhiguo"
      },
      {
        "surname": "Yuan",
        "given_name": "Qianghua"
      },
      {
        "surname": "Peng",
        "given_name": "Chengzong"
      }
    ]
  },
  {
    "title": "Leveraging spiking neural networks for topic modeling",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106494",
    "abstract": "This article investigates the application of spiking neural networks (SNNs) to the problem of topic modeling (TM): the identification of significant groups of words that represent human-understandable topics in large sets of documents. Our research is based on the hypothesis that an SNN that implements the Hebbian learning paradigm is capable of becoming specialized in the detection of statistically significant word patterns in the presence of adequately tailored sequential input. To support this hypothesis, we propose a novel spiking topic model (STM) that transforms text into a sequence of spikes and uses that sequence to train single-layer SNNs. In STM, each SNN neuron represents one topic, and each of the neuron’s weights corresponds to one word. STM synaptic connections are modified according to spike-timing-dependent plasticity; after training, the neurons’ strongest weights are interpreted as the words that represent topics. We compare the performance of STM with four other TM methods Latent Dirichlet Allocation (LDA), Biterm Topic Model (BTM), Embedding Topic Model (ETM) and BERTopic on three datasets: 20Newsgroups, BBC news, and AG news. The results demonstrate that STM can discover high-quality topics and successfully compete with comparative classical methods. This sheds new light on the possibility of the adaptation of SNN models in unsupervised natural language processing.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004180",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Embedding",
      "Genetics",
      "Hebbian theory",
      "Latent Dirichlet allocation",
      "Layer (electronics)",
      "Linguistics",
      "Machine learning",
      "Natural language processing",
      "Optics",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Receptor",
      "Sequence (biology)",
      "Software engineering",
      "Spike (software development)",
      "Spike-timing-dependent plasticity",
      "Spiking neural network",
      "Synaptic plasticity",
      "Topic model",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Białas",
        "given_name": "Marcin"
      },
      {
        "surname": "Mirończuk",
        "given_name": "Marcin Michał"
      },
      {
        "surname": "Mańdziuk",
        "given_name": "Jacek"
      }
    ]
  },
  {
    "title": "Self-architectural knowledge distillation for spiking neural networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106475",
    "abstract": "Spiking neural networks (SNNs) have attracted attention due to their biological plausibility and the potential for low-energy applications on neuromorphic hardware. Two mainstream approaches are commonly used to obtain SNNs, i.e., ANN-to-SNN conversion methods, and Directly-trained-SNN methods. However, the former achieve excellent performance at the cost of a large number of time steps (i.e., latency), while the latter exhibit lower latency but suffers from suboptimal performance. To tackle the performance-latency trade-off, we propose Self-Architectural Knowledge Distillation (SAKD), an intuitive and effective method for SNNs leveraging Knowledge Distillation (KD). We adopt a bilevel teacher–student training strategy in SAKD, i.e., level-1 involves directly transferring same-architectural pre-trained ANN weights to SNNs, and level-2 encourages the SNNs to mimic ANN’s behavior, considering both final responses and intermediate features aspects. Learning with informative supervision signals fostered by labels and ANNs, our SAKD achieves new state-of-the-art (SOTA) performance with a few time steps on widely-used classification benchmark datasets. On ImageNet-1K, with only 4 time steps, our Spiking-ResNet34 model attains a Top-1 accuracy of 70.04%, outperforming the previous same-architectural SOTA methods. Notably, our SEW-ResNet152 model reaches a Top-1 accuracy of 77.30% on ImageNet-1K, setting a new SOTA benchmark for SNNs. Furthermore, we apply our SAKD to various dense prediction downstream tasks, such as object detection and semantic segmentation, demonstrating strong generalization ability and superior performance. In conclusion, our proposed SAKD framework presents a promising approach for achieving both high performance and low latency in SNNs, potentially paving the way for future advancements in the field.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400399X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Deep neural networks",
      "Generalization",
      "Geodesy",
      "Geography",
      "Latency (audio)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Neuromorphic engineering",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Spiking neural network",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Qiu",
        "given_name": "Haonan"
      },
      {
        "surname": "Ning",
        "given_name": "Munan"
      },
      {
        "surname": "Song",
        "given_name": "Zeyin"
      },
      {
        "surname": "Fang",
        "given_name": "Wei"
      },
      {
        "surname": "Chen",
        "given_name": "Yanqi"
      },
      {
        "surname": "Sun",
        "given_name": "Tao"
      },
      {
        "surname": "Ma",
        "given_name": "Zhengyu"
      },
      {
        "surname": "Yuan",
        "given_name": "Li"
      },
      {
        "surname": "Tian",
        "given_name": "Yonghong"
      }
    ]
  },
  {
    "title": "HyGloadAttack: Hard-label black-box textual adversarial attacks via hybrid optimization",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106461",
    "abstract": "Hard-label black-box textual adversarial attacks present a highly challenging task due to the discrete and non-differentiable nature of text data and the lack of direct access to the model’s predictions. Research in this issue is still in its early stages, and the performance and efficiency of existing methods has potential for improvement. For instance, exchange-based and gradient-based attacks may become trapped in local optima and require excessive queries, hindering the generation of adversarial examples with high semantic similarity and low perturbation under limited query conditions. To address these issues, we propose a novel framework called HyGloadAttack (adversarial Attacks via Hybrid optimization and Global random initialization) for crafting high-quality adversarial examples. HyGloadAttack utilizes a perturbation matrix in the word embedding space to find nearby adversarial examples after global initialization and selects synonyms that maximize similarity while maintaining adversarial properties. Furthermore, we introduce a gradient-based quick search method to accelerate the search process of optimization. Extensive experiments on five datasets of text classification and natural language inference, as well as two real APIs, demonstrate the significant superiority of our proposed HyGloadAttack method over state-of-the-art baseline methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400385X",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Black box",
      "Computer science",
      "Differentiable function",
      "Embedding",
      "Inference",
      "Initialization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Programming language",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Zhaorong"
      },
      {
        "surname": "Xiong",
        "given_name": "Xi"
      },
      {
        "surname": "Li",
        "given_name": "Yuanyuan"
      },
      {
        "surname": "Yu",
        "given_name": "Yan"
      },
      {
        "surname": "Lu",
        "given_name": "Jiazhong"
      },
      {
        "surname": "Zhang",
        "given_name": "Shuai"
      },
      {
        "surname": "Xiong",
        "given_name": "Fei"
      }
    ]
  },
  {
    "title": "A grid fault diagnosis framework based on adaptive integrated decomposition and cross-modal attention fusion",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106400",
    "abstract": "In large-scale power systems, accurately detecting and diagnosing the type of faults when they occur in the grid is a challenging problem. The classification performance of most existing grid fault diagnosis methods depends on the richness and reliability of the data, in addition, it is difficult to obtain sufficient feature information from unimodal circuit signals. To address these issues, we propose a deep residual convolutional neural network (DRCNN)-based framework for grid fault diagnosis. First, we design a comprehensive information entropy value (CIEV) evaluation metric that combines fuzzy entropy (FuzEn) and mutual approximation entropy (MutEn) to integrate multiple decomposition subsequences. Then, DRCNN and heterogeneous graph transformer (HGT) are constructed for extracting multimodal features and considering modal variability. In addition, to obtain the implicit information of multimodal features and control the degree of their performance, we propose to incorporate the cross-modal attention fusion (CMAF) mechanism in the synthesis framework. We validate the proposed method on the three-phase transmission line dataset and VSB power line dataset with accuracies of 99.4 % and 99.0 %, respectively. The proposed method also achieves superior performance compared to classical and state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003241",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Entropy (arrow of time)",
      "Geometry",
      "Grid",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Jiangxun"
      },
      {
        "surname": "Duan",
        "given_name": "Zhu"
      },
      {
        "surname": "Liu",
        "given_name": "Hui"
      }
    ]
  },
  {
    "title": "HyperComm: Hypergraph-based communication in multi-agent reinforcement learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106432",
    "abstract": "In the realm of fully cooperative multi-agent reinforcement learning (MARL), effective communication can induce implicit cooperation among agents and improve overall performance. In current communication strategies, agents are allowed to exchange local observations or latent embeddings, which can augment individual local policy inputs and mitigate uncertainty in local decision-making processes. Unfortunately, in previous communication schemes, agents may potentially receive irrelevant information, which increases training difficulty and leads to poor performance in complex settings. Furthermore, most existing works lack the consideration of the impact of small coalitions formed by agents in the multi-agent system. To address these challenges, we propose HyperComm, a novel framework that uses the hypergraph to model the multi-agent system, improving the accuracy and specificity of communication among agents. Our approach brings the concept of hypergraph for the first time in multi-agent communication for MARL. Within this framework, each agent can communicate more effectively with other agents within the same hyperedge, leading to better cooperation in environments with multiple agents. Compared to those state-of-the-art communication-based approaches, HyperComm demonstrates remarkable performance in scenarios involving a large number of agents.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003563",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Discrete mathematics",
      "Distributed computing",
      "Hypergraph",
      "Machine learning",
      "Mathematics",
      "Multi-agent system",
      "Reinforcement learning",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Tianyu"
      },
      {
        "surname": "Shi",
        "given_name": "Xinli"
      },
      {
        "surname": "Xu",
        "given_name": "Xiangping"
      },
      {
        "surname": "Gui",
        "given_name": "Jie"
      },
      {
        "surname": "Cao",
        "given_name": "Jinde"
      }
    ]
  },
  {
    "title": "Facial micro-expression recognition using stochastic graph convolutional network and dual transferred learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106421",
    "abstract": "Micro-expression recognition (MER) has drawn increasing attention due to its wide application in lie detection, criminal detection and psychological consultation. However, the best recognition accuracy on recent public dataset is still low compared to the accuracy of macro-expression recognition. In this paper, we propose a novel graph convolution network (GCN) for MER achieving state-of-the-art accuracy. Different to existing GCN with fixed graph structure, we define a stochastic graph structure in which some neighbors are selected randomly. As shown by numerical examples, randomness enables better feature characterization while reducing computational complexity. The whole network consists of two branches, one is the spatial branch taking micro-expression images as input, the other is the temporal branch taking optical flow images as input. Because the micro-expression dataset does not have enough images for training the GCN, we employ the transfer learning mechanism. That is, different stochastic GCNs (SGCN) have been trained by the macro-expression dataset in the source network. Then the well-trained SGCNs are transferred to the target network. It is shown that our proposed method achieves the state-of-art performance on all four well-known datasets. This paper explores stochastic GCN and transfer learning with this random structure in the MER task, which is of great importance to improve the recognition performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003459",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Graph",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Randomness",
      "Statistics",
      "Theoretical computer science",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Hui"
      },
      {
        "surname": "Chai",
        "given_name": "Li"
      }
    ]
  },
  {
    "title": "Heterogeneous coexisting attractors, large-scale amplitude control and finite-time synchronization of central cyclic memristive neural networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106412",
    "abstract": "Memristors are of great theoretical and practical significance for chaotic dynamics research of brain-like neural networks due to their excellent physical properties such as brain synapse-like memorability and nonlinearity, especially crucial for the promotion of AI big models, cloud computing, and intelligent systems in the artificial intelligence field. In this paper, we introduce memristors as self-connecting synapses into a four-dimensional Hopfield neural network, constructing a central cyclic memristive neural network (CCMNN), and achieving its effective control. The model adopts a central loop topology and exhibits a variety of complex dynamic behaviors such as chaos, bifurcation, and homogeneous and heterogeneous coexisting attractors. The complex dynamic behaviors of the CCMNN are investigated in depth numerically by equilibrium point stability analysis as well as phase trajectory maps, bifurcation maps, time-domain maps, and LEs. It is found that with the variation of the internal parameters of the memristor, asymmetric heterogeneous attractor coexistence phenomena appear under different initial conditions, including the multi-stable coexistence behaviors of periodic-periodic, periodic-stable point, periodic-chaotic, and stable point-chaotic. In addition, by adjusting the structural parameters, a wide range of amplitude control can be realized without changing the chaotic state of the system. Finally, based on the CCMNN model, an adaptive synchronization controller is designed to achieve finite-time synchronization control, and its application prospect in simple secure communication is discussed. A microcontroller-based hardware circuit and NIST test are conducted to verify the correctness of the numerical results and theoretical analysis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003368",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Attractor",
      "Channel (broadcasting)",
      "Chaotic",
      "Combinatorics",
      "Complex dynamics",
      "Computer network",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Equilibrium point",
      "Lyapunov exponent",
      "Mathematical analysis",
      "Mathematics",
      "Memristor",
      "Nonlinear system",
      "Physics",
      "Quantum mechanics",
      "Synchronization (alternating current)",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Lai",
        "given_name": "Qiang"
      },
      {
        "surname": "Guo",
        "given_name": "Shicong"
      }
    ]
  },
  {
    "title": "KLSANet: Key local semantic alignment Network for few-shot image classification",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106456",
    "abstract": "Few-shot image classification involves recognizing new classes with a limited number of labeled samples. Current local descriptor-based methods, while leveraging consistent low-level features across visible and invisible classes, face challenges including redundant adjacent information, irrelevant partial representation, and limited interpretability. This paper proposes KLSANet, a few-shot image classification approach based on key local semantic alignment network, which aligns key local semantics for accurate classification. Furthermore, we introduce a key local screening module to mitigate the influence of semantically irrelevant image parts on classification. KLSANet demonstrates superior performance on three benchmark datasets (CUB, Stanford Dogs, Stanford Cars), outperforming state-of-the-art methods in 1-shot and 5-shot settings with average improvements of 3.95% and 2.56% respectively. Visualization experiments demonstrate the interpretability of KLSANet predictions. Code is available at: https://github.com/ZitZhengWang/KLSANet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003800",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Chemistry",
      "Code (set theory)",
      "Computer science",
      "Computer security",
      "Contextual image classification",
      "Engineering",
      "Face (sociological concept)",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Interpretability",
      "Key (lock)",
      "Law",
      "Machine learning",
      "Mechanical engineering",
      "One shot",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Semantics (computer science)",
      "Set (abstract data type)",
      "Shot (pellet)",
      "Social science",
      "Sociology",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Zhe"
      },
      {
        "surname": "Zheng",
        "given_name": "Wang"
      },
      {
        "surname": "Guo",
        "given_name": "Pengfei"
      }
    ]
  },
  {
    "title": "Narrowing the semantic gaps in U-Net with learnable skip connections: The case of medical image segmentation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106546",
    "abstract": "Current state-of-the-art medical image segmentation techniques predominantly employ the encoder–decoder architecture. Despite its widespread use, this U-shaped framework exhibits limitations in effectively capturing multi-scale features through simple skip connections. In this study, we made a thorough analysis to investigate the potential weaknesses of connections across various segmentation tasks, and suggest two key aspects of potential semantic gaps crucial to be considered: the semantic gap among multi-scale features in different encoding stages and the semantic gap between the encoder and the decoder. To bridge these semantic gaps, we introduce a novel segmentation framework, which incorporates a Dual Attention Transformer module for capturing channel-wise and spatial-wise relationships, and a Decoder-guided Recalibration Attention module for fusing DAT tokens and decoder features. These modules establish a principle of learnable connection that resolves the semantic gaps, leading to a high-performance segmentation model for medical images. Furthermore, it provides a new paradigm for effectively incorporating the attention mechanism into the traditional convolution-based architecture. Comprehensive experimental results demonstrate that our model achieves consistent, significant gains and outperforms state-of-the-art methods with relatively fewer parameters. This study contributes to the advancement of medical image segmentation by offering a more effective and efficient framework for addressing the limitations of current encoder–decoder architectures. Code: https://github.com/McGregorWwww/UDTransNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004702",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Encoder",
      "Encoding (memory)",
      "Image (mathematics)",
      "Image retrieval",
      "Image segmentation",
      "Operating system",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Semantic gap"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Haonan"
      },
      {
        "surname": "Cao",
        "given_name": "Peng"
      },
      {
        "surname": "Yang",
        "given_name": "Jinzhu"
      },
      {
        "surname": "Zaiane",
        "given_name": "Osmar"
      }
    ]
  },
  {
    "title": "On energy complexity of fully-connected layers",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106419",
    "abstract": "The massive increase in the size of deep neural networks (DNNs) is accompanied by a significant increase in energy consumption of their hardware implementations which is critical for their widespread deployment in low-power mobile devices. In our previous work, an abstract hardware-independent model of energy complexity for convolutional neural networks (CNNs) has been proposed and experimentally validated. Based on this model, we provide a theoretical analysis of energy complexity related to the computation of a fully-connected layer when its inputs, outputs, and weights are transferred between two kinds of memories (DRAM and Buffer). First, we establish a general lower bound on this energy complexity. Then, we present two dataflows and calculate their energy costs to achieve the corresponding upper bounds. In the case of a partitioned Buffer, we prove by the weak duality theorem from linear programming that the lower and upper bounds coincide up to an additive constant, and therefore establish the optimal energy complexity. Finally, the asymptotically optimal quadratic energy complexity of fully-connected layers is experimentally validated by estimating their energy consumption on the Simba and Eyeriss hardware.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003435",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Combinatorics",
      "Computational complexity theory",
      "Computer hardware",
      "Computer science",
      "Dram",
      "Ecology",
      "Energy (signal processing)",
      "Energy consumption",
      "Geometry",
      "Mathematical analysis",
      "Mathematics",
      "Parallel computing",
      "Quadratic equation",
      "Statistics",
      "Time complexity",
      "Topology (electrical circuits)",
      "Upper and lower bounds"
    ],
    "authors": [
      {
        "surname": "Šíma",
        "given_name": "Jiří"
      },
      {
        "surname": "Cabessa",
        "given_name": "Jérémie"
      },
      {
        "surname": "Vidnerová",
        "given_name": "Petra"
      }
    ]
  },
  {
    "title": "Hydra: Multi-head low-rank adaptation for parameter efficient fine-tuning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106414",
    "abstract": "The recent surge in large-scale foundation models has spurred the development of efficient methods for adapting these models to various downstream tasks. Low-rank adaptation methods, such as LoRA, have gained significant attention due to their outstanding parameter efficiency and no additional inference latency. This paper investigates a more general form of adapter module based on the analysis that parallel and sequential adaptation branches learn novel and general features during fine-tuning, respectively. The proposed method, named Hydra, combines parallel and sequential branch to integrate capabilities, which is more expressive than existing single branch methods and enables the exploration of a broader range of optimal points in the fine-tuning process. In addition, the proposed method explicitly leverages the pre-trained weights by performing a linear combination of the pre-trained features. It allows the learned features to have better generalization performance across diverse downstream tasks. Furthermore, we perform a comprehensive analysis of the characteristics of each adaptation branch with empirical evidence. Through an extensive range of experiments, we substantiate the efficiency and demonstrate the superior performance of Hydra. This comprehensive evaluation underscores the potential impact and effectiveness of Hydra in a variety of applications. The source code of this work is publicly opened on https://github.com/extremebird/Hydra.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003381",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Classics",
      "Computer science",
      "Fine-tuning",
      "Generalization",
      "History",
      "Inference",
      "Lernaean Hydra",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Physics",
      "Quantum mechanics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Sanghyeon"
      },
      {
        "surname": "Yang",
        "given_name": "Hyunmo"
      },
      {
        "surname": "Kim",
        "given_name": "Yunghyun"
      },
      {
        "surname": "Hong",
        "given_name": "Youngjoon"
      },
      {
        "surname": "Park",
        "given_name": "Eunbyung"
      }
    ]
  },
  {
    "title": "NFMPAtt-Unet: Neighborhood Fuzzy C-means Multi-scale Pyramid Hybrid Attention Unet for medical image segmentation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106489",
    "abstract": "Medical image segmentation is crucial for understanding anatomical or pathological changes, playing a key role in computer-aided diagnosis and advancing intelligent healthcare. Currently, important issues in medical image segmentation need to be addressed, particularly the problem of segmenting blurry edge regions and the generalizability of segmentation models. Therefore, this study focuses on different medical image segmentation tasks and the issue of blurriness. By addressing these tasks, the study significantly improves diagnostic efficiency and accuracy, contributing to the overall enhancement of healthcare outcomes. To optimize segmentation performance and leverage feature information, we propose a Neighborhood Fuzzy c-Means Multiscale Pyramid Hybrid Attention Unet (NFMPAtt-Unet) model. NFMPAtt-Unet comprises three core components: the Multiscale Dynamic Weight Feature Pyramid module (MDWFP), the Hybrid Weighted Attention mechanism (HWA), and the Neighborhood Rough Set-based Fuzzy c-Means Feature Extraction module (NFCMFE). The MDWFP dynamically adjusts weights across multiple scales, improving feature information capture. The HWA enhances the network’s ability to capture and utilize crucial features, while the NFCMFE, grounded in neighborhood rough set concepts, aids in fuzzy C-means feature extraction, addressing complex structures and uncertainties in medical images, thereby enhancing adaptability. Experimental results demonstrate that NFMPAtt-Unet outperforms state-of-the-art models, highlighting its efficacy in medical image segmentation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004131",
    "keywords": [
      "Artificial intelligence",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Fuzzy logic",
      "Geography",
      "Geometry",
      "Image (mathematics)",
      "Image segmentation",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pyramid (geometry)",
      "Scale (ratio)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Xinpeng"
      },
      {
        "surname": "Xu",
        "given_name": "Weihua"
      }
    ]
  },
  {
    "title": "Compressing neural networks via formal methods",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106411",
    "abstract": "Advancements in Neural Networks have led to larger models, challenging implementation on embedded devices with memory, battery, and computational constraints. Consequently, network compression has flourished, offering solutions to reduce operations and parameters. However, many methods rely on heuristics, often requiring re-training for accuracy. Model reduction techniques extend beyond Neural Networks, relevant in Verification and Performance Evaluation fields. This paper bridges widely-used reduction strategies with formal concepts like lumpability, designed for analyzing Markov Chains. We propose a pruning approach based on lumpability, preserving exact behavioral outcomes without data dependence or fine-tuning. Relaxing strict quotienting method definitions enables a formal understanding of common reduction techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003356",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Machine learning"
    ],
    "authors": [
      {
        "surname": "Ressi",
        "given_name": "Dalila"
      },
      {
        "surname": "Romanello",
        "given_name": "Riccardo"
      },
      {
        "surname": "Rossi",
        "given_name": "Sabina"
      },
      {
        "surname": "Piazza",
        "given_name": "Carla"
      }
    ]
  },
  {
    "title": "Intellectual assessment of amyotrophic lateral sclerosis using deep resemble forward neural network",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106478",
    "abstract": "ALS (Amyotrophic Lateral Sclerosis) is a neurodegenerative disorder causing profound physical disability that severely impairs a patient's life expectancy and quality of life. It also leads to muscular atrophy and progressive weakness of muscles due to insufficient nutrition in the body. At present, there are no disease-modifying therapies to cure ALS, and there is a lack of preventive tools. The general clinical assessments are based on symptom reports, neurophysiological tests, neurological examinations, and neuroimaging. But, these techniques possess various limitations of low reliability, lack of standardized protocols, and lack of sensitivity, especially in the early stages of disease. So, effective methods are required to detect the progression of the disease and minimize the suffering of patients. Extensive studies concentrated on investigating the causes of neurological disease, which creates a barrier to precise identification and classification of genes accompanied with ALS disease. Hence, the proposed system implements a deep RSFFNNCNN (Resemble Single Feed Forward Neural Network-Convolutional Neural Network) algorithm to effectively classify the clinical associations of ALS. It involves the addition of custom weights to the kernel initializer and neutralizer ‘k’ parameter to each hidden layer in the network. This is done to increase the stability and learning ability of the classifier. Additionally, the comparison of the proposed approach is performed with SFNN (Single Feed NN) and ML (Machine Learning) based algorithms, namely, NB (Naïve Bayes), XGBoost (Extreme Gradient Boosting) and RF (Random Forest), to estimate the efficacy of the proposed model. The reliability of the proposed algorithm is measured by deploying performance metrics such as precision, recall, F1 score, and accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004027",
    "keywords": [
      "Amyotrophic lateral sclerosis",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Disease",
      "Machine learning",
      "Medicine",
      "Naive Bayes classifier",
      "Pathology",
      "Physical medicine and rehabilitation",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Alqahtani",
        "given_name": "Abdullah"
      },
      {
        "surname": "Alsubai",
        "given_name": "Shtwai"
      },
      {
        "surname": "Sha",
        "given_name": "Mohemmed"
      },
      {
        "surname": "Dutta",
        "given_name": "Ashit Kumar"
      },
      {
        "surname": "Zhang",
        "given_name": "Yu-Dong"
      }
    ]
  },
  {
    "title": "A unified framework to control estimation error in reinforcement learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106483",
    "abstract": "In reinforcement learning, accurate estimation of the Q-value is crucial for acquiring an optimal policy. However, current successful Actor–Critic methods still suffer from underestimation bias. Additionally, there exists a significant estimation bias, regardless of the method used in the critic initialization phase. To address these challenges and reduce estimation errors, we propose CEILING, a simple and compatible framework that can be applied to any model-free Actor–Critic methods. The core idea of CEILING is to evaluate the superiority of different estimation methods by incorporating the true Q-value, calculated using Monte Carlo, during the training process. CEILING consists of two implementations: the Direct Picking Operation and the Exponential Softmax Weighting Operation. The first implementation selects the optimal method at each fixed step and applies it in subsequent interactions until the next selection. The other implementation utilizes a nonlinear weighting function that dynamically assigns larger weights to more accurate methods. Theoretically, we demonstrate that our methods provide a more accurate and stable Q-value estimation. Additionally, we analyze the upper bound of the estimation bias. Based on two implementations, we propose specific algorithms and their variants, and our methods achieve superior performance on several benchmark tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004076",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Bellman equation",
      "Benchmark (surveying)",
      "Computer science",
      "Geodesy",
      "Geography",
      "Initialization",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Medicine",
      "Nonlinear system",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Radiology",
      "Reinforcement learning",
      "Softmax function",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yujia"
      },
      {
        "surname": "Li",
        "given_name": "Lin"
      },
      {
        "surname": "Wei",
        "given_name": "Wei"
      },
      {
        "surname": "Lv",
        "given_name": "Yunpeng"
      },
      {
        "surname": "Liang",
        "given_name": "Jiye"
      }
    ]
  },
  {
    "title": "Cluster-CAM: Cluster-weighted visual interpretation of CNNs’ decision in image classification",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106473",
    "abstract": "Despite the tremendous success of convolutional neural networks (CNNs) in computer vision, the mechanism of CNNs still lacks clear interpretation. Currently, class activation mapping (CAM), a famous visualization technique to interpret CNN’s decision, has drawn increasing attention. Gradient-based CAMs are efficient, while the performance is heavily affected by gradient vanishing and exploding. In contrast, gradient-free CAMs can avoid computing gradients to produce more understandable results. However, they are quite time-consuming because hundreds of forward interference per image are required. In this paper, we proposed Cluster-CAM, an effective and efficient gradient-free CNN interpretation algorithm. Cluster-CAM can significantly reduce the times of forward propagation by splitting the feature maps into clusters. Furthermore, we propose an artful strategy to forge a cognition-base map and cognition-scissors from clustered feature maps. The final salience heatmap will be produced by merging the above cognition maps. Qualitative results conspicuously show that Cluster-CAM can produce heatmaps where the highlighted regions match the human’s cognition more precisely than existing CAMs. The quantitative evaluation further demonstrates the superiority of Cluster-CAM in both effectiveness and efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003976",
    "keywords": [
      "Artificial intelligence",
      "Cluster (spacecraft)",
      "Cognition",
      "Computer science",
      "Convolutional neural network",
      "Interpretation (philosophy)",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Programming language",
      "Psychology",
      "Salience (neuroscience)",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Feng",
        "given_name": "Zhenpeng"
      },
      {
        "surname": "Ji",
        "given_name": "Hongbing"
      },
      {
        "surname": "Daković",
        "given_name": "Miloš"
      },
      {
        "surname": "Cui",
        "given_name": "Xiyang"
      },
      {
        "surname": "Zhu",
        "given_name": "Mingzhe"
      },
      {
        "surname": "Stanković",
        "given_name": "Ljubiša"
      }
    ]
  },
  {
    "title": "FDAA: A feature distribution-aware transferable adversarial attack method",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106467",
    "abstract": "In recent years, the research on transferable feature-level adversarial attack has become a hot spot due to attacking unknown deep neural networks successfully. But the following problems limit its transferability. Existing feature disruption methods often focus on computing feature weights precisely, while overlooking the noise influence of feature maps, which results in disturbing non-critical features. Meanwhile, geometric augmentation algorithms are used to enhance image diversity but compromise information integrity, which hamper models from capturing comprehensive features. Furthermore, current feature perturbation could not pay attention to the density distribution of object-relevant key features, which mainly concentrate in salient region and fewer in the most distributed background region, and get limited transferability. To tackle these challenges, a feature distribution-aware transferable adversarial attack method, called FDAA, is proposed to implement distinct strategies for different image regions in the paper. A novel Aggregated Feature Map Attack (AFMA) is presented to significantly denoise feature maps, and an input transformation strategy, called Smixup, is introduced to help feature disruption algorithms to capture comprehensive features. Extensive experiments demonstrate that scheme proposed achieves better transferability with an average success rate of 78.6% on adversarially trained models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003915",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Linguistics",
      "Logit",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Salient",
      "Transferability"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Jiachun"
      },
      {
        "surname": "Hu",
        "given_name": "Yuchao"
      },
      {
        "surname": "Yan",
        "given_name": "Cheng"
      }
    ]
  },
  {
    "title": "Aligning the domains in cross domain model inversion attack",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106490",
    "abstract": "Model Inversion Attack reconstructs confidential training dataset from a target deep learning model. Most of the existing methods assume the adversary has an auxiliary dataset that has similar distribution with the private dataset. However, this assumption does not always hold in real-world scenarios. Since the private dataset is unknown, the domain divergence between the auxiliary dataset and the private dataset is inevitable. In this paper, we use Cross Domain Model Inversion Attack to represent the distribution divergence scenario in MIA. With the distribution divergence between the private images and auxiliary images, the distribution between the feature vectors of the private images and those of the auxiliary images is also different. Moreover, the outputted prediction vectors of the auxiliary images are also misclassified. The inversion attack is thus hard to be performed. We perform both the feature vector inversion task and prediction vector inversion task in this cross domain setting. For feature vector inversion, Domain Alignment MIA (DA-MIA) is proposed. While performing the reconstruction task, DA-MIA aligns the feature vectors of auxiliary images with the feature vectors of private images in an adversarial manner to mitigate the domain divergence between them. Thus, semantically meaningful images can be reconstructed. For prediction vector inversion, we further introduce an auxiliary classifier and propose Domain Alignment MIA with Auxiliary Classifier (DA-MIA-AC). The auxiliary classifier is pretrained by the auxiliary dataset and fine-tuned during the adversarial training stage. Thus, the misclassification problem caused by domain divergence can be solved, and the images can be reconstructed correctly. Various experiments are performed to show the advancement of our methods, the results show that DA-MIA can improve the SSIM score of the reconstructed images for up to 191%, DA-MIA-AC can increase the classification accuracy score of the reconstructed images from 9.18% to 81.32% in Cross Domain Model Inversion Attack.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004143",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Biology",
      "Classifier (UML)",
      "Computer science",
      "Divergence (linguistics)",
      "Feature vector",
      "Inversion (geology)",
      "Linguistics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Structural basin",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zeping"
      },
      {
        "surname": "Huang",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "Attention-based Sparse and Collaborative Spectral Abundance Learning for Hyperspectral Subpixel Target Detection",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106416",
    "abstract": "The subpixel target detection in hyperspectral image processing persists as a formidable challenge. In this paper, we present a novel subpixel target detector termed attention-based sparse and collaborative spectral abundance learning for subpixel target detection in hyperspectral images. To help suppress background during subpixel target detection, the proposed method presents a pixel attention-based background sample selection method for background dictionary construction. Besides, the proposed method integrates a band attention-based spectral abundance learning model, replete with sparse and collaborative constraints, in which the band attention map can contribute to enhancing the discriminative ability of the detector in identifying targets from backgrounds. Ultimately, the detection result of the proposed detector is achieved by the learned target spectral abundance after solving the designed model using the alternating direction method of multipliers algorithm. Rigorous experiments conducted on four benchmark datasets, including one simulated and three real-world datasets, validate the effectiveness of the detector with the probability of detection of 90.88%, 96.86%, and 97.79% on the PHI, RIT Campus, and Reno Urban data, respectively, under fixed false alarm rate equal 0.01, indicating that the proposed method yields superior hyperspectral subpixel detection performance and outperforms existing methodologies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400340X",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Computer vision",
      "Constant false alarm rate",
      "Detector",
      "Discriminative model",
      "False alarm",
      "Geodesy",
      "Geography",
      "Hyperspectral imaging",
      "Object detection",
      "Pattern recognition (psychology)",
      "Pixel",
      "Subpixel rendering",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Dehui"
      },
      {
        "surname": "Zhong",
        "given_name": "Ping"
      },
      {
        "surname": "Du",
        "given_name": "Bo"
      },
      {
        "surname": "Zhang",
        "given_name": "Liangpei"
      }
    ]
  },
  {
    "title": "A novel bounded loss framework for support vector machines",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106476",
    "abstract": "This paper introduces a novel bounded loss framework for SVM and SVR. Specifically, using the Pinball loss as an illustration, we devise a novel bounded exponential quantile loss ( L eq -loss) for both support vector machine classification and regression tasks. For L eq -loss, it not only enhances the robustness of SVM and SVR against outliers but also improves the robustness of SVM to resampling from a different perspective. Furthermore, EQSVM and EQSVR were constructed based on L eq -loss, and the influence functions and breakdown point lower bounds of their estimators are derived. It is proved that the influence functions are bounded, and the breakdown point lower bounds can reach the highest asymptotic breakdown point of 1 / 2 . Additionally, we demonstrated the robustness of EQSVM to resampling and derived its generalization error bound based on Rademacher complexity. Due to the L eq -loss being non-convex, we can use the concave–convex procedure (CCCP) technique to transform the problem into a series of convex optimization problems and use the ClipDCD algorithm to solve these convex optimization problems. Numerous experiments have been conducted to confirm the effectiveness of the proposed EQSVM and EQSVR.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004003",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bounded function",
      "Computer science",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Feihong"
      },
      {
        "surname": "Yang",
        "given_name": "Hu"
      }
    ]
  },
  {
    "title": "Distillation of multi-class cervical lesion cell detection via synthesis-aided pre-training and patch-level feature alignment",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106405",
    "abstract": "Automated detection of cervical abnormal cells from Thin-prep cytologic test (TCT) images is crucial for efficient cervical abnormal screening using computer-aided diagnosis systems. However, the construction of the detection model is hindered by the preparation of the training images, which usually suffers from issues of class imbalance and incomplete annotations. Additionally, existing methods often overlook the visual feature correlations among cells, which are crucial in cervical lesion cell detection as pathologists commonly rely on surrounding cells for identification. In this paper, we propose a distillation framework that utilizes a patch-level pre-training network to guide the training of an image-level detection network, which can be applied to various detectors without changing their architectures during inference. The main contribution is three-fold: (1) We propose the Balanced Pre-training Model (BPM) as the patch-level cervical cell classification model, which employs an image synthesis model to construct a class-balanced patch dataset for pre-training. (2) We design the Score Correction Loss (SCL) to enable the detection network to distill knowledge from the BPM model, thereby mitigating the impact of incomplete annotations. (3) We design the Patch Correlation Consistency (PCC) strategy to exploit the correlation information of extracted cells, consistent with the behavior of cytopathologists. Experiments on public and private datasets demonstrate the superior performance of the proposed distillation method, as well as its adaptability to various detection architectures.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003290",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Computer vision",
      "Computer-aided",
      "Feature (linguistics)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Fei",
        "given_name": "Manman"
      },
      {
        "surname": "Shen",
        "given_name": "Zhenrong"
      },
      {
        "surname": "Song",
        "given_name": "Zhiyun"
      },
      {
        "surname": "Wang",
        "given_name": "Xin"
      },
      {
        "surname": "Cao",
        "given_name": "Maosong"
      },
      {
        "surname": "Yao",
        "given_name": "Linlin"
      },
      {
        "surname": "Zhao",
        "given_name": "Xiangyu"
      },
      {
        "surname": "Wang",
        "given_name": "Qian"
      },
      {
        "surname": "Zhang",
        "given_name": "Lichi"
      }
    ]
  },
  {
    "title": "An adaptive kernel dictionary-based low-rank representation method for subspace clustering",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106434",
    "abstract": "Low-rank representation (LRR) is a classic subspace clustering (SC) algorithm, and many LRR-based methods have been proposed. Generally, LRR-based methods use denoized data as dictionaries for data reconstruction purpose. However, the dictionaries used in LRR-based algorithms are fixed, leading to poor clustering performance. In addition, most of these methods assume that the input data are linearly correlated. However, in practice, data are mostly nonlinearly correlated. To address these problems, we propose a novel adaptive kernel dictionary-based LRR (AKDLRR) method for SC. Specifically, to explore nonlinear information, the given data are mapped to the Hilbert space via the kernel technique. The dictionary in AKDLRR is not fixed; it adaptively learns from the data in the kernel space, making AKDLRR robust to noise and yielding good clustering performance. To solve the AKDLRR model, an efficient procedure including an alternative optimization strategy is proposed. In addition, a theoretical analysis of the convergence performance of AKDLRR is presented, which reveals that AKDLRR can converge in at most three iterations under certain conditions. The experimental results show that AKDLRR can achieve the best clustering performance and has excellent speed in comparison with other algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003587",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Combinatorics",
      "Computer science",
      "Data mining",
      "Hilbert space",
      "Image (mathematics)",
      "Kernel (algebra)",
      "Kernel method",
      "Law",
      "Mathematical analysis",
      "Mathematics",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Rank (graph theory)",
      "Representation (politics)",
      "Reproducing kernel Hilbert space",
      "Subspace topology",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Kan",
        "given_name": "Yaozu"
      },
      {
        "surname": "Lu",
        "given_name": "Gui-Fu"
      },
      {
        "surname": "Du",
        "given_name": "Yangfan"
      }
    ]
  },
  {
    "title": "A spinal circuit model with asymmetric cervical-lumbar layout controls backward locomotion and scratching in quadrupeds",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106422",
    "abstract": "Locomotion and scratching are basic motor functions which are critically important for animal survival. Although the spinal circuits governing forward locomotion have been extensively investigated, the organization of spinal circuits and neural mechanisms regulating backward locomotion and scratching remain unclear. Here, we extend a model by Danner et al. to propose a spinal circuit model with asymmetrical cervical-lumbar layout to investigate these issues. In the model, the left–right alternation within the cervical and lumbar circuits is mediated by V 0 D and V 0 V commissural interneurons (CINs), respectively. With different control strategies, the model closely reproduces multiple experimental data of quadrupeds in different motor behaviors. Specifically, under the supraspinal drive, walk and trot are expressed in control condition, half-bound is expressed after deletion of V 0 V CINs, and bound is expressed after deletion of V0 ( V 0 D and V 0 V ) CINs; in addition, unilateral hindlimb scratching occurs in control condition and synchronous bilateral hindlimb scratching appears after deletion of V 0 V CINs. Under the combined drive of afferent feedback and perineal stimulation, different coordination patterns between hindlimbs during BBS (backward-biped-spinal) locomotion are generated. The results suggest that (1) the cervical and lumbar circuits in the spinal network are asymmetrically recruited during particular rhythmic limb movements. (2) Multiple motor behaviors share a single spinal network under the reconfiguration of the spinal network by supraspinal inputs or somatosensory feedback. Our model provides new insights into the organization of motor circuits and neural control of rhythmic limb movements.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003460",
    "keywords": [
      "Acoustics",
      "Anatomy",
      "Biological neural network",
      "Biology",
      "Central pattern generator",
      "Hindlimb",
      "Internal medicine",
      "Lumbar",
      "Medicine",
      "Motor control",
      "Neuroscience",
      "Physics",
      "Rhythm",
      "Scratching",
      "Spinal cord"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Qinghua"
      },
      {
        "surname": "Han",
        "given_name": "Fang"
      },
      {
        "surname": "Yu",
        "given_name": "Ying"
      },
      {
        "surname": "Wang",
        "given_name": "Fengjie"
      },
      {
        "surname": "Wang",
        "given_name": "Qingyun"
      },
      {
        "surname": "Shakeel",
        "given_name": "Awais"
      }
    ]
  },
  {
    "title": "Spiking generative adversarial network with attention scoring decoding",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106423",
    "abstract": "Generative models based on neural networks present a substantial challenge within deep learning. As it stands, such models are primarily limited to the domain of artificial neural networks. Spiking neural networks, as the third generation of neural networks, offer a closer approximation to brain-like processing due to their rich spatiotemporal dynamics. However, generative models based on spiking neural networks are not well studied. Particularly, previous works on generative adversarial networks based on spiking neural networks are conducted on simple datasets and do not perform well. In this work, we pioneer constructing a spiking generative adversarial network capable of handling complex images and having higher performance. Our first task is to identify the problems of out-of-domain inconsistency and temporal inconsistency inherent in spiking generative adversarial networks. We address these issues by incorporating the Earth-Mover distance and an attention-based weighted decoding method, significantly enhancing the performance of our algorithm across several datasets. Experimental results reveal that our approach outperforms existing methods on the MNIST, FashionMNIST, CIFAR10, and CelebA. In addition to our examination of static datasets, this study marks our inaugural investigation into event-based data, through which we achieved noteworthy results. Moreover, compared with hybrid spiking generative adversarial networks, where the discriminator is an artificial analog neural network, our methodology demonstrates closer alignment with the information processing patterns observed in the mouse. Our code can be found at https://github.com/Brain-Cog-Lab/sgad.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003472",
    "keywords": [
      "Adversarial system",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Decoding methods",
      "Deep learning",
      "Generative adversarial network",
      "Generative grammar",
      "Machine learning",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Feng",
        "given_name": "Linghao"
      },
      {
        "surname": "Zhao",
        "given_name": "Dongcheng"
      },
      {
        "surname": "Zeng",
        "given_name": "Yi"
      }
    ]
  },
  {
    "title": "Learning sequence attractors in recurrent networks with hidden neurons",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106466",
    "abstract": "The brain is targeted for processing temporal sequence information. It remains largely unclear how the brain learns to store and retrieve sequence memories. Here, we study how recurrent networks of binary neurons learn sequence attractors to store predefined pattern sequences and retrieve them robustly. We show that to store arbitrary pattern sequences, it is necessary for the network to include hidden neurons even though their role in displaying sequence memories is indirect. We develop a local learning algorithm to learn sequence attractors in the networks with hidden neurons. The algorithm is proven to converge and lead to sequence attractors. We demonstrate that the network model can store and retrieve sequences robustly on synthetic and real-world datasets. We hope that this study provides new insights in understanding sequence memory and temporal information processing in the brain.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003903",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Attractor",
      "Biology",
      "Computer science",
      "Genetics",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Recurrent neural network",
      "Sequence (biology)",
      "Sequence learning"
    ],
    "authors": [
      {
        "surname": "Lu",
        "given_name": "Yao"
      },
      {
        "surname": "Wu",
        "given_name": "Si"
      }
    ]
  },
  {
    "title": "Omnidirectional image super-resolution via position attention network",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106464",
    "abstract": "For convenient transmission, omnidirectional images (ODIs) usually follow the equirectangular projection (ERP) format and are low-resolution. To provide better immersive experience, omnidirectional image super resolution (ODISR) is essential. However, ERP ODIs suffer from serious geometric distortion and pixel stretching across latitudes, generating massive redundant information at high latitudes. This characteristic poses a huge challenge for the traditional SR methods, which can only obtain the suboptimal ODISR performance. To address this issue, we propose a novel position attention network (PAN) for ODISR in this paper. Specifically, a two-branch structure is introduced, in which the basic enhancement branch (BE) serves to achieve coarse deep feature enhancement for extracted shallow features. Meanwhile, the position attention enhancement branch (PAE) builds a positional attention mechanism to dynamically adjust the contribution of features at different latitudes in the ERP representation according to their positions and stretching degrees, which achieves the enhancement for the differentiated information, suppresses the redundant information, and modulate the deep features with spatial distortion. Subsequently, the features of two branches are fused effectively to achieve the further refinement and adapt the distortion characteristic of ODIs. After that, we exploit a long-term memory module (LM), promoting information interactions and fusions between the branches to enhance the perception of the distortion, aggregating the prior hierarchical features to keep the long-term memory and boosting the ODISR performance. Extensive results demonstrate the state-of-the-art performance and the high efficiency of our PAN in ODISR.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003885",
    "keywords": [
      "Amplifier",
      "Antenna (radio)",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Boosting (machine learning)",
      "Computer science",
      "Computer vision",
      "Distortion (music)",
      "Economics",
      "Feature (linguistics)",
      "Finance",
      "Linguistics",
      "Omnidirectional antenna",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Position (finance)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xin"
      },
      {
        "surname": "Wang",
        "given_name": "Shiqi"
      },
      {
        "surname": "Li",
        "given_name": "Jinxing"
      },
      {
        "surname": "Li",
        "given_name": "Mu"
      },
      {
        "surname": "Li",
        "given_name": "Jinkai"
      },
      {
        "surname": "Xu",
        "given_name": "Yong"
      }
    ]
  },
  {
    "title": "Containment control for fractional-order networked system with intermittent sampled position communication",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106425",
    "abstract": "This paper investigates containment control for fractional-order networked systems. Two novel intermittent sampled position communication protocols, where controllers only need to keep working during communication width of every sampling period under the past sampled position communication of neighbors’ agents. Then, some necessary and sufficient conditions are derived to guarantee containment about the differential order, sampling period, communication width, coupling strengths, and networked structure. Taking into account of the delay, a detailed discussion to guarantee containment is given with respect to the delay, sampling period, and communication width. Interestingly, it is discovered that containment control cannot be guaranteed without delay or past sampled position communication under the proposed protocols. Finally, the effectiveness of theoretical results is demonstrated by some numerical simulations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003496",
    "keywords": [
      "Artificial intelligence",
      "Communications system",
      "Computer network",
      "Computer science",
      "Containment (computer programming)",
      "Control (management)",
      "Control theory (sociology)",
      "Detector",
      "Economics",
      "Finance",
      "Position (finance)",
      "Programming language",
      "Sampling (signal processing)",
      "Telecommunications",
      "Telecommunications network"
    ],
    "authors": [
      {
        "surname": "Ye",
        "given_name": "Yanyan"
      },
      {
        "surname": "Chen",
        "given_name": "Hongzhe"
      },
      {
        "surname": "Tao",
        "given_name": "Jie"
      },
      {
        "surname": "Cai",
        "given_name": "Qianqian"
      },
      {
        "surname": "Shi",
        "given_name": "Peng"
      }
    ]
  },
  {
    "title": "Adaptively identify and refine ill-posed regions for accurate stereo matching",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106394",
    "abstract": "Stereo matching cost constrains the consistency between pixel pairs. However, the consistency constraint becomes unreliable in ill-posed regions such as occluded or ambiguous regions of the images, making it difficult to explore hidden correspondences. To address this challenge, we introduce an Error-area Feature Refinement Mechanism (EFR) that supplies context features for ill-posed regions. In EFR, we innovatively obtain the suspected error region according to aggregation perturbations, then a simple Transformer module is designed to synthesize global context and correspondence relation with the identified error mask. To better overcome existing texture overfitting, we put forward a Dual-constraint Cost Volume (DCV) that integrates supplementary constraints. This effectively improves the robustness and diversity of disparity clues, resulting in enhanced details and structural accuracy. Finally, we propose a highly accurate stereo matching network called Error-rectify Feature Guided Stereo Matching Network (ERCNet), which is based on DCV and EFR. We evaluate our model on several benchmark datasets, achieving state-of-the-art performance and demonstrating excellent generalization across datasets. The code is available at https://github.com/dean7liu/ERCNet_2023.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003186",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Changlin"
      },
      {
        "surname": "Sun",
        "given_name": "Linjun"
      },
      {
        "surname": "Ning",
        "given_name": "Xin"
      },
      {
        "surname": "Xu",
        "given_name": "Jian"
      },
      {
        "surname": "Yu",
        "given_name": "Lina"
      },
      {
        "surname": "Zhang",
        "given_name": "Kaijie"
      },
      {
        "surname": "Li",
        "given_name": "Weijun"
      }
    ]
  },
  {
    "title": "A topological description of loss surfaces based on Betti Numbers",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106465",
    "abstract": "In the context of deep learning models, attention has recently been paid to studying the surface of the loss function in order to better understand training with methods based on gradient descent. This search for an appropriate description, both analytical and topological, has led to numerous efforts in identifying spurious minima and characterize gradient dynamics. Our work aims to contribute to this field by providing a topological measure for evaluating loss complexity in the case of multilayer neural networks. We compare deep and shallow architectures with common sigmoidal activation functions by deriving upper and lower bounds for the complexity of their respective loss functions and revealing how that complexity is influenced by the number of hidden units, training models, and the activation function used. Additionally, we found that certain variations in the loss function or model architecture, such as adding an ℓ 2 regularization term or implementing skip connections in a feedforward network, do not affect loss topology in specific cases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003897",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Betti number",
      "Combinatorics",
      "Computer science",
      "Mathematics",
      "Pure mathematics",
      "Topological data analysis",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Bucarelli",
        "given_name": "Maria Sofia"
      },
      {
        "surname": "D’Inverno",
        "given_name": "Giuseppe Alessio"
      },
      {
        "surname": "Bianchini",
        "given_name": "Monica"
      },
      {
        "surname": "Scarselli",
        "given_name": "Franco"
      },
      {
        "surname": "Silvestri",
        "given_name": "Fabrizio"
      }
    ]
  },
  {
    "title": "GeneWorker: An end-to-end robotic reinforcement learning approach with collaborative generator and worker networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106472",
    "abstract": "Reinforcement learning aided by the skill conception exhibits potent capabilities in guiding autonomous agents toward acquiring meaningful behaviors. However, in the current landscape of reinforcement learning, a skill is often merely a rudimentary abstraction of a sequence of primitive actions, serving as a component of the input to policy networks with fixed network parameters. This rigid methodology presents obstacles when attempting to integrate with burgeoning techniques such as meta-learning and large language models. To address this issue, we introduce a unique neural skill representation that abstracts the activation of neurons in each neural layer. Based on this, a novel end-to-end robotic reinforcement learning algorithm is proposed, in which two sub-networks, i.e., generator and worker networks, implement collaborative inferences via neural skills. Specifically, the generator produces a series of multi-spatial neural skills, providing efficient guidance for subsequent decision-making; by integrating these skills, the worker can determine its own network weights and biases to cope with various environmental conditions. Therefore, actions can be sampled with flexibly changeable network parameters through the collaboration between generator and worker networks. The experiments demonstrate that GeneWorker can achieve a mean success rate of over 90.67% on continuous robotic tasks and outperforms previous state-of-the-art methods by a minimum of 54% on the pick-and-place task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003964",
    "keywords": [
      "Abstraction",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "End-to-end principle",
      "Epistemology",
      "Generator (circuit theory)",
      "Genetics",
      "Law",
      "Machine learning",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Power (physics)",
      "Quantum mechanics",
      "Reinforcement learning",
      "Representation (politics)",
      "Robot",
      "Sequence (biology)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Hao"
      },
      {
        "surname": "Man",
        "given_name": "Hengyu"
      },
      {
        "surname": "Cui",
        "given_name": "Wenxue"
      },
      {
        "surname": "Lu",
        "given_name": "Riyu"
      },
      {
        "surname": "Cai",
        "given_name": "Chenxin"
      },
      {
        "surname": "Fan",
        "given_name": "Xiaopeng"
      }
    ]
  },
  {
    "title": "Unifying emotion-oriented and cause-oriented predictions for emotion-cause pair extraction",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106431",
    "abstract": "Emotion-cause pair extraction (ECPE) is an extraction task aiming to simultaneously identify the emotions and causes from the text without emotion annotations. Let c i and c j represent the emotion clause and the cause clause of a document, respectively, and we can predict one from the other and vice versa. Previous works fail to take advantage of this bidirectional opportunity. We refer to the prediction from c i to c j , i.e., c i → c j , as an emotion-oriented cause prediction (EoCP) task and the prediction from c j to c i , i.e., c j → c i , as a cause-oriented emotion prediction (CoEP) task. After redefining the ECPE task, we propose a novel unified architecture for ECPE, which incorporates EoCP and CoEP as cells and unifies them into a single-chain architecture. Additionally, we redefine emotion-cause pair extraction as a closed-loop structure detection problem to alleviate the mismatch between emotion and cause clauses. To enhance the training of the architecture, we provide a procedure for estimating the confidence of the extraction system for its emotion-cause pairs. We demonstrate the superiority of our proposed model through extensive experiments on two public datasets, achieving a new state-of-the-art performance. Furthermore, our method particularly achieves significant improvements in multiple emotion-cause pair extraction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003551",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Cognitive psychology",
      "Computer science",
      "Extraction (chemistry)",
      "Psychology"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Guimin"
      },
      {
        "surname": "Zhao",
        "given_name": "Yi"
      },
      {
        "surname": "Lu",
        "given_name": "Guangming"
      }
    ]
  },
  {
    "title": "Input-to-state stability of delayed memristor-based inertial neural networks via non-reduced order method",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106545",
    "abstract": "This paper is concerned with the input-to-state stability (ISS) for a kind of delayed memristor-based inertial neural networks (DMINNs). Based on the nonsmooth analysis and stability theory, novel delay-dependent and delay-independent criteria on the ISS of DMINNs are obtained by constructing different Lyapunov functions. Moreover, compared with the reduced order approach used in the previous works, this paper consider the ISS of DMINNs via non-reduced order approach. Directly analysis the model of DMINNs can better maintain its physical backgrounds, which reduces the complexity of calculations and is more rigorous in practical application. Additionally, the novel proposed results on the ISS of DMINNs here incorporate and complement the existing studies on memristive neural network dynamical systems. Lastly, a numerical example is provided to show that the obtained criteria are reliable.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024004696",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Electronic engineering",
      "Engineering",
      "Inertial frame of reference",
      "Machine learning",
      "Memristor",
      "Physics",
      "Quantum mechanics",
      "Stability (learning theory)",
      "State (computer science)"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Yuxin"
      },
      {
        "surname": "Zhu",
        "given_name": "Song"
      },
      {
        "surname": "Liu",
        "given_name": "Xiaoyang"
      },
      {
        "surname": "Wen",
        "given_name": "Shiping"
      },
      {
        "surname": "Mu",
        "given_name": "Chaoxu"
      }
    ]
  },
  {
    "title": "Self-balancing Incremental Broad Learning System with privacy protection",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106436",
    "abstract": "Incremental learning algorithms have been developed as an efficient solution for fast remodeling in Broad Learning Systems (BLS) without a retraining process. Even though the structure and performance of broad learning are gradually showing superiority, private data leakage in broad learning systems is still a problem that needs to be solved. Recently, Multiparty Secure Broad Learning System (MSBLS) is proposed to allow two clients to participate training. However, privacy-preserving broad learning across multiple clients has received limited attention. In this paper, we propose a Self-Balancing Incremental Broad Learning System (SIBLS) with privacy protection by considering the effect of different data sample sizes from clients, which allows multiple clients to be involved in the incremental learning. Specifically, we design a client selection strategy to select two clients in each round by reducing the gap in the number of data samples in the incremental updating process. To ensure the security under the participation of multiple clients, we introduce a mediator in the data encryption and feature mapping process. Three classical datasets are used to validate the effectiveness of our proposed SIBLS, including MNIST, Fashion and NORB datasets. Experimental results show that our proposed SIBLS can have comparable performance with MSBLS while achieving better performance than federated learning in terms of accuracy and running time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003605",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Computer security",
      "Data mining",
      "Deep learning",
      "Encryption",
      "Feature selection",
      "Incremental learning",
      "International trade",
      "MNIST database",
      "Machine learning",
      "Operating system",
      "Process (computing)",
      "Retraining"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Weiwen"
      },
      {
        "surname": "Liu",
        "given_name": "Ziyu"
      },
      {
        "surname": "Jiang",
        "given_name": "Yifeng"
      },
      {
        "surname": "Chen",
        "given_name": "Wuxing"
      },
      {
        "surname": "Zhao",
        "given_name": "Bowen"
      },
      {
        "surname": "Yang",
        "given_name": "Kaixiang"
      }
    ]
  },
  {
    "title": "Advancing neural network calibration: The role of gradient decay in large-margin Softmax optimization",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106457",
    "abstract": "This study introduces a novel hyperparameter in the Softmax function to regulate the rate of gradient decay, which is dependent on sample probability. Our theoretical and empirical analyses reveal that both model generalization and calibration are significantly influenced by the gradient decay rate, particularly as confidence probability increases. Notably, the gradient decay varies in a convex or concave manner with rising sample probability. When employing a smaller gradient decay, we observe a curriculum learning sequence. This sequence highlights hard samples only after easy samples are adequately trained, and allows well-separated samples to receive a higher gradient, effectively reducing intra-class distances. However, this approach has a drawback: small gradient decay tends to exacerbate model overconfidence, shedding light on the calibration issues prevalent in modern neural networks. In contrast, a larger gradient decay addresses these issues effectively, surpassing even models that utilize post-calibration methods. Our findings provide substantial evidence that large margin Softmax can influence the local Lipschitz constraint by manipulating the probability-dependent gradient decay rate. This research contributes a fresh perspective and understanding of the interplay between large margin Softmax, curriculum learning, and model calibration through an exploration of gradient decay rates. Additionally, we propose a novel warm-up strategy that dynamically adjusts the gradient decay for a smoother L -constraint in early training, then mitigating overconfidence in the final model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003812",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Calibration",
      "Computer science",
      "Gradient descent",
      "Lipschitz continuity",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematical analysis",
      "Mathematics",
      "Overconfidence effect",
      "Psychology",
      "Social psychology",
      "Softmax function",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Siyuan"
      },
      {
        "surname": "Xie",
        "given_name": "Linbo"
      }
    ]
  },
  {
    "title": "Detail-preserving image warping by enforcing smooth image sampling",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106426",
    "abstract": "Multi-phase dynamic contrast-enhanced magnetic resonance imaging image registration makes a substantial contribution to medical image analysis. However, existing methods (e.g., VoxelMorph, CycleMorph) often encounter the problem of image information misalignment in deformable registration tasks, posing challenges to the practical application. To address this issue, we propose a novel smooth image sampling method to align full organic information to realize detail-preserving image warping. In this paper, we clarify that the phenomenon about image information mismatch is attributed to imbalanced sampling. Then, a sampling frequency map constructed by sampling frequency estimators is utilized to instruct smooth sampling by reducing the spatial gradient and discrepancy between all-ones matrix and sampling frequency map. In addition, our estimator determines the sampling frequency of a grid voxel in the moving image by aggregating the sum of interpolation weights from warped non-grid sampling points in its vicinity and vectorially constructs sampling frequency map through projection and scatteration. We evaluate the effectiveness of our approach through experiments on two in-house datasets. The results showcase that our method preserves nearly complete details with ideal registration accuracy compared with several state-of-the-art registration methods. Additionally, our method exhibits a statistically significant difference in the regularity of the registration field compared to other methods, at a significance level of p < 0.05. Our code will be released at https://github.com/QingRui-Sha/SFM.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003502",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Filter (signal processing)",
      "Image (mathematics)",
      "Image warping",
      "Pattern recognition (psychology)",
      "Sampling (signal processing)"
    ],
    "authors": [
      {
        "surname": "Sha",
        "given_name": "Qingrui"
      },
      {
        "surname": "Sun",
        "given_name": "Kaicong"
      },
      {
        "surname": "Jiang",
        "given_name": "Caiwen"
      },
      {
        "surname": "Xu",
        "given_name": "Mingze"
      },
      {
        "surname": "Xue",
        "given_name": "Zhong"
      },
      {
        "surname": "Cao",
        "given_name": "Xiaohuan"
      },
      {
        "surname": "Shen",
        "given_name": "Dinggang"
      }
    ]
  },
  {
    "title": "Sliding mode control for uncertain fractional-order reaction–diffusion memristor neural networks with time delays",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106402",
    "abstract": "This paper investigates a sliding mode control method for a class of uncertain delayed fractional-order reaction–diffusion memristor neural networks. Different from most existing literature on sliding mode control for fractional-order reaction–diffusion systems, this study constructs a linear sliding mode switching function and designs the corresponding sliding mode control law. The sufficient theory for the globally asymptotic stability of the sliding mode dynamics are provided, and it is proven that the sliding mode surface is finite-time reachable under the proposed control law, with an estimate of the maximum reaching time. Finally, a numerical test is presented to validate the effectiveness of the theoretical analysis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003265",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Diffusion",
      "Economics",
      "Electronic engineering",
      "Engineering",
      "Finance",
      "Mathematical analysis",
      "Mathematics",
      "Memristor",
      "Mode (computer interface)",
      "Nonlinear system",
      "Operating system",
      "Order (exchange)",
      "Physics",
      "Quantum mechanics",
      "Reaction–diffusion system",
      "Sliding mode control",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Cao",
        "given_name": "Yue"
      },
      {
        "surname": "Kao",
        "given_name": "Yonggui"
      },
      {
        "surname": "Wang",
        "given_name": "Zhen"
      },
      {
        "surname": "Yang",
        "given_name": "Xinsong"
      },
      {
        "surname": "Park",
        "given_name": "Ju H."
      },
      {
        "surname": "Xie",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "An invisible, robust copyright protection method for DNN-generated content",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106391",
    "abstract": "Wide deployment of deep neural networks (DNNs) based applications (e.g., style transfer, cartoonish), stimulating the need for copyright protection of such application’s production. Though some traditional visible copyright techniques exist, they often introduce undesired artifacts and compromise the aesthetic quality of the images. In this paper, we propose a novel invisible, robust copyright protection method, which is composed of two networks: the copyright encoder and the copyright decoder. The former projects the copyright information to the invisible perturbation with the drive of both the input of images and copyright information, thereby adding it to the image and yielding encoded images. The copyright decoder extracts copyright information from encoded images. Moreover, a robustness module is integrated to enhance the decoder’s ability to decipher images against various distortions encountered on social media platforms. Furthermore, the loss function is elaborately designed, taking into account both feature space and color space, to guarantee the quality of encoded and decoded copyright images. Extensively objective and subjective experiments validate the effectiveness of the proposed method. Additionally, the physical test is conducted by posting the encoded images to social media (e.g., Weibo and Twitter) and downloading them to verify the feasibility of the proposed method in practice.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003150",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Content (measure theory)",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Donghua"
      },
      {
        "surname": "Yao",
        "given_name": "Wen"
      },
      {
        "surname": "Jiang",
        "given_name": "Tingsong"
      },
      {
        "surname": "Zhou",
        "given_name": "Weien"
      },
      {
        "surname": "Lin",
        "given_name": "Lang"
      },
      {
        "surname": "Chen",
        "given_name": "Xiaoqian"
      }
    ]
  },
  {
    "title": "EWT: Efficient Wavelet-Transformer for single image denoising",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106378",
    "abstract": "Transformer-based image denoising methods have shown remarkable potential but suffer from high computational cost and large memory footprint due to their linear operations for capturing long-range dependencies. In this work, we aim to develop a more resource-efficient Transformer-based image denoising method that maintains high performance. To this end, we propose an Efficient Wavelet Transformer (EWT), which incorporates a Frequency-domain Conversion Pipeline (FCP) to reduce image resolution without losing critical features, and a Multi-level Feature Aggregation Module (MFAM) with a Dual-stream Feature Extraction Block (DFEB) to harness hierarchical features effectively. EWT achieves a faster processing speed by over 80% and reduces GPU memory usage by more than 60% compared to the original Transformer, while still delivering denoising performance on par with state-of-the-art methods. Extensive experiments show that EWT significantly improves the efficiency of Transformer-based image denoising, providing a more balanced approach between performance and resource consumption.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003022",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Image denoising",
      "Noise reduction",
      "Pattern recognition (psychology)",
      "Transformer",
      "Voltage",
      "Wavelet"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Juncheng"
      },
      {
        "surname": "Cheng",
        "given_name": "Bodong"
      },
      {
        "surname": "Chen",
        "given_name": "Ying"
      },
      {
        "surname": "Gao",
        "given_name": "Guangwei"
      },
      {
        "surname": "Shi",
        "given_name": "Jun"
      },
      {
        "surname": "Zeng",
        "given_name": "Tieyong"
      }
    ]
  },
  {
    "title": "SSTE: Syllable-Specific Temporal Encoding to FORCE-learn audio sequences with an associative memory approach",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106368",
    "abstract": "The circuitry and pathways in the brains of humans and other species have long inspired researchers and system designers to develop accurate and efficient systems capable of solving real-world problems and responding in real-time. We propose the Syllable-Specific Temporal Encoding (SSTE) to learn vocal sequences in a reservoir of Izhikevich neurons, by forming associations between exclusive input activities and their corresponding syllables in the sequence. Our model converts the audio signals to cochleograms using the CAR-FAC model to simulate a brain-like auditory learning and memorization process. The reservoir is trained using a hardware-friendly approach to FORCE learning. Reservoir computing could yield associative memory dynamics with far less computational complexity compared to RNNs. The SSTE-based learning enables competent accuracy and stable recall of spatiotemporal sequences with fewer reservoir inputs compared with existing encodings in the literature for similar purpose, offering resource savings. The encoding points to syllable onsets and allows recalling from a desired point in the sequence, making it particularly suitable for recalling subsets of long vocal sequences. The SSTE demonstrates the capability of learning new signals without forgetting previously memorized sequences and displays robustness against occasional noise, a characteristic of real-world scenarios. The components of this model are configured to improve resource consumption and computational intensity, addressing some of the cost-efficiency issues that might arise in future implementations aiming for compactness and real-time, low-power operation. Overall, this model proposes a brain-inspired pattern generation network for vocal sequences that can be extended with other bio-inspired computations to explore their potentials for brain-like auditory perception. Future designs could inspire from this model to implement embedded devices that learn vocal sequences and recall them as needed in real-time. Such systems could acquire language and speech, operate as artificial assistants, and transcribe text to speech, in the presence of natural noise and corruption on audio data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002922",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Associative property",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Content-addressable memory",
      "Encoding (memory)",
      "Forgetting",
      "Gene",
      "Genetics",
      "Linguistics",
      "Mathematics",
      "Mathematics education",
      "Memorization",
      "Operating system",
      "Philosophy",
      "Process (computing)",
      "Pure mathematics",
      "Robustness (evolution)",
      "Sequence (biology)",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Jannesar",
        "given_name": "Nastaran"
      },
      {
        "surname": "Akbarzadeh-Sherbaf",
        "given_name": "Kaveh"
      },
      {
        "surname": "Safari",
        "given_name": "Saeed"
      },
      {
        "surname": "Vahabie",
        "given_name": "Abdol-Hossein"
      }
    ]
  },
  {
    "title": "DualFluidNet: An attention-based dual-pipeline network for fluid simulation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106401",
    "abstract": "Fluid motion can be considered as a point cloud transformation when using the SPH method. Compared to traditional numerical analysis methods, using machine learning techniques to learn physics simulations can achieve near-accurate results, while significantly increasing efficiency. In this paper, we propose an innovative approach for 3D fluid simulations utilizing an Attention-based Dual-pipeline Network, which employs a dual-pipeline architecture, seamlessly integrated with an Attention-based Feature Fusion Module. Unlike previous methods, which often make difficult trade-offs between global fluid control and physical law constraints, we find a way to achieve a better balance between these two crucial aspects with a well-designed dual-pipeline approach. Additionally, we design a Type-aware Input Module to adaptively recognize particles of different types and perform feature fusion afterward, such that fluid-solid coupling issues can be better dealt with. Furthermore, we propose a new dataset, Tank3D, to further explore the network’s ability to handle more complicated scenes. The experiments demonstrate that our approach not only attains a quantitative enhancement in various metrics, surpassing the state-of-the-art methods, but also signifies a qualitative leap in neural network-based simulation by faithfully adhering to the physical laws. Code and video demonstrations are available at https://github.com/chenyu-xjtu/DualFluidNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003253",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Computer science",
      "Dual (grammatical number)",
      "Dual purpose",
      "Engineering",
      "Fluid simulation",
      "Literature",
      "Machine learning",
      "Mechanical engineering",
      "Mechanics",
      "Physics",
      "Pipeline (software)",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yu"
      },
      {
        "surname": "Zheng",
        "given_name": "Shuai"
      },
      {
        "surname": "Jin",
        "given_name": "Menglong"
      },
      {
        "surname": "Chang",
        "given_name": "Yan"
      },
      {
        "surname": "Wang",
        "given_name": "Nianyi"
      }
    ]
  },
  {
    "title": "Improving span-based Aspect Sentiment Triplet Extraction with part-of-speech filtering and contrastive learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106381",
    "abstract": "Aspect Sentiment Triple Extraction (ASTE), a subtask of fine-grained sentiment analysis, aims to extract aspect terms, opinion terms, and their corresponding sentiment polarities from sentences. Previous methods often enumerated all possible spans of aspects and opinions as candidate spans that contain many invalid and irrelevant spans. This made the model training and prediction more difficult due to noised spans, leading to poor performance. To address this issue, we first propose a novel span-level approach that explicitly considers prior grammatical knowledge to generate possible candidate spans by part-of-speech filtering. In this way, our approach can make the model easier to be trained and achieve higher performance at the test stage. Besides, the quality of span-level representation of aspects and opinions is crucial for predicting their sentiment relation. To build a high-quality span-level representation of aspects and opinions, we first incorporate the contextual embedding of the entire sequence into span-level representations. Then, we introduce an auxiliary loss based on contrastive learning to make a more compact representation of the same polarities. Experimental evaluations on the 14Lap, 14Res, 15Res, and 16Res datasets demonstrate the effectiveness of our model, achieving state-of-the-art performance in span-based triplet extraction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003058",
    "keywords": [
      "Artificial intelligence",
      "Character (mathematics)",
      "Civil engineering",
      "Computer science",
      "Data mining",
      "Engineering",
      "Epistemology",
      "Geometry",
      "Law",
      "Machine learning",
      "Mathematics",
      "Natural language processing",
      "Philosophy",
      "Political science",
      "Politics",
      "Quality (philosophy)",
      "Relation (database)",
      "Representation (politics)",
      "Sentiment analysis",
      "Span (engineering)",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Qingling"
      },
      {
        "surname": "Wen",
        "given_name": "Wushao"
      },
      {
        "surname": "Qin",
        "given_name": "Jinghui"
      }
    ]
  },
  {
    "title": "Segmenting medical images with limited data",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106367",
    "abstract": "While computer vision has proven valuable for medical image segmentation, its application faces challenges such as limited dataset sizes and the complexity of effectively leveraging unlabeled images. To address these challenges, we present a novel semi-supervised, consistency-based approach termed the data-efficient medical segmenter (DEMS). The DEMS features an encoder–decoder architecture and incorporates the developed online automatic augmenter (OAA) and residual robustness enhancement (RRE) blocks. The OAA augments input data with various image transformations, thereby diversifying the dataset to improve the generalization ability. The RRE enriches feature diversity and introduces perturbations to create varied inputs for different decoders, thereby providing enhanced variability. Moreover, we introduce a sensitive loss to further enhance consistency across different decoders and stabilize the training process. Extensive experimental results on both our own and three public datasets affirm the effectiveness of DEMS. Under extreme data shortage scenarios, our DEMS achieves 16.85% and 10.37% improvement in dice score compared with the U-Net and top-performed state-of-the-art method, respectively. Given its superior data efficiency, DEMS could present significant advancements in medical segmentation under small data regimes. The project homepage can be accessed at https://github.com/NUS-Tim/DEMS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002910",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Computer science",
      "Computer vision",
      "Market segmentation",
      "Marketing",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Zhaoshan"
      },
      {
        "surname": "Lv",
        "given_name": "Qiujie"
      },
      {
        "surname": "Lee",
        "given_name": "Chau Hung"
      },
      {
        "surname": "Shen",
        "given_name": "Lei"
      }
    ]
  },
  {
    "title": "Human attention guided explainable artificial intelligence for computer vision models",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106392",
    "abstract": "Explainable artificial intelligence (XAI) has been increasingly investigated to enhance the transparency of black-box artificial intelligence models, promoting better user understanding and trust. Developing an XAI that is faithful to models and plausible to users is both a necessity and a challenge. This work examines whether embedding human attention knowledge into saliency-based XAI methods for computer vision models could enhance their plausibility and faithfulness. Two novel XAI methods for object detection models, namely FullGrad-CAM and FullGrad-CAM++, were first developed to generate object-specific explanations by extending the current gradient-based XAI methods for image classification models. Using human attention as the objective plausibility measure, these methods achieve higher explanation plausibility. Interestingly, all current XAI methods when applied to object detection models generally produce saliency maps that are less faithful to the model than human attention maps from the same object detection task. Accordingly, human attention-guided XAI (HAG-XAI) was proposed to learn from human attention how to best combine explanatory information from the models to enhance explanation plausibility by using trainable activation functions and smoothing kernels to maximize the similarity between XAI saliency map and human attention map. The proposed XAI methods were evaluated on widely used BDD-100K, MS-COCO, and ImageNet datasets and compared with typical gradient-based and perturbation-based XAI methods. Results suggest that HAG-XAI enhanced explanation plausibility and user trust at the expense of faithfulness for image classification models, and it enhanced plausibility, faithfulness, and user trust simultaneously and outperformed existing state-of-the-art XAI methods for object detection models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003162",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Cognitive science",
      "Computer science",
      "Computer vision",
      "Machine learning",
      "Psychology"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Guoyang"
      },
      {
        "surname": "Zhang",
        "given_name": "Jindi"
      },
      {
        "surname": "Chan",
        "given_name": "Antoni B."
      },
      {
        "surname": "Hsiao",
        "given_name": "Janet H."
      }
    ]
  },
  {
    "title": "Emergence of integrated behaviors through direct optimization for homeostasis",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106379",
    "abstract": "Homeostasis is a self-regulatory process, wherein an organism maintains a specific internal physiological state. Homeostatic reinforcement learning (RL) is a framework recently proposed in computational neuroscience to explain animal behavior. Homeostatic RL organizes the behaviors of autonomous embodied agents according to the demands of the internal dynamics of their bodies, coupled with the external environment. Thus, it provides a basis for real-world autonomous agents, such as robots, to continually acquire and learn integrated behaviors for survival. However, prior studies have generally explored problems pertaining to limited size, as the agent must handle observations of such coupled dynamics. To overcome this restriction, we developed an advanced method to realize scaled-up homeostatic RL using deep RL. Furthermore, several rewards for homeostasis have been proposed in the literature. We identified that the reward definition that uses the difference in drive function yields the best results. We created two benchmark environments for homeostasis and performed a behavioral analysis. The analysis showed that the trained agents in each environment changed their behavior based on their internal physiological states. Finally, we extended our method to address vision using deep convolutional neural networks. The analysis of a trained agent revealed that it has visual saliency rooted in the survival environment and internal representations resulting from multimodal input.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003034",
    "keywords": [
      "Artificial intelligence",
      "Autonomous agent",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Embodied cognition",
      "Evolutionary biology",
      "Function (biology)",
      "Geodesy",
      "Geography",
      "Neuroscience",
      "Operating system",
      "Process (computing)",
      "Reinforcement learning"
    ],
    "authors": [
      {
        "surname": "Yoshida",
        "given_name": "Naoto"
      },
      {
        "surname": "Daikoku",
        "given_name": "Tatsuya"
      },
      {
        "surname": "Nagai",
        "given_name": "Yukie"
      },
      {
        "surname": "Kuniyoshi",
        "given_name": "Yasuo"
      }
    ]
  },
  {
    "title": "A Multi-Level Relation-Aware Transformer model for occluded person re-identification",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106382",
    "abstract": "Occluded person re-identification (Re-ID) is a challenging task, as pedestrians are often obstructed by various occlusions, such as non-pedestrian objects or non-target pedestrians. Previous methods have heavily relied on auxiliary models to obtain information in unoccluded regions, such as human pose estimation. However, these auxiliary models fall short in accounting for pedestrian occlusions, thereby leading to potential misrepresentations. In addition, some previous works learned feature representations from single images, ignoring the potential relations among samples. To address these issues, this paper introduces a Multi-Level Relation-Aware Transformer (MLRAT) model for occluded person Re-ID. This model mainly encompasses two novel modules: Patch-Level Relation-Aware (PLRA) and Sample-Level Relation-Aware (SLRA). PLRA learns fine-grained local features by modeling the structural relations between key patches, bypassing the dependency on auxiliary models. It adopts a model-free method to select key patches that have high semantic correlation with the final pedestrian representation. In particular, to alleviate the interference of occlusion, PLRA captures the structural relations among key patches via a two-layer Graph Convolution Network (GCN), effectively guiding the local feature fusion and learning. SLRA is designed to facilitate the model to learn discriminative features by modeling the relations among samples. Specifically, to mitigate noisy relations of irrelevant samples, we present a Relation-Aware Transformer (RAT) block to capture the relations among neighbors. Furthermore, to bridge the gap between training and testing phases, a self-distillation method is employed to transfer the sample-level relations captured by SLRA to the backbone. Extensive experiments are conducted on four occluded datasets, two partial datasets and two holistic datasets. The results show that the proposed MLRAT model significantly outperforms existing baselines on four occluded datasets, while maintains top performance on two partial datasets and two holistic datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400306X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Discriminative model",
      "Electrical engineering",
      "Engineering",
      "Feature (linguistics)",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Pedestrian",
      "Philosophy",
      "Relation (database)",
      "Transformer",
      "Transport engineering",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Guorong"
      },
      {
        "surname": "Bao",
        "given_name": "Zhiqiang"
      },
      {
        "surname": "Huang",
        "given_name": "Zhenhua"
      },
      {
        "surname": "Li",
        "given_name": "Zuoyong"
      },
      {
        "surname": "Zheng",
        "given_name": "Wei-shi"
      },
      {
        "surname": "Chen",
        "given_name": "Yunwen"
      }
    ]
  },
  {
    "title": "A robust multi-scale feature extraction framework with dual memory module for multivariate time series anomaly detection",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106395",
    "abstract": "Although existing reconstruction-based multivariate time series anomaly detection (MTSAD) methods have shown advanced performance, most assume the training data is clean. When faced with noise or contamination in training data, they can also reconstruct the anomaly well, weakening the distinction between normal and anomaly. Some probabilistic generation-based methods have been used to address this issue because of their implicit robust structure to noise, but the training process and suppression of anomalous generalization are not stable. The recently proposed explicit method based on the memory module would also sacrifice the reconstruction effect of normal patterns, resulting in limited performance improvement. Moreover, most existing MTSAD methods use a single fixed-length window for input, which weakens their ability to extract long-term dependency. This paper proposes a robust multi-scale feature extraction framework with the dual memory module to comprehensively extract features fusing different levels of semantic information and lengths of temporal dependency. First, this paper designs consecutive neighboring windows as inputs to allow the model to extract local and long-term dependency information. Secondly, a dual memory-augmented encoder is proposed to extract global typical patterns and local common features. It ensures the reconstruction ability of normal data while suppressing the generalization of the anomaly. Finally, this paper proposes a multi-scale fusion module to fuse latent variables representing different levels of semantic information and uses the reconstructed latent variables to reconstruct samples for anomaly detection. Experimental results on five datasets from diverse domains show that the proposed method outperforms 16 typical baseline methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003198",
    "keywords": [
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Computer science",
      "Condensed matter physics",
      "Data mining",
      "Dependency (UML)",
      "Feature (linguistics)",
      "Generalization",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Probabilistic logic",
      "Time series"
    ],
    "authors": [
      {
        "surname": "Xue",
        "given_name": "Bing"
      },
      {
        "surname": "Gao",
        "given_name": "Xin"
      },
      {
        "surname": "Li",
        "given_name": "Baofeng"
      },
      {
        "surname": "Zhai",
        "given_name": "Feng"
      },
      {
        "surname": "Lu",
        "given_name": "Jiansheng"
      },
      {
        "surname": "Yu",
        "given_name": "Jiahao"
      },
      {
        "surname": "Fu",
        "given_name": "Shiyuan"
      },
      {
        "surname": "Xiao",
        "given_name": "Chun"
      }
    ]
  },
  {
    "title": "AdaDFKD: Exploring adaptive inter-sample relationship in data-free knowledge distillation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106386",
    "abstract": "In scenarios like privacy protection or large-scale data transmission, data-free knowledge distillation (DFKD) methods are proposed to learn Knowledge Distillation (KD) when data is not accessible. They generate pseudo samples by extracting the knowledge from teacher model, and utilize above pseudo samples for KD. The challenge in previous DFKD methods lies in the static nature of their target distributions and they focus on learning the instance-level distributions, causing its reliance on the pretrained teacher model. To address above concerns, our study introduces a novel DFKD approach known as AdaDFKD, designed to establish and utilize relationships among pseudo samples, which is adaptive to the student model, and finally effectively mitigates the aforementioned risk. We achieve this by generating from “easy-to-discriminate” samples to “hard-to-discriminate” samples as human does. We design a relationship refinement module (R 2 M) to optimize the generation process, wherein we learn a progressive conditional distribution of negative samples and maximize the log-likelihood of inter-sample similarity of pseudosamples. Theoretically, we discover that such design of AdaDFKD both minimize the divergence and maximize the mutual information between the distribution of teacher and student models. Above results demonstrate the superiority of our approach over state-of-the-art (SOTA) DFKD methods across various benchmarks, teacher–student pairs, and evaluation metrics, as well as robustness and fast convergence.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003101",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Data mining",
      "Distillation",
      "Divergence (linguistics)",
      "Gene",
      "Linguistics",
      "Machine learning",
      "Operating system",
      "Organic chemistry",
      "Philosophy",
      "Process (computing)",
      "Robustness (evolution)",
      "Sample (material)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Jingru"
      },
      {
        "surname": "Zhou",
        "given_name": "Sheng"
      },
      {
        "surname": "Li",
        "given_name": "Liangcheng"
      },
      {
        "surname": "Wang",
        "given_name": "Haishuai"
      },
      {
        "surname": "Bu",
        "given_name": "Jiajun"
      },
      {
        "surname": "Yu",
        "given_name": "Zhi"
      }
    ]
  },
  {
    "title": "Regularization, early-stopping and dreaming: A Hopfield-like setup to address generalization and overfitting",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106389",
    "abstract": "In this work we approach attractor neural networks from a machine learning perspective: we look for optimal network parameters by applying a gradient descent over a regularized loss function. Within this framework, the optimal neuron-interaction matrices turn out to be a class of matrices which correspond to Hebbian kernels revised by a reiterated unlearning protocol. Remarkably, the extent of such unlearning is proved to be related to the regularization hyperparameter of the loss function and to the training time. Thus, we can design strategies to avoid overfitting that are formulated in terms of regularization and early-stopping tuning. The generalization capabilities of these attractor networks are also investigated: analytical results are obtained for random synthetic datasets, next, the emerging picture is corroborated by numerical experiments that highlight the existence of several regimes (i.e., overfitting, failure and success) as the dataset parameters are varied.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003137",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Early stopping",
      "Generalization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Overfitting",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "Agliari",
        "given_name": "E."
      },
      {
        "surname": "Alemanno",
        "given_name": "F."
      },
      {
        "surname": "Aquaro",
        "given_name": "M."
      },
      {
        "surname": "Fachechi",
        "given_name": "A."
      }
    ]
  },
  {
    "title": "Active Dynamic Weighting for multi-domain adaptation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106398",
    "abstract": "Multi-source unsupervised domain adaptation aims to transfer knowledge from multiple labeled source domains to an unlabeled target domain. Existing methods either seek a mixture of distributions across various domains or combine multiple single-source models for weighted fusion in the decision process, with little insight into the distributional discrepancy between different source domains and the target domain. Considering the discrepancies in global and local feature distributions between different domains and the complexity of obtaining category boundaries across domains, this paper proposes a novel Active Dynamic Weighting (ADW) for multi-source domain adaptation. Specifically, to effectively utilize the locally advantageous features in the source domains, ADW designs a multi-source dynamic adjustment mechanism during the training process to dynamically control the degree of feature alignment between each source and target domain in the training batch. In addition, to ensure the cross-domain categories can be distinguished, ADW devises a dynamic boundary loss to guide the model to focus on the hard samples near the decision boundary, which enhances the clarity of the decision boundary and improves the model’s classification ability. Meanwhile, ADW applies active learning to multi-source unsupervised domain adaptation for the first time, guided by dynamic boundary loss, proposes an efficient importance sampling strategy to select target domain hard samples to annotate at a minimal annotation budget, integrates it into the training process, and further refines the domain alignment at the category level. Experiments on various benchmark datasets consistently demonstrate the superiority of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003228",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Boundary (topology)",
      "Computer science",
      "Data mining",
      "Domain (mathematical analysis)",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Process (computing)",
      "Radiology",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Long"
      },
      {
        "surname": "Zhou",
        "given_name": "Bo"
      },
      {
        "surname": "Zhao",
        "given_name": "Zhipeng"
      },
      {
        "surname": "Liu",
        "given_name": "Zening"
      }
    ]
  },
  {
    "title": "Modeling Bellman-error with logistic distribution with applications in reinforcement learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106387",
    "abstract": "In modern Reinforcement Learning (RL) approaches, optimizing the Bellman error is a critical element across various algorithms, notably in deep Q-Learning and related methodologies. Traditional approaches predominantly employ the mean-squared Bellman error (MSELoss) as the standard loss function. However, the assumption of Bellman errors following the Gaussian distribution may oversimplify the nuanced characteristics of RL applications. In this work, we revisit the distribution of Bellman error in RL training, demonstrating that it tends to follow the Logistic distribution rather than the commonly assumed Normal distribution. We propose replacing MSELoss with a Logistic maximum likelihood function (LLoss) and rigorously test this hypothesis through extensive numerical experiments across diverse online and offline RL environments. Our findings consistently show that integrating the Logistic correction into the loss functions of various baseline RL methods leads to superior performance compared to their MSE counterparts. Additionally, we employ Kolmogorov–Smirnov tests to substantiate that the Logistic distribution offers a more accurate fit for approximating Bellman errors. This study also offers a novel theoretical contribution by establishing a clear connection between the distribution of Bellman error and the practice of proportional reward scaling, a common technique for performance enhancement in RL. Moreover, we explore the sample-accuracy trade-off involved in approximating the Logistic distribution, leveraging the Bias–Variance decomposition to mitigate excessive computational resources. The theoretical and empirical insights presented in this study lay a significant foundation for future research, potentially advancing methodologies, and understanding in RL, particularly in the distribution-based optimization of Bellman error.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003113",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Distribution (mathematics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Reinforcement learning"
    ],
    "authors": [
      {
        "surname": "Lv",
        "given_name": "Outongyi"
      },
      {
        "surname": "Zhou",
        "given_name": "Bingxin"
      },
      {
        "surname": "Yang",
        "given_name": "Lin F."
      }
    ]
  },
  {
    "title": "Do we really need a large number of visual prompts?",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106390",
    "abstract": "Due to increasing interest in adapting models on resource-constrained edges, parameter-efficient transfer learning has been widely explored. Among various methods, Visual Prompt Tuning (VPT), prepending learnable prompts to input space, shows competitive fine-tuning performance compared to training of full network parameters. However, VPT increases the number of input tokens, resulting in additional computational overhead. In this paper, we analyze the impact of the number of prompts on fine-tuning performance and self-attention operation in a vision transformer architecture. Through theoretical and empirical analysis we show that adding more prompts does not lead to linear performance improvement. Further, we propose a Prompt Condensation (PC) technique that aims to prevent performance degradation from using a small number of prompts. We validate our methods on FGVC and VTAB-1k tasks and show that our approach reduces the number of prompts by ∼ 70% while maintaining accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003149",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Youngeun"
      },
      {
        "surname": "Li",
        "given_name": "Yuhang"
      },
      {
        "surname": "Moitra",
        "given_name": "Abhishek"
      },
      {
        "surname": "Yin",
        "given_name": "Ruokai"
      },
      {
        "surname": "Panda",
        "given_name": "Priyadarshini"
      }
    ]
  },
  {
    "title": "Structure enhanced prototypical alignment for unsupervised cross-domain node classification",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106396",
    "abstract": "Graph Neural Networks (GNNs) have demonstrated remarkable success in graph node classification task. However, their performance heavily relies on the availability of high-quality labeled data, which can be time-consuming and labor-intensive to acquire for graph-structured data. Therefore, the task of transferring knowledge from a label-rich graph (source domain) to a completely unlabeled graph (target domain) becomes crucial. In this paper, we propose a novel unsupervised graph domain adaptation framework called Structure Enhanced Prototypical Alignment (SEPA), which aims to learn domain-invariant representations on non-IID (non-independent and identically distributed) data. Specifically, SEPA captures class-wise semantics by constructing a prototype-based graph and introduces an explicit domain discrepancy metric to align the source and target domains. The proposed SEPA framework is optimized in an end-to-end manner, which could be incorporated into various GNN architectures. Experimental results on several real-world datasets demonstrate that our proposed framework outperforms recent state-of-the-art baselines with different gains.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003204",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Domain (mathematical analysis)",
      "Mathematical analysis",
      "Mathematics",
      "Node (physics)",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Meihan"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhen"
      },
      {
        "surname": "Ma",
        "given_name": "Ning"
      },
      {
        "surname": "Gu",
        "given_name": "Ming"
      },
      {
        "surname": "Wang",
        "given_name": "Haishuai"
      },
      {
        "surname": "Zhou",
        "given_name": "Sheng"
      },
      {
        "surname": "Bu",
        "given_name": "Jiajun"
      }
    ]
  },
  {
    "title": "Multi-level feature interaction image super-resolution network based on convolutional nonlinear spiking neural model",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106366",
    "abstract": "Image super-resolution (ISR) is designed to recover lost detail information from low-resolution images, resulting in high-quality and high-definition high-resolution images. In the existing single ISR (SISR) methods based on convolutional neural networks (CNN), however, most of the models cannot effectively combine global and local information and are also easy to ignore the correlation between different hierarchical feature information. To address these problems, this study proposes a multi-level feature interactive image super-resolution network, which is constructed by the convolutional units inspired by nonlinear spiking mechanism in nonlinear spiking neural P systems, including shallow feature processing, deep feature extraction and fusion, and reconstruction modules. The different omni domain self-attention blocks are introduced to extract global information in the deep feature extraction and fusion stage and formed a feature enhancement module having a Transformer structure using a novel convolutional unit for extracting local information. Furthermore, to adaptively fuse features between different hierarchies, we design a multi-level feature fusion module, which not only can adaptively fuse features between different hierarchies, but also can better interact with contextual information. The proposed model is compared with 16 state-of-the-art or baseline models on five benchmark datasets. The experimental results show that the proposed model not only achieves good reconstruction performance, but also strikes a good balance between model parameters and performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002909",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Convolutional neural network",
      "Electrical engineering",
      "Engineering",
      "Feature (linguistics)",
      "Feature extraction",
      "Fuse (electrical)",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Ye",
        "given_name": "Lulin"
      },
      {
        "surname": "Zhou",
        "given_name": "Chi"
      },
      {
        "surname": "Peng",
        "given_name": "Hong"
      },
      {
        "surname": "Wang",
        "given_name": "Jun"
      },
      {
        "surname": "Liu",
        "given_name": "Zhicai"
      },
      {
        "surname": "Yang",
        "given_name": "Qian"
      }
    ]
  },
  {
    "title": "M 2 ixKG: Mixing for harder negative samples in knowledge graph",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106358",
    "abstract": "Knowledge graph embedding (KGE) involves mapping entities and relations to low-dimensional dense embeddings, enabling a wide range of real-world applications. The mapping is achieved via distinguishing the positive and negative triplets in knowledge graphs. Therefore, how to design high-quality negative triplets is critical in the effectiveness of KEG models. Existing KGE models face challenges in generating high-quality negative triplets. Some models employ simple static distributions, i.e. uniform or Bernoulli distribution, and it is difficult for these methods to be trained distinguishably because of the sampled uninformative negative triplets. Furthermore, current methods are confined to constructing negative triplets from existing entities within the knowledge graph, limiting their ability to explore harder negatives. We introduce a novel mixing strategy in knowledge graphs called M 2 ixKG. M 2 ixKG adopts mixing operation in generating harder negative samples from two aspects: one is mixing among the heads and tails in triplets with the same relation to strengthen the robustness and generalization of the entity embeddings; the other is mixing the negatives with high scores to generate harder negatives. Our experiments, utilizing three datasets and four classical score functions, highlight the exceptional performance of M 2 ixKG in comparison to previous negative sampling algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400282X",
    "keywords": [
      "Aerospace engineering",
      "Algorithm",
      "Artificial intelligence",
      "Bernoulli's principle",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Embedding",
      "Engineering",
      "Gene",
      "Generalization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Mixing (physics)",
      "Physics",
      "Quantum mechanics",
      "Robustness (evolution)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Che",
        "given_name": "Feihu"
      },
      {
        "surname": "Tao",
        "given_name": "Jianhua"
      }
    ]
  },
  {
    "title": "A robust self-supervised image hashing method for content identification with forensic detection of content-preserving manipulations",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106357",
    "abstract": "Image content identification systems have many applications in industry and academia. In particular, a hash-based content identification system uses a robust image hashing function that computes a short binary identifier summarizing the perceptual content in a picture and is invariant against a set of expected manipulations while being capable of differentiating between different pictures. A common approach to designing these algorithms is crafting a processing pipeline by hand. Unfortunately, once the context changes, the researcher may need to define a new function to adapt. A deep hashing approach exploits the feature learning capabilities in deep networks to generate a feature vector that summarizes the perceptual content in the image, achieving outstanding performance for the image retrieval task, which requires measuring semantic and perceptual similarity between items. However, its application to robust content identification systems is an open area of opportunity. Also, image hashing functions are valuable tools for image authentication. However, to our knowledge, its application to content-preserving manipulation detection for image forensics tasks is still an open research area. In this work, we propose a deep hashing method exploiting the metric learning capabilities in contrastive self-supervised learning with a new modular loss function for robust image hashing. Moreover, we propose a novel approach for content-preserving manipulation detection for image forensics through a sensitivity component in our loss function. We validate our method through extensive experimentation in different data sets and configurations, validating the generalization properties in our work.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002818",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Botany",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Gene",
      "Hash function",
      "Identification (biology)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Fonseca-Bustos",
        "given_name": "Jesús"
      },
      {
        "surname": "Ramírez-Gutiérrez",
        "given_name": "Kelsey Alejandra"
      },
      {
        "surname": "Feregrino-Uribe",
        "given_name": "Claudia"
      }
    ]
  },
  {
    "title": "Advanced optimal tracking integrating a neural critic technique for asymmetric constrained zero-sum games",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106388",
    "abstract": "This paper investigates the optimal tracking issue for continuous-time (CT) nonlinear asymmetric constrained zero-sum games (ZSGs) by exploiting the neural critic technique. Initially, an improved algorithm is constructed to tackle the tracking control problem of nonlinear CT multiplayer ZSGs. Also, we give a novel nonquadratic function to settle the asymmetric constraints. One thing worth noting is that the method used in this paper to solve asymmetric constraints eliminates the strict restriction on the control matrix compared to the previous ones. Further, the optimal controls, the worst disturbances, and the tracking Hamilton–Jacobi–Isaacs equation are derived. Next, a single critic neural network is built to estimate the optimal cost function, thus obtaining the approximations of the optimal controls and the worst disturbances. The critic network weight is updated by the normalized steepest descent algorithm. Additionally, based on the Lyapunov method, the stability of the tracking error and the weight estimation error of the critic network is analyzed. In the end, two examples are offered to validate the theoretical results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003125",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Deep neural networks",
      "Linguistics",
      "Mathematical optimization",
      "Mathematics",
      "Nash equilibrium",
      "Pedagogy",
      "Philosophy",
      "Psychology",
      "Tracking (education)",
      "Zero (linguistics)",
      "Zero-sum game"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Menghua"
      },
      {
        "surname": "Wang",
        "given_name": "Ding"
      },
      {
        "surname": "Ren",
        "given_name": "Jin"
      },
      {
        "surname": "Qiao",
        "given_name": "Junfei"
      }
    ]
  },
  {
    "title": "Unsupervised domain adaptive segmentation algorithm based on two-level category alignment",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106399",
    "abstract": "To enhance the model’s generalization ability in unsupervised domain adaptive segmentation tasks, most approaches have primarily focused on pixel-level local features, but neglected the clue in category information. This limitation results in the segmentation network only learning global inter-domain invariant features but ignoring the category-specific inter-domain invariant features, which degenerates the segmentation performance. To address this issue, we present an Unsupervised Domain Adaptive algorithm based on two-level Category Alignment in two different spaces for semantic segmentation tasks, denoted as UDA c a + . The first level is image-level category alignment based on class activation map (CAM), and the second one is pixel-level category alignment based on pseudo label. By utilizing category information, UDA c a + can effectively capture domain-invariant yet category-discriminative feature representations to improve segmentation accuracy. In addition, an adversarial learning-based strategy in mixed domain is designed to train the proposed network. Moreover, a confidence calculation method is introduced to mitigate the misleading issues of negative transfer and over-alignment caused by the noise in image-level pseudo labels. UDA c a + achieves the state-of-the-art (SOTA) performance on two synthetic-to-real adaptative tasks, and verifies its effectiveness for image segmentation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400323X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Discriminative model",
      "Generalization",
      "Image segmentation",
      "Invariant (physics)",
      "Mathematical analysis",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Scale-space segmentation",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Dong",
        "given_name": "Wenyong"
      },
      {
        "surname": "Liang",
        "given_name": "Zhixue"
      },
      {
        "surname": "Wang",
        "given_name": "Liping"
      },
      {
        "surname": "Tian",
        "given_name": "Gang"
      },
      {
        "surname": "Long",
        "given_name": "Qianhui"
      }
    ]
  },
  {
    "title": "Prototype-based sample-weighted distillation unified framework adapted to missing modality sentiment analysis",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106397",
    "abstract": "Missing modality sentiment analysis is a prevalent and challenging issue in real life. Furthermore, the heterogeneity of multimodality often leads to an imbalance in optimization when attempting to optimize the same objective across all modalities in multimodal networks. Previous works have consistently overlooked the optimization imbalance of the network in cases when modalities are absent. This paper presents a Prototype-Based Sample-Weighted Distillation Unified Framework Adapted to Missing Modality Sentiment Analysis (PSWD). Specifically, it fuses features with a more efficient transformer-based cross-modal hierarchical cyclic fusion module. Subsequently, we propose two strategies, namely sample-weighted distillation and prototype regularization network, to address the issues of missing modality and optimization imbalance. The sample-weighted distillation strategy assigns higher weights to samples that are located closer to class boundaries. This facilitates the obtaining of complete knowledge by the student network from the teacher’s network. The prototype regularization network calculates a balanced metric for each modality, which adaptively adjusts the gradient based on the prototype cross-entropy loss. Unlike conventional approaches, PSWD not only connects the sentiment analysis study in the missing modality to the full modality, but the proposed prototype regularization network is not reliant on the network structure and can be expanded to more multimodal studies. Massive experiments conducted on IEMOCAP and MSP-IMPROV show that our method achieves the best results compared to the latest baseline methods, which demonstrates its value for application in sentiment analysis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003216",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Data mining",
      "Distillation",
      "Machine learning",
      "Missing data",
      "Modalities",
      "Modality (human–computer interaction)",
      "Multimodality",
      "Organic chemistry",
      "Regularization (linguistics)",
      "Sample (material)",
      "Social science",
      "Sociology",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yujuan"
      },
      {
        "surname": "Liu",
        "given_name": "Fang’ai"
      },
      {
        "surname": "Zhuang",
        "given_name": "Xuqiang"
      },
      {
        "surname": "Hou",
        "given_name": "Ying"
      },
      {
        "surname": "Zhang",
        "given_name": "Yuling"
      }
    ]
  },
  {
    "title": "G2ViT: Graph Neural Network-Guided Vision Transformer Enhanced Network for retinal vessel and coronary angiograph segmentation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106356",
    "abstract": "Blood vessel segmentation is a crucial stage in extracting morphological characteristics of vessels for the clinical diagnosis of fundus and coronary artery disease. However, traditional convolutional neural networks (CNNs) are confined to learning local vessel features, making it challenging to capture the graph structural information and fail to perceive the global context of vessels. Therefore, we propose a novel graph neural network-guided vision transformer enhanced network (G2ViT) for vessel segmentation. G2ViT skillfully orchestrates the Convolutional Neural Network, Graph Neural Network, and Vision Transformer to enhance comprehension of the entire graphical structure of blood vessels. To achieve deeper insights into the global graph structure and higher-level global context cognizance, we investigate a graph neural network-guided vision transformer module. This module constructs graph-structured representation in an unprecedented manner using the high-level features extracted by CNNs for graph reasoning. To increase the receptive field while ensuring minimal loss of edge information, G2ViT introduces a multi-scale edge feature attention module (MEFA), leveraging dilated convolutions with different dilation rates and the Sobel edge detection algorithm to obtain multi-scale edge information of vessels. To avoid critical information loss during upsampling and downsampling, we design a multi-level feature fusion module (MLF2) to fuse complementary information between coarse and fine features. Experiments on retinal vessel datasets (DRIVE, STARE, CHASE_DB1, and HRF) and coronary angiography datasets (DCA1 and CHUAC) indicate that the G2ViT excels in robustness, generality, and applicability. Furthermore, it has acceptable inference time and computational complexity and presents a new solution for blood vessel segmentation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002806",
    "keywords": [
      "Anatomy",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Graph",
      "Medicine",
      "Ophthalmology",
      "Pattern recognition (psychology)",
      "Retinal",
      "Segmentation",
      "Theoretical computer science",
      "Transformer",
      "Vascular network",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Hao"
      },
      {
        "surname": "Wu",
        "given_name": "Yun"
      }
    ]
  },
  {
    "title": "Unsupervised Bidirectional Contrastive Reconstruction and Adaptive Fine-Grained Channel Attention Networks for image dehazing",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106314",
    "abstract": "Recently, Unsupervised algorithms has achieved remarkable performance in image dehazing. However, the CycleGAN framework can lead to confusion in generator learning due to inconsistent data distributions, and the DisentGAN framework lacks effective constraints on generated images, resulting in the loss of image content details and color distortion. Moreover, Squeeze and Excitation channel attention employs only fully connected layers to capture global information, lacking interaction with local information, resulting in inaccurate feature weight allocation for image dehazing. To solve the above problems, in this paper, we propose an Unsupervised Bidirectional Contrastive Reconstruction and Adaptive Fine-Grained Channel Attention Networks (UBRFC-Net). Specifically, an Unsupervised Bidirectional Contrastive Reconstruction Framework (BCRF) is proposed, aiming to establish bidirectional contrastive reconstruction constraints, not only to avoid the generator learning confusion in CycleGAN but also to enhance the constraint capability for clear images and the reconstruction ability of the unsupervised dehazing network. Furthermore, an Adaptive Fine-Grained Channel Attention (FCA) is developed to utilize the correlation matrix to capture the correlation between global and local information at various granularities promotes interaction between them, achieving more efficient feature weight assignment. Experimental results on challenging benchmark datasets demonstrate the superiority of our UBRFC-Net over state-of-the-art unsupervised image dehazing methods. This study successfully introduces an enhanced unsupervised image dehazing approach, addressing limitations of existing methods and achieving superior dehazing results. The source code is available at https://github.com/Lose-Code/UBRFC-Net.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002387",
    "keywords": [
      "Adaptation (eye)",
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Benchmark (surveying)",
      "Channel (broadcasting)",
      "Code (set theory)",
      "Computer network",
      "Computer science",
      "Distortion (music)",
      "Feature (linguistics)",
      "Generator (circuit theory)",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Linguistics",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Power (physics)",
      "Programming language",
      "Quantum mechanics",
      "Set (abstract data type)",
      "Source code",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Hang"
      },
      {
        "surname": "Wen",
        "given_name": "Yang"
      },
      {
        "surname": "Feng",
        "given_name": "Huijing"
      },
      {
        "surname": "Zheng",
        "given_name": "Yuelin"
      },
      {
        "surname": "Mei",
        "given_name": "Qi"
      },
      {
        "surname": "Ren",
        "given_name": "Dong"
      },
      {
        "surname": "Yu",
        "given_name": "Mei"
      }
    ]
  },
  {
    "title": "LordNet: An efficient neural network for learning to solve parametric partial differential equations without simulated data",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106354",
    "abstract": "Neural operators, as a powerful approximation to the non-linear operators between infinite-dimensional function spaces, have proved to be promising in accelerating the solution of partial differential equations (PDE). However, it requires a large amount of simulated data, which can be costly to collect. This can be avoided by learning physics from the physics-constrained loss, which we refer to it as mean squared residual (MSR) loss constructed by the discretized PDE. We investigate the physical information in the MSR loss, which we called long-range entanglements, and identify the challenge that the neural network requires the capacity to model the long-range entanglements in the spatial domain of the PDE, whose patterns vary in different PDEs. To tackle the challenge, we propose LordNet, a tunable and efficient neural network for modeling various entanglements. Inspired by the traditional solvers, LordNet models the long-range entanglements with a series of matrix multiplications, which can be seen as the low-rank approximation to the general fully-connected layers and extracts the dominant pattern with reduced computational cost. The experiments on solving Poisson’s equation and (2D and 3D) Navier–Stokes equation demonstrate that the long-range entanglements from the MSR loss can be well modeled by the LordNet, yielding better accuracy and generalization ability than other neural networks. The results show that the Lordnet can be 40 × faster than traditional PDE solvers. In addition, LordNet outperforms other modern neural network architectures in accuracy and efficiency with the smallest parameter size.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002788",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Composite material",
      "Computer science",
      "Discretization",
      "Evolutionary biology",
      "Function (biology)",
      "Generalization",
      "Materials science",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Parametric statistics",
      "Partial differential equation",
      "Range (aeronautics)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Xinquan"
      },
      {
        "surname": "Shi",
        "given_name": "Wenlei"
      },
      {
        "surname": "Gao",
        "given_name": "Xiaotian"
      },
      {
        "surname": "Wei",
        "given_name": "Xinran"
      },
      {
        "surname": "Zhang",
        "given_name": "Jia"
      },
      {
        "surname": "Bian",
        "given_name": "Jiang"
      },
      {
        "surname": "Yang",
        "given_name": "Mao"
      },
      {
        "surname": "Liu",
        "given_name": "Tie-Yan"
      }
    ]
  },
  {
    "title": "FPGA-based fast bin-ratio spiking ensemble network for radioisotope identification",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106332",
    "abstract": "In this work, we demonstrate the training, conversion, and implementation flow of an FPGA-based bin-ratio ensemble spiking neural network applied for radioisotope identification. The combination of techniques including learned step quantisation (LSQ) and pruning facilitated the implementation by compressing the network’s parameters down to 30% yet retaining the accuracy of 97.04% with an accuracy loss of less than 1%. Meanwhile, the proposed ensemble network of 20 3-layer spiking neural networks (SNNs), which incorporates 1160 spiking neurons, only needs 334 μ s for a single inference with the given clock frequency of 100 MHz. Under such optimisation, this FPGA implementation in an Artix-7 board consumes 157 μ J per inference by estimation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002569",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Bin",
      "Biology",
      "Botany",
      "Computer hardware",
      "Computer science",
      "Field-programmable gate array",
      "Identification (biology)",
      "Inference",
      "Pattern recognition (psychology)",
      "Pruning",
      "Spiking neural network"
    ],
    "authors": [
      {
        "surname": "Xie",
        "given_name": "Shouyu"
      },
      {
        "surname": "Jones",
        "given_name": "Edward"
      },
      {
        "surname": "Zhang",
        "given_name": "Siru"
      },
      {
        "surname": "Marsden",
        "given_name": "Edward"
      },
      {
        "surname": "Baistow",
        "given_name": "Ian"
      },
      {
        "surname": "Furber",
        "given_name": "Steve"
      },
      {
        "surname": "Mitra",
        "given_name": "Srinjoy"
      },
      {
        "surname": "Hamilton",
        "given_name": "Alister"
      }
    ]
  },
  {
    "title": "Distributed adaptive robust containment control for reaction–diffusion neural networks with external disturbances under directed graphs",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106363",
    "abstract": "In this paper, the leader–follower robust synchronization issue is mainly addressed for reaction–diffusion neural networks (RDNNs) with multiple leaders and external disturbances under directed graphs. Based on the σ modification approach, we propose a novel distributed adaptive controller by adding a term − θ i ( c i − 1 ) 2 to avoid the phenomenon of parameter drift, that is, the adaptive parameters grow to infinity. Meanwhile, different from the adaptive control algorithm proposed in the undirected graph, we introduce a new function χ i ( t ) to provide additional freedom for the design to achieve robust containment when confronted with external disturbances. Further, the robustness of tracking synchronization with one leader is guaranteed by the proposed adaptive controller when the external disturbances concerning L 2 norm are bounded. Finally, relevant numerical simulation graphics are displayed separately to verify the correctness of the related theoretical results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002879",
    "keywords": [
      "Adaptive control",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Containment (computer programming)",
      "Control (management)",
      "Control theory (sociology)",
      "Diffusion",
      "Mathematical analysis",
      "Mathematics",
      "Physics",
      "Programming language",
      "Reaction–diffusion system",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Qiu",
        "given_name": "Qian"
      },
      {
        "surname": "Su",
        "given_name": "Housheng"
      }
    ]
  },
  {
    "title": "Multimodal information bottleneck for deep reinforcement learning with multiple sensors",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106347",
    "abstract": "Reinforcement learning has achieved promising results on robotic control tasks but struggles to leverage information effectively from multiple sensory modalities that differ in many characteristics. Recent works construct auxiliary losses based on reconstruction or mutual information to extract joint representations from multiple sensory inputs to improve the sample efficiency and performance of reinforcement learning algorithms. However, the representations learned by these methods could capture information irrelevant to learning a policy and may degrade the performance. We argue that compressing information in the learned joint representations about raw multimodal observations is helpful, and propose a multimodal information bottleneck model to learn task-relevant joint representations from egocentric images and proprioception. Our model compresses and retains the predictive information in multimodal observations for learning a compressed joint representation, which fuses complementary information from visual and proprioceptive feedback and meanwhile filters out task-irrelevant information in raw multimodal observations. We propose to minimize the upper bound of our multimodal information bottleneck objective for computationally tractable optimization. Experimental evaluations on several challenging locomotion tasks with egocentric images and proprioception show that our method achieves better sample efficiency and zero-shot robustness to unseen white noise than leading baselines. We also empirically demonstrate that leveraging information from egocentric images and proprioception is more helpful for learning policies on locomotion tasks than solely using one single modality.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002715",
    "keywords": [
      "Artificial intelligence",
      "Bottleneck",
      "Computer science",
      "Economics",
      "Embedded system",
      "Information bottleneck method",
      "Leverage (statistics)",
      "Machine learning",
      "Management",
      "Modalities",
      "Mutual information",
      "Reinforcement learning",
      "Social science",
      "Sociology",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "You",
        "given_name": "Bang"
      },
      {
        "surname": "Liu",
        "given_name": "Huaping"
      }
    ]
  },
  {
    "title": "Neural critic learning with accelerated value iteration for nonlinear model predictive control",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106364",
    "abstract": "In practical industrial processes, the receding optimization solution of nonlinear model predictive control (NMPC) is always a very knotty problem. Based on adaptive dynamic programming, the accelerated value iteration predictive control (AVI-PC) algorithm is developed in this paper. Integrating iteration learning with the receding horizon mechanism of NMPC, a novel receding optimization solution pattern is exploited to resolve the optimal control law in each prediction horizon. Besides, the basic architecture and the specific form of the AVI-PC algorithm are demonstrated, including the relationship among the iterative learning process, the prediction process, and the control process. On this basis, the convergence and admissibility conditions are established, and the relevant properties are comprehensively analyzed when the accelerated factor satisfies the established conditions. Furthermore, the accelerated value iterative function is approximated through the single critic network constructed by utilizing the multiple linear regression method. Finally, the plentiful simulation experiments are conducted from various perspectives to verify the effectiveness and progressiveness of the AVI-PC algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002880",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Bellman equation",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Convergence (economics)",
      "Dynamic programming",
      "Economic growth",
      "Economics",
      "Geometry",
      "Horizon",
      "Mathematical optimization",
      "Mathematics",
      "Model predictive control",
      "Nonlinear system",
      "Operating system",
      "Optimal control",
      "Physics",
      "Process (computing)",
      "Quantum mechanics",
      "Reinforcement learning"
    ],
    "authors": [
      {
        "surname": "Xin",
        "given_name": "Peng"
      },
      {
        "surname": "Wang",
        "given_name": "Ding"
      },
      {
        "surname": "Liu",
        "given_name": "Ao"
      },
      {
        "surname": "Qiao",
        "given_name": "Junfei"
      }
    ]
  },
  {
    "title": "Fast synchronization control and application for encryption-decryption of coupled neural networks with intermittent random disturbance",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106404",
    "abstract": "In this paper, we design a new class of coupled neural networks with stochastically intermittent disturbances, in which the perturbation mechanism is different from other existed random neural networks. It is significant to construct the new models, which can simulate a class of the real neural networks in the disturbed environment, and the fast synchronization control strategies are studied by an adjustable parameter α . A controller with coupling signal is designed to study the exponential synchronization problem, meanwhile, another effective controller with not only adjustable synchronization rate but also with infinite gain avoided is used to investigate the preset-time synchronization. The fast synchronization conditions have been obtained by Lyapunov stability principle, Laplacian matrix and some inequality techniques. A numerical example shows the effectiveness of the control schemes, and the different control factors for synchronization rate are given to discuss the control effect. In particular, the image encryption–decryption based on drive-response networks has been successfully applied.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003289",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Controller (irrigation)",
      "Encryption",
      "Graph",
      "Laplacian matrix",
      "Lyapunov stability",
      "Operating system",
      "Synchronization (alternating current)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Xianghui"
      },
      {
        "surname": "Cao",
        "given_name": "Jinde"
      },
      {
        "surname": "Guan",
        "given_name": "Zhi-Hong"
      },
      {
        "surname": "Wang",
        "given_name": "Xin"
      },
      {
        "surname": "Kong",
        "given_name": "Fanchao"
      }
    ]
  },
  {
    "title": "Interactive attack-defense for generalized person re-identification",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106349",
    "abstract": "Generalized Person Re-Identification (GReID) aims to develop a model capable of robust generalization across unseen target domains, even with training on a limited set of observed domains. Recently, methods based on the Attack-Defense mechanism are emerging as a prevailing technology to this issue, which treats domain transformation as a type of attack and enhances the model’s generalization performance on the target domain by equipping it with a defense module. However, a significant limitation of most existing approaches is their inability to effectively model complex domain transformations, largely due to the separation of attack and defense components. To overcome this limitation, we introduce an innovative Interactive Attack-Defense (IAD) mechanism for GReID. The core of IAD is the interactive learning of two models: an attack model and a defense model. The attack model dynamically generates directional attack information responsive to the current state of the defense model, while the defense model is designed to derive generalizable representations by utilizing a variety of attack samples. The training approach involves a dual process: for the attack model, the aim is to increase the challenge for the defense model in countering the attack; conversely, for the defense model, the focus is on minimizing the effects instigated by the attack model. This interactive framework allows for mutual learning between attack and defense, creating a synergistic learning environment. Our diverse experiments across datasets confirm IAD’s effectiveness, consistently surpassing current state-of-the-art methods, and using MSMT17 as the target domain in different protocols resulted in a notable 13.4% improvement in GReID task average Rank-1 accuracy. Code is available at: https://github.com/lhf12278/IAD.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002739",
    "keywords": [
      "Artificial intelligence",
      "Attack model",
      "Biology",
      "Botany",
      "Computer science",
      "Computer security",
      "Domain (mathematical analysis)",
      "Generalization",
      "Identification (biology)",
      "Mathematical analysis",
      "Mathematics",
      "Programming language",
      "Set (abstract data type)",
      "Variety (cybernetics)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Huafeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Chen"
      },
      {
        "surname": "Hu",
        "given_name": "Zhanxuan"
      },
      {
        "surname": "Zhang",
        "given_name": "Yafei"
      },
      {
        "surname": "Yu",
        "given_name": "Zhengtao"
      }
    ]
  },
  {
    "title": "Learning shared template representation with augmented feature for multi-object pose estimation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106352",
    "abstract": "Template matching pose estimation methods based on deep learning have made significant advancements via metric learning or reconstruction learning. Existing approaches primarily build distinct template representation libraries (codebooks) from rendered images for each object, which complicate the training process and increase memory cost for multi-object tasks. Additionally, they struggle to effectively handle discrepancies between the distributions of training and test sets, particularly for occluded objects, resulting in suboptimal matching accuracy. In this study, we propose a shared template representation learning method with augmented semantic features to address these issues. Our method learns representations concurrently using metric and reconstruction learning as similarity constraints, and augments response of network to objects through semantic feature constraints for better generalization performance. Furthermore, rotation matrices serve as templates for codebook construction, leading to excellent matching accuracy compared to rendered images. Notably, it contributes to the effective decoupling of object categories and templates, necessitating the maintenance of only a shared codebook in multi-object pose estimation tasks. Extensive experiments on Linemod, Linemod-Occluded and TLESS datasets demonstrate that the proposed method employing shared templates achieves superior matching accuracy. Moreover, proposed method exhibits robustness on a collected aircraft dataset, further validating its efficacy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002764",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Codebook",
      "Computer science",
      "Computer vision",
      "Economics",
      "Feature (linguistics)",
      "Feature learning",
      "Gene",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Matching (statistics)",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pose",
      "Programming language",
      "Robustness (evolution)",
      "Statistics",
      "Template",
      "Template matching"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Qifeng"
      },
      {
        "surname": "Xu",
        "given_name": "Ting-Bing"
      },
      {
        "surname": "Liu",
        "given_name": "Fulin"
      },
      {
        "surname": "Li",
        "given_name": "Tianren"
      },
      {
        "surname": "Wei",
        "given_name": "Zhenzhong"
      }
    ]
  },
  {
    "title": "Local spatial and temporal relation discovery model based on attention mechanism for traffic forecasting",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106365",
    "abstract": "Recognizing the evolution pattern of traffic condition and making accurate prediction play a vital role in intelligent transportation systems (ITS). With the massive increase of available traffic data, deep learning-based models have attracted considerable attention for their impressive performance in traffic forecasting. However, the majority of existing approaches neglect to model of asynchronously dynamic spatio-temporal correlation and fail to consider the impact of historical traffic data on future condition. Additionally, the attribute of deep learning method presents challenges in interpreting the explicit spatiotemporal relationships. In order to enhance the accuracy of traffic prediction as well as extract comprehensive and explainable spatial–temporal relevance in traffic networks, we propose a novel attention-based local spatial and temporal relation discovery (ALSTRD) model. Our model firstly implements feature representation learning to effectively express latent input traffic information. Then, a local attention mechanism structure is established to model asynchronous dependencies of historical input data. Finally, another attention network and the Pearson Correlation Coefficient method are introduced to extract the elaborate influence of the historical traffic condition of neighboring roads on the future condition of the target road. The experiment results on several datasets demonstrate that our model achieves significant improvements in prediction accuracy compared to other baseline methods, which can be attributed to its ability to extract the fine-grained correlation among historical traffic data and capture the dynamic association between past and future data. In addition, the incorporation of attention mechanism and Pearson Correlation Coefficient promotes the model’s ability to elucidate spatiotemporal correlations among traffic data, thereby providing a more robust explanation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002892",
    "keywords": [
      "Artificial intelligence",
      "Asynchronous communication",
      "Computer network",
      "Computer science",
      "Correlation",
      "Data mining",
      "Deep learning",
      "Epistemology",
      "Feature (linguistics)",
      "Feature learning",
      "Geometry",
      "Law",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Mechanism (biology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Relation (database)",
      "Relevance (law)",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Chenyang"
      },
      {
        "surname": "Xu",
        "given_name": "Changqing"
      }
    ]
  },
  {
    "title": "A deep reinforcement learning algorithm framework for solving multi-objective traveling salesman problem based on feature transformation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106359",
    "abstract": "As a special type of multi-objective combinatorial optimization problems (MOCOPs), the multi-objective traveling salesman problem (MOTSP) plays an important role in practical fields such as transportation and robot control. However, due to the complexity of its solution space and the conflicts between different objectives, it is difficult to obtain satisfactory solutions in a short time. This paper proposes an end-to-end algorithm framework for solving MOTSP based on deep reinforcement learning (DRL). By decomposing strategies, solving MOTSP is transformed into solving multiple single-objective optimization subproblems. Through linear transformation, the features of the MOTSP are combined with the weights of the objective function. Subsequently, a modified graph pointer network (GPN) model is used to solve the decomposed subproblems. Compared with the previous DRL model, the proposed algorithm can solve all the subproblems using only one model without adding weight information as input features. Furthermore, our algorithm can output a corresponding solution for each weight, which increases the diversity of solutions. In order to verify the performance of our proposed algorithm, it is compared with four classical evolutionary algorithms and two DRL algorithms on several MOTSP instances. The comparison shows that our proposed algorithm outperforms the compared algorithms both in terms of training time and the quality of the resulting solutions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002831",
    "keywords": [
      "2-opt",
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Feature (linguistics)",
      "Gene",
      "Linguistics",
      "Mathematical optimization",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Reinforcement learning",
      "Transformation (genetics)",
      "Travelling salesman problem"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Shijie"
      },
      {
        "surname": "Gu",
        "given_name": "Shenshen"
      }
    ]
  },
  {
    "title": "A novel garment transfer method supervised by distilled knowledge of virtual try-on model",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106353",
    "abstract": "Garment transfer can wear the garment of the model image onto the personal image. As garment transfer leverages wild and cheap garment input, it has attracted tremendous attention in the community and has a huge commercial potential. Since the ground truth of garment transfer is almost unavailable in reality, previous studies have treated garment transfer as either pose transfer or garment-pose disentanglement, and trained garment transfer in self-supervised learning, However, these implementation methods do not cover garment transfer intentions completely and face the robustness issue in the testing phase. Notably, virtual try-on technology has exhibited superior performance using self-supervised learning, we propose to supervise the garment transfer training via knowledge distillation from virtual try-on. Specifically, the overall pipeline is first to infer a garment transfer parsing, and to use it to guide downstream warping and inpainting tasks. The transfer parsing reasoning model learns the response and feature knowledge from the try-on parsing reasoning model and absorbs the hard knowledge from the ground truth. The progressive flow warping model learns the content knowledge from virtual try-on for a reasonable and precise garment warping. To enhance transfer realism, we propose an arm regrowth task to infer exposed skin. Experiments demonstrate that our method has state-of-the-art performance in transferring garments between persons compared with other virtual try-on and garment transfer methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002776",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Human–computer interaction",
      "Image warping",
      "Knowledge management",
      "Knowledge transfer",
      "Machine learning",
      "Natural language processing",
      "Parallel computing",
      "Parsing",
      "Pipeline (software)",
      "Programming language",
      "Transfer (computing)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Fang",
        "given_name": "Naiyu"
      },
      {
        "surname": "Qiu",
        "given_name": "Lemiao"
      },
      {
        "surname": "Zhang",
        "given_name": "Shuyou"
      },
      {
        "surname": "Wang",
        "given_name": "Zili"
      },
      {
        "surname": "Hu",
        "given_name": "Kerui"
      },
      {
        "surname": "Tan",
        "given_name": "Jianrong"
      }
    ]
  },
  {
    "title": "A proximal neurodynamic model for a system of non-linear inverse mixed variational inequalities",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106323",
    "abstract": "In this article, we introduce a system of non-linear inverse mixed variational inequalities (SNIMVIs). We propose a proximal neurodynamic model (PNDM) for solving SNIMVIs, leveraging proximal mappings. The uniqueness of the continuous solution for the PNDM is proved by assuming Lipschitz continuity. Moreover, we establish the global asymptotic stability of equilibrium points of the PNDM, contingent upon Lipschitz continuity and strong monotonicity. Additionally, an iterative algorithm involving proximal mappings for solving the SNIMVIs is presented. Finally, we provide illustrative examples to support our main findings. Furthermore, we provide an example where the SNIMVIs violate the strong monotonicity condition and exhibit the divergence nature of the trajectories of the corresponding PNDM.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002478",
    "keywords": [
      "Applied mathematics",
      "Divergence (linguistics)",
      "Geometry",
      "Inverse",
      "Linguistics",
      "Lipschitz continuity",
      "Mathematical analysis",
      "Mathematics",
      "Monotonic function",
      "Philosophy",
      "Pure mathematics",
      "Uniqueness",
      "Variational inequality"
    ],
    "authors": [
      {
        "surname": "Upadhyay",
        "given_name": "Anjali"
      },
      {
        "surname": "Pandey",
        "given_name": "Rahul"
      }
    ]
  },
  {
    "title": "Solving the non-submodular network collapse problems via Decision Transformer",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106328",
    "abstract": "Given a graph G , the network collapse problem (NCP) selects a vertex subset S of minimum cardinality from G such that the difference in the values of a given measure function f ( G ) − f ( G ∖ S ) is greater than a predefined collapse threshold. Many graph analytic applications can be formulated as NCPs with different measure functions, which often pose a significant challenge due to their NP-hard nature. As a result, traditional greedy algorithms, which select the vertex with the highest reward at each step, may not effectively find the optimal solution. In addition, existing learning-based algorithms do not have the ability to model the sequence of actions taken during the decision-making process, making it difficult to capture the combinatorial effect of selected vertices on the final solution. This limits the performance of learning-based approaches in non-submodular NCPs. To address these limitations, we propose a unified framework called DT-NC, which adapts the Decision Transformer to the Network Collapse problems. DT-NC takes into account the historical actions taken during the decision-making process and effectively captures the combinatorial effect of selected vertices. The ability of DT-NC to model the dependency among selected vertices allows it to address the difficulties caused by the non-submodular property of measure functions in some NCPs effectively. Through extensive experiments on various NCPs and graphs of different sizes, we demonstrate that DT-NC outperforms the state-of-the-art methods and exhibits excellent transferability and generalizability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002521",
    "keywords": [
      "Computer science",
      "Graph",
      "Mathematical optimization",
      "Mathematics",
      "Submodular set function",
      "Theoretical computer science",
      "Vertex (graph theory)"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Kaili"
      },
      {
        "surname": "Yang",
        "given_name": "Han"
      },
      {
        "surname": "Yang",
        "given_name": "Shanchao"
      },
      {
        "surname": "Zhao",
        "given_name": "Kangfei"
      },
      {
        "surname": "Li",
        "given_name": "Lanqing"
      },
      {
        "surname": "Chen",
        "given_name": "Yongqiang"
      },
      {
        "surname": "Huang",
        "given_name": "Junzhou"
      },
      {
        "surname": "Cheng",
        "given_name": "James"
      },
      {
        "surname": "Rong",
        "given_name": "Yu"
      }
    ]
  },
  {
    "title": "Online continual decoding of streaming EEG signal with a balanced and informative memory buffer",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106338",
    "abstract": "Electroencephalography (EEG) based Brain Computer Interface (BCI) systems play a significant role in facilitating how individuals with neurological impairments effectively interact with their environment. In real world applications of BCI system for clinical assistance and rehabilitation training, the EEG classifier often needs to learn on sequentially arriving subjects in an online manner. As patterns of EEG signals can be significantly different for different subjects, the EEG classifier can easily erase knowledge of learnt subjects after learning on later ones as it performs decoding in online streaming scenario, namely catastrophic forgetting. In this work, we tackle this problem with a memory-based approach, which considers the following conditions: (1) subjects arrive sequentially in an online manner, with no large scale dataset available for joint training beforehand, (2) data volume from the different subjects could be imbalanced, (3) decoding difficulty of the sequential streaming signal vary, (4) continual classification for a long time is required. This online sequential EEG decoding problem is more challenging than classic cross subject EEG decoding as there is no large-scale training data from the different subjects available beforehand. The proposed model keeps a small balanced memory buffer during sequential learning, with memory data dynamically selected based on joint consideration of data volume and informativeness. Furthermore, for the more general scenarios where subject identity is unknown to the EEG decoder, aka. subject agnostic scenario, we propose a kernel based subject shift detection method that identifies underlying subject changes on the fly in a computationally efficient manner. We develop challenging benchmarks of streaming EEG data from sequentially arriving subjects with both balanced and imbalanced data volumes, and performed extensive experiments with a detailed ablation study on the proposed model. The results show the effectiveness of our proposed approach, enabling the decoder to maintain performance on all previously seen subjects over a long period of sequential decoding. The model demonstrates the potential for real-world applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002624",
    "keywords": [
      "Artificial intelligence",
      "Brain–computer interface",
      "Classifier (UML)",
      "Computer science",
      "Decoding methods",
      "Electroencephalography",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Psychiatry",
      "Psychology",
      "Speech recognition",
      "Support vector machine",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Duan",
        "given_name": "Tiehang"
      },
      {
        "surname": "Wang",
        "given_name": "Zhenyi"
      },
      {
        "surname": "Li",
        "given_name": "Fang"
      },
      {
        "surname": "Doretto",
        "given_name": "Gianfranco"
      },
      {
        "surname": "Adjeroh",
        "given_name": "Donald A."
      },
      {
        "surname": "Yin",
        "given_name": "Yiyi"
      },
      {
        "surname": "Tao",
        "given_name": "Cui"
      }
    ]
  },
  {
    "title": "Composite attention mechanism network for deep contrastive multi-view clustering",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106361",
    "abstract": "Contrastive learning-based deep multi-view clustering methods have become a mainstream solution for unlabeled multi-view data. These methods usually utilize a basic structure that combines autoencoder, contrastive learning, or/and MLP projectors to generate more representative latent representations for the final clustering stage. However, existing deep contrastive multi-view clustering ignores two key points: (i) the latent representations projecting from one or more layers of MLP or new representations directly obtained from autoencoder fail to mine inherent relationship inner-view or cross-views; (ii) more existing frameworks only employ a one or dual-contrastive learning module, i.e., view- or/and category-oriented, which may result in the lack of communication between latent representations and clustering assignments. This paper proposes a new composite attention framework for contrastive multi-view clustering to address the above two challenges. Our method learns latent representations utilizing composite attention structure, i.e., Hierarchical Transformer for each view and Shared Attention for all views, rather than simple MLP. As a result, the learned representations can simultaneously preserve important features inside the view and balance the contributions across views. In addition, we add a new communication loss in our new dual contrastive framework. The common semantics will be brought into clustering assignments by pushing clustering assignments closer to the fused latent representations. Therefore, our method will provide a higher quality of clustering assignments for the segmentation problem of unlabeled multi-view data. The extensive experiments on several real data demonstrate that the proposed method can achieve superior performance over many state-of-the-art clustering algorithms, especially the significant improvement of an average of 10% on datasets Caltech and its subsets according to accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002855",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Composite number",
      "Computer science",
      "Mechanism (biology)",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Du",
        "given_name": "Tingting"
      },
      {
        "surname": "Zheng",
        "given_name": "Wei"
      },
      {
        "surname": "Xu",
        "given_name": "Xingang"
      }
    ]
  },
  {
    "title": "Adaptive penalty-based neurodynamic approach for nonsmooth interval-valued optimization problem",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106337",
    "abstract": "The complex and diverse practical background drives this paper to explore a new neurodynamic approach (NA) to solve nonsmooth interval-valued optimization problems (IVOPs) constrained by interval partial order and more general sets. On the one hand, to deal with the uncertainty of interval-valued information, the LU-optimality condition of IVOPs is established through a deterministic form. On the other hand, according to the penalty method and adaptive controller, the interval partial order constraint and set constraint are punished by one adaptive parameter, which is a key enabler for the feasibility of states while having a lower solution space dimension and avoiding estimating exact penalty parameters. Through nonsmooth analysis and Lyapunov theory, the proposed adaptive penalty-based neurodynamic approach (APNA) is proven to converge to an LU-solution of the considered IVOPs. Finally, the feasibility of the proposed APNA is illustrated by numerical simulations and an investment decision-making problem.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002612",
    "keywords": [
      "Agronomy",
      "Biology",
      "Combinatorics",
      "Computer science",
      "Constraint (computer-aided design)",
      "Controller (irrigation)",
      "Dimension (graph theory)",
      "Geometry",
      "Interval (graph theory)",
      "Mathematical optimization",
      "Mathematics",
      "Optimization problem",
      "Penalty method",
      "Programming language",
      "Pure mathematics",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Luan",
        "given_name": "Linhua"
      },
      {
        "surname": "Wen",
        "given_name": "Xingnan"
      },
      {
        "surname": "Xue",
        "given_name": "Yuhan"
      },
      {
        "surname": "Qin",
        "given_name": "Sitian"
      }
    ]
  },
  {
    "title": "SEGAL time series classification — Stable explanations using a generative model and an adaptive weighting method for LIME",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106345",
    "abstract": "Local Interpretability Model-agnostic Explanations (LIME) is a well-known post-hoc technique for explaining black-box models. While very useful, recent research highlights challenges around the explanations generated. In particular, there is a potential lack of stability, where the explanations provided vary over repeated runs of the algorithm, casting doubt on their reliability. This paper investigates the stability of LIME when applied to multivariate time series classification. We demonstrate that the traditional methods for generating neighbours used in LIME carry a high risk of creating ‘fake’ neighbours, which are out-of-distribution in respect to the trained model and far away from the input to be explained. This risk is particularly pronounced for time series data because of their substantial temporal dependencies. We discuss how these out-of-distribution neighbours contribute to unstable explanations. Furthermore, LIME weights neighbours based on user-defined hyperparameters which are problem-dependent and hard to tune. We show how unsuitable hyperparameters can impact the stability of explanations. We propose a two-fold approach to address these issues. First, a generative model is employed to approximate the distribution of the training data set, from which within-distribution samples and thus meaningful neighbours can be created for LIME. Second, an adaptive weighting method is designed in which the hyperparameters are easier to tune than those of the traditional method. Experiments on real-world data sets demonstrate the effectiveness of the proposed method in providing more stable explanations using the LIME framework. In addition, in-depth discussions are provided on the reasons behind these results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002697",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Generative grammar",
      "Generative model",
      "Hyperparameter",
      "Interpretability",
      "Lime",
      "Machine learning",
      "Materials science",
      "Medicine",
      "Metallurgy",
      "Paleontology",
      "Radiology",
      "Series (stratigraphy)",
      "Stability (learning theory)",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Meng",
        "given_name": "Han"
      },
      {
        "surname": "Wagner",
        "given_name": "Christian"
      },
      {
        "surname": "Triguero",
        "given_name": "Isaac"
      }
    ]
  },
  {
    "title": "Bridging flexible goal-directed cognition and consciousness: The Goal-Aligning Representation Internal Manipulation theory",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106292",
    "abstract": "Goal-directed manipulation of internal representations is a key element of human flexible behaviour, while consciousness is commonly associated with higher-order cognition and human flexibility. Current perspectives have only partially linked these processes, thus preventing a clear understanding of how they jointly generate flexible cognition and behaviour. Moreover, these limitations prevent an effective exploitation of this knowledge for technological scopes. We propose a new theoretical perspective that extends our ‘three-component theory of flexible cognition’ toward higher-order cognition and consciousness, based on the systematic integration of key concepts from Cognitive Neuroscience and AI/Robotics. The theory proposes that the function of conscious processes is to support the alignment of representations with multi-level goals. This higher alignment leads to more flexible and effective behaviours. We analyse here our previous model of goal-directed flexible cognition (validated with more than 20 human populations) as a starting GARIM-inspired model. By bridging the main theories of consciousness and goal-directed behaviour, the theory has relevant implications for scientific and technological fields. In particular, it contributes to developing new experimental tasks and interpreting clinical evidence. Finally, it indicates directions for improving machine learning and robotics systems and for informing real-world applications (e.g., in digital-twin healthcare and roboethics).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002168",
    "keywords": [
      "Artificial intelligence",
      "Bridging (networking)",
      "Cognition",
      "Cognitive architecture",
      "Cognitive robotics",
      "Cognitive science",
      "Computer network",
      "Computer science",
      "Consciousness",
      "Flexibility (engineering)",
      "Human–computer interaction",
      "Law",
      "Mathematics",
      "Neuroscience",
      "Political science",
      "Politics",
      "Psychology",
      "Representation (politics)",
      "Robot",
      "Robotics",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Granato",
        "given_name": "Giovanni"
      },
      {
        "surname": "Baldassarre",
        "given_name": "Gianluca"
      }
    ]
  },
  {
    "title": "Ensuring spatial scalability with temporal-wise spatial attentive pooling for temporal action detection",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106321",
    "abstract": "Recent temporal action detection models have focused on end-to-end trainable approaches to utilize the representational power of backbone networks. Despite the advantages of end-to-end trainable methods, these models still employ a small spatial resolution (e.g., 96 × 96) due to the inefficient trade-off between computational cost and spatial resolution. In this study, we argue that a simple pooling method (e.g., adaptive average pooling) acts as a bottleneck at the spatial aggregation part, restricting representational power. To address this issue, we propose a temporal-wise spatial attentive pooling (TSAP), which alleviates the bottleneck between the backbone and the detection head using a temporal-wise attention mechanism. Our approach mitigates the inefficient trade-off between spatial resolution and computational cost, thereby enhancing spatial scalability in temporal action detection. Moreover, TSAP is adaptable to previous end-to-end approaches by simply replacing the spatial pooling part. Our experiments demonstrated the essential role of spatial aggregation, and consistent improvements are observed by incorporating TSAP into previous end-to-end methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002454",
    "keywords": [
      "Action (physics)",
      "Artificial intelligence",
      "Computer science",
      "Database",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Pooling",
      "Quantum mechanics",
      "Scalability"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Ho-Joong"
      },
      {
        "surname": "Lee",
        "given_name": "Seong-Whan"
      }
    ]
  },
  {
    "title": "On the Kolmogorov neural networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106333",
    "abstract": "In this paper, we show that the Kolmogorov two hidden layer neural network model with a continuous, discontinuous bounded and unbounded activation function in the second hidden layer can precisely represent continuous, discontinuous bounded and all unbounded multivariate functions, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002570",
    "keywords": [
      "Activation function",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Bounded function",
      "Chemistry",
      "Computer science",
      "Continuous function (set theory)",
      "Differential algebraic equation",
      "Differential equation",
      "Evolutionary biology",
      "Function (biology)",
      "Kolmogorov equations (Markov jump process)",
      "Layer (electronics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Multivariate statistics",
      "Ordinary differential equation",
      "Organic chemistry"
    ],
    "authors": [
      {
        "surname": "Ismayilova",
        "given_name": "Aysu"
      },
      {
        "surname": "Ismailov",
        "given_name": "Vugar E."
      }
    ]
  },
  {
    "title": "A Joint Time-Frequency Domain Transformer for multivariate time series forecasting",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106334",
    "abstract": "In order to enhance the performance of Transformer models for long-term multivariate forecasting while minimizing computational demands, this paper introduces the Joint Time-Frequency Domain Transformer (JTFT). JTFT combines time and frequency domain representations to make predictions. The frequency domain representation efficiently extracts multi-scale dependencies while maintaining sparsity by utilizing a small number of learnable frequencies. Simultaneously, the time domain (TD) representation is derived from a fixed number of the most recent data points, strengthening the modeling of local relationships and mitigating the effects of non-stationarity. Importantly, the length of the representation remains independent of the input sequence length, enabling JTFT to achieve linear computational complexity. Furthermore, a low-rank attention layer is proposed to efficiently capture cross-dimensional dependencies, thus preventing performance degradation resulting from the entanglement of temporal and channel-wise modeling. Experimental results on eight real-world datasets demonstrate that JTFT outperforms state-of-the-art baselines in predictive performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002582",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computational complexity theory",
      "Computer science",
      "Computer vision",
      "Frequency domain",
      "Law",
      "Machine learning",
      "Multivariate statistics",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Representation (politics)",
      "Time domain",
      "Time series",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yushu"
      },
      {
        "surname": "Liu",
        "given_name": "Shengzhuo"
      },
      {
        "surname": "Yang",
        "given_name": "Jinzhe"
      },
      {
        "surname": "Jing",
        "given_name": "Hao"
      },
      {
        "surname": "Zhao",
        "given_name": "Wenlai"
      },
      {
        "surname": "Yang",
        "given_name": "Guangwen"
      }
    ]
  },
  {
    "title": "TGIN: Document-level event extraction with two-phase graph inference network",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106343",
    "abstract": "Document-level event extraction aims to extract event records from a whole document that contain numerous entities scattered across multiple sentences. Efficiently modeling the interactions among these entities is crucial. However, previous methods suffer from two main shortcomings. Firstly, they tend to implicitly model key information, which can result in representations with higher levels of noise. Secondly, they excessively consider irrelevant entities, thereby reducing extraction efficiency and precision. To address these issues, we propose a novel Two-phase Graph Inference Network (TGIN) approach for extracting document-level events. In the first phase, TGIN constructs a heterogeneous document-level graph to capture complex interactions among nodes of different granularity, enabling the acquisition of document-aware features. Subsequently, a dedicated module is developed to extract relevant entity pairs within the same event record. This module utilizes a key information aggregator with an attention mechanism to explicitly aggregate key sentences for entity pairs. In the second phase, the entity links predicted in the first phase serve as prior information to construct the entity-level graph, which focuses on modeling interactions between entity pairs that potentially share the same event link, effectively reducing error propagation. Experimental results on the publicly available document-level event extraction dataset ChFinAnn demonstrate the superiority of our framework over most existing models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002673",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Construct (python library)",
      "Data mining",
      "Event (particle physics)",
      "Granularity",
      "Graph",
      "Inference",
      "Information extraction",
      "Information retrieval",
      "Key (lock)",
      "News aggregator",
      "Operating system",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhong",
        "given_name": "Yu"
      },
      {
        "surname": "Shen",
        "given_name": "Bo"
      },
      {
        "surname": "Wang",
        "given_name": "Tao"
      }
    ]
  },
  {
    "title": "High-performance deep spiking neural networks via at-most-two-spike exponential coding",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106346",
    "abstract": "Spiking neural networks (SNNs) provide necessary models and algorithms for neuromorphic computing. A popular way of building high-performance deep SNNs is to convert ANNs to SNNs, taking advantage of advanced and well-trained ANNs. Here we propose an ANN to SNN conversion methodology that uses a time-based coding scheme, named At-most-two-spike Exponential Coding (AEC), and a corresponding AEC spiking neuron model for ANN-SNN conversion. AEC neurons employ quantization-compensating spikes to improve coding accuracy and capacity, with each neuron generating up to two spikes within the time window. Two exponential decay functions with tunable parameters are proposed to represent the dynamic encoding thresholds, based on which pixel intensities are encoded into spike times and spike times are decoded into pixel intensities. The hyper-parameters of AEC neurons are fine-tuned based on the loss function of SNN-decoded values and ANN-activation values. In addition, we design two regularization terms for the number of spikes, providing the possibility to achieve the best trade-off between accuracy, latency and power consumption. The experimental results show that, compared to other similar methods, the proposed scheme not only obtains deep SNNs with higher accuracy, but also has more significant advantages in terms of energy efficiency and inference latency. More details can be found at https://github.com/RPDS2020/AEC.git.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002703",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Coding (social sciences)",
      "Computer science",
      "Deep neural networks",
      "Exponential function",
      "Mathematical analysis",
      "Mathematics",
      "Neural coding",
      "Neuromorphic engineering",
      "Pattern recognition (psychology)",
      "Pixel",
      "Quantization (signal processing)",
      "Software engineering",
      "Spike (software development)",
      "Spike train",
      "Spiking neural network",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yunhua"
      },
      {
        "surname": "Feng",
        "given_name": "Ren"
      },
      {
        "surname": "Xiong",
        "given_name": "Zhimin"
      },
      {
        "surname": "Xiao",
        "given_name": "Jinsheng"
      },
      {
        "surname": "Liu",
        "given_name": "Jian K."
      }
    ]
  },
  {
    "title": "The role of directed cycles in a directed neural network",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106329",
    "abstract": "This paper investigates the dynamics of a directed acyclic neural network by edge adding control. We find that the local stability and Hopf bifurcation of the controlled network only depend on the size and intersection of directed cycles, instead of the number and position of the added edges. More specifically, if there is no cycle in the controlled network, the local dynamics of the network will remain unchanged and Hopf bifurcation will not occur even if the number of added edges is sufficient. However, if there exist cycles, then the network may undergo Hopf bifurcation. Our results show that the cycle structure is a necessary condition for the generation of Hopf bifurcation, and the bifurcation threshold is determined by the number, size, and intersection of cycles. Numerical experiments are provided to support the validity of the theory.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002533",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Nervous system network models",
      "Time delay neural network",
      "Types of artificial neural networks"
    ],
    "authors": [
      {
        "surname": "Dai",
        "given_name": "Qinrui"
      },
      {
        "surname": "Zhou",
        "given_name": "Jin"
      },
      {
        "surname": "Kong",
        "given_name": "Zhengmin"
      }
    ]
  },
  {
    "title": "Improved weight initialization for deep and narrow feedforward neural network",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106362",
    "abstract": "Appropriate weight initialization settings, along with the ReLU activation function, have become cornerstones of modern deep learning, enabling the training and deployment of highly effective and efficient neural network models across diverse areas of artificial intelligence. The problem of “dying ReLU,” where ReLU neurons become inactive and yield zero output, presents a significant challenge in the training of deep neural networks with ReLU activation function. Theoretical research and various methods have been introduced to address the problem. However, even with these methods and research, training remains challenging for extremely deep and narrow feedforward networks with ReLU activation function. In this paper, we propose a novel weight initialization method to address this issue. We establish several properties of our initial weight matrix and demonstrate how these properties enable the effective propagation of signal vectors. Through a series of experiments and comparisons with existing methods, we demonstrate the effectiveness of the novel initialization method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002867",
    "keywords": [
      "Activation function",
      "Artificial intelligence",
      "Artificial neural network",
      "Backpropagation",
      "Biology",
      "Computer science",
      "Control engineering",
      "Deep learning",
      "Engineering",
      "Evolutionary biology",
      "Feed forward",
      "Feedforward neural network",
      "Function (biology)",
      "Initialization",
      "Machine learning",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Hyunwoo"
      },
      {
        "surname": "Kim",
        "given_name": "Yunho"
      },
      {
        "surname": "Yang",
        "given_name": "Seung Yeop"
      },
      {
        "surname": "Choi",
        "given_name": "Hayoung"
      }
    ]
  },
  {
    "title": "Uncertainty-guided label correction with wavelet-transformed discriminative representation enhancement",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106383",
    "abstract": "Label noises, categorized into closed-set noise and open-set noise, are prevalent in real-world scenarios and can seriously hinder the generalization ability of models. Identifying noise is challenging because noisy samples closely resemble true positives. Existing approaches often assume a single noise source, oversimplify closed-set noise, or treat open-set noise as toxic and eliminate it, resulting in limited practical effects. To address these issues, we present a novel approach named uncertainty-guided label correction with wavelet-transformed discriminative representation enhancement (Ultra), designed to mitigate the effects of mixed noise. Specifically, our approach considers a more practical noise setting. To achieve robust mixed-noise identification, we initially look into a learnable wavelet filter for obtaining discriminative features and filtering spurious cues automatically at the representation level. Subsequently, we introduce a two-fold uncertainty estimation to stably locate noise within the corrupted supervised signal level. These insights pave the way for a simple yet potent label correction technique, enabling comprehensive utilization of open-set noise, which can be rendered non-toxic in a specific manner, in contrast to harmful closed-set noise. Experimental validation on datasets with synthetic mixed noise, web noise corruption, and a real-world dataset confirms the effectiveness and generality of Ultra. Furthermore, our approach enhances the application of efficient techniques (e.g., supervised contrastive learning) within label noise scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003071",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Discriminative model",
      "Filter (signal processing)",
      "Generalization",
      "Image (mathematics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Noise (video)",
      "Noise measurement",
      "Noise reduction",
      "Pattern recognition (psychology)",
      "Programming language",
      "Set (abstract data type)",
      "Spurious relationship",
      "Wavelet"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Tingting"
      },
      {
        "surname": "Ding",
        "given_name": "Xiao"
      },
      {
        "surname": "Zhang",
        "given_name": "Hao"
      },
      {
        "surname": "Tang",
        "given_name": "Minji"
      },
      {
        "surname": "Qin",
        "given_name": "Bing"
      },
      {
        "surname": "Liu",
        "given_name": "Ting"
      }
    ]
  },
  {
    "title": "Threshold learning algorithm for memristive neural network with binary switching behavior",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106355",
    "abstract": "On-chip learning is an effective method for adjusting artificial neural networks in neuromorphic computing systems by considering hardware intrinsic properties. However, it faces challenges due to hardware nonidealities, such as the nonlinearity of potentiation and depression and limitations on fine weight adjustment. In this study, we propose a threshold learning algorithm for a variation-tolerant ternary neural network in a memristor crossbar array. This algorithm utilizes two tightly separated resistance states in memristive devices to represent weight values. The high-resistance state (HRS) and low-resistance state (LRS) defined as read current of < 0.1 μA and > 1 μA, respectively, were successfully programmed in a 32 × 32 crossbar array, and exhibited half-normal distributions due to the programming method. To validate our approach experimentally, a 64 × 10 single-layer fully connected network were trained in the fabricated crossbar for an 8 × 8 MNIST dataset using the threshold learning algorithm, where the weight value is updated when a gradient determined by backpropagation exceeds a threshold value. Thanks to the large margin between the two states of the memristor, we observed only a 0.42 % drop in classification accuracy compared to the baseline network results. The threshold learning algorithm is expected to alleviate the programming burden and be utilized in variation-tolerant neuromorphic architectures.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400279X",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Artificial neural network",
      "Backpropagation",
      "Binary number",
      "Computer science",
      "Crossbar switch",
      "Electronic engineering",
      "Engineering",
      "MNIST database",
      "Mathematics",
      "Memristor",
      "Neuromorphic engineering",
      "Synaptic weight",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Youn",
        "given_name": "Sangwook"
      },
      {
        "surname": "Hwang",
        "given_name": "Yeongjin"
      },
      {
        "surname": "Kim",
        "given_name": "Tae-Hyeon"
      },
      {
        "surname": "Kim",
        "given_name": "Sungjoon"
      },
      {
        "surname": "Hwang",
        "given_name": "Hwiho"
      },
      {
        "surname": "Park",
        "given_name": "Jinwoo"
      },
      {
        "surname": "Kim",
        "given_name": "Hyungjin"
      }
    ]
  },
  {
    "title": "Learning dynamic graph representations through timespan view contrasts",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106384",
    "abstract": "The rich information underlying graphs has inspired further investigation of unsupervised graph representation. Existing studies mainly depend on node features and topological properties within static graphs to create self-supervised signals, neglecting the temporal components carried by real-world graph data, such as timestamps of edges. To overcome this limitation, this paper explores how to model temporal evolution on dynamic graphs elegantly. Specifically, we introduce a new inductive bias, namely temporal translation invariance, which illustrates the tendency of the identical node to keep similar labels across different timespans. Based on this assumption, we develop a dynamic graph representation framework CLDG that encourages the node to maintain locally consistent temporal translation invariance through contrastive learning on different timespans. Except for standard CLDG which only considers explicit topological links, our further proposed CLDG＋＋additionally employs graph diffusion to uncover global contextual correlations between nodes, and designs a multi-scale contrastive learning objective composed of local–local, local–global, and global–global contrasts to enhance representation capabilities. Interestingly, by measuring the consistency between different timespans to shape anomaly indicators, CLDG and CLDG＋＋are seamlessly integrated with the task of spotting anomalies on dynamic graphs, which has broad applications in many high-impact domains, such as finance, cybersecurity, and healthcare. Experiments demonstrate that CLDG and CLDG＋＋both exhibit desirable performance in downstream tasks including node classification and dynamic graph anomaly detection. Moreover, CLDG significantly reduces time and space complexity by implicitly exploiting temporal cues instead of complicated sequence models. The code and data are available at https://github.com/yimingxu24/CLDG.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003083",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Engineering",
      "Feature learning",
      "Graph",
      "Law",
      "Node (physics)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Structural engineering",
      "Theoretical computer science",
      "Timestamp"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Yiming"
      },
      {
        "surname": "Peng",
        "given_name": "Zhen"
      },
      {
        "surname": "Shi",
        "given_name": "Bin"
      },
      {
        "surname": "Hua",
        "given_name": "Xu"
      },
      {
        "surname": "Dong",
        "given_name": "Bo"
      }
    ]
  },
  {
    "title": "Span-based few-shot event detection via aligning external knowledge",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106327",
    "abstract": "Few-shot Event Detection (FSED) aims to identify novel event types in new domains with very limited annotated data. Previous PN-based (Prototypical Network) joint methods suffer from insufficient learning of token-wise label dependency and inaccurate prototypes. To solve these problems, we propose a span-based FSED model, called SpanFSED, which decomposes FSED into two subprocesses, including span extractor and event classifier. In span extraction, we convert sequential labels into a global boundary matrix that enables the span extractor to acquire precise boundary information irrespective of label dependency. In event classification, we align event types with an outside knowledge base like FrameNet and construct an enhanced support set, which injects more trigger information into the prototypical network of event prototypes. The superior performance of SpanFSED is demonstrated through extensive experiments on four event detection datasets, i.e., ACE2005, ERE, MAVEN and FewEvent. Access to our code and data is facilitated through the following link: .",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400251X",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Civil engineering",
      "Computer science",
      "Computer vision",
      "Engineering",
      "Event (particle physics)",
      "Mechanical engineering",
      "One shot",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Shot (pellet)",
      "Span (engineering)"
    ],
    "authors": [
      {
        "surname": "Ling",
        "given_name": "Tongtao"
      },
      {
        "surname": "Chen",
        "given_name": "Lei"
      },
      {
        "surname": "Lai",
        "given_name": "Yutao"
      },
      {
        "surname": "Liu",
        "given_name": "Hai-Lin"
      }
    ]
  },
  {
    "title": "Multi-scale full spike pattern for semantic segmentation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106330",
    "abstract": "Spiking neural networks (SNNs), as the brain-inspired neural networks, encode information in spatio-temporal dynamics. They have the potential to serve as low-power alternatives to artificial neural networks (ANNs) due to their sparse and event-driven nature. However, existing SNN-based models for pixel-level semantic segmentation tasks suffer from poor performance and high memory overhead, failing to fully exploit the computational effectiveness and efficiency of SNNs. To address these challenges, we propose the multi-scale and full spike segmentation network (MFS-Seg), which is based on the deep direct trained SNN and represents the first attempt to train a deep SNN with surrogate gradients for semantic segmentation. Specifically, we design an efficient fully-spike residual block (EFS-Res) to alleviate representation issues caused by spiking noise on different channels. EFS-Res utilizes depthwise separable convolution to improve the distributions of spiking feature maps. The visualization shows that our model can effectively extract the edge features of segmented objects. Furthermore, it can significantly reduce the memory overhead and energy consumption of the network. In addition, we theoretically analyze and prove that EFS-Res can avoid the degradation problem based on block dynamical isometry theory. Experimental results on the Camvid dataset, the DDD17 dataset, and the DSEC-Semantic dataset show that our model achieves comparable performance to the mainstream UNet network with up to 31 × fewer parameters, while significantly reducing power consumption by over 13 × . Overall, our MFS-Seg model demonstrates promising results in terms of performance, memory efficiency, and energy consumption, showcasing the potential of deep SNNs for semantic segmentation tasks. Our code is available in https://github.com/BICLab/MFS-Seg.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002545",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Block (permutation group theory)",
      "Computer science",
      "Convolutional neural network",
      "Geometry",
      "Mathematics",
      "Operating system",
      "Overhead (engineering)",
      "Pattern recognition (psychology)",
      "Segmentation",
      "Software engineering",
      "Spike (software development)",
      "Spiking neural network"
    ],
    "authors": [
      {
        "surname": "Su",
        "given_name": "Qiaoyi"
      },
      {
        "surname": "He",
        "given_name": "Weihua"
      },
      {
        "surname": "Wei",
        "given_name": "Xiaobao"
      },
      {
        "surname": "Xu",
        "given_name": "Bo"
      },
      {
        "surname": "Li",
        "given_name": "Guoqi"
      }
    ]
  },
  {
    "title": "Long-term causal effects estimation via latent surrogates representation learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106336",
    "abstract": "Estimating long-term causal effects based on short-term surrogates is a significant but challenging problem in many real-world applications such as marketing and medicine. Most existing methods estimate causal effects in an idealistic and simplistic manner — disregarding unobserved surrogates and treating all short-term outcomes as surrogates. However, such methods are not well-suited to real-world scenarios where the partially observed surrogates are mixed with the proxies of unobserved surrogates among short-term outcomes. To address this issue, we develop our flexible method called LASER to estimate long-term causal effects in a more realistic situation where the surrogates are either observed or have observed proxies. In LASER, we employ an identifiable variational autoencoder to learn the latent surrogate representation by using all the surrogate candidates without the need to distinguish observed surrogates or proxies of unobserved surrogates. With the learned representation, we further devise a theoretically guaranteed and unbiased estimation of long-term causal effects. Extensive experimental results on the real-world and semi-synthetic datasets demonstrate the effectiveness of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002600",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Economics",
      "Estimation",
      "Latent variable",
      "Law",
      "Machine learning",
      "Management",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Quantum mechanics",
      "Representation (politics)",
      "Term (time)"
    ],
    "authors": [
      {
        "surname": "Cai",
        "given_name": "Ruichu"
      },
      {
        "surname": "Chen",
        "given_name": "Weilin"
      },
      {
        "surname": "Yang",
        "given_name": "Zeqin"
      },
      {
        "surname": "Wan",
        "given_name": "Shu"
      },
      {
        "surname": "Zheng",
        "given_name": "Chen"
      },
      {
        "surname": "Yang",
        "given_name": "Xiaoqing"
      },
      {
        "surname": "Guo",
        "given_name": "Jiecheng"
      }
    ]
  },
  {
    "title": "Mixing neural networks, continuation and symbolic computation to solve parametric systems of non linear equations",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106316",
    "abstract": "We consider a square non linear parametric equations system F(P,X) = 0 which is constituted of n non differential equations in the n unknowns { x 1 , … , x n } that are the components of X while P = { p 1 , … , p m } is a set of m parameters that play a role in the definition of the equations F . We assume that P is restricted to lie in a bounded region and we are interested in developing a solver for obtaining all real solutions exactly (a notion that is defined in the paper) for any parameter values within the bounded region. The starting point of the proposed approach is that we assume that a numerical methods has allowed us to determine the real solutions (but not necessarily all of them) for a very limited number of fixed P called the initial solution set. Starting from this set we show that we can create multiple pairs (parameters, solution) and that these pairs may be structured into coherent learning sets that will be used to train multi-layer perceptrons (MLP). The training process is specific: although it still uses a decrease of a loss function its main objective is to maximize the success rate i.e. the number of occurrences, expressed in percentage of number of samples of the training set, for which the Newton scheme, initialized with the MLP prediction, converges toward the expected solution. We then show that for a sufficiently large number of MLPs we may obtain a 100% success rate for all learning sets. The solver is obtained by running a set of local solvers each of which is based on a specific MLP whose prediction may lead to an exact solution of the system. This solver is tested on verification sets i.e. set of samples constituted of parameter values (all different from the samples in the learning set) and all the solutions of the corresponding system. We show that these sets may be automatically generated and that they may also be used in a self-learning process for improving the performance of the solver established from the initial solution set. This approach is illustrated on two engineering problems in robotics and chemistry and it is shown that the solver provides all solutions for any instance of P with a high probability although we cannot guarantee it. The time required to design the final solver is large but the solving time is extremely low so that this approach should be used when the system has to be solved for a sufficient number of occurrences of the P . Furthermore we will show that the computation time required for establishing the solver may be drastically reduced by using a distributed implementation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002405",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Bounded function",
      "Computer science",
      "Evolutionary biology",
      "Function (biology)",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Parametric statistics",
      "Perceptron",
      "Programming language",
      "Set (abstract data type)",
      "Solver",
      "Statistics",
      "System of linear equations"
    ],
    "authors": [
      {
        "surname": "Merlet",
        "given_name": "J.-P."
      }
    ]
  },
  {
    "title": "Learning active subspaces and discovering important features with Gaussian radial basis functions neural networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106335",
    "abstract": "Providing a model that achieves a strong predictive performance and is simultaneously interpretable by humans is one of the most difficult challenges in machine learning research due to the conflicting nature of these two objectives. To address this challenge, we propose a modification of the radial basis function neural network model by equipping its Gaussian kernel with a learnable precision matrix. We show that precious information is contained in the spectrum of the precision matrix that can be extracted once the training of the model is completed. In particular, the eigenvectors explain the directions of maximum sensitivity of the model revealing the active subspace and suggesting potential applications for supervised dimensionality reduction. At the same time, the eigenvectors highlight the relationship in terms of absolute variation between the input and the latent variables, thereby allowing us to extract a ranking of the input variables based on their importance to the prediction task enhancing the model interpretability. We conducted numerical experiments for regression, classification, and feature selection tasks, comparing our model against popular machine learning models, the state-of-the-art deep learning-based embedding feature selection techniques, and a transformer model for tabular data. Our results demonstrate that the proposed model does not only yield an attractive prediction performance compared to the competitors but also provides meaningful and interpretable results that potentially could assist the decision-making process in real-world applications. A PyTorch implementation of the model is available on GitHub at the following link. 1 1 https://github.com/dannyzx/Gaussian-RBFNN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002594",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Dimensionality reduction",
      "Feature selection",
      "Gaussian",
      "Gaussian process",
      "Interpretability",
      "MNIST database",
      "Machine learning",
      "Model selection",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "D’Agostino",
        "given_name": "Danny"
      },
      {
        "surname": "Ilievski",
        "given_name": "Ilija"
      },
      {
        "surname": "Shoemaker",
        "given_name": "Christine Annette"
      }
    ]
  },
  {
    "title": "Channel reflection: Knowledge-driven data augmentation for EEG-based brain–computer interfaces",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106351",
    "abstract": "A brain–computer interface (BCI) enables direct communication between the human brain and external devices. Electroencephalography (EEG) based BCIs are currently the most popular for able-bodied users. To increase user-friendliness, usually a small amount of user-specific EEG data are used for calibration, which may not be enough to develop a pure data-driven decoding model. To cope with this typical calibration data shortage challenge in EEG-based BCIs, this paper proposes a parameter-free channel reflection (CR) data augmentation approach that incorporates prior knowledge on the channel distributions of different BCI paradigms in data augmentation. Experiments on eight public EEG datasets across four different BCI paradigms (motor imagery, steady-state visual evoked potential, P300, and seizure classifications) using different decoding algorithms demonstrated that: (1) CR is effective, i.e., it can noticeably improve the classification accuracy; (2) CR is robust, i.e., it consistently outperforms existing data augmentation approaches in the literature; and, (3) CR is flexible, i.e., it can be combined with other data augmentation approaches to further improve the performance. We suggest that data augmentation approaches like CR should be an essential step in EEG-based BCIs. Our code is available online.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002752",
    "keywords": [
      "Artificial intelligence",
      "Brain–computer interface",
      "Channel (broadcasting)",
      "Computer science",
      "Electroencephalography",
      "Human–computer interaction",
      "Neuroscience",
      "Programming language",
      "Psychology",
      "Reflection (computer programming)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Ziwei"
      },
      {
        "surname": "Li",
        "given_name": "Siyang"
      },
      {
        "surname": "Luo",
        "given_name": "Jingwei"
      },
      {
        "surname": "Liu",
        "given_name": "Jiajing"
      },
      {
        "surname": "Wu",
        "given_name": "Dongrui"
      }
    ]
  },
  {
    "title": "Salience Interest Option: Temporal abstraction with salience interest functions",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106342",
    "abstract": "Reinforcement Learning (RL) is a significant machine learning subfield that emphasizes learning actions based on environment to obtain optimal behavior policy. RL agents can make decisions at variable time scales in the form of temporal abstractions, also known as options. The issue of discovering options has seen a considerable research effort. Most notably, the Interest Option Critic (IOC) algorithm first extends the initial set to the interest function, providing a method for learning options specialized to certain state space regions. This approach offers a specific attention mechanism for action selection. Unfortunately, this method still suffers from the classic issues of poor data efficiency and lack of flexibility in RL when learning options end-to-end through backpropagation. This paper proposes a new approach called Salience Interest Option Critic (SIOC), which chooses subsets of existing initiation sets for RL. Specifically, these subsets are not learned by backpropagation, which is slow and tends to overfit, but through particle filters. This approach enables the rapid and flexible identification of critical subsets using only reward feedback. We conducted experiments in discrete and continuous domains, and our proposed method demonstrate higher efficiency and flexibility than other methods. The generated options are more valuable within a single task and exhibited greater interpretability and reusability in multi-task learning scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002661",
    "keywords": [
      "Abstraction",
      "Artificial intelligence",
      "Cognitive psychology",
      "Computer science",
      "Epistemology",
      "Philosophy",
      "Psychology",
      "Salience (neuroscience)"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Xianchao"
      },
      {
        "surname": "Zhao",
        "given_name": "Liang"
      },
      {
        "surname": "Zhu",
        "given_name": "William"
      }
    ]
  },
  {
    "title": "Any region can be perceived equally and effectively on rotation pretext task using full rotation and weighted-region mixture",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106350",
    "abstract": "In recent years, self-supervised learning has emerged as a powerful approach to learning visual representations without requiring extensive manual annotation. One popular technique involves using rotation transformations of images, which provide a clear visual signal for learning semantic representation. However, in this work, we revisit the pretext task of predicting image rotation in self-supervised learning and discover that it tends to marginalise the perception of features located near the centre of an image. To address this limitation, we propose a new self-supervised learning method, namely FullRot, which spotlights underrated regions by resizing the randomly selected and cropped regions of images. Moreover, FullRot increases the complexity of the rotation pretext task by applying the degree-free rotation to the region cropped into a circle. To encourage models to learn from different general parts of an image, we introduce a new data mixture technique called WRMix, which merges two random intra-image patches. By combining these innovative crop and rotation methods with the data mixture scheme, our approach, FullRot + WRMix, surpasses the state-of-the-art self-supervision methods in classification, segmentation, and object detection tasks on ten benchmark datasets with an improvement of up to +13.98% accuracy on STL-10, +8.56% accuracy on CIFAR-10, +10.20% accuracy on Sports-100, +15.86% accuracy on Mammals-45, +15.15% accuracy on PAD-UFES-20, +32.44% mIoU on VOC 2012, +7.62% mIoU on ISIC 2018, +9.70% mIoU on FloodArea, +25.16% AP50 on VOC 2007, and +58.69% AP50 on UTDAC 2020. The code is available at https://github.com/anthonyweidai/FullRot_WRMix.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002740",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Cartography",
      "Computer science",
      "Computer vision",
      "Economics",
      "Geography",
      "Image (mathematics)",
      "Law",
      "Machine learning",
      "Management",
      "Neuroscience",
      "Object detection",
      "Pattern recognition (psychology)",
      "Perception",
      "Political science",
      "Politics",
      "Pretext",
      "Rotation (mathematics)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Dai",
        "given_name": "Wei"
      },
      {
        "surname": "Wu",
        "given_name": "Tianyi"
      },
      {
        "surname": "Liu",
        "given_name": "Rui"
      },
      {
        "surname": "Wang",
        "given_name": "Min"
      },
      {
        "surname": "Yin",
        "given_name": "Jianqin"
      },
      {
        "surname": "Liu",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Dual-learning Multi-hop Nonnegative Matrix Factorization for community detection",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106360",
    "abstract": "As an important branch of network science, community detection has garnered significant attention. Among various community detection methods, nonnegative matrix factorization (NMF)-based community detection approaches have become a popular research topic. However, most NMF-based methods overlook the network’s multi-hop information, let alone the community detection results specific to each hop of the network. In this paper, we propose Dual-learning Multi-hop NMF (DL-MHNMF), a method that considers not only the multi-hop connectivity between two nodes but also factors in the shared results across multiple hops and the impact of differences in the specific results at each hop on the shared outcomes. An efficient iterative optimization algorithm with guaranteed theoretical convergence is proposed for solving DL-MHNMF. Methodologically, by iteratively removing the specific results during the optimization process of DL-MHNMF, we achieve enhanced detection accuracy, which is also verified by subsequent experiments. Specifically, we compare fourteen algorithms on eleven publicly available datasets, and experimental results show that our algorithm outperforms most state-of-the-art methods. The source code is availiable at https://github.com/bx20000827/DL-MHNMF.git.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002843",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer network",
      "Computer science",
      "Convergence (economics)",
      "Dual (grammatical number)",
      "Economic growth",
      "Economics",
      "Eigenvalues and eigenvectors",
      "Hop (telecommunications)",
      "Literature",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Matrix decomposition",
      "Non-negative matrix factorization",
      "Optimization algorithm",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Bai",
        "given_name": "Xu"
      },
      {
        "surname": "Chen",
        "given_name": "Bilian"
      },
      {
        "surname": "Zhuo",
        "given_name": "Zhijian"
      }
    ]
  },
  {
    "title": "Multi-modal long document classification based on Hierarchical Prompt and Multi-modal Transformer",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106322",
    "abstract": "In the realm of long document classification (LDC), previous research has predominantly focused on modeling unimodal texts, overlooking the potential of multi-modal documents incorporating images. To address this gap, we introduce an innovative approach for multi-modal long document classification based on the Hierarchical Prompt and Multi-modal Transformer (HPMT). The proposed HPMT method facilitates multi-modal interactions at both the section and sentence levels, enabling a comprehensive capture of hierarchical structural features and complex multi-modal associations of long documents. Specifically, a Multi-scale Multi-modal Transformer (MsMMT) is tailored to capture the multi-granularity correlations between sentences and images. This is achieved through the incorporation of multi-scale convolutional kernels on sentence features, enhancing the model’s ability to discern intricate patterns. Furthermore, to facilitate cross-level information interaction and promote learning of specific features at different levels, we introduce a Hierarchical Prompt (HierPrompt) block. This block incorporates section-level prompts and sentence-level prompts, both derived from a global prompt via distinct projection networks. Extensive experiments are conducted on four challenging multi-modal long document datasets. The results conclusively demonstrate the superiority of our proposed method, showcasing its performance advantages over existing techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002466",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Electrical engineering",
      "Engineering",
      "Granularity",
      "Machine learning",
      "Modal",
      "Natural language processing",
      "Operating system",
      "Pattern recognition (psychology)",
      "Polymer chemistry",
      "Sentence",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Tengfei"
      },
      {
        "surname": "Hu",
        "given_name": "Yongli"
      },
      {
        "surname": "Gao",
        "given_name": "Junbin"
      },
      {
        "surname": "Wang",
        "given_name": "Jiapu"
      },
      {
        "surname": "Sun",
        "given_name": "Yanfeng"
      },
      {
        "surname": "Yin",
        "given_name": "Baocai"
      }
    ]
  },
  {
    "title": "Towards complex dynamic physics system simulation with graph neural ordinary equations",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106341",
    "abstract": "The great learning ability of deep learning facilitates us to comprehend the real physical world, making learning to simulate complicated particle systems a promising endeavour both in academia and industry. However, the complex laws of the physical world pose significant challenges to the learning based simulations, such as the varying spatial dependencies between interacting particles and varying temporal dependencies between particle system states in different time stamps, which dominate particles’ interacting behavior and the physical systems’ evolution patterns. Existing learning based methods fail to fully account for the complexities, making them unable to yield satisfactory simulations. To better comprehend the complex physical laws, we propose a novel model – Graph Networks with Spatial–Temporal neural Ordinary Differential Equations (GNSTODE) – that characterizes the varying spatial and temporal dependencies in particle systems using a united end-to-end framework. Through training with real-world particle–particle interaction observations, GNSTODE can simulate any possible particle systems with high precisions. We empirically evaluate GNSTODE’s simulation performance on two real-world particle systems, Gravity and Coulomb, with varying levels of spatial and temporal dependencies. The results show that GNSTODE yields better simulations than state-of-the-art methods, showing that GNSTODE can serve as an effective tool for particle simulation in real-world applications. Our code is made available at https://github.com/Guangsi-Shi/AI-for-physics-GNSTODE.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400265X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Complex system",
      "Computer science",
      "Differential equation",
      "Geology",
      "Graph",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Oceanography",
      "Operating system",
      "Ordinary differential equation",
      "Particle (ecology)",
      "Particle system",
      "Physical law",
      "Physical system",
      "Physics",
      "Quantum mechanics",
      "Statistical physics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Guangsi"
      },
      {
        "surname": "Zhang",
        "given_name": "Daokun"
      },
      {
        "surname": "Jin",
        "given_name": "Ming"
      },
      {
        "surname": "Pan",
        "given_name": "Shirui"
      },
      {
        "surname": "Yu",
        "given_name": "Philip S."
      }
    ]
  },
  {
    "title": "Contrastive Prototype-Guided Generation for Generalized Zero-Shot Learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106324",
    "abstract": "Generalized zero-shot learning (GZSL) aims to recognize both seen and unseen classes, while only samples from seen classes are available for training. The mainstream methods mitigate the lack of unseen training data by simulating the visual unseen samples. However, the sample generator is actually learned with just seen-class samples, and semantic descriptions of unseen classes are just provided to the pre-trained sample generator for unseen data generation, therefore, the generator would have bias towards seen categories, and the unseen generation quality, including both precision and diversity, is still the main learning challenge. To this end, we propose a Prototype-Guided Generation for Generalized Zero-Shot Learning (PGZSL), in order to guide the sample generation with unseen knowledge. First, unseen data generation is guided and rectified in PGZSL by contrastive prototypical anchors with both class semantic consistency and feature discriminability. Second, PGZSL introduces Certainty-Driven Mixup for generator to enrich the diversity of generated unseen samples, while suppress the generation of uncertain boundary samples as well. Empirical results over five benchmark datasets show that PGZSL significantly outperforms the SOTA methods in both ZSL and GZSL tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400248X",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Chemistry",
      "Chromatography",
      "Class (philosophy)",
      "Computer science",
      "Consistency (knowledge bases)",
      "Feature (linguistics)",
      "Generator (circuit theory)",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Sample (material)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yunyun"
      },
      {
        "surname": "Mao",
        "given_name": "Jian"
      },
      {
        "surname": "Guo",
        "given_name": "Chenguang"
      },
      {
        "surname": "Chen",
        "given_name": "Songcan"
      }
    ]
  },
  {
    "title": "Binary matrix factorization via collaborative neurodynamic optimization",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106348",
    "abstract": "Binary matrix factorization is an important tool for dimension reduction for high-dimensional datasets with binary attributes and has been successfully applied in numerous areas. This paper presents a collaborative neurodynamic optimization approach to binary matrix factorization based on the original combinatorial optimization problem formulation and quadratic unconstrained binary optimization problem reformulations. The proposed approach employs multiple discrete Hopfield networks operating concurrently in search of local optima. In addition, a particle swarm optimization rule is used to reinitialize neuronal states iteratively to escape from local minima toward better ones. Experimental results on eight benchmark datasets are elaborated to demonstrate the superior performance of the proposed approach against six baseline algorithms in terms of factorization error. Additionally, the viability of the proposed approach is demonstrated for pattern discovery on three datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002727",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Binary number",
      "Chemistry",
      "Composite material",
      "Computer science",
      "Dimension (graph theory)",
      "Eigenvalues and eigenvectors",
      "Factorization",
      "Geodesy",
      "Geography",
      "Group (periodic table)",
      "Logical matrix",
      "Materials science",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Matrix decomposition",
      "Maxima and minima",
      "Optimization problem",
      "Organic chemistry",
      "Physics",
      "Pure mathematics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Hongzong"
      },
      {
        "surname": "Wang",
        "given_name": "Jun"
      },
      {
        "surname": "Zhang",
        "given_name": "Nian"
      },
      {
        "surname": "Zhang",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Decentralized stochastic sharpness-aware minimization algorithm",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106325",
    "abstract": "In recent years, distributed stochastic algorithms have become increasingly useful in the field of machine learning. However, similar to traditional stochastic algorithms, they face a challenge where achieving high fitness on the training set does not necessarily result in good performance on the test set. To address this issue, we propose to use of a distributed network topology to improve the generalization ability of the algorithms. We specifically focus on the Sharpness-Aware Minimization (SAM) algorithm, which relies on perturbation weights to find the maximum point with better generalization ability. In this paper, we present the decentralized stochastic sharpness-aware minimization (D-SSAM) algorithm, which incorporates the distributed network topology. We also provide sublinear convergence results for non-convex targets, which is comparable to consequence of Decentralized Stochastic Gradient Descent (DSGD). Finally, we empirically demonstrate the effectiveness of these results in deep networks and discuss their relationship to the generalization behavior of SAM.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002491",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convergence (economics)",
      "Distributed algorithm",
      "Distributed computing",
      "Economic growth",
      "Economics",
      "Generalization",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Minification",
      "Network topology",
      "Operating system",
      "Stochastic gradient descent",
      "Sublinear function"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Simiao"
      },
      {
        "surname": "Deng",
        "given_name": "Xiaoge"
      },
      {
        "surname": "Xu",
        "given_name": "Dongpo"
      },
      {
        "surname": "Sun",
        "given_name": "Tao"
      },
      {
        "surname": "Li",
        "given_name": "Dongsheng"
      }
    ]
  },
  {
    "title": "Life regression based patch slimming for vision transformers",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106340",
    "abstract": "Vision transformers have achieved remarkable success in computer vision tasks by using multi-head self-attention modules to capture long-range dependencies within images. However, the high inference computation cost poses a new challenge. Several methods have been proposed to address this problem, mainly by slimming patches. In the inference stage, these methods classify patches into two classes, one to keep and the other to discard in multiple layers. This approach results in additional computation at every layer where patches are discarded, which hinders inference acceleration. In this study, we tackle the patch slimming problem from a different perspective by proposing a life regression module that determines the lifespan of each image patch in one go. During inference, the patch is discarded once the current layer index exceeds its life. Our proposed method avoids additional computation and parameters in multiple layers to enhance inference speed while maintaining competitive performance. Additionally, our approach 1 1 https://github.com/cjwcommuny/life-regression. requires fewer training epochs than other patch slimming methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002648",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computation",
      "Computer science",
      "Electrical engineering",
      "Engineering",
      "Inference",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Regression",
      "Statistics",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Jiawei"
      },
      {
        "surname": "Chen",
        "given_name": "Lin"
      },
      {
        "surname": "Yang",
        "given_name": "Jiang"
      },
      {
        "surname": "Shi",
        "given_name": "Tianqi"
      },
      {
        "surname": "Cheng",
        "given_name": "Lechao"
      },
      {
        "surname": "Feng",
        "given_name": "Zunlei"
      },
      {
        "surname": "Song",
        "given_name": "Mingli"
      }
    ]
  },
  {
    "title": "Blinding and blurring the multi-object tracker with adversarial perturbations",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106331",
    "abstract": "Adversarial attack reveals a potential imperfection in deep models that they are susceptible to being tricked by imperceptible perturbations added to images. Recent deep multi-object trackers combine the functionalities of detection and association, rendering attacks on either the detector or the association component an effective means of deception. Existing attacks focus on increasing the frequency of ID switching, which greatly damages tracking stability, but is not enough to make the tracker completely ineffective. To fully explore the potential of adversarial attacks, we propose Blind-Blur Attack (BBA), a novel attack method based on spatio-temporal motion information to fool multi-object trackers. Specifically, a simple but efficient perturbation generator is trained with the blind-blur loss, simultaneously making the target invisible to the tracker and letting the background be regarded as moving targets. We take TraDeS as our main research tracker, and verify our attack method on other excellent algorithms (i.e., CenterTrack, FairMOT, and ByteTrack) on MOT-Challenge benchmark datasets (i.e., MOT16, MOT17, and MOT20). BBA attack reduced the MOTA of TraDeS and ByteTrack from 69.1 and 80.3 to −238.1 and −357.0, respectively, indicating that it is an efficient method with a high degrees of transferability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002557",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Blinding",
      "Computer science",
      "Computer vision",
      "Law",
      "MEDLINE",
      "Object (grammar)",
      "Political science"
    ],
    "authors": [
      {
        "surname": "Pang",
        "given_name": "Haibo"
      },
      {
        "surname": "Ma",
        "given_name": "Rongqi"
      },
      {
        "surname": "Su",
        "given_name": "Jie"
      },
      {
        "surname": "Liu",
        "given_name": "Chengming"
      },
      {
        "surname": "Gao",
        "given_name": "Yufei"
      },
      {
        "surname": "Jin",
        "given_name": "Qun"
      }
    ]
  },
  {
    "title": "DCDLN: A densely connected convolutional dynamic learning network for malaria disease diagnosis",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106339",
    "abstract": "Malaria is a significant health concern worldwide, particularly in Africa where its prevalence is still alarmingly high. Using artificial intelligence algorithms to diagnose cells with malaria provides great convenience for clinicians. In this paper, a densely connected convolutional dynamic learning network (DCDLN) is proposed for the diagnosis of malaria disease. Specifically, after data processing and partitioning of the dataset, the densely connected block is trained as a feature extractor. To classify the features extracted by the feature extractor, a classifier based on a dynamic learning network is proposed in this paper. Based on experimental results, the proposed DCDLN method demonstrates a diagnostic accuracy rate of 97.23%, surpassing the diagnostic performance than existing advanced methods on an open malaria cell dataset. This accurate diagnostic effect provides convincing evidence for clinicians to make a correct diagnosis. In addition, to validate the superiority and generalization capability of the DCDLN algorithm, we also applied the algorithm to the skin cancer and garbage classification datasets. DCDLN achieved good results on these datasets as well, demonstrating that the DCDLN algorithm possesses superiority and strong generalization performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002636",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Engineering",
      "Extractor",
      "Feature (linguistics)",
      "Generalization",
      "Immunology",
      "Linguistics",
      "Machine learning",
      "Malaria",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process engineering"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zhijun"
      },
      {
        "surname": "Ding",
        "given_name": "Cheng"
      },
      {
        "surname": "Zhang",
        "given_name": "Mingyang"
      },
      {
        "surname": "Luo",
        "given_name": "YaMei"
      },
      {
        "surname": "Mai",
        "given_name": "Jiajie"
      }
    ]
  },
  {
    "title": "SLTRN: Sample-level transformer-based relation network for few-shot classification",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106344",
    "abstract": "Few-shot classification recognizes novel categories with limited labeled samples. The classic Relation Network (RN) compares support-query sample pairs for few-shot classification but overlooks support set contextual information, limiting its comparison capabilities. This work reformulates learning the relationship between query samples and each support class as a seq2seq problem. We introduce a Sample-level Transformer-based Relation Network (SLTRN) that utilizes sample-level self-attention to enhance the comparison ability of the relationship module by mining potential relationships among support classes. SLTRN demonstrates comparable performance with state-of-the-art methods on benchmarks, particularly excelling in the 1-shot setting with 52.11% and 67.55% accuracy on miniImageNet and CUB, respectively. Extensive ablation experiments validate the effectiveness and optimal settings of SLTRN. The experimental code for this work is available at https://github.com/ZitZhengWang/SLTRN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002685",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Data mining",
      "Engineering",
      "Limiting",
      "Machine learning",
      "Mechanical engineering",
      "One shot",
      "Physics",
      "Quantum mechanics",
      "Relation (database)",
      "Sample (material)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Zhe"
      },
      {
        "surname": "Zheng",
        "given_name": "Wang"
      },
      {
        "surname": "Wang",
        "given_name": "Mingyang"
      }
    ]
  },
  {
    "title": "TFRS: A task-level feature rectification and separation method for few-shot video action recognition",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106326",
    "abstract": "Few-shot video action recognition (FS-VAR) is a challenging task that requires models to have significant expressive power in order to identify previously unseen classes using only a few labeled examples. However, due to the limited number of support samples, the model’s performance is highly sensitive to the distribution of the sampled data. The representativeness of the support data is insufficient to cover the entire class, and the support features may contain shared information that confuses the classifier, leading to biased classification. In response to this difficulty, we present a task-level feature rectification and separation (TFRS) method that effectively resolves the sample bias issue. Our main idea is to leverage prior information from base classes to rectify the support samples while removing the commonality of task-level features. This enhances the distinguishability and separability of features in space. Furthermore, TFRS offers a straightforward yet versatile solution that can be seamlessly integrated into various established FS-VAR frameworks. Our design yields significant performance enhancements across various existing works by implementing TFRS, resulting in competitive outcomes on datasets such as UCF101, Kinetics, SSv2, and HMDB51.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002508",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Economics",
      "Engineering",
      "Extractor",
      "Feature vector",
      "Leverage (statistics)",
      "Machine learning",
      "Management",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Power (physics)",
      "Process engineering",
      "Quantum mechanics",
      "Rectification",
      "Representativeness heuristic",
      "Statistics",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Qin",
        "given_name": "Yanfei"
      },
      {
        "surname": "Liu",
        "given_name": "Baolin"
      }
    ]
  },
  {
    "title": "Tackling the curse of dimensionality with physics-informed neural networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106369",
    "abstract": "The curse-of-dimensionality taxes computational resources heavily with exponentially increasing computational cost as the dimension increases. This poses great challenges in solving high-dimensional partial differential equations (PDEs), as Richard E. Bellman first pointed out over 60 years ago. While there has been some recent success in solving numerical PDEs in high dimensions, such computations are prohibitively expensive, and true scaling of general nonlinear PDEs to high dimensions has never been achieved. We develop a new method of scaling up physics-informed neural networks (PINNs) to solve arbitrary high-dimensional PDEs. The new method, called Stochastic Dimension Gradient Descent (SDGD), decomposes a gradient of PDEs’ and PINNs’ residual into pieces corresponding to different dimensions and randomly samples a subset of these dimensional pieces in each iteration of training PINNs. We prove theoretically the convergence and other desired properties of the proposed method. We demonstrate in various diverse tests that the proposed method can solve many notoriously hard high-dimensional PDEs, including the Hamilton–Jacobi-Bellman (HJB) and the Schrödinger equations in tens of thousands of dimensions very fast on a single GPU using the PINNs mesh-free approach. Notably, we solve nonlinear PDEs with nontrivial, anisotropic, and inseparable solutions in less than one hour for 1000 dimensions and in 12 h for 100,000 dimensions on a single GPU using SDGD with PINNs. Since SDGD is a general training methodology of PINNs, it can be applied to any current and future variants of PINNs to scale them up for arbitrary high-dimensional PDEs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002934",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Curse",
      "Curse of dimensionality",
      "Machine learning",
      "Physics",
      "Sociology",
      "Statistical physics"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Zheyuan"
      },
      {
        "surname": "Shukla",
        "given_name": "Khemraj"
      },
      {
        "surname": "Karniadakis",
        "given_name": "George Em"
      },
      {
        "surname": "Kawaguchi",
        "given_name": "Kenji"
      }
    ]
  },
  {
    "title": "DeforT: Deformable transformer for visual tracking",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106380",
    "abstract": "Most trackers formulate visual tracking as common classification and regression (i.e., bounding box regression) tasks. Correlation features that are computed through depth-wise convolution or channel-wise multiplication operations are input into both the classification and regression branches for inference. However, this matching computation with the linear correlation method tends to lose semantic features and obtain only a local optimum. Moreover, these trackers use an unreliable ranking based on the classification score and the intersection over union (IoU) loss for the regression training, thus degrading the tracking performance. In this paper, we introduce a deformable transformer model, which effectively computes the correlation features of the training and search sets. A new loss called the quality-aware focal loss (QAFL) is used to train the classification network; it efficiently alleviates the inconsistency between the classification and localization quality predictions. We use a new regression loss called α -GIoU to train the regression network, and it effectively improves localization accuracy. To further improve the tracker’s robustness, the candidate object location is predicted by using a combination of online learning scores with a transformer-assisted framework and classification scores. An extensive experiment on six testing datasets demonstrates the effectiveness of our method. In particular, the proposed method attains a success score of 71.7% on the OTB-2015 dataset and an AUC score of 67.3% on the NFS30 dataset, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024003046",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "BitTorrent tracker",
      "Chemistry",
      "Computer science",
      "Correlation",
      "Eye tracking",
      "Gene",
      "Geometry",
      "Image (mathematics)",
      "Machine learning",
      "Mathematics",
      "Minimum bounding box",
      "Pattern recognition (psychology)",
      "Regression",
      "Robustness (evolution)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Kai"
      },
      {
        "surname": "Li",
        "given_name": "Qun"
      },
      {
        "surname": "Tian",
        "given_name": "Chunwei"
      },
      {
        "surname": "Zhang",
        "given_name": "Haijun"
      },
      {
        "surname": "Shi",
        "given_name": "Aiwu"
      },
      {
        "surname": "Li",
        "given_name": "Jinkai"
      }
    ]
  },
  {
    "title": "Frequency compensated diffusion model for real-scene dehazing",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106281",
    "abstract": "Due to distribution shift, deep learning based methods for image dehazing suffer from performance degradation when applied to real-world hazy images. In this paper, this study considers a dehazing framework based on conditional diffusion models for improved generalization to real haze. First, our work finds that optimizing the training objective of diffusion models, i.e., Gaussian noise vectors, is non-trivial. The spectral bias of deep networks hinders the higher frequency modes in Gaussian vectors from being learned and hence impairs the reconstruction of image details. To tackle this issue, this study designs a network unit, named Frequency Compensation block (FCB), with a bank of filters that jointly emphasize the mid-to-high frequencies of an input signal. Our work demonstrates that diffusion models with FCB achieve significant gains in both perceptual and distortion metrics. Second, to further boost the generalization performance, this study proposed a novel data synthesis pipeline, HazeAug, to augment haze in terms of degree and diversity. Within the framework, a solid baseline for blind dehazing is set up where models are trained on synthetic hazy-clean pairs, and directly generalize to real data. Extensive evaluations on real dehazing datasets demonstrate the superior performance of the proposed dehazing diffusion model in distortion metrics. Compared to recent methods pre-trained on large-scale, high-quality image datasets, our model achieves a significant PSNR improvement of over 1 dB on challenging databases such as Dense-Haze and Nh-Haze.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002053",
    "keywords": [
      "Algorithm",
      "Amplifier",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Distortion (music)",
      "Generalization",
      "Haze",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Meteorology",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Physics",
      "Pipeline (software)",
      "Programming language"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jing"
      },
      {
        "surname": "Wu",
        "given_name": "Songtao"
      },
      {
        "surname": "Yuan",
        "given_name": "Zhiqiang"
      },
      {
        "surname": "Tong",
        "given_name": "Qiang"
      },
      {
        "surname": "Xu",
        "given_name": "Kuanhong"
      }
    ]
  },
  {
    "title": "Physics-informed neural wavefields with Gabor basis functions",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106286",
    "abstract": "Recently, Physics-Informed Neural Networks (PINNs) have gained significant attention for their versatile interpolation capabilities in solving partial differential equations (PDEs). Despite their potential, the training can be computationally demanding, especially for intricate functions like wavefields. This is primarily due to the neural-based (learned) basis functions, biased toward low frequencies, as they are dominated by polynomial calculations, which are not inherently wavefield-friendly. In response, we propose an approach to enhance the efficiency and accuracy of neural network wavefield solutions by modeling them as linear combinations of Gabor basis functions that satisfy the wave equation. Specifically, for the Helmholtz equation, we augment the fully connected neural network model with an adaptable Gabor layer constituting the final hidden layer, employing a weighted summation of these Gabor neurons to compute the predictions (output). These weights/coefficients of the Gabor functions are learned from the previous hidden layers that include nonlinear activation functions. To ensure the Gabor layer’s utilization across the model space, we incorporate a smaller auxiliary network to forecast the center of each Gabor function based on input coordinates. Realistic assessments showcase the efficacy of this novel implementation compared to the vanilla PINN, particularly in scenarios involving high-frequencies and realistic models that are often challenging for PINNs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002107",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Basis (linear algebra)",
      "Basis function",
      "Computer science",
      "Computer vision",
      "Geometry",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Statistical physics"
    ],
    "authors": [
      {
        "surname": "Alkhalifah",
        "given_name": "Tariq"
      },
      {
        "surname": "Huang",
        "given_name": "Xinquan"
      }
    ]
  },
  {
    "title": "Applying Convolutional Neural Networks to data on unstructured meshes with space-filling curves",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106198",
    "abstract": "This paper presents the first classical Convolutional Neural Network (CNN) that can be applied directly to data from unstructured finite element meshes or control volume grids. CNNs have been hugely influential in the areas of image classification and image compression, both of which typically deal with data on structured grids. Unstructured meshes are frequently used to solve partial differential equations and are particularly suitable for problems that require the mesh to conform to complex geometries or for problems that require variable mesh resolution. Central to our approach are space-filling curves, which traverse the nodes or cells of a mesh tracing out a path that is as short as possible (in terms of numbers of edges) and that visits each node or cell exactly once. The space-filling curves (SFCs) are used to find an ordering of the nodes or cells that can transform multi-dimensional solutions on unstructured meshes into a one-dimensional (1D) representation, to which 1D convolutional layers can then be applied. Although developed in two dimensions, the approach is applicable to higher dimensional problems. To demonstrate the approach, the network we choose is a convolutional autoencoder (CAE), although other types of CNN could be used. The approach is tested by applying CAEs to data sets that have been reordered with a space-filling curve. Sparse layers are used at the input and output of the autoencoder, and the use of multiple SFCs is explored. We compare the accuracy of the SFC-based CAE with that of a classical CAE applied to two idealised problems on structured meshes, and then apply the approach to solutions of flow past a cylinder obtained using the finite-element method and an unstructured mesh.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001229",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Computer graphics (images)",
      "Computer science",
      "Convolutional neural network",
      "Finite element method",
      "Geodesy",
      "Geography",
      "Law",
      "Mesh generation",
      "Pattern recognition (psychology)",
      "Physics",
      "Political science",
      "Politics",
      "Polygon mesh",
      "Representation (politics)",
      "Thermodynamics",
      "Traverse",
      "Volume mesh"
    ],
    "authors": [
      {
        "surname": "Heaney",
        "given_name": "Claire E."
      },
      {
        "surname": "Li",
        "given_name": "Yuling"
      },
      {
        "surname": "Matar",
        "given_name": "Omar K."
      },
      {
        "surname": "Pain",
        "given_name": "Christopher C."
      }
    ]
  },
  {
    "title": "Layerwised multimodal knowledge distillation for vision-language pretrained model",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106272",
    "abstract": "The transformer-based model can simultaneously learn the representation for both images and text, providing excellent performance for multimodal applications. Practically, the large scale of parameters may hinder its deployment in resource-constrained devices, creating a need for model compression. To accomplish this goal, recent studies suggest using knowledge distillation to transfer knowledge from a larger trained teacher model to a small student model without any performance sacrifice. However, this only works with trained parameters of the student model by using the last layer of the teacher, which makes the student model easily overfit in the distillation procedure. Furthermore, the mutual interference between modalities causes more difficulties for distillation. To address these issues, the study proposed a layerwised multimodal knowledge distillation for a vision-language pretrained model. In addition to the last layer, the intermediate layers of the teacher were also used for knowledge transfer. To avoid interference between modalities, we split the multimodality into separate modalities and added them as extra inputs. Then, two auxiliary losses were implemented to encourage each modality to distill more effectively. Comparative experiments on four different multimodal tasks show that the proposed layerwised multimodality distillation achieves better performance than other KD methods for vision-language pretrained models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001965",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Computer science",
      "Distillation",
      "Language model",
      "Machine learning",
      "Modalities",
      "Modality (human–computer interaction)",
      "Organic chemistry",
      "Overfitting",
      "Physics",
      "Quantum mechanics",
      "Social science",
      "Sociology",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jin"
      },
      {
        "surname": "Liao",
        "given_name": "Dawei"
      },
      {
        "surname": "Zhang",
        "given_name": "You"
      },
      {
        "surname": "Xu",
        "given_name": "Dan"
      },
      {
        "surname": "Zhang",
        "given_name": "Xuejie"
      }
    ]
  },
  {
    "title": "Deep causal learning for pancreatic cancer segmentation in CT sequences",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106294",
    "abstract": "Segmenting the irregular pancreas and inconspicuous tumor simultaneously is an essential but challenging step in diagnosing pancreatic cancer. Current deep-learning (DL) methods usually segment the pancreas or tumor independently using mixed image features, which are disrupted by surrounding complex and low-contrast background tissues. Here, we proposed a deep causal learning framework named CausegNet for pancreas and tumor co-segmentation in 3D CT sequences. Specifically, a causality-aware module and a counterfactual loss are employed to enhance the DL network's comprehension of the anatomical causal relationship between the foreground elements (pancreas and tumor) and the background. By integrating causality into CausegNet, the network focuses solely on extracting intrinsic foreground causal features while effectively learning the potential causality between the pancreas and the tumor. Then based on the extracted causal features, CausegNet applies a counterfactual inference to significantly reduce the background interference and sequentially search for pancreas and tumor from the foreground. Consequently, our approach can handle deformable pancreas and obscure tumors, resulting in superior co-segmentation performance in both public and real clinical datasets, achieving the highest pancreas/tumor Dice coefficients of 86.67%/84.28%. The visualized features and anti-noise experiments further demonstrate the causal interpretability and stability of our method. Furthermore, our approach improves the accuracy and sensitivity of downstream pancreatic cancer risk assessment task by 12.50% and 50.00%, respectively, compared to experienced clinicians, indicating promising clinical applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002181",
    "keywords": [
      "Artificial intelligence",
      "Cancer",
      "Causal inference",
      "Causality (physics)",
      "Computer science",
      "Counterfactual thinking",
      "Deep learning",
      "Inference",
      "Internal medicine",
      "Interpretability",
      "Machine learning",
      "Medicine",
      "Pancreas",
      "Pancreatic cancer",
      "Pathology",
      "Pattern recognition (psychology)",
      "Physics",
      "Psychology",
      "Quantum mechanics",
      "Segmentation",
      "Social psychology"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Chengkang"
      },
      {
        "surname": "Mao",
        "given_name": "Yishen"
      },
      {
        "surname": "Liang",
        "given_name": "Shuyu"
      },
      {
        "surname": "Li",
        "given_name": "Ji"
      },
      {
        "surname": "Wang",
        "given_name": "Yuanyuan"
      },
      {
        "surname": "Guo",
        "given_name": "Yi"
      }
    ]
  },
  {
    "title": "Bayesian hypernetwork collaborates with time-difference evolutional network for temporal knowledge prediction",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106146",
    "abstract": "A Temporal Knowledge Graph (TKG) is a sequence of Knowledge Graphs (KGs) attached with time information, in which each KG contains the facts that co-occur at the same timestamp. Temporal knowledge prediction (TKP) aims to predict future events given observed historical KGs in TKGs, which is essential for many applications to provide intelligent analysis services. However, most existing TKP methods focus on entity and relation prediction tasks but ignore the importance of time prediction tasks. Furthermore, there is uncertainty in time prediction, and it is difficult for prediction models to model it completely. In this work, we propose a collaboration framework with Bayesian Hypernetwork and Time-Difference Evolutional Network (BH-TDEN) to address these problems. First, we begin with the time prediction task, and we present a Bayesian hypernetwork to model the uncertainty of events time. For the input of Bayesian hypernetwork, we design a novel time-difference evolutional network to obtain the entities and relations embedding. Specifically, we propose an auto-regressive time gate parameterized by the time difference of adjacent KGs in entity and relation encoder to learn the time-sensitive TKG embedding, which not only learns the relationship between the given time information and TKG embedding but also provides more expressive TKG embedding for Bayesian hypernetwork to accurately predict the time of future events. Furthermore, we also present a novel relation updating mechanism that employs the neighbor relations of the subject corresponding to the current relation to learn more adaptive relation embedding. Extensive experiments demonstrate that the proposed method obtains considerable time prediction and link prediction performance on four TKG benchmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000704",
    "keywords": [
      "Artificial intelligence",
      "Bayesian network",
      "Bayesian probability",
      "Biology",
      "Computer science",
      "Computer security",
      "Data mining",
      "Embedding",
      "Genetics",
      "Machine learning",
      "Relation (database)",
      "Sequence (biology)",
      "Theoretical computer science",
      "Timestamp"
    ],
    "authors": [
      {
        "surname": "Shao",
        "given_name": "Pengpeng"
      },
      {
        "surname": "Wen",
        "given_name": "Yang"
      },
      {
        "surname": "Tao",
        "given_name": "Jianhua"
      }
    ]
  },
  {
    "title": "A Dual Robust Graph Neural Network Against Graph Adversarial Attacks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106276",
    "abstract": "Graph Neural Networks (GNNs) have gained widespread usage and achieved remarkable success in various real-world applications. Nevertheless, recent studies reveal the vulnerability of GNNs to graph adversarial attacks that fool them by modifying graph structure. This vulnerability undermines the robustness of GNNs and poses significant security and privacy risks across various applications. Hence, it is crucial to develop robust GNN models that can effectively defend against such attacks. One simple approach is to remodel the graph. However, most existing methods cannot fully preserve the similarity relationship among the original nodes while learning the node representation required for reweighting the edges. Furthermore, they lack supervision information regarding adversarial perturbations, hampering their ability to recognize adversarial edges. To address these limitations, we propose a novel Dual Robust Graph Neural Network (DualRGNN) against graph adversarial attacks. DualRGNN first incorporates a node-similarity-preserving graph refining (SPGR) module to prune and refine the graph based on the learned node representations, which contain the original nodes’ similarity relationships, weakening the poisoning of graph adversarial attacks on graph data. DualRGNN then employs an adversarial-supervised graph attention (ASGAT) network to enhance the model’s capability in identifying adversarial edges by treating these edges as supervised signals. Through extensive experiments conducted on four benchmark datasets, DualRGNN has demonstrated remarkable robustness against various graph adversarial attacks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002004",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Gene",
      "Graph",
      "Machine learning",
      "Robustness (evolution)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Tao",
        "given_name": "Qian"
      },
      {
        "surname": "Liao",
        "given_name": "Jianpeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Enze"
      },
      {
        "surname": "Li",
        "given_name": "Lusi"
      }
    ]
  },
  {
    "title": "Predefined-time distributed optimization and anti-disturbance control for nonlinear multi-agent system with neural network estimator: A hierarchical framework",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106270",
    "abstract": "This paper addresses the predefined-time distributed optimization of nonlinear multi-agent system using a hierarchical control approach. Considering unknown nonlinear functions and external disturbances, we propose a two-layer hierarchical control framework. At the first layer, a predefined-time distributed estimator is employed to produce optimal consensus trajectories. At the second layer, a neural-network-based predefined-time disturbance observer is introduced to estimate the disturbance, with neural networks used to approximate the unknown nonlinear functions. A neural-network-based anti-disturbance sliding mode control mechanism is presented to ensure that the system trajectories can track the optimal trajectories within a predefined time. The feasibility of this hierarchical control framework is verified by utilizing the Lyapunov method. Numerical simulations are conducted separately using models of robotic arms and mobile robots to validate the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001941",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Estimator",
      "Mathematics",
      "Nonlinear system",
      "Physics",
      "Quantum mechanics",
      "Sliding mode control",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Haitao"
      },
      {
        "surname": "Liu",
        "given_name": "Qingshan"
      },
      {
        "surname": "Xu",
        "given_name": "Chentao"
      }
    ]
  },
  {
    "title": "Weakly supervised temporal action localization with actionness-guided false positive suppression",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106307",
    "abstract": "Weakly supervised temporal action localization aims to locate the temporal boundaries of action instances in untrimmed videos using video-level labels and assign them the corresponding action category. Generally, it is solved by a pipeline called “localization-by-classification”, which finds the action instances by classifying video snippets. However, since this approach optimizes the video-level classification objective, the generated activation sequences often suffer interference from class-related scenes, resulting in a large number of false positives in the prediction results. Many existing works treat background as an independent category, forcing models to learn to distinguish background snippets. However, under weakly supervised conditions, the background information is fuzzy and uncertain, making this method extremely difficult. To alleviate the impact of false positives, we propose a new actionness-guided false positive suppression framework. Our method seeks to suppress false positive backgrounds without introducing the background category. Firstly, we propose a self-training actionness branch to learn class-agnostic actionness, which can minimize the interference of class-related scene information by ignoring the video labels. Secondly, we propose a false positive suppression module to mine false positive snippets and suppress them. Finally, we introduce the foreground enhancement module, which guides the model to learn the foreground with the help of the attention mechanism as well as class-agnostic actionness. We conduct extensive experiments on three benchmarks (THUMOS14, ActivityNet1.2, and ActivityNet1.3). The results demonstrate the effectiveness of our method in suppressing false positives and it achieves the state-of-the-art performance. Code: https://github.com/lizhilin-ustc/AFPS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002314",
    "keywords": [
      "Action (physics)",
      "Artificial intelligence",
      "Class (philosophy)",
      "Code (set theory)",
      "Computer science",
      "False positive paradox",
      "False positives and false negatives",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Pipeline (software)",
      "Programming language",
      "Quantum mechanics",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Zhilin"
      },
      {
        "surname": "Wang",
        "given_name": "Zilei"
      },
      {
        "surname": "Liu",
        "given_name": "Qinying"
      }
    ]
  },
  {
    "title": "MTKSVCR: A novel multi-task multi-class support vector machine with safe acceleration rule",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106317",
    "abstract": "Regularized multi-task learning (RMTL) has shown good performance in tackling multi-task binary problems. Although RMTL can be used to handle multi-class problems based on “one-versus-one” and “one-versus-rest” techniques, the information of the samples is not fully utilized and the class imbalance problem occurs. Motivated by the regularization technique in RMTL, we propose an original multi-task multi-class model termed MTKSVCR based on “one-versus-one-versus-rest” strategy to achieve better testing accuracy. Due to the utilization of the idea of RMTL, the related information included in multiple tasks is mined by setting different penalty parameters before task-common and task-specific regularization terms. However, the proposed MTKSVCR is time-consuming since it employs all samples in each optimization problem. Therefore, a multi-parameter safe acceleration rule termed SA is further presented to reduce the time consumption. It identifies and deletes most of the superfluous samples corresponding to 0 elements in the dual optimal solution before solving. Then, only a reduced dual problem is to be solved and the computational efficiency is improved accordingly. The biggest advantage of the proposed SA lies in safety. Namely, it derives an identical optimal solution to the primal problem without SA. In addition, our method remains effective when multiple parameters change simultaneously. Experiments on different artificial datasets and benchmark datasets verify the validity of the proposed methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002417",
    "keywords": [
      "Acceleration",
      "Arithmetic",
      "Art",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Binary classification",
      "Binary number",
      "Class (philosophy)",
      "Classical mechanics",
      "Computer science",
      "Dual (grammatical number)",
      "Economics",
      "Geodesy",
      "Geography",
      "Literature",
      "Machine learning",
      "Management",
      "Mathematical optimization",
      "Mathematics",
      "Physics",
      "Regularization (linguistics)",
      "Support vector machine",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Pang",
        "given_name": "Xinying"
      },
      {
        "surname": "Xu",
        "given_name": "Chang"
      },
      {
        "surname": "Xu",
        "given_name": "Yitian"
      }
    ]
  },
  {
    "title": "Face anti-spoofing with cross-stage relation enhancement and spoof material perception",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106275",
    "abstract": "Face Anti-Spoofing (FAS) seeks to protect face recognition systems from spoofing attacks, which is applied extensively in scenarios such as access control, electronic payment, and security surveillance systems. Face anti-spoofing requires the integration of local details and global semantic information. Existing CNN-based methods rely on small stride or image patch-based feature extraction structures, which struggle to capture spatial and cross-layer feature correlations effectively. Meanwhile, Transformer-based methods have limitations in extracting discriminative detailed features. To address the aforementioned issues, we introduce a multi-stage CNN-Transformer-based framework, which extracts local features through the convolutional layer and long-distance feature relationships via self-attention. Based on this, we proposed a cross-attention multi-stage feature fusion, employing semantically high-stage features to query task-relevant features in low-stage features for further cross-stage feature fusion. To enhance the discrimination of local features for subtle differences, we design pixel-wise material classification supervision and add a auxiliary branch in the intermediate layers of the model. Moreover, to address the limitations of a single acquisition environment and scarcity of acquisition devices in the existing Near-Infrared dataset, we create a large-scale Near-Infrared Face Anti-Spoofing dataset with 380k pictures of 1040 identities. The proposed method could achieve the state-of-the-art in OULU-NPU and our proposed Near-Infrared dataset at just 1.3GFlops and 3.2M parameter numbers, which demonstrate the effective of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001990",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Face (sociological concept)",
      "Geology",
      "Neuroscience",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Perception",
      "Psychology",
      "Relation (database)",
      "Social science",
      "Sociology",
      "Stage (stratigraphy)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Daiyuan"
      },
      {
        "surname": "Chen",
        "given_name": "Guo"
      },
      {
        "surname": "Wu",
        "given_name": "Xixian"
      },
      {
        "surname": "Yu",
        "given_name": "Zitong"
      },
      {
        "surname": "Tan",
        "given_name": "Mingkui"
      }
    ]
  },
  {
    "title": "Connectional-style-guided contextual representation learning for brain disease diagnosis",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106296",
    "abstract": "Structural magnetic resonance imaging (sMRI) has shown great clinical value and has been widely used in deep learning (DL) based computer-aided brain disease diagnosis. Previous DL-based approaches focused on local shapes and textures in brain sMRI that may be significant only within a particular domain. The learned representations are likely to contain spurious information and have poor generalization ability in other diseases and datasets. To facilitate capturing meaningful and robust features, it is necessary to first comprehensively understand the intrinsic pattern of the brain that is not restricted within a single data/task domain. Considering that the brain is a complex connectome of interlinked neurons, the connectional properties in the brain have strong biological significance, which is shared across multiple domains and covers most pathological information. In this work, we propose a connectional style contextual representation learning model (CS-CRL) to capture the intrinsic pattern of the brain, used for multiple brain disease diagnosis. Specifically, it has a vision transformer (ViT) encoder and leverages mask reconstruction as the proxy task and Gram matrices to guide the representation of connectional information. It facilitates the capture of global context and the aggregation of features with biological plausibility. The results indicate that CS-CRL achieves superior accuracy in multiple brain disease diagnosis tasks across six datasets and three diseases and outperforms state-of-the-art models. Furthermore, we demonstrate that CS-CRL captures more brain-network-like properties, and better aggregates features, is easier to optimize, and is more robust to noise, which explains its superiority in theory.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400220X",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Feature learning",
      "Functional connectivity",
      "Functional magnetic resonance imaging",
      "Human Connectome Project",
      "Law",
      "Machine learning",
      "Neuroscience",
      "Paleontology",
      "Political science",
      "Politics",
      "Psychology",
      "Representation (politics)",
      "Spurious relationship"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Gongshu"
      },
      {
        "surname": "Jiang",
        "given_name": "Ning"
      },
      {
        "surname": "Ma",
        "given_name": "Yunxiao"
      },
      {
        "surname": "Chen",
        "given_name": "Duanduan"
      },
      {
        "surname": "Wu",
        "given_name": "Jinglong"
      },
      {
        "surname": "Li",
        "given_name": "Guoqi"
      },
      {
        "surname": "Liang",
        "given_name": "Dong"
      },
      {
        "surname": "Yan",
        "given_name": "Tianyi"
      }
    ]
  },
  {
    "title": "A memristive all-inclusive hypernetwork for parallel analog deployment of full search space architectures",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106312",
    "abstract": "In recent years, there has been a significant advancement in memristor-based neural networks, positioning them as a pivotal processing-in-memory deployment architecture for a wide array of deep learning applications. Within this realm of progress, the emerging parallel analog memristive platforms are prominent for their ability to generate multiple feature maps in a single processing cycle. However, a notable limitation is that they are specifically tailored for neural networks with fixed structures. As an orthogonal direction, recent research reveals that neural architecture should be specialized for tasks and deployment platforms. Building upon this, the neural architecture search (NAS) methods effectively explore promising architectures in a large design space. However, these NAS-based architectures are generally heterogeneous and diversified, making it challenging for deployment on current single-prototype, customized, parallel analog memristive hardware circuits. Therefore, investigating memristive analog deployment that overrides the full search space is a promising and challenging problem. Inspired by this, and beginning with the DARTS search space, we study the memristive hardware design of primitive operations and propose the memristive all-inclusive hypernetwork that covers 2 × 1 0 25 network architectures. Our computational simulation results on 3 representative architectures (DARTS-V1, DARTS-V2, PDARTS) show that our memristive all-inclusive hypernetwork achieves promising results on the CIFAR10 dataset (89.2 % of PDARTS with 8-bit quantization precision), and is compatible with all architectures in the DARTS full-space. The hardware performance simulation indicates that the memristive all-inclusive hypernetwork costs slightly more resource consumption (nearly the same in power, 22 % ∼ 25 % increase in Latency, 1 . 5 × in Area) relative to the individual deployment, which is reasonable and may reach a tolerable trade-off deployment scheme for industrial scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002363",
    "keywords": [
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer architecture",
      "Computer engineering",
      "Computer science",
      "Design space exploration",
      "Distributed computing",
      "Electronic engineering",
      "Embedded system",
      "Engineering",
      "Memristor",
      "Operating system",
      "Software deployment",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Lyu",
        "given_name": "Bo"
      },
      {
        "surname": "Yang",
        "given_name": "Yin"
      },
      {
        "surname": "Cao",
        "given_name": "Yuting"
      },
      {
        "surname": "Shi",
        "given_name": "Tuo"
      },
      {
        "surname": "Chen",
        "given_name": "Yiran"
      },
      {
        "surname": "Huang",
        "given_name": "Tingwen"
      },
      {
        "surname": "Wen",
        "given_name": "Shiping"
      }
    ]
  },
  {
    "title": "Migrate demographic group for fair Graph Neural Networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106264",
    "abstract": "Graph Neural networks (GNNs) have been applied in many scenarios due to the superior performance of graph learning. However, fairness is always ignored when designing GNNs. As a consequence, biased information in training data can easily affect vanilla GNNs, causing biased results toward particular demographic groups (divided by sensitive attributes, such as race and age). There have been efforts to address the fairness issue. However, existing fair techniques generally divide the demographic groups by raw sensitive attributes and assume that are fixed. The biased information correlated with raw sensitive attributes will run through the training process regardless of the implemented fair techniques. It is urgent to resolve this problem for training fair GNNs. To tackle this problem, we propose a brand new framework, FairMigration, which is able to migrate the demographic groups dynamically, instead of keeping that fixed with raw sensitive attributes. FairMigration is composed of two training stages. In the first stage, the GNNs are initially optimized by personalized self-supervised learning, and the demographic groups are adjusted dynamically. In the second stage, the new demographic groups are frozen and supervised learning is carried out under the constraints of new demographic groups and adversarial training. Extensive experiments reveal that FairMigration achieves a high trade-off between model performance and fairness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001886",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Graph",
      "Machine learning",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "YanMing"
      },
      {
        "surname": "Liao",
        "given_name": "TianChi"
      },
      {
        "surname": "Chen",
        "given_name": "JiaLong"
      },
      {
        "surname": "Bian",
        "given_name": "Jing"
      },
      {
        "surname": "Zheng",
        "given_name": "ZiBin"
      },
      {
        "surname": "Chen",
        "given_name": "Chuan"
      }
    ]
  },
  {
    "title": "A novel interactive deep cascade spectral graph convolutional network with multi-relational graphs for disease prediction",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106285",
    "abstract": "Graph neural networks (GNNs) have recently grown in popularity for disease prediction. Existing GNN-based methods primarily build the graph topological structure around a single modality and combine it with other modalities to acquire feature representations of acquisitions. The complicated relationship in each modality, however, may not be well highlighted due to its specificity. Further, relatively shallow networks restrict adequate extraction of high-level features, affecting disease prediction performance. Accordingly, this paper develops a new interactive deep cascade spectral graph convolutional network with multi-relational graphs (IDCGN) for disease prediction tasks. Its crucial points lie in constructing multiple relational graphs and dual cascade spectral graph convolution branches with interaction (DCSGBI). Specifically, the former designs a pairwise imaging-based edge generator and a pairwise non-imaging-based edge generator from different modalities by devising two learnable networks, which adaptively capture graph structures and provide various views of the same acquisition to aid in disease diagnosis. Again, DCSGBI is established to enrich high-level semantic information and low-level details of disease data. It devises a cascade spectral graph convolution operator for each branch and incorporates the interaction strategy between different branches into the network, successfully forming a deep model and capturing complementary information from diverse branches. In this manner, more favorable and sufficient features are learned for a reliable diagnosis. Experiments on several disease datasets reveal that IDCGN exceeds state-of-the-art models and achieves promising results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002090",
    "keywords": [
      "Artificial intelligence",
      "Cascade",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Deep learning",
      "Graph",
      "Graph traversal",
      "Pairwise comparison",
      "Pattern recognition (psychology)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Sihui"
      },
      {
        "surname": "Zhang",
        "given_name": "Rui"
      }
    ]
  },
  {
    "title": "Unsupervised Sentence Representation Learning with Frequency-induced Adversarial tuning and Incomplete sentence filtering",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106315",
    "abstract": "Pre-trained Language Model (PLM) is nowadays the mainstay of Unsupervised Sentence Representation Learning (USRL). However, PLMs are sensitive to the frequency information of words from their pre-training corpora, resulting in anisotropic embedding space, where the embeddings of high-frequency words are clustered but those of low-frequency words disperse sparsely. This anisotropic phenomenon results in two problems of similarity bias and information bias, lowering the quality of sentence embeddings. To solve the problems, we fine-tune PLMs by leveraging the frequency information of words and propose a novel USRL framework, namely Sentence Representation Learning with Frequency-induced Adversarial tuning and Incomplete sentence filtering (Slt-fai). We calculate the word frequencies over the pre-training corpora of PLMs and assign words thresholding frequency labels. With them, (1) we incorporate a similarity discriminator used to distinguish the embeddings of high-frequency and low-frequency words, and adversarially tune the PLM with it, enabling to achieve uniformly frequency-invariant embedding space; and (2) we propose a novel incomplete sentence detection task, where we incorporate an information discriminator to distinguish the embeddings of original sentences and incomplete sentences by randomly masking several low-frequency words, enabling to emphasize the more informative low-frequency words. Our Slt-fai is a flexible and plug-and-play framework, and it can be integrated with existing USRL techniques. We evaluate Slt-fai with various backbones on benchmark datasets. Empirical results indicate that Slt-fai can be superior to the existing USRL baselines.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002399",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Detector",
      "Discriminator",
      "Embedding",
      "Geometry",
      "Law",
      "Mathematics",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Sentence",
      "Speech recognition",
      "Telecommunications",
      "Word (group theory)",
      "Word lists by frequency"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Bing"
      },
      {
        "surname": "Li",
        "given_name": "Ximing"
      },
      {
        "surname": "Yang",
        "given_name": "Zhiyao"
      },
      {
        "surname": "Guan",
        "given_name": "Yuanyuan"
      },
      {
        "surname": "Li",
        "given_name": "Jiayin"
      },
      {
        "surname": "Wang",
        "given_name": "Shengsheng"
      }
    ]
  },
  {
    "title": "Score mismatching for generative modeling",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106311",
    "abstract": "We propose a new score-based model with one-step sampling. Previously, score-based models were burdened with heavy computations due to iterative sampling. For substituting the iterative process, we train a standalone generator to compress all the time steps with the gradient backpropagated from the score network. In order to produce meaningful gradients for the generator, the score network is trained to simultaneously match the real data distribution and mismatch the fake data distribution. This model has the following advantages: (1) For sampling, it generates a fake image with only one step forward. (2) For training, it only needs 10 diffusion steps. (3) Compared with consistency model, it is free of the ill-posed problem caused by consistency loss. On the popular CIFAR-10 dataset, our model outperforms Consistency Model and Denoising Score Matching, which demonstrates the potential of the framework. We further provide more examples on the MINIST and LSUN datasets. The code is available on GitHub.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002351",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Code (set theory)",
      "Computation",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Data mining",
      "Filter (signal processing)",
      "Generative grammar",
      "Generative model",
      "Generator (circuit theory)",
      "Machine learning",
      "Matching (statistics)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Power (physics)",
      "Programming language",
      "Quantum mechanics",
      "Sampling (signal processing)",
      "Set (abstract data type)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Ye",
        "given_name": "Senmao"
      },
      {
        "surname": "Liu",
        "given_name": "Fei"
      }
    ]
  },
  {
    "title": "Mutual Correlation Network for few-shot learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106289",
    "abstract": "Most metric-based Few-Shot Learning (FSL) methods focus on learning good embeddings of images. However, these methods either lack the ability to explore the cross-correlation (i.e., correlated information) between image pairs or explore limited consensus among the correlation map constrained by the limited receptive field of CNN. We propose a Mutual Correlation Network (MCNet) to explore global consensus among the correlation map by using the self-attention mechanism which has a global receptive field. Our MCNet contains two core modules: (1) a multi-level embedding module that generates multi-level embeddings for an image pair which capture hierarchical semantics, and (2) a mutual correlation module that refines correlation map of two embeddings and generates more robust relational embeddings. Extensive experiments show that our MCNet achieves competitive results on four widely-used few-shot classification benchmarks miniImageNet, tieredImageNet, CUB-200-2011, and CIFAR-FS. Code is available at https://github.com/DRGreat/MCNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002132",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Correlation",
      "Economics",
      "Embedding",
      "Focus (optics)",
      "Geometry",
      "Image (mathematics)",
      "Mathematics",
      "Metric (unit)",
      "Mutual information",
      "Operations management",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Semantics (computer science)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Derong"
      },
      {
        "surname": "Chen",
        "given_name": "Feiyu"
      },
      {
        "surname": "Ouyang",
        "given_name": "Deqiang"
      },
      {
        "surname": "Shao",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "Expressive power of ReLU and step networks under floating-point operations",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106297",
    "abstract": "The study of the expressive power of neural networks has investigated the fundamental limits of neural networks. Most existing results assume real-valued inputs and parameters as well as exact operations during the evaluation of neural networks. However, neural networks are typically executed on computers that can only represent a tiny subset of the reals and apply inexact operations, i.e., most existing results do not apply to neural networks used in practice. In this work, we analyze the expressive power of neural networks under a more realistic setup: when we use floating-point numbers and operations as in practice. Our first set of results assumes floating-point operations where the significand of a float is represented by finite bits but its exponent can take any integer value. Under this setup, we show that neural networks using a binary threshold unit or ReLU can memorize any finite input/output pairs and can approximate any continuous function within an arbitrary error. In particular, the number of parameters in our constructions for universal approximation and memorization coincides with that in classical results assuming exact mathematical operations. We also show similar results on memorization and universal approximation when floating-point operations use finite bits for both significand and exponent; these results are applicable to many popular floating-point formats such as those defined in the IEEE 754 standard (e.g., 32-bit single-precision format) and bfloat16.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002211",
    "keywords": [
      "Algorithm",
      "Arithmetic",
      "Artificial intelligence",
      "Artificial neural network",
      "Binary number",
      "Computer science",
      "Discrete mathematics",
      "Exponent",
      "Floating point",
      "Geometry",
      "Integer (computer science)",
      "Linguistics",
      "Mathematics",
      "Mathematics education",
      "Memorization",
      "Philosophy",
      "Point (geometry)",
      "Power of two",
      "Programming language",
      "Set (abstract data type)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Park",
        "given_name": "Yeachan"
      },
      {
        "surname": "Hwang",
        "given_name": "Geonho"
      },
      {
        "surname": "Lee",
        "given_name": "Wonyeol"
      },
      {
        "surname": "Park",
        "given_name": "Sejun"
      }
    ]
  },
  {
    "title": "vEpiNet: A multimodal interictal epileptiform discharge detection method based on video and electroencephalogram data",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106319",
    "abstract": "To enhance deep learning-based automated interictal epileptiform discharge (IED) detection, this study proposes a multimodal method, vEpiNet, that leverages video and electroencephalogram (EEG) data. Datasets comprise 24 931 IED (from 484 patients) and 166 094 non-IED 4-second video-EEG segments. The video data is processed by the proposed patient detection method, with frame difference and Simple Keypoints (SKPS) capturing patients’ movements. EEG data is processed with EfficientNetV2. The video and EEG features are fused via a multilayer perceptron. We developed a comparative model, termed nEpiNet, to test the effectiveness of the video feature in vEpiNet. The 10-fold cross-validation was used for testing. The 10-fold cross-validation showed high areas under the receiver operating characteristic curve (AUROC) in both models, with a slightly superior AUROC (0.9902) in vEpiNet compared to nEpiNet (0.9878). Moreover, to test the model performance in real-world scenarios, we set a prospective test dataset, containing 215 h of raw video-EEG data from 50 patients. The result shows that the vEpiNet achieves an area under the precision–recall curve (AUPRC) of 0.8623, surpassing nEpiNet’s 0.8316. Incorporating video data raises precision from 70% (95% CI, 69.8%–70.2%) to 76.6% (95% CI, 74.9%–78.2%) at 80% sensitivity and reduces false positives by nearly a third, with vEpiNet processing one-hour video-EEG data in 5.7 min on average. Our findings indicate that video data can significantly improve the performance and precision of IED detection, especially in prospective real clinic testing. It suggests that vEpiNet is a clinically viable and effective tool for IED analysis in real-world applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002430",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Electroencephalography",
      "Electronic engineering",
      "Engineering",
      "False positive paradox",
      "Ictal",
      "Machine learning",
      "Medicine",
      "Pattern recognition (psychology)",
      "Programming language",
      "Psychiatry",
      "Receiver operating characteristic",
      "Sensitivity (control systems)",
      "Test data"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Nan"
      },
      {
        "surname": "Gao",
        "given_name": "Weifang"
      },
      {
        "surname": "Li",
        "given_name": "Lian"
      },
      {
        "surname": "Chen",
        "given_name": "Junhui"
      },
      {
        "surname": "Liang",
        "given_name": "Zi"
      },
      {
        "surname": "Yuan",
        "given_name": "Gonglin"
      },
      {
        "surname": "Sun",
        "given_name": "Heyang"
      },
      {
        "surname": "Liu",
        "given_name": "Qing"
      },
      {
        "surname": "Chen",
        "given_name": "Jianhua"
      },
      {
        "surname": "Jin",
        "given_name": "Liri"
      },
      {
        "surname": "Huang",
        "given_name": "Yan"
      },
      {
        "surname": "Zhou",
        "given_name": "Xiangqin"
      },
      {
        "surname": "Zhang",
        "given_name": "Shaobo"
      },
      {
        "surname": "Hu",
        "given_name": "Peng"
      },
      {
        "surname": "Dai",
        "given_name": "Chaoyue"
      },
      {
        "surname": "He",
        "given_name": "Haibo"
      },
      {
        "surname": "Dong",
        "given_name": "Yisu"
      },
      {
        "surname": "Cui",
        "given_name": "Liying"
      },
      {
        "surname": "Lu",
        "given_name": "Qiang"
      }
    ]
  },
  {
    "title": "Uncertainty-aware prototypical learning for anomaly detection in medical images",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106284",
    "abstract": "Anomalous object detection (AOD) in medical images aims to recognize the anomalous lesions, and is crucial for early clinical diagnosis of various cancers. However, it is a difficult task because of two reasons: (1) the diversity of the anomalous lesions and (2) the ambiguity of the boundary between anomalous lesions and their normal surroundings. Unlike existing single-modality AOD models based on deterministic mapping, we constructed a probabilistic and deterministic AOD model. Specifically, we designed an uncertainty-aware prototype learning framework, which considers the diversity and ambiguity of anomalous lesions. A prototypical learning transformer (Pformer) is established to extract and store the prototype features of different anomalous lesions. Moreover, Bayesian neural uncertainty quantizer, a probabilistic model, is designed to model the distributions over the outputs of the model to measure the uncertainty of the model’s detection results for each pixel. Essentially, the uncertainty of the model’s anomaly detection result for a pixel can reflect the anomalous ambiguity of this pixel. Furthermore, an uncertainty-guided reasoning transformer (Uformer) is devised to employ the anomalous ambiguity, encouraging the proposed model to focus on pixels with high uncertainty. Notably, prototypical representations stored in Pformer are also utilized in anomaly reasoning that enables the model to perceive diversities of the anomalous objects. Extensive experiments on five benchmark datasets demonstrate the superiority of our proposed method. The source code will be available in github.com/umchaohuang/UPformer.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002089",
    "keywords": [
      "Ambiguity",
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Condensed matter physics",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Pixel",
      "Probabilistic logic",
      "Programming language",
      "Quantum mechanics",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Chao"
      },
      {
        "surname": "Shi",
        "given_name": "Yushu"
      },
      {
        "surname": "Zhang",
        "given_name": "Bob"
      },
      {
        "surname": "Lyu",
        "given_name": "Ke"
      }
    ]
  },
  {
    "title": "Observer-based resilient dissipativity control for discrete-time memristor-based neural networks with unbounded or bounded time-varying delays",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106279",
    "abstract": "This work focuses on the issue of observer-based resilient dissipativity control of discrete-time memristor-based neural networks (DTMBNNs) with unbounded or bounded time-varying delays. Firstly, the Luenberger observer is designed, and additionally based on the observed states, the observer-based resilient controller is proposed. An augmented system is presented by considering both the error system and the DTMBNNs with the controller. Secondly, a novel sufficient extended exponential dissipativity condition is obtained for the augmented system with unbounded time-varying delays by proposing a system solutions-based estimation approach. This method is based on system solutions and without constructing any Lyapunov–Krasovskii functionals (LKF), thereby reducing the complexity of theoretical derivation and computational workload. In addition, an algorithm is proposed to solve the nonlinear inequalities in the sufficient condition. Thirdly, the sufficient extended exponential dissipativity condition for the augmented system with bounded time-varying delays is also obtained. Finally, the effectiveness of the theoretical results is illustrated through two simulation examples.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400203X",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Bounded function",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Controller (irrigation)",
      "Discrete time and continuous time",
      "Electrical engineering",
      "Engineering",
      "Exponential stability",
      "Mathematical analysis",
      "Mathematics",
      "Memristor",
      "Nonlinear system",
      "Observer (physics)",
      "Physics",
      "Quantum mechanics",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Tu",
        "given_name": "Kairong"
      },
      {
        "surname": "Xue",
        "given_name": "Yu"
      },
      {
        "surname": "Zhang",
        "given_name": "Xian"
      }
    ]
  },
  {
    "title": "Non-local degradation modeling for spatially adaptive single image super-resolution",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106293",
    "abstract": "Existing methods for single image super-resolution (SISR) model the blur kernel as spatially invariant across the entire image, and are susceptible to the adverse effects of textureless patches. To achieve improved results, adaptive estimation of the degradation kernel is necessary. We explore the synergy of joint global and local degradation modeling for spatially adaptive blind SISR. Our model, named spatially adaptive network for blind super-resolution (SASR), employs a simple encoder to estimate global degradation representations and a decoder to extract local degradation. These two representations are fused with a cross-attention mechanism and applied using spatially adaptive filtering to enhance the local image detail. Specifically, SASR contains two novel features: (1) a non-local degradation modeling with contrastive learning to learn global and local degradation representations, and (2) a non-local spatially adaptive filtering module (SAFM) that incorporates the global degradation and spatial-detail factors to preserve and enhance local details. We demonstrate that SASR can efficiently estimate degradation representations and handle multiple types of degradation. The local representations avoid the detrimental effect of estimating the entire super-resolved image with only one kernel through locally adaptive adjustments. Extensive experiments are performed to quantitatively and qualitatively demonstrate that SASR not only performs favorably for degradation estimation but also leads to state-of-the-art blind SISR performance when compared to alternative approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400217X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Degradation (telecommunications)",
      "Encoder",
      "Estimator",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Kernel (algebra)",
      "Kernel density estimation",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Qianyu"
      },
      {
        "surname": "Zheng",
        "given_name": "Bolun"
      },
      {
        "surname": "Li",
        "given_name": "Zongpeng"
      },
      {
        "surname": "Liu",
        "given_name": "Yu"
      },
      {
        "surname": "Zhu",
        "given_name": "Zunjie"
      },
      {
        "surname": "Slabaugh",
        "given_name": "Gregory"
      },
      {
        "surname": "Yuan",
        "given_name": "Shanxin"
      }
    ]
  },
  {
    "title": "Robust sound-guided image manipulation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106271",
    "abstract": "Recent successes suggest that an image can be manipulated by a text prompt, e.g., a landscape scene on a sunny day is manipulated into the same scene on a rainy day driven by a text input “raining”. These approaches often utilize a StyleCLIP-based image generator, which leverages multi-modal (text and image) embedding space. However, we observe that such text inputs are often bottlenecked in providing and synthesizing rich semantic cues, e.g., differentiating heavy rain from rain with thunderstorms. To address this issue, we advocate leveraging an additional modality, sound, which has notable advantages in image manipulation as it can convey more diverse semantic cues (vivid emotions or dynamic expressions of the natural world) than texts. In this paper, we propose a novel approach that first extends the image–text joint embedding space with sound and applies a direct latent optimization method to manipulate a given image based on audio input, e.g., the sound of rain. Our extensive experiments show that our sound-guided image manipulation approach produces semantically and visually more plausible manipulation results than the state-of-the-art text and sound-guided image manipulation methods, which are further confirmed by our human evaluations. Our downstream task evaluations also show that our learned image–text-sound joint embedding space effectively encodes sound inputs. Examples are provided in our project page: https://kuai-lab.github.io/robust-demo/.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001953",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Physics",
      "Sound (geography)"
    ],
    "authors": [
      {
        "surname": "Lee",
        "given_name": "Seung Hyun"
      },
      {
        "surname": "Chi",
        "given_name": "Hyung-gun"
      },
      {
        "surname": "Oh",
        "given_name": "Gyeongrok"
      },
      {
        "surname": "Byeon",
        "given_name": "Wonmin"
      },
      {
        "surname": "Yoon",
        "given_name": "Sang Ho"
      },
      {
        "surname": "Park",
        "given_name": "Hyunje"
      },
      {
        "surname": "Cho",
        "given_name": "Wonjun"
      },
      {
        "surname": "Kim",
        "given_name": "Jinkyu"
      },
      {
        "surname": "Kim",
        "given_name": "Sangpil"
      }
    ]
  },
  {
    "title": "Neural Q-learning for discrete-time nonlinear zero-sum games with adjustable convergence rate",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106274",
    "abstract": "In this paper, an adjustable Q-learning scheme is developed to solve the discrete-time nonlinear zero-sum game problem, which can accelerate the convergence rate of the iterative Q-function sequence. First, the monotonicity and convergence of the iterative Q-function sequence are analyzed under some conditions. Moreover, by employing neural networks, the model-free tracking control problem can be overcome for zero-sum games. Second, two practical algorithms are designed to guarantee the convergence with accelerated learning. In one algorithm, an adjustable acceleration phase is added to the iteration process of Q-learning, which can be adaptively terminated with convergence guarantee. In another algorithm, a novel acceleration function is developed, which can adjust the relaxation factor to ensure the convergence. Finally, through a simulation example with the practical physical background, the fantastic performance of the developed algorithm is demonstrated with neural networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001989",
    "keywords": [
      "Acceleration",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Channel (broadcasting)",
      "Classical mechanics",
      "Computer network",
      "Computer science",
      "Control (management)",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Evolutionary biology",
      "Function (biology)",
      "Genetics",
      "Iterative learning control",
      "Linguistics",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Monotonic function",
      "Nonlinear system",
      "Philosophy",
      "Physics",
      "Psychology",
      "Quantum mechanics",
      "Rate of convergence",
      "Relaxation (psychology)",
      "Sequence (biology)",
      "Social psychology",
      "Zero (linguistics)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yuan"
      },
      {
        "surname": "Wang",
        "given_name": "Ding"
      },
      {
        "surname": "Zhao",
        "given_name": "Mingming"
      },
      {
        "surname": "Liu",
        "given_name": "Nan"
      },
      {
        "surname": "Qiao",
        "given_name": "Junfei"
      }
    ]
  },
  {
    "title": "A neurocomputational model of decision and confidence in object recognition task",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106318",
    "abstract": "How does the brain process natural visual stimuli to make a decision? Imagine driving through fog. An object looms ahead. What do you do? This decision requires not only identifying the object but also choosing an action based on your decision confidence. In this circumstance, confidence is making a bridge between seeing and believing. Our study unveils how the brain processes visual information to make such decisions with an assessment of confidence, using a model inspired by the visual cortex. To computationally model the process, this study uses a spiking neural network inspired by the hierarchy of the visual cortex in mammals to investigate the dynamics of feedforward object recognition and decision-making in the brain. The model consists of two modules: a temporal dynamic object representation module and an attractor neural network-based decision-making module. Unlike traditional models, ours captures the evolution of evidence within the visual cortex, mimicking how confidence forms in the brain. This offers a more biologically plausible approach to decision-making when encountering real-world stimuli. We conducted experiments using natural stimuli and measured accuracy, reaction time, and confidence. The model's estimated confidence aligns remarkably well with human-reported confidence. Furthermore, the model can simulate the human change-of-mind phenomenon, reflecting the ongoing evaluation of evidence in the brain. Also, this finding offers decision-making and confidence encoding share the same neural circuit.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002429",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Cognitive neuroscience of visual object recognition",
      "Computer science",
      "Law",
      "Machine learning",
      "Object (grammar)",
      "Operating system",
      "Political science",
      "Politics",
      "Process (computing)",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Roshan",
        "given_name": "Setareh Sadat"
      },
      {
        "surname": "Sadeghnejad",
        "given_name": "Naser"
      },
      {
        "surname": "Sharifizadeh",
        "given_name": "Fatemeh"
      },
      {
        "surname": "Ebrahimpour",
        "given_name": "Reza"
      }
    ]
  },
  {
    "title": "Generalized latent multi-view clustering with tensorized bipartite graph",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106282",
    "abstract": "Tensor-based multi-view spectral clustering algorithms use tensors to model the structure of multi-dimensional data to take advantage of the complementary information and high-order correlations embedded in the graph, thus achieving impressive clustering performance. However, these algorithms use linear models to obtain consensus, which prevents the learned consensus from adequately representing the nonlinear structure of complex data. In order to address this issue, we propose a method called Generalized Latent Multi-View Clustering with Tensorized Bipartite Graph (GLMC-TBG). Specifically, in this paper we introduce neural networks to learn highly nonlinear mappings that encode nonlinear structures in graphs into latent representations. In addition, multiple views share the same latent consensus through nonlinear interactions. In this way, a more comprehensive common representation from multiple views can be achieved. An Augmented Lagrangian Multiplier with Alternating Direction Minimization (ALM-ADM) framework is designed to optimize the model. Experiments on seven real-world data sets verify that the proposed algorithm is superior to state-of-the-art algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002065",
    "keywords": [
      "Artificial intelligence",
      "Bipartite graph",
      "Cluster analysis",
      "Computer science",
      "Graph",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Probabilistic latent semantic analysis",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Dongping"
      },
      {
        "surname": "Huang",
        "given_name": "Haonan"
      },
      {
        "surname": "Zhao",
        "given_name": "Qibin"
      },
      {
        "surname": "Zhou",
        "given_name": "Guoxu"
      }
    ]
  },
  {
    "title": "MV-SHIF: Multi-view symmetric hypothesis inference fusion network for emotion-cause pair extraction in documents",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106283",
    "abstract": "Emotion-cause pair extraction (ECPE) is a challenging task that aims to automatically identify pairs of emotions and their causes from documents. The difficulty of ECPE lies in distinguishing valid emotion-cause pairs from many irrelevant ones. Most previous methods have primarily focused on utilizing multi-task learning to extract semantic information solely from documents without explicitly encoding the relations between clauses. We propose a new approach that incorporates textual entailment paradigm aiming to infer the entailment relationship between the original document as the premise and the clauses or pairs described as the hypothesis. Our approach designs label-view hypothesis templates to improve ECPE by filtering out irrelevant emotion and cause clauses. Furthermore, we formulate candidate emotion-cause pairs as hypothesis statements, and define explicit multi-view symmetric templates to capture the emotion-cause relation semantics. The text entailment recognition for ECPE is finally implemented by fusing multi-view semantic information using a simplified capsule network. Our proposed model achieves state-of-the-art performance on ECPE compared to previous baselines. More importantly, this work demonstrates a novel effective way of applying the textual entailment paradigm to ECPE or clause-level causal discovery by designing multi-view hypothesis inference and information fusion.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002077",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Economics",
      "Inference",
      "Information extraction",
      "Linguistics",
      "Logical consequence",
      "Management",
      "Natural language processing",
      "Philosophy",
      "Premise",
      "Programming language",
      "Relationship extraction",
      "Semantics (computer science)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Cheng"
      },
      {
        "surname": "Zhang",
        "given_name": "Hua"
      },
      {
        "surname": "Chen",
        "given_name": "Bi"
      },
      {
        "surname": "Jiang",
        "given_name": "Bo"
      },
      {
        "surname": "Wang",
        "given_name": "Ye"
      }
    ]
  },
  {
    "title": "Spatial reconstructed local attention Res2Net with F0 subband for fake speech detection",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106320",
    "abstract": "The rhythm of bonafide speech is often difficult to replicate, which causes that the fundamental frequency (F0) of synthetic speech is significantly different from that of real speech. It is expected that the F0 feature contains the discriminative information for the fake speech detection (FSD) task. In this paper, we propose a novel F0 subband for FSD. In addition, to effectively model the F0 subband so as to improve the performance of FSD, the spatial reconstructed local attention Res2Net (SR-LA Res2Net) is proposed. Specifically, Res2Net is used as a backbone network to obtain multiscale information, and enhanced with a spatial reconstruction mechanism to avoid losing important information when the channel group is constantly superimposed. In addition, local attention is designed to make the model focus on the local information of the F0 subband. Experimental results on the ASVspoof 2019 LA dataset show that our proposed method obtains an equal error rate (EER) of 0.47% and a minimum tandem detection cost function (min t-DCF) of 0.0159, achieving the state-of-the-art performance among all of the single systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002442",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Discriminative model",
      "Economics",
      "Evolutionary biology",
      "Feature (linguistics)",
      "Focus (optics)",
      "Function (biology)",
      "Linguistics",
      "Management",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Replicate",
      "Speech recognition",
      "Statistics",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "Cunhang"
      },
      {
        "surname": "Xue",
        "given_name": "Jun"
      },
      {
        "surname": "Tao",
        "given_name": "Jianhua"
      },
      {
        "surname": "Yi",
        "given_name": "Jiangyan"
      },
      {
        "surname": "Wang",
        "given_name": "Chenglong"
      },
      {
        "surname": "Zheng",
        "given_name": "Chengshi"
      },
      {
        "surname": "Lv",
        "given_name": "Zhao"
      }
    ]
  },
  {
    "title": "Structural deep multi-view clustering with integrated abstraction and detail",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106287",
    "abstract": "Deep multi-view clustering, which can obtain complementary information from different views, has received considerable attention in recent years. Although some efforts have been made and achieve decent performances, most of them overlook the structural information and are susceptible to poor quality views, which may seriously restrict the capacity for clustering. To this end, we propose Structural deep Multi-View Clustering with integrated abstraction and detail (SMVC). Specifically, multi-layer perceptrons are used to extract features from specific views, which are then concatenated to form the global features. Besides, a global target distribution is constructed and guides the soft cluster assignments of specific views. In addition to the exploitation of the top-level abstraction, we also design the mining of the underlying details. We construct instance-level contrastive learning using high-order adjacency matrices, which has an equivalent effect to graph attention network and reduces feature redundancy. By integrating the top-level abstraction and underlying detail into a unified framework, our model can jointly optimize the cluster assignments and feature embeddings. Extensive experiments on four benchmark datasets have demonstrated that the proposed SMVC consistently outperforms the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002119",
    "keywords": [
      "Abstraction",
      "Adjacency list",
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Epistemology",
      "Feature (linguistics)",
      "Geodesy",
      "Geography",
      "Graph",
      "Linguistics",
      "Machine learning",
      "Operating system",
      "Philosophy",
      "Redundancy (engineering)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Bowei"
      },
      {
        "surname": "Xu",
        "given_name": "Sen"
      },
      {
        "surname": "Xu",
        "given_name": "Heyang"
      },
      {
        "surname": "Bian",
        "given_name": "Xuesheng"
      },
      {
        "surname": "Guo",
        "given_name": "Naixuan"
      },
      {
        "surname": "Xu",
        "given_name": "Xiufang"
      },
      {
        "surname": "Hua",
        "given_name": "Xiaopeng"
      },
      {
        "surname": "Zhou",
        "given_name": "Tian"
      }
    ]
  },
  {
    "title": "Medical image segmentation network based on multi-scale frequency domain filter",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106280",
    "abstract": "With the development of deep learning, medical image segmentation in computer-aided diagnosis has become a research hotspot. Recently, UNet and its variants have become the most powerful medical image segmentation methods. However, these methods suffer from (1) insufficient sensing field and insufficient depth; (2) computational nonlinearity and redundancy of channel features; and (3) ignoring the interrelationships among feature channels. These problems lead to poor network segmentation performance and weak generalization ability. Therefore, first of all, we propose an effective replacement scheme of UNet base block, Double residual depthwise atrous convolution (DRDAC) block, to effectively improve the deficiency of receptive field and depth. Secondly, a new linear module, the Multi-scale frequency domain filter (MFDF), is designed to capture global information from the frequency domain. The high order multi-scale relationship is extracted by combining the depthwise atrous separable convolution with the frequency domain filter. Finally, a channel attention called Axial selection channel attention (ASCA) is redesigned to enhance the network’s ability to model feature channel interrelationships. Further, we design a novel frequency domain medical image segmentation baseline method FDFUNet based on the above modules. We conduct extensive experiments on five publicly available medical image datasets and demonstrate that the present method has stronger segmentation performance as well as generalization ability compared to other state-of-the-art baseline methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002041",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Frequency domain",
      "Image segmentation",
      "Pattern recognition (psychology)",
      "Scale-space segmentation",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yufeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaoqian"
      },
      {
        "surname": "Peng",
        "given_name": "Lifan"
      },
      {
        "surname": "He",
        "given_name": "Youdong"
      },
      {
        "surname": "Sun",
        "given_name": "Feng"
      },
      {
        "surname": "Sun",
        "given_name": "Huaijiang"
      }
    ]
  },
  {
    "title": "Adversarial infrared blocks: A multi-view black-box attack to thermal infrared detectors in physical world",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106310",
    "abstract": "Thermal infrared detectors have a vast array of potential applications in pedestrian detection and autonomous driving, and their safety performance is of great concern. Recent works use bulb plate, “QR” suit, and infrared patches as physical perturbations to perform white-box attacks on thermal infrared detectors, which are effective but not practical for real-world scenarios. Some researchers have tried to utilize hot and cold blocks as physical perturbations for black-box attacks on thermal infrared detectors. However, this attempts has not yielded robust and multi-view physical attacks, indicating limitations in the approach. To overcome the limitations of existing approaches, we introduce a novel black-box physical attack method, called adversarial infrared blocks (AdvIB). By optimizing the physical parameters of the infrared blocks and deploying them to pedestrians from multiple views, including the front, side, and back, AdvIB can execute robust and multi-view attacks on thermal infrared detectors. Our physical tests show that the proposed method achieves a success rate of over 80% under most distance and view conditions, validating its effectiveness. For stealthiness, our method involves attaching the adversarial infrared block to the inside of clothing, enhancing its stealthiness. Additionally, we perform comprehensive experiments and compare the experimental results with baseline to verify the robustness of our method. In summary, AdvIB allows for potent multi-view black-box attacks, profoundly influencing ethical considerations in today’s society. Potential consequences, including disasters from technology misuse and attackers’ legal liability, highlight crucial ethical and security issues associated with AdvIB. Considering these concerns, we urge heightened attention to the proposed AdvIB. Our code can be accessed from the following link: https://github.com/ChengYinHu/AdvIB.git.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400234X",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Black box",
      "Block (permutation group theory)",
      "Computer science",
      "Computer security",
      "Detector",
      "Geometry",
      "Infrared",
      "Infrared detector",
      "Mathematics",
      "Optics",
      "Physics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Chengyin"
      },
      {
        "surname": "Shi",
        "given_name": "Weiwen"
      },
      {
        "surname": "Jiang",
        "given_name": "Tingsong"
      },
      {
        "surname": "Yao",
        "given_name": "Wen"
      },
      {
        "surname": "Tian",
        "given_name": "Ling"
      },
      {
        "surname": "Chen",
        "given_name": "Xiaoqian"
      },
      {
        "surname": "Zhou",
        "given_name": "Jingzhi"
      },
      {
        "surname": "Li",
        "given_name": "Wen"
      }
    ]
  },
  {
    "title": "Bayesian tensor network structure search and its application to tensor completion",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106290",
    "abstract": "Tensor network (TN) has demonstrated remarkable efficacy in the compact representation of high-order data. In contrast to the TN methods with pre-determined structures, the recently introduced tensor network structure search (TNSS) methods automatically learn a compact TN structure from the data, gaining increasing attention. Nonetheless, TNSS requires time-consuming manual adjustments of the penalty parameters that control the model complexity to achieve better performance, especially in the presence of missing or noisy data. To provide an effective solution to this problem, in this paper, we propose a parameters tuning-free TNSS algorithm based on Bayesian modeling, aiming at conducting TNSS in a fully data-driven manner. Specifically, the uncertainty in the data corruption is well-incorporated in the prior setting of the probabilistic model. For TN structure determination, we reframe it as a rank learning problem of the fully-connected tensor network (FCTN), integrating the generalized inverse Gaussian (GIG) distribution for low-rank promotion. To eliminate the need for hyperparameter tuning, we adopt a fully Bayesian approach and propose an efficient Markov chain Monte Carlo (MCMC) algorithm for posterior distribution sampling. Compared with the previous TNSS method, experiment results demonstrate the proposed algorithm can effectively and efficiently find the latent TN structures of the data under various missing and noise conditions and achieves the best recovery results. Furthermore, our method exhibits superior performance in tensor completion with real-world data compared to other state-of-the-art tensor-decomposition-based completion methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002144",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bayesian probability",
      "Computer science",
      "Hyperparameter",
      "Machine learning",
      "Markov chain Monte Carlo",
      "Mathematical optimization",
      "Mathematics",
      "Missing data",
      "Pure mathematics",
      "Tensor (intrinsic definition)"
    ],
    "authors": [
      {
        "surname": "Zeng",
        "given_name": "Junhua"
      },
      {
        "surname": "Zhou",
        "given_name": "Guoxu"
      },
      {
        "surname": "Qiu",
        "given_name": "Yuning"
      },
      {
        "surname": "Li",
        "given_name": "Chao"
      },
      {
        "surname": "Zhao",
        "given_name": "Qibin"
      }
    ]
  },
  {
    "title": "Investigation of out-of-distribution detection across various models and training methodologies",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106288",
    "abstract": "Machine learning-based algorithms demonstrate impressive performance across numerous fields; however, they continue to suffer from certain limitations. Even sophisticated and precise algorithms often make erroneous predictions when implemented with datasets having different distributions compared to the training set. Out-of-distribution (OOD) detection, which distinguishes data with different distributions from that of the training set, is a critical research area necessary to overcome these limitations and create more reliable algorithms. The OOD issue, particularly concerning image data, has been extensively studied. However, recently developed OOD methods do not fulfill the expectation that OOD performance will increase as the accuracy of in-distribution classification improves. Our research presents a comprehensive study on OOD detection performance across multiple models and training methodologies to verify this phenomenon. Specifically, we explore various pre-trained models popular in the computer vision field with both old and new OOD detection methods. The experimental results highlight the performance disparity in existing OOD methods. Based on these observations, we introduce Trimmed Rank with Inverse softMax probability (TRIM), a remarkably simple yet effective method for model weights with newly developed training methods. The proposed method could serve as a potential tool for enhancing OOD detection performance owing to its promising results. The OOD performance of TRIM is highly compatible with the in-distribution accuracy model and may bridge the efforts on improving in-distribution accuracy to the ability to distinguish OOD data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002120",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Data mining",
      "Field (mathematics)",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Programming language",
      "Pure mathematics",
      "Set (abstract data type)",
      "Softmax function",
      "Training set",
      "Trim"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Byung Chun"
      },
      {
        "surname": "Kim",
        "given_name": "Byungro"
      },
      {
        "surname": "Hyun",
        "given_name": "Yoonsuk"
      }
    ]
  },
  {
    "title": "Self-paced regularized adaptive multi-view unsupervised feature selection",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106295",
    "abstract": "Multi-view unsupervised feature selection (MUFS) is an efficient approach for dimensional reduction of heterogeneous data. However, existing MUFS approaches mostly assign the samples the same weight, thus the diversity of samples is not utilized efficiently. Additionally, due to the presence of various regularizations, the resulting MUFS problems are often non-convex, making it difficult to find the optimal solutions. To address this issue, a novel MUFS method named Self-paced Regularized Adaptive Multi-view Unsupervised Feature Selection (SPAMUFS) is proposed. Specifically, the proposed approach firstly trains the MUFS model with simple samples, and gradually learns complex samples by using self-paced regularizer. l 2 , p -norm ( 0 < p ≤ 1 ) is employed to measure the learning error and as the sparse regularization to accommodate various sparsity requirements across different datasets. Moreover, hypergraph Laplacian matrices are constructed for each view to better preserve the local manifold structure and encode high-order relationships within the data space. They are adaptively assigned weights to learn the underlying correlated and diverse information among different views. An iterative optimization algorithm is proposed to solve SPAMUFS and the convergence and computational complexity are also analyzed. The effectiveness of SPAMUFS is substantiated by comparing with eight state-of-the-art algorithms on nine public multi-view datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002193",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Discrete mathematics",
      "Feature selection",
      "Hypergraph",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Xuanhao"
      },
      {
        "surname": "Che",
        "given_name": "Hangjun"
      },
      {
        "surname": "Leung",
        "given_name": "Man-Fai"
      },
      {
        "surname": "Wen",
        "given_name": "Shiping"
      }
    ]
  },
  {
    "title": "Generalizability and robustness evaluation of attribute-based zero-shot learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106278",
    "abstract": "In the field of deep learning, large quantities of data are typically required to effectively train models. This challenge has given rise to techniques like zero-shot learning (ZSL), which trains models on a set of “seen” classes and evaluates them on a set of “unseen” classes. Although ZSL has shown considerable potential, particularly with the employment of generative methods, its generalizability to real-world scenarios remains uncertain. The hypothesis of this work is that the performance of ZSL models is systematically influenced by the chosen “splits”; in particular, the statistical properties of the classes and attributes used in training. In this paper, we test this hypothesis by introducing the concepts of generalizability and robustness in attribute-based ZSL and carry out a variety of experiments to stress-test ZSL models against different splits. Our aim is to lay the groundwork for future research on ZSL models’ generalizability, robustness, and practical applications. We evaluate the accuracy of state-of-the-art models on benchmark datasets and identify consistent trends in generalizability and robustness. We analyze how these properties vary based on the dataset type, differentiating between coarse- and fine-grained datasets, and our findings indicate significant room for improvement in both generalizability and robustness. Furthermore, our results demonstrate the effectiveness of dimensionality reduction techniques in improving the performance of state-of-the-art models in fine-grained datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002028",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Gene",
      "Generalizability theory",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Mathematics",
      "Robustness (evolution)",
      "Statistics",
      "Training set"
    ],
    "authors": [
      {
        "surname": "Rossi",
        "given_name": "Luca"
      },
      {
        "surname": "Fiorentino",
        "given_name": "Maria Chiara"
      },
      {
        "surname": "Mancini",
        "given_name": "Adriano"
      },
      {
        "surname": "Paolanti",
        "given_name": "Marina"
      },
      {
        "surname": "Rosati",
        "given_name": "Riccardo"
      },
      {
        "surname": "Zingaretti",
        "given_name": "Primo"
      }
    ]
  },
  {
    "title": "Multiband task related components enhance rapid cognition decoding for both small and similar objects",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106313",
    "abstract": "The cortically-coupled target recognition system based on rapid serial visual presentation (RSVP) has a wide range of applications in brain computer interface (BCI) fields such as medical and military. However, in the complex natural environment backgrounds, the identification of event-related potentials (ERP) of both small and similar objects that are quickly presented is a research challenge. Therefore, we designed corresponding experimental paradigms and proposed a multi-band task related components matching (MTRCM) method to improve the rapid cognitive decoding of both small and similar objects. We compared the areas under the receiver operating characteristic curve (AUC) between MTRCM and other 9 methods under different numbers of training sample using RSVP-ERP data from 50 subjects. The results showed that MTRCM maintained an overall superiority and achieved the highest average AUC (0.6562 ± 0.0091). We also optimized the frequency band and the time parameters of the method. The verification on public data sets further showed the necessity of designing MTRCM method. The MTRCM method provides a new approach for neural decoding of both small and similar RSVP objects, which is conducive to promote the further development of RSVP-BCI.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002375",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Botany",
      "Brain–computer interface",
      "Bubble",
      "Cognition",
      "Computer science",
      "Decoding methods",
      "Economics",
      "Electroencephalography",
      "Identification (biology)",
      "Interface (matter)",
      "Machine learning",
      "Management",
      "Matching (statistics)",
      "Mathematics",
      "Maximum bubble pressure method",
      "Neuroscience",
      "Parallel computing",
      "Pattern recognition (psychology)",
      "Psychiatry",
      "Psychology",
      "Rapid serial visual presentation",
      "Receiver operating characteristic",
      "Statistics",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Yusong"
      },
      {
        "surname": "Yang",
        "given_name": "Banghua"
      },
      {
        "surname": "Wang",
        "given_name": "Changyong"
      }
    ]
  },
  {
    "title": "Gossip-based distributed stochastic mirror descent for constrained optimization",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106291",
    "abstract": "This paper considers a distributed constrained optimization problem over a multi-agent network in the non-Euclidean sense. The gossip protocol is adopted to relieve the communication burden, which also adapts to the constantly changing topology of the network. Based on this idea, a gossip-based distributed stochastic mirror descent (GB-DSMD) algorithm is proposed to handle the problem under consideration. The performances of GB-DSMD algorithms with constant and diminishing step sizes are analyzed, respectively. When the step size is constant, the error bound between the optimal function value and the expected function value corresponding to the average iteration output of the algorithm is derived. While for the case of the diminishing step size, it is proved that the output of the algorithm uniformly approaches to the optimal value with probability 1. Finally, as a numerical example, the distributed logistic regression is reported to demonstrate the effectiveness of the GB-DSMD algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002156",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Constant (computer programming)",
      "Distributed algorithm",
      "Evolutionary biology",
      "Function (biology)",
      "Gossip",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Programming language",
      "Psychology",
      "Social psychology",
      "Stochastic gradient descent",
      "Upper and lower bounds"
    ],
    "authors": [
      {
        "surname": "Fang",
        "given_name": "Xianju"
      },
      {
        "surname": "Zhang",
        "given_name": "Baoyong"
      },
      {
        "surname": "Yuan",
        "given_name": "Deming"
      }
    ]
  },
  {
    "title": "Observer-based differential evolution constrained control for safe reference tracking in robots",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106273",
    "abstract": "Big torque inputs in controls could increase energy consumption, and big estimated perturbations in observers could produce device damages. Therefore, it would be interesting to propose a constrained control for safe reference tracking and a constrained observer for safe perturbation estimation in robots. Furthermore, the best gains in controls produce a balance between safe reference tracking and save energy consumption. Therefore, it would be interesting to propose a method to find the best gains. In this paper, an observer-based differential evolution constrained control is proposed for safe reference tracking in robots. The contributions are described as follows: (1) a constrained observer is proposed for safe perturbation estimation in robots, (2) a constrained control is proposed for safe reference tracking in robots, (3) a differential evolution optimizer is used to find the best gains in an observer-based constrained control, (4) the robust stability in an observer-based constrained control is assured, (5) the pseudo-code of an observer-based differential evolution constrained control is detailed. The proposed observer-based differential evolution constrained control is applied for safe reference tracking in two robots.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001977",
    "keywords": [
      "Aerospace engineering",
      "Artificial intelligence",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Differential (mechanical device)",
      "Engineering",
      "Observer (physics)",
      "Perturbation (astronomy)",
      "Physics",
      "Quantum mechanics",
      "Robot"
    ],
    "authors": [
      {
        "surname": "de Jesús Rubio",
        "given_name": "José"
      },
      {
        "surname": "Orozco",
        "given_name": "Eduardo"
      },
      {
        "surname": "Cordova",
        "given_name": "Daniel Andres"
      },
      {
        "surname": "Hernandez",
        "given_name": "Mario Alberto"
      },
      {
        "surname": "Rosas",
        "given_name": "Francisco Javier"
      },
      {
        "surname": "Pacheco",
        "given_name": "Jaime"
      }
    ]
  },
  {
    "title": "A lightweight and gradient-stable neural layer",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106269",
    "abstract": "To enhance resource efficiency and model deployability of neural networks, we propose a neural-layer architecture based on Householder weighting and absolute-value activating, called Householder-absolute neural layer or simply Han-layer. Compared to a fully connected layer with d -neurons and d outputs, a Han-layer reduces the number of parameters and the corresponding computational complexity from O ( d 2 ) to O ( d ) . The Han-layer structure guarantees that the Jacobian of the layer function is always orthogonal, thus ensuring gradient stability (i.e., free of gradient vanishing or exploding issues) for any Han-layer sub-networks. Extensive numerical experiments show that one can strategically use Han-layers to replace fully connected (FC) layers, reducing the number of model parameters while maintaining or even improving the generalization performance. We will also showcase the capabilities of the Han-layer architecture on a few small stylized models, and discuss its current limitations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400193X",
    "keywords": [
      "Acoustics",
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Evolutionary biology",
      "Function (biology)",
      "Generalization",
      "Jacobian matrix and determinant",
      "Layer (electronics)",
      "Machine learning",
      "Materials science",
      "Mathematical analysis",
      "Mathematics",
      "Nanotechnology",
      "Physics",
      "Stability (learning theory)",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Yueyao"
      },
      {
        "surname": "Zhang",
        "given_name": "Yin"
      }
    ]
  },
  {
    "title": "LollipopE: Bi-centered lollipop embedding for complex logic query on knowledge graph",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106277",
    "abstract": "Answering complex First-Order Logic (FOL) query plays a vital role in multi-hop knowledge graph (KG) reasoning. Geometric methods have emerged as a promising category of approaches in this context. However, existing best-performing geometric query embedding (QE) model is still up against three-fold potential problems: (i) underutilization of embedding space, (ii) overreliance on angle information, (iii) uncaptured hierarchy structure. To bridge the gap, we propose a lollipop-like bi-centered query embedding method named LollipopE. To fully utilize embedding space, LollipopE employs learnable centroid positions to represent multiple entities distributed along the same axis. To address the potential overreliance on angular metrics, we design an angular-based and centroid-based metric. This involves calculating both an angular distance and a centroid-based geodesic distance, which empowers the model to make more informed selections of relevant answers from a wider perspective. To effectively capture the hierarchical relationships among entities within the KG, we incorporate dynamic moduli, which allows for the representation of the hierarchical structure among entities. Extensive experiments demonstrate that LollipopE surpasses the state-of-the-art geometric methods. Especially, on more hierarchical datasets, LollipopE achieves the most significant improvement.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024002016",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Centroid",
      "Computer science",
      "Context (archaeology)",
      "Economics",
      "Embedding",
      "Geodesic",
      "Geometry",
      "Graph",
      "Graph embedding",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Paleontology",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Shiyao"
      },
      {
        "surname": "Tian",
        "given_name": "Changyuan"
      },
      {
        "surname": "Zhang",
        "given_name": "Zequn"
      },
      {
        "surname": "Xu",
        "given_name": "Guangluan"
      }
    ]
  },
  {
    "title": "SecureNet: Proactive intellectual property protection and model security defense for DNNs based on backdoor learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106199",
    "abstract": "With the widespread application of deep neural networks (DNNs), the risk of privacy breaches against DNN models is constantly on the rise, resulting in an increasing need for intellectual property (IP) protection for such models. Although neural network watermarking techniques are widely used to safeguard the IP of DNNs, they can only achieve passive protection and cannot actively prevent unauthorized users from illicit use or embezzlement of the trained DNN models. Therefore, the development of proactive protection techniques to prevent IP infringement is imperative. To this end, we propose SecureNet, a key-based access license framework for DNN models. The proposed approach involves injecting license keys into the model through backdoor learning, enabling correct model functionality only when the appropriate license key is included in the input. To ensure the reusability of DNN models, we also propose a license key replacement algorithm. In addition, based on SecureNet, we designed defense mechanisms against adversarial attacks and backdoor attacks, respectively. Furthermore, we introduce a fine-grained authorization method that enables flexible granting of model permissions to different users. We have designed four license-key schemes with different privileges, tailored to various scenarios. We evaluated SecureNet on five benchmark datasets including MNIST, Cifar10, Cifar100, FaceScrub, and CelebA, and assessed its performance on six classic DNN models: LeNet-5, VGG16, ResNet18, ResNet101, NFNet-F5, and MobileNetV3. The results demonstrate that our approach outperforms the state-of-the-art model parameter encryption methods by at least 95% in terms of computational efficiency. Additionally, it provides effective defense against adversarial attacks and backdoor attacks without compromising the model’s overall performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001230",
    "keywords": [
      "Artificial intelligence",
      "Backdoor",
      "Computer science",
      "Computer security",
      "Intellectual property",
      "Key (lock)",
      "License",
      "Operating system"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Peihao"
      },
      {
        "surname": "Huang",
        "given_name": "Jie"
      },
      {
        "surname": "Wu",
        "given_name": "Huaqing"
      },
      {
        "surname": "Zhang",
        "given_name": "Zeping"
      },
      {
        "surname": "Qi",
        "given_name": "Chunyang"
      }
    ]
  },
  {
    "title": "Enhanced deep unrolling networks for snapshot compressive hyperspectral imaging",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106250",
    "abstract": "Snapshot compressive hyperspectral imaging necessitates the reconstruction of a complete hyperspectral image from its compressive snapshot measurement, presenting a challenging inverse problem. This paper proposes an enhanced deep unrolling neural network, called EDUNet, to tackle this problem. The EDUNet is constructed via the deep unrolling of a proximal gradient descent algorithm and introduces two innovative modules for gradient-driven update and proximal mapping reflectivity. The gradient-driven update module leverages a memory-assistant descent approach inspired by momentum-based acceleration techniques, for enhancing the unrolled reconstruction process and improving convergence. The proximal mapping is modeled by a sub-network with a cross-stage spectral self-attention, which effectively exploits the inherent self-similarities present in hyperspectral images along the spectral axis. It also enhances feature flow throughout the network, contributing to reconstruction performance gain. Furthermore, we introduce a spectral geometry consistency loss, encouraging EDUNet to prioritize the geometric layouts of spectral curves, leading to a more precise capture of spectral information in hyperspectral images. Experiments are conducted using three benchmark datasets including KAIST, ICVL, and Harvard, along with some real data, comprising a total of 73 samples. The experimental results demonstrate that EDUNet outperforms 15 competing models across four metrics including PSNR, SSIM, SAM, and ERGAS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001746",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Compiler",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Geodesy",
      "Geography",
      "Gradient descent",
      "Hyperspectral imaging",
      "Iterative reconstruction",
      "Loop unrolling",
      "Operating system",
      "Pattern recognition (psychology)",
      "Programming language",
      "Snapshot (computer storage)",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Qin",
        "given_name": "Xinran"
      },
      {
        "surname": "Quan",
        "given_name": "Yuhui"
      },
      {
        "surname": "Ji",
        "given_name": "Hui"
      }
    ]
  },
  {
    "title": "Distribution-free Bayesian regularized learning framework for semi-supervised learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106262",
    "abstract": "In machine learning it is often necessary to assume or know the distribution of the data, however it is difficult to do so in practical applications. Aiming to this problem, this work, we propose a novel distribution-free Bayesian regularized learning framework for semi-supervised learning, which is called Hessian regularized twin minimax probability extreme learning machine (HRTMPELM). In this framework, we attempt to construct two non-parallel hyperplanes by introducing the high separation probability assumption, such that each hyperplane separates samples from one class with maximum probability while moving away from samples from the other class. Subsidiently, the framework can be utilized to construct reasonable semi-supervised classifiers by using the information of the inherent geometric distribution of the samples through the Hessian regularization term. Additionally, the proposed framework controls the misclassification error of samples by minimizing the upper limit of the worst-case misclassification probability, and improves the generalization performance of the model by introducing the idea of regularization to avoid the occurrence of ill-posedness and overfitting problems. More importantly, the framework has no hyperparameters, making the learning process very simplified and efficient. Finally, a simple and reliable algorithm with globally optimal solutions via multivariate Chebyshev inequalities is designed for solving the proposed learning framework. Experiments on multiple datasets demonstrate the reliability and effectiveness of the proposed learning framework compared to other methods. Especially, we applied the framework to Ningxia wolfberry quality detection, which greatly enriches and facilitates the application of machine learning algorithms in the agricultural field.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001862",
    "keywords": [
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Bayesian probability",
      "Computer science",
      "Hessian matrix",
      "Hyperparameter",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Minimax",
      "Prior probability",
      "Probability distribution",
      "Regularization (linguistics)",
      "Semi-supervised learning",
      "Statistics",
      "Supervised learning"
    ],
    "authors": [
      {
        "surname": "Ma",
        "given_name": "Jun"
      },
      {
        "surname": "Yu",
        "given_name": "Guolin"
      }
    ]
  },
  {
    "title": "DWSSA: Alleviating over-smoothness for deep Graph Neural Networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106228",
    "abstract": "Graph Neural Networks (GNNs) have demonstrated great potential in achieving outstanding performance in various graph-related tasks, e.g., graph classification and link prediction. However, most of them suffer from the following issue: shallow networks capture very limited knowledge. Prior works design deep GNNs with more layers to solve the issue, which however introduces a new challenge, i.e., the infamous over-smoothness. Graph representation over emphasizes node features but only considers the static graph structure with a uniform weight are the key reasons for the over-smoothness issue. To alleviate the issue, this paper proposes a Dynamic Weighting Strategy (DWS) for addressing over-smoothness. We first employ Fuzzy C-Means (FCM) to cluster all nodes into several groups and get each node’s fuzzy assignment, based on which a novel metric function is devised for dynamically adjusting the aggregation weights. This dynamic weighting strategy not only enables the intra-cluster interactions, but also inter-cluster aggregations, which well addresses undifferentiated aggregation caused by uniform weights. Based on DWS, we further design a Structure Augmentation (SA) step for addressing the issue of underutilizing the graph structure, where some potentially meaningful connections (i.e., edges) are added to the original graph structure via a parallelable KNN algorithm. In general, the optimized Dynamic Weighting Strategy with Structure Augmentation (DWSSA) alleviates over-smoothness by reducing noisy aggregations and utilizing topological knowledge. Extensive experiments on eleven homophilous or heterophilous graph benchmarks demonstrate the effectiveness of our proposed method DWSSA in alleviating over-smoothness and enhancing deep GNNs performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001527",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Graph",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Radiology",
      "Smoothness",
      "Theoretical computer science",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Qirong"
      },
      {
        "surname": "Li",
        "given_name": "Jin"
      },
      {
        "surname": "Ye",
        "given_name": "Qingqing"
      },
      {
        "surname": "Lin",
        "given_name": "Yuxi"
      },
      {
        "surname": "Chen",
        "given_name": "Xinlong"
      },
      {
        "surname": "Fu",
        "given_name": "Yang-Geng"
      }
    ]
  },
  {
    "title": "Generalization analysis of deep CNNs under maximum correntropy criterion",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106226",
    "abstract": "Convolutional neural networks (CNNs) have gained immense popularity in recent years, finding their utility in diverse fields such as image recognition, natural language processing, and bio-informatics. Despite the remarkable progress made in deep learning theory, most studies on CNNs, especially in regression tasks, tend to heavily rely on the least squares loss function. However, there are situations where such learning algorithms may not suffice, particularly in the presence of heavy-tailed noises or outliers. This predicament emphasizes the necessity of exploring alternative loss functions that can handle such scenarios more effectively, thereby unleashing the true potential of CNNs. In this paper, we investigate the generalization error of deep CNNs with the rectified linear unit (ReLU) activation function for robust regression problems within an information-theoretic learning framework. Our study demonstrates that when the regression function exhibits an additive ridge structure and the noise possesses a finite p th moment, the empirical risk minimization scheme, generated by the maximum correntropy criterion and deep CNNs, achieves fast convergence rates. Notably, these rates align with the mini-max optimal convergence rates attained by fully connected neural network model with the Huber loss function up to a logarithmic factor. Additionally, we further establish the convergence rates of deep CNNs under the maximum correntropy criterion when the regression function resides in a Sobolev space on the sphere.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001503",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Generalization",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Outlier",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Rate of convergence",
      "Regression",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Yingqiao"
      },
      {
        "surname": "Fang",
        "given_name": "Zhiying"
      },
      {
        "surname": "Fan",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "An elastic competitive and discriminative collaborative representation method for image classification",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106231",
    "abstract": "Collaborative representation-based (CR) methods have become prevalent for pattern classification tasks, achieving formidable performance. Theoretically, we expect the learned class-specific representation of the correct class to be discriminative against others, with the representation of the correct class contributing dominantly in CR. However, most existing CR methods focus on improving discrimination while having a limited impact on enhancing the representation contribution of the correct category. In this work, we propose a novel CR approach for image classification called the elastic competitive and discriminative collaborative representation-based classifier (ECDCRC) to simultaneously strengthen representation contribution and discrimination of the correct class. The ECDCRC objective function penalizes two key terms by fully incorporating label information. The competitive term integrates the nearest subspace representation with corresponding elastic factors into the model, allowing each class to have varying competition intensities based on similarity with the query sample. This enhances the representation contribution of the correct class in CR. To further improve discrimination, the discriminative term introduces an elastic factor as a weight in the model to represent the gap between the query sample and the representation of each class. Moreover, instead of focusing on representation coefficients, the designed ECDCRC weights associated with representation components directly relate to the representation of each class, enabling more direct and precise discrimination improvement. Concurrently, sparsity is also enhanced through the two terms, further boosting model performance. Additionally, we propose a robust ECDCRC (R-ECDCRC) to handle image classification with noise. Extensive experiments on seven public databases demonstrate the proposed method’s superior performance over related state-of-the-art CR methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001552",
    "keywords": [
      "Artificial intelligence",
      "Boosting (machine learning)",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Contextual image classification",
      "Discriminative model",
      "Image (mathematics)",
      "Law",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Subspace topology"
    ],
    "authors": [
      {
        "surname": "Mi",
        "given_name": "Jian-Xun"
      },
      {
        "surname": "Chen",
        "given_name": "Jianfei"
      },
      {
        "surname": "Yin",
        "given_name": "Shijie"
      },
      {
        "surname": "Li",
        "given_name": "Weisheng"
      }
    ]
  },
  {
    "title": "Memristor-based circuit design of episodic memory neural network and its application in hurricane category prediction",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106268",
    "abstract": "Episodic memory, as a type of long-term memory (LTM), is used to learn and store the unique personal experience. Based on the episodic memory biological mechanism, this paper proposes a bionic episodic memory memristive neural network circuit. The proposed memristive neural network circuit includes a neocortical module, a parahippocampal module and a hippocampus module. The neocortical module with the two paths structure is used to receive the sensory signal, and is also used to separate and transmit the spatial information and the non-spatial information involved in the sensory signal. The parahippocampal module is composed of the parahippocampal cortex-MEA and the perirhinal cortex-LEA, which receives the two types of information from the neocortical module respectively. As the last module, the hippocampus module receives and integrates the output information of the parahippocampal module as well as generates the corresponding episodic memory. Meanwhile, the specific scenario information with the certain temporal signal from the generated episodic memory is also extracted by the hippocampus module. The simulation results in PSPICE show that the proposed memristive neural network circuit can generate the various episodic memories and extract the specific scenario information successfully. By configuring the memristor parameters, the proposed bionic episodic memory memristive neural network circuit can be applied to the hurricane category prediction, which verifies the feasibility of this work.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001928",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Cognition",
      "Computer science",
      "Electronic engineering",
      "Engineering",
      "Epilepsy",
      "Episodic memory",
      "Hippocampus",
      "Memristor",
      "Neuroscience",
      "Perirhinal cortex",
      "Psychology",
      "Temporal lobe"
    ],
    "authors": [
      {
        "surname": "Wan",
        "given_name": "Qiuzhen"
      },
      {
        "surname": "Liu",
        "given_name": "Jiong"
      },
      {
        "surname": "Liu",
        "given_name": "Tieqiao"
      },
      {
        "surname": "Sun",
        "given_name": "Kunliang"
      },
      {
        "surname": "Qin",
        "given_name": "Peng"
      }
    ]
  },
  {
    "title": "PSE-Net: Channel pruning for Convolutional Neural Networks with parallel-subnets estimator",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106263",
    "abstract": "Channel Pruning is one of the most widespread techniques used to compress deep neural networks while maintaining their performances. Currently, a typical pruning algorithm leverages neural architecture search to directly find networks with a configurable width, the key step of which is to identify representative subnet for various pruning ratios by training a supernet. However, current methods mainly follow a serial training strategy to optimize supernet, which is very time-consuming. In this work, we introduce PSE-Net, a novel parallel-subnets estimator for efficient channel pruning. Specifically, we propose a parallel-subnets training algorithm that simulate the forward–backward pass of multiple subnets by droping extraneous features on batch dimension, thus various subnets could be trained in one round. Our proposed algorithm facilitates the efficiency of supernet training and equips the network with the ability to interpolate the accuracy of unsampled subnets, enabling PSE-Net to effectively evaluate and rank the subnets. Over the trained supernet, we develop a prior-distributed-based sampling algorithm to boost the performance of classical evolutionary search. Such algorithm utilizes the prior information of supernet training phase to assist in the search of optimal subnets while tackling the challenge of discovering samples that satisfy resource constraints due to the long-tail distribution of network configuration. Extensive experiments demonstrate PSE-Net outperforms previous state-of-the-art channel pruning methods on the ImageNet dataset while retaining superior supernet training efficiency. For example, under 300M FLOPs constraint, our pruned MobileNetV2 achieves 75.2% Top-1 accuracy on ImageNet dataset, exceeding the original MobileNetV2 by 2.6 units while only cost 30%/16% times than BCNet/AutoAlim.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001874",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Convolutional neural network",
      "Estimator",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Pruning",
      "Statistics",
      "Subnet"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Shiguang"
      },
      {
        "surname": "Xie",
        "given_name": "Tao"
      },
      {
        "surname": "Liu",
        "given_name": "Haijun"
      },
      {
        "surname": "Zhang",
        "given_name": "Xingcheng"
      },
      {
        "surname": "Cheng",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "A novel predefined-time neurodynamic approach for mixed variational inequality problems and applications",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106247",
    "abstract": "In this paper, we propose a novel neurodynamic approach with predefined-time stability that offers a solution to address mixed variational inequality problems. Our approach introduces an adjustable time parameter, thereby enhancing flexibility and applicability compared to conventional fixed-time stability methods. By satisfying certain conditions, the proposed approach is capable of converging to a unique solution within a predefined-time, which sets it apart from fixed-time stability and finite-time stability approaches. Furthermore, our approach can be extended to address a wide range of mathematical optimization problems, including variational inequalities, nonlinear complementarity problems, sparse signal recovery problems, and nash equilibria seeking problems in noncooperative games. We provide numerical simulations to validate the theoretical derivation and showcase the effectiveness and feasibility of our proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001710",
    "keywords": [
      "Biology",
      "Complementarity (molecular biology)",
      "Complementarity theory",
      "Composite material",
      "Computer science",
      "Flexibility (engineering)",
      "Genetics",
      "Machine learning",
      "Materials science",
      "Mathematical optimization",
      "Mathematics",
      "Nonlinear system",
      "Optimization problem",
      "Physics",
      "Quantum mechanics",
      "Range (aeronautics)",
      "Stability (learning theory)",
      "Statistics",
      "Variational inequality"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Jinlan"
      },
      {
        "surname": "Ju",
        "given_name": "Xingxing"
      },
      {
        "surname": "Zhang",
        "given_name": "Naimin"
      },
      {
        "surname": "Xu",
        "given_name": "Dongpo"
      }
    ]
  },
  {
    "title": "Resilient event-triggering adaptive neural network control for networked systems under mixed cyber attacks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106249",
    "abstract": "This paper addresses the resilient event-triggering adaptive neural network (NN) control problem for networked control systems under mixed cyber attacks. Compared with the conventional event-triggered mechanism (ETM) with constant threshold, a novel resilient ETM is designed to withstand the affect of denial-of-service attacks and conserve communication resources. Different from the energy-bounded deception attacks, an unknown state-dependent nonlinear attack signal is considered in this work. To identify the deception attack, the NN technique is utilized to approximate the unknown attack signal. Subsequently, an adaptive controller is established to compensate for the malicious affects of deception attacks on the system. Furthermore, sufficient conditions for the boundedness of the system are derived via applying the Lyapunov functional, and a co-design strategy for control gain and event-triggering parameter is provided. Finally, the feasibility of the proposed approach is validated through a robot manipulator system.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001734",
    "keywords": [
      "Adaptive control",
      "Agronomy",
      "Artificial intelligence",
      "Artificial neural network",
      "Authentication (law)",
      "Biology",
      "Bounded function",
      "Computer science",
      "Computer security",
      "Control (management)",
      "Control theory (sociology)",
      "Controller (irrigation)",
      "Cyber-attack",
      "Deception",
      "Denial-of-service attack",
      "Energy (signal processing)",
      "Event (particle physics)",
      "Lyapunov function",
      "Mathematical analysis",
      "Mathematics",
      "Nonlinear system",
      "Physics",
      "Programming language",
      "Psychology",
      "Quantum mechanics",
      "Replay attack",
      "SIGNAL (programming language)",
      "Social psychology",
      "Statistics",
      "The Internet",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Ning"
      },
      {
        "surname": "Zhao",
        "given_name": "Dongke"
      },
      {
        "surname": "Liu",
        "given_name": "Yongchao"
      }
    ]
  },
  {
    "title": "PDE-LEARN: Using deep learning to discover partial differential equations from noisy, limited data",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106242",
    "abstract": "In this paper, we introduce PDE-LEARN, a novel deep learning algorithm that can identify governing partial differential equations (PDEs) directly from noisy, limited measurements of a physical system of interest. PDE-LEARN uses a Rational Neural Network, U , to approximate the system response function and a sparse, trainable vector, ξ , to characterize the hidden PDE that the system response function satisfies. Our approach couples the training of U and ξ using a loss function that (1) makes U approximate the system response function, (2) encapsulates the fact that U satisfies a hidden PDE that ξ characterizes, and (3) promotes sparsity in ξ using ideas from iteratively reweighted least-squares. Further, PDE-LEARN can simultaneously learn from several data sets, allowing it to incorporate results from multiple experiments. This approach yields a robust algorithm to discover PDEs directly from realistic scientific data. We demonstrate the efficacy of PDE-LEARN by identifying several PDEs from noisy and limited measurements.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001667",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Deep learning",
      "Evolutionary biology",
      "Function (biology)",
      "Mathematical analysis",
      "Mathematics",
      "Noisy data",
      "Partial differential equation"
    ],
    "authors": [
      {
        "surname": "Stephany",
        "given_name": "Robert"
      },
      {
        "surname": "Earls",
        "given_name": "Christopher"
      }
    ]
  },
  {
    "title": "MuLAN: Multi-level attention-enhanced matching network for few-shot knowledge graph completion",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106222",
    "abstract": "Recent years have witnessed increasing interest in the few-shot knowledge graph completion due to its potential to augment the coverage of few-shot relations in knowledge graphs. Existing methods often use the one-hop neighbors of the entity to enhance its embedding and match the query instance and support set at the instance level. However, such methods cannot handle inter-neighbor interaction, local entity matching and the varying significance of feature dimensions. To bridge this gap, we propose the Multi-Level Attention-enhanced matching Network (MuLAN) for few-shot knowledge graph completion. In MuLAN, a multi-head self-attention neighbor encoder is designed to capture the inter-neighbor interaction and learn the entity embeddings. Then, entity-level attention and instance-level attention are responsible for matching the query instance and support set from the local and global perspectives, respectively, while feature-level attention is utilized to calculate the weights of the feature dimensions. Furthermore, we design a consistency constraint to ensure the support instance embeddings are close to each other. Extensive experiments based on two well-known datasets (i.e., NELL-One and Wiki-One) demonstrate significant advantages of MuLAN over 11 state-of-the-art competitors. Compared to the best-performing baseline, MuLAN achieves 14.5% higher MRR and 13.3% higher Hits@K on average.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001461",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Embedding",
      "Encoder",
      "Feature (linguistics)",
      "Graph",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Qianyu"
      },
      {
        "surname": "Feng",
        "given_name": "Bozheng"
      },
      {
        "surname": "Tang",
        "given_name": "Xiaoli"
      },
      {
        "surname": "Yu",
        "given_name": "Han"
      },
      {
        "surname": "Song",
        "given_name": "Hengjie"
      }
    ]
  },
  {
    "title": "Modeling failures in smart grids by a bilinear logistic regression approach",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106245",
    "abstract": "Modeling and recognizing events in complex systems through machine learning techniques is a challenging task. Especially if the model is constrained to be explainable and interpretable, while ensuring high levels of accuracy. In this paper, we adopt a bilinear logistic regression model in which the parameters are trained in a data-driven fashion on a real-world dataset of power grid failure data. The bilinear white-box model – grounded on a specific neural architecture – has been proven effective in classifying faulty states with a performance comparable to several classifiers in technical literature. Additionally, the low computational complexity of the bilinear model, in terms of the number of free parameters, allows gaining insights into the fault phenomenon correlating the events that impact the power grid (exogenous causes) with its constitutive characteristics, thence eliciting the relational information hidden in the data. The proposed model is also able to estimate a vulnerability vector that can be associated, as a suitable characteristic “label”, to power grid components, opening the way, as will be deeply demonstrated in the following, not only to predictive maintenance programs or condition monitoring tasks but also to risk assessment and scenario analyses in line with the explainable AI paradigm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001692",
    "keywords": [
      "Artificial intelligence",
      "Bilinear interpolation",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Data mining",
      "Electrical engineering",
      "Engineering",
      "Fault (geology)",
      "Geology",
      "Geometry",
      "Grid",
      "Logistic regression",
      "Machine learning",
      "Mathematics",
      "Seismology",
      "Smart grid",
      "Support vector machine",
      "Systems engineering",
      "Task (project management)",
      "Vulnerability (computing)"
    ],
    "authors": [
      {
        "surname": "De Santis",
        "given_name": "Enrico"
      },
      {
        "surname": "Rizzi",
        "given_name": "Antonello"
      }
    ]
  },
  {
    "title": "FE-Net: Feature enhancement segmentation network",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106232",
    "abstract": "Semantic segmentation is one of the directions in image research. It aims to obtain the contours of objects of interest, facilitating subsequent engineering tasks such as measurement and feature selection. However, existing segmentation methods still lack precision in class edge, particularly in multi-class mixed region. To this end, we present the Feature Enhancement Network (FE-Net), a novel approach that leverages edge label and pixel-wise weights to enhance segmentation performance in complex backgrounds. Firstly, we propose a Smart Edge Head (SE-Head) to process shallow-level information from the backbone network. It is combined with the FCN-Head and SepASPP-Head, located at deeper layers, to form a transitional structure where the loss weights gradually transition from edge labels to semantic labels and a mixed loss is also designed to support this structure. Additionally, we propose a pixel-wise weight evaluation method, a pixel-wise weight block, and a feature enhancement loss to improve training effectiveness in multi-class regions. FE-Net achieves significant performance improvements over baselines on publicly datasets Pascal VOC2012, SBD, and ATR, with best mIoU enhancements of 15.19%, 1.42% and 3.51%, respectively. Furthermore, experiments conducted on Pole&Hole match dataset from our laboratory environment demonstrate the superior effectiveness of FE-Net in segmenting defined key pixels.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001564",
    "keywords": [
      "Artificial intelligence",
      "Backbone network",
      "Block (permutation group theory)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Enhanced Data Rates for GSM Evolution",
      "Feature (linguistics)",
      "Geometry",
      "Image segmentation",
      "Linguistics",
      "Mathematics",
      "Net (polyhedron)",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Programming language",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Zhangyan"
      },
      {
        "surname": "Chen",
        "given_name": "Xiaoming"
      },
      {
        "surname": "Cao",
        "given_name": "Jingjing"
      },
      {
        "surname": "Zhao",
        "given_name": "Qiangwei"
      },
      {
        "surname": "Liu",
        "given_name": "Wenxi"
      }
    ]
  },
  {
    "title": "CTF-former: A novel simplified multi-task learning strategy for simultaneous multivariate chaotic time series prediction",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106234",
    "abstract": "Multivariate chaotic time series prediction is a challenging task, especially when multiple variables are predicted simultaneously. For multiple related prediction tasks typically require multiple models, however, multiple models are difficult to keep synchronization, making immediate communication between predicted values challenging. Although multi-task learning can be applied to this problem, the principles of allocation and layout options between shared and specific representations are ambiguous. To address this issue, a novel simplified multi-task learning method was proposed for the precise implementation of simultaneous multiple chaotic time series prediction tasks. The scheme proposed consists of a cross-convolution operator designed to capture variable correlations and sequence correlations, and an attention module proposed to capture the information embedded in the sequence structure. In the attention module, a non-linear transformation was implemented with convolution, and its local receptive field and the global dependency of the attention mechanism achieve complementarity. In addition, an attention weight calculation was devised that takes into account not only the synergy of time and frequency domain features, but also the fusion of series and channel information. Notably the scheme proposed a purely simplified design principle of multi-task learning by reducing the specific network to single neuron. The precision of the proposed solution and its potential for engineering applications were verified with the Lorenz system and power consumption. The mean absolute error of the proposed method was reduced by an average of 82.9% in the Lorenz system and 19.83% in power consumption compared to the Gated Recurrent Unit.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001588",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chaotic",
      "Chemistry",
      "Computer science",
      "Gene",
      "Lorenz system",
      "Machine learning",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Fu",
        "given_name": "Ke"
      },
      {
        "surname": "Li",
        "given_name": "He"
      },
      {
        "surname": "Shi",
        "given_name": "Xiaotian"
      }
    ]
  },
  {
    "title": "Geometry-driven self-supervision for 3D human pose estimation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106237",
    "abstract": "Although 3D human pose estimation has recently made strides, it is still difficult to precisely recreate a 3D human posture from a single image without the aid of 3D annotation for the following reasons. Firstly, the process of reconstruction inherently suffers from ambiguity, as multiple 3D poses can be projected onto the same 2D pose. Secondly, accurately measuring camera rotation without laborious camera calibration is a difficult task. While some approaches attempt to address these issues using traditional computer vision algorithms, they are not differentiable and cannot be optimized through training. This paper introduces two modules that explicitly leverage geometry to overcome these challenges, without requiring any 3D ground-truth or camera parameters. The first module, known as the relative depth estimation module, effectively mitigates depth ambiguity by narrowing down the possible depths for each joint to only two candidates. The second module, referred to as the differentiable pose alignment module, calculates camera rotation by aligning poses from different views. The use of these geometrically interpretable modules reduces the complexity of training and yields superior performance. By adopting our proposed method, we achieve state-of-the-art results on standard benchmark datasets, surpassing other self-supervised methods and even outperforming several fully-supervised approaches that heavily rely on 3D annotations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001618",
    "keywords": [
      "3D pose estimation",
      "Ambiguity",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Camera resectioning",
      "Computer science",
      "Computer vision",
      "Differentiable function",
      "Economics",
      "Geodesy",
      "Geography",
      "Ground truth",
      "Leverage (statistics)",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Pose",
      "Process (computing)",
      "Programming language",
      "Rotation (mathematics)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Geon-Jun"
      },
      {
        "surname": "Kim",
        "given_name": "Jun-Hee"
      },
      {
        "surname": "Lee",
        "given_name": "Seong-Whan"
      }
    ]
  },
  {
    "title": "Cosine convolutional neural network and its application for seizure detection",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106267",
    "abstract": "Traditional convolutional neural networks (CNNs) often suffer from high memory consumption and redundancy in their kernel representations, leading to overfitting problems and limiting their application in real-time, low-power scenarios such as seizure detection systems. In this work, a novel cosine convolutional neural network (CosCNN), which replaces traditional kernels with the robust cosine kernel modulated by only two learnable factors, is presented, and its effectiveness is validated on the tasks of seizure detection. Meanwhile, based on the cosine lookup table and KL-divergence, an effective post-training quantization algorithm is proposed for CosCNN hardware implementation. With quantization, CosCNN can achieve a nearly 75% reduction in the memory cost with almost no accuracy loss. Moreover, we design a configurable cosine convolution accelerator on Field Programmable Gate Array (FPGA) and deploy the quantized CosCNN on Zedboard, proving the proposed seizure detection system can operate in real-time and low-power scenarios. Extensive experiments and comparisons were conducted using two publicly available epileptic EEG databases, the Bonn database and the CHB-MIT database. The results highlight the performance superiority of the CosCNN over traditional CNNs as well as other seizure detection methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001916",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Convolutional neural network",
      "Discrete cosine transform",
      "Embedded system",
      "Field-programmable gate array",
      "Image (mathematics)",
      "Kernel (algebra)",
      "Lookup table",
      "Mathematics",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Programming language",
      "Quantization (signal processing)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Guoyang"
      },
      {
        "surname": "Tian",
        "given_name": "Lan"
      },
      {
        "surname": "Wen",
        "given_name": "Yiming"
      },
      {
        "surname": "Yu",
        "given_name": "Weize"
      },
      {
        "surname": "Zhou",
        "given_name": "Weidong"
      }
    ]
  },
  {
    "title": "Attributed Multi-Order Graph Convolutional Network for Heterogeneous Graphs",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106225",
    "abstract": "Heterogeneous graph neural networks play a crucial role in discovering discriminative node embeddings and relations from multi-relational networks. One of the key challenges in heterogeneous graph learning lies in designing learnable meta-paths, which significantly impact the quality of learned embeddings. In this paper, we propose an Attributed Multi-Order Graph Convolutional Network (AMOGCN), which automatically explores meta-paths that involve multi-hop neighbors by aggregating multi-order adjacency matrices. The proposed model first constructs different orders of adjacency matrices from manually designed node connections. Next, AMOGCN fuses these various orders of adjacency matrices to create an intact multi-order adjacency matrix. This process is supervised by the node semantic information, which is extracted from the node homophily evaluated by attributes. Eventually, we employ a one-layer simplifying graph convolutional network with the learned multi-order adjacency matrix, which is equivalent to the cross-hop node information propagation with multi-layer graph neural networks. Substantial experiments reveal that AMOGCN achieves superior semi-supervised classification performance compared with state-of-the-art competitors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001497",
    "keywords": [
      "Adjacency list",
      "Adjacency matrix",
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Discriminative model",
      "Engineering",
      "Graph",
      "Node (physics)",
      "Structural engineering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Zhaoliang"
      },
      {
        "surname": "Wu",
        "given_name": "Zhihao"
      },
      {
        "surname": "Zhong",
        "given_name": "Luying"
      },
      {
        "surname": "Plant",
        "given_name": "Claudia"
      },
      {
        "surname": "Wang",
        "given_name": "Shiping"
      },
      {
        "surname": "Guo",
        "given_name": "Wenzhong"
      }
    ]
  },
  {
    "title": "Goal-oriented inference of environment from redundant observations",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106246",
    "abstract": "The agent learns to organize decision behavior to achieve a behavioral goal, such as reward maximization, and reinforcement learning is often used for this optimization. Learning an optimal behavioral strategy is difficult under the uncertainty that events necessary for learning are only partially observable, called as Partially Observable Markov Decision Process (POMDP). However, the real-world environment also gives many events irrelevant to reward delivery and an optimal behavioral strategy. The conventional methods in POMDP, which attempt to infer transition rules among the entire observations, including irrelevant states, are ineffective in such an environment. Supposing Redundantly Observable Markov Decision Process (ROMDP), here we propose a method for goal-oriented reinforcement learning to efficiently learn state transition rules among reward-related “core states” from redundant observations. Starting with a small number of initial core states, our model gradually adds new core states to the transition diagram until it achieves an optimal behavioral strategy consistent with the Bellman equation. We demonstrate that the resultant inference model outperforms the conventional method for POMDP. We emphasize that our model only containing the core states has high explainability. Furthermore, the proposed method suits online learning as it suppresses memory consumption and improves learning speed.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001709",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Core (optical fiber)",
      "Inference",
      "Machine learning",
      "Markov chain",
      "Markov decision process",
      "Markov model",
      "Markov process",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Observable",
      "Operating system",
      "Partially observable Markov decision process",
      "Physics",
      "Process (computing)",
      "Quantum mechanics",
      "Reinforcement learning",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Takahashi",
        "given_name": "Kazuki"
      },
      {
        "surname": "Fukai",
        "given_name": "Tomoki"
      },
      {
        "surname": "Sakai",
        "given_name": "Yutaka"
      },
      {
        "surname": "Takekawa",
        "given_name": "Takashi"
      }
    ]
  },
  {
    "title": "Contrastive representation learning on dynamic networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106240",
    "abstract": "Representation learning for dynamic networks is designed to learn the low-dimensional embeddings of nodes that can well preserve the snapshot structure, properties and temporal evolution of dynamic networks. However, current dynamic network representation learning methods tend to focus on estimating or generating observed snapshot structures, paying excessive attention to network details, and disregarding distinctions between snapshots with larger time intervals, resulting in less robustness for sparse or noisy networks. To alleviate these challenges, this paper proposes a contrastive mechanism for temporal representation learning on dynamic networks, inspired by the success of contrastive learning in visual and static network representation learning. This paper proposes a novel Dynamic Network Contrastive representation Learning (DNCL) model. Specifically, contrast objective functions are constructed using intra-snapshot and inter-snapshot contrasts to capture the network topology, node feature information, and network evolution information, respectively. Rather than estimating or generating ground-truth network features, the proposed approach maximizes mutual information between nodes from different time steps and views generated. The experimental results of link prediction, node classification, and clustering on several real-world and synthetic networks demonstrate the superiority of DNCL over state-of-the-art methods, indicating the effectiveness of the proposed approach for dynamic network representation learning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001643",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Cluster analysis",
      "Computer network",
      "Computer science",
      "Dynamic network analysis",
      "Feature learning",
      "Gene",
      "Law",
      "Machine learning",
      "Operating system",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Robustness (evolution)",
      "Snapshot (computer storage)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Jiao",
        "given_name": "Pengfei"
      },
      {
        "surname": "Chen",
        "given_name": "Hongjiang"
      },
      {
        "surname": "Tang",
        "given_name": "Huijun"
      },
      {
        "surname": "Bao",
        "given_name": "Qing"
      },
      {
        "surname": "Zhang",
        "given_name": "Long"
      },
      {
        "surname": "Zhao",
        "given_name": "Zhidong"
      },
      {
        "surname": "Wu",
        "given_name": "Huaming"
      }
    ]
  },
  {
    "title": "ARPruning: An automatic channel pruning based on attention map ranking",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106220",
    "abstract": "Structured pruning is a representative model compression technology for convolutional neural networks (CNNs), aiming to prune some less important filters or channels of CNNs. Most recent structured pruning methods have established some criteria to measure the importance of filters, which are mainly based on the magnitude of weights or other parameters in CNNs. However, these judgment criteria lack explainability, and it is insufficient to simply rely on the numerical values of the network parameters to assess the relationship between the channel and the model performance. Moreover, directly utilizing these pruning criteria for global pruning may lead to suboptimal solutions, therefore, it is necessary to complement search algorithms to determine the pruning ratio for each layer. To address these issues, we propose ARPruning (Attention-map-based Ranking Pruning), which reconstructs a new pruning criterion as the importance of the intra-layer channels and further develops a new local neighborhood search algorithm for determining the optimal inter-layer pruning ratio. To measure the relationship between the channel to be pruned and the model performance, we construct an intra-layer channel importance criterion by considering the attention map for each layer. Then, we propose an automatic pruning strategy searching method that can search for the optimal solution effectively and efficiently. By integrating the well-designed pruning criteria and search strategy, our ARPruning can not only maintain a high compression rate but also achieve outstanding accuracy. In our work, it is also experimentally concluded that compared with state-of-the-art pruning methods, our ARPruning method is capable of achieving better compression results. The code can be obtained at https://github.com/dozingLee/ARPruning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001448",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Channel (broadcasting)",
      "Chemistry",
      "Complement (music)",
      "Complementation",
      "Computer network",
      "Computer science",
      "Convolutional neural network",
      "Gene",
      "Layer (electronics)",
      "Machine learning",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Phenotype",
      "Pruning",
      "Ranking (information retrieval)"
    ],
    "authors": [
      {
        "surname": "Yuan",
        "given_name": "Tongtong"
      },
      {
        "surname": "Li",
        "given_name": "Zulin"
      },
      {
        "surname": "Liu",
        "given_name": "Bo"
      },
      {
        "surname": "Tang",
        "given_name": "Yinan"
      },
      {
        "surname": "Liu",
        "given_name": "Yujia"
      }
    ]
  },
  {
    "title": "Structure-aware contrastive hashing for unsupervised cross-modal retrieval",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106211",
    "abstract": "Cross-modal hashing has attracted a lot of attention and achieved remarkable success in large-scale cross-media similarity retrieval applications because of its superior computational efficiency and low storage overhead. However, constructing similarity relationship among samples in cross-modal unsupervised hashing is challenging because of the lack of manual annotation. Most existing unsupervised methods directly use the representations extracted from the backbone of their respective modality to construct instance similarity matrices, leading to inaccurate similarity matrices and resulting in suboptimal hash codes. To address this issue, a novel unsupervised hashing model, named Structure-aware Contrastive Hashing for Unsupervised Cross-modal Retrieval (SACH), is proposed in this paper. Specifically, we concurrently employ both high-dimensional representations and discriminative representations learned by the network to construct a more informative semantic correlative matrix across modalities. Moreover, we design a multimodal structure-aware alignment network to minimize heterogeneous gap in the high-order semantic space of each modality, effectively reducing disparities within heterogeneous data sources and enhancing the consistency of semantic information across modalities. Extensive experimental results on two widely utilized datasets demonstrate the superiority of our proposed SACH method in cross-modal retrieval tasks over existing state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001357",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Hash function",
      "Modal",
      "Natural language processing",
      "Pattern recognition (psychology)",
      "Polymer chemistry"
    ],
    "authors": [
      {
        "surname": "Cui",
        "given_name": "Jinrong"
      },
      {
        "surname": "He",
        "given_name": "Zhipeng"
      },
      {
        "surname": "Huang",
        "given_name": "Qiong"
      },
      {
        "surname": "Fu",
        "given_name": "Yulu"
      },
      {
        "surname": "Li",
        "given_name": "Yuting"
      },
      {
        "surname": "Wen",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "Tolerant Self-Distillation for image classification",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106215",
    "abstract": "Deep neural networks tend to suffer from the overfitting issue when the training data are not enough. In this paper, we introduce two metrics from the intra-class distribution of correct-predicted and incorrect-predicted samples to provide a new perspective on the overfitting issue. Based on it, we propose a knowledge distillation approach without pretraining a teacher model in advance named Tolerant Self-Distillation (TSD) for alleviating the overfitting issue. It introduces an online updating memory and selectively stores the class predictions of the samples from the past iterations, making it possible to distill knowledge across the iterations. Specifically, the class predictions stored in the memory bank serve as the soft labels for supervising the samples from the same class for the current iteration in a reverse way, i.e. the correct-predicted samples are supervised with the incorrect predictions while the incorrect-predicted samples are supervised with the correct predictions. Consequently, the premature convergence issue caused by the over-confident samples would be mitigated, which helps the model to converge to a better local optimum. Extensive experimental results on several image classification benchmarks, including small-scale, large-scale, and fine-grained datasets, demonstrate the superiority of the proposed TSD.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001394",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Class (philosophy)",
      "Computer science",
      "Convergence (economics)",
      "Distillation",
      "Economic growth",
      "Economics",
      "Image (mathematics)",
      "Machine learning",
      "Organic chemistry",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Mushui"
      },
      {
        "surname": "Yu",
        "given_name": "Yunlong"
      },
      {
        "surname": "Ji",
        "given_name": "Zhong"
      },
      {
        "surname": "Han",
        "given_name": "Jungong"
      },
      {
        "surname": "Zhang",
        "given_name": "Zhongfei"
      }
    ]
  },
  {
    "title": "RoMAT: Role-based multi-agent transformer for generalizable heterogeneous cooperation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106129",
    "abstract": "Multi-task multi-agent systems (MASs) are challenging to model because they involve heterogeneous agents with different behavior patterns that need to cooperate across various tasks. Existing networks for single-agent policies are not suitable for this setting, as they cannot share policies among agents without losing task-specific performance. We propose a novel framework called Role-based Multi-Agent Transformer (RoMAT), which uses a sequence modeling technique and a role-based actor to enable agents to adapt to different tasks and roles in MASs. RoMAT has a modular model architecture, where backbone networks are shared by all agents, but a small part of the parameters (role-based actor) is independent, depending on the agents’ exclusive structures. We evaluate RoMAT on several benchmark tasks and show that it can capture the behavior patterns of heterogeneous agents and achieve better performance and generalization than other methods in both single and multi-task settings.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000455",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Distributed computing",
      "Electrical engineering",
      "Engineering",
      "Generalization",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Modular design",
      "Operating system",
      "Systems engineering",
      "Task (project management)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Dongzi"
      },
      {
        "surname": "Zhong",
        "given_name": "Fangwei"
      },
      {
        "surname": "Li",
        "given_name": "Minglong"
      },
      {
        "surname": "Wen",
        "given_name": "Muning"
      },
      {
        "surname": "Peng",
        "given_name": "Yuanxi"
      },
      {
        "surname": "Li",
        "given_name": "Teng"
      },
      {
        "surname": "Yang",
        "given_name": "Adam"
      }
    ]
  },
  {
    "title": "Adversarial pair-wise distribution matching for remote sensing image cross-scene classification",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106241",
    "abstract": "Remarkable achievements have been made in the field of remote sensing cross-scene classification in recent years. However, most methods directly align the entire image features for cross-scene knowledge transfer. They usually ignore the high background complexity and low category consistency of remote sensing images, which can significantly impair the performance of distribution alignment. Besides, shortcomings of the adversarial training paradigm and the inability to guarantee the prediction discriminability and diversity can also hinder cross-scene classification performance. To alleviate the above problems, we propose a novel cross-scene classification framework in a discriminator-free adversarial paradigm, called Adversarial Pair-wise Distribution Matching (APDM), to avoid irrelevant knowledge transfer and enable effective cross-domain modeling. Specifically, we propose the pair-wise cosine discrepancy for both inter-domain and intra-domain prediction measurements to fully leverage the prediction information, which can suppress negative semantic features and implicitly align the cross-scene distributions. Nuclear-norm maximization and minimization are introduced to enhance the target prediction quality and increase the applicability of the source knowledge, respectively. As a general cross-scene framework, APDM can be easily embedded with existing methods to boost the performance. Experimental results and analyses demonstrate that APDM can achieve competitive and effective performance on cross-scene classification tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001655",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Contextual image classification",
      "Detector",
      "Discriminator",
      "Image (mathematics)",
      "Leverage (statistics)",
      "Machine learning",
      "Matching (statistics)",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Pattern recognition (psychology)",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zhu",
        "given_name": "Sihan"
      },
      {
        "surname": "Wu",
        "given_name": "Chen"
      },
      {
        "surname": "Du",
        "given_name": "Bo"
      },
      {
        "surname": "Zhang",
        "given_name": "Liangpei"
      }
    ]
  },
  {
    "title": "Exploring sparsity in graph transformers",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106265",
    "abstract": "Graph Transformers (GTs) have achieved impressive results on various graph-related tasks. However, the huge computational cost of GTs hinders their deployment and application, especially in resource-constrained environments. Therefore, in this paper, we explore the feasibility of sparsifying GTs, a significant yet under-explored topic. We first discuss the redundancy of GTs based on the characteristics of existing GT models, and then propose a comprehensive Graph Transformer SParsification (GTSP) framework that helps to reduce the computational complexity of GTs from four dimensions: the input graph data, attention heads, model layers, and model weights. Specifically, GTSP designs differentiable masks for each individual compressible component, enabling effective end-to-end pruning. We examine our GTSP through extensive experiments on prominent GTs, including GraphTrans, Graphormer, and GraphGPS. The experimental results demonstrate that GTSP effectively reduces computational costs, with only marginal decreases in accuracy or, in some instances, even improvements. For example, GTSP results in a 30% reduction in Floating Point Operations while contributing to a 1.8% increase in Area Under the Curve accuracy on the OGBG-HIV dataset. Furthermore, we provide several insights on the characteristics of attention heads and the behavior of attention mechanisms, all of which have immense potential to inspire future research endeavors in this domain. Our code is available at https://github.com/LiuChuang0059/GTSP.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001898",
    "keywords": [
      "Computer science",
      "Differentiable function",
      "Distributed computing",
      "Graph",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Physics",
      "Quantum mechanics",
      "Redundancy (engineering)",
      "Software deployment",
      "Theoretical computer science",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Chuang"
      },
      {
        "surname": "Zhan",
        "given_name": "Yibing"
      },
      {
        "surname": "Ma",
        "given_name": "Xueqi"
      },
      {
        "surname": "Ding",
        "given_name": "Liang"
      },
      {
        "surname": "Tao",
        "given_name": "Dapeng"
      },
      {
        "surname": "Wu",
        "given_name": "Jia"
      },
      {
        "surname": "Hu",
        "given_name": "Wenbin"
      },
      {
        "surname": "Du",
        "given_name": "Bo"
      }
    ]
  },
  {
    "title": "Efficient learning of Scale-Adaptive Nearly Affine Invariant Networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106229",
    "abstract": "Recent research has demonstrated the significance of incorporating invariance into neural networks. However, existing methods require direct sampling over the entire transformation set, notably computationally taxing for large groups like the affine group. In this study, we propose a more efficient approach by addressing the invariances of the subgroups within a larger group. For tackling affine invariance, we split it into the Euclidean group E ( n ) and uni-axial scaling group U S ( n ) , handling invariance individually. We employ an E ( n ) -invariant model for E ( n ) -invariance and average model outputs over data augmented from a U S ( n ) distribution for U S ( n ) -invariance. Our method maintains a favorable computational complexity of O ( N 2 ) in 2D and O ( N 4 ) in 3D scenarios, in contrast to the O ( N 6 ) (2D) and O ( N 12 ) (3D) complexities of averaged models. Crucially, the scale range for augmentation adapts during training to avoid excessive scale invariance. This is the first time nearly exact affine invariance is incorporated into neural networks without directly sampling the entire group. Extensive experiments unequivocally confirm its superiority, achieving new state-of-the-art results in affNIST and SIM2MNIST classifications while consuming less than 15% of inference time and fewer computational resources and model parameters compared to averaged models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001539",
    "keywords": [
      "Affine transformation",
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Composite material",
      "Computer science",
      "Euclidean geometry",
      "Gene",
      "Geometry",
      "Inference",
      "Invariant (physics)",
      "Materials science",
      "Mathematical physics",
      "Mathematics",
      "Pure mathematics",
      "Range (aeronautics)",
      "Scale invariance",
      "Scaling",
      "Statistics",
      "Transformation (genetics)",
      "Transformation group"
    ],
    "authors": [
      {
        "surname": "Shen",
        "given_name": "Zhengyang"
      },
      {
        "surname": "Qiu",
        "given_name": "Yeqing"
      },
      {
        "surname": "Liu",
        "given_name": "Jialun"
      },
      {
        "surname": "He",
        "given_name": "Lingshen"
      },
      {
        "surname": "Lin",
        "given_name": "Zhouchen"
      }
    ]
  },
  {
    "title": "Low dimensional approximation and generalization of multivariate functions on smooth manifolds using deep ReLU neural networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106223",
    "abstract": "The expressive power of deep neural networks is manifested by their remarkable ability to approximate multivariate functions in a way that appears to overcome the curse of dimensionality. This ability is exemplified by their success in solving high-dimensional problems where traditional numerical solvers fail due to their limitations in accurately representing high-dimensional structures. To provide a theoretical framework for explaining this phenomenon, we analyze the approximation of Hölder functions defined on a d -dimensional smooth manifold M embedded in R D , with d ≪ D , using deep neural networks. We prove that the uniform convergence estimates of the approximation and generalization errors by deep neural networks with ReLU activation functions do not depend on the ambient dimension D of the function but only on its lower manifold dimension d , in a precise sense. Our result improves existing results from the literature where approximation and generalization errors were shown to depend weakly on D .",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001473",
    "keywords": [
      "Activation function",
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Convergence (economics)",
      "Curse of dimensionality",
      "Deep learning",
      "Deep neural networks",
      "Dimension (graph theory)",
      "Economic growth",
      "Economics",
      "Engineering",
      "Evolutionary biology",
      "Function (biology)",
      "Function approximation",
      "Generalization",
      "Machine learning",
      "Manifold (fluid mechanics)",
      "Mathematical analysis",
      "Mathematics",
      "Mechanical engineering",
      "Multivariate statistics",
      "Pure mathematics"
    ],
    "authors": [
      {
        "surname": "Labate",
        "given_name": "Demetrio"
      },
      {
        "surname": "Shi",
        "given_name": "Ji"
      }
    ]
  },
  {
    "title": "Attention-based investigation and solution to the trade-off issue of adversarial training",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106224",
    "abstract": "Adversarial training has become the mainstream method to boost adversarial robustness of deep models. However, it often suffers from the trade-off dilemma, where the use of adversarial examples hurts the standard generalization of models on natural data. To study this phenomenon, we investigate it from the perspective of spatial attention. In brief, standard training typically encourages a model to conduct a comprehensive check to input space. But adversarial training often causes a model to overly concentrate on sparse spatial regions. This reduced tendency is beneficial to avoid adversarial accumulation but easily makes the model ignore abundant discriminative information, thereby resulting in weak generalization. To address this issue, this paper introduces an Attention-Enhanced Learning Framework (AELF) for robustness training. The main idea is to enable the model to inherit the attention pattern of standard pre-trained model through an embedding-level regularization. To be specific, given a teacher model built on natural examples, the embedding distribution of teacher model is used as a static constraint to regulate the embedding outputs of the objective model. This design is mainly supported with that the embedding feature of standard model is usually recognized as a rich semantic integration of input. For implementation, we present a simplified AELFs that can achieve the regularization with single cross entropy loss via the parameter initialization and parameter update strategy. This avoids the extra consistency comparison operation between embedding vectors. Experimental observations verify the rationality of our argument, and experimental results demonstrate that it can achieve remarkable improvements in generalization under the high-level robustness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001485",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Discriminative model",
      "Embedding",
      "Gene",
      "Initialization",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Programming language",
      "Regularization (linguistics)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Shao",
        "given_name": "Changbin"
      },
      {
        "surname": "Li",
        "given_name": "Wenbin"
      },
      {
        "surname": "Huo",
        "given_name": "Jing"
      },
      {
        "surname": "Feng",
        "given_name": "Zhenhua"
      },
      {
        "surname": "Gao",
        "given_name": "Yang"
      }
    ]
  },
  {
    "title": "BAGAIL: Multi-modal imitation learning from imbalanced demonstrations",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106251",
    "abstract": "Expert demonstrations in imitation learning often contain different behavioral modes, e.g., driving modes such as driving on the left, keeping the lane, and driving on the right in the driving tasks. Although most existing multi-modal imitation learning methods allow learning from demonstrations of multiple modes, they have strict constraints on the data of each mode, generally requiring a near data ratio of all modes. Otherwise, it tends to fall into a mode collapse or only learn the data distribution of the mode that has the largest data volume. To address the problem, an algorithm that balances real-fake loss and classification loss by modifying the output of the discriminator, referred to as BAlanced Generative Adversarial Imitation Learning (BAGAIL), is proposed. With this modification, the generator is only rewarded for generating real trajectories with correct modes. BAGAIL is therefore able to deal with imbalanced expert demonstrations and carry out efficient learning for each mode. The learning process of BAGAIL is divided into a pre-training stage and an imitation learning stage. During the pre-training stage, BAGAIL initializes the generator parameters by means of conditional Behavioral Cloning, laying the foundation for the direction of parameter optimization. During the imitation learning stage, BAGAIL optimizes the parameters by using the adversary between the generator and the modified discriminator so that the finally obtained policy can successfully learn the distribution of imbalanced expert data. The experiments showed that BAGAIL accurately distinguished different behavioral modes with imbalanced demonstrations. What is more, the learning result of each mode is close to the expert standard and more stable than other multi-modal imitation learning methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001758",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Detector",
      "Discriminator",
      "Generator (circuit theory)",
      "Human–computer interaction",
      "Imitation",
      "Machine learning",
      "Modal",
      "Mode (computer interface)",
      "Operating system",
      "Physics",
      "Polymer chemistry",
      "Power (physics)",
      "Process (computing)",
      "Psychology",
      "Quantum mechanics",
      "Social psychology",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Gu",
        "given_name": "Sijia"
      },
      {
        "surname": "Zhu",
        "given_name": "Fei"
      }
    ]
  },
  {
    "title": "Set stabilization of logical control networks: A minimum node control approach",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106266",
    "abstract": "In network systems, control using minimum nodes or pinning control can be effectively used for stabilization problems to cut down the cost of control. In this paper, we investigate the set stabilization problem of logical control networks. In particular, we study the set stabilization problem of probabilistic Boolean networks (PBNs) and probabilistic Boolean control networks (PBCNs) via controlling minimal nodes. Firstly, an algorithm is given to search for the minimum index set of pinning nodes. Then, based on the analysis of its high computational complexity, we present optimized algorithms with lower computational complexity to ascertain the network control using minimum node sets. Moreover, some sufficient and necessary conditions are proposed to ensure the feasibility and effectiveness of the proposed algorithms. Furthermore, a theorem is presented for PBCNs to devise all state-feedback controllers corresponding to the set of pinning nodes. Finally, two models of gene regulatory networks are considered to show the efficacy of obtained results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001904",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computational complexity theory",
      "Computer science",
      "Control (management)",
      "Engineering",
      "Mathematical optimization",
      "Mathematics",
      "Node (physics)",
      "Probabilistic logic",
      "Programming language",
      "Set (abstract data type)",
      "Structural engineering"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Jiayang"
      },
      {
        "surname": "Wang",
        "given_name": "Lina"
      },
      {
        "surname": "Yerudkar",
        "given_name": "Amol"
      },
      {
        "surname": "Liu",
        "given_name": "Yang"
      }
    ]
  },
  {
    "title": "A universal ANN-to-SNN framework for achieving high accuracy and low latency deep Spiking Neural Networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106244",
    "abstract": "Spiking Neural Networks (SNNs) have become one of the most prominent next-generation computational models owing to their biological plausibility, low power consumption, and the potential for neuromorphic hardware implementation. Among the various methods for obtaining available SNNs, converting Artificial Neural Networks (ANNs) into SNNs is the most cost-effective approach. The early challenges in ANN-to-SNN conversion work revolved around the susceptibility of converted SNNs to conversion errors. Some recent endeavors have attempted to mitigate these conversion errors by altering the original ANNs. Despite their ability to enhance the accuracy of SNNs, these methods lack generality and cannot be directly applied to convert the majority of existing ANNs. In this paper, we present a framework named DNISNM for converting ANN to SNN, with the aim of addressing conversion errors arising from differences in the discreteness and asynchrony of network transmission between ANN and SNN. The DNISNM consists of two mechanisms, Data-based Neuronal Initialization (DNI) and Signed Neuron with Memory (SNM), designed to respectively address errors stemming from discreteness and asynchrony disparities. This framework requires no additional modifications to the original ANN and can result in SNNs with improved accuracy performance, simultaneously ensuring universality, high precision, and low inference latency. We verify it experimentally on challenging object recognition datasets, including CIFAR10, CIFAR100, and ImageNet-1k. Experimental results show that the SNN converted by our framework has very high accuracy even at extremely low latency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001680",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Initialization",
      "Machine learning",
      "Neuromorphic engineering",
      "Pattern recognition (psychology)",
      "Programming language",
      "Spiking neural network"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yuchen"
      },
      {
        "surname": "Liu",
        "given_name": "Hanwen"
      },
      {
        "surname": "Zhang",
        "given_name": "Malu"
      },
      {
        "surname": "Luo",
        "given_name": "Xiaoling"
      },
      {
        "surname": "Qu",
        "given_name": "Hong"
      }
    ]
  },
  {
    "title": "Functional loops: Monitoring functional organization of deep neural networks using algebraic topology",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106239",
    "abstract": "Various topological methods have emerged in recent years to investigate the inner workings of deep neural networks (DNNs) based on the structural and weight information. However, their effectiveness is restricted due to the stratified structure and volatile weight information. In this study, we explore the relationship between functional organizations and network performance using algebraic topology. Our results indicate that functional loops reveal functional interaction patterns of multiple neurons in DNNs. We also propose functional persistence as a measure of functional complexity and develop an early stopping criterion that achieves competitive results without requiring a validation set.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001631",
    "keywords": [
      "Algebraic number",
      "Algebraic structure",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Combinatorics",
      "Computer science",
      "Ecology",
      "Functional connectivity",
      "Functional diversity",
      "Mathematical analysis",
      "Mathematics",
      "Neuroscience",
      "Programming language",
      "Pure mathematics",
      "Set (abstract data type)",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Ben"
      },
      {
        "surname": "Lin",
        "given_name": "Hongwei"
      }
    ]
  },
  {
    "title": "A universal ANN-to-SNN framework for achieving high accuracy and low latency deep Spiking Neural Networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106244",
    "abstract": "Spiking Neural Networks (SNNs) have become one of the most prominent next-generation computational models owing to their biological plausibility, low power consumption, and the potential for neuromorphic hardware implementation. Among the various methods for obtaining available SNNs, converting Artificial Neural Networks (ANNs) into SNNs is the most cost-effective approach. The early challenges in ANN-to-SNN conversion work revolved around the susceptibility of converted SNNs to conversion errors. Some recent endeavors have attempted to mitigate these conversion errors by altering the original ANNs. Despite their ability to enhance the accuracy of SNNs, these methods lack generality and cannot be directly applied to convert the majority of existing ANNs. In this paper, we present a framework named DNISNM for converting ANN to SNN, with the aim of addressing conversion errors arising from differences in the discreteness and asynchrony of network transmission between ANN and SNN. The DNISNM consists of two mechanisms, Data-based Neuronal Initialization (DNI) and Signed Neuron with Memory (SNM), designed to respectively address errors stemming from discreteness and asynchrony disparities. This framework requires no additional modifications to the original ANN and can result in SNNs with improved accuracy performance, simultaneously ensuring universality, high precision, and low inference latency. We verify it experimentally on challenging object recognition datasets, including CIFAR10, CIFAR100, and ImageNet-1k. Experimental results show that the SNN converted by our framework has very high accuracy even at extremely low latency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001680",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Initialization",
      "Machine learning",
      "Neuromorphic engineering",
      "Pattern recognition (psychology)",
      "Programming language",
      "Spiking neural network"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yuchen"
      },
      {
        "surname": "Liu",
        "given_name": "Hanwen"
      },
      {
        "surname": "Zhang",
        "given_name": "Malu"
      },
      {
        "surname": "Luo",
        "given_name": "Xiaoling"
      },
      {
        "surname": "Qu",
        "given_name": "Hong"
      }
    ]
  },
  {
    "title": "Facilitating interaction between partial differential equation-based dynamics and unknown dynamics for regional wind speed prediction",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106233",
    "abstract": "Regional wind speed prediction is an important spatiotemporal prediction problem which is crucial for optimizing wind power utilization. Nevertheless, the complex dynamics of wind speed pose a formidable challenge to prediction tasks. The evolving dynamics of wind could be governed by underlying physical principles that can be described by partial differential equations (PDE). This study proposes a novel approach called PDE-assisted network (PaNet) for regional wind speed prediction. In PaNet, a new architecture is devised, incorporating both PDE-based dynamics (PDE dynamics) and unknown dynamics. Specifically, this architecture establishes interactions between the two dynamics, regulated by an inter-dynamics communication unit that controls interactions through attention gates. Additionally, recognizing the significance of the initial state for PDE dynamics, an adaptive frequency-gated unit is introduced to generate a suitable initial state for the PDE dynamics by selecting essential frequency components. To evaluate the predictive performance of PaNet, this study conducts comprehensive experiments on two real-world wind speed datasets. The experimental results indicated that the proposed method is superior to other baseline methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001576",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Dynamics (music)",
      "Electrical engineering",
      "Engineering",
      "Mathematical analysis",
      "Mathematics",
      "Meteorology",
      "Partial differential equation",
      "Physics",
      "Wind power",
      "Wind speed"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Shidong"
      },
      {
        "surname": "Zhang",
        "given_name": "Baoquan"
      },
      {
        "surname": "Li",
        "given_name": "Xutao"
      },
      {
        "surname": "Ye",
        "given_name": "Yunming"
      },
      {
        "surname": "Lin",
        "given_name": "Kenghong"
      }
    ]
  },
  {
    "title": "Multi-level feature fusion and joint refinement for simultaneous object pose estimation and camera localization",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106238",
    "abstract": "Object pose estimation and camera localization are critical in various applications. However, achieving algorithm universality, which refers to category-level pose estimation and scene-independent camera localization, presents challenges for both techniques. Although the two tasks keep close relationships due to spatial geometry constraints, different tasks require distinct feature extractions. This paper pays attention to a unified RGB-D based framework that simultaneously performs category-level object pose estimation and scene-independent camera localization. The framework consists of a pose estimation branch called SLO-ObjNet, a localization branch called SLO-LocNet, a pose confidence calculation process and object-level optimization. At the start, we obtain the initial camera and object results from SLO-LocNet and SLO-ObjNet. In these two networks, we design there-level feature fusion modules as well as the loss function to achieve feature sharing between two tasks. Then the proposed approach involves a confidence calculation process to determine the accuracy of object poses obtained. Additionally, an object-level Bundle Adjustment (BA) optimization algorithm is further used to improve the precision of these techniques. The BA algorithm establishes relationships among feature points, objects, and cameras with the usage of camera-point, camera-object, and object-point metrics. To evaluate the performance of this approach, experiments are conducted on localization and pose estimation datasets including REAL275, CAMERA25, LineMOD, YCB-Video, 7 Scenes, ScanNet and TUM RGB-D. The results show that this approach outperforms existing methods in terms of both estimation and localization accuracy. Additionally, SLO-LocNet and SLO-ObjNet are trained on ScanNet data and tested on 7 Scenes and TUM RGB-D datasets to demonstrate its universality performance. Finally, we also highlight the positive effects of fusion modules, loss function, confidence process and BA for improving overall performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400162X",
    "keywords": [
      "3D pose estimation",
      "Artificial intelligence",
      "Bundle adjustment",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Linguistics",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pose",
      "RGB color model"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Junyi"
      },
      {
        "surname": "Qi",
        "given_name": "Yue"
      }
    ]
  },
  {
    "title": "FCPN: Pruning redundant part-whole relations for more streamlined pattern parsing",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106258",
    "abstract": "Cropping-and-segmenting pattern parsers often combine diverse inner correlations into a single metric/scheme, resulting in over-generalizations and redundant representations. It is proposed to streamline pattern parsing by using presenting a redundant association elimination network (RAEN) with capsule attention twisters (CATs) and capsule-attention routing agreement (CARA). CATs trim delicate relationships between parts and wholes that are weak and interchangeable. Senior entities can only be updated by primary entities that meet the requirements of inter-part diversity and intra-object cohesiveness. In order to enhance results, CARA is designed to protect against the unnecessary voting signals of traditional routing protocols. Experiments involving facial and human segmentation show that RAEN is better than current remarkable methods, particularly for defining detailed semantic boundaries.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001825",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Biology",
      "Computer network",
      "Computer science",
      "Machine learning",
      "Natural language processing",
      "Object (grammar)",
      "Parsing",
      "Pattern recognition (psychology)",
      "Pruning",
      "Routing (electronic design automation)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Zhongqi"
      },
      {
        "surname": "Xu",
        "given_name": "Linye"
      },
      {
        "surname": "Zheng",
        "given_name": "Zengwei"
      }
    ]
  },
  {
    "title": "Multi-tailed vision transformer for efficient inference",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106235",
    "abstract": "Recently, Vision Transformer (ViT) has achieved promising performance in image recognition and gradually serves as a powerful backbone in various vision tasks. To satisfy the sequential input of Transformer, the tail of ViT first splits each image into a sequence of visual tokens with a fixed length. Then, the following self-attention layers construct the global relationship between tokens to produce useful representation for the downstream tasks. Empirically, representing the image with more tokens leads to better performance, yet the quadratic computational complexity of self-attention layer to the number of tokens could seriously influence the efficiency of ViT’s inference. For computational reduction, a few pruning methods progressively prune uninformative tokens in the Transformer encoder, while leaving the number of tokens before the Transformer untouched. In fact, fewer tokens as the input for the Transformer encoder can directly reduce the following computational cost. In this spirit, we propose a Multi-Tailed Vision Transformer (MT-ViT) in the paper. MT-ViT adopts multiple tails to produce visual sequences of different lengths for the following Transformer encoder. A tail predictor is introduced to decide which tail is the most efficient for the image to produce accurate prediction. Both modules are optimized in an end-to-end fashion, with the Gumbel-Softmax trick. Experiments on ImageNet-1K demonstrate that MT-ViT can achieve a significant reduction on FLOPs with no degradation of the accuracy and outperform compared methods in both accuracy and FLOPs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400159X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computational complexity theory",
      "Computer science",
      "Encoder",
      "FLOPS",
      "Inference",
      "Operating system",
      "Parallel computing",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Softmax function",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yunke"
      },
      {
        "surname": "Du",
        "given_name": "Bo"
      },
      {
        "surname": "Wang",
        "given_name": "Wenyuan"
      },
      {
        "surname": "Xu",
        "given_name": "Chang"
      }
    ]
  },
  {
    "title": "Communication-efficient distributed cubic Newton with compressed lazy Hessian",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106212",
    "abstract": "Recently, second-order distributed optimization algorithms have been becoming a research hot in distributed learning, due to their faster convergence rate than the first-order algorithms. However, second-order algorithms always suffer from serious communication bottleneck. To conquer such challenge, we propose communication-efficient second-order distributed optimization algorithms in the parameter-server framework, by incorporating cubic Newton methods with compressed lazy Hessian. Specifically, our algorithms require each worker communicate compressed Hessians with the server only at some particular iterations, which can save both communication bits and communication rounds. For non-convex problems, we theoretically prove that our algorithms can reduce the communication cost comparing to the state-of-the-art second-order algorithms, while maintaining the same iteration complexity order O ( ϵ − 3 / 2 ) as the centralized cubic Newton methods. By further using gradient regularization technique, our algorithms can achieve global convergence for convex problems. Moreover, for strongly convex problems, our algorithms can achieve local superlinear convergence rate without any requirement on initial conditions. Finally, numerical experiments are conducted to show the high efficiency of the proposed algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001369",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Computer science",
      "Hessian matrix",
      "Mathematics",
      "Newton's method",
      "Nonlinear system",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zhen"
      },
      {
        "surname": "Che",
        "given_name": "Keqin"
      },
      {
        "surname": "Yang",
        "given_name": "Shaofu"
      },
      {
        "surname": "Xu",
        "given_name": "Wenying"
      }
    ]
  },
  {
    "title": "Effects of impulse on prescribed-time synchronization of switching complex networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106248",
    "abstract": "The specified convergence time, designated by the user, is highly attractive for many high-demand applications such as industrial robot control, missile guidance, and autonomous vehicles. For the application of neural networks in the field of secure communication and power systems, the importance of prescribed-time synchronization(PTs) and stable performance of the system is more prominent. This paper introduces a prescribed-time controller without the fractional power function and sign function, which can reach synchronization at a prescribed time and greatly reduce the chattering phenomenon of neural networks. Additionally, by constructing synchronizing/desynchronizing impulse sequences, the PTs of switching complex networks(SCN) is achieved with impulse effects, where the time sequences of switching and impulse occurrences in the networks are constrained by the average dwell time. This approach effectively reduces the impact of frequent mode switching on network synchronization, and the synchronization time can be flexibly adjusted within any physically allowable range to accommodate different application requirements. Finally, the effectiveness of the proposed control strategy is demonstrated by two examples.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001722",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Channel (broadcasting)",
      "Clinical psychology",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Dwell time",
      "Impulse (physics)",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Physics",
      "Quantum mechanics",
      "Real-time computing",
      "Sign function",
      "Synchronization (alternating current)",
      "Synchronizing",
      "Telecommunications",
      "Transmission (telecommunications)"
    ],
    "authors": [
      {
        "surname": "Tang",
        "given_name": "Qian"
      },
      {
        "surname": "Qu",
        "given_name": "Shaocheng"
      },
      {
        "surname": "Zhang",
        "given_name": "Chen"
      },
      {
        "surname": "Tu",
        "given_name": "Zhengwen"
      },
      {
        "surname": "Cao",
        "given_name": "Yuting"
      }
    ]
  },
  {
    "title": "Multi-view graph pooling with coarsened graph disentanglement",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106221",
    "abstract": "Multi-view graph pooling utilizes information from multiple perspectives to generate a coarsened graph, exhibiting superior performance in graph-level tasks. However, existing methods mainly focus on the types of multi-view information to improve graph pooling operations, lacking explicit control over the pooling process and theoretical analysis of the relationships between views. In this paper, we rethink the current paradigm of multi-view graph pooling from an information theory perspective, subsequently introducing GDMGP, an innovative method for multi-view graph pooling derived from the principles of graph disentanglement. This approach effectively simplifies the original graph into a more structured, disentangled coarsened graph, enhancing the clarity and utility of the graph representation. Our approach begins with the design of a novel view mapper that dynamically integrates the node and topology information of the original graph. This integration enhances its information sufficiency. Next, we introduce a view fusion mechanism based on conditional entropy to accurately regulate the task-relevant information in the views, aiming to minimize information loss in the pooling process. Finally, to further enhance the expressiveness of the coarsened graph, we disentangle the fused view into task-relevant and task-irrelevant subgraphs through mutual information minimization, retaining the task-relevant subgraph for downstream tasks. We theoretically demonstrate that the performance of the coarsened graph generated by our GDMGP is superior to that of any single input view. The effectiveness of GDMGP is further validated by experimental results on seven public datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400145X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Graph",
      "Graph property",
      "Line graph",
      "Pooling",
      "Theoretical computer science",
      "Voltage graph"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zidong"
      },
      {
        "surname": "Fan",
        "given_name": "Huilong"
      }
    ]
  },
  {
    "title": "On the approximation of bi-Lipschitz maps by invertible neural networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106214",
    "abstract": "Invertible neural networks (INNs) represent an important class of deep neural network architectures that have been widely used in applications. The universal approximation properties of INNs have been established recently. However, the approximation rate of INNs is largely missing. In this work, we provide an analysis of the capacity of a class of coupling-based INNs to approximate bi-Lipschitz continuous mappings on a compact domain, and the result shows that it can well approximate both forward and inverse maps simultaneously. Furthermore, we develop an approach for approximating bi-Lipschitz maps on infinite-dimensional spaces that simultaneously approximate the forward and inverse maps, by combining model reduction with principal component analysis and INNs for approximating the reduced map, and we analyze the overall approximation error of the approach. Preliminary numerical results show the feasibility of the approach for approximating the solution operator for parameterized second-order elliptic problems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001382",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Approximation error",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Geometry",
      "Inverse",
      "Invertible matrix",
      "Lipschitz continuity",
      "Mathematical analysis",
      "Mathematics",
      "Parameterized complexity",
      "Pure mathematics"
    ],
    "authors": [
      {
        "surname": "Jin",
        "given_name": "Bangti"
      },
      {
        "surname": "Zhou",
        "given_name": "Zehui"
      },
      {
        "surname": "Zou",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Multi-agent Continuous Control with Generative Flow Networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106243",
    "abstract": "Generative Flow Networks (GFlowNets) aim to generate diverse trajectories from a distribution in which the final states of the trajectories are proportional to the reward, serving as a powerful alternative to reinforcement learning for exploratory control tasks. However, the individual-flow matching constraint in GFlowNets limits their applications for multi-agent systems, especially continuous joint-control problems. In this paper, we propose a novel Multi-Agent generative Continuous Flow Networks (MACFN) method to enable multiple agents to perform cooperative exploration for various compositional continuous objects. Technically, MACFN trains decentralized individual-flow-based policies in a centralized global-flow-based matching fashion. During centralized training, MACFN introduces a continuous flow decomposition network to deduce the flow contributions of each agent in the presence of only global rewards. Then agents can deliver actions solely based on their assigned local flow in a decentralized way, forming a joint policy distribution proportional to the rewards. To guarantee the expressiveness of continuous flow decomposition, we theoretically derive a consistency condition on the decomposition network. Experimental results demonstrate that the proposed method yields results superior to the state-of-the-art counterparts and better exploration capability. Our code is available at https://github.com/isluoshuang/MACFN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001679",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Consistency (knowledge bases)",
      "Control (management)",
      "Decentralised system",
      "Decomposition",
      "Distributed computing",
      "Ecology",
      "Flow (mathematics)",
      "Generative grammar",
      "Generative model",
      "Geometry",
      "Matching (statistics)",
      "Mathematical optimization",
      "Mathematics",
      "Reinforcement learning",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Shuang"
      },
      {
        "surname": "Li",
        "given_name": "Yinchuan"
      },
      {
        "surname": "Liu",
        "given_name": "Shunyu"
      },
      {
        "surname": "Zhang",
        "given_name": "Xu"
      },
      {
        "surname": "Shao",
        "given_name": "Yunfeng"
      },
      {
        "surname": "Wu",
        "given_name": "Chao"
      }
    ]
  },
  {
    "title": "Source-free unsupervised domain adaptation: A survey",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106230",
    "abstract": "Unsupervised domain adaptation (UDA) via deep learning has attracted appealing attention for tackling domain-shift problems caused by distribution discrepancy across different domains. Existing UDA approaches highly depend on the accessibility of source domain data, which is usually limited in practical scenarios due to privacy protection, data storage and transmission cost, and computation burden. To tackle this issue, many source-free unsupervised domain adaptation (SFUDA) methods have been proposed recently, which perform knowledge transfer from a pre-trained source model to the unlabeled target domain with source data inaccessible. A comprehensive review of these works on SFUDA is of great significance. In this paper, we provide a timely and systematic literature review of existing SFUDA approaches from a technical perspective. Specifically, we categorize current SFUDA studies into two groups, i.e., white-box SFUDA and black-box SFUDA, and further divide them into finer subcategories based on different learning strategies they use. We also investigate the challenges of methods in each subcategory, discuss the advantages/disadvantages of white-box and black-box SFUDA methods, conclude the commonly used benchmark datasets, and summarize the popular techniques for improved generalizability of models learned without using source data. We finally discuss several promising future directions in this field.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001540",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Black box",
      "Categorization",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Data science",
      "Deep learning",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Field (mathematics)",
      "Generalizability theory",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Physics",
      "Pure mathematics",
      "Statistics",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Fang",
        "given_name": "Yuqi"
      },
      {
        "surname": "Yap",
        "given_name": "Pew-Thian"
      },
      {
        "surname": "Lin",
        "given_name": "Weili"
      },
      {
        "surname": "Zhu",
        "given_name": "Hongtu"
      },
      {
        "surname": "Liu",
        "given_name": "Mingxia"
      }
    ]
  },
  {
    "title": "A self-supervised network for image denoising and watermark removal",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106218",
    "abstract": "In image watermark removal, popular methods depend on given reference non-watermark images in a supervised way to remove watermarks. However, reference non-watermark images are difficult to be obtained in the real world. At the same time, they often suffer from the influence of noise when captured by digital devices. To resolve these issues, in this paper, we present a self-supervised network for image denoising and watermark removal (SSNet). SSNet uses a parallel network in a self-supervised learning way to remove noise and watermarks. Specifically, each sub-network contains two sub-blocks. The upper sub-network uses the first sub-block to remove noise, according to noise-to-noise. Then, the second sub-block in the upper sub-network is used to remove watermarks, according to the distributions of watermarks. To prevent the loss of important information, the lower sub-network is used to simultaneously learn noise and watermarks in a self-supervised learning way. Moreover, two sub-networks interact via attention to extract more complementary salient information. The proposed method does not depend on paired images to learn a blind denoising and watermark removal model, which is very meaningful for real applications. Also, it is more effective than the popular image watermark removal methods in public datasets. Codes can be found at https://github.com/hellloxiaotian/SSNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001424",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Image denoising",
      "Noise reduction",
      "Pattern recognition (psychology)",
      "Watermark"
    ],
    "authors": [
      {
        "surname": "Tian",
        "given_name": "Chunwei"
      },
      {
        "surname": "Xiao",
        "given_name": "Jingyu"
      },
      {
        "surname": "Zhang",
        "given_name": "Bob"
      },
      {
        "surname": "Zuo",
        "given_name": "Wangmeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Yudong"
      },
      {
        "surname": "Lin",
        "given_name": "Chia-Wen"
      }
    ]
  },
  {
    "title": "Finite-time guarantee-cost H ∞ consensus control of second-order multi-agent systems based on sampled-data event-triggered mechanisms",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106261",
    "abstract": "This study presents a solution to the challenges of tracking consensus and guarantee-cost H ∞ control in a specific set of second-order multi-agent systems with external disturbances. A proposed event-triggered control method based on periodic sampling data is presented for second-order multi-agent systems that include external disturbances. In contrast to the real-time monitoring of system state information used in the previous event-triggered mechanism, this approach collects system state information through periodic sampling. This ensures that the interval between two consecutive triggering moments is at least one sampling cycle, thereby preventing the controller from triggering infinitely within a finite time frame. A finite-time controller based on the sampled-data event-triggered mechanism is designed, and sufficient conditions to ensure the finite-time stability of the closed-loop system at a specified attenuation level are established using theoretical methods such as matrix analysis. For the given sampled-data event-triggered control protocol with a finite-time controller, a quadratic guarantee-cost function is introduced, and by designing control inputs and determining the parameters such as the finite-time upper bound T ∗ and the H ∞ performance index γ , the exact value of the upper bound of the system’s guarantee-cost function under the action of the designed controller is derived. Finally, the feasibility of the proposed control scheme is verified through numerical simulation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001850",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Combinatorics",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Controller (irrigation)",
      "Evolutionary biology",
      "Function (biology)",
      "Interval (graph theory)",
      "Mathematical analysis",
      "Mathematics",
      "Upper and lower bounds"
    ],
    "authors": [
      {
        "surname": "Yao",
        "given_name": "Yuejie"
      },
      {
        "surname": "Luo",
        "given_name": "Yiping"
      },
      {
        "surname": "Cao",
        "given_name": "Jinde"
      }
    ]
  },
  {
    "title": "An Inductive Reasoning Model based on Interpretable Logical Rules over temporal knowledge graph",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106219",
    "abstract": "Extrapolating future events based on historical information in temporal knowledge graphs (TKGs) holds significant research value and practical applications. In this field, the methods currently utilized can be classified as either embedding-based or logical rule-based. Embedding-based methods depend on learned entity and relation embeddings for prediction, but they suffer from the lack of interpretability due to the opaque reasoning process. On the other hand, logical rule-based methods face scalability challenges as they heavily rely on predefined logical rules. To overcome these limitations, we propose a hybrid model that combines embedding-based and logical rule-based methods to capture deep causal logic. Our model, called the Inductive Reasoning Model based on Interpretable Logical Rule (ILR-IR), aims to provide interpretable insights while effectively predicting future events in TKGs. ILR-IR delves into historical information, extracting valuable insights from logical rules embedded within relations and interaction preferences between entities. By considering both logical rules and interaction preferences, ILR-IR offers a comprehensive perspective for predicting future events. In addition, we propose the incorporation of a one-class augmented matching loss during optimization, which serves to enhance performance of the model during training. We evaluate ILR-IR on multiple datasets, including ICEWS14, ICEWS0515, and ICEWS18. Experimental results demonstrate that ILR-IR outperforms state-of-the-art baselines, showcasing its superior performance in TKG extrapolation reasoning. Moreover, ILR-IR demonstrates remarkable generalization capabilities, even when applied to related datasets that share a common relation vocabulary. This suggests that our proposed model exhibits robust zero-shot reasoning abilities. For interested parties, we have made our code publicly available at https://github.com/mxadorable/ILR-IR.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001436",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Database",
      "Embedding",
      "Interpretability",
      "Linguistics",
      "Logical consequence",
      "Machine learning",
      "Philosophy",
      "Premise",
      "Relation (database)",
      "Scalability",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Mei",
        "given_name": "Xin"
      },
      {
        "surname": "Yang",
        "given_name": "Libin"
      },
      {
        "surname": "Jiang",
        "given_name": "Zuowei"
      },
      {
        "surname": "Cai",
        "given_name": "Xiaoyan"
      },
      {
        "surname": "Gao",
        "given_name": "Dehong"
      },
      {
        "surname": "Han",
        "given_name": "Junwei"
      },
      {
        "surname": "Pan",
        "given_name": "Shirui"
      }
    ]
  },
  {
    "title": "Adaptive selection of local and non-local attention mechanisms for speech enhancement",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106236",
    "abstract": "In speech enhancement tasks, local and non-local attention mechanisms have been significantly improved and well studied. However, a natural speech signal contains many dynamic and fast-changing acoustic features, and focusing on one type of attention mechanism (local or non-local) cannot precisely capture the most discriminative information for estimating target speech from background interference. To address this issue, we introduce an adaptive selection network to dynamically select an appropriate route that determines whether to use the attention mechanisms and which to use for the task. We train the adaptive selection network using reinforcement learning with a developed difficulty-adjusted reward that is related to the performance, complexity, and difficulty of target speech estimation from the noisy mixtures. Consequently, we propose an Attention Selection Speech Enhancement Network (ASSENet) with the innovative dynamic block that consists of an adaptive selection network and a local and non-local attention based speech enhancement network. In particular, the ASSENet incorporates both local and non-local attention and develops the attention mechanism selection technique to explore the appropriate route of local and non-local attention mechanisms for speech enhancement tasks. The results show that our method achieves comparable and superior performance to existing approaches with attractive computational costs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001606",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer network",
      "Computer science",
      "Discriminative model",
      "Economics",
      "Geometry",
      "Local area network",
      "Machine learning",
      "Management",
      "Mathematics",
      "Noise reduction",
      "Reinforcement learning",
      "Selection (genetic algorithm)",
      "Speech enhancement",
      "Speech recognition",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Xinmeng"
      },
      {
        "surname": "Tu",
        "given_name": "Weiping"
      },
      {
        "surname": "Yang",
        "given_name": "Yuhong"
      }
    ]
  },
  {
    "title": "Adaptive Relation-Aware Network for zero-shot classification",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106227",
    "abstract": "Supervised learning-based image classification in computer vision relies on visual samples containing a large amount of labeled information. Considering that it is labor-intensive to collect and label images and construct datasets manually, Zero-Shot Learning (ZSL) achieves knowledge transfer from seen categories to unseen categories by mining auxiliary information, which reduces the dependence on labeled image samples and is one of the current research hotspots in computer vision. However, most ZSL methods fail to properly measure the relationships between classes, or do not consider the differences and similarities between classes at all. In this paper, we propose Adaptive Relation-Aware Network (ARAN), a novel ZSL approach that incorporates the improved triplet loss from deep metric learning into a VAE-based generative model, which helps to model inter-class and intra-class relationships for different classes in ZSL datasets and generate an arbitrary amount of high-quality visual features containing more discriminative information. Moreover, we validate the effectiveness and superior performance of our ARAN through experimental evaluations under ZSL and more practical GZSL settings on three popular datasets AWA2, CUB, and SUN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001515",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Class (philosophy)",
      "Computer science",
      "Construct (python library)",
      "Data mining",
      "Discriminative model",
      "Economics",
      "Generative grammar",
      "Image (mathematics)",
      "Machine learning",
      "Measure (data warehouse)",
      "Metric (unit)",
      "Operations management",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Programming language",
      "Relation (database)",
      "Shot (pellet)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Xun"
      },
      {
        "surname": "Liu",
        "given_name": "Yang"
      },
      {
        "surname": "Dang",
        "given_name": "Yuhao"
      },
      {
        "surname": "Gao",
        "given_name": "Xinbo"
      },
      {
        "surname": "Han",
        "given_name": "Jungong"
      },
      {
        "surname": "Shao",
        "given_name": "Ling"
      }
    ]
  },
  {
    "title": "Towards a unified framework for graph-based multi-view clustering",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106197",
    "abstract": "Recently, clustering data collected from various sources has become a hot topic in real-world applications. The most common methods for multi-view clustering can be divided into several categories: Spectral clustering algorithms, subspace multi-view clustering algorithms, matrix factorization approaches, and kernel methods. Despite the high performance of these methods, they directly fuse all similarity matrices of all views and separate the affinity learning process from the multiview clustering process. The performance of these algorithms can be affected by noisy affinity matrices. To overcome this drawback, this paper presents a novel method called One Step Multi-view Clustering via Consensus Graph Learning and Nonnegative Embedding (OSMGNE). Instead of directly merging the similarity matrices of different views, which may contain noise, a step of learning a consensus similarity matrix is performed. This step forces the similarity matrices of different views to be too similar, which eliminates the problem of noisy data. Moreover, the use of the nonnegative embedding matrix (soft cluster assignment matrix makes it possible to directly obtain the final clustering result without any extra step. The proposed method can solve five subtasks simultaneously. It jointly estimates the similarity matrix of all views, the similarity matrix of each view, the corresponding spectral projection matrix, the unified clustering indicator matrix, and automatically gives the weight of each view without the use of hyper-parameters. In addition, another version of our method is also studied in this paper. This method differs from the first one by using a consensus spectral projection matrix and a consensus Laplacian matrix over all views. An iterative algorithm is proposed to solve the optimization problem of these two methods. The two proposed methods are tested on several real datasets, which prove their superiority.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001217",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Cluster analysis",
      "Composite material",
      "Computer science",
      "Correlation clustering",
      "Eigenvalues and eigenvectors",
      "Fuzzy clustering",
      "Graph",
      "Image (mathematics)",
      "Laplacian matrix",
      "Materials science",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Matrix decomposition",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Similarity (geometry)",
      "Spectral clustering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Dornaika",
        "given_name": "F."
      },
      {
        "surname": "El Hajjar",
        "given_name": "S."
      }
    ]
  },
  {
    "title": "Towards a better negative sampling strategy for dynamic graphs",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106175",
    "abstract": "As dynamic graphs have become indispensable in numerous fields due to their capacity to represent evolving relationships over time, there has been a concomitant increase in the development of Temporal Graph Neural Networks (TGNNs). When training TGNNs for dynamic graph link prediction, the commonly used negative sampling method often produces starkly contrasting samples, which can lead the model to overfit these pronounced differences and compromise its ability to generalize effectively to new data. To address this challenge, we introduce an innovative negative sampling approach named Enhanced Negative Sampling (ENS). This strategy takes into account two pervasive traits observed in dynamic graphs: (1) Historical dependence, indicating that nodes frequently reestablish connections they held in the past, and (2) Temporal proximity preference, which posits that nodes are more inclined to connect with those they have recently interacted with. Specifically, our technique employs a designed scheduling function to strategically control the progression of difficulty of the negative samples throughout the training. This ensures that the training progresses in a balanced manner, becoming incrementally challenging, and thereby enhancing TGNNs’ proficiency in predicting links within dynamic graphs. In our empirical evaluation across multiple datasets, we discerned that our ENS, when integrated as a modular component, notably augments the performance of four SOTA baselines. Additionally, we further investigated the applicability of ENS in handling dynamic graphs of varied attributes. Our code is available at https://github.com/qqaazxddrr/ENS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000996",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Machine learning",
      "Modular design",
      "Operating system",
      "Overfitting",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Kuang"
      },
      {
        "surname": "Liu",
        "given_name": "Chuang"
      },
      {
        "surname": "Wu",
        "given_name": "Jia"
      },
      {
        "surname": "Du",
        "given_name": "Bo"
      },
      {
        "surname": "Hu",
        "given_name": "Wenbin"
      }
    ]
  },
  {
    "title": "MSRMNet: Multi-scale skip residual and multi-mixed features network for salient object detection",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106144",
    "abstract": "The current models for the salient object detection (SOD) have made remarkable progress through multi-scale feature fusion strategies. However, the existing models have large deviations in the detection of different scales, and the target boundaries of the prediction images are still blurred. In this paper, we propose a new model addressing these issues using a transformer backbone to capture multiple feature layers. The model uses multi-scale skip residual connections during encoding to improve the accuracy of the model’s predicted object position and edge pixel information. Furthermore, to extract richer multi-scale semantic information, we perform multiple mixed feature operations in the decoding stage. In addition, we add the structure similarity index measure (SSIM) function with coefficients in the loss function to enhance the accurate prediction performance of the boundaries. Experiments demonstrate that our algorithm achieves state-of-the-art results on five public datasets, and improves the performance metrics of the existing SOD tasks. Codes and results are available at: https://github.com/xxwudi508/MSRMNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000601",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Decoding methods",
      "Encoding (memory)",
      "Feature (linguistics)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pixel",
      "Quantum mechanics",
      "Residual",
      "Salient",
      "Scale (ratio)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Xinlong"
      },
      {
        "surname": "Wang",
        "given_name": "Luping"
      }
    ]
  },
  {
    "title": "Lie–Poisson Neural Networks (LPNets): Data-based computing of Hamiltonian systems with symmetries",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106162",
    "abstract": "An accurate data-based prediction of the long-term evolution of Hamiltonian systems requires a network that preserves the appropriate structure under each time step. Every Hamiltonian system contains two essential ingredients: the Poisson bracket and the Hamiltonian. Hamiltonian systems with symmetries, whose paradigm examples are the Lie–Poisson systems, have been shown to describe a broad category of physical phenomena, from satellite motion to underwater vehicles, fluids, geophysical applications, complex fluids, and plasma physics. The Poisson bracket in these systems comes from the symmetries, while the Hamiltonian comes from the underlying physics. We view the symmetry of the system as primary, hence the Lie–Poisson bracket is known exactly, whereas the Hamiltonian is regarded as coming from physics and is considered not known, or known approximately. Using this approach, we develop a network based on transformations that exactly preserve the Poisson bracket and the special functions of the Lie–Poisson systems (Casimirs) to machine precision. We present two flavors of such systems: one, where the parameters of transformations are computed from data using a dense neural network (LPNets), and another, where the composition of transformations is used as building blocks (G-LPNets). We also show how to adapt these methods to a larger class of Poisson brackets. We apply the resulting methods to several examples, such as rigid body (satellite) motion, underwater vehicles, a particle in a magnetic field, and others. The methods developed in this paper are important for the construction of accurate data-based methods for simulating the long-term dynamics of physical systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000868",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Classical mechanics",
      "Computer science",
      "Geometry",
      "Hamiltonian (control theory)",
      "Hamiltonian mechanics",
      "Hamiltonian system",
      "Homogeneous space",
      "Lie algebra",
      "Mathematical optimization",
      "Mathematics",
      "Phase space",
      "Physics",
      "Poisson bracket",
      "Poisson distribution",
      "Pure mathematics",
      "Quantum mechanics",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Eldred",
        "given_name": "Christopher"
      },
      {
        "surname": "Gay-Balmaz",
        "given_name": "François"
      },
      {
        "surname": "Huraka",
        "given_name": "Sofiia"
      },
      {
        "surname": "Putkaradze",
        "given_name": "Vakhtang"
      }
    ]
  },
  {
    "title": "Higher-order neurodynamical equation for simplex prediction",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106185",
    "abstract": "It is demonstrated that higher-order patterns beyond pairwise relations can significantly enhance the learning capability of existing graph-based models, and simplex is one of the primary form for graphically representing higher-order patterns. Predicting unknown (disappeared) simplices in real-world complex networks can provide us with deeper insights, thereby assisting us in making better decisions. Nevertheless, previous efforts to predict simplices suffer from two issues: (i) they mainly focus on 2- or 3-simplices, and there are few models available for predicting simplices of arbitrary orders, and (ii) they lack the ability to analyze and learn the features of simplices from the perspective of dynamics. In this paper, we present a Higher-order Neurodynamical Equation for Simplex Prediction of arbitrary order (HNESP), which is a framework that combines neural networks and neurodynamics. Specifically, HNESP simulates the dynamical coupling process of nodes in simplicial complexes through different relations (i.e., strong pairwise relation, weak pairwise relation, and simplex) to learn node-level representations, while explaining the learning mechanism of neural networks from neurodynamics. To enrich the higher-order information contained in simplices, we exploit the entropy and normalized multivariate mutual information of different sub-structures of simplices to acquire simplex-level representations. Furthermore, simplex-level representations and multi-layer perceptron are used to quantify the existence probability of simplices. The effectiveness of HNESP is demonstrated by extensive simulations on seven higher-order benchmarks. Experimental results show that HNESP improves the AUC values of the state-of-the-art baselines by an average of 8.32%. Our implementations will be publicly available at: https://github.com/jianruichen/HNESP.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001096",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Engineering",
      "Geometry",
      "Graph",
      "Inverse",
      "Mathematics",
      "Node (physics)",
      "Pairwise comparison",
      "Perceptron",
      "Simplex",
      "Structural engineering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zhihui"
      },
      {
        "surname": "Chen",
        "given_name": "Jianrui"
      },
      {
        "surname": "Gong",
        "given_name": "Maoguo"
      },
      {
        "surname": "Shao",
        "given_name": "Zhongshi"
      }
    ]
  },
  {
    "title": "Weisfeiler–Lehman goes dynamic: An analysis of the expressive power of Graph Neural Networks for attributed and dynamic graphs",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106213",
    "abstract": "Graph Neural Networks (GNNs) are a large class of relational models for graph processing. Recent theoretical studies on the expressive power of GNNs have focused on two issues. On the one hand, it has been proven that GNNs are as powerful as the Weisfeiler–Lehman test (1-WL) in their ability to distinguish graphs. Moreover, it has been shown that the equivalence enforced by 1-WL equals unfolding equivalence. On the other hand, GNNs turned out to be universal approximators on graphs modulo the constraints enforced by 1-WL/unfolding equivalence. However, these results only apply to Static Attributed Undirected Homogeneous Graphs (SAUHG) with node attributes. In contrast, real-life applications often involve a much larger variety of graph types. In this paper, we conduct a theoretical analysis of the expressive power of GNNs for two other graph domains that are particularly interesting in practical applications, namely dynamic graphs and SAUGHs with edge attributes. Dynamic graphs are widely used in modern applications; hence, the study of the expressive capability of GNNs in this domain is essential for practical reasons and, in addition, it requires a new analyzing approach due to the difference in the architecture of dynamic GNNs compared to static ones. On the other hand, the examination of SAUHGs is of particular relevance since they act as a standard form for all graph types: it has been shown that all graph types can be transformed without loss of information to SAUHGs with both attributes on nodes and edges. This paper considers generic GNN models and appropriate 1-WL tests for those domains. Then, the known results on the expressive power of GNNs are extended to the mentioned domains: it is proven that GNNs have the same capability as the 1-WL test, the 1-WL equivalence equals unfolding equivalence and that GNNs are universal approximators modulo 1-WL/unfolding equivalence. Moreover, the proof of the approximation capability is mostly constructive and allows us to deduce hints on the architecture of GNNs that can achieve the desired approximation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001370",
    "keywords": [
      "Computer science",
      "Discrete mathematics",
      "Equivalence (formal languages)",
      "Expressive power",
      "Graph",
      "Graph product",
      "Line graph",
      "Mathematics",
      "Pathwidth",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Beddar-Wiesing",
        "given_name": "Silvia"
      },
      {
        "surname": "D’Inverno",
        "given_name": "Giuseppe Alessio"
      },
      {
        "surname": "Graziani",
        "given_name": "Caterina"
      },
      {
        "surname": "Lachi",
        "given_name": "Veronica"
      },
      {
        "surname": "Moallemy-Oureh",
        "given_name": "Alice"
      },
      {
        "surname": "Scarselli",
        "given_name": "Franco"
      },
      {
        "surname": "Thomas",
        "given_name": "Josephine Maria"
      }
    ]
  },
  {
    "title": "Knowledge distillation under ideal joint classifier assumption",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106160",
    "abstract": "Knowledge distillation constitutes a potent methodology for condensing substantial neural networks into more compact and efficient counterparts. Within this context, softmax regression representation learning serves as a widely embraced approach, leveraging a pre-established teacher network to guide the learning process of a diminutive student network. Notably, despite the extensive inquiry into the efficacy of softmax regression representation learning, the intricate underpinnings governing the knowledge transfer mechanism remain inadequately elucidated. This study introduces the ‘Ideal Joint Classifier Knowledge Distillation’ (IJCKD) framework, an overarching paradigm that not only furnishes a lucid and exhaustive comprehension of prevailing knowledge distillation techniques but also establishes a theoretical underpinning for prospective investigations. Employing mathematical methodologies derived from domain adaptation theory, this investigation conducts a comprehensive examination of the error boundary of the student network contingent upon the teacher network. Consequently, our framework facilitates efficient knowledge transference between teacher and student networks, thereby accommodating a diverse spectrum of applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000844",
    "keywords": [
      "Architectural engineering",
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Classifier (UML)",
      "Computer science",
      "Distillation",
      "Engineering",
      "Epistemology",
      "Ideal (ethics)",
      "Joint (building)",
      "Machine learning",
      "Margin classifier",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Huayu"
      },
      {
        "surname": "Chen",
        "given_name": "Xiwen"
      },
      {
        "surname": "Ditzler",
        "given_name": "Gregory"
      },
      {
        "surname": "Roveda",
        "given_name": "Janet"
      },
      {
        "surname": "Li",
        "given_name": "Ao"
      }
    ]
  },
  {
    "title": "Priors-assisted dehazing network with attention supervision and detail preservation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106165",
    "abstract": "Single image dehazing is a challenging computer vision task for other high-level applications, e . g . , object detection, navigation, and positioning systems. Recently, most existing dehazing methods have followed a “black box” recovery paradigm that obtains the haze-free image from its corresponding hazy input by network learning. Unfortunately, these algorithms ignore the effective utilization of relevant image priors and non-uniform haze distribution problems, causing insufficient or excessive dehazing performance. In addition, they pay little attention to image detail preservation during the dehazing process, thus inevitably producing blurry results. To address the above problems, we propose a novel priors-assisted dehazing network (called PADNet), which fully explores relevant image priors from two new perspectives: attention supervision and detail preservation. For one thing, we leverage the dark channel prior to constrain the attention map generation that denotes the haze pixel position information, thereby better extracting non-uniform feature distributions from hazy images. For another, we find that the residual channel prior of the hazy images contains rich structural information, so it is natural to incorporate it into our dehazing architecture to preserve more structural detail information. Furthermore, since the attention map and dehazed image are simultaneously predicted during the convergence of our model, a self-paced semi-curriculum learning strategy is utilized to alleviate the learning ambiguity. Extensive quantitative and qualitative experiments on several benchmark datasets demonstrate that our PADNet can perform favorably against existing state-of-the-art methods. The code will be available at https://github.com/leandepk/PADNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000893",
    "keywords": [
      "Artificial intelligence",
      "Bayesian probability",
      "Computer science",
      "Machine learning",
      "Prior probability"
    ],
    "authors": [
      {
        "surname": "Yi",
        "given_name": "Weichao"
      },
      {
        "surname": "Dong",
        "given_name": "Liquan"
      },
      {
        "surname": "Liu",
        "given_name": "Ming"
      },
      {
        "surname": "Hui",
        "given_name": "Mei"
      },
      {
        "surname": "Kong",
        "given_name": "Lingqin"
      },
      {
        "surname": "Zhao",
        "given_name": "Yuejin"
      }
    ]
  },
  {
    "title": "Extended Dynamic Mode Decomposition with Invertible Dictionary Learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106177",
    "abstract": "The Koopman operator has received attention for providing a potentially global linearization representation of the nonlinear dynamical system. To estimate or control the original system, the invertibility problem is introduced into the data-driven modeling, i.e., the observables are required to be reconstructed the original system’s states. Existing methods cannot solve this problem perfectly. Only linear or nonlinear but lossy reconstruction can be achieved. This paper proposed a novel data-driven modeling approach, denoted as the Extended Dynamic Mode Decomposition with Invertible Dictionary Learning (EDMD-IDL) to address this issue, which can be interpreted as a further extension of the classical Extended Dynamic Mode Decomposition (EDMD). The Invertible Neural Network (INN) is introduced in the proposed method, where its inverse process provides the explicit inverse on the dictionary functions, thus allowing the nonlinear and lossless reconstruction. An iterative algorithm is designed to solve the extended optimization problem defined by the Koopman operator and INN by combining the optimization algorithm based on the gradient descent and the classical EDMD method, making the method successfully obtain the finite-dimensional approximation of the Koopman operator. The method is tested on various canonical nonlinear dynamical systems and is shown that the predictions obtained in a linear fashion and the ground truth match well over the long-term, where only the initial status is provided. Comparison experiments highlight the superiority of the proposed method over the other EDMD-based methods. Notably, a typical example in fluid dynamics, cylinder wake, illustrates the potential of the method to be further extended to the high-dimensional system with tens of thousands of states. By combining the Proper Orthogonal Decomposition technique, nontrivial Kármán vortex sheet phenomenon is perfectly reconstructed. Our proposed method provides a new paradigm for solving the finite-dimensional approximation of the Koopman operator and applying it to data-driven modeling.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001011",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Dynamic mode decomposition",
      "Dynamical systems theory",
      "Gene",
      "Gradient descent",
      "Invertible matrix",
      "Linearization",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Nonlinear system",
      "Operator (biology)",
      "Physics",
      "Pure mathematics",
      "Quantum mechanics",
      "Repressor",
      "Transcription factor"
    ],
    "authors": [
      {
        "surname": "Jin",
        "given_name": "Yuhong"
      },
      {
        "surname": "Hou",
        "given_name": "Lei"
      },
      {
        "surname": "Zhong",
        "given_name": "Shun"
      }
    ]
  },
  {
    "title": "Methodology based on spiking neural networks for univariate time-series forecasting",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106171",
    "abstract": "Spiking Neural Networks (SNN) are recognised as well-suited for processing spatiotemporal information with ultra-low energy consumption. However, proposals based on SNN for classification tasks are more common than for forecasting problems. In this sense, this paper presents a new general training methodology for univariate time-series forecasting based on SNN. The methodology is focused on one-step ahead forecasting problems and combines a PulseWidth Modulation based encoding–decoding algorithm with a Surrogate Gradient method as supervised training algorithm. In order to validate the generality of the presented methodology sine-wave, 3 UCI and 1 available real-world datasets are used. The results show very satisfactory forecasting results ( M A E ∈ [ 0 . 0094 , 0 . 2891 ] ) regardless of the characteristics of the dataset or the application field. In addition, weights can be initialised just once to achieve robust results, boosting the advantages of computational and energy cost of SNN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000959",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Boosting (machine learning)",
      "Computer science",
      "Data mining",
      "Ecology",
      "Energy consumption",
      "Generality",
      "Machine learning",
      "Multivariate statistics",
      "Psychology",
      "Psychotherapist",
      "Spiking neural network",
      "Time series",
      "Univariate"
    ],
    "authors": [
      {
        "surname": "Lucas",
        "given_name": "Sergio"
      },
      {
        "surname": "Portillo",
        "given_name": "Eva"
      }
    ]
  },
  {
    "title": "A novel physical activity recognition approach using deep ensemble optimized transformers and reinforcement learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106159",
    "abstract": "In recent years, human physical activity recognition has increasingly attracted attention from different research fields such as healthcare, computer-human interaction, lifestyle monitoring, and athletics. Deep learning models have been extensively employed in developing physical activity recognition systems. To improve these models, their hyperparameters need to be initialized with optimal values. However, tuning these hyperparameters manually is time-consuming and may lead to inaccurate results. Moreover, the application of these models to different data resources and the integration of their results into the overall data processing pipeline are challenging issues in physical activity recognition systems. In this paper, we propose a novel ensemble method for physical activity recognition based on a deep transformer-based time-series classification model that uses heart rate, speed, and distance time-series data to recognize physical activities. In particular, we develop a modified arithmetic optimization algorithm to automatically adjust the optimal values of the classification models’ hyperparameters. Moreover, a reinforcement learning-based ensemble approach is proposed to optimally integrate the results of the classification models obtained using heart rate, speed, and distance time-series data and, subsequently, recognize the physical activities. Experiments performed on a real-world dataset demonstrated that the proposed method achieves promising efficiency in comparison to other state-of-the-art models. More specifically, the proposed method increases the performance compared to the second-best performer by around 3.44 %, 9.45 %, 5.43 %, 2.54 %, and 7.53 % based on accuracy, precision, recall, specificity, and F1-score evaluation metrics, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000832",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Electrical engineering",
      "Engineering",
      "Ensemble learning",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Reinforcement",
      "Reinforcement learning",
      "Structural engineering",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Ahmadian",
        "given_name": "Sajad"
      },
      {
        "surname": "Rostami",
        "given_name": "Mehrdad"
      },
      {
        "surname": "Farrahi",
        "given_name": "Vahid"
      },
      {
        "surname": "Oussalah",
        "given_name": "Mourad"
      }
    ]
  },
  {
    "title": "CGO-ensemble: Chaos game optimization algorithm-based fusion of deep neural networks for accurate Mpox detection",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106183",
    "abstract": "The rising global incidence of human Mpox cases necessitates prompt and accurate identification for effective disease control. Previous studies have predominantly delved into traditional ensemble methods for detection, we introduce a novel approach by leveraging a metaheuristic-based ensemble framework. In this research, we present an innovative CGO-Ensemble framework designed to elevate the accuracy of detecting Mpox infection in patients. Initially, we employ five transfer learning base models that integrate feature integration layers and residual blocks. These components play a crucial role in capturing significant features from the skin images, thereby enhancing the models' efficacy. In the next step, we employ a weighted averaging scheme to consolidate predictions generated by distinct models. To achieve the optimal allocation of weights for each base model in the ensemble process, we leverage the Chaos Game Optimization (CGO) algorithm. This strategic weight assignment enhances classification outcomes considerably, surpassing the performance of randomly assigned weights. Implementing this approach yields notably enhanced prediction accuracy compared to using individual models. We evaluate the effectiveness of our proposed approach through comprehensive experiments conducted on two widely recognized benchmark datasets: the Mpox Skin Lesion Dataset (MSLD) and the Mpox Skin Image Dataset (MSID). To gain insights into the decision-making process of the base models, we have performed Gradient Class Activation Mapping (Grad-CAM) analysis. The experimental results showcase the outstanding performance of the CGO-ensemble, achieving an impressive accuracy of 100% on MSLD and 94.16% on MSID. Our approach significantly outperforms other state-of-the-art optimization algorithms, traditional ensemble methods, and existing techniques in the context of Mpox detection on these datasets. These findings underscore the effectiveness and superiority of the CGO-Ensemble in accurately identifying Mpox cases, highlighting its potential in disease detection and classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001072",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Data mining",
      "Ensemble forecasting",
      "Ensemble learning",
      "Geodesy",
      "Geography",
      "Leverage (statistics)",
      "Machine learning",
      "Operating system",
      "Process (computing)"
    ],
    "authors": [
      {
        "surname": "Asif",
        "given_name": "Sohaib"
      },
      {
        "surname": "Zhao",
        "given_name": "Ming"
      },
      {
        "surname": "Li",
        "given_name": "Yangfan"
      },
      {
        "surname": "Tang",
        "given_name": "Fengxiao"
      },
      {
        "surname": "Zhu",
        "given_name": "Yusen"
      }
    ]
  },
  {
    "title": "CeCR: Cross-entropy contrastive replay for online class-incremental continual learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106163",
    "abstract": "Aiming at the realization of learning continually from an online data stream, replay-based methods have shown superior potential. The main challenge of replay-based methods is the selection of representative samples which are stored in the buffer and replayed. In this paper, we propose the Cross-entropy Contrastive Replay (CeCR) method in the online class-incremental setting. First, we present the Class-focused Memory Retrieval method that proceeds the class-level sampling without replacement. Second, we put forward the class-mean approximation memory update method that selectively replaces the mistakenly classified training samples with samples of current input batch. In addition, the Cross-entropy Contrastive Loss is proposed to implement the model training with obtaining more solid knowledge to achieve effective learning. Experiments show that the CeCR method has comparable or improved performance in two benchmark datasets in comparison with the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400087X",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Class (philosophy)",
      "Computer science",
      "Cross entropy",
      "Entropy (arrow of time)",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Guanglu"
      },
      {
        "surname": "Ji",
        "given_name": "Baolun"
      },
      {
        "surname": "Liang",
        "given_name": "Lili"
      },
      {
        "surname": "Chen",
        "given_name": "Minghui"
      }
    ]
  },
  {
    "title": "Data-driven learning of chaotic dynamical systems using Discrete-Temporal Sobolev Networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106152",
    "abstract": "We introduce the Discrete-Temporal Sobolev Network (DTSN), a neural network loss function that assists dynamical system forecasting by minimizing variational differences between the network output and the training data via a temporal Sobolev norm. This approach is entirely data-driven, architecture agnostic, and does not require derivative information from the estimated system. The DTSN is particularly well suited to chaotic dynamical systems as it minimizes noise in the network output which is crucial for such sensitive systems. For our test cases we consider discrete approximations of the Lorenz-63 system and the Chua circuit. For the network architectures we use the Long Short-Term Memory (LSTM) and the Transformer. The performance of the DTSN is compared with the standard MSE loss for both architectures, as well as with the Physics Informed Neural Network (PINN) loss for the LSTM. The DTSN loss is shown to substantially improve accuracy for both architectures, while requiring less information than the PINN and without noticeably increasing computational time, thereby demonstrating its potential to improve neural network forecasting of dynamical systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000765",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Chaotic",
      "Computer science",
      "Dynamical system (definition)",
      "Dynamical systems theory",
      "Mathematical analysis",
      "Mathematics",
      "Physics",
      "Quantum mechanics",
      "Sobolev space"
    ],
    "authors": [
      {
        "surname": "Kennedy",
        "given_name": "Connor"
      },
      {
        "surname": "Crowdis",
        "given_name": "Trace"
      },
      {
        "surname": "Hu",
        "given_name": "Haoran"
      },
      {
        "surname": "Vaidyanathan",
        "given_name": "Sankaran"
      },
      {
        "surname": "Zhang",
        "given_name": "Hong-Kun"
      }
    ]
  },
  {
    "title": "Triplet-constrained deep hashing for chest X-ray image retrieval in COVID-19 assessment",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106182",
    "abstract": "Radiology images of the chest, such as computer tomography scans and X-rays, have been prominently used in computer-aided COVID-19 analysis. Learning-based radiology image retrieval has attracted increasing attention recently, which generally involves image feature extraction and finding matches in extensive image databases based on query images. Many deep hashing methods have been developed for chest radiology image search due to the high efficiency of retrieval using hash codes. However, they often overlook the complex triple associations between images; that is, images belonging to the same category tend to share similar characteristics and vice versa. To this end, we develop a triplet-constrained deep hashing (TCDH) framework for chest radiology image retrieval to facilitate automated analysis of COVID-19. The TCDH consists of two phases, including (a) feature extraction and (b) image retrieval. For feature extraction, we have introduced a triplet constraint and an image reconstruction task to enhance discriminative ability of learned features, and these features are then converted into binary hash codes to capture semantic information. Specifically, the triplet constraint is designed to pull closer samples within the same category and push apart samples from different categories. Additionally, an auxiliary image reconstruction task is employed during feature extraction to help effectively capture anatomical structures of images. For image retrieval, we utilize learned hash codes to conduct searches for medical images. Extensive experiments on 30,386 chest X-ray images demonstrate the superiority of the proposed method over several state-of-the-art approaches in automated image search. The code is now available online.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001060",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Binary code",
      "Binary number",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Constraint (computer-aided design)",
      "Deep learning",
      "Discriminative model",
      "Feature (linguistics)",
      "Feature extraction",
      "Geometry",
      "Hash function",
      "Image (mathematics)",
      "Image retrieval",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Linmin"
      },
      {
        "surname": "Wang",
        "given_name": "Qianqian"
      },
      {
        "surname": "Wang",
        "given_name": "Xiaochuan"
      },
      {
        "surname": "Ma",
        "given_name": "Yunling"
      },
      {
        "surname": "Zhang",
        "given_name": "Limei"
      },
      {
        "surname": "Liu",
        "given_name": "Mingxia"
      }
    ]
  },
  {
    "title": "Fast multi-view clustering via correntropy-based orthogonal concept factorization",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106170",
    "abstract": "Owing to its ability to handle negative data and promising clustering performance, concept factorization (CF), an improved version of non-negative matrix factorization, has been incorporated into multi-view clustering recently. Nevertheless, existing CF-based multi-view clustering methods still have the following issues: (1) they directly conduct factorization in the original data space, which means its efficiency is sensitive to the feature dimension; (2) they ignore the high degree of factorization freedom of standard CF, which may lead to non-uniqueness factorization thereby causing reduced effectiveness; (3) traditional robust norms they used are unable to handle complex noises, significantly challenging their robustness. To address these issues, we establish a fast multi-view clustering via correntropy-based orthogonal concept factorization (FMVCCF). Specifically, FMVCCF executes factorization on a learned consensus anchor graph rather than directly decomposing the original data, lessening the dimensionality sensitivity. Then, a lightweight graph regularization term is incorporated to refine the factorization process with a low computational burden. Moreover, an improved multi-view correntropy-based orthogonal CF model is developed, which can enhance the effectiveness and robustness under the orthogonal constraint and correntropy criterion, respectively. Extensive experiments demonstrate that FMVCCF can achieve promising effectiveness and robustness on various real-world datasets with high efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000947",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Cluster analysis",
      "Computer science",
      "Curse of dimensionality",
      "Dimensionality reduction",
      "Eigenvalues and eigenvectors",
      "Factorization",
      "Gene",
      "Matrix decomposition",
      "Non-negative matrix factorization",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Jinghan"
      },
      {
        "surname": "Yang",
        "given_name": "Ben"
      },
      {
        "surname": "Xue",
        "given_name": "Zhiyuan"
      },
      {
        "surname": "Zhang",
        "given_name": "Xuetao"
      },
      {
        "surname": "Lin",
        "given_name": "Zhiping"
      },
      {
        "surname": "Chen",
        "given_name": "Badong"
      }
    ]
  },
  {
    "title": "Mode combinability: Exploring convex combinations of permutation aligned models",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106204",
    "abstract": "We explore element-wise convex combinations of two permutation-aligned neural network parameter vectors Θ A and Θ B of size d . We conduct extensive experiments by examining various distributions of such model combinations parametrized by elements of the hypercube [ 0 , 1 ] d and its vicinity. Our findings reveal that broad regions of the hypercube form surfaces of low loss values, indicating that the notion of linear mode connectivity extends to a more general phenomenon which we call mode combinability. We also make several novel observations regarding linear mode connectivity and model re-basin. We demonstrate a transitivity property: two models re-based to a common third model are also linear mode connected, and a robustness property: even with significant perturbations of the neuron matchings the resulting combinations continue to form a working model. Moreover, we analyze the functional and weight similarity of model combinations and show that such combinations are non-vacuous in the sense that there are significant functional differences between the resulting models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400128X",
    "keywords": [
      "Acoustics",
      "Artificial intelligence",
      "Computer science",
      "Geometry",
      "Mathematics",
      "Mode (computer interface)",
      "Operating system",
      "Permutation (music)",
      "Physics",
      "Regular polygon"
    ],
    "authors": [
      {
        "surname": "Csiszárik",
        "given_name": "Adrián"
      },
      {
        "surname": "Kiss",
        "given_name": "Melinda F."
      },
      {
        "surname": "Kőrösi-Szabó",
        "given_name": "Péter"
      },
      {
        "surname": "Muntag",
        "given_name": "Márton"
      },
      {
        "surname": "Papp",
        "given_name": "Gergely"
      },
      {
        "surname": "Varga",
        "given_name": "Dániel"
      }
    ]
  },
  {
    "title": "One-step Bayesian example-dependent cost classification: The OsC-MLP method",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106168",
    "abstract": "Example-dependent cost classification problems are those where the decision costs depend not only on the true and the attributed classes but also on the sample features. Discriminative algorithms that carry out such classification tasks must take this dependence into account. In some applications, the decision costs are known for the training set but not in production, which complicates the problem. In this paper, we introduce a new one-step Bayesian formulation to train Neural Networks and solve the above limitation for binary cases with one-step Learning Machines, avoiding the drawbacks that unknown analytical forms of the example-dependent costs create. The formulation is based on defining an artificial likelihood ratio by using the available training classification costs in its definition, and proposes a test that does not require the values of the costs for unseen samples. Furthermore, it also includes Bayesian rebalancing mechanisms to combat the negative effects of class imbalance. Experimental results support the consistency and effectiveness of the corresponding algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000923",
    "keywords": [
      "Arithmetic",
      "Artificial intelligence",
      "Artificial neural network",
      "Bayesian probability",
      "Binary classification",
      "Binary number",
      "Chemistry",
      "Chromatography",
      "Class (philosophy)",
      "Computer science",
      "Consistency (knowledge bases)",
      "Discriminative model",
      "Machine learning",
      "Mathematics",
      "Programming language",
      "Sample (material)",
      "Set (abstract data type)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Mediavilla-Relaño",
        "given_name": "Javier"
      },
      {
        "surname": "Lázaro",
        "given_name": "Marcelino"
      }
    ]
  },
  {
    "title": "Multi-level multilingual semantic alignment for zero-shot cross-lingual transfer learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106217",
    "abstract": "Recently, cross-lingual transfer learning has attracted extensive attention from both academia and industry. Previous studies usually focus only on the single-level alignment (e.g., word-level, sentence-level), based on pre-trained language models. However, it leads to suboptimal performance in downstream tasks of the low-resource language due to the missing correlation of hierarchical semantic information (e.g., sentence-to-word, word-to-word). Therefore, in this paper, we propose a novel multi-level alignment framework, which hierarchically learns the semantic correlation between multiple levels by leveraging well-designed alignment training tasks. In addition, we devise an attention-based fusion mechanism (AFM) to infuse semantic information from high levels. Extensive experiments on mainstream cross-lingual tasks (e.g., text classification, paraphrase identification, and named entity recognition) demonstrate the effectiveness of our proposed method, and also show that our model achieves state-of-the-art performance across various benchmarks compared to other strong baselines.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001412",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Focus (optics)",
      "Linguistics",
      "Natural language processing",
      "Optics",
      "Paraphrase",
      "Philosophy",
      "Physics",
      "Sentence",
      "Word (group theory)"
    ],
    "authors": [
      {
        "surname": "Gui",
        "given_name": "Anchun"
      },
      {
        "surname": "Xiao",
        "given_name": "Han"
      }
    ]
  },
  {
    "title": "Enhancing and improving the performance of imbalanced class data using novel GBO and SSG: A comparative analysis",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106157",
    "abstract": "Class imbalance problem (CIP) in a dataset is a major challenge that significantly affects the performance of Machine Learning (ML) models resulting in biased predictions. Numerous techniques have been proposed to address CIP, including, but not limited to, Oversampling, Undersampling, and cost-sensitive approaches. Due to its ability to generate synthetic data, oversampling techniques such as the Synthetic Minority Oversampling Technique (SMOTE) are the most widely used methodology by researchers. However, one of SMOTE’s potential disadvantages is that newly created minor samples overlap with major samples. Therefore, the probability of ML models’ biased performance toward major classes increases. Generative adversarial network (GAN) has recently garnered much attention due to their ability to create real samples. However, GAN is hard to train even though it has much potential. Considering these opportunities, this work proposes two novel techniques: GAN-based Oversampling (GBO) and Support Vector Machine-SMOTE-GAN (SSG) to overcome the limitations of the existing approaches. The preliminary results show that SSG and GBO performed better on the nine imbalanced benchmark datasets than several existing SMOTE-based approaches. Additionally, it can be observed that the proposed SSG and GBO methods can accurately classify the minor class with more than 90% accuracy when tested with 20%, 30%, and 40% of the test data. The study also revealed that the minor sample generated by SSG demonstrates Gaussian distributions, which is often difficult to achieve using original SMOTE and SVM-SMOTE.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000819",
    "keywords": [
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Benchmark (surveying)",
      "Class (philosophy)",
      "Computer network",
      "Computer science",
      "Data mining",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Oversampling",
      "Pattern recognition (psychology)",
      "Support vector machine",
      "Undersampling"
    ],
    "authors": [
      {
        "surname": "Ahsan",
        "given_name": "Md Manjurul"
      },
      {
        "surname": "Ali",
        "given_name": "Md Shahin"
      },
      {
        "surname": "Siddique",
        "given_name": "Zahed"
      }
    ]
  },
  {
    "title": "A comprehensive and reliable feature attribution method: Double-sided remove and reconstruct (DoRaR)",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106166",
    "abstract": "The limited transparency of the inner decision-making mechanism in deep neural networks (DNN) and other machine learning (ML) models has hindered their application in several domains. In order to tackle this issue, feature attribution methods have been developed to identify the crucial features that heavily influence decisions made by these black box models. However, many feature attribution methods have inherent downsides. For example, one category of feature attribution methods suffers from the artifacts problem, which feeds out-of-distribution masked inputs directly through the classifier that was originally trained on natural data points. Another category of feature attribution method finds explanations by using jointly trained feature selectors and predictors. While avoiding the artifacts problem, this new category suffers from the Encoding Prediction in the Explanation (EPITE) problem, in which the predictor’s decisions rely not on the features, but on the masks that selects those features. As a result, the credibility of attribution results is undermined by these downsides. In this research, we introduce the Double-sided Remove and Reconstruct (DoRaR) feature attribution method based on several improvement methods that addresses these issues. By conducting thorough testing on MNIST, CIFAR10 and our own synthetic dataset, we demonstrate that the DoRaR feature attribution method can effectively bypass the above issues and can aid in training a feature selector that outperforms other state-of-the-art feature attribution methods. Our code is available at https://github.com/dxq21/DoRaR.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400090X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Qin",
        "given_name": "Dong"
      },
      {
        "surname": "Amariucai",
        "given_name": "George T."
      },
      {
        "surname": "Qiao",
        "given_name": "Daji"
      },
      {
        "surname": "Guan",
        "given_name": "Yong"
      },
      {
        "surname": "Fu",
        "given_name": "Shen"
      }
    ]
  },
  {
    "title": "A Comprehensive Survey on Deep Graph Representation Learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106207",
    "abstract": "Graph representation learning aims to effectively encode high-dimensional sparse graph-structured data into low-dimensional dense vectors, which is a fundamental task that has been widely studied in a range of fields, including machine learning and data mining. Classic graph embedding methods follow the basic idea that the embedding vectors of interconnected nodes in the graph can still maintain a relatively close distance, thereby preserving the structural information between the nodes in the graph. However, this is sub-optimal due to: (i) traditional methods have limited model capacity which limits the learning performance; (ii) existing techniques typically rely on unsupervised learning strategies and fail to couple with the latest learning paradigms; (iii) representation learning and downstream tasks are dependent on each other which should be jointly enhanced. With the remarkable success of deep learning, deep graph representation learning has shown great potential and advantages over shallow (traditional) methods, there exist a large number of deep graph representation learning techniques have been proposed in the past decade, especially graph neural networks. In this survey, we conduct a comprehensive survey on current deep graph representation learning algorithms by proposing a new taxonomy of existing state-of-the-art literature. Specifically, we systematically summarize the essential components of graph representation learning and categorize existing approaches by the ways of graph neural network architectures and the most recent advanced learning paradigms. Moreover, this survey also provides the practical and promising applications of deep graph representation learning. Last but not least, we state new perspectives and suggest challenging directions which deserve further investigations in the future.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400131X",
    "keywords": [
      "Artificial intelligence",
      "Categorization",
      "Computer science",
      "Deep learning",
      "Embedding",
      "Feature learning",
      "Graph",
      "Graph embedding",
      "Machine learning",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Ju",
        "given_name": "Wei"
      },
      {
        "surname": "Fang",
        "given_name": "Zheng"
      },
      {
        "surname": "Gu",
        "given_name": "Yiyang"
      },
      {
        "surname": "Liu",
        "given_name": "Zequn"
      },
      {
        "surname": "Long",
        "given_name": "Qingqing"
      },
      {
        "surname": "Qiao",
        "given_name": "Ziyue"
      },
      {
        "surname": "Qin",
        "given_name": "Yifang"
      },
      {
        "surname": "Shen",
        "given_name": "Jianhao"
      },
      {
        "surname": "Sun",
        "given_name": "Fang"
      },
      {
        "surname": "Xiao",
        "given_name": "Zhiping"
      },
      {
        "surname": "Yang",
        "given_name": "Junwei"
      },
      {
        "surname": "Yuan",
        "given_name": "Jingyang"
      },
      {
        "surname": "Zhao",
        "given_name": "Yusheng"
      },
      {
        "surname": "Wang",
        "given_name": "Yifan"
      },
      {
        "surname": "Luo",
        "given_name": "Xiao"
      },
      {
        "surname": "Zhang",
        "given_name": "Ming"
      }
    ]
  },
  {
    "title": "Confounder balancing in adversarial domain adaptation for pre-trained large models fine-tuning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106173",
    "abstract": "The excellent generalization, contextual learning, and emergence abilities in the pre-trained large models (PLMs) handle specific tasks without direct training data, making them the better foundation models in the adversarial domain adaptation (ADA) methods to transfer knowledge learned from the source domain to target domains. However, existing ADA methods fail to account for the confounder properly, which is the root cause of the source data distribution that differs from the target domains. This study proposes a confounder balancing method in adversarial domain adaptation for PLMs fine-tuning (CadaFT), which includes a PLM as the foundation model for a feature extractor, a domain classifier and a confounder classifier, and they are jointly trained with an adversarial loss. This loss is designed to improve the domain-invariant representation learning by diluting the discrimination in the domain classifier. At the same time, the adversarial loss also balances the confounder distribution among source and unmeasured domains in training. Compared to newest ADA methods, CadaFT can correctly identify confounders in domain-invariant features, thereby eliminating the confounder biases in the extracted features from PLMs. The confounder classifier in CadaFT is designed as a plug-and-play and can be applied in the confounder measurable, unmeasurable, or partially measurable environments. Empirical results on natural language processing and computer vision downstream tasks show that CadaFT outperforms the newest GPT-4, LLaMA2, ViT and ADA methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000972",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Confounding",
      "Domain adaptation",
      "Machine learning",
      "Mathematics",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Shuoran"
      },
      {
        "surname": "Chen",
        "given_name": "Qingcai"
      },
      {
        "surname": "Xiang",
        "given_name": "Yang"
      },
      {
        "surname": "Pan",
        "given_name": "Youcheng"
      },
      {
        "surname": "Wu",
        "given_name": "Xiangping"
      },
      {
        "surname": "Lin",
        "given_name": "Yukang"
      }
    ]
  },
  {
    "title": "DDK: Dynamic structure pruning based on differentiable search and recursive knowledge distillation for BERT",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106164",
    "abstract": "Large-scale pre-trained models, such as BERT, have demonstrated outstanding performance in Natural Language Processing (NLP). Nevertheless, the high number of parameters in these models has increased the demand for hardware storage and computational resources while posing a challenge for their practical deployment. In this article, we propose a combined method of model pruning and knowledge distillation to compress and accelerate large-scale pre-trained language models. Specifically, we introduce a dynamic structure pruning method based on differentiable search and recursive knowledge distillation to automatically prune the BERT model, named DDK. We define the search space for network pruning as all feed-forward layer channels and self-attention heads at each layer of the network, and utilize differentiable methods to determine their optimal number. Additionally, we design a recursive knowledge distillation method that employs adaptive weighting to extract the most important features from multiple intermediate layers of the teacher model and fuse them to supervise the student network learning. Our experimental results on the GLUE benchmark dataset and ablation analysis demonstrate that our proposed method outperforms other advanced methods in terms of average performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000881",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Chemistry",
      "Computer science",
      "Differentiable function",
      "Distillation",
      "Geodesy",
      "Geography",
      "Language model",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Organic chemistry",
      "Physics",
      "Pruning",
      "Quantum mechanics",
      "Radiology",
      "Scale (ratio)",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zhou"
      },
      {
        "surname": "Lu",
        "given_name": "Yang"
      },
      {
        "surname": "Wang",
        "given_name": "Tengfei"
      },
      {
        "surname": "Wei",
        "given_name": "Xing"
      },
      {
        "surname": "Wei",
        "given_name": "Zhen"
      }
    ]
  },
  {
    "title": "A regularized orthogonal activated inverse-learning neural network for regression and classification with outliers",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106208",
    "abstract": "A novel regularized orthogonal activated inverse-learning (ROAIL) neural network is proposed and investigated for reducing the impact of outliers in regression and classification fields. The proposed ROAIL network does not require extensive iterative computations. Instead, it can achieve the desired results with a single step of computation, allowing for the efficient acquisition of network weights. By extending the Gegenbauer polynomials to a multi-variate version, and integrating the ℓ 2 regularization and Welsch loss function into the orthogonal activated inverse-learning framework, two forms of ROAIL are obtained, i.e., ℓ 2 norm ROAIL ( ℓ 2 -ROAIL) and Welsch-ROAIL (W-ROAIL). ℓ 2 -ROAIL neural network is proposed to minimize the empirical and structural risk simultaneously since taking the structural risk as a part of loss function can effectively reduce the complexity of the model and thus improve the generalization ability. W-ROAIL neural network further improves the robustness of the ℓ 2 -ROAIL neural network by replacing the original two-norm in loss function with Welsch function. The Welsch function can determine the weights of each sample according to its output error, and influence of outliers could be weakened since the weights of outliers would be reduced. Both regression and classification experiments show that W-ROAIL neural network has strong ability to suppress the influence of outliers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001321",
    "keywords": [
      "Activation function",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computation",
      "Computer science",
      "Gene",
      "Geometry",
      "Inverse",
      "Mathematics",
      "Outlier",
      "Pattern recognition (psychology)",
      "Regression",
      "Robustness (evolution)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Zhijun"
      },
      {
        "surname": "Song",
        "given_name": "Yating"
      },
      {
        "surname": "Chen",
        "given_name": "Tao"
      },
      {
        "surname": "He",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "A feature refinement and adaptive generative adversarial network for thermal infrared image colorization",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106184",
    "abstract": "Colorizing thermal infrared images poses a significant challenge as current methods struggle with issues such as unrealistic color saturation and limited texture. To address these challenges, we propose the Feature Refinement and Adaptive Generative Adversarial Network (FRAGAN). Our approach enhances the detailed, semantic, and contextual capabilities of image coloring by combining multi-level interactions that integrate the lost detailed information from the encoding stage with the semantic information from the decoding stage. Additionally, we introduce the Residual Feature Refinement Module (RFRM) to improve both the accuracy and generalization ability of the model, thereby elevating the quality of colorization results. The Feature Adaptation Module (FAM) is employed to mitigate sub-region information loss during downsampling. Furthermore, we introduce the Trinity Attention Module (TAM) to accurately capture the spatial and channel-wise interaction features of local semantic information. Extensive experimentation on the KAIST dataset and the FLIR dataset demonstrates the superiority of our proposed FRAGAN methodology, surpassing both the performance metrics and visual quality of current state-of-the-art methods. The colorized images generated by our proposed FRAGAN exhibit enhanced clarity and realism. Our code and models are available at GitHub.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001084",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computer science",
      "Decoding methods",
      "Detector",
      "Discriminator",
      "Encoder",
      "Encoding (memory)",
      "Feature (linguistics)",
      "Feature learning",
      "Image (mathematics)",
      "Linguistics",
      "Machine learning",
      "Mutual information",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Telecommunications",
      "Upsampling"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yu"
      },
      {
        "surname": "Zhan",
        "given_name": "Weida"
      },
      {
        "surname": "Jiang",
        "given_name": "Yichun"
      },
      {
        "surname": "Zhu",
        "given_name": "Depeng"
      },
      {
        "surname": "Xu",
        "given_name": "Xiaoyu"
      },
      {
        "surname": "Hao",
        "given_name": "Ziqiang"
      },
      {
        "surname": "Li",
        "given_name": "Jin"
      },
      {
        "surname": "Guo",
        "given_name": "Jinxin"
      }
    ]
  },
  {
    "title": "Spatial multi-attention conditional neural processes",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106201",
    "abstract": "Spatial prediction tasks are challenging when observed samples are sparse and prediction samples are abundant. Gaussian processes (GPs) are commonly used in spatial prediction tasks and have the advantage of measuring the uncertainty of the interpolation result. However, as the sample size increases, GPs suffer from significant overhead. Standard neural networks (NNs) provide a powerful and scalable solution for modeling spatial data, but they often overfit small sample data. Based on conditional neural processes (CNPs), which combine the advantages of GPs and NNs, we propose a new framework called Spatial Multi-Attention Conditional Neural Processes (SMACNPs) for spatial small sample prediction tasks. SMACNPs are a modular model that can predict targets by employing different attention mechanisms to extract relevant information from different forms of sample data. The task representation is inferred by measuring the spatial correlation contained in different sample points and the relationship contained in attribute variables, respectively. The distribution of the target variable is predicted by GPs parameterized by NNs. SMACNPs allow us to obtain accurate predictions of the target value while quantifying the prediction uncertainty. Experiments on spatial prediction tasks on simulated and real-world datasets demonstrate that this framework flexibly incorporates spatial context and correlation into the model, achieving state-of-the-art results in spatial small sample prediction tasks in terms of both predictive performance and reliability. For example, on the California housing dataset, our method reduces MAE by 8% and MSE by 7% compared to the second-best method. In addition, a spatiotemporal prediction task to forecast traffic speed further confirms the effectiveness and generality of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001254",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Global Positioning System",
      "Kriging",
      "Machine learning",
      "Mathematics",
      "Overfitting",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Sample (material)",
      "Spatial analysis",
      "Statistics",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Bao",
        "given_name": "Li-Li"
      },
      {
        "surname": "Zhang",
        "given_name": "Jiang-She"
      },
      {
        "surname": "Zhang",
        "given_name": "Chun-Xia"
      }
    ]
  },
  {
    "title": "Graph-based social relation inference with multi-level conditional attention",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106216",
    "abstract": "Social relation inference intrinsically requires high-level semantic understanding. In order to accurately infer relations of persons in images, one needs not only to understand scenes and objects in images, but also to adaptively attend to important clues. Unlike prior works of classifying social relations using attention on detected objects, we propose a MUlti-level Conditional Attention (MUCA) mechanism for social relation inference, which attends to scenes, objects and human interactions based on each person pair. Then, we develop a transformer-style network to achieve the MUCA mechanism. The novel network named as Graph-based Relation Inference Transformer (i.e., GRIT) consists of two modules, i.e., a Conditional Query Module (CQM) and a Relation Attention Module (RAM). Specifically, we design a graph-based CQM to generate informative relation queries for all person pairs, which fuses local features and global context for each person pair. Moreover, we fully take advantage of transformer-style networks in RAM for multi-level attentions in classifying social relations. To our best knowledge, GRIT is the first for inferring social relations with multi-level conditional attention. GRIT is end-to-end trainable and significantly outperforms existing methods on two benchmark datasets, e.g., with performance improvement of 7.8% on PIPA and 9.6% on PISC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001400",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Conditional random field",
      "Data mining",
      "Graph",
      "Inference",
      "Machine learning",
      "Natural language processing",
      "Physics",
      "Quantum mechanics",
      "Relation (database)",
      "Rule of inference",
      "Theoretical computer science",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Xiaotian"
      },
      {
        "surname": "Yi",
        "given_name": "Hanling"
      },
      {
        "surname": "Tang",
        "given_name": "Qie"
      },
      {
        "surname": "Huang",
        "given_name": "Kun"
      },
      {
        "surname": "Hu",
        "given_name": "Wenze"
      },
      {
        "surname": "Zhang",
        "given_name": "Shiliang"
      },
      {
        "surname": "Wang",
        "given_name": "Xiaoyu"
      }
    ]
  },
  {
    "title": "Unsupervised distribution-aware keypoints generation from 3D point clouds",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106158",
    "abstract": "Keypoints extraction from 3D objects is a fundamental task in point cloud processing. The ideal keypoints should be an ordered and well-aligned set of points that effectively reflect the shape and structure of the object. To this end, this paper proposes an unsupervised 3D point cloud keypoints generation network with the consideration of the probability distribution of keypoints and spatial distribution among keypoints. The network downsamples and groups the 3D point cloud, obtaining local features of the point cloud. The local features are leveraged to explicitly learn the mixture probability distribution of keypoint position. A composite loss function that comprehensively considers shape similarity, point importance, and geometric constraint is proposed to guide the network in generating keypoints with semantic consistency and regular spatial distribution. The experimental results and quantitative comparisons on the ShapeNet and KeypointNet datasets demonstrate that the proposed method achieves ordered, well-aligned, and robust keypoints generation for 3D point clouds. The source code of the proposed method is available at https://github.com/djzgroup/Keypoints.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000820",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Code (set theory)",
      "Computer science",
      "Computer vision",
      "Consistency (knowledge bases)",
      "Constraint (computer-aided design)",
      "Distribution (mathematics)",
      "Economics",
      "Finance",
      "Geometry",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Point (geometry)",
      "Point cloud",
      "Position (finance)",
      "Programming language",
      "Set (abstract data type)",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Yiqi"
      },
      {
        "surname": "Chen",
        "given_name": "Xingye"
      },
      {
        "surname": "Huang",
        "given_name": "Xuan"
      },
      {
        "surname": "Song",
        "given_name": "Kelin"
      },
      {
        "surname": "Zhang",
        "given_name": "Dejun"
      }
    ]
  },
  {
    "title": "Fading memory as inductive bias in residual recurrent networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106179",
    "abstract": "Residual connections have been proposed as an architecture-based inductive bias to mitigate the problem of exploding and vanishing gradients and increased task performance in both feed-forward and recurrent networks (RNNs) when trained with the backpropagation algorithm. Yet, little is known about how residual connections in RNNs influence their dynamics and fading memory properties. Here, we introduce weakly coupled residual recurrent networks (WCRNNs) in which residual connections result in well-defined Lyapunov exponents and allow for studying properties of fading memory. We investigate how the residual connections of WCRNNs influence their performance, network dynamics, and memory properties on a set of benchmark tasks. We show that several distinct forms of residual connections yield effective inductive biases that result in increased network expressivity. In particular, those are residual connections that (i) result in network dynamics at the proximity of the edge of chaos, (ii) allow networks to capitalize on characteristic spectral properties of the data, and (iii) result in heterogeneous memory properties. In addition, we demonstrate how our results can be extended to non-linear residuals and introduce a weakly coupled residual initialization scheme that can be used for Elman RNNs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001035",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Backpropagation",
      "Benchmark (surveying)",
      "Computer science",
      "Decoding methods",
      "Fading",
      "Geodesy",
      "Geography",
      "Initialization",
      "Programming language",
      "Recurrent neural network",
      "Residual"
    ],
    "authors": [
      {
        "surname": "Dubinin",
        "given_name": "Igor"
      },
      {
        "surname": "Effenberger",
        "given_name": "Felix"
      }
    ]
  },
  {
    "title": "Hebbian dreaming for small datasets",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106174",
    "abstract": "The dreaming Hopfield model constitutes a generalization of the Hebbian paradigm for neural networks, that is able to perform on-line learning when “awake” and also to account for off-line “sleeping” mechanisms. The latter have been shown to enhance storing in such a way that, in the long sleep-time limit, this model can reach the maximal storage capacity achievable by networks equipped with symmetric pairwise interactions. In this paper, we inspect the minimal amount of information that must be supplied to such a network to guarantee a successful generalization, and we test it both on random synthetic and on standard structured datasets ( i . e . , MNIST, Fashion-MNIST and Olivetti). By comparing these minimal thresholds of information with those required by the standard ( i . e . , always “awake”) Hopfield model, we prove that the present network can save up to ∼ 90 % of the dataset size, yet preserving the same performance of the standard counterpart. This suggests that sleep may play a pivotal role in explaining the gap between the large volumes of data required to train artificial neural networks and the relatively small volumes needed by their biological counterparts. Further, we prove that the model Cost function (typically used in statistical mechanics) admits a representation in terms of a standard Loss function (typically used in machine learning) and this allows us to analyze its emergent computational skills both theoretically and computationally: a quantitative picture of its capabilities as a function of its control parameters is achieved and consistency between the two approaches is highlighted. The resulting network is an associative memory for pattern recognition tasks that learns from examples on-line, generalizes correctly (in suitable regions of its control parameters) and optimizes its storage capacity by off-line sleeping: such a reduction of the training cost can be inspiring toward sustainable AI and in situations where data are relatively sparse.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000984",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Hebbian theory",
      "Machine learning"
    ],
    "authors": [
      {
        "surname": "Agliari",
        "given_name": "Elena"
      },
      {
        "surname": "Alemanno",
        "given_name": "Francesco"
      },
      {
        "surname": "Aquaro",
        "given_name": "Miriam"
      },
      {
        "surname": "Barra",
        "given_name": "Adriano"
      },
      {
        "surname": "Durante",
        "given_name": "Fabrizio"
      },
      {
        "surname": "Kanter",
        "given_name": "Ido"
      }
    ]
  },
  {
    "title": "Expansion of the editorial team",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106209",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001333",
    "keywords": [
      "Artificial intelligence",
      "Computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "DeLiang"
      },
      {
        "surname": "Forti",
        "given_name": "Mauro"
      },
      {
        "surname": "Liu",
        "given_name": "Tongliang"
      },
      {
        "surname": "Toyoizumi",
        "given_name": "Taro"
      }
    ]
  },
  {
    "title": "Robust noise-aware algorithm for randomized neural network and its convergence properties",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106202",
    "abstract": "The concept of randomized neural networks (RNNs), such as the random vector functional link network (RVFL) and extreme learning machine (ELM), is a widely accepted and efficient network method for constructing single-hidden layer feedforward networks (SLFNs). Due to its exceptional approximation capabilities, RNN is being extensively used in various fields. While the RNN concept has shown great promise, its performance can be unpredictable in imperfect conditions, such as weight noises and outliers. Thus, there is a need to develop more reliable and robust RNN algorithms. To address this issue, this paper proposes a new objective function that addresses the combined effect of weight noise and training data outliers for RVFL networks. Based on the half-quadratic optimization method, we then propose a novel algorithm, named noise-aware RNN (NARNN), to optimize the proposed objective function. The convergence of the NARNN is also theoretically validated. We also discuss the way to use the NARNN for ensemble deep RVFL (edRVFL) networks. Finally, we present an extension of the NARNN to concurrently address weight noise, stuck-at-fault, and outliers. The experimental results demonstrate that the proposed algorithm outperforms a number of state-of-the-art robust RNN algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001266",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Image (mathematics)",
      "Machine learning",
      "Noise (video)",
      "Outlier",
      "Recurrent neural network"
    ],
    "authors": [
      {
        "surname": "Xiao",
        "given_name": "Yuqi"
      },
      {
        "surname": "Adegoke",
        "given_name": "Muideen"
      },
      {
        "surname": "Leung",
        "given_name": "Chi-Sing"
      },
      {
        "surname": "Leung",
        "given_name": "Kwok Wa"
      }
    ]
  },
  {
    "title": "A universal multi-source domain adaptation method with unsupervised clustering for mechanical fault diagnosis under incomplete data",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106167",
    "abstract": "Recently, due to the difficulty of collecting condition data covering all mechanical fault types in industrial scenarios, the fault diagnosis problem under incomplete data is receiving increasing attention where no target prior information can be available. The existing open-set or universal domain adaptation (DA) diagnosis methods typically treat private fault samples in the target as a generalized “unknown” fault class, neglecting their inherent structure. This oversight can lead to confusion in latent feature space representations and difficulties in separating unknown samples. Therefore, a universal DA method with unsupervised clustering is developed to explore the intrinsic structure of the target samples for mechanical fault diagnosis, where multi-source information on different working conditions is considered to transfer complementary knowledge. First, a composite clustering metric combining single-domain and cross-domain evaluation is constructed to recognize shared and unknown health classes on source–target domains. Second, to alleviate the intra-class shift while enlarging the inter-class gap, a class-wise DA algorithm is suggested which operates on the basis of maximum mean discrepancy. Finally, an entropy regularization criterion is utilized to facilitate clustering of different health classes. The efficacy of the presented approach in the fault diagnosis issues when monitoring data is inadequate has been verified through extensive experiments on three rotating machinery datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000911",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Classifier (UML)",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Fault (geology)",
      "Geology",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Seismology",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Tian",
        "given_name": "Jinghui"
      },
      {
        "surname": "Han",
        "given_name": "Dongying"
      },
      {
        "surname": "Karimi",
        "given_name": "Hamid Reza"
      },
      {
        "surname": "Zhang",
        "given_name": "Yu"
      },
      {
        "surname": "Shi",
        "given_name": "Peiming"
      }
    ]
  },
  {
    "title": "Defense against adversarial attacks based on color space transformation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106176",
    "abstract": "Deep Learning algorithms have achieved state-of-the-art performance in various important tasks. However, recent studies have found that an elaborate perturbation may cause a network to misclassify, which is known as an adversarial attack. Based on current research, it is suggested that adversarial examples cannot be eliminated completely. Consequently, it is always possible to determine an attack that is effective against a defense model. We render existing adversarial examples invalid by altering the classification boundaries. Meanwhile, for valid adversarial examples generated against the defense model, the adversarial perturbations are increased so that they can be distinguished by the human eye. This paper proposes a method for implementing the abovementioned concepts through color space transformation. Experiments on CIFAR-10, CIFAR-100, and Mini-ImageNet demonstrate the effectiveness and versatility of our defense method. To the best of our knowledge, this is the first defense model based on the amplification of adversarial perturbations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400100X",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Deep neural networks",
      "Gene",
      "Machine learning",
      "Operating system",
      "Space (punctuation)",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Haoyu"
      },
      {
        "surname": "Wu",
        "given_name": "Chunhua"
      },
      {
        "surname": "Zheng",
        "given_name": "Kangfeng"
      }
    ]
  },
  {
    "title": "Cross-modality interaction for few-shot multispectral object detection with semantic knowledge",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106156",
    "abstract": "Multispectral object detection (MOD), which incorporates additional information from thermal images into object detection (OD) to robustly cope with complex illumination conditions, has garnered significant attention. However, existing MOD methods always demand a considerable amount of annotated data for training. Inspired by the concept of few-shot learning, we propose a novel task called few-shot multispectral object detection (FSMOD) that aims to accomplish MOD using only a few annotated data from each category. Specifically, we first design a cross-modality interaction (CMI) module, which leverages different attention mechanisms to interact with the information from visible and thermal modalities during backbone feature extraction. With the guidance of interaction process, the detector is able to extract modality-specific backbone features with better discrimination. To improve the few-shot learning ability of the detector, we also design a semantic prototype metric (SPM) loss that integrates semantic knowledge, i.e., word embeddings, into the optimization process of embedding space. Semantic knowledge provides stable category representation when visual information is insufficient. Extensive experiments on the customized FSMOD dataset demonstrate that the proposed method achieves state-of-the-art performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000807",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Economics",
      "Feature (linguistics)",
      "Feature learning",
      "Law",
      "Linguistics",
      "Management",
      "Metric (unit)",
      "Modality (human–computer interaction)",
      "Multispectral image",
      "Natural language processing",
      "Object (grammar)",
      "Object detection",
      "Operating system",
      "Operations management",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Process (computing)",
      "Representation (politics)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Lian"
      },
      {
        "surname": "Peng",
        "given_name": "Zongju"
      },
      {
        "surname": "Chen",
        "given_name": "Fen"
      },
      {
        "surname": "Dai",
        "given_name": "Shaosheng"
      },
      {
        "surname": "He",
        "given_name": "Ziqiang"
      },
      {
        "surname": "Liu",
        "given_name": "Kesheng"
      }
    ]
  },
  {
    "title": "Delay-dependent Lurie–Postnikov type Lyapunov–Krasovskii functionals for stability analysis of discrete-time delayed neural networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106195",
    "abstract": "This paper addresses the influence of time-varying delay and nonlinear activation functions with sector restrictions on the stability of discrete-time neural networks. Compared to previous works that mainly focuses on the influence of delay information, this paper devotes to activation nonlinear functions information to help compensate the analysis technique based on Lyapunov–Krasovskii functional (LKF). A class of delay-dependent Lurie–Postnikov type integral terms involving sector constraints of nonlinear activation function is proposed to complement the LKF construction. The less conservative criteria for the stability analysis of discrete-time delayed networks is given by using improved LKF. Numerical examples show that conservatism can be reduced by the delay-dependent integral terms involving nonlinear activation functions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001199",
    "keywords": [
      "Activation function",
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Complement (music)",
      "Complementation",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Discrete time and continuous time",
      "Ecology",
      "Evolutionary biology",
      "Function (biology)",
      "Gene",
      "Machine learning",
      "Mathematics",
      "Nonlinear system",
      "Phenotype",
      "Physics",
      "Quantum mechanics",
      "Stability (learning theory)",
      "Statistics",
      "Type (biology)"
    ],
    "authors": [
      {
        "surname": "Xie",
        "given_name": "Ke-You"
      },
      {
        "surname": "Zhang",
        "given_name": "Chuan-Ke"
      },
      {
        "surname": "Lee",
        "given_name": "Sangmoon"
      },
      {
        "surname": "He",
        "given_name": "Yong"
      },
      {
        "surname": "Liu",
        "given_name": "Yajuan"
      }
    ]
  },
  {
    "title": "Node-personalized multi-graph convolutional networks for recommendation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106169",
    "abstract": "Graph neural networks have revealed powerful potential in ranking recommendation. Existing methods based on bipartite graphs for ranking recommendation mainly focus on homogeneous graphs and usually treat user and item nodes as the same kind of nodes, however, the user–item bipartite graph is always heterogeneous. Additionally, various types of nodes have varying effects on recommendations, and a good node representation can be learned by successfully differentiating the same type of nodes. In this paper, we develop a node-personalized multi-graph convolutional network (NP-MGCN) for ranking recommendation. It consists of a node importance awareness block, a graph construction module, and a node information propagation and aggregation framework. Specifically, a node importance awareness block is proposed to encode nodes using node degree information to highlight the differences between nodes. Subsequently, the Jaccard similarity and co-occurrence matrix fusion graph construction module is devised to acquire user–user and item–item graphs, enriching correlation information between users and between items. Finally, a composite hop node information propagation and aggregation framework, including single-hop and double-hop branches, is designed. The high-order connectivity is used to aggregate heterogeneous information for the single-hop branch, while the multi-hop dependency is utilized to aggregate homogeneous information for the double-hop branch. It makes user and item node embedding more discriminative and integrates the different nodes’ heterogeneity into the model. Experiments on several datasets manifest that NP-MGCN achieves outstanding recommendation performance than existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000935",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Engineering",
      "Graph",
      "Node (physics)",
      "Structural engineering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Tiantian"
      },
      {
        "surname": "Ye",
        "given_name": "Hailiang"
      },
      {
        "surname": "Cao",
        "given_name": "Feilong"
      }
    ]
  },
  {
    "title": "PSA-GNN: An augmented GNN framework with priori subgraph knowledge",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106155",
    "abstract": "Graph neural networks have become the primary graph representation learning paradigm, in which nodes update their embeddings by aggregating messages from their neighbors iteratively. However, current message passing based GNNs exploit the higher-order subgraph information other than 1st-order neighbors insufficiently. In contrast, the long-standing graph research has investigated various subgraphs such as motif, clique, core, and truss that contain important structural information to downstream tasks like node classification, which deserve to be preserved by GNNs. In this work, we propose to use the pre-mined subgraphs as priori knowledge to extend the receptive field of GNNs and enhance their expressive power to go beyond the 1st-order Weisfeiler-Lehman isomorphism test. For that, we introduce a general framework called PSA-GNN (Priori Subgraph Augmented Graph Neural Network), which augments each GNN layer by a pair of parallel convolution layers based on a bipartite graph between nodes and priori subgraphs. PSA-GNN intrinsically builds a hybrid receptive field by incorporating priori subgraphs as neighbors, while the embeddings and weights of subgraphs are trainable. Moreover, PSA-GNN can purify the noisy subgraphs both heuristically before training and deterministically during training based on a novel metric called homogeneity. Experimental results show that PSA-GNN achieves an improved performance compared with state-of-the-art message passing based GNN models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000790",
    "keywords": [
      "A priori and a posteriori",
      "Algorithm",
      "Artificial intelligence",
      "Bipartite graph",
      "Computer science",
      "Epistemology",
      "Graph",
      "Graph isomorphism",
      "Induced subgraph isomorphism problem",
      "Line graph",
      "Message passing",
      "Philosophy",
      "Programming language",
      "Sketch",
      "Subgraph isomorphism problem",
      "Theoretical computer science",
      "Voltage graph"
    ],
    "authors": [
      {
        "surname": "Xue",
        "given_name": "Guotong"
      },
      {
        "surname": "Zhong",
        "given_name": "Ming"
      },
      {
        "surname": "Qian",
        "given_name": "Tieyun"
      },
      {
        "surname": "Li",
        "given_name": "Jianxin"
      }
    ]
  },
  {
    "title": "SCMEA: A stacked co-enhanced model for entity alignment based on multi-aspect information fusion and bidirectional contrastive learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106178",
    "abstract": "Entity alignment refers to discovering the entity pairs with the same realistic meaning in different knowledge graphs. This technology is of great significance for completing and fusing knowledge graphs. Recently, methods based on knowledge representation learning have achieved remarkable achievements in entity alignment. However, most existing approaches do not mine hidden information in the knowledge graph as much as possible. This paper suggests SCMEA, a novel cross-lingual entity alignment framework based on multi-aspect information fusion and bidirectional contrastive learning. SCMEA initially adopts diverse representation learning models to embed multi-aspect information of entities and integrates them into a unified embedding space with an adaptive weighted mechanism to overcome the missing information and the problem of different-aspect information are not uniform. Then, we propose a stacked relation-entity co-enhanced model to further improve the representations of entities, wherein relation representation is modeled using an Entity Collector with Global Entity Attention. Finally, a combined loss function based on improved bidirectional contrastive learning is introduced to optimize model parameters and entity representation, effectively mitigating the hubness problem and accelerating model convergence. We conduct extensive experiments to evaluate the alignment performance of SCMEA. The overall experimental results, ablation studies, and analysis performed on five cross-lingual datasets demonstrate that our model achieves varying degrees of performance improvement and verifies the effectiveness and robustness of the model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001023",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Embedding",
      "Feature learning",
      "Fusion",
      "Fusion mechanism",
      "Gene",
      "Graph",
      "Knowledge graph",
      "Law",
      "Linguistics",
      "Lipid bilayer fusion",
      "Machine learning",
      "Natural language processing",
      "Philosophy",
      "Political science",
      "Politics",
      "Relation (database)",
      "Representation (politics)",
      "Robustness (evolution)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Yunfeng"
      },
      {
        "surname": "Zhu",
        "given_name": "Cui"
      },
      {
        "surname": "Zhu",
        "given_name": "Wenjun"
      },
      {
        "surname": "Li",
        "given_name": "Hongyang"
      }
    ]
  },
  {
    "title": "How to evaluate uncertainty estimates in machine learning for regression?",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106203",
    "abstract": "As neural networks become more popular, the need for accompanying uncertainty estimates increases. There are currently two main approaches to test the quality of these estimates. Most methods output a density. They can be compared by evaluating their loglikelihood on a test set. Other methods output a prediction interval directly. These methods are often tested by examining the fraction of test points that fall inside the corresponding prediction intervals. Intuitively, both approaches seem logical. However, we demonstrate through both theoretical arguments and simulations that both ways of evaluating the quality of uncertainty estimates have serious flaws. Firstly, both approaches cannot disentangle the separate components that jointly create the predictive uncertainty, making it difficult to evaluate the quality of the estimates of these components. Specifically, the quality of a confidence interval cannot reliably be tested by estimating the performance of a prediction interval. Secondly, the loglikelihood does not allow a comparison between methods that output a prediction interval directly and methods that output a density. A better loglikelihood also does not necessarily guarantee better prediction intervals, which is what the methods are often used for in practice. Moreover, the current approach to test prediction intervals directly has additional flaws. We show why testing a prediction or confidence interval on a single test set is fundamentally flawed. At best, marginal coverage is measured, implicitly averaging out overconfident and underconfident predictions. A much more desirable property is pointwise coverage, requiring the correct coverage for each prediction. We demonstrate through practical examples that these effects can result in favouring a method, based on the predictive uncertainty, that has undesirable behaviour of the confidence or prediction intervals. Finally, we propose a simulation-based testing approach that addresses these problems while still allowing easy comparison between different methods. This approach can be used for the development of new uncertainty quantification methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001278",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Confidence interval",
      "Coverage probability",
      "Epistemology",
      "Interval (graph theory)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Philosophy",
      "Pointwise",
      "Prediction interval",
      "Programming language",
      "Quality (philosophy)",
      "Regression",
      "Set (abstract data type)",
      "Statistics",
      "Test set",
      "Uncertainty quantification"
    ],
    "authors": [
      {
        "surname": "Sluijterman",
        "given_name": "Laurens"
      },
      {
        "surname": "Cator",
        "given_name": "Eric"
      },
      {
        "surname": "Heskes",
        "given_name": "Tom"
      }
    ]
  },
  {
    "title": "Efficient spiking neural network design via neural architecture search",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106172",
    "abstract": "Spiking neural networks (SNNs) are brain-inspired models that utilize discrete and sparse spikes to transmit information, thus having the property of energy efficiency. Recent advances in learning algorithms have greatly improved SNN performance due to the automation of feature engineering. While the choice of neural architecture plays a significant role in deep learning, the current SNN architectures are mainly designed manually, which is a time-consuming and error-prone process. In this paper, we propose a spiking neural architecture search (NAS) method that can automatically find efficient SNNs. To tackle the challenge of long search time faced by SNNs when utilizing NAS, the proposed NAS encodes candidate architectures in a branchless spiking supernet which significantly reduces the computation requirements in the search process. Considering that real-world tasks prefer efficient networks with optimal accuracy under a limited computational budget, we propose a Synaptic Operation (SynOps)-aware optimization to automatically find the computationally efficient subspace of the supernet. Experimental results show that, in less search time, our proposed NAS can find SNNs with higher accuracy and lower computational cost than state-of-the-art SNNs. We also conduct experiments to validate the search process and the trade-off between accuracy and computational cost.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000960",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Feature (linguistics)",
      "Linguistics",
      "Machine learning",
      "Operating system",
      "Philosophy",
      "Process (computing)",
      "Spiking neural network"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Jiaqi"
      },
      {
        "surname": "Liu",
        "given_name": "Qianhui"
      },
      {
        "surname": "Zhang",
        "given_name": "Malu"
      },
      {
        "surname": "Feng",
        "given_name": "Lang"
      },
      {
        "surname": "Ma",
        "given_name": "De"
      },
      {
        "surname": "Li",
        "given_name": "Haizhou"
      },
      {
        "surname": "Pan",
        "given_name": "Gang"
      }
    ]
  },
  {
    "title": "Hierarchical matching and reasoning for multi-query image retrieval",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106200",
    "abstract": "As a promising field, Multi-Query Image Retrieval (MQIR) aims at searching for the semantically relevant image given multiple region-specific text queries. Existing works mainly focus on a single-level similarity between image regions and text queries, which neglect the hierarchical guidance of multi-level similarities and result in incomplete alignments. Besides, the high-level semantic correlations that intrinsically connect different region–query pairs are rarely considered. To address above limitations, we propose a novel Hierarchical Matching and Reasoning Network (HMRN) for MQIR. It disentangles MQIR into three hierarchical semantic representations, which is responsible to capture fine-grained local details, contextual global scopes, and high-level inherent correlations. HMRN consists of two modules: Scalar-based Matching (SM) module and Vector-based Reasoning (VR) module. Specifically, the SM module characterizes the multi-level alignment similarity, which consists of a fine-grained local-level similarity and a context-aware global-level similarity. Afterwards, the VR module is developed to excavate the potential semantic correlations among multiple region–query pairs, which further explores the high-level reasoning similarity. Finally, these three-level similarities are aggregated into a joint similarity space to form the ultimate similarity. Extensive experiments on the benchmark dataset demonstrate that our HMRN substantially surpasses the current state-of-the-art methods. For instance, compared with the existing best method Drill-down, the metric R@1 in the last round is improved by 23.4%. Our source codes will be released at https://github.com/LZH-053/HMRN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001242",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Data mining",
      "Economics",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Image retrieval",
      "Information retrieval",
      "Matching (statistics)",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Semantic similarity",
      "Similarity (geometry)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Ji",
        "given_name": "Zhong"
      },
      {
        "surname": "Li",
        "given_name": "Zhihao"
      },
      {
        "surname": "Zhang",
        "given_name": "Yan"
      },
      {
        "surname": "Wang",
        "given_name": "Haoran"
      },
      {
        "surname": "Pang",
        "given_name": "Yanwei"
      },
      {
        "surname": "Li",
        "given_name": "Xuelong"
      }
    ]
  },
  {
    "title": "Multi-node knowledge graph assisted distributed fault detection for large-scale industrial processes based on graph attention network and bidirectional LSTMs",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106210",
    "abstract": "Modern industrial processes are characterized by extensive, multiple operation units, and strong coupled correlation of subsystems. Fault detection of large-scale processes is still a challenging problem, especially for tandem plant-wide processes in multiple fields such as water treatment process. In this paper, a novel distributed graph attention network-bidirectional long short-term memory (D-GATBLSTM) fault detection model is proposed for large-scale industrial processes. Firstly, a multi-node knowledge graph (MNKG) is constructed using a joint data and knowledge driven strategy. Secondly, for large-scale industrial process, a global feature extractor of graph attention networks (GATs) is constructed, on the basis of which, sub-blocks are decomposed based on MNKG. Then, local feature extractors of bidirectional long short-term memory (Bi-LSTM) for each sub-block are constructed, in which correlations among multiple sub-blocks are considered. Finally, a multi-subblock fusion collaborative prediction model is constructed and the comprehensive fault detection results are given by the grid search method. The effectiveness of our D-GATBLSTM is exemplified in a secure water treatment process case, where it outperforms baseline models compared, with 27% improvement in precision, 15% increase in recall, and overall F -score enhancement of 0.22.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001345",
    "keywords": [
      "Actuator",
      "Artificial intelligence",
      "Computer science",
      "Data mining",
      "Fault detection and isolation",
      "Graph",
      "Operating system",
      "Pattern recognition (psychology)",
      "Process (computing)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Qing"
      },
      {
        "surname": "Wang",
        "given_name": "Yangfan"
      },
      {
        "surname": "Dong",
        "given_name": "Jie"
      },
      {
        "surname": "Zhang",
        "given_name": "Chi"
      },
      {
        "surname": "Peng",
        "given_name": "Kaixiang"
      }
    ]
  },
  {
    "title": "Enhancing adversarial attacks with resize-invariant and logical ensemble",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106194",
    "abstract": "In black-box scenarios, most transfer-based attacks usually improve the transferability of adversarial examples by optimizing the gradient calculation of the input image. Unfortunately, since the gradient information is only calculated and optimized for each pixel point in the image individually, the generated adversarial examples tend to overfit the local model and have poor transferability to the target model. To tackle the issue, we propose a resize-invariant method (RIM) and a logical ensemble transformation method (LETM) to enhance the transferability of adversarial examples. Specifically, RIM is inspired by the resize-invariant property of Deep Neural Networks (DNNs). The range of resizable pixel is first divided into multiple intervals, and then the input image is randomly resized and padded within each interval. Finally, LETM performs logical ensemble of multiple images after RIM transformation to calculate the final gradient update direction. The proposed method adequately considers the information of each pixel in the image and the surrounding pixels. The probability of duplication of image transformations is minimized and the overfitting effect of adversarial examples is effectively mitigated. Numerous experiments on the ImageNet dataset show that our approach outperforms other advanced methods and is capable of generating more transferable adversarial examples.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001187",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Contextual image classification",
      "Deep learning",
      "Gene",
      "Image (mathematics)",
      "Invariant (physics)",
      "Logit",
      "MNIST database",
      "Machine learning",
      "Mathematical physics",
      "Mathematics",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Pixel",
      "Transferability",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Shao",
        "given_name": "Yanling"
      },
      {
        "surname": "Zhang",
        "given_name": "Yuzhi"
      },
      {
        "surname": "Dong",
        "given_name": "Wenyong"
      },
      {
        "surname": "Zhang",
        "given_name": "Qikun"
      },
      {
        "surname": "Shan",
        "given_name": "Pingping"
      },
      {
        "surname": "Guo",
        "given_name": "Junying"
      },
      {
        "surname": "Xu",
        "given_name": "Hairui"
      }
    ]
  },
  {
    "title": "Quasi-synchronization for variable-order fractional complex dynamical networks with hybrid delay-dependent impulses",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106161",
    "abstract": "This paper focuses on addressing the problem of quasi-synchronization in heterogeneous variable-order fractional complex dynamical networks (VFCDNs) with hybrid delay-dependent impulses. Firstly, a mathematics model of VFCDNs with short memory is established under multi-weighted networks and mismatched parameters, which is more diverse and practical. Secondly, under the framework of variable-order fractional derivative, a novel fractional differential inequality has been proposed to handle the issue of quasi-synchronization with hybrid delay-dependent impulses. Additionally, the quasi-synchronization criterion for VFCDNs is developed using differential inclusion theory and Lyapunov method. Finally, the practicality and feasibility of this theoretical analysis are demonstrated through numerical examples.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000856",
    "keywords": [
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Differential inclusion",
      "Fractional calculus",
      "Lyapunov function",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Nonlinear system",
      "Physics",
      "Quantum mechanics",
      "Synchronization (alternating current)",
      "Topology (electrical circuits)",
      "Variable (mathematics)"
    ],
    "authors": [
      {
        "surname": "Wei",
        "given_name": "Chen"
      },
      {
        "surname": "Wang",
        "given_name": "Xiaoping"
      },
      {
        "surname": "Ren",
        "given_name": "Fangmin"
      },
      {
        "surname": "Zeng",
        "given_name": "Zhigang"
      }
    ]
  },
  {
    "title": "Noncompact uniform universal approximation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106181",
    "abstract": "The universal approximation theorem is generalised to uniform convergence on the (noncompact) input space R n . All continuous functions that vanish at infinity can be uniformly approximated by neural networks with one hidden layer, for all activation functions φ that are continuous, nonpolynomial, and asymptotically polynomial at ± ∞ . When φ is moreover bounded, we exactly determine which functions can be uniformly approximated by neural networks, with the following unexpected results. Let N φ l ( R n ) ¯ denote the vector space of functions that are uniformly approximable by neural networks with l hidden layers and n inputs. For all n and all l ≥ 2 , N φ l ( R n ) ¯ turns out to be an algebra under the pointwise product. If the left limit of φ differs from its right limit (for instance, when φ is sigmoidal) the algebra N φ l ( R n ) ¯ ( l ≥ 2 ) is independent of φ and l , and equals the closed span of products of sigmoids composed with one-dimensional projections. If the left limit of φ equals its right limit, N φ l ( R n ) ¯ ( l ≥ 1 ) equals the (real part of the) commutative resolvent algebra, a C*-algebra which is used in mathematical approaches to quantum theory. In the latter case, the algebra is independent of l ≥ 1 , whereas in the former case N φ 2 ( R n ) ¯ is strictly bigger than N φ 1 ( R n ) ¯ .",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001059",
    "keywords": [
      "Applied mathematics",
      "Artificial intelligence",
      "Computer science",
      "Mathematics"
    ],
    "authors": [
      {
        "surname": "van Nuland",
        "given_name": "Teun D.H."
      }
    ]
  },
  {
    "title": "An ADMM-LSTM framework for short-term load forecasting",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106150",
    "abstract": "Accurate short-term load forecasting (STLF) is crucial for maintaining reliable and efficient operations within power systems. With the continuous increase in volume and variety of energy data provided by renewables, electric vehicles and other sources, long short-term memory (LSTM) has emerged as an attractive approach for STLF due to its superiorities in extracting the dynamic temporal information. However, traditional LSTM training methods rely on stochastic gradient methods that have several limitations. This paper presents an innovative LSTM optimization framework via the alternating direction method of multipliers (ADMM) for STLF, dubbed ADMM-LSTM. Explicitly, we train the LSTM network distributedly by the ADMM algorithm. More specifically, we introduce a novel approach to update the parameters in the ADMM-LSTM framework, using a backward-forward order, significantly reducing computational time. Additionally, within the proposed framework, the solution to each subproblem is achieved by utilizing either the proximal point algorithm or local linear approximation, preventing the need for supplementary numerical solvers. This approach confers several advantages, including avoiding issues associated with exploding or vanishing gradients, thanks to the inherent gradient-free characteristics of ADMM-LSTM. Furthermore, we offer a comprehensive theoretical analysis that elucidates the convergence properties inherent to the ADMM-LSTM framework. This analysis provides a deeper understanding of the algorithm’s convergence behavior. Lastly, the efficacy of our method is substantiated through a series of experiments conducted on two publicly available datasets. The experimental results demonstrate the superior performance of our approach when compared to existing methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000741",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Machine learning",
      "Physics",
      "Quantum mechanics",
      "Term (time)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Shuo"
      },
      {
        "surname": "Kong",
        "given_name": "Zhengmin"
      },
      {
        "surname": "Huang",
        "given_name": "Tao"
      },
      {
        "surname": "Du",
        "given_name": "Yang"
      },
      {
        "surname": "Xiang",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "TCDformer: A transformer framework for non-stationary time series forecasting based on trend and change-point detection",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106196",
    "abstract": "Although time series prediction models based on Transformer architecture have achieved significant advances, concerns have arisen regarding their performance with non-stationary real-world data. Traditional methods often use stabilization techniques to boost predictability, but this often results in the loss of non-stationarity, notably underperforming when tackling major events in practical applications. To address this challenge, this research introduces an innovative method named TCDformer (Trend and Change-point Detection Transformer). TCDformer employs a unique strategy, initially encoding abrupt changes in non-stationary time series using the local linear scaling approximation (LLSA) module. The reconstructed contextual time series is then decomposed into trend and seasonal components. The final prediction results are derived from the additive combination of a multilayer perceptron (MLP) for predicting trend components and wavelet attention mechanisms for seasonal components. Comprehensive experimental results show that on standard time series prediction datasets, TCDformer significantly surpasses existing benchmark models in terms of performance, reducing MSE by 47.36% and MAE by 31.12%. This approach offers an effective framework for managing non-stationary time series, achieving a balance between performance and interpretability, making it especially suitable for addressing non-stationarity challenges in real-world scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001205",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Data mining",
      "Interpretability",
      "Machine learning",
      "Mathematics",
      "Multilayer perceptron",
      "Paleontology",
      "Physics",
      "Predictability",
      "Quantum mechanics",
      "Series (stratigraphy)",
      "Statistics",
      "Time series",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Wan",
        "given_name": "Jiashan"
      },
      {
        "surname": "Xia",
        "given_name": "Na"
      },
      {
        "surname": "Yin",
        "given_name": "Yutao"
      },
      {
        "surname": "Pan",
        "given_name": "Xulei"
      },
      {
        "surname": "Hu",
        "given_name": "Jin"
      },
      {
        "surname": "Yi",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Graph Neural Network contextual embedding for Deep Learning on tabular data",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106180",
    "abstract": "All industries are trying to leverage Artificial Intelligence (AI) based on their existing big data which is available in so called tabular form, where each record is composed of a number of heterogeneous continuous and categorical columns also known as features. Deep Learning (DL) has constituted a major breakthrough for AI in fields related to human skills like natural language processing, but its applicability to tabular data has been more challenging. More classical Machine Learning (ML) models like tree-based ensemble ones usually perform better. This paper presents a novel DL model using Graph Neural Network (GNN) more specifically Interaction Network (IN), for contextual embedding and modeling interactions among tabular features. Its results outperform those of a recently published survey with DL benchmark based on seven public datasets, also achieving competitive results when compared to boosted-tree solutions.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024001047",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Big data",
      "Categorical variable",
      "Computer science",
      "Data mining",
      "Deep learning",
      "Embedding",
      "Geodesy",
      "Geography",
      "Graph",
      "Leverage (statistics)",
      "Machine learning",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Villaizán-Vallelado",
        "given_name": "Mario"
      },
      {
        "surname": "Salvatori",
        "given_name": "Matteo"
      },
      {
        "surname": "Carro",
        "given_name": "Belén"
      },
      {
        "surname": "Sanchez-Esguevillas",
        "given_name": "Antonio Javier"
      }
    ]
  },
  {
    "title": "Online estimation of objective function for continuous-time deterministic systems",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106116",
    "abstract": "We developed two online data-driven methods for estimating an objective function in continuous-time linear and nonlinear deterministic systems. The primary focus addressed the challenge posed by unknown input dynamics (control mapping function) in the expert system, a critical element for an online solution of the problem. Our methods leverage both the learner’s and expert’s data for effective problem-solving. The first approach, which is model-free, estimates the expert’s policy and integrates it into the learner agent to approximate the objective function associated with the optimal policy. The second approach estimates the input dynamics from the learner’s data and combines it with the expert’s input-state observations to tackle the objective function estimation problem. Compared to other methods for deterministic systems that rely on both the learner’s and expert’s data, our approaches offer reduced complexity by eliminating the need to estimate an optimal policy after each objective function update. We conduct a convergence analysis of the estimation techniques using Lyapunov-based methods. Numerical experiments validate the effectiveness of our developed methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000303",
    "keywords": [
      "Biology",
      "Computer science",
      "Convergence (economics)",
      "Data mining",
      "Economic growth",
      "Economics",
      "Evolutionary biology",
      "Function (biology)",
      "Leverage (statistics)",
      "Lyapunov function",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Nonlinear system",
      "Physics",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Asl",
        "given_name": "Hamed Jabbari"
      },
      {
        "surname": "Uchibe",
        "given_name": "Eiji"
      }
    ]
  },
  {
    "title": "Invariant feature based label correction for DNN when Learning with Noisy Labels",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106137",
    "abstract": "Learning with Noisy Labels (LNL) methods have been widely studied in recent years, which aims to improve the performance of Deep Neural Networks (DNNs) when the training dataset contains incorrectly annotated labels. Popular existing LNL methods rely on semantic features extracted by the DNN to detect and mitigate label noise. However, these extracted features are often spurious and contain unstable correlations with the label across different environments (domains), which can occasionally lead to incorrect prediction and compromise the efficacy of LNL methods. To mitigate this insufficiency, we propose Invariant Feature based Label Correction (IFLC), which reduces spurious features and accurately utilizes the learned invariant features that contain stable correlation to correct label noise. To the best of our knowledge, this is the first attempt to mitigate the issue of spurious features for LNL methods. IFLC consists of two critical processes: The Label Disturbing (LD) process and the Representation Decorrelation (RD) process. The LD process aims to encourage DNN to attain stable performance across different environments, thus reducing the captured spurious features. The RD process strengthens independence between each dimension of the representation vector, thus enabling accurate utilization of the learned invariant features for label correction. We then utilize robust linear regression for the feature representation to conduct label correction. We evaluated the effectiveness of our proposed method and compared it with state-of-the-art (sota) LNL methods on four benchmark datasets, CIFAR-10, CIFAR-100, Animal-10N, and Clothing1M. The experimental results show that our proposed method achieved comparable or even better performance than the existing sota methods. The source codes are available at https://github.com/yangbo1973/IFLC.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000534",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Decorrelation",
      "Feature (linguistics)",
      "Feature vector",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Invariant (physics)",
      "Linguistics",
      "Machine learning",
      "Mathematical physics",
      "Mathematics",
      "Noise (video)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Spurious relationship"
    ],
    "authors": [
      {
        "surname": "Deng",
        "given_name": "Lihui"
      },
      {
        "surname": "Yang",
        "given_name": "Bo"
      },
      {
        "surname": "Kang",
        "given_name": "Zhongfeng"
      },
      {
        "surname": "Xiang",
        "given_name": "Yanping"
      }
    ]
  },
  {
    "title": "Embedding-Based Entity Alignment of Cross-Lingual Temporal Knowledge Graphs",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106143",
    "abstract": "Entity alignment aims to construct a complete knowledge graph (KG) by matching the same entities in multi-source KGs. Existing researches on entity alignment mainly focuses on static multi-relational data in knowledge graphs. However, the relationships or attributes between entities often possess temporal characteristics as well. Neglecting these temporal characteristics can frequently lead to alignment errors. Compared to studying entity alignment in temporal knowledge graphs, there are relatively few efforts on entity alignment in cross-lingual temporal knowledge graphs. Therefore, in this paper, we put forward an entity alignment method for cross-lingual temporal knowledge graphs, namely CTEA. Based on GCN and TransE, CTEA combines entity embeddings, relation embeddings and attribute embeddings to design a joint embedding model, which is more conducive to generating transferable entity embedding. In the meantime, the distance calculation between elements and the similarity calculation of entity pairs are combined to enhance the reliability of cross-lingual entity alignment. Experiments shows that the proposed CTEA model improves Hits@m and MRR by about 0.8∼2.4 percentage points compared with the latest methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000595",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Construct (python library)",
      "Embedding",
      "Entity linking",
      "Graph",
      "Image (mathematics)",
      "Information retrieval",
      "Knowledge base",
      "Knowledge graph",
      "Matching (statistics)",
      "Mathematics",
      "Natural language processing",
      "Programming language",
      "Similarity (geometry)",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Bai",
        "given_name": "Luyi"
      },
      {
        "surname": "Li",
        "given_name": "Nan"
      },
      {
        "surname": "Li",
        "given_name": "Guishun"
      },
      {
        "surname": "Zhang",
        "given_name": "Ziyi"
      },
      {
        "surname": "Zhu",
        "given_name": "Lin"
      }
    ]
  },
  {
    "title": "What is behind the meta-learning initialization of adaptive filter? — A naive method for accelerating convergence of adaptive multichannel active noise control",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106145",
    "abstract": "Active noise control (ANC) is a typical signal-processing technique that has recently been utilized extensively to combat the urban noise problem. Although numerous advanced adaptive algorithms have been devised to enhance noise reduction performance, few of them have been implemented in actual ANC products due to their high computational complexity and slow convergence. With the rapid development of deep learning technology, Meta-learning-based initialization appears to become an efficient and cost-effective method for accelerating the convergence of adaptive algorithms. However, few dedicated Meta-learning algorithms exist for adaptive signal processing applications, particularly multichannel active noise control (MCANC). Hence, we proposed a modified Model-Agnostic Meta-Learning (MAML) initialization for the MCANC system. 1 1 The code of the proposed method is available on https://github.com/ShiDongyuan/Meta_Learning_for_MCANC. Additional theatrical research reveals that the nature of MAML, when applied to signal processing, is the expectation of a weight-sum gradient. Based on this discovery, we devised the Monte-Carlo Gradient Meta-learning (MCGM) algorithm, which employed a more straightforward procedure to accomplish the same performance as the Modified MAML algorithm. Furthermore, the numerical simulation of ANC using raw noise samples on measured paths validates the efficacy of the proposed methods in accelerating the convergence of the multichannel-filtered reference least mean square algorithm (McFxLMS).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000698",
    "keywords": [
      "Active noise control",
      "Adaptive filter",
      "Adaptive learning",
      "Algorithm",
      "Artificial intelligence",
      "Computer hardware",
      "Computer science",
      "Computer vision",
      "Convergence (economics)",
      "Digital signal processing",
      "Economic growth",
      "Economics",
      "Engineering",
      "Filter (signal processing)",
      "Image (mathematics)",
      "Initialization",
      "Least mean squares filter",
      "Machine learning",
      "Meta learning (computer science)",
      "Noise (video)",
      "Noise reduction",
      "Programming language",
      "Signal processing",
      "Systems engineering",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Shi",
        "given_name": "Dongyuan"
      },
      {
        "surname": "Gan",
        "given_name": "Woon-seng"
      },
      {
        "surname": "Shen",
        "given_name": "Xiaoyi"
      },
      {
        "surname": "Luo",
        "given_name": "Zhengding"
      },
      {
        "surname": "Ji",
        "given_name": "Junwei"
      }
    ]
  },
  {
    "title": "psoResNet: An improved PSO-based residual network search algorithm",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106104",
    "abstract": "Neural Architecture Search (NAS) methods are widely employed to address the time-consuming and costly challenges associated with manual operation and design of deep convolutional neural networks (DCNNs). Nonetheless, prevailing methods still encounter several pressing obstacles, including limited network architecture design, excessively lengthy search periods, and insufficient utilization of the search space. In light of these concerns, this study proposes an optimization strategy for residual networks that leverages an enhanced Particle swarm optimization algorithm. Primarily, low-complexity residual architecture block is employed as the foundational unit for architecture exploration, facilitating a more diverse investigation into network architectures while minimizing parameters. Additionally, we employ a depth initialization strategy to confine the search space within a reasonable range, thereby mitigating unnecessary particle exploration. Lastly, we present a novel approach for computing particle differences and updating velocity mechanisms to enhance the exploration of updated trajectories. This method significantly contributes to the improved utilization of the search space and the augmentation of particle diversity. Moreover, we constructed a crime-dataset comprising 13 classes to assess the effectiveness of the proposed algorithm. Experimental results demonstrate that our algorithm can design lightweight networks with superior classification performance on both benchmark datasets and the crime-dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000182",
    "keywords": [
      "Aerospace engineering",
      "Algorithm",
      "Architecture",
      "Art",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Block (permutation group theory)",
      "Computer science",
      "Convolutional neural network",
      "Engineering",
      "Geodesy",
      "Geography",
      "Geometry",
      "Initialization",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Particle swarm optimization",
      "Programming language",
      "Range (aeronautics)",
      "Residual",
      "Search algorithm",
      "Visual arts"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Dianwei"
      },
      {
        "surname": "Zhai",
        "given_name": "Leilei"
      },
      {
        "surname": "Fang",
        "given_name": "Jie"
      },
      {
        "surname": "Li",
        "given_name": "Yuanqing"
      },
      {
        "surname": "Xu",
        "given_name": "Zhijie"
      }
    ]
  },
  {
    "title": "Causal Disentanglement Domain Generalization for time-series signal fault diagnosis",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106099",
    "abstract": "Domain generalization-based fault diagnosis (DGFD) presents significant prospects for recognizing faults without the accessibility of the target domain. Previous DGFD methods have achieved significant progress; however, there are some limitations. First, most DGFG methods statistically model the dependence between time-series data and labels, and they are superficial descriptions to the actual data-generating process. Second, most of the existing DGFD methods are only verified on vibrational time-series datasets, which is insufficient to show the potential of domain generalization in the fault diagnosis area. In response to the above issues, this paper first proposes a DGFD method named Causal Disentanglement Domain Generalization (CDDG), which can reestablish the data-generating process by disentangling time-series data into the causal factors (fault-related representation) and no-casual factors (domain-related representation) with a structural causal model. Specifically, in CDDG, causal aggregation loss is designed to separate the unobservable causal and non-causal factors. Meanwhile, the reconstruction loss is proposed to ensure the information completeness of the disentangled factors. We also introduce a redundancy reduction loss to learn efficient features. The proposed CDDG is verified on five cross-machine vibrational fault diagnosis cases and three cross-environment acoustical anomaly detection cases by comparing it with eight state-of-the-art (SOTA) DGFD methods. We publicize the open-source time-series DGFD Benchmark containing CDDG and the eight SOTA methods. The code repository will be available at https://github.com/ShaneSpace/DGFDBenchmark.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000133",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Econometrics",
      "Fault (geology)",
      "Generalization",
      "Geodesy",
      "Geography",
      "Geology",
      "Law",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Process (computing)",
      "Representation (politics)",
      "Seismology",
      "Series (stratigraphy)",
      "Time domain",
      "Time series",
      "Unobservable"
    ],
    "authors": [
      {
        "surname": "Jia",
        "given_name": "Linshan"
      },
      {
        "surname": "Chow",
        "given_name": "Tommy W.S."
      },
      {
        "surname": "Yuan",
        "given_name": "Yixuan"
      }
    ]
  },
  {
    "title": "Dynamic decomposition graph convolutional neural network for SSVEP-based brain–computer interface",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.029",
    "abstract": "The SSVEP-based paradigm serves as a prevalent approach in the realm of brain–computer interface (BCI). However, the processing of multi-channel electroencephalogram (EEG) data introduces challenges due to its non-Euclidean characteristic, necessitating methodologies that account for inter-channel topological relations. In this paper, we introduce the Dynamic Decomposition Graph Convolutional Neural Network (DDGCNN) designed for the classification of SSVEP EEG signals. Our approach incorporates layerwise dynamic graphs to address the oversmoothing issue in Graph Convolutional Networks (GCNs), employing a dense connection mechanism to mitigate the gradient vanishing problem. Furthermore, we enhance the traditional linear transformation inherent in GCNs with graph dynamic fusion, thereby elevating feature extraction and adaptive aggregation capabilities. Our experimental results demonstrate the effectiveness of proposed approach in learning and extracting features from EEG topological structure. The results shown that DDGCNN outperforms other state-of-the-art (SOTA) algorithms reported on two datasets (Dataset 1: 54 subjects, 4 targets, 2 sessions; Dataset 2: 35 subjects, 40 targets). Additionally, we showcase the implementation of DDGCNN in the context of synchronized BCI robotic fish control. This work represents a significant advancement in the field of EEG signal processing for SSVEP-based BCIs. Our proposed method processes SSVEP time domain signals directly as an end-to-end system, making it easy to deploy. The code is available at https://github.com/zshubin/DDGCNN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007360",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Brain–computer interface",
      "Bubble",
      "Computer science",
      "Convolutional neural network",
      "Decomposition",
      "Ecology",
      "Electroencephalography",
      "Graph",
      "Interface (matter)",
      "Maximum bubble pressure method",
      "Neuroscience",
      "Parallel computing",
      "Pattern recognition (psychology)",
      "Psychology",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Shubin"
      },
      {
        "surname": "An",
        "given_name": "Dong"
      },
      {
        "surname": "Liu",
        "given_name": "Jincun"
      },
      {
        "surname": "Chen",
        "given_name": "Jiannan"
      },
      {
        "surname": "Wei",
        "given_name": "Yaoguang"
      },
      {
        "surname": "Sun",
        "given_name": "Fuchun"
      }
    ]
  },
  {
    "title": "Tradeoff analysis between time cost and energy cost for fixed-time synchronization of discontinuous neural networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106118",
    "abstract": "This article focuses on the tradeoff analysis between time and energy costs for fixed-time synchronization (FXTS) of discontinuous neural networks (DNNs) with time-varying delays and mismatched parameters. First, a more comprehensive lemma is systematically established to study fixed-time stability, which is less conservative than those in most current results. Besides, theoretical proof has proven that the upper bounds of the settling time (ST) in this article are more accurate compared to existing results. Second, on the grounds of the new fixed-time stability lemma, fixed-time synchronization problem for discontinuous neural networks with time-varying delays and mismatched parameters is explored, and sufficient conditions for fixed-time synchronization are obtained. Further, the upper bounds of energy cost during the synchronization process are estimated. Third, in order to achieve a balance between time cost and energy cost, the genetic algorithm is utilized to find the satisfactory control parameter. Finally, a numerical example is provided to verify the theoretical analysis’s correctness and the control mechanism’s feasibility.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000327",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Control (management)",
      "Control engineering",
      "Control theory (sociology)",
      "Correctness",
      "Ecology",
      "Energy (signal processing)",
      "Engineering",
      "Lemma (botany)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Poaceae",
      "Settling time",
      "Stability (learning theory)",
      "Statistics",
      "Step response",
      "Synchronization (alternating current)",
      "Upper and lower bounds"
    ],
    "authors": [
      {
        "surname": "Kang",
        "given_name": "Qiaokun"
      },
      {
        "surname": "Ren",
        "given_name": "Guoquan"
      },
      {
        "surname": "Gan",
        "given_name": "Qintao"
      },
      {
        "surname": "Li",
        "given_name": "Ruihong"
      },
      {
        "surname": "Meng",
        "given_name": "Mingqiang"
      }
    ]
  },
  {
    "title": "Bidirectional visual-tactile cross-modal generation using latent feature space flow model",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.042",
    "abstract": "Inspired by visual-tactile cross-modal bidirectional mapping of the human brain, this paper introduces a novel approach to bidirectional mapping between visual and tactile data, an area not fully explored in the predominantly unidirectional existing studies. First, we adopt separate Variational AutoEncoder (VAE) models for visual and tactile data. Furthermore, we introduce a conditional flow model built on the VAE latent feature space, enabling cross-modal bidirectional mapping between visual and tactile data using one model. The experimental results show that our method achieves excellent performance in terms of the similarity between the generated data and the original data (Structural Similarity Index (SSIM) of visual data: 0.58, SSIM of tactile data: 0.80), the classification accuracy on generated data (visual data: 91.60%, tactile data: 88.05%), and the zero-shot classification accuracy between generated data and language (visual data: 44.49%, tactile data: 45.03%). To the best of our knowledge, the method proposed in this paper is the first one to utilize a single model to achieve bidirectional mapping between visual and tactile data. Our model and code will be made public after the acceptance of the paper.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007499",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Feature vector",
      "Image (mathematics)",
      "Linguistics",
      "Modal",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Polymer chemistry",
      "Similarity (geometry)"
    ],
    "authors": [
      {
        "surname": "Fang",
        "given_name": "Yu"
      },
      {
        "surname": "Zhang",
        "given_name": "Xuehe"
      },
      {
        "surname": "Xu",
        "given_name": "Wenqiang"
      },
      {
        "surname": "Liu",
        "given_name": "Gangfeng"
      },
      {
        "surname": "Zhao",
        "given_name": "Jie"
      }
    ]
  },
  {
    "title": "EMAT: Efficient feature fusion network for visual tracking via optimized multi-head attention",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106110",
    "abstract": "The tracking methods based on Transformer have shown great potential in visual tracking and achieved significant tracking performance. The traditional transformer based feature fusion network divides a whole feature map into multiple image patches as its inputs, and then directly processes them in parallel, which will occupy a lot of computing resources and affect the computing efficiency of multi-head attention. In this paper, we design a novel feature fusion network with optimized multi-head attention in encoder and decoder architecture based on Transformer. The designed feature fusion network preprocess the input features and change the calculations of multi-head attention by using both the efficient multi-head self-attention module and efficient multi-head spatial reduction attention module. The two modules can reduce the influence of irrelevant background information, enhance the representation ability of template features and search region features, and greatly reduce the computational complexity. We propose a novel Transformer tracking method (named EMAT) based on the designed feature fusion network. The proposed EMAT is evaluated on seven challenging tracking benchmarks to demonstrate its superiority, including LaSOT, GOT-10k, TrackingNet, UAV123, VOT2018, NfS and VOT-RGBT2019. The proposed tracker achieves well tracking performance, and obtains precision score of 89.0% on UAV123, AUC score of 64.6% on LaSOT, EAO score of 34.8% on VOT-RGBT2019, which outperforms most advanced trackers. EMAT runs at a real-time speed of about 35 FPS during tracking.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000248",
    "keywords": [
      "Artificial intelligence",
      "BitTorrent tracker",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Encoder",
      "Engineering",
      "Eye tracking",
      "Feature (linguistics)",
      "Feature extraction",
      "Linguistics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Jun"
      },
      {
        "surname": "Lai",
        "given_name": "Changwang"
      },
      {
        "surname": "Wang",
        "given_name": "Yuanyun"
      },
      {
        "surname": "Zhang",
        "given_name": "Wenshuang"
      }
    ]
  },
  {
    "title": "CONet: Crowd and occlusion-aware network for occluded human pose estimation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106109",
    "abstract": "Human pose estimation has numerous applications in motion recognition, virtual reality, human–computer interaction, and other related fields. However, multi-person pose estimation in crowded and occluded scenes is challenging. One major issue about the current top-down human pose estimation approaches is that they are limited to predicting the pose of a single person, even when the bounding box contains multiple individuals. To address this problem, we propose a novel Crowd and Occlusion-aware Network (CONet) using a divide-and-conquer strategy. Our approach includes a Crowd and Occlusion-aware Head (COHead) which estimates the pose of both the occluder and the occluded person using two separate branches. We also use the attention mechanism to guide the branches for differentiated learning, aiming to improve feature representation. Additionally, we propose a novel interference point loss to enhance the model’s anti-interference ability. Our CONet is simple yet effective, and it outperforms the state-of-the-art model by +1.6 AP, achieving 71.6 AP on CrowdPose. Our proposed model has achieved state-of-the-art results on the CrowdPose dataset, demonstrating its effectiveness in improving the accuracy of human pose estimation in crowded and occluded scenes. This achievement highlights the potential of our model in many real-world applications where accurate human pose estimation is crucial, such as surveillance, sports analysis, and human–computer interaction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000236",
    "keywords": [
      "3D pose estimation",
      "Articulated body pose estimation",
      "Artificial intelligence",
      "Augmented reality",
      "Bounding overwatch",
      "Computer graphics",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Geometry",
      "Image (mathematics)",
      "Law",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Minimum bounding box",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Point (geometry)",
      "Political science",
      "Politics",
      "Pose",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Bai",
        "given_name": "Xiuxiu"
      },
      {
        "surname": "Wei",
        "given_name": "Xing"
      },
      {
        "surname": "Wang",
        "given_name": "Zengying"
      },
      {
        "surname": "Zhang",
        "given_name": "Miao"
      }
    ]
  },
  {
    "title": "Bayesian DivideMix++ for Enhanced Learning with Noisy Labels",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106122",
    "abstract": "Leveraging inexpensive and human intervention-based annotating methodologies, such as crowdsourcing and web crawling, often leads to datasets with noisy labels. Noisy labels can have a detrimental impact on the performance and generalization of deep neural networks. Robust models that are able to handle and mitigate the effect of these noisy labels are thus essential. In this work, we explore the open challenges of neural network memorization and uncertainty in creating robust learning algorithms with noisy labels. To overcome them, we propose a novel framework called “Bayesian DivideMix++” with two critical components: (i) DivideMix++, to enhance the robustness against memorization and (ii) Monte-Carlo MixMatch, which focuses on improving the effectiveness towards label uncertainty. DivideMix++ improves the pipeline by integrating the warm-up and augmentation pipeline with self-supervised pre-training and dedicated different data augmentations for loss analysis and backpropagation. Monte-Carlo MixMatch leverages uncertainty measurements to mitigate the influence of uncertain samples by reducing their weight in the data augmentation MixMatch step. We validate our proposed pipeline using four datasets encompassing various synthetic and real-world noise settings. We demonstrate the effectiveness and merits of our proposed pipeline using extensive experiments. Bayesian DivideMix++ outperforms the state-of-the-art models by considerable differences in all experiments. Our findings underscore the potential of leveraging these modifications to enhance the performance and generalization of deep neural networks in practical scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000364",
    "keywords": [
      "Anatomy",
      "Artificial intelligence",
      "Artificial neural network",
      "Bayesian probability",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Crawling",
      "Deep learning",
      "Deep neural networks",
      "Gene",
      "Generalization",
      "Image (mathematics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Medicine",
      "Noise (video)",
      "Pipeline (software)",
      "Programming language",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Nagarajan",
        "given_name": "Bhalaji"
      },
      {
        "surname": "Marques",
        "given_name": "Ricardo"
      },
      {
        "surname": "Aguilar",
        "given_name": "Eduardo"
      },
      {
        "surname": "Radeva",
        "given_name": "Petia"
      }
    ]
  },
  {
    "title": "Coordination as inference in multi-agent reinforcement learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106101",
    "abstract": "The Centralized Training and Decentralized Execution (CTDE) paradigm, where a centralized critic is allowed to access global information during the training phase while maintaining the learned policies executed with only local information in a decentralized way, has achieved great progress in recent years. Despite the progress, CTDE may suffer from the issue of Centralized–Decentralized Mismatch (CDM): the suboptimality of one agent’s policy can exacerbate policy learning of other agents through the centralized joint critic. In contrast to centralized learning, the cooperative model that most closely resembles the way humans cooperate in nature is fully decentralized, i.e. Independent Learning (IL). However, there are still two issues that need to be addressed before agents coordinate through IL: (1) how agents are aware of the presence of other agents, and (2) how to coordinate with other agents to improve joint policy under IL. In this paper, we propose an inference-based coordinated MARL method: Deep Motor System (DMS). DMS first presents the idea of individual intention inference where agents are allowed to disentangle other agents from their environment. Secondly, causal inference was introduced to enhance coordination by reasoning each agent’s effect on others’ behavior. The proposed model was extensively experimented on a series of Multi-Agent MuJoCo and StarCraftII tasks. Results show that the proposed method outperforms independent learning algorithms and the coordination behavior among agents can be learned even without the CTDE paradigm compared to the state-of-the-art baselines including IPPO and HAPPO.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000157",
    "keywords": [
      "Artificial intelligence",
      "Causal inference",
      "Computer science",
      "Control (management)",
      "Decentralised system",
      "Econometrics",
      "Economics",
      "Inference",
      "Machine learning",
      "Policy learning",
      "Reinforcement learning"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Zhiyuan"
      },
      {
        "surname": "Wu",
        "given_name": "Lijun"
      },
      {
        "surname": "Su",
        "given_name": "Kaile"
      },
      {
        "surname": "Wu",
        "given_name": "Wei"
      },
      {
        "surname": "Jing",
        "given_name": "Yulin"
      },
      {
        "surname": "Wu",
        "given_name": "Tong"
      },
      {
        "surname": "Duan",
        "given_name": "Weiwei"
      },
      {
        "surname": "Yue",
        "given_name": "Xiaofeng"
      },
      {
        "surname": "Tong",
        "given_name": "Xiyi"
      },
      {
        "surname": "Han",
        "given_name": "Yizhou"
      }
    ]
  },
  {
    "title": "RGDAN: A random graph diffusion attention network for traffic prediction",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.106093",
    "abstract": "Traffic Prediction based on graph structures is a challenging task given that road networks are typically complex structures and the data to be analyzed contains variable temporal features. Further, the quality of the spatial feature extraction is highly dependent on the weight settings of the graph structures. In the transportation field, the weights of these graph structures are currently calculated based on factors like the distance between roads. However, these methods do not take into account the characteristics of the road itself or the correlations between different traffic flows. Existing approaches usually pay more attention to local spatial dependencies extraction while global spatial dependencies are ignored. Another major problem is how to extract sufficient information at limited depth of graph structures. To address these challenges, we propose a Random Graph Diffusion Attention Network (RGDAN) for traffic prediction. RGDAN comprises a graph diffusion attention module and a temporal attention module. The graph diffusion attention module can adjust its weights by learning from data like a CNN to capture more realistic spatial dependencies. The temporal attention module captures the temporal correlations. Experiments on three large-scale public datasets demonstrate that RGDAN produces predictions with 2%–5% more precision than state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007542",
    "keywords": [
      "Artificial intelligence",
      "Attention network",
      "Computer science",
      "Data mining",
      "Graph",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Fan",
        "given_name": "Jin"
      },
      {
        "surname": "Weng",
        "given_name": "Wenchao"
      },
      {
        "surname": "Tian",
        "given_name": "Hao"
      },
      {
        "surname": "Wu",
        "given_name": "Huifeng"
      },
      {
        "surname": "Zhu",
        "given_name": "Fu"
      },
      {
        "surname": "Wu",
        "given_name": "Jia"
      }
    ]
  },
  {
    "title": "Novel criteria of sampled-data synchronization controller design for gated recurrent unit neural networks under mismatched parameters",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.035",
    "abstract": "Synchronization between neural networks (NNs) has been intensively investigated to analyze stability, convergence properties, neuronal behaviors and response to various inputs. However, synchronization techniques of NNs with gated recurrent units (GRUs) have not been provided until now due to their complicated nonlinearity. In this paper, we address the sampled-data synchronization problems of GRUs for the first time, and propose controller design methods using discretely sampled control inputs to synchronize master and slave GRUs. The master and slave GRUs are mathematically modeled as a linear parameter varying (LPV) system in which the parameter of the slave GRUs is constructed independently of the master GRUs. This distinctive modeling feature provides flexibility to extend the existing master and slave NNs into a more general structure. Indeed, the sampled-data synchronization can be achieved by formulating the design condition in terms of linear matrix inequalities (LMIs). The novel sampled-data synchronization criteria are devised by combining the H ∞ controller design with the looped-functional approach. The synthesized synchronization controllers guarantee not only asymptotic stability of the synchronization error system with aperiodic sampling, but also provides a satisfactory H ∞ control performance. Moreover, the communication efficiency is improved by using the proposed method in which the sampled-data synchronization controller is combined with the event-triggered mechanism. Finally, the numerical example validates the proposed theoretical contributions via simulation results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007438",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Controller (irrigation)",
      "Flexibility (engineering)",
      "Mathematics",
      "Statistics",
      "Synchronization (alternating current)"
    ],
    "authors": [
      {
        "surname": "Han",
        "given_name": "Seungyong"
      },
      {
        "surname": "Kommuri",
        "given_name": "Suneel Kumar"
      },
      {
        "surname": "Jin",
        "given_name": "Yongsik"
      }
    ]
  },
  {
    "title": "PeNet: A feature excitation learning approach to advertisement click-through rate prediction",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106127",
    "abstract": "Since the physical meaning of the fields of the dataset is unknown, we have to use the feature interaction method to select the correlated features and exclude uncorrelated features. The current state-of-the-art methods employ various methods based on feature interaction to predict advertisement Click-Through Rate (CTR); however, the feature interaction based on potential new feature mining is rarely considered, which can provide effective assistance for feature interaction. This motivates us to investigate methods that combine potential new features and feature interactions. Thus, we propose a potential feature excitation learning network ( PeNet ), which is a neural network model based on feature combination and feature interaction. In PeNet , we treat the row compression and column compression of the original feature matrix as potential new features, and proposed the excitation learning mechanism that is a weighted mechanism based on residual principle. Through this excitation learning mechanism, the original embedded features and potential new features are subjected to weighted interaction based on the residual principle. Moreover, a deep neural network is exploited to iteratively learn and iteratively combine features. The excitation learning structure of PeNet neural network is well demonstrated in this paper, that is, the control flow of embedding, compression, excitation and output, which further strengthens the correlated features and weakens the uncorrelated features by compressing and expanding the features. Experimental results on multiple benchmark datasets indicate the PeNet as a general-purpose plug-in has more superior performance and better efficiency than previous state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000431",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Computer science",
      "Feature (linguistics)",
      "Feature learning",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Yin",
        "given_name": "Yunfei"
      },
      {
        "surname": "Ochieng",
        "given_name": "Nyambega David"
      },
      {
        "surname": "Sun",
        "given_name": "Jingqin"
      },
      {
        "surname": "Bao",
        "given_name": "Xianjian"
      },
      {
        "surname": "Wang",
        "given_name": "Zhuowei"
      }
    ]
  },
  {
    "title": "Learning the consensus and complementary information for large-scale multi-view clustering",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106103",
    "abstract": "The multi-view data clustering has attracted much interest from researchers, and the large-scale multi-view clustering has many important applications and significant research value. In this article, we fully make use of the consensus and complementary information, and exploit a bipartite graph to depict the duality relationship between original points and anchor points. To be specific, representative anchor points are selected for each view to construct corresponding anchor representation matrices, and all views’ anchor points are utilized to construct a common representation matrix. Using anchor points also reduces the computation complexity. Next, the bipartite graph is built by fusing these representation matrices, and a Laplacian rank constraint is enforced on the bipartite graph. This will make the bipartite graph have k connected components to obtain accurate clustering labels, where the bipartite graph is specifically designed for a large-scale dataset problem. In addition, the anchor points are also updated by dictionary learning. The experimental results on the four benchmark image processing datasets have demonstrated superior performance of the proposed large-scale multi-view clustering algorithm over other state-of-the-art multi-view clustering algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000170",
    "keywords": [
      "Adjacency matrix",
      "Artificial intelligence",
      "Bipartite graph",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Data point",
      "Graph",
      "Laplacian matrix",
      "Mathematics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Maoshan"
      },
      {
        "surname": "Palade",
        "given_name": "Vasile"
      },
      {
        "surname": "Zheng",
        "given_name": "Zhonglong"
      }
    ]
  },
  {
    "title": "Towards generalizable Graph Contrastive Learning: An information theory perspective",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106125",
    "abstract": "Graph Contrastive Learning (GCL) is increasingly employed in graph representation learning with the primary aim of learning node/graph representations from a predefined pretext task that can generalize to various downstream tasks. Meanwhile, the transition from a specific pretext task to diverse and unpredictable downstream tasks poses a significant challenge for GCL’s generalization ability. Most existing GCL approaches maximize mutual information between two views derived from the original graph, either randomly or heuristically. However, the generalization ability of GCL and its theoretical principles are still less studied. In this paper, we introduce a novel metric GCL-GE, to quantify the generalization gap between predefined pretext and agnostic downstream tasks. Given the inherent intractability of GCL-GE, we leverage concepts from information theory to derive a mutual information upper bound that is independent of the downstream tasks, thus enabling the metric’s optimization despite the variability in downstream tasks. Based on the theoretical insight, we propose InfoAdv, a GCL framework to directly enhance generalization by jointly optimizing GCL-GE and InfoMax. Extensive experiments validate the capability of InfoAdv to enhance performance across a wide variety of downstream tasks, demonstrating its effectiveness in improving the generalizability of GCL.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000418",
    "keywords": [
      "Artificial intelligence",
      "Cognitive science",
      "Combinatorics",
      "Computer science",
      "Contrastive analysis",
      "Graph",
      "Graph theory",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Natural language processing",
      "Perspective (graphical)",
      "Philosophy",
      "Psychology",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Yuan",
        "given_name": "Yige"
      },
      {
        "surname": "Xu",
        "given_name": "Bingbing"
      },
      {
        "surname": "Shen",
        "given_name": "Huawei"
      },
      {
        "surname": "Cao",
        "given_name": "Qi"
      },
      {
        "surname": "Cen",
        "given_name": "Keting"
      },
      {
        "surname": "Zheng",
        "given_name": "Wen"
      },
      {
        "surname": "Cheng",
        "given_name": "Xueqi"
      }
    ]
  },
  {
    "title": "A multimodal dynamical variational autoencoder for audiovisual speech representation learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106120",
    "abstract": "High-dimensional data such as natural images or speech signals exhibit some form of regularity, preventing their dimensions from varying independently. This suggests that there exists a lower dimensional latent representation from which the high-dimensional observed data were generated. Uncovering the hidden explanatory features of complex data is the goal of representation learning, and deep latent variable generative models have emerged as promising unsupervised approaches. In particular, the variational autoencoder (VAE) which is equipped with both a generative and an inference model allows for the analysis, transformation, and generation of various types of data. Over the past few years, the VAE has been extended to deal with data that are either multimodal or dynamical (i.e., sequential). In this paper, we present a multimodal and dynamical VAE (MDVAE) applied to unsupervised audiovisual speech representation learning. The latent space is structured to dissociate the latent dynamical factors that are shared between the modalities from those that are specific to each modality. A static latent variable is also introduced to encode the information that is constant over time within an audiovisual speech sequence. The model is trained in an unsupervised manner on an audiovisual emotional speech dataset, in two stages. In the first stage, a vector quantized VAE (VQ-VAE) is learned independently for each modality, without temporal modeling. The second stage consists in learning the MDVAE model on the intermediate representation of the VQ-VAEs before quantization. The disentanglement between static versus dynamical and modality-specific versus modality-common information occurs during this second training stage. Extensive experiments are conducted to investigate how audiovisual speech latent factors are encoded in the latent space of MDVAE. These experiments include manipulating audiovisual speech, audiovisual facial image denoising, and audiovisual speech emotion recognition. The results show that MDVAE effectively combines the audio and visual information in its latent space. They also show that the learned static representation of audiovisual speech can be used for emotion recognition with few labeled data, and with better accuracy compared with unimodal baselines and a state-of-the-art supervised model based on an audiovisual transformer architecture.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000340",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Computer science",
      "Discriminative model",
      "Feature learning",
      "Generative grammar",
      "Generative model",
      "Inference",
      "Latent variable",
      "Law",
      "Modality (human–computer interaction)",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Speech recognition",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Sadok",
        "given_name": "Samir"
      },
      {
        "surname": "Leglaive",
        "given_name": "Simon"
      },
      {
        "surname": "Girin",
        "given_name": "Laurent"
      },
      {
        "surname": "Alameda-Pineda",
        "given_name": "Xavier"
      },
      {
        "surname": "Séguier",
        "given_name": "Renaud"
      }
    ]
  },
  {
    "title": "Medical object detector jointly driven by knowledge and data",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.038",
    "abstract": "Most of the existing object detection algorithms are trained on medical datasets and then used for prediction. When the features of an object are not obvious in an image, these models are prone to mislocalize and misclassify it. In this paper, we propose a medical Object Detection algorithm jointly driven by Knowledge and Data (ODKD). It enables medical semantic knowledge provided by specialized physicians to be effective and helpful when traditional models have difficulty in correctly detecting objects relying on features alone. Our model consists of a base object detector together with a fusion module: the base object detector is trained based on medical datasets to obtain data-driven results; then we use a graph to represent external semantic knowledge and map the data-driven results to the nodes embedding of this graph structure. In the fusion module, a graph convolution network is used to fuse the data-driven results with the external semantic knowledge to output category adjustment coefficients. Finally, the adjustment coefficients are used to adjust the data-driven results to obtain results jointly driven by knowledge and data. Experiments show that professional medical semantic knowledge can effectively correct the erroneous results of the base detector, and the effect of our model outperforms Faster Rcnn, YOLOv5, YOLOv7, etc. on three medical datasets, Camus, Synapse, and AMOS.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007451",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolution (computer science)",
      "Data mining",
      "Detector",
      "Embedding",
      "Graph",
      "Knowledge base",
      "Machine learning",
      "Object (grammar)",
      "Object detection",
      "Pattern recognition (psychology)",
      "Telecommunications",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zeng",
        "given_name": "Xianhua"
      },
      {
        "surname": "Liu",
        "given_name": "Yuhang"
      },
      {
        "surname": "Zhang",
        "given_name": "Jian"
      },
      {
        "surname": "Guo",
        "given_name": "Yongli"
      }
    ]
  },
  {
    "title": "TransformerG2G: Adaptive time-stepping for learning temporal graph embeddings using transformers",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.040",
    "abstract": "Dynamic graph embedding has emerged as a very effective technique for addressing diverse temporal graph analytic tasks (i.e., link prediction, node classification, recommender systems, anomaly detection, and graph generation) in various applications. Such temporal graphs exhibit heterogeneous transient dynamics, varying time intervals, and highly evolving node features throughout their evolution. Hence, incorporating long-range dependencies from the historical graph context plays a crucial role in accurately learning their temporal dynamics. In this paper, we develop a graph embedding model with uncertainty quantification, TransformerG2G, by exploiting the advanced transformer encoder to first learn intermediate node representations from its current state ( t ) and previous context (over timestamps [ t − 1 , t − l ], l is the length of context). Moreover, we employ two projection layers to generate lower-dimensional multivariate Gaussian distributions as each node’s latent embedding at timestamp t . We consider diverse benchmarks with varying levels of “novelty” as measured by the TEA (Temporal Edge Appearance) plots. Our experiments demonstrate that the proposed TransformerG2G model outperforms conventional multi-step methods and our prior work (DynG2G) in terms of both link prediction accuracy and computational efficiency, especially for high degree of novelty. Furthermore, the learned time-dependent attention weights across multiple graph snapshots reveal the development of an automatic adaptive time stepping enabled by the transformer. Importantly, by examining the attention weights, we can uncover temporal dependencies, identify influential elements, and gain insights into the complex interactions within the graph structure. For example, we identified a strong correlation between attention weights and node degree at the various stages of the graph topology evolution.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007475",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Graph",
      "Physics",
      "Quantum mechanics",
      "Theoretical computer science",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Varghese",
        "given_name": "Alan John"
      },
      {
        "surname": "Bora",
        "given_name": "Aniruddha"
      },
      {
        "surname": "Xu",
        "given_name": "Mengjia"
      },
      {
        "surname": "Karniadakis",
        "given_name": "George Em"
      }
    ]
  },
  {
    "title": "Event-triggered hybrid impulsive control for synchronization of fractional-order multilayer signed networks under cyber attacks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106124",
    "abstract": "In this paper, we consider the exponential bipartite synchronization (EBS) problem of fractional-order multilayer signed networks with time-varying delays (FO-MSNT) under random cyber attacks. In contrast to the existing literature, the proposed hybrid event-triggered controller combines the advantages of feedback controller and impulsive controller, and the event-triggered condition is constructed by applying the network topology and the Lyapunov function of the subsystem, rather than the state function of the subsystem. Based on the Lyapunov–Razumikhin method and the graph theory, some sufficient conditions for achieving EBS of FO-MSNT under cyber attacks which are related to the topology of networks, the event-triggered parameters, the order of fractional derivative and the signal sent by the enemy are obtained. Furthermore, fractional-order coupled Chua’s circuits model and fractional-order power systems built on MSNT are established and the EBS issues under cyber attacks are analyzed. Numerical examples and simulations are provided to show the validity of our theories.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000406",
    "keywords": [
      "Agronomy",
      "Applied mathematics",
      "Artificial intelligence",
      "Biology",
      "Combinatorics",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Controller (irrigation)",
      "Fractional calculus",
      "Lyapunov function",
      "Mathematical analysis",
      "Mathematics",
      "Network topology",
      "Nonlinear system",
      "Operating system",
      "Physics",
      "Quantum mechanics",
      "Sign function",
      "Synchronization (alternating current)",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Xin"
      },
      {
        "surname": "Chen",
        "given_name": "Lili"
      },
      {
        "surname": "Zhao",
        "given_name": "Yanfeng"
      },
      {
        "surname": "Li",
        "given_name": "Honglin"
      }
    ]
  },
  {
    "title": "Aggregating intrinsic information to enhance BCI performance through federated learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106100",
    "abstract": "Insufficient data is a long-standing challenge for Brain–Computer Interface (BCI) to build a high-performance deep learning model. Though numerous research groups and institutes collect a multitude of EEG datasets for the same BCI task, sharing EEG data from multiple sites is still challenging due to the heterogeneity of devices. The significance of this challenge cannot be overstated, given the critical role of data diversity in fostering model robustness. However, existing works rarely discuss this issue, predominantly centering their attention on model training within a single dataset, often in the context of inter-subject or inter-session settings. In this work, we propose a hierarchical personalized Federated Learning EEG decoding (FLEEG) framework to surmount this challenge. This innovative framework heralds a new learning paradigm for BCI, enabling datasets with disparate data formats to collaborate in the model training process. Each client is assigned a specific dataset and trains a hierarchical personalized model to manage diverse data formats and facilitate information exchange. Meanwhile, the server coordinates the training procedure to harness knowledge gleaned from all datasets, thus elevating overall performance. The framework has been evaluated in Motor Imagery (MI) classification with nine EEG datasets collected by different devices but implementing the same MI task. Results demonstrate that the proposed framework can boost classification performance up to 8.4% by enabling knowledge sharing between multiple datasets, especially for smaller datasets. Visualization results also indicate that the proposed framework can empower the local models to put a stable focus on task-related areas, yielding better performance. To the best of our knowledge, this is the first end-to-end solution to address this important challenge.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000145",
    "keywords": [
      "Alternative medicine",
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Brain–computer interface",
      "Bubble",
      "Chemistry",
      "Computer science",
      "Context (archaeology)",
      "Data sharing",
      "Economics",
      "Electroencephalography",
      "Gene",
      "Human–computer interaction",
      "Interface (matter)",
      "Machine learning",
      "Management",
      "Maximum bubble pressure method",
      "Medicine",
      "Paleontology",
      "Parallel computing",
      "Pathology",
      "Psychiatry",
      "Psychology",
      "Robustness (evolution)",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Rui"
      },
      {
        "surname": "Chen",
        "given_name": "Yuanyuan"
      },
      {
        "surname": "Li",
        "given_name": "Anran"
      },
      {
        "surname": "Ding",
        "given_name": "Yi"
      },
      {
        "surname": "Yu",
        "given_name": "Han"
      },
      {
        "surname": "Guan",
        "given_name": "Cuntai"
      }
    ]
  },
  {
    "title": "Consideration on the learning efficiency of multiple-layered neural networks with linear units",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106132",
    "abstract": "In the last two decades, remarkable progress has been done in singular learning machine theories on the basis of algebraic geometry. These theories reveal that we need to find resolution maps of singularities for analyzing asymptotic behavior of state probability functions when the number of data increases. In particular, it is essential to construct normal crossing divisors of average log loss functions. However, there are few examples for obtaining these for singular models. In this paper, we determine the resolution map and normal crossing divisors for multiple-layered neural networks with linear units. Moreover, we have the exact values for the learning efficiency, which is so called learning coefficients. Multiple-layered neural networks with linear units are simple, however, very important models because these models give the essential information from data of input–output pairs. Moreover, these models are very close to multiple-layered neural networks with rectified linear units (ReLU). We show the learning coefficients of multiple-layered neural networks with linear units are bounded even though the number of layers goes to infinity, which means that the main term of asymptotic expansion of the free energy and generalization error of singular models are much smaller than the dimension of its parameter space.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000480",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Machine learning"
    ],
    "authors": [
      {
        "surname": "Aoyagi",
        "given_name": "Miki"
      }
    ]
  },
  {
    "title": "It takes two: Dual Branch Augmentation Module for domain generalization",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.106094",
    "abstract": "Although great success has been achieved in various computer vision tasks, deep neural networks (DNNs) suffer dramatic performance degradation when evaluated on out-of-distribution data. Domain generalization (DG) is proposed to handle this problem by learning domain-agnostic information from multiple source domains to generalize well on unseen target domains. Several methods resort to Fourier transform due to its simplicity and efficiency. They argue that amplitude spectra imply domain-specific information, which should be suppressed, while phase counterparts imply domain-agnostic information, which should be preserved. However, these methods only suppress the domain-specific information in source domains and neglect the relationship with target domains, leading to the persistence of the domain gap. Besides, these methods preserve domain-agnostic information by keeping phase components unchanged, causing them to be underutilized. In this paper, we propose Dual Branch Augmentation Module (DBAM) by leveraging Fourier transform and taking advantage of both amplitude and phase spectra. For the amplitude branch, we propose Inner-domain Amplitude Distribution Rectification (IADR) and Cross-domain Amplitude Dirichlet Mixup (CADM) to stabilize the training process and explore more feature space. In addition, we propose Test-time Amplitude Prototype Calibration (TAPC) to construct the connection between source and target domains during evaluation to further mitigate the domain gap. For the phase branch, we propose Random Symmetric Phase Perturbation (RSPP) to enhance the robustness for recognizing domain-agnostic information. With the combined contributions of the two branches, DBAM significantly surpasses other state-of-the-art (SOTA) methods. Extensive experiments on four benchmarks and further analysis demonstrate the effectiveness of DBAM.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007554",
    "keywords": [
      "Algorithm",
      "Amplitude",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Domain (mathematical analysis)",
      "Fourier transform",
      "Frequency domain",
      "Gene",
      "Generalization",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Rectification",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Jingwei"
      },
      {
        "surname": "Li",
        "given_name": "Yuan"
      },
      {
        "surname": "Tan",
        "given_name": "Jie"
      },
      {
        "surname": "Liu",
        "given_name": "Chengbao"
      }
    ]
  },
  {
    "title": "Cross-modality integration framework with prediction, perception and discrimination for video anomaly detection",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106138",
    "abstract": "Video anomaly detection is an important task for public security in the multimedia field. It aims to distinguish events that deviate from normal patterns. As important semantic representation, the textual information can effectively perceive different contents for anomaly detection. However, most existing methods primarily rely on visual modality, with limited incorporation of textual modality in anomaly detection. In this paper, a cross-modality integration framework (CIForAD) is proposed for anomaly detection, which combines both textual and visual modalities for prediction, perception and discrimination. Firstly, a feature fusion prediction (FUP) module is designed to predict the target regions by fusing the visual features and textual features for prompting, which can amplify the discriminative distance. Then an image-text semantic perception (ISP) module is developed to judge semantic consistency by associating the fine-grained visual features with textual features, where a strategy of local training and global inference is introduced to perceive local details and global semantic correlation. Finally, a self-supervised time attention discrimination (TAD) module is built to explore the inter-frame relation and further distinguish abnormal sequences from normal sequences. Extensive experiments on the three challenging benchmarks indicate that our CIForAD obtains state-of-the-art anomaly detection performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000546",
    "keywords": [
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Condensed matter physics",
      "Consistency (knowledge bases)",
      "Discriminative model",
      "Feature (linguistics)",
      "Frame (networking)",
      "Law",
      "Linguistics",
      "Modality (human–computer interaction)",
      "Natural language processing",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Perception",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Chaobo"
      },
      {
        "surname": "Li",
        "given_name": "Hongjun"
      },
      {
        "surname": "Zhang",
        "given_name": "Guoan"
      }
    ]
  },
  {
    "title": "Reinforcement learning-based consensus control for MASs with intermittent constraints",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106105",
    "abstract": "In this article, an adaptive optimal consensus control problem is studied for multiagent systems in the strict-feedback structure with intermittent constraints (the constraints appear intermittently). More specifically, by designing a novel switch-like function and an improved coordinate transformation, the constrained states are converted into unconstrained states, and the problem of intermittent constraints is resolved without requiring “feasibility conditions”. In addition, using the composite learning algorithm and neural networks to construct the identifier, a simplified identifier-actor-critic-based reinforcement learning strategy is proposed to obtain the approximate optimal controller under the framework of backstepping. Meanwhile, with the aid of the nonlinear dynamic surface control technique, the issue of “explosion of complexity” in backstepping is removed, and the requirements for filter parameters are loosened. Based on Lyapunov stability theory, it is demonstrated that all signals in the closed-loop system are bounded. Finally, two simulation examples are used to verify the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000194",
    "keywords": [
      "Adaptive control",
      "Agronomy",
      "Artificial intelligence",
      "Artificial neural network",
      "Backstepping",
      "Biochemistry",
      "Biology",
      "Bounded function",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Control (management)",
      "Control theory (sociology)",
      "Controller (irrigation)",
      "Filter (signal processing)",
      "Gene",
      "Identifier",
      "Lyapunov function",
      "Machine learning",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Nonlinear system",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Reinforcement learning",
      "Stability (learning theory)",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Ao"
      },
      {
        "surname": "Zhou",
        "given_name": "Qi"
      },
      {
        "surname": "Ren",
        "given_name": "Hongru"
      },
      {
        "surname": "Ma",
        "given_name": "Hui"
      },
      {
        "surname": "Lu",
        "given_name": "Renquan"
      }
    ]
  },
  {
    "title": "Investigating navigation strategies in the Morris Water Maze through deep reinforcement learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.004",
    "abstract": "Navigation is a complex skill with a long history of research in animals and humans. In this work, we simulate the Morris Water Maze in 2D to train deep reinforcement learning agents. We perform automatic classification of navigation strategies, analyze the distribution of strategies used by artificial agents, and compare them with experimental data to show similar learning dynamics as those seen in humans and rodents. We develop environment-specific auxiliary tasks and examine factors affecting their usefulness. We suggest that the most beneficial tasks are potentially more biologically feasible for real agents to use. Lastly, we explore the development of internal representations in the activations of artificial agent neural networks. These representations resemble place cells and head-direction cells found in mouse brains, and their presence has correlation to the navigation strategies that artificial agents employ.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006986",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Cognition",
      "Computer science",
      "Deep learning",
      "Machine learning",
      "Morris water navigation task",
      "Neuroscience",
      "Psychology",
      "Reinforcement",
      "Reinforcement learning",
      "Social psychology"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Andrew"
      },
      {
        "surname": "Borisyuk",
        "given_name": "Alla"
      }
    ]
  },
  {
    "title": "Contrastive and adversarial regularized multi-level representation learning for incomplete multi-view clustering",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106102",
    "abstract": "Incomplete multi-view clustering is a significant task in machine learning, given that complex systems in nature and society cannot be fully observed; it provides an opportunity to exploit the structure and functions of underlying systems. Current algorithms are criticized for failing either to balance data restoration and clustering or to capture the consistency of the representation of various views. To address these problems, a novel Multi-level Representation Learning Contrastive and Adversarial Learning (aka MRL_CAL) for incomplete multi-view clustering is proposed, in which data restoration, consistent representation, and clustering are jointly learned by exploiting features in various subspaces. Specifically, MRL_CAL employs v auto-encoder to obtain a low-level specific-view representation of instances, which restores data by estimating the distribution of the original incomplete data with adversarial learning. Then, MRL_CAL extracts a high-level representation of instances, in which the consistency of various views and labels of clusters is incorporated with contrastive learning. In this case, MRL_CAL simultaneously learns multi-level features of instances in various subspaces, which not only overcomes the confliction of representations but also improves the quality of features. Finally, MRL_CAL transforms incomplete multi-view clustering into an overall objective, where features are learned under the guidance of clustering. Extensive experimental results indicate that MRL_CAL outperforms state-of-the-art algorithms in terms of various measurements, implying that the proposed method is promising for incomplete multi-view clustering.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000169",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Autoencoder",
      "Cluster analysis",
      "Computer science",
      "Consistency (knowledge bases)",
      "Deep learning",
      "Feature learning",
      "Geometry",
      "Law",
      "Linear subspace",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Haiyue"
      },
      {
        "surname": "Zhang",
        "given_name": "Wensheng"
      },
      {
        "surname": "Ma",
        "given_name": "Xiaoke"
      }
    ]
  },
  {
    "title": "CI-GNN: A Granger causality-inspired graph neural network for interpretable brain network-based psychiatric diagnosis",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106147",
    "abstract": "There is a recent trend to leverage the power of graph neural networks (GNNs) for brain-network based psychiatric diagnosis, which, in turn, also motivates an urgent need for psychiatrists to fully understand the decision behavior of the used GNNs. However, most of the existing GNN explainers are either post-hoc in which another interpretive model needs to be created to explain a well-trained GNN, or do not consider the causal relationship between the extracted explanation and the decision, such that the explanation itself contains spurious correlations and suffers from weak faithfulness. In this work, we propose a granger causality-inspired graph neural network (CI-GNN), a built-in interpretable model that is able to identify the most influential subgraph (i.e., functional connectivity within brain regions) that is causally related to the decision (e.g., major depressive disorder patients or healthy controls), without the training of an auxillary interpretive network. CI-GNN learns disentangled subgraph-level representations α and β that encode, respectively, the causal and non-causal aspects of original graph under a graph variational autoencoder framework, regularized by a conditional mutual information (CMI) constraint. We theoretically justify the validity of the CMI regulation in capturing the causal relationship. We also empirically evaluate the performance of CI-GNN against three baseline GNNs and four state-of-the-art GNN explainers on synthetic data and three large-scale brain disease datasets. We observe that CI-GNN achieves the best performance in a wide range of metrics and provides more reliable and concise explanations which have clinical evidence. The source code and implementation details of CI-GNN are freely available at GitHub repository (https://github.com/ZKZ-Brain/CI-GNN/).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000716",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Causal model",
      "Causality (physics)",
      "Cognitive psychology",
      "Computer science",
      "Constraint (computer-aided design)",
      "Geometry",
      "Granger causality",
      "Graph",
      "Leverage (statistics)",
      "Machine learning",
      "Mathematics",
      "Physics",
      "Psychology",
      "Quantum mechanics",
      "Spurious relationship",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Kaizhong"
      },
      {
        "surname": "Yu",
        "given_name": "Shujian"
      },
      {
        "surname": "Chen",
        "given_name": "Badong"
      }
    ]
  },
  {
    "title": "Self-supervised anomaly detection in computer vision and beyond: A survey and outlook",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106106",
    "abstract": "Anomaly detection (AD) plays a crucial role in various domains, including cybersecurity, finance, and healthcare, by identifying patterns or events that deviate from normal behavior. In recent years, significant progress has been made in this field due to the remarkable growth of deep learning models. Notably, the advent of self-supervised learning has sparked the development of novel AD algorithms that outperform the existing state-of-the-art approaches by a considerable margin. This paper aims to provide a comprehensive review of the current methodologies in self-supervised anomaly detection. We present technical details of the standard methods and discuss their strengths and drawbacks. We also compare the performance of these models against each other and other state-of-the-art anomaly detection models. Finally, the paper concludes with a discussion of future directions for self-supervised anomaly detection, including the development of more effective and efficient algorithms and the integration of these techniques with other related fields, such as multi-modal learning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000200",
    "keywords": [
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Condensed matter physics",
      "Data science",
      "Deep learning",
      "Field (mathematics)",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematics",
      "Physics",
      "Pure mathematics",
      "State of art",
      "Supervised learning"
    ],
    "authors": [
      {
        "surname": "Hojjati",
        "given_name": "Hadi"
      },
      {
        "surname": "Ho",
        "given_name": "Thi Kieu Khanh"
      },
      {
        "surname": "Armanfard",
        "given_name": "Narges"
      }
    ]
  },
  {
    "title": "CI-GNN: A Granger causality-inspired graph neural network for interpretable brain network-based psychiatric diagnosis",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106147",
    "abstract": "There is a recent trend to leverage the power of graph neural networks (GNNs) for brain-network based psychiatric diagnosis, which, in turn, also motivates an urgent need for psychiatrists to fully understand the decision behavior of the used GNNs. However, most of the existing GNN explainers are either post-hoc in which another interpretive model needs to be created to explain a well-trained GNN, or do not consider the causal relationship between the extracted explanation and the decision, such that the explanation itself contains spurious correlations and suffers from weak faithfulness. In this work, we propose a granger causality-inspired graph neural network (CI-GNN), a built-in interpretable model that is able to identify the most influential subgraph (i.e., functional connectivity within brain regions) that is causally related to the decision (e.g., major depressive disorder patients or healthy controls), without the training of an auxillary interpretive network. CI-GNN learns disentangled subgraph-level representations α and β that encode, respectively, the causal and non-causal aspects of original graph under a graph variational autoencoder framework, regularized by a conditional mutual information (CMI) constraint. We theoretically justify the validity of the CMI regulation in capturing the causal relationship. We also empirically evaluate the performance of CI-GNN against three baseline GNNs and four state-of-the-art GNN explainers on synthetic data and three large-scale brain disease datasets. We observe that CI-GNN achieves the best performance in a wide range of metrics and provides more reliable and concise explanations which have clinical evidence. The source code and implementation details of CI-GNN are freely available at GitHub repository (https://github.com/ZKZ-Brain/CI-GNN/).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000716",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Causal model",
      "Causality (physics)",
      "Cognitive psychology",
      "Computer science",
      "Constraint (computer-aided design)",
      "Geometry",
      "Granger causality",
      "Graph",
      "Leverage (statistics)",
      "Machine learning",
      "Mathematics",
      "Physics",
      "Psychology",
      "Quantum mechanics",
      "Spurious relationship",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Kaizhong"
      },
      {
        "surname": "Yu",
        "given_name": "Shujian"
      },
      {
        "surname": "Chen",
        "given_name": "Badong"
      }
    ]
  },
  {
    "title": "Self-supervised anomaly detection in computer vision and beyond: A survey and outlook",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106106",
    "abstract": "Anomaly detection (AD) plays a crucial role in various domains, including cybersecurity, finance, and healthcare, by identifying patterns or events that deviate from normal behavior. In recent years, significant progress has been made in this field due to the remarkable growth of deep learning models. Notably, the advent of self-supervised learning has sparked the development of novel AD algorithms that outperform the existing state-of-the-art approaches by a considerable margin. This paper aims to provide a comprehensive review of the current methodologies in self-supervised anomaly detection. We present technical details of the standard methods and discuss their strengths and drawbacks. We also compare the performance of these models against each other and other state-of-the-art anomaly detection models. Finally, the paper concludes with a discussion of future directions for self-supervised anomaly detection, including the development of more effective and efficient algorithms and the integration of these techniques with other related fields, such as multi-modal learning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000200",
    "keywords": [
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Condensed matter physics",
      "Data science",
      "Deep learning",
      "Field (mathematics)",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematics",
      "Physics",
      "Pure mathematics",
      "State of art",
      "Supervised learning"
    ],
    "authors": [
      {
        "surname": "Hojjati",
        "given_name": "Hadi"
      },
      {
        "surname": "Ho",
        "given_name": "Thi Kieu Khanh"
      },
      {
        "surname": "Armanfard",
        "given_name": "Narges"
      }
    ]
  },
  {
    "title": "Adversarially robust neural networks with feature uncertainty learning and label embedding",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.041",
    "abstract": "Deep neural networks (DNNs) are vulnerable to the attacks of adversarial examples, which bring serious security risks to the learning systems. In this paper, we propose a new defense method to improve the adversarial robustness of DNNs based on stochastic neural networks (SNNs), termed as Margin-SNN. The proposed Margin-SNN mainly includes two modules, i.e., feature uncertainty learning module and label embedding module. The first module introduces uncertainty to the latent feature space by giving each sample a distributional representation rather than a fixed point representation, and leverages the advantages of variational information bottleneck method in achieving good intra-class compactness in latent space. The second module develops a label embedding mechanism to take advantage of the semantic information underlying the labels, which maps the labels into the same latent space with the features, in order to capture the similarity between sample and its class centroid, where a penalty term is equipped to elegantly enlarge the margin between different classes for better inter-class separability. Since no adversarial information is introduced, the proposed model can be learned in standard training to improve adversarial robustness, which is much more efficient than adversarial training. Extensive experiments on data sets MNIST, FASHION MNIST, CIFAR10, CIFAR100 and SVHN demonstrate superior defensive ability of the proposed method. Our code is available at https://github.com/humeng24/Margin-SNN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007487",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Embedding",
      "Feature learning",
      "Feature vector",
      "Gene",
      "MNIST database",
      "Machine learning",
      "Margin (machine learning)",
      "Pattern recognition (psychology)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Ran"
      },
      {
        "surname": "Ke",
        "given_name": "Haopeng"
      },
      {
        "surname": "Hu",
        "given_name": "Meng"
      },
      {
        "surname": "Wu",
        "given_name": "Wenhui"
      }
    ]
  },
  {
    "title": "Reinforcement learning-based consensus control for MASs with intermittent constraints",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106105",
    "abstract": "In this article, an adaptive optimal consensus control problem is studied for multiagent systems in the strict-feedback structure with intermittent constraints (the constraints appear intermittently). More specifically, by designing a novel switch-like function and an improved coordinate transformation, the constrained states are converted into unconstrained states, and the problem of intermittent constraints is resolved without requiring “feasibility conditions”. In addition, using the composite learning algorithm and neural networks to construct the identifier, a simplified identifier-actor-critic-based reinforcement learning strategy is proposed to obtain the approximate optimal controller under the framework of backstepping. Meanwhile, with the aid of the nonlinear dynamic surface control technique, the issue of “explosion of complexity” in backstepping is removed, and the requirements for filter parameters are loosened. Based on Lyapunov stability theory, it is demonstrated that all signals in the closed-loop system are bounded. Finally, two simulation examples are used to verify the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000194",
    "keywords": [
      "Adaptive control",
      "Agronomy",
      "Artificial intelligence",
      "Artificial neural network",
      "Backstepping",
      "Biochemistry",
      "Biology",
      "Bounded function",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Control (management)",
      "Control theory (sociology)",
      "Controller (irrigation)",
      "Filter (signal processing)",
      "Gene",
      "Identifier",
      "Lyapunov function",
      "Machine learning",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Nonlinear system",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Reinforcement learning",
      "Stability (learning theory)",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Ao"
      },
      {
        "surname": "Zhou",
        "given_name": "Qi"
      },
      {
        "surname": "Ren",
        "given_name": "Hongru"
      },
      {
        "surname": "Ma",
        "given_name": "Hui"
      },
      {
        "surname": "Lu",
        "given_name": "Renquan"
      }
    ]
  },
  {
    "title": "Fourier feature decorrelation based sample attention for dense crowd localization",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106131",
    "abstract": "Crowd localization, which prevails to extract the independent individual features, plays an significant role in critical analysis for crowd scene. Dense trivial features of individual targets are frequently susceptible to interference from complex background features, which makes it difficult to obtain satisfactory predictions for individual targets. Aiming at this issue, a Fourier feature decorrelation based sample attention is proposed for dense crowd localization. The correlation between features are decoupled in the Fourier transform domain, which induces the model to focus more on the true correlation between individual target features and labels. From the perspective of Fourier feature correlation between samples, independence test statistic optimization with cross-covariance operator is developed for feature decorrelation within the sample attention framework. The sample attention with global weight learning is iteratively optimized through matching the prediction loss, which can induce model partial out the spurious correlation between target-irrelevant features and labels. Experimental results show that the method proposed in this paper outperforms the current advanced crowd location methods on public dense crowd datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000479",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Correlation",
      "Covariance",
      "Decorrelation",
      "Feature (linguistics)",
      "Feature vector",
      "Focus (optics)",
      "Fourier transform",
      "Geometry",
      "Independence (probability theory)",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Sample (material)",
      "Spurious relationship",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wen",
        "given_name": "Chao"
      },
      {
        "surname": "He",
        "given_name": "Hongqiang"
      },
      {
        "surname": "Qian",
        "given_name": "Yuhua"
      },
      {
        "surname": "Xie",
        "given_name": "Yu"
      },
      {
        "surname": "Wang",
        "given_name": "Wenjian"
      }
    ]
  },
  {
    "title": "Generation, division and training: A promising method for source-free unsupervised domain adaptation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106142",
    "abstract": "Conventional unsupervised domain adaptation (UDA) methods often presuppose the existence of labeled source domain samples while adapting the source model to the target domain. Nevertheless, this premise is not always tenable in the context of source-free UDA (SFUDA) attributed to data privacy considerations. Some existing methods address this challenging SFUDA problem by self-supervised learning. But inaccurate pseudo-labels are always unavoidable to degrade the performance of the target model among these methods. Therefore, we propose a promising SFUDA method, namely Generation, Division and Training (GDT) which aims to promote the reliability of pseudo-labels for self-supervised learning and encourage similar features to have closer predictions than dissimilar ones by contrastive learning. Specifically in our GDT method, we first refine pseudo-labels with deep clustering for target samples and then split them into reliable samples and unreliable samples. After that, we adopt self-supervised learning and information maximization for reliable samples training. And for unreliable samples, we conduct contrastive learning via the perspective of similarity and disparity to attract similar samples and repulse dissimilar samples, which helps pull the similar features closed and push the dissimilar features away, leading to efficient feature clustering. Thorough experimentation on three benchmark datasets substantiates the excellence of our proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000583",
    "keywords": [
      "Adaptation (eye)",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Classifier (UML)",
      "Cluster analysis",
      "Computer science",
      "Context (archaeology)",
      "Domain (mathematical analysis)",
      "Domain adaptation",
      "Economics",
      "Geodesy",
      "Geography",
      "Image (mathematics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Optics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Physics",
      "Power (physics)",
      "Quantum mechanics",
      "Reliability (semiconductor)",
      "Similarity (geometry)",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Tian",
        "given_name": "Qing"
      },
      {
        "surname": "Zhao",
        "given_name": "Mengna"
      }
    ]
  },
  {
    "title": "Are transformer-based models more robust than CNN-based models?",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.045",
    "abstract": "As the deployment of artificial intelligence (AI) models in real-world settings grows, their open-environment robustness becomes increasingly critical. This study aims to dissect the robustness of deep learning models, particularly comparing transformer-based models against CNN-based models. We focus on unraveling the sources of robustness from two key perspectives: structural and process robustness. Our findings suggest that transformer-based models generally outperform convolution-based models in robustness across multiple metrics. However, we contend that these metrics may not wholly represent true model robustness, such as the mean of corruption error. To better understand the underpinnings of this robustness advantage, we analyze models through the lens of Fourier transform and game interaction. From our insights, we propose a calibrated evaluation metric for robustness against real-world data, and a blur-based method to enhance robustness performance. Our approach achieves state-of-the-art results, with mCE scores of 2.1% on CIFAR-10-C, 12.4% on CIFAR-100-C, and 24.9% on TinyImageNet-C.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007529",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Fuzzy logic",
      "Gene",
      "Machine learning",
      "Robustness (evolution)",
      "Robustness testing"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Zhendong"
      },
      {
        "surname": "Qian",
        "given_name": "Shuwei"
      },
      {
        "surname": "Xia",
        "given_name": "Changhong"
      },
      {
        "surname": "Wang",
        "given_name": "Chongjun"
      }
    ]
  },
  {
    "title": "Sampling complex topology structures for spiking neural networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106121",
    "abstract": "Spiking Neural Networks (SNNs) have been considered a potential competitor to Artificial Neural Networks (ANNs) due to their high biological plausibility and energy efficiency. However, the architecture design of SNN has not been well studied. Previous studies either use ANN architectures or directly search for SNN architectures under a highly constrained search space. In this paper, we aim to introduce much more complex connection topologies to SNNs to further exploit the potential of SNN architectures. To this end, we propose the topology-aware search space, which is the first search space that enables a more diverse and flexible design for both the spatial and temporal topology of the SNN architecture. Then, to efficiently obtain architecture from our search space, we propose the spatio-temporal topology sampling (STTS) algorithm. By leveraging the benefits of random sampling, STTS can yield powerful architecture without the need for an exhaustive search process, making it significantly more efficient than alternative search strategies. Extensive experiments on CIFAR-10, CIFAR-100, and ImageNet demonstrate the effectiveness of our method. Notably, we obtain 70.79% top-1 accuracy on ImageNet with only 4 time steps, 1.79% higher than the second best model. Our code is available under https://github.com/stiger1000/Random-Sampling-SNN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000352",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Combinatorics",
      "Computer science",
      "Computer vision",
      "Filter (signal processing)",
      "Machine learning",
      "Mathematics",
      "Network topology",
      "Operating system",
      "Sampling (signal processing)",
      "Spiking neural network",
      "Theoretical computer science",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Yan",
        "given_name": "Shen"
      },
      {
        "surname": "Meng",
        "given_name": "Qingyan"
      },
      {
        "surname": "Xiao",
        "given_name": "Mingqing"
      },
      {
        "surname": "Wang",
        "given_name": "Yisen"
      },
      {
        "surname": "Lin",
        "given_name": "Zhouchen"
      }
    ]
  },
  {
    "title": "Perturbation diversity certificates robust generalization",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106117",
    "abstract": "Whilst adversarial training has been proven to be one most effective defending method against adversarial attacks for deep neural networks, it suffers from over-fitting on training adversarial data and thus may not guarantee the robust generalization. This may result from the fact that the conventional adversarial training methods generate adversarial perturbations usually in a supervised way so that the resulting adversarial examples are highly biased towards the decision boundary, leading to an inhomogeneous data distribution. To mitigate this limitation, we propose to generate adversarial examples from a perturbation diversity perspective. Specifically, the generated perturbed samples are not only adversarial but also diverse so as to certify robust generalization and significant robustness improvement through a homogeneous data distribution. We provide theoretical and empirical analysis, establishing a foundation to support the proposed method. As a major contribution, we prove that promoting perturbations diversity can lead to a better robust generalization bound. To verify our methods’ effectiveness, we conduct extensive experiments over different datasets (e.g., CIFAR-10, CIFAR-100, SVHN) with different adversarial attacks (e.g., PGD, CW). Experimental results show that our method outperforms other state-of-the-art (e.g., PGD and Feature Scattering) in robust generalization performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000315",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Decision boundary",
      "Deep neural networks",
      "Gene",
      "Generalization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Perturbation (astronomy)",
      "Physics",
      "Quantum mechanics",
      "Robustness (evolution)",
      "Support vector machine",
      "Training set"
    ],
    "authors": [
      {
        "surname": "Qian",
        "given_name": "Zhuang"
      },
      {
        "surname": "Zhang",
        "given_name": "Shufei"
      },
      {
        "surname": "Huang",
        "given_name": "Kaizhu"
      },
      {
        "surname": "Wang",
        "given_name": "Qiufeng"
      },
      {
        "surname": "Yi",
        "given_name": "Xinping"
      },
      {
        "surname": "Gu",
        "given_name": "Bin"
      },
      {
        "surname": "Xiong",
        "given_name": "Huan"
      }
    ]
  },
  {
    "title": "CRESPR: Modular sparsification of DNNs to improve pruning performance and model interpretability",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.021",
    "abstract": "Modern DNNs often include a huge number of parameters that are expensive for both computation and memory. Pruning can significantly reduce model complexity and lessen resource demands, and less complex models can also be easier to explain and interpret. In this paper, we propose a novel pruning algorithm, Cluster-Restricted Extreme Sparsity Pruning of Redundancy (CRESPR), to prune a neural network into modular units and achieve better pruning efficiency. With the Hessian matrix, we provide an analytic explanation of why modular structures in a sparse DNN can better maintain performance, especially at an extreme high pruning ratio. In CRESPR, each modular unit contains mostly internal connections, which clearly shows how subgroups of input features are processed through a DNN and eventually contribute to classification decisions. Such process-level revealing of internal working mechanisms undoubtedly leads to better interpretability of a black-box DNN model. Extensive experiments were conducted with multiple DNN architectures and datasets, and CRESPR achieves higher pruning performance than current state-of-the-art methods at high and extremely high pruning ratios. Additionally, we show how CRESPR improves model interpretability through a concrete example.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007268",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Interpretability",
      "Machine learning",
      "Modular design",
      "Operating system",
      "Pruning"
    ],
    "authors": [
      {
        "surname": "Kang",
        "given_name": "Tianyu"
      },
      {
        "surname": "Ding",
        "given_name": "Wei"
      },
      {
        "surname": "Chen",
        "given_name": "Ping"
      }
    ]
  },
  {
    "title": "AMHGCN: Adaptive multi-level hypergraph convolution network for human motion prediction",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106153",
    "abstract": "Human motion prediction is the key technology for many real-life applications, e.g., self-driving and human–robot interaction. The recent approaches adopt the unrestricted full-connection graph representation to capture the relationships inside the human skeleton. However, there are two issues to be solved: (i) these unrestricted full-connection graph representation methods neglect the inherent dependencies across the joints of the human body; (ii) these methods represent human motions using the features extracted from a single level and thus can neither fully exploit the various connection relationships among the human body nor guarantee the human motion prediction results to be reasonable. To tackle the above issues, we propose an adaptive multi-level hypergraph convolution network (AMHGCN), which uses the adaptive multi-level hypergraph representation to capture various dependencies among the human body. Our method has four different levels of hypergraph representations, including (i) the joint-level hypergraph representation to capture inherent kinetic dependencies in the human body, (ii) the part-level hypergraph representation to exploit the kinetic characteristics at a higher level (in comparison to the joint-level) by viewing some part of the human body as an entirety, (iii) the component-level hypergraph representation to model the semantic information, and (iv) the global-level hypergraph representation to extract long-distance dependencies in the human body. In addition, to take full advantage of the knowledge carried in the training data, we propose a reverse loss (i.e., adopting the future human poses to predict the historical poses reversely) to realize data augmentation. Extensive experiments show that our proposed AMHGCN can achieve state-of-the-art performance on three benchmarks, i.e., Human3.6M, CMU-Mocap, and 3DPW.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000777",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer security",
      "Convolution (computer science)",
      "Discrete mathematics",
      "Exploit",
      "Graph",
      "Hypergraph",
      "Law",
      "Mathematics",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Jinkai"
      },
      {
        "surname": "Wang",
        "given_name": "Jinghua"
      },
      {
        "surname": "Wu",
        "given_name": "Lian"
      },
      {
        "surname": "Wang",
        "given_name": "Xin"
      },
      {
        "surname": "Luo",
        "given_name": "Xiaoling"
      },
      {
        "surname": "Xu",
        "given_name": "Yong"
      }
    ]
  },
  {
    "title": "Logarithmic Learning Differential Convolutional Neural Network",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106114",
    "abstract": "Convolutional Neural Networks (CNNs) have revolutionized image classification through their innovative design and training methodologies in computer vision. Differential convolutional neural network with simultaneous multidimensional filter realization improved the performance of the convolutional neural network with calculation cost drawback. This paper introduces logarithmic learning integration into the differential Convolutional neural network to overcome the drawback by supplying faster error minimization and convergence. This task is done by incorporating LogRelu activation, a Logarithmic Cost Function, and unique logarithmic learning method. The effectiveness of the proposed approaches are evaluated by using various datasets and SGD/Adam optimizers. The first step is the adaptation of LogRelu activation function to convolutional and differential convolutional neural networks. The experiment results show that LogRelu integration to convolutional neural network and differential convolutional neural network yields performance improvements ranging from 1.61% to 5.44%. The same integration on ResNet-18, ResNet-34, and ResNet-50 enhances top-1 accuracy in the range of 3.07% and 9.96%. In addition to LogRelu activation function, a Logarithmic Cost Function with logarithmic learning method is also proposed and adapted to differential convolutional neural network. These improvements lead to a new differential convolutional neural network named as Logarithmic Differential Convolutional Neural Network (LDiffCNN), It consistently outperforms standard CNN by increasing the accuracy up to 3.02%. Notably, Logarithmic Differential Convolutional Neural Network demonstrates reduced training iterations up to 38% with faster convergence. The experimental results proved the efficiency of the proposed approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000285",
    "keywords": [
      "Activation function",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Logarithm",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Yasin",
        "given_name": "Magombe"
      },
      {
        "surname": "Sarıgül",
        "given_name": "Mehmet"
      },
      {
        "surname": "Avci",
        "given_name": "Mutlu"
      }
    ]
  },
  {
    "title": "Physics-informed kernel function neural networks for solving partial differential equations",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106098",
    "abstract": "This paper proposes an improved version of physics-informed neural networks (PINNs), the physics-informed kernel function neural networks (PIKFNNs), to solve various linear and some specific nonlinear partial differential equations (PDEs). It can also be considered as a novel radial basis function neural network (RBFNN). In the proposed PIKFNNs, it employs one-hidden-layer shallow neural network with the physics-informed kernel functions (PIKFs) as the customized activation functions. The PIKFs fully or partially contain PDE information, which can be chosen as fundamental solutions, green's functions, T-complete functions, harmonic functions, radial Trefftz functions, probability density functions and even the solutions of some linear simplified PDEs and so on. The main difference between the PINNs and the proposed PIKFNNs is that the PINNs add PDE constraints to the loss function, and the proposed PIKFNNs embed PDE information into the activation functions of the neural network. The feasibility and accuracy of the proposed PIKFNNs are validated by some benchmark examples.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000042",
    "keywords": [
      "Activation function",
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Differential equation",
      "Discrete mathematics",
      "Evolutionary biology",
      "Function (biology)",
      "Geodesy",
      "Geography",
      "Kernel (algebra)",
      "Mathematical analysis",
      "Mathematics",
      "Nonlinear system",
      "Partial differential equation",
      "Physics",
      "Quantum mechanics",
      "Radial basis function"
    ],
    "authors": [
      {
        "surname": "Fu",
        "given_name": "Zhuojia"
      },
      {
        "surname": "Xu",
        "given_name": "Wenzhi"
      },
      {
        "surname": "Liu",
        "given_name": "Shuainan"
      }
    ]
  },
  {
    "title": "Optimization of epilepsy detection method based on dynamic EEG channel screening",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106119",
    "abstract": "To decrease the interference in the process of epileptic feature extraction caused by insufficient detection capability in partial channels of focal epilepsy, this paper proposes a novel epilepsy detection method based on dynamic electroencephalogram (EEG) channel screening. This method not only extracts more effective epilepsy features but also finds common features among different epilepsy subjects, providing an effective approach and theoretical support for across-subject epilepsy detection in clinical scenarios. Firstly, we use the Refine Composite Multiscale Dispersion Entropy (RCMDE) to measure the complexity of EEG signals between normal and seizure states and realize the dynamic EEG channel screening among different subjects, which can enhance the capability of feature extraction and the robustness of epilepsy detection. Subsequently, we discover common epilepsy features in 3–15 Hz among different subjects by the screened EEG channels. By this finding, we construct the Residual Convolutional Long Short-Term Memory (ResCon-LSTM) neural network to accomplish across-subject epilepsy detection. The experiment results on the CHB-MIT dataset indicate that the highest accuracy of epilepsy detection in the single-subject experiment is 98.523 %, improved by 5.298 % compared with non-channel screening. In the across-subject experiment, the average accuracy is 96.596 %. Therefore, this method could be effectively applied to different subjects by dynamically screening optimal channels and keep a good detection performance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000339",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Convolutional neural network",
      "Electroencephalography",
      "Epilepsy",
      "Feature (linguistics)",
      "Feature extraction",
      "Gene",
      "Linguistics",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Psychology",
      "Robustness (evolution)",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Yuebin"
      },
      {
        "surname": "Fan",
        "given_name": "Chunling"
      },
      {
        "surname": "Mao",
        "given_name": "Xiaoqian"
      }
    ]
  },
  {
    "title": "V2IED: Dual-view learning framework for detecting events of interictal epileptiform discharges",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106136",
    "abstract": "Interictal epileptiform discharges (IED) as large intermittent electrophysiological events are associated with various severe brain disorders. Automated IED detection has long been a challenging task, and mainstream methods largely focus on singling out IEDs from backgrounds from the perspective of waveform, leaving normal sharp transients/artifacts with similar waveforms almost unattended. An open issue still remains to accurately detect IED events that directly reflect the abnormalities in brain electrophysiological activities, minimizing the interference from irrelevant sharp transients with similar waveforms only. This study then proposes a dual-view learning framework (namely V2IED) to detect IED events from multi-channel EEG via aggregating features from the two phases: (1) Morphological Feature Learning: directly treating the EEG as a sequence with multiple channels, a 1D-CNN (Convolutional Neural Network) is applied to explicitly learning the deep morphological features; and (2) Spatial Feature Learning: viewing the EEG as a 3D tensor embedding channel topology, a CNN captures the spatial features at each sampling point followed by an LSTM (Long Short-Term Memories) to learn the evolution of these features. Experimental results from a public EEG dataset against the state-of-the-art counterparts indicate that: (1) compared with the existing optimal models, V2IED achieves a larger area under the receiver operating characteristic (ROC) curve in detecting IEDs from normal sharp transients with a 5.25% improvement in accuracy; (2) the introduction of spatial features improves performance by 2.4% in accuracy; and (3) V2IED also performs excellently in distinguishing IEDs from background signals especially benign variants.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000522",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Electroencephalography",
      "Feature (linguistics)",
      "Feature learning",
      "Focus (optics)",
      "Ictal",
      "Linguistics",
      "Neuroscience",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Psychology",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Ming",
        "given_name": "Zhekai"
      },
      {
        "surname": "Chen",
        "given_name": "Dan"
      },
      {
        "surname": "Gao",
        "given_name": "Tengfei"
      },
      {
        "surname": "Tang",
        "given_name": "Yunbo"
      },
      {
        "surname": "Tu",
        "given_name": "Weiping"
      },
      {
        "surname": "Chen",
        "given_name": "Jingying"
      }
    ]
  },
  {
    "title": "GSB: Group superposition binarization for vision transformer with limited training samples",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106133",
    "abstract": "Vision Transformer (ViT) has performed remarkably in various computer vision tasks. Nonetheless, affected by the massive amount of parameters, ViT usually suffers from serious overfitting problems with a relatively limited number of training samples. In addition, ViT generally demands heavy computing resources, which limit its deployment on resource-constrained devices. As a type of model-compression method, model binarization is potentially a good choice to solve the above problems. Compared with the full-precision one, the model with the binarization method replaces complex tensor multiplication with simple bit-wise binary operations and represents full-precision model parameters and activations with only 1-bit ones, which potentially solves the problem of model size and computational complexity, respectively. In this paper, we investigate a binarized ViT model. Empirically, we observe that the existing binarization technology designed for Convolutional Neural Networks (CNN) cannot migrate well to a ViT’s binarization task. We also find that the decline of the accuracy of the binary ViT model is mainly due to the information loss of the Attention module and the Value vector. Therefore, we propose a novel model binarization technique, called Group Superposition Binarization (GSB), to deal with these issues. Furthermore, in order to further improve the performance of the binarization model, we have investigated the gradient calculation procedure in the binarization process and derived more proper gradient calculation equations for GSB to reduce the influence of gradient mismatch. Then, the knowledge distillation technique is introduced to alleviate the performance degradation caused by model binarization. Analytically, model binarization can limit the parameter’s search space during parameter updates while training a model. Therefore, the binarization process can actually play an implicit regularization role and help solve the problem of overfitting in the case of insufficient training data. Experiments on three datasets with limited numbers of training samples demonstrate that the proposed GSB model achieves state-of-the-art performance among the binary quantization schemes and exceeds its full-precision counterpart on some indicators. Code and models are available at: https://github.com/IMRL/GSB-Vision-Transformer.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000492",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Superposition principle",
      "Training set",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Gao",
        "given_name": "Tian"
      },
      {
        "surname": "Xu",
        "given_name": "Cheng-Zhong"
      },
      {
        "surname": "Zhang",
        "given_name": "Le"
      },
      {
        "surname": "Kong",
        "given_name": "Hui"
      }
    ]
  },
  {
    "title": "A continuous-time neurodynamic approach in matrix form for rank minimization",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106128",
    "abstract": "This article proposes a continuous-time neurodynamic approach for solving the rank minimization under affine constraints. As opposed to the traditional neurodynamic approach, the proposed neurodynamic approach extends the form of the variables from the vector form to the matrix form. First, a continuous-time neurodynamic approach with variables in matrix form is developed by combining the optimal rank r projection and the gradient. Then, the optimality of the proposed neurodynamic approach is rigorously analyzed by demonstrating that the objective function satisfies the functional property which is called as ( 2 r , 4 r ) -restricted strong convexity and smoothness ( ( 2 r , 4 r ) -RSCS). Furthermore, the convergence and stability analysis of the proposed neurodynamic approach is rigorously conducted by establishing appropriate Lyapunov functions and considering the relevant restricted isometry property (RIP) condition associated with the affine transformation. Finally, through experiments involving low-rank matrix recovery under affine transformations and the completion of low-rank real image, the effectiveness of this approach has been demonstrated, along with its superiority compared to the vector-based approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000443",
    "keywords": [
      "Affine transformation",
      "Applied mathematics",
      "Biochemistry",
      "Chemistry",
      "Combinatorics",
      "Composite material",
      "Computer science",
      "Convexity",
      "Economics",
      "Epistemology",
      "Financial economics",
      "Gene",
      "Materials science",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Matrix (chemical analysis)",
      "Philosophy",
      "Property (philosophy)",
      "Pure mathematics",
      "Rank (graph theory)",
      "Smoothness",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Meng"
      },
      {
        "surname": "He",
        "given_name": "Xing"
      }
    ]
  },
  {
    "title": "IremulbNet: Rethinking the inverted residual architecture for image recognition",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106140",
    "abstract": "An increasing need of running Convolutional Neural Network (CNN) models on mobile devices encourages the studies on efficient and lightweight neural network model. In this paper, an Inverse Residual Multi-Branch Network named IremulbNet is proposed to solve the problem of insufficient classification accuracy in existing lightweight network models. The core module of this model is to reconstruct an inverse residual structure, in which a special feature fusion method, multi-branch feature extraction, and depthwise separable convolution techniques are used to improve the classification accuracy. The performance of model is tested using image databases. Experimental results show that for the fine-grained image dataset Imagenet-woof, IremulbNet achieved 10.9%, 12.2%, and 15.3% higher accuracy than that of MobileNet V3, ShuffleNet V2, and PeleeNet, respectively. Moreover, it can reduce inference time (GPU) about 42.09% and 75.56% compared to classic ResNet50 and DenseNet121.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400056X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Contextual image classification",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Data mining",
      "Feature (linguistics)",
      "Feature extraction",
      "Image (mathematics)",
      "Inference",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Residual"
    ],
    "authors": [
      {
        "surname": "Su",
        "given_name": "Tiantian"
      },
      {
        "surname": "Liu",
        "given_name": "Anan"
      },
      {
        "surname": "Shi",
        "given_name": "Yongran"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiaofeng"
      }
    ]
  },
  {
    "title": "Trainable Spiking-YOLO for low-latency and high-performance object detection",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.106092",
    "abstract": "Spiking neural networks (SNNs) are considered an attractive option for edge-side applications due to their sparse, asynchronous and event-driven characteristics. However, the application of SNNs to object detection tasks faces challenges in achieving good detection accuracy and high detection speed. To overcome the aforementioned challenges, we propose an end-to-end Trainable Spiking-YOLO (Tr-Spiking-YOLO) for low-latency and high-performance object detection. We evaluate our model on not only frame-based PASCAL VOC dataset but also event-based GEN1 Automotive Detection dataset, and investigate the impacts of different decoding methods on detection performance. The experimental results show that our model achieves competitive/better performance in terms of accuracy, latency and energy consumption compared to similar artificial neural network (ANN) and conversion-based SNN object detection model. Furthermore, when deployed on an edge device, our model achieves a processing speed of approximately from 14 to 39 FPS while maintaining a desirable mean Average Precision (mAP), which is capable of real-time detection on resource-constrained platforms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007530",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Asynchronous communication",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Frame rate",
      "Latency (audio)",
      "Object detection",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Programming language",
      "Real-time computing",
      "Spiking neural network",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Yuan",
        "given_name": "Mengwen"
      },
      {
        "surname": "Zhang",
        "given_name": "Chengjun"
      },
      {
        "surname": "Wang",
        "given_name": "Ziming"
      },
      {
        "surname": "Liu",
        "given_name": "Huixiang"
      },
      {
        "surname": "Pan",
        "given_name": "Gang"
      },
      {
        "surname": "Tang",
        "given_name": "Huajin"
      }
    ]
  },
  {
    "title": "SATCount: A scale-aware transformer-based class-agnostic counting framework",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106126",
    "abstract": "This paper studies the class-agnostic counting problem, which aims to count objects regardless of their class, and relies only on a limited number of exemplar objects. Existing methods usually extract visual features from query and exemplar images, compute similarity between them using convolution operations, and finally use this information to estimate object counts. However, these approaches often overlook the scale information of the exemplar objects, leading to lower counting accuracy for objects with multi-scale characteristics. Additionally, convolution operations are local linear matching processes that may result in a loss of semantic information, which can limit the performance of the counting algorithm. To address these issues, we devise a new scale-aware transformer-based feature fusion module that integrates visual and scale information of exemplar objects and models similarity between samples and queries using cross-attention. Finally, we propose an object counting algorithm based on a feature extraction backbone, a feature fusion module and a density map regression head, called SATCount. Our experiments on the FSC-147 and the CARPK demonstrate that our model outperforms the state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400042X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Class (philosophy)",
      "Computer science",
      "Convolution (computer science)",
      "Data mining",
      "Feature (linguistics)",
      "Feature extraction",
      "Image (mathematics)",
      "Linguistics",
      "Matching (statistics)",
      "Mathematics",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)",
      "Similarity (geometry)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yutian"
      },
      {
        "surname": "Yang",
        "given_name": "Bin"
      },
      {
        "surname": "Wang",
        "given_name": "Xi"
      },
      {
        "surname": "Liang",
        "given_name": "Chao"
      },
      {
        "surname": "Chen",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Importance-aware adaptive dataset distillation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106154",
    "abstract": "Herein, we propose a novel dataset distillation method for constructing small informative datasets that preserve the information of the large original datasets. The development of deep learning models is enabled by the availability of large-scale datasets. Despite unprecedented success, large-scale datasets considerably increase the storage and transmission costs, resulting in a cumbersome model training process. Moreover, using raw data for training raises privacy and copyright concerns. To address these issues, a new task named dataset distillation has been introduced, aiming to synthesize a compact dataset that retains the essential information from the large original dataset. State-of-the-art (SOTA) dataset distillation methods have been proposed by matching gradients or network parameters obtained during training on real and synthetic datasets. The contribution of different network parameters to the distillation process varies, and uniformly treating them leads to degraded distillation performance. Based on this observation, we propose an importance-aware adaptive dataset distillation (IADD) method that can improve distillation performance by automatically assigning importance weights to different network parameters during distillation, thereby synthesizing more robust distilled datasets. IADD demonstrates superior performance over other SOTA dataset distillation methods based on parameter matching on multiple benchmark datasets and outperforms them in terms of cross-architecture generalization. In addition, the analysis of self-adaptive weights demonstrates the effectiveness of IADD. Furthermore, the effectiveness of IADD is validated in a real-world medical application such as COVID-19 detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000789",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Distillation",
      "Generalization",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Matching (statistics)",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Organic chemistry",
      "Physics",
      "Process (computing)",
      "Quantum mechanics",
      "Scale (ratio)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Guang"
      },
      {
        "surname": "Togo",
        "given_name": "Ren"
      },
      {
        "surname": "Ogawa",
        "given_name": "Takahiro"
      },
      {
        "surname": "Haseyama",
        "given_name": "Miki"
      }
    ]
  },
  {
    "title": "Understanding the role of pathways in a deep neural network",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106095",
    "abstract": "Deep neural networks have demonstrated superior performance in artificial intelligence applications, but the opaqueness of their inner working mechanism is one major drawback in their application. The prevailing unit-based interpretation is a statistical observation of stimulus–response data, which fails to show a detailed internal process of inherent mechanisms of neural networks. In this work, we analyze a convolutional neural network (CNN) trained in the classification task and present an algorithm to extract the diffusion pathways of individual pixels to identify the locations of pixels in an input image associated with object classes. The pathways allow us to test the causal components which are important for classification and the pathway-based representations are clearly distinguishable between categories. We find that the few largest pathways of an individual pixel from an image tend to cross the feature maps in each layer that is important for classification. And the large pathways of images of the same category are more consistent in their trends than those of different categories. We also apply the pathways to understanding adversarial attacks, object completion, and movement perception. Further, the total number of pathways on feature maps in all layers can clearly discriminate the original, deformed, and target samples.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000017",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Classifier (UML)",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Feature (linguistics)",
      "Linguistics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel"
    ],
    "authors": [
      {
        "surname": "Lyu",
        "given_name": "Lei"
      },
      {
        "surname": "Pang",
        "given_name": "Chen"
      },
      {
        "surname": "Wang",
        "given_name": "Jihua"
      }
    ]
  },
  {
    "title": "Strangeness-driven exploration in multi-agent reinforcement learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106149",
    "abstract": "In this study, a novel exploration method for centralized training and decentralized execution (CTDE)-based multi-agent reinforcement learning (MARL) is introduced. The method uses the concept of strangeness, which is determined by evaluating (1) the level of the unfamiliarity of the observations an agent encounters and (2) the level of the unfamiliarity of the entire state the agents visit. An exploration bonus, which is derived from the concept of strangeness, is combined with the extrinsic reward obtained from the environment to form a mixed reward, which is then used for training CTDE-based MARL algorithms. Additionally, a separate action-value function is also proposed to prevent the high exploration bonus from overwhelming the sensitivity to extrinsic rewards during MARL training. This separate function is used to design the behavioral policy for generating transitions. The proposed method is not much affected by stochastic transitions commonly observed in MARL tasks and improves the stability of CTDE-based MARL algorithms when used with an exploration method. By providing didactic examples and demonstrating the substantial performance improvement of our proposed exploration method in CTDE-based MARL algorithms, we illustrate the advantages of our approach. These evaluations highlight how our method outperforms state-of-the-art MARL baselines on challenging tasks within the StarCraft II micromanagement benchmark, underscoring its effectiveness in improving MARL.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400073X",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Baryon",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Evolutionary biology",
      "Function (biology)",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Marl",
      "Paleontology",
      "Particle physics",
      "Physics",
      "Psychology",
      "Reinforcement",
      "Reinforcement learning",
      "Social psychology",
      "Stability (learning theory)",
      "State (computer science)",
      "Strangeness",
      "Structural basin"
    ],
    "authors": [
      {
        "surname": "Kim",
        "given_name": "Ju-Bong"
      },
      {
        "surname": "Choi",
        "given_name": "Ho-Bin"
      },
      {
        "surname": "Han",
        "given_name": "Youn-Hee"
      }
    ]
  },
  {
    "title": "Explanatory subgraph attacks against Graph Neural Networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106097",
    "abstract": "Graph Neural Networks (GNNs) are often viewed as black boxes due to their lack of transparency, which hinders their application in critical fields. Many explanation methods have been proposed to address the interpretability issue of GNNs. These explanation methods reveal explanatory information about graphs from different perspectives. However, the explanatory information may also pose an attack risk to GNN models. In this work, we will explore this problem from the explanatory subgraph perspective. To this end, we utilize a powerful GNN explanation method called SubgraphX and deploy it locally to obtain explanatory subgraphs from given graphs. Then we propose methods for conducting evasion attacks and backdoor attacks based on the local explainer. In evasion attacks, the attacker gets explanatory subgraphs of test graphs from the local explainer and replace their explanatory subgraphs with an explanatory subgraph of other labels, making the target model misclassify test graphs as wrong labels. In backdoor attacks, the attacker employs the local explainer to select an explanatory trigger and locate suitable injection locations. We validate the effectiveness of our proposed attacks on state-of-art GNN models and different datasets. The results also demonstrate that our proposed backdoor attack is more efficient, adaptable, and concealed than previous backdoor attacks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000030",
    "keywords": [
      "Artificial intelligence",
      "Backdoor",
      "Computer science",
      "Computer security",
      "Explanatory model",
      "Interpretability",
      "Machine learning",
      "Mathematics",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Huiwei"
      },
      {
        "surname": "Liu",
        "given_name": "Tianhua"
      },
      {
        "surname": "Sheng",
        "given_name": "Ziyu"
      },
      {
        "surname": "Li",
        "given_name": "Huaqing"
      }
    ]
  },
  {
    "title": "Corruption depth: Analysis of DNN depth for misclassification",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.035",
    "abstract": "Many large and complex deep neural networks have been shown to provide higher performance on various computer vision tasks. However, very little is known about the relationship between the complexity of the input data along with the type of noise and the depth needed for correct classification. Existing studies do not address the issue of common corruptions adequately, especially in understanding what impact these corruptions leave on the individual part of a deep neural network. Therefore, we can safely assume that the classification (or misclassification) might be happening at a particular layer(s) of a network that accumulates to draw a final correct or incorrect prediction. In this paper, we introduce a novel concept of corruption depth, which identifies the location of the network layer/depth until the misclassification persists. We assert that the identification of such layers will help in better designing the network by pruning certain layers in comparison to the purification of the entire network which is computationally heavy. Through our extensive experiments, we present a coherent study to understand the processing of examples through the network. Our approach also illustrates different philosophies of example memorization and a one-dimensional view of sample or query difficulty. We believe that the understanding of the corruption depth can open a new dimension of model explainability and model compression, where in place of just visualizing the attention map, the classification progress can be seen throughout the network.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006585",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Botany",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Data mining",
      "Identification (biology)",
      "Layer (electronics)",
      "Machine learning",
      "Mathematics",
      "Mathematics education",
      "Memorization",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Pruning",
      "Sample (material)"
    ],
    "authors": [
      {
        "surname": "Agarwal",
        "given_name": "Akshay"
      },
      {
        "surname": "Vatsa",
        "given_name": "Mayank"
      },
      {
        "surname": "Singh",
        "given_name": "Richa"
      },
      {
        "surname": "Ratha",
        "given_name": "Nalini"
      }
    ]
  },
  {
    "title": "Fixed-time synchronization of complex-valued neural networks for image protection and 3D point cloud information protection",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.043",
    "abstract": "This paper studies the fixed-time synchronization (FDTS) of complex-valued neural networks (CVNNs) based on quantized intermittent control (QIC) and applies it to image protection and 3D point cloud information protection. A new controller was designed which achieved FDTS of the CVNNs, with the estimation of the convergence time not dependent on the initial state. Our approach divides the neural network into two real-valued systems and then combines the framework of the Lyapunov method to give criteria for FDTS. Applying synchronization to image protection, the image will be encrypted with a drive system sequence and decrypted with a response system sequence. The quality of image encryption and decryption depends on the synchronization error. Meanwhile, the depth image of the object is encrypted and then the 3D point cloud is reconstructed based on the decrypted depth image. This means that the 3D point cloud information is protected. Finally, simulation examples verify the efficacy of the controller and the synchronization criterion, giving results for applications in image protection and 3D point cloud information protection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007505",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Channel (broadcasting)",
      "Cloud computing",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Controller (irrigation)",
      "Encryption",
      "Image (mathematics)",
      "Operating system",
      "Point cloud",
      "Real-time computing",
      "Synchronization (alternating current)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Wenqiang"
      },
      {
        "surname": "Huang",
        "given_name": "Junjian"
      },
      {
        "surname": "He",
        "given_name": "Xing"
      },
      {
        "surname": "Wen",
        "given_name": "Shiping"
      }
    ]
  },
  {
    "title": "Reversible gender privacy enhancement via adversarial perturbations",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106130",
    "abstract": "The significant advancement in deep learning has made it feasible to extract gender from faces accurately. However, such unauthorized extraction would pose potential threats to individual privacy. Existing protection schemes for gender privacy have exhibited satisfactory performance. Nevertheless, they suffer from gender inference from gender-related attributes and fail to support the recovery of the original image. In this paper, we propose a novel gender privacy protection scheme that aims to enhance gender privacy while supporting reversibility. Firstly, our scheme utilizes continuously optimized adversarial perturbations to prevent gender recognition from unauthorized classifiers. Meanwhile, gender-related attributes are concealed for classifiers, which prevents the inference of gender from these attributes, thereby enhancing gender privacy. Moreover, an identity preservation constraint is added to maintain identity preservation. Secondly, reversibility is supported by a reversible image transformation, allowing the perturbations to be securely removed to losslessly recover the original face when required. Extensive experiments demonstrate the effectiveness of our scheme in gender privacy protection, identity preservation, and reversibility.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000467",
    "keywords": [
      "Acoustics",
      "Adversarial system",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Constraint (computer-aided design)",
      "Gene",
      "Geometry",
      "Identity (music)",
      "Image (mathematics)",
      "Inference",
      "Mathematical analysis",
      "Mathematics",
      "Physics",
      "Scheme (mathematics)",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Xie",
        "given_name": "Yiyi"
      },
      {
        "surname": "Zhou",
        "given_name": "Yuqian"
      },
      {
        "surname": "Wang",
        "given_name": "Tao"
      },
      {
        "surname": "Wen",
        "given_name": "Wenying"
      },
      {
        "surname": "Yi",
        "given_name": "Shuang"
      },
      {
        "surname": "Zhang",
        "given_name": "Yushu"
      }
    ]
  },
  {
    "title": "Physics-guided neural network for predicting asphalt mixture rutting with balanced accuracy, stability and rationality",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.039",
    "abstract": "The prediction of rutting performance of asphalt materials poses a significant challenge due to the intricate relationships between the rutting performance and its influencing factors. Machine learning models have gained popularity to address this challenge by offering sophisticated model structures and algorithms. However, existing models often prioritize model accuracy over stability and rationality. The increasingly complicated model structure may lead to an imbalance between the data and the model, resulting in issues such as overfitting and reduced model applicability and interpretability. In this context, this study proposes a novel modeling framework to predict the rutting performance of asphalt mixture by utilizing autoencoder for feature selection and feedforward neural network for rut depth prediction. Notably, physics information of the selected variables is implemented into the neural network to achieve the appropriate balance of model accuracy, stability, and rationality. The results demonstrate that while maintaining high model accuracy, the implementation of physics information significantly enhances the model’s stability and rationality. This framework holds great potential for accurate and reliable predictions of pavement distress by leveraging the complementary strengths of data-driven machine learning and physics-based domain knowledge.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007463",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Asphalt",
      "Autoencoder",
      "Biology",
      "Cartography",
      "Computer science",
      "Context (archaeology)",
      "Feature selection",
      "Geography",
      "Interpretability",
      "Law",
      "Machine learning",
      "Overfitting",
      "Paleontology",
      "Political science",
      "Rationality",
      "Rut",
      "Stability (learning theory)"
    ],
    "authors": [
      {
        "surname": "Deng",
        "given_name": "Yong"
      },
      {
        "surname": "Wang",
        "given_name": "Haifeng"
      },
      {
        "surname": "Shi",
        "given_name": "Xianming"
      }
    ]
  },
  {
    "title": "A novel transformer autoencoder for multi-modal emotion recognition with incomplete data",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106111",
    "abstract": "Multi-modal signals have become essential data for emotion recognition since they can represent emotions more comprehensively. However, in real-world environments, it is often impossible to acquire complete data on multi-modal signals, and the problem of missing modalities causes severe performance degradation in emotion recognition. Therefore, this paper represents the first attempt to use a transformer-based architecture, aiming to fill the modality-incomplete data from partially observed data for multi-modal emotion recognition (MER). Concretely, this paper proposes a novel unified model called transformer autoencoder (TAE), comprising a modality-specific hybrid transformer encoder, an inter-modality transformer encoder, and a convolutional decoder. The modality-specific hybrid transformer encoder bridges a convolutional encoder and a transformer encoder, allowing the encoder to learn local and global context information within each particular modality. The inter-modality transformer encoder builds and aligns global cross-modal correlations and models long-range contextual information with different modalities. The convolutional decoder decodes the encoding features to produce more precise recognition. Besides, a regularization term is introduced into the convolutional decoder to force the decoder to fully leverage the complete and incomplete data for emotional recognition of missing data. 96.33%, 95.64%, and 92.69% accuracies are attained on the available data of the DEAP and SEED-IV datasets, and 93.25%, 92.23%, and 81.76% accuracies are obtained on the missing data. Particularly, the model acquires a 5.61% advantage with 70% missing data, demonstrating that the model outperforms some state-of-the-art approaches in incomplete multi-modal learning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802400025X",
    "keywords": [
      "Artificial intelligence",
      "Autoencoder",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Electrical engineering",
      "Encoder",
      "Engineering",
      "Operating system",
      "Pattern recognition (psychology)",
      "Speech recognition",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Cheng",
        "given_name": "Cheng"
      },
      {
        "surname": "Liu",
        "given_name": "Wenzhe"
      },
      {
        "surname": "Fan",
        "given_name": "Zhaoxin"
      },
      {
        "surname": "Feng",
        "given_name": "Lin"
      },
      {
        "surname": "Jia",
        "given_name": "Ziyu"
      }
    ]
  },
  {
    "title": "Sampled-based adaptive event-triggered resilient control for multiagent systems with hybrid cyber-attacks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.044",
    "abstract": "The multiagent systems have shared broad application in many practical systems including unmanned aircraft clusters, intelligent robots, and intelligent transportation. However, many unexpected cyber-attacks may disturb or disrupt the normal communication of the agents, thus reducing the interacting efficiency of multiagent systems. Ever since the cyber-attacks have been proposed, the resilient control problem for multiagent systems has been intensively explored in light of the communication network growth. However, most of the consequences only focused on denial-of-service (DoS) attacks or deception attacks independently. Distinguished from the existing resilient control mechanisms, the current investigation represents the first attempt at designing an adaptive resilient controller for multiagent systems according to the sampled-based adaptive event-triggered manner, where denial-of-service (DoS) attacks and deception attacks are both considered. First, the hybrid cyber-attacks model and its impact on the closed-loop system are addressed. And then, an adaptive event-triggered strategy is proposed to reduce network resource consumption and ease the communication burden, where the designed adaptive law can automatically adjust the triggering threshold. Finally, the consensus state of multiagent systems is capable of achieving via a series of reasonable control rules formulated through Lyapunov functional approach despite suffering hybrid cyber-attacks. And a simulation example is given to substantiate the feasibility of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007517",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer security",
      "Control (management)",
      "Controller (irrigation)",
      "Deception",
      "Denial-of-service attack",
      "Distributed computing",
      "Event (particle physics)",
      "Multi-agent system",
      "Physics",
      "Psychology",
      "Quantum mechanics",
      "Social psychology",
      "The Internet",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Tan",
        "given_name": "Lihua"
      },
      {
        "surname": "Wang",
        "given_name": "Xin"
      }
    ]
  },
  {
    "title": "Self-guided disentangled representation learning for single image dehazing",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106107",
    "abstract": "Image dehazing has received extensive research attention as images collected in hazy weather are limited by low visibility and information dropout. Recently, disentangled representation learning has made excellent progress in various vision tasks. However, existing networks for low-level vision tasks lack efficient feature interaction and delivery mechanisms in the disentanglement process or an evaluation mechanism for the degree of decoupling in the reconstruction process, rendering direct application to image dehazing challenging. We propose a self-guided disentangled representation learning (SGDRL) algorithm with a self-guided disentangled network to realize multi-level progressive feature decoupling through sharing and interaction. The self-guided disentangled (SGD) network extracts image features using the multi-layer backbone network, and attribute features are weighted using the self-guided attention mechanism for the backbone features. In addition, we introduce a disentanglement-guided (DG) module to evaluate the degree of feature decomposition and guide the feature fusion process in the reconstruction stage. Accordingly, we develop SGDRL-based unsupervised and semi-supervised single image dehazing networks. Extensive experiments demonstrate the superiority of the proposed method for real-world image dehazing. The source code is available at https://github.com/dehazing/SGDRL.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000212",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Control engineering",
      "Decoupling (probability)",
      "Dropout (neural networks)",
      "Engineering",
      "Feature (linguistics)",
      "Feature learning",
      "Image (mathematics)",
      "Law",
      "Linguistics",
      "Machine learning",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Process (computing)",
      "Rendering (computer graphics)",
      "Representation (politics)",
      "Visibility"
    ],
    "authors": [
      {
        "surname": "Jia",
        "given_name": "Tongyao"
      },
      {
        "surname": "Li",
        "given_name": "Jiafeng"
      },
      {
        "surname": "Zhuo",
        "given_name": "Li"
      },
      {
        "surname": "Zhang",
        "given_name": "Jing"
      }
    ]
  },
  {
    "title": "Preserving domain private information via mutual information maximization",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106112",
    "abstract": "Recent advances in unsupervised domain adaptation have shown that mitigating the domain divergence by extracting the domain-invariant features could significantly improve the generalization of a model with respect to a new data domain. However, current methodologies often neglect to retain domain private information, which is the unique information inherent to the unlabeled new domain, compromising generalization. This paper presents a novel method that utilizes mutual information to protect this domain-specific information, ensuring that the latent features of the unlabeled data not only remain domain-invariant but also reflect the unique statistics of the unlabeled domain. We show that simultaneous maximization of mutual information and reduction of domain divergence can effectively preserve domain-private information. We further illustrate that a neural estimator can aptly estimate the mutual information between the unlabeled input space and its latent feature space. Both theoretical analysis and empirical results validate the significance of preserving such unique information of the unlabeled domain for cross-domain generalization. Comparative evaluations reveal our method’s superiority over existing state-of-the-art techniques across multiple benchmark datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000261",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Divergence (linguistics)",
      "Domain (mathematical analysis)",
      "Generalization",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Mutual information",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Jiahong"
      },
      {
        "surname": "Wang",
        "given_name": "Jing"
      },
      {
        "surname": "Lin",
        "given_name": "Weipeng"
      },
      {
        "surname": "Zhang",
        "given_name": "Kuangen"
      },
      {
        "surname": "de Silva",
        "given_name": "Clarence W."
      }
    ]
  },
  {
    "title": "Distributed continuous-time accelerated neurodynamic approaches for sparse recovery via smooth approximation to L 1 -minimization",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106123",
    "abstract": "This paper develops two continuous-time distributed accelerated neurodynamic approaches for solving sparse recovery via smooth approximation to L 1 -norm minimization problem. First, the L 1 -norm minimization problem is converted into a distributed smooth optimization problem by utilizing multiagent consensus theory and smooth approximation. Then, a distributed primal–dual accelerated neurodynamic approach is designed by using Karush–Kuhn–Tucker (KKT) condition and Nesterov’s accelerated method. Furthermore, in order to reduce the structure complexity of the presented neurodynamic approach, based on the projection matrix, we eliminate a dual variable in the KKT condition and propose a distributed accelerated neurodynamic approach with a simpler structure. It is proved that the two proposed distributed neurodynamic approaches both achieve O ( 1 t 2 ) convergence rate. Finally, the simulation results of sparse recovery are given to demonstrate the effectiveness of the proposed approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000376",
    "keywords": [
      "Algorithm",
      "Art",
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Compressed sensing",
      "Computer network",
      "Computer science",
      "Convergence (economics)",
      "Dual (grammatical number)",
      "Economic growth",
      "Economics",
      "Karush–Kuhn–Tucker conditions",
      "Literature",
      "Mathematical optimization",
      "Mathematics",
      "Minification",
      "Projection (relational algebra)",
      "Rate of convergence"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Junpeng"
      },
      {
        "surname": "He",
        "given_name": "Xing"
      }
    ]
  },
  {
    "title": "ConTIG: Continuous representation learning on temporal interaction graphs",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106151",
    "abstract": "Representation learning on temporal interaction graphs (TIG) aims to model complex networks with the dynamic evolution of interactions on a wide range of web and social graph applications. However, most existing works on TIG either (a) rely on discretely updated node embeddings merely when an interaction occurs that fail to capture the continuous evolution of embedding trajectories of nodes, or (b) overlook the rich temporal patterns hidden in the ever-changing graph data that presumably lead to sub-optimal models. In this paper, we propose a two-module framework named ConTIG, a novel representation learning method on TIG that captures the continuous dynamic evolution of node embedding trajectories. With two essential modules, our model exploits three-fold factors in dynamic networks including latest interaction, neighbor features, and inherent characteristics. In the first update module, we employ a continuous inference block to learn the nodes’ state trajectories from time-adjacent interaction patterns using ordinary differential equations. In the second transform module, we introduce a self-attention mechanism to predict future node embeddings by aggregating historical temporal interaction information. Experiment results demonstrate the superiority of ConTIG on temporal link prediction, temporal node recommendation, and dynamic node classification tasks of four datasets compared with a range of state-of-the-art baselines, especially for long-interval interaction prediction.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000753",
    "keywords": [
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Computer science",
      "Embedding",
      "Engineering",
      "Feature learning",
      "Geometry",
      "Graph",
      "Graph embedding",
      "Inference",
      "Law",
      "Mathematics",
      "Node (physics)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Structural engineering",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zihui"
      },
      {
        "surname": "Yang",
        "given_name": "Peizhen"
      },
      {
        "surname": "Fan",
        "given_name": "Xiaoliang"
      },
      {
        "surname": "Yan",
        "given_name": "Xu"
      },
      {
        "surname": "Wu",
        "given_name": "Zonghan"
      },
      {
        "surname": "Pan",
        "given_name": "Shirui"
      },
      {
        "surname": "Chen",
        "given_name": "Longbiao"
      },
      {
        "surname": "Zang",
        "given_name": "Yu"
      },
      {
        "surname": "Wang",
        "given_name": "Cheng"
      },
      {
        "surname": "Yu",
        "given_name": "Rongshan"
      }
    ]
  },
  {
    "title": "Instantaneous estimation of momentary affective responses using neurophysiological signals and a spatiotemporal emotional intensity regression network",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.034",
    "abstract": "Previous studies in affective computing often use a fixed emotional label to train an emotion classifier with electroencephalography (EEG) from individuals experiencing an affective stimulus. However, EEGs encode emotional dynamics that include varying intensities within a given emotional category. To investigate these variations in emotional intensity, we propose a framework that obtains momentary affective labels for fine-grained segments of EEGs with human feedback. We then model these labeled segments using a novel spatiotemporal emotional intensity regression network (STEIR-Net). It integrates temporal EEG patterns from nine predefined cortical regions to provide a continuous estimation of emotional intensity. We demonstrate that the STEIR-Net outperforms classical regression models by reducing the root mean square error (RMSE) by an average of 4∼9 % and 2∼4 % for the SEED and SEED-IV databases, respectively. We find that the frontal and temporal cortical regions contribute significantly to the affective intensity's variation. Higher absolute values of the Spearman correlation coefficient between the model estimation and momentary affective labels under happiness (0.2114) and fear (0.2072) compared to neutral (0.1694) and sad (0.1895) emotions were observed. Besides, increasing the input length of the EEG segments from 4 to 20 s further reduces the RMSE from 1.3548 to 1.3188.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007414",
    "keywords": [
      "Affective computing",
      "Artificial intelligence",
      "Cognitive psychology",
      "Computer science",
      "Correlation",
      "Electroencephalography",
      "Geometry",
      "Happiness",
      "Mathematics",
      "Mean squared error",
      "Neurophysiology",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Psychology",
      "Regression",
      "Regression analysis",
      "Social psychology",
      "Statistics",
      "Stimulus (psychology)"
    ],
    "authors": [
      {
        "surname": "Gan",
        "given_name": "Kaiyu"
      },
      {
        "surname": "Li",
        "given_name": "Ruiding"
      },
      {
        "surname": "Zhang",
        "given_name": "Jianhua"
      },
      {
        "surname": "Sun",
        "given_name": "Zhanquan"
      },
      {
        "surname": "Yin",
        "given_name": "Zhong"
      }
    ]
  },
  {
    "title": "Decoding emotion with phase–amplitude fusion features of EEG functional connectivity network",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106148",
    "abstract": "Decoding emotional neural representations from the electroencephalographic (EEG)-based functional connectivity network (FCN) is of great scientific importance for uncovering emotional cognition mechanisms and developing harmonious human–computer interactions. However, existing methods mainly rely on phase-based FCN measures (e.g., phase locking value [PLV]) to capture dynamic interactions between brain oscillations in emotional states, which fail to reflect the energy fluctuation of cortical oscillations over time. In this study, we initially examined the efficacy of amplitude-based functional networks (e.g., amplitude envelope correlation [AEC]) in representing emotional states. Subsequently, we proposed an efficient phase–amplitude fusion framework (PAF) to fuse PLV and AEC and used common spatial pattern (CSP) to extract fused spatial topological features from PAF for multi-class emotion recognition. We conducted extensive experiments on the DEAP and MAHNOB-HCI datasets. The results showed that: (1) AEC-derived discriminative spatial network topological features possess the ability to characterize emotional states, and the differential network patterns of AEC reflect dynamic interactions in brain regions associated with emotional cognition. (2) The proposed fusion features outperformed other state-of-the-art methods in terms of classification accuracy for both datasets. Moreover, the spatial filter learned from PAF is separable and interpretable, enabling a description of affective activation patterns from both phase and amplitude perspectives.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000728",
    "keywords": [
      "Algorithm",
      "Amplitude",
      "Artificial intelligence",
      "Computer science",
      "Decoding methods",
      "Electroencephalography",
      "Functional connectivity",
      "Fusion",
      "Linguistics",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Phase (matter)",
      "Philosophy",
      "Physics",
      "Psychology",
      "Quantum mechanics",
      "Speech recognition"
    ],
    "authors": [
      {
        "surname": "Hu",
        "given_name": "Liangliang"
      },
      {
        "surname": "Tan",
        "given_name": "Congming"
      },
      {
        "surname": "Xu",
        "given_name": "Jiayang"
      },
      {
        "surname": "Qiao",
        "given_name": "Rui"
      },
      {
        "surname": "Hu",
        "given_name": "Yilin"
      },
      {
        "surname": "Tian",
        "given_name": "Yin"
      }
    ]
  },
  {
    "title": "Contrastive learning of graphs under label noise",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106113",
    "abstract": "In the domain of graph-structured data learning, semi-supervised node classification serves as a critical task, relying mainly on the information from unlabeled nodes and a minor fraction of labeled nodes for training. However, real-world graph-structured data often suffer from label noise, which significantly undermines the performance of Graph Neural Networks (GNNs). This problem becomes increasingly severe in situations where labels are scarce. To tackle this issue of sparse and noisy labels, we propose a novel approach Contrastive Robust Graph Neural Network (CR-GNN), Firstly, considering label sparsity and noise, we employ unsupervised contrastive loss and further incorporate homophily in the graph structure, thus introducing neighbor contrastive loss. Moreover, data augmentation is typically used to construct positive and negative samples in contrastive learning, which may result in inconsistent prediction outcomes. Based on this, we propose a dynamic cross-entropy loss, which selects the nodes with consistent predictions as reliable nodes for cross-entropy loss and benefits to mitigate the overfitting to labeling noise. Finally, we propose cross-space consistency to narrow the semantic gap between the contrast and classification spaces. Extensive experiments on multiple publicly available datasets demonstrate that CR-GNN notably outperforms existing methods in resisting label noise.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000273",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Cross entropy",
      "Entropy (arrow of time)",
      "Graph",
      "Machine learning",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Xianxian"
      },
      {
        "surname": "Li",
        "given_name": "Qiyu"
      },
      {
        "surname": "Li",
        "given_name": "De"
      },
      {
        "surname": "Qian",
        "given_name": "Haodong"
      },
      {
        "surname": "Wang",
        "given_name": "Jinyan"
      }
    ]
  },
  {
    "title": "Graph global attention network with memory: A deep learning approach for fake news detection",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106115",
    "abstract": "With the proliferation of social media, the detection of fake news has become a critical issue that poses a significant threat to society. The dissemination of fake information can lead to social harm and damage the credibility of information. To address this issue, deep learning has emerged as a promising approach, especially with the development of Natural Language Processing (NLP). This study introduces a novel approach called Graph Global Attention Network with Memory (GANM) for detecting fake news. This approach leverages NLP techniques to encode nodes with news context and user content. It employs three graph convolutional networks to extract informative features from the news propagation network and aggregates endogenous and exogenous user information. This methodology aims to address the challenge of identifying fake news within the context of social media. Innovatively, the GANM combines two strategies. First, a novel global attention mechanism with memory is employed in the GANM to learn the structural homogeneity of news propagation networks, which is the attention mechanism of a single graph with a history of all graphs. Second, we design a module for partial key information learning aggregation to emphasize the acquisition of partial key information in the graph and merge node-level embeddings with graph-level embeddings into fine-grained joint information. Our proposed method provides a new direction in news detection research with a combination of global and partial information and achieves promising performance on real-world datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000297",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Credibility",
      "ENCODE",
      "Gene",
      "Graph",
      "Law",
      "Machine learning",
      "Political science",
      "Social media",
      "Theoretical computer science",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Chang",
        "given_name": "Qian"
      },
      {
        "surname": "Li",
        "given_name": "Xia"
      },
      {
        "surname": "Duan",
        "given_name": "Zhao"
      }
    ]
  },
  {
    "title": "Local structure-aware graph contrastive representation learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.037",
    "abstract": "Traditional Graph Neural Network (GNN), as a graph representation learning method, is constrained by label information. However, Graph Contrastive Learning (GCL) methods, which tackles the label problem effectively, mainly focus on the feature information of the global graph or small subgraph structure (e.g., the first-order neighborhood). In this paper, we propose a Local Structure-aware Graph Contrastive representation Learning method (LS-GCL) to model the structural information of nodes from multiple views. Specifically, we construct the semantic subgraphs that are not limited to the first-order neighbors. For the local view, the semantic subgraph of each target node is input into a shared GNN encoder to obtain the target node embeddings at the subgraph-level. Then, we use a pooling function to generate the subgraph-level graph embeddings. For the global view, considering the original graph preserves indispensable semantic information of nodes, we leverage the shared GNN encoder to learn the target node embeddings at the global graph-level. The proposed LS-GCL model is optimized to maximize the common information among similar instances at three various perspectives through a multi-level contrastive loss function. Experimental results on six datasets illustrate that our method outperforms state-of-the-art graph representation learning approaches for both node classification and link prediction tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802300744X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Feature learning",
      "Graph",
      "Pattern recognition (psychology)",
      "Pooling",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Kai"
      },
      {
        "surname": "Liu",
        "given_name": "Yuan"
      },
      {
        "surname": "Zhao",
        "given_name": "Zijuan"
      },
      {
        "surname": "Ding",
        "given_name": "Peijin"
      },
      {
        "surname": "Zhao",
        "given_name": "Wenqian"
      }
    ]
  },
  {
    "title": "Neural networks with ReLU powers need less depth",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.027",
    "abstract": "Despite the widespread success of deep learning in various applications, neural network theory has been lagging behind. The choice of the activation function plays a critical role in the expressivity of a neural network but for reasons that are not yet fully understood. While the rectified linear unit (ReLU) is currently one of the most popular activation functions, ReLU squared has only recently been empirically shown to be pivotal in producing consistently superior results for state-of-the-art deep learning tasks (So et al., 2021). To analyze the expressivity of neural networks with ReLU powers, we employ the novel framework of Gribonval et al. (2022) based on the classical concept of approximation spaces. We consider the class of functions for which the approximation error decays at a sufficiently fast rate as network complexity, measured by the number of weights, increases. We show that when approximating sufficiently smooth functions that cannot be represented by sufficiently low-degree polynomials, networks with ReLU powers need less depth than those with ReLU. Moreover, if they have the same depth, networks with ReLU powers can have potentially faster approximation rates. Lastly, our computational experiments on approximating the Rastrigin and Ackley functions with deep neural networks showed that ReLU squared and ReLU cubed networks consistently outperform ReLU networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007359",
    "keywords": [
      "Activation function",
      "Algorithm",
      "Approximation error",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Class (philosophy)",
      "Computer science",
      "Deep learning",
      "Deep neural networks",
      "Evolutionary biology",
      "Function (biology)",
      "Function approximation",
      "Mathematics",
      "Time delay neural network",
      "Types of artificial neural networks"
    ],
    "authors": [
      {
        "surname": "Cabanilla",
        "given_name": "Kurt Izak M."
      },
      {
        "surname": "Mohammad",
        "given_name": "Rhudaina Z."
      },
      {
        "surname": "Lope",
        "given_name": "Jose Ernie C."
      }
    ]
  },
  {
    "title": "SecBERT: Privacy-preserving pre-training based neural network inference system",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106135",
    "abstract": "Pre-trained models such as BERT have made great achievements in natural language processing tasks in recent years. In this paper, we investigate the privacy-preserving pre-training based neural network inference in a two-server framework based on additive secret sharing technique. Our protocol allows a resource-restrained client to request two powerful servers to cooperatively process the natural processing tasks without revealing any useful information about its data. We first design a series of secure sub-protocols for non-linear functions used in BERT model. These sub-protocols are expected to have broad applications and of independent interest. Based on the building sub-protocols, we propose SecBERT, a privacy-preserving pre-training based neural network inference protocol. SecBERT is the first cryptographically secure privacy-preserving pre-training based neural network inference protocol. We show security, efficiency and accuracy of SecBERT protocol through comprehensive theoretical analysis and experiments.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000510",
    "keywords": [
      "Alternative medicine",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer network",
      "Computer science",
      "Data mining",
      "Inference",
      "Machine learning",
      "Medicine",
      "Operating system",
      "Pathology",
      "Process (computing)",
      "Protocol (science)",
      "Server"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Hai"
      },
      {
        "surname": "Wang",
        "given_name": "Yongjian"
      }
    ]
  },
  {
    "title": "Rectify representation bias in vision-language models for long-tailed recognition",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106134",
    "abstract": "Natural data typically exhibits a long-tailed distribution, presenting great challenges for recognition tasks. Due to the extreme scarcity of training instances, tail classes often show inferior performance. In this paper, we investigate the problem within the trendy visual-language (VL) framework and find that the performance bottleneck mainly arises from the recognition confusion between tail classes and their highly correlated head classes. Building upon this observation, unlike previous research primarily emphasizing class frequency in addressing long-tailed issues, we take a novel perspective by incorporating a crucial additional factor namely class correlation. Specifically, we model the representation learning procedure for each sample as two parts, i.e., a special part that learns the unique properties of its own class and a common part that learns shared characteristics among classes. By analysis, we discover that the learning process of common representation is easily biased toward head classes. Because of the bias, the network may lean towards the biased common representation as classification criteria, rather than prioritizing the crucial information encapsulated within the specific representation, ultimately leading to recognition confusion. To solve the problem, based on the VL framework, we introduce the rectification contrastive term (ReCT) to rectify the representation bias, according to semantic hints and training status. Extensive experiments on three widely-used long-tailed datasets demonstrate the effectiveness of ReCT. On iNaturalist2018, it achieves an overall accuracy of 75.4%, surpassing the baseline by 3.6 points in a ResNet-50 visual backbone.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000509",
    "keywords": [
      "Artificial intelligence",
      "Bottleneck",
      "Class (philosophy)",
      "Computer science",
      "Embedded system",
      "Law",
      "Machine learning",
      "Natural language processing",
      "Operating system",
      "Pattern recognition (psychology)",
      "Perspective (graphical)",
      "Political science",
      "Politics",
      "Process (computing)",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Bo"
      },
      {
        "surname": "Yao",
        "given_name": "Yongqiang"
      },
      {
        "surname": "Tan",
        "given_name": "Jingru"
      },
      {
        "surname": "Gong",
        "given_name": "Ruihao"
      },
      {
        "surname": "Lu",
        "given_name": "Jianwei"
      },
      {
        "surname": "Luo",
        "given_name": "Ye"
      }
    ]
  },
  {
    "title": "A generic plug & play diffusion-based denosing module for medical image segmentation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106096",
    "abstract": "Medical image segmentation faces challenges because of the small sample size of the dataset and the fact that images often have noise and artifacts. In recent years, diffusion models have proven very effective in image generation and have been used widely in computer vision. This paper presents a new feature map denoising module (FMD) based on the diffusion model for feature refinement, which is plug-and-play, allowing flexible integration into popular used segmentation networks for seamless end-to-end training. We evaluate the performance of the FMD module on four models, UNet, UNeXt, TransUNet, and IB-TransUNet, by conducting experiments on four datasets. The experimental data analysis shows that adding the FMD module significantly positively impacts the model performance. Furthermore, especially for small lesion areas and minor organs, adding the FMD module allows users to obtain more accurate segmentation results than the original model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000029",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Feature (linguistics)",
      "Image (mathematics)",
      "Image segmentation",
      "Linguistics",
      "Noise (video)",
      "Noise reduction",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Plug-in",
      "Programming language",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Guangju"
      },
      {
        "surname": "Jin",
        "given_name": "Dehu"
      },
      {
        "surname": "Zheng",
        "given_name": "Yuanjie"
      },
      {
        "surname": "Cui",
        "given_name": "Jia"
      },
      {
        "surname": "Gai",
        "given_name": "Wei"
      },
      {
        "surname": "Qi",
        "given_name": "Meng"
      }
    ]
  },
  {
    "title": "MSDCNN: A multiscale dilated convolution neural network for fine-grained 3D shape classification",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106141",
    "abstract": "Multi-view deep neural networks have shown excellent performance on 3D shape classification tasks. However, global features aggregated from multiple views data often lack content information and spatial relationship, which leads to difficult identification the small variance among subcategories in the same category. To solve this problem, in this paper, a novel multiscale dilated convolution neural network termed as MSDCNN is proposed for multi-view fine-grained 3D shape classification. Firstly, a sequence of views are rendered from 12-viewpoints around the input 3D shape by the sequential view capturing module. Then, the first 22 convolution layers of ResNeXt50 is employed to extract the semantic features of each view, and a global mixed feature map is obtained through the element-wise maximum operation of the 12 output feature maps. Furthermore, attention dilated module (ADM), which combines four concatenated attention dilated block (ADB), is designed to extract larger receptive field features from global mixed feature map to enhance context information among the views. Specifically, each ADB is consisted by an attention mechanism module and a dilated convolution with different dilation rates. In addition, prediction module with label smoothing is proposed to classify features, which contains 3 × 3 convolution and adaptive average pooling. The performance of our method is validated experimentally on the ModelNet10, ModelNet40 and FG3D datasets. Experimental results demonstrate the effectiveness and superiority of the proposed MSDCNN framework for 3D shape fine-grained classification.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000571",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Convolution (computer science)",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Linguistics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pooling",
      "Smoothing"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Wei"
      },
      {
        "surname": "Zheng",
        "given_name": "Fujian"
      },
      {
        "surname": "Zhao",
        "given_name": "Yiheng"
      },
      {
        "surname": "Pang",
        "given_name": "Yiran"
      },
      {
        "surname": "Yi",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "DctViT: Discrete Cosine Transform meet vision transformers",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2024.106139",
    "abstract": "Vision transformers (ViTs) have become one of the dominant frameworks for vision tasks in recent years because of their ability to efficiently capture long-range dependencies in image recognition tasks using self-attention. In fact, both CNNs and ViTs have advantages and disadvantages in vision tasks, and some studies suggest that the use of both may be an effective way to balance performance and computational cost. In this paper, we propose a new hybrid network based on CNN and transformer, using CNN to extract local features and transformer to capture long-distance dependencies. We also proposed a new feature map resolution reduction based on Discrete Cosine Transform and self-attention, named DCT-Attention Down-sample (DAD). Our DctViT-L achieves 84.8% top-1 accuracy on ImageNet 1K, far outperforming CMT, Next-ViT, SpectFormer and other state-of-the-art models, with lower computational costs. Using DctViT-B as the backbone, RetinaNet can achieve 46.8% mAP on COCO val2017, which improves mAP by 2.5% and 1.1% with less calculation cost compared with CMT-S and SpectFormer as the backbone.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608024000558",
    "keywords": [
      "Artificial intelligence",
      "Backbone network",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Discrete cosine transform",
      "Image (mathematics)",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Su",
        "given_name": "Keke"
      },
      {
        "surname": "Cao",
        "given_name": "Lihua"
      },
      {
        "surname": "Zhao",
        "given_name": "Botong"
      },
      {
        "surname": "Li",
        "given_name": "Ning"
      },
      {
        "surname": "Wu",
        "given_name": "Di"
      },
      {
        "surname": "Han",
        "given_name": "Xiyu"
      },
      {
        "surname": "Liu",
        "given_name": "Yangfan"
      }
    ]
  },
  {
    "title": "Adaptive-weighted deep multi-view clustering with uniform scale representation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.066",
    "abstract": "Multi-view clustering has attracted growing attention owing to its powerful capacity of multi-source information integration. Although numerous advanced methods have been proposed in past decades, most of them generally fail to distinguish the unequal importance of multiple views to the clustering task and overlook the scale uniformity of learned latent representation among different views, resulting in blurry physical meaning and suboptimal model performance. To address these issues, in this paper, we propose a joint learning framework, termed Adaptive-weighted deep Multi-view Clustering with Uniform scale representation (AMCU). Specifically, to achieve more reasonable multi-view fusion, we introduce an adaptive weighting strategy, which imposes simplex constraints on heterogeneous views for measuring their varying degrees of contribution to consensus prediction. Such a simple yet effective strategy shows its clear physical meaning for the multi-view clustering task. Furthermore, a novel regularizer is incorporated to learn multiple latent representations sharing approximately the same scale, so that the objective for calculating clustering loss cannot be sensitive to the views and thus the entire model training process can be guaranteed to be more stable as well. Through comprehensive experiments on eight popular real-world datasets, we demonstrate that our proposal performs better than several state-of-the-art single-view and multi-view competitors.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006895",
    "keywords": [
      "Artificial intelligence",
      "Cluster analysis",
      "Computer science",
      "Data mining",
      "Economics",
      "Feature learning",
      "Law",
      "Machine learning",
      "Management",
      "Medicine",
      "Operating system",
      "Physics",
      "Political science",
      "Politics",
      "Process (computing)",
      "Quantum mechanics",
      "Radiology",
      "Representation (politics)",
      "Scale (ratio)",
      "Task (project management)",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Rui"
      },
      {
        "surname": "Tang",
        "given_name": "Yongqiang"
      },
      {
        "surname": "Zhang",
        "given_name": "Wensheng"
      },
      {
        "surname": "Feng",
        "given_name": "Wenlong"
      }
    ]
  },
  {
    "title": "Generalized robust loss functions for machine learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.013",
    "abstract": "Loss function is a critical component of machine learning. Some robust loss functions are proposed to mitigate the adverse effects caused by noise. However, they still face many challenges. Firstly, there is currently a lack of unified frameworks for building robust loss functions in machine learning. Secondly, most of them only care about the occurring noise and pay little attention to those normal points. Thirdly, the resulting performance gain is limited. To this end, we put forward a general framework of robust loss functions for machine learning (RML) with rigorous theoretical analyses, which can smoothly and adaptively flatten any unbounded loss function and apply to various machine learning problems. In RML, an unbounded loss function serves as the target, with the aim of being flattened. A scale parameter is utilized to limit the maximum value of noise points, while a shape parameter is introduced to control both the compactness and the growth rate of the flattened loss function. Later, this framework is employed to flatten the Hinge loss function and the Square loss function. Based on this, we build two robust kernel classifiers called FHSVM and FLSSVM, which can distinguish different types of data. The stochastic variance reduced gradient (SVRG) approach is used to optimize FHSVM and FLSSVM. Extensive experiments demonstrate their superiority, with both consistently occupying the top two positions among all evaluated methods, achieving an average accuracy of 81.07% (accompanied by an F-score of 73.25%) for FHSVM and 81.54% (with an F-score of 75.71%) for FLSSVM.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007189",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Combinatorics",
      "Computer science",
      "Evolutionary biology",
      "Function (biology)",
      "Hinge loss",
      "Image (mathematics)",
      "Kernel (algebra)",
      "Machine learning",
      "Mathematics",
      "Noise (video)",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Fu",
        "given_name": "Saiji"
      },
      {
        "surname": "Wang",
        "given_name": "Xiaoxiao"
      },
      {
        "surname": "Tang",
        "given_name": "Jingjing"
      },
      {
        "surname": "Lan",
        "given_name": "Shulin"
      },
      {
        "surname": "Tian",
        "given_name": "Yingjie"
      }
    ]
  },
  {
    "title": "Uncovering a stability signature of brain dynamics associated with meditation experience using massive time-series feature extraction",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.007",
    "abstract": "Previous research has examined resting electroencephalographic (EEG) data to explore brain activity related to meditation. However, previous research has mostly examined power in different frequency bands. The practical objective of this study was to comprehensively test whether other types of time-series analysis methods are better suited to characterize brain activity related to meditation. To achieve this, we compared >7000 time-series features of the EEG signal to comprehensively characterize brain activity differences in meditators, using many measures that are novel in meditation research. Eyes-closed resting-state EEG data from 49 meditators and 46 non-meditators was decomposed into the top eight principal components (PCs). We extracted 7381 time-series features from each PC and each participant and used them to train classification algorithms to identify meditators. Highly differentiating individual features from successful classifiers were analysed in detail. Only the third PC (which had a central-parietal maximum) showed above-chance classification accuracy (67 %, p FDR = 0.007), for which 405 features significantly distinguished meditators (all p FDR < 0.05). Top-performing features indicated that meditators exhibited more consistent statistical properties across shorter subsegments of their EEG time-series (higher stationarity) and displayed an altered distributional shape of values about the mean. By contrast, classifiers trained with traditional band-power measures did not distinguish the groups (p FDR > 0.05). Our novel analysis approach suggests the key signatures of meditators’ brain activity are higher temporal stability and a distribution of time-series values suggestive of longer, larger, or more frequent non-outlying voltage deviations from the mean within the third PC of their EEG data. The higher temporal stability observed in this EEG component might underpin the higher attentional stability associated with meditation. The novel time-series properties identified here have considerable potential for future exploration in meditation research and the analysis of neural dynamics more broadly.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007013",
    "keywords": [
      "Artificial intelligence",
      "Brain activity and meditation",
      "Computer science",
      "Electroencephalography",
      "Machine learning",
      "Meditation",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Psychology",
      "Resting state fMRI",
      "Stability (learning theory)",
      "Theology"
    ],
    "authors": [
      {
        "surname": "Bailey",
        "given_name": "Neil W"
      },
      {
        "surname": "Fulcher",
        "given_name": "Ben D."
      },
      {
        "surname": "Caldwell",
        "given_name": "Bridget"
      },
      {
        "surname": "Hill",
        "given_name": "Aron T"
      },
      {
        "surname": "Fitzgibbon",
        "given_name": "Bernadette"
      },
      {
        "surname": "van Dijk",
        "given_name": "Hanneke"
      },
      {
        "surname": "Fitzgerald",
        "given_name": "Paul B"
      }
    ]
  },
  {
    "title": "Modular hierarchical reinforcement learning for multi-destination navigation in hybrid crowds",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.032",
    "abstract": "Real-world robot applications usually require navigating agents to face multiple destinations. Besides, the real-world crowded environments usually contain dynamic and static crowds that implicitly interact with each other during navigation. To address this challenging task, a novel modular hierarchical reinforcement learning (MHRL) method is developed in this paper. MHRL is composed of three modules, i.e., destination evaluation, policy switch, and motion network, which are designed exactly according to the three phases of solving the original navigation problem. First, the destination evaluation module rates all destinations and selects the one with the lowest cost. Subsequently, the policy switch module decides which motion network to be used according to the selected destination and the obstacle state. Finally, the selected motion network outputs the robot action. Owing to the complementary strengths of a variety of motion networks and the cooperation of modules in each layer, MHRL is able to deal with hybrid crowds effectively. Extensive simulation experiments demonstrate that MHRL achieves better performance than state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007396",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Astronomy",
      "Computer science",
      "Computer security",
      "Crowds",
      "Distributed computing",
      "Engineering",
      "Law",
      "Mobile robot",
      "Modular design",
      "Motion (physics)",
      "Obstacle",
      "Obstacle avoidance",
      "Operating system",
      "Physics",
      "Political science",
      "Reinforcement learning",
      "Robot",
      "State (computer science)",
      "Systems engineering",
      "Task (project management)",
      "Trajectory",
      "Variety (cybernetics)"
    ],
    "authors": [
      {
        "surname": "Ou",
        "given_name": "Wen"
      },
      {
        "surname": "Luo",
        "given_name": "Biao"
      },
      {
        "surname": "Wang",
        "given_name": "Bingchuan"
      },
      {
        "surname": "Zhao",
        "given_name": "Yuqian"
      }
    ]
  },
  {
    "title": "Training multi-source domain adaptation network by mutual information estimation and minimization",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.022",
    "abstract": "We address the problem of Multi-Source Domain Adaptation (MSDA), which trains a neural network using multiple labeled source datasets and an unlabeled target dataset, and expects the trained network to well classify the unlabeled target data. The main challenge in this problem is that the datasets are generated by relevant but different joint distributions. In this paper, we propose to address this challenge by estimating and minimizing the mutual information in the network latent feature space, which leads to the alignment of the source joint distributions and target joint distribution simultaneously. Here, the estimation of the mutual information is formulated into a convex optimization problem, such that the global optimal solution can be easily found. We conduct experiments on several public datasets, and show that our algorithm statistically outperforms its competitors. Video and code are available at https://github.com/sentaochen/Mutual-Information-Estimation-and-Minimization.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802300727X",
    "keywords": [
      "Adaptation (eye)",
      "Architectural engineering",
      "Artificial intelligence",
      "Artificial neural network",
      "Code (set theory)",
      "Computer science",
      "Data mining",
      "Domain (mathematical analysis)",
      "Engineering",
      "Feature (linguistics)",
      "Joint (building)",
      "Joint probability distribution",
      "Linguistics",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Minification",
      "Multi-source",
      "Mutual information",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Programming language",
      "Set (abstract data type)",
      "Source code",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Wen",
        "given_name": "Lisheng"
      },
      {
        "surname": "Chen",
        "given_name": "Sentao"
      },
      {
        "surname": "Xie",
        "given_name": "Mengying"
      },
      {
        "surname": "Liu",
        "given_name": "Cheng"
      },
      {
        "surname": "Zheng",
        "given_name": "Lin"
      }
    ]
  },
  {
    "title": "Dominating Set Model Aggregation for communication-efficient decentralized deep learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.057",
    "abstract": "Decentralized deep learning algorithms leverage peer-to-peer communication of model parameters and/or gradients over communication graphs among the learning agents with access to their private data sets. The majority of the studies in this area focus on achieving high accuracy, with many at the expense of increased communication overhead among the agents. However, large peer-to-peer communication overhead often becomes a practical challenge, especially in harsh environments such as for an underwater sensor network. In this paper, we aim to reduce communication overhead while achieving similar performance as the state-of-the-art algorithms. To achieve this, we use the concept of Minimum Connected Dominating Set from graph theory that is applied in ad hoc wireless networks to address communication overhead issues. Specifically, we propose a new decentralized deep learning algorithm called minimum connected Dominating Set Model Aggregation (DSMA). We investigate the efficacy of our method for different communication graph topologies with a small to large number of agents using varied neural network model architectures. Empirical results on benchmark data sets show a significant (up to 100X) reduction in communication time while preserving the accuracy or in some cases, increasing it compared to the state-of-the-art methods. We also present an analysis to show the convergence of our proposed algorithm.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006809",
    "keywords": [
      "Artificial intelligence",
      "Communication",
      "Computer network",
      "Computer science",
      "Connected dominating set",
      "Distributed computing",
      "Graph",
      "Leverage (statistics)",
      "Machine learning",
      "Models of communication",
      "Network topology",
      "Operating system",
      "Overhead (engineering)",
      "Peer-to-peer",
      "Sociology",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Fotouhi",
        "given_name": "Fateme"
      },
      {
        "surname": "Balu",
        "given_name": "Aditya"
      },
      {
        "surname": "Jiang",
        "given_name": "Zhanhong"
      },
      {
        "surname": "Esfandiari",
        "given_name": "Yasaman"
      },
      {
        "surname": "Jahani",
        "given_name": "Salman"
      },
      {
        "surname": "Sarkar",
        "given_name": "Soumik"
      }
    ]
  },
  {
    "title": "A deep learning model for the detection of various dementia and MCI pathologies based on resting-state electroencephalography data: A retrospective multicentre study",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.009",
    "abstract": "Dementia and mild cognitive impairment (MCI) represent significant health challenges in an aging population. As the search for noninvasive, precise and accessible diagnostic methods continues, the efficacy of electroencephalography (EEG) combined with deep convolutional neural networks (DCNNs) in varied clinical settings remains unverified, particularly for pathologies underlying MCI such as Alzheimer's disease (AD), dementia with Lewy bodies (DLB) and idiopathic normal-pressure hydrocephalus (iNPH). Addressing this gap, our study evaluates the generalizability of a DCNN trained on EEG data from a single hospital (Hospital #1). For data from Hospital #1, the DCNN achieved a balanced accuracy (bACC) of 0.927 in classifying individuals as healthy (n = 69) or as having AD, DLB, or iNPH (n = 188). The model demonstrated robustness across institutions, maintaining bACCs of 0.805 for data from Hospital #2 (n = 73) and 0.920 at Hospital #3 (n = 139). Additionally, the model could differentiate AD, DLB, and iNPH cases with bACCs of 0.572 for data from Hospital #1 (n = 188), 0.619 for Hospital #2 (n = 70), and 0.508 for Hospital #3 (n = 139). Notably, it also identified MCI pathologies with a bACC of 0.715 for Hospital #1 (n = 83), despite being trained on overt dementia cases instead of MCI cases. These outcomes confirm the DCNN's adaptability and scalability, representing a significant stride toward its clinical application. Additionally, our findings suggest a potential for identifying shared EEG signatures between MCI and dementia, contributing to the field's understanding of their common pathophysiological mechanisms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007037",
    "keywords": [
      "Dementia",
      "Developmental psychology",
      "Disease",
      "Electroencephalography",
      "Environmental health",
      "Generalizability theory",
      "Internal medicine",
      "Medicine",
      "Population",
      "Psychiatry",
      "Psychology"
    ],
    "authors": [
      {
        "surname": "Watanabe",
        "given_name": "Yusuke"
      },
      {
        "surname": "Miyazaki",
        "given_name": "Yuki"
      },
      {
        "surname": "Hata",
        "given_name": "Masahiro"
      },
      {
        "surname": "Fukuma",
        "given_name": "Ryohei"
      },
      {
        "surname": "Aoki",
        "given_name": "Yasunori"
      },
      {
        "surname": "Kazui",
        "given_name": "Hiroaki"
      },
      {
        "surname": "Araki",
        "given_name": "Toshihiko"
      },
      {
        "surname": "Taomoto",
        "given_name": "Daiki"
      },
      {
        "surname": "Satake",
        "given_name": "Yuto"
      },
      {
        "surname": "Suehiro",
        "given_name": "Takashi"
      },
      {
        "surname": "Sato",
        "given_name": "Shunsuke"
      },
      {
        "surname": "Kanemoto",
        "given_name": "Hideki"
      },
      {
        "surname": "Yoshiyama",
        "given_name": "Kenji"
      },
      {
        "surname": "Ishii",
        "given_name": "Ryouhei"
      },
      {
        "surname": "Harada",
        "given_name": "Tatsuya"
      },
      {
        "surname": "Kishima",
        "given_name": "Haruhiko"
      },
      {
        "surname": "Ikeda",
        "given_name": "Manabu"
      },
      {
        "surname": "Yanagisawa",
        "given_name": "Takufumi"
      }
    ]
  },
  {
    "title": "Bridging the gap with grad: Integrating active learning into semi-supervised domain generalization",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.017",
    "abstract": "Domain generalization (DG) aims to generalize from a large amount of source data that are fully annotated. However, it is laborious to collect labels for all source data in practice. Some research gets inspiration from semi-supervised learning (SSL) and develops a new task called semi-supervised domain generalization (SSDG). Unlabeled source data is trained jointly with labeled one to significantly improve the performance. Nevertheless, different research adopts different settings, leading to unfair comparisons. Moreover, the initial annotation of unlabeled source data is random, causing unstable and unreliable training. To this end, we first specify the training paradigm, and then leverage active learning (AL) to handle the issues. We further develop a new task called Active Semi-supervised Domain Generalization (ASSDG), which consists of two parts, i.e., SSDG and AL. We delve deep into the commonalities of SSL and AL and propose a unified framework called Gradient-Similarity-based Sample Filtering and Sorting (GSSFS) to iteratively train the SSDG and AL parts. Gradient similarity is utilized to select reliable and informative unlabeled source samples for these two parts respectively. Our methods are simple yet efficient, and extensive experiments demonstrate that our methods can achieve the best results on the DG datasets in the low-data regime without bells and whistles.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007220",
    "keywords": [
      "Annotation",
      "Artificial intelligence",
      "Artificial neural network",
      "Bridging (networking)",
      "Computer network",
      "Computer science",
      "Domain (mathematical analysis)",
      "Economics",
      "Generalization",
      "Image (mathematics)",
      "Labeled data",
      "Leverage (statistics)",
      "Machine learning",
      "Management",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Semi-supervised learning",
      "Similarity (geometry)",
      "Supervised learning",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Jingwei"
      },
      {
        "surname": "Li",
        "given_name": "Yuan"
      },
      {
        "surname": "Tan",
        "given_name": "Jie"
      },
      {
        "surname": "Liu",
        "given_name": "Chengbao"
      }
    ]
  },
  {
    "title": "Optimizing dense feed-forward neural networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.015",
    "abstract": "Deep learning models have been widely used during the last decade due to their outstanding learning and abstraction capacities. However, one of the main challenges any scientist has to face using deep learning models is to establish the network’s architecture. Due to this difficulty, data scientists usually build over complex models and, as a result, most of them result computationally intensive and impose a large memory footprint, generating huge costs, contributing to climate change and hindering their use in computational-limited devices. In this paper, we propose a novel dense feed-forward neural network constructing method based on pruning and transfer learning. Its performance has been thoroughly assessed in classification and regression problems. Without any accuracy loss, our approach can compress the number of parameters by more than 70%. Even further, choosing the pruning parameter carefully, most of the refined models outperform original ones. Furthermore, we have verified that our method not only identifies a better network architecture but also facilitates knowledge transfer between the original and refined models. The results obtained show that our constructing method not only helps in the design of more efficient models but also more effective ones.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007219",
    "keywords": [
      "Abstraction",
      "Agronomy",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Deep learning",
      "Epistemology",
      "Face (sociological concept)",
      "Footprint",
      "Machine learning",
      "Memory footprint",
      "Operating system",
      "Paleontology",
      "Philosophy",
      "Pruning",
      "Scratch",
      "Social science",
      "Sociology",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Balderas",
        "given_name": "Luis"
      },
      {
        "surname": "Lastra",
        "given_name": "Miguel"
      },
      {
        "surname": "Benítez",
        "given_name": "José M."
      }
    ]
  },
  {
    "title": "Self-supervised Learning for DNA sequences with circular dilated convolutional networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.002",
    "abstract": "DNA molecules commonly exhibit wide interactions between the nucleobases. Modeling the interactions is important for obtaining accurate sequence-based inference. Although many deep learning methods have recently been developed for modeling DNA sequences, they still suffer from two major issues: 1) most existing methods can handle only short DNA fragments and fail to capture long-range information; 2) current methods always require massive supervised labels, which are hard to obtain in practice. We propose a new method to address both issues. Our neural network employs circular dilated convolutions as building blocks in the backbone. As a result, our network can take long DNA sequences as input without any condensation. We also incorporate the neural network into a self-supervised learning framework to capture inherent information in DNA without expensive supervised labeling. We have tested our model in two DNA inference tasks, the human variant effect and the open chromatin region of plants, where the experimental results show that our method outperforms five other deep learning models. Our code is available at https://github.com/wiedersehne/cdilDNA.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006962",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Code (set theory)",
      "Composite material",
      "Computer science",
      "Convolutional neural network",
      "DNA",
      "Deep learning",
      "Genetics",
      "Inference",
      "Machine learning",
      "Materials science",
      "Operating system",
      "Pattern recognition (psychology)",
      "Programming language",
      "Range (aeronautics)",
      "Set (abstract data type)",
      "Source code"
    ],
    "authors": [
      {
        "surname": "Cheng",
        "given_name": "Lei"
      },
      {
        "surname": "Yu",
        "given_name": "Tong"
      },
      {
        "surname": "Khalitov",
        "given_name": "Ruslan"
      },
      {
        "surname": "Yang",
        "given_name": "Zhirong"
      }
    ]
  },
  {
    "title": "Dual-domain strip attention for image restoration",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.003",
    "abstract": "Image restoration aims to reconstruct a latent high-quality image from a degraded observation. Recently, the usage of Transformer has significantly advanced the state-of-the-art performance of various image restoration tasks due to its powerful ability to model long-range dependencies. However, the quadratic complexity of self-attention hinders practical applications. Moreover, sufficiently leveraging the huge spectral disparity between clean and degraded image pairs can also be conducive to image restoration. In this paper, we develop a dual-domain strip attention mechanism for image restoration by enhancing representation learning, which consists of spatial and frequency strip attention units. Specifically, the spatial strip attention unit harvests the contextual information for each pixel from its adjacent locations in the same row or column under the guidance of the learned weights via a simple convolutional branch. In addition, the frequency strip attention unit refines features in the spectral domain via frequency separation and modulation, which is implemented by simple pooling techniques. Furthermore, we apply different strip sizes for enhancing multi-scale learning, which is beneficial for handling degradations of various sizes. By employing the dual-domain attention units in different directions, each pixel can implicitly perceive information from an expanded region. Taken together, the proposed dual-domain strip attention network (DSANet) achieves state-of-the-art performance on 12 different datasets for four image restoration tasks, including image dehazing, image desnowing, image denoising, and image defocus deblurring. The code and models are available at https://github.com/c-yn/DSANet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006974",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deblurring",
      "Image (mathematics)",
      "Image processing",
      "Image restoration",
      "Pattern recognition (psychology)",
      "Pixel"
    ],
    "authors": [
      {
        "surname": "Cui",
        "given_name": "Yuning"
      },
      {
        "surname": "Knoll",
        "given_name": "Alois"
      }
    ]
  },
  {
    "title": "Improving domain generalization by hybrid domain attention and localized maximum sensitivity",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.014",
    "abstract": "Domain generalization has attracted much interest in recent years due to its practical application scenarios, in which the model is trained using data from various source domains but is tested using data from an unseen target domain. Existing domain generalization methods concern all visual features, including irrelevant ones with the same priority, which easily results in poor generalization performance of the trained model. In contrast, human beings have strong generalization capabilities to distinguish images from different domains by focusing on important features while suppressing irrelevant features with respect to labels. Motivated by this observation, we propose a channel-wise and spatial-wise hybrid domain attention mechanism to force the model to focus on more important features associated with labels in this work. In addition, models with higher robustness with respect to small perturbations of inputs are expected to have higher generalization capability, which is preferable in domain generalization. Therefore, we propose to reduce the localized maximum sensitivity of the small perturbations of inputs in order to improve the network’s robustness and generalization capability. Extensive experiments on PACS, VLCS, and Office-Home datasets validate the effectiveness of the proposed method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007190",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Domain (mathematical analysis)",
      "Gene",
      "Generalization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Robustness (evolution)"
    ],
    "authors": [
      {
        "surname": "Ng",
        "given_name": "Wing W.Y."
      },
      {
        "surname": "Zhang",
        "given_name": "Qin"
      },
      {
        "surname": "Zhong",
        "given_name": "Cankun"
      },
      {
        "surname": "Zhang",
        "given_name": "Jianjun"
      }
    ]
  },
  {
    "title": "Decentralized policy learning with partial observation and mechanical constraints for multiperson modeling",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.068",
    "abstract": "Extracting the rules of real-world multi-agent behaviors is a current challenge in various scientific and engineering fields. Biological agents independently have limited observation and mechanical constraints; however, most of the conventional data-driven models ignore such assumptions, resulting in lack of biological plausibility and model interpretability for behavioral analyses. Here we propose sequential generative models with partial observation and mechanical constraints in a decentralized manner, which can model agents’ cognition and body dynamics, and predict biologically plausible behaviors. We formulate this as a decentralized multi-agent imitation-learning problem, leveraging binary partial observation and decentralized policy models based on hierarchical variational recurrent neural networks with physical and biomechanical penalties. Using real-world basketball and soccer datasets, we show the effectiveness of our method in terms of the constraint violations, long-term trajectory prediction, and partial observation. Our approach can be used as a multi-agent simulator to generate realistic trajectories using real-world data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006949",
    "keywords": [
      "Artificial intelligence",
      "Astronomy",
      "Computer science",
      "Constraint (computer-aided design)",
      "Geometry",
      "Imitation",
      "Interpretability",
      "Machine learning",
      "Mathematics",
      "Physics",
      "Psychology",
      "Social psychology",
      "Trajectory"
    ],
    "authors": [
      {
        "surname": "Fujii",
        "given_name": "Keisuke"
      },
      {
        "surname": "Takeishi",
        "given_name": "Naoya"
      },
      {
        "surname": "Kawahara",
        "given_name": "Yoshinobu"
      },
      {
        "surname": "Takeda",
        "given_name": "Kazuya"
      }
    ]
  },
  {
    "title": "On the compression of neural networks using ℓ 0 -norm regularization and weight pruning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.019",
    "abstract": "Despite the growing availability of high-capacity computational platforms, implementation complexity still has been a great concern for the real-world deployment of neural networks. This concern is not exclusively due to the huge costs of state-of-the-art network architectures, but also due to the recent push towards edge intelligence and the use of neural networks in embedded applications. In this context, network compression techniques have been gaining interest due to their ability for reducing deployment costs while keeping inference accuracy at satisfactory levels. The present paper is dedicated to the development of a novel compression scheme for neural networks. To this end, a new form of ℓ 0 -norm-based regularization is firstly developed, which is capable of inducing strong sparseness in the network during training. Then, targeting the smaller weights of the trained network with pruning techniques, smaller yet highly effective networks can be obtained. The proposed compression scheme also involves the use of ℓ 2 -norm regularization to avoid overfitting as well as fine tuning to improve the performance of the pruned network. Experimental results are presented aiming to show the effectiveness of the proposed scheme as well as to make comparisons with competing approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007244",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Data mining",
      "Machine learning",
      "Overfitting",
      "Pruning",
      "Regularization (linguistics)",
      "Software deployment",
      "Software engineering"
    ],
    "authors": [
      {
        "surname": "de Resende Oliveira",
        "given_name": "Felipe Dennis"
      },
      {
        "surname": "Batista",
        "given_name": "Eduardo Luiz Ortiz"
      },
      {
        "surname": "Seara",
        "given_name": "Rui"
      }
    ]
  },
  {
    "title": "Black-box attacks on dynamic graphs via adversarial topology perturbations",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.060",
    "abstract": "Research and analysis of attacks on dynamic graph is beneficial for information systems to investigate vulnerabilities and strength abilities in resisting malicious attacks. Existing attacks on dynamic graphs mainly focus on rewiring original graph structures, which are often infeasible in real-world scenarios. To address this issue, we adopt a novel strategy by injecting both fake nodes and links to attack dynamic graphs. Based on that, we present the first study on attacking dynamic graphs via adversarial topology perturbations in a restricted black-box setting, in which downstream graph learning tasks are unknown. Specifically, we first divide dynamic graph structure perturbations into three sub-tasks and transform them as a sequential decision making process. Then, we propose a hierarchical reinforcement learning based black-box attack (HRBBA) framework to model three sub-tasks as attack policies. In addition, an imperceptible perturbation constraint to guarantee the concealment of attacks is incorporated into HRBBA. Finally, HRBBA is optimized based on the actor-critic process. Extensive experiments on four real-world dynamic graphs show that the performance of diverse dynamic graph learning methods (victim methods) on tasks like link prediction, node classification and network clustering can be substantially degraded under HRBBA attack.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006834",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Combinatorics",
      "Computer network",
      "Computer science",
      "Dynamic network analysis",
      "Graph",
      "Line graph",
      "Mathematics",
      "Pathwidth",
      "Reinforcement learning",
      "Theoretical computer science",
      "Topological graph theory",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Tao",
        "given_name": "Haicheng"
      },
      {
        "surname": "Cao",
        "given_name": "Jie"
      },
      {
        "surname": "Chen",
        "given_name": "Lei"
      },
      {
        "surname": "Sun",
        "given_name": "Hongliang"
      },
      {
        "surname": "Shi",
        "given_name": "Yong"
      },
      {
        "surname": "Zhu",
        "given_name": "Xingquan"
      }
    ]
  },
  {
    "title": "Joint estimation of pose, depth, and optical flow with a competition–cooperation transformer network",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.020",
    "abstract": "Estimating depth, ego-motion, and optical flow from consecutive frames is a critical task in robot navigation and has received significant attention in recent years. In this study, we propose PDF-Former, an unsupervised joint estimation network comprising a full transformer-based framework, as well as a competition and cooperation mechanism. The transformer framework captures global feature dependencies and is customized for different task types, thereby improving the performance of sequential tasks. The competition and cooperation mechanisms enable the network to obtain additional supervisory information at different training stages. Specifically, the competition mechanism is implemented early in training to achieve iterative optimization of 6 DOF poses (rotation and translation information from the target image to the two reference images), the depth of target image, and optical flow (from the target image to the two reference images) estimation in a competitive manner. In contrast, the cooperation mechanism is implemented later in training to facilitate the transmission of results among the three networks and mutually optimize the estimation results. We conducted experiments on the KITTI dataset, and the results indicate that PDF-Former has significant potential to enhance the accuracy and robustness of sequential tasks in robot navigation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007256",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Electrical engineering",
      "Engineering",
      "Fusion",
      "Fusion mechanism",
      "Gene",
      "Image (mathematics)",
      "Linguistics",
      "Lipid bilayer fusion",
      "Machine learning",
      "Optical flow",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Robot",
      "Robustness (evolution)",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Xiaochen"
      },
      {
        "surname": "Zhang",
        "given_name": "Tao"
      },
      {
        "surname": "Liu",
        "given_name": "Mingming"
      }
    ]
  },
  {
    "title": "MSEDNet: Multi-scale fusion and edge-supervised network for RGB-T salient object detection",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.031",
    "abstract": "RGB-T Salient object detection (SOD) is to accurately segment salient regions in both visible light images and thermal infrared images. However, most of existing methods for SOD neglects the critical complementarity between multiple modalities images, which is beneficial to further improve the detection accuracy. Therefore, this work introduces the MSEDNet RGB-T SOD method. We utilize an encoder to extract multi-level modalities features from both visible light images and thermal infrared images, which are subsequently categorized into high, medium, and low level. Additionally, we propose three separate feature fusion modules to comprehensively extract complementary information between different modalities during the fusion process. These modules are applied to specific feature levels: the Edge Dilation Sharpening module for low-level features, the Spatial and Channel-Aware module for mid-level features, and the Cross-Residual Fusion module for high-level features. Finally, we introduce an edge fusion loss function for supervised learning, which effectively extracts edge information from different modalities and suppresses background noise. Comparative demonstrate the superiority of the proposed MSEDNet over other state-of-the-art methods. The code and results can be found at the following link: https://github.com/Zhou-wy/MSEDNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007384",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Encoder",
      "Enhanced Data Rates for GSM Evolution",
      "Fusion",
      "Fusion mechanism",
      "Linguistics",
      "Lipid bilayer fusion",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "RGB color model",
      "Sharpening"
    ],
    "authors": [
      {
        "surname": "Peng",
        "given_name": "Daogang"
      },
      {
        "surname": "Zhou",
        "given_name": "Weiyi"
      },
      {
        "surname": "Pan",
        "given_name": "Junzhen"
      },
      {
        "surname": "Wang",
        "given_name": "Danhao"
      }
    ]
  },
  {
    "title": "A collective neurodynamic penalty approach to nonconvex distributed constrained optimization",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.011",
    "abstract": "A nonconvex distributed optimization problem involving nonconvex objective functions and inequality constraints within an undirected multi-agent network is considered. Each agent communicates with its neighbors while only obtaining its individual local information (i.e. its constraint and objective function information). To overcome the challenge caused by the nonconvexity of the objective function, a collective neurodynamic penalty approach in the framework of particle swarm optimization is proposed. The state solution convergence of every neurodynamic penalty approach is directed towards the critical point ensemble of the nonconvex distributed optimization problem. Furthermore, employing their individual neurodynamic models, each neural network conducts accurate local searches within constraints. Through the utilization of both locally best-known solution information and globally best-known solution information, along with the incremental enhancement of solution quality through iterations, the globally optimal solution for a nonconvex distributed optimization problem can be found. Simulations and an application are presented to demonstrate the effectiveness and feasibility.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007050",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Constraint (computer-aided design)",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Evolutionary biology",
      "Function (biology)",
      "Geometry",
      "Mathematical optimization",
      "Mathematics",
      "Optimization problem",
      "Particle swarm optimization",
      "Penalty method"
    ],
    "authors": [
      {
        "surname": "Jia",
        "given_name": "Wenwen"
      },
      {
        "surname": "Huang",
        "given_name": "Tingwen"
      },
      {
        "surname": "Qin",
        "given_name": "Sitian"
      }
    ]
  },
  {
    "title": "Memristor-induced hyperchaos, multiscroll and extreme multistability in fractional-order HNN: Image encryption and FPGA implementation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.008",
    "abstract": "Fractional-order differentiation (FOD) can record information from the past, present, and future. Compared with integer-order systems, FOD systems have higher complexity and more accurate ability to describe the real world. In this paper, two types of fractional-order memristors are proposed and one type is proved to have extreme multistability, local activity, and non-volatility. By using memristors to simulate the autapse of a neuron and to describe the phenomenon of electromagnetic induction caused by electromagnetic radiation, we establish a new 5D FOD memristive HNN (FOMHNN). Through dynamic simulation, rich dynamic behaviors are found, such as hyperchaos, multiscroll, extreme multistability, and “overclocking” behavior caused by order reduction. To the best of our knowledge, this is the first time that such rich dynamic behaviors are found in FOMHNN simultaneously. Based on this FOMHNN, a very efficient and secure image encryption scheme is designed. Security analysis shows that the encrypted Lena image has extremely low adjacent pixel correlation and high randomness, with information entropy of 7.9995. Despite discarding diffusion and scrambling, it has excellent plaintext sensitivity, with NCPR = 99.6095% and UACI = 33.4671%. Finally, this paper implements the proposed FOMHNN and image encryption on field programmable gate array (FPGA). To our knowledge, the related work of fully hardware implementation of fractional-order neural networks and image encryption schemes based on this is rare.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007025",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bitstream",
      "Chaotic",
      "Computer engineering",
      "Computer hardware",
      "Computer network",
      "Computer science",
      "Decoding methods",
      "Electronic engineering",
      "Encryption",
      "Engineering",
      "Field-programmable gate array",
      "Mathematics",
      "Memristor",
      "Multistability",
      "Nonlinear system",
      "Physics",
      "Plaintext",
      "Quantum mechanics",
      "Randomness",
      "Scrambling",
      "Statistics",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Kong",
        "given_name": "Xinxin"
      },
      {
        "surname": "Yu",
        "given_name": "Fei"
      },
      {
        "surname": "Yao",
        "given_name": "Wei"
      },
      {
        "surname": "Cai",
        "given_name": "Shuo"
      },
      {
        "surname": "Zhang",
        "given_name": "Jin"
      },
      {
        "surname": "Lin",
        "given_name": "Hairong"
      }
    ]
  },
  {
    "title": "Meta-structure-based graph attention networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.025",
    "abstract": "Due to the ubiquity of graph-structured data, Graph Neural Network (GNN) have been widely used in different tasks and domains and good results have been achieved in tasks such as node classification and link prediction. However, there are still many challenges in representation learning of heterogeneous networks. Existing graph neural network models are partly based on homogeneous graphs, which do not take into account the rich semantic information of nodes and edges due to their different types; And partly based on heterogeneous graphs, which require predefined meta-structures (include meta-paths and meta-graphs) and do not take into account the different effects of different meta-structures on node representation. In this paper, we propose the MS-GAN model, which consists of four parts: graph structure learner, graph structure expander, graph structure filter and graph structure parser. The graph structure learner automatically generates a graph structure consisting of useful meta-paths by selecting and combining the sub-adjacent matrices in the original graph using a 1 × 1 convolution. The graph structure expander further generates a graph structure containing meta-graphs by Hadamard product based on the previous step. The graph structure filterer filters out graph structures that are more effective for downstream classification tasks based on diversity. The graph structure parser assigns different weights to graph structures consisting of different meta-structures by a semantic hierarchical attention. Finally, through experiments on four datasets and meta-structure visualization analysis, it is shown that MS-GAN can automatically generate useful meta-structures and assign different weights to different meta-structures.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802300730X",
    "keywords": [
      "Adjacency matrix",
      "Algorithm",
      "Clique-width",
      "Computer science",
      "Data structure",
      "Graph",
      "Graph property",
      "Line graph",
      "Null graph",
      "Programming language",
      "Theoretical computer science",
      "Voltage graph"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Jin"
      },
      {
        "surname": "Sun",
        "given_name": "Qingyu"
      },
      {
        "surname": "Zhang",
        "given_name": "Feng"
      },
      {
        "surname": "Yang",
        "given_name": "Beining"
      }
    ]
  },
  {
    "title": "Gradient-aware learning for joint biases: Label noise and class imbalance",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.028",
    "abstract": "Data biases such as class imbalance and label noise always exist in large-scale datasets in real-world. These problems bring huge challenges to deep learning methods. Some previous works adopted loss re-weighting, sample re-weighting, or data-dependent regularization to mitigate the influence of these training biases. But these methods usually pay more attention to class imbalance problem when both the class imbalance and label noise exist in training set simultaneously. These methods may overfit noisy labels, which leads to a great degradation in performance. In this paper, we propose a gradient-aware learning method for the combination of the two biases. During the training process, we update only a part of crucial parameters regularly and rectify the update direction of the rest redundant parameters. This update rule is conducted both in the encoder and classifier of the deep network to decouple label noise and class imbalance implicitly. The experimental results verify the effectiveness of the proposed method on synthetic and real-world data biases.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007347",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Class (philosophy)",
      "Classifier (UML)",
      "Computer science",
      "Data mining",
      "Image (mathematics)",
      "Machine learning",
      "Medicine",
      "Noise (video)",
      "Overfitting",
      "Pattern recognition (psychology)",
      "Radiology",
      "Regularization (linguistics)",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Shichuan"
      },
      {
        "surname": "Zhu",
        "given_name": "Chenglu"
      },
      {
        "surname": "Li",
        "given_name": "Honglin"
      },
      {
        "surname": "Cai",
        "given_name": "Jiatong"
      },
      {
        "surname": "Yang",
        "given_name": "Lin"
      }
    ]
  },
  {
    "title": "Distributed time-varying optimization control protocol for multi-agent systems via finite-time consensus approach",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.067",
    "abstract": "This paper addresses a distributed time-varying optimization problem with inequality constraints based on multi-agent systems over switching communication graphs. To reduce the influence of time-varying inequality constraints, an exact penalty method and smoothing technique are employed. Then, a Hessian-based distributed control protocol is presented to seek the time-varying optimal solution of the distributed time-varying optimization problem by virtue of only local information and interaction. It is shown that all agents not only achieve finite-time consensus but also track the time-varying global optimal target eventually. Compared with the existing distributed optimization protocols, the proposed control protocol is suitable for more general distributed time-varying optimization problems and enjoys high-efficiency convergence. Finally, numerical examples and experiment on moving target tracking of Unmanned Aircraft Vehicle (UAV) are performed to illustrate the effectiveness of the proposed control protocol.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006925",
    "keywords": [
      "Algorithm",
      "Alternative medicine",
      "Applied mathematics",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Control (management)",
      "Control theory (sociology)",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Hessian matrix",
      "Mathematical optimization",
      "Mathematics",
      "Medicine",
      "Multi-agent system",
      "Optimization problem",
      "Pathology",
      "Protocol (science)",
      "Smoothing"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Haojin"
      },
      {
        "surname": "Yue",
        "given_name": "Xiaofeng"
      },
      {
        "surname": "Qin",
        "given_name": "Sitian"
      }
    ]
  },
  {
    "title": "Corrigendum to “Lower and upper bounds for numbers of linear regions of graph convolutional networks” [Neural Networks Volume 168, November 2023, Pages 394–404]",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.061",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006871",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Graph",
      "Mathematics",
      "Physics",
      "Theoretical computer science",
      "Thermodynamics",
      "Volume (thermodynamics)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Hao"
      },
      {
        "surname": "Wang",
        "given_name": "Yu Guang"
      },
      {
        "surname": "Xiong",
        "given_name": "Huan"
      }
    ]
  },
  {
    "title": "Hierarchical attention-guided multiscale aggregation network for infrared small target detection",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.036",
    "abstract": "All man-made flying objects in the sky, ships in the ocean can be regarded as small infrared targets, and the method of tracking them has been received widespread attention in recent years. In search of a further efficient method for infrared small target recognition, we propose a hierarchical attention-guided multiscale aggregation network (HAMANet) in this thesis. The proposed HAMANet mainly consists of a compound guide multilayer perceptron (CG-MLP) block embedded in the backbone net, a spatial-interactive attention module (SiAM), a pixel-interactive attention module (PiAM) and a contextual fusion module (CFM). The CG-MLP marked the width-axis, height-axis, and channel-axis, which can result in a better segmentation effect while reducing computational complexity. SiAM improves global semantic information exchange by increasing the connections between different channels, while PiAM changes the extraction of local key information features by enhancing information exchange at the pixel level. CFM fuses low-level positional information and high-level channel information of the target through coding to improve network stability and target feature utilization. Compared with other state-of-the-art methods on public infrared small target datasets, the results show that our proposed HAMANet has high detection accuracy and a low false-alarm rate.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007426",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Backbone network",
      "Block (permutation group theory)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "False alarm",
      "Feature (linguistics)",
      "Feature extraction",
      "Geometry",
      "Information exchange",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Perceptron",
      "Philosophy",
      "Pixel",
      "Segmentation",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zhong",
        "given_name": "Shunshun"
      },
      {
        "surname": "Zhou",
        "given_name": "Haibo"
      },
      {
        "surname": "Zheng",
        "given_name": "Zhongxu"
      },
      {
        "surname": "Ma",
        "given_name": "Zhu"
      },
      {
        "surname": "Zhang",
        "given_name": "Fan"
      },
      {
        "surname": "Duan",
        "given_name": "Ji'an"
      }
    ]
  },
  {
    "title": "On a framework of data assimilation for hyperparameter estimation of spiking neuronal networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.016",
    "abstract": "When handling real-world data modeled by a complex network dynamical system, the number of the parameters is often much more than the size of the data. Therefore, in many cases, it is impossible to estimate these parameters and the exact value of each parameter is frequently less interesting than the distribution of the parameters. In this paper, we aim to estimate the distribution of the parameters in the mesoscopic neuronal network model from the macroscopic experimental data, for example, the BOLD (blood oxygen level dependent) signal. Herein, we assume that the parameters of the neurons and synapses are inhomogeneous but independently and identically distributed from certain distributions with unknown hyperparameters. Thus, we estimate these hyperparameters of the distributions of the parameters, instead of estimating the parameters themselves. We formulate this problem under the framework of data assimilation and hierarchical Bayesian method and present an efficient method named Hierarchical Data Assimilation (HDA) to conduct the statistical inference on the neuronal network model with the BOLD signal data simulated by the hemodynamic model. We consider the Leaky Integral-Fire (LIF) neuronal networks with four synapses and show that the proposed algorithm can estimate the BOLD signals and the hyperparameters with high preciseness. In addition, we discuss the influence on the performance of the algorithm configuration and the LIF network model setup.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006391",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bayesian inference",
      "Bayesian probability",
      "Computer science",
      "Data assimilation",
      "Hyperparameter",
      "Inference",
      "Mathematics",
      "Meteorology",
      "Physics",
      "Statistical inference",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Wenyong"
      },
      {
        "surname": "Chen",
        "given_name": "Boyu"
      },
      {
        "surname": "Feng",
        "given_name": "Jianfeng"
      },
      {
        "surname": "Lu",
        "given_name": "Wenlian"
      }
    ]
  },
  {
    "title": "Bio-inspired affordance learning for 6-DoF robotic grasping: A transformer-based global feature encoding approach",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.005",
    "abstract": "The 6-Degree-of-Freedom (6-DoF) robotic grasping is a fundamental task in robot manipulation, aimed at detecting graspable points and corresponding parameters in a 3D space, i.e affordance learning, and then a robot executes grasp actions with the detected affordances. Existing research works on affordance learning predominantly focus on learning local features directly for each grid in a voxel scene or each point in a point cloud scene, subsequently filtering the most promising candidate for execution. Contrarily, cognitive models of grasping highlight the significance of global descriptors, such as size, shape, and orientation, in grasping. These global descriptors indicate a grasp path closely tied to actions. Inspired by this, we propose a novel bio-inspired neural network that explicitly incorporates global feature encoding. In particular, our method utilizes a Truncated Signed Distance Function (TSDF) as input, and employs the recently proposed Transformer model to encode the global features of a scene directly. With the effective global representation, we then use deconvolution modules to decode multiple local features to generate graspable candidates. In addition, to integrate global and local features, we propose using a skip-connection module to merge lower-layer global features with higher-layer local features. Our approach, when tested on a recently proposed pile and packed grasping dataset for a decluttering task, surpassed state-of-the-art local feature learning methods by approximately 5% in terms of success and declutter rates. We also evaluated its running time and generalization ability, further demonstrating its superiority. We deployed our model on a Franka Panda robot arm, with real-world results aligning well with simulation data. This underscores our approach’s effectiveness for generalization and real-world applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006998",
    "keywords": [
      "Affordance",
      "Artificial intelligence",
      "Computer science",
      "Feature learning",
      "Human–computer interaction",
      "Machine learning",
      "Pattern recognition (psychology)"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Zhenjie"
      },
      {
        "surname": "Yu",
        "given_name": "Hang"
      },
      {
        "surname": "Wu",
        "given_name": "Hang"
      },
      {
        "surname": "Zhang",
        "given_name": "Xuebo"
      }
    ]
  },
  {
    "title": "Document-level Relation Extraction with Relation Correlations",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.062",
    "abstract": "Document-level relation extraction faces two often overlooked challenges: long-tail problem and multi-label problem. Previous work focuses mainly on obtaining better contextual representations for entity pairs, hardly address the above challenges. In this paper, we analyze the co-occurrence correlation of relations, and introduce it into the document-level relation extraction task for the first time. We argue that the correlations can not only transfer knowledge between data-rich relations and data-scarce ones to assist in the training of long-tailed relations, but also reflect semantic distance guiding the classifier to identify semantically close relations for multi-label entity pairs. Specifically, we use relation embedding as a medium, and propose two co-occurrence prediction sub-tasks from both coarse- and fine-grained perspectives to capture relation correlations. Finally, the learned correlation-aware embeddings are used to guide the extraction of relational facts. Substantial experiments on two popular datasets (i.e., DocRED and DWIE) are conducted, and our method achieves superior results compared to baselines. Insightful analysis also demonstrates the potential of relation correlations to address the above challenges. The data and code are released at https://github.com/RidongHan/DocRE-Co-Occur.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006883",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Classifier (UML)",
      "Cognition",
      "Computer science",
      "Correlation",
      "Data mining",
      "Economics",
      "Embedding",
      "Geometry",
      "Machine learning",
      "Management",
      "Mathematics",
      "Natural language processing",
      "Neuroscience",
      "Relation (database)",
      "Relationship extraction",
      "Semantic relation",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Han",
        "given_name": "Ridong"
      },
      {
        "surname": "Peng",
        "given_name": "Tao"
      },
      {
        "surname": "Wang",
        "given_name": "Benyou"
      },
      {
        "surname": "Liu",
        "given_name": "Lu"
      },
      {
        "surname": "Tiwari",
        "given_name": "Prayag"
      },
      {
        "surname": "Wan",
        "given_name": "Xiang"
      }
    ]
  },
  {
    "title": "Asynchronous adaptive event-triggered fault detection for delayed Markov jump neural networks: A delay-variation-dependent approach",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.010",
    "abstract": "This paper presents a delay-variation-dependent approach to fault detection of a discrete-time Markov jump neural network (MJNN) with a time-varying delay and mismatched modes. The goal is to detect the potential fault of delayed MJNNs by constructing an appropriate adaptive event-triggered and asynchronous H ∞ filter. By choosing a delay-product-type Lyapunov–Krasovskii (L–K) functional with a delay-dependent matrix and exploiting some matrix polynomial inequalities, bounded real lemmas (BRLs) are obtained on the existence of suitable adaptive event generator and filters. These BRLs are dependent not only on the delay bounds but also on the delay variation rate. Simulation results are given to show the validity of the proposed theoretical method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007049",
    "keywords": [
      "Actuator",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Astrophysics",
      "Asynchronous communication",
      "Computer network",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Event (particle physics)",
      "Fault detection and isolation",
      "Jump",
      "Machine learning",
      "Markov chain",
      "Physics",
      "Quantum mechanics",
      "Variation (astronomy)"
    ],
    "authors": [
      {
        "surname": "Lin",
        "given_name": "Wen-Juan"
      },
      {
        "surname": "Wang",
        "given_name": "Qingzhi"
      },
      {
        "surname": "Tan",
        "given_name": "Guoqiang"
      }
    ]
  },
  {
    "title": "Position-based anchor optimization for point supervised dense nuclei detection",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.006",
    "abstract": "Nuclei detection is one of the most fundamental and challenging problems in histopathological image analysis, which can localize nuclei to provide effective computer-aided cancer diagnosis, treatment decision, and prognosis. The fully-supervised nuclei detector requires a large number of nuclei annotations on high-resolution digital images, which is time-consuming and needs human annotators with professional knowledge. In recent years, weakly-supervised learning has attracted significant attention in reducing the labeling burden. However, detecting dense nuclei of complex crowded distribution and diverse appearances remains a challenge. To solve this problem, we propose a novel point-supervised dense nuclei detection framework that introduces position-based anchor optimization to complete morphology-based pseudo-label supervision. Specifically, we first generate cellular-level pseudo labels (CPL) for the detection head via a morphology-based mechanism, which can help to build a baseline point-supervised detection network. Then, considering the crowded distribution of the dense nuclei, we propose a mechanism called Position-based Anchor-quality Estimation (PAE), which utilizes the positional deviation between an anchor and its corresponding point label to suppress low-quality detections far from each nucleus. Finally, to better handle the diverse appearances of nuclei, an Adaptive Anchor Selector (AAS) operation is proposed to automatically select positive and negative anchors according to morphological and positional statistical characteristics of nuclei. We conduct comprehensive experiments on two widely used benchmarks, MO and Lizard, using ResNet50 and PVTv2 as backbones. The results demonstrate that the proposed approach has superior capacity compared with other state-of-the-art methods. In particularly, in dense nuclei scenarios, our method can achieve 95.1% performance of the fully-supervised approach. The code is available at https://github.com/NucleiDet/DenseNucleiDet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007001",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Detector",
      "Digital pathology",
      "Economics",
      "Finance",
      "Geometry",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Point (geometry)",
      "Position (finance)",
      "Supervised learning",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Yao",
        "given_name": "Jieru"
      },
      {
        "surname": "Han",
        "given_name": "Longfei"
      },
      {
        "surname": "Guo",
        "given_name": "Guangyu"
      },
      {
        "surname": "Zheng",
        "given_name": "Zhaohui"
      },
      {
        "surname": "Cong",
        "given_name": "Runmin"
      },
      {
        "surname": "Huang",
        "given_name": "Xiankai"
      },
      {
        "surname": "Ding",
        "given_name": "Jin"
      },
      {
        "surname": "Yang",
        "given_name": "Kaihui"
      },
      {
        "surname": "Zhang",
        "given_name": "Dingwen"
      },
      {
        "surname": "Han",
        "given_name": "Junwei"
      }
    ]
  },
  {
    "title": "Towards performance-maximizing neural network pruning via global channel attention",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.065",
    "abstract": "Network pruning has attracted increasing attention recently for its capability of transferring large-scale neural networks (e.g., CNNs) into resource-constrained devices. Such a transfer is typically achieved by removing redundant network parameters while retaining its generalization performance in a static or dynamic manner. Concretely, static pruning usually maintains a larger and fit-to-all (samples) compressed network by removing the same channels for all samples, which cannot maximally excavate redundancy in the given network. In contrast, dynamic pruning can adaptively remove (more) different channels for different samples and obtain state-of-the-art performance along with a higher compression ratio. However, since the system has to preserve the complete network information for sample-specific pruning, the dynamic pruning methods are usually not memory-efficient. In this paper, our interest is to explore a static alternative, dubbed GlobalPru, from a different perspective by respecting the differences among data. Specifically, a novel channel attention-based learn-to-rank framework is proposed to learn a global ranking of channels with respect to network redundancy. In this method, each sample-wise (local) channel attention is forced to reach an agreement on the global ranking among different data. Hence, all samples can empirically share the same ranking of channels and make the pruning statically in practice. Extensive experiments on ImageNet, SVHN, and CIFAR-10/100 demonstrate that the proposed GlobalPru achieves superior performance than state-of-the-art static and dynamic pruning methods by significant margins.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006937",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Data mining",
      "Generalization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Pruning",
      "Ranking (information retrieval)",
      "Redundancy (engineering)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Yingchun"
      },
      {
        "surname": "Guo",
        "given_name": "Song"
      },
      {
        "surname": "Guo",
        "given_name": "Jingcai"
      },
      {
        "surname": "Zhang",
        "given_name": "Jie"
      },
      {
        "surname": "Zhang",
        "given_name": "Weizhan"
      },
      {
        "surname": "Yan",
        "given_name": "Caixia"
      },
      {
        "surname": "Zhang",
        "given_name": "Yuanhong"
      }
    ]
  },
  {
    "title": "Energy controls wave propagation in a neural network with spatial stimuli",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.042",
    "abstract": "Nervous system has distinct anisotropy and some intrinsic biophysical properties enable neurons present various firing modes in neural activities. In presence of realistic electromagnetic fields, non-uniform radiation activates these neurons with energy diversity. By using a feasible model, energy function is obtained to predict the growth of synaptic connections of these neurons. Distribution of average value of the Hamilton energy function vs. intensity of noisy disturbance can predict the occurrence of coherence resonance, which the neural activities show high regularity by applying noisy disturbance with moderate intensity. From physical viewpoint, the average energy value has similar role average power for the neuron. Non-uniform spatial disturbance is applied and energy is injected into the neural network, statistical synchronization factor is calculated to predict the network synchronization stability and wave propagation. The intensity for field coupling is adaptively controlled by energy diversity between adjacent neurons. Local energy balance will terminate further growth of the coupling intensity; otherwise, heterogeneity is formed in the network due to energy diversity. Furthermore, memristive channel current is introduced into the neuron model for perceiving the effect of electromagnetic induction and radiation, and a memristive neuron is obtained. The circuit implement of memristive circuit depends on the connection to a magnetic flux-controlled memristor into the mentioned neural circuit in an additive branch circuit. The connection and activation of this memristive neural network are controlled under external spatial electromagnetic radiation by capturing enough field energy. Continuous energy collection and exchange generate energy diversity and synaptic connection is created to regulate the synchronous firing patterns and energy balance.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006652",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biological system",
      "Biology",
      "Channel (broadcasting)",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Electrical engineering",
      "Energy (signal processing)",
      "Engineering",
      "Physics",
      "Quantum mechanics",
      "Synchronization (alternating current)",
      "Telecommunications",
      "Topology (electrical circuits)"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Yitong"
      },
      {
        "surname": "Lv",
        "given_name": "Mi"
      },
      {
        "surname": "Wang",
        "given_name": "Chunni"
      },
      {
        "surname": "Ma",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Blockwise compression of transformer-based models without retraining",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.001",
    "abstract": "Transformer-based models, exemplified by GPT-3, ChatGPT, and GPT-4, have recently garnered considerable attention in both academia and industry due to their promising performance in general language tasks. Nevertheless, these models typically involve computationally encoding processes, and in some cases, decoding processes as well, both of which are fundamentally large-scale matrix multiplication. These operations bring the inevitable challenges of massive computation resources and huge memory footprint, usually requiring at least 1 0 23 FLOPs and hundreds of gigabytes, respectively. A common method to address this issue is to reduce the computational and memory requirements by applying layerwise quantization to the transformer, replacing the usual fp32 data type with a low-bit equivalent. Unfortunately, this method often leads to decreased model accuracy and necessitates time-consuming retraining. Such retraining not only requires fine-tuning skills but also substantial computational resources, posing challenges for users. To specifically tackle these issues, we propose BCT, a framework of blockwise compression for transformers without retraining, aiming to facilitate model deployment. Unlike layerwise compression methods, BCT achieves finer compression of the entire transformer by operating blockwise. This method mitigates data distribution deviation caused by quantization, eliminating the requirement for retraining. BCT effectively compresses all components of the model, including but not limited to the embedding, matrix multiplication, GELU, Softmax, layer normalization, and intermediate results. In a case study, an efficient model is compressed by BCT achieving up to 7.988x compression. Subsequently, we also evaluate it on several General Language Understanding Evaluation (GLUE) datasets. Experimental results on the majority of GLUE benchmark demonstrate the effectiveness of our method, as BCT achieves less than a 0.9% degradation in accuracy compared to the more than a 1% degradation seen with other methods providing similar or inferior compression ratios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006950",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Business",
      "Computation",
      "Computer engineering",
      "Computer science",
      "Electrical engineering",
      "Encoder",
      "Engineering",
      "International trade",
      "Operating system",
      "Quantization (signal processing)",
      "Retraining",
      "Softmax function",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Dong",
        "given_name": "Gaochen"
      },
      {
        "surname": "Chen",
        "given_name": "W."
      }
    ]
  },
  {
    "title": "SIA: A sustainable inference attack framework in split learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.033",
    "abstract": "Split learning is a widely recognized distributed learning framework suitable for joint training scenarios with limited computing resources. However, recent research indicates that the malicious server can achieve high-quality reconstruction of the client’s data through feature space hijacking attacks, leading to severe privacy leakage concerns. In this paper, we further enhance this attack to enable efficient data reconstruction while maintaining acceptable performance on the main task. Another significant advantage of our attack framework lies in its ability to fool the state-of-the-art attack detection mechanism, thus minimizing the risk of attacker exposure and making sustainable attacks possible. Moreover, we adaptively refine and adjust the attack strategy, extending the data reconstruction attack for the first time to the more challenging scenario of vertically partitioned data in split learning. In addition, we introduce three training modes for the attack framework, allowing the attacker to choose according to their requirements freely. Finally, we conduct extensive experiments on three datasets and evaluate the attack performance of attack frameworks in different scenarios, parameter settings, and defense mechanisms. The results demonstrate our attack framework’s effectiveness, invisibility, and generality. Our research comprehensively highlights the potential privacy risks associated with split learning and sounds the alarm for secure applications of split learning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007402",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Economics",
      "Exploit",
      "Generality",
      "Inference",
      "Machine learning",
      "Management",
      "Psychology",
      "Psychotherapist",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Fangchao"
      },
      {
        "surname": "Wang",
        "given_name": "Lina"
      },
      {
        "surname": "Zeng",
        "given_name": "Bo"
      },
      {
        "surname": "Zhao",
        "given_name": "Kai"
      },
      {
        "surname": "Wu",
        "given_name": "Tian"
      },
      {
        "surname": "Pang",
        "given_name": "Zhi"
      }
    ]
  },
  {
    "title": "Top-down generation of low-resolution representations improves visual perception and imagination",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.030",
    "abstract": "Perception or imagination requires top-down signals from high-level cortex to primary visual cortex (V1) to reconstruct or simulate the representations bottom-up stimulated by the seen images. Interestingly, top-down signals in V1 have lower spatial resolution than bottom-up representations. It is unclear why the brain uses low-resolution signals to reconstruct or simulate high-resolution representations. By modeling the top-down pathway of the visual system using the decoder of a variational auto-encoder (VAE), we reveal that low-resolution top-down signals can better reconstruct or simulate the information contained in the sparse activities of V1 simple cells, which facilitates perception and imagination. This advantage of low-resolution generation is related to facilitating high-level cortex to form geometry-respecting representations observed in experiments. Furthermore, we present two findings regarding this phenomenon in the context of AI-generated sketches, a style of drawings made of lines. First, we found that the quality of the generated sketches critically depends on the thickness of the lines in the sketches: thin-line sketches are harder to generate than thick-line sketches. Second, we propose a technique to generate high-quality thin-line sketches: instead of directly using original thin-line sketches, we use blurred sketches to train VAE or GAN (generative adversarial network), and then infer the thin-line sketches from the VAE- or GAN-generated blurred sketches. Collectively, our work suggests that low-resolution top-down generation is a strategy the brain uses to improve visual perception and imagination, which inspires new sketch-generation AI techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007372",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Geometry",
      "Line (geometry)",
      "Mathematics",
      "Neuroscience",
      "Paleontology",
      "Perception",
      "Resolution (logic)",
      "Sketch",
      "Software engineering",
      "Top-down and bottom-up design",
      "Visual cortex"
    ],
    "authors": [
      {
        "surname": "Bi",
        "given_name": "Zedong"
      },
      {
        "surname": "Li",
        "given_name": "Haoran"
      },
      {
        "surname": "Tian",
        "given_name": "Liang"
      }
    ]
  },
  {
    "title": "Enhancing robustness in video recognition models: Sparse adversarial attacks and beyond",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.056",
    "abstract": "Recent years have witnessed increasing interest in adversarial attacks on images, while adversarial video attacks have seldom been explored. In this paper, we propose a sparse adversarial attack strategy on videos (DeepSAVA). Our model aims to add a small human-imperceptible perturbation to the key frame of the input video to fool the classifiers. To carry out an effective attack that mirrors real-world scenarios, our algorithm integrates spatial transformation perturbations into the frame. Instead of using the l p norm to gauge the disparity between the perturbed frame and the original frame, we employ the structural similarity index (SSIM), which has been established as a more suitable metric for quantifying image alterations resulting from spatial perturbations. We employ a unified optimisation framework to combine spatial transformation with additive perturbation, thereby attaining a more potent attack. We design an effective and novel optimisation scheme that alternatively utilises Bayesian Optimisation (BO) to identify the most critical frame in a video and stochastic gradient descent (SGD) based optimisation to produce both additive and spatial-transformed perturbations. Doing so enables DeepSAVA to perform a very sparse attack on videos for maintaining human imperceptibility while still achieving state-of-the-art performance in terms of both attack success rate and adversarial transferability. Furthermore, built upon the strong perturbations produced by DeepSAVA, we design a novel adversarial training framework to improve the robustness of video classification models. Our intensive experiments on various types of deep neural networks and video datasets confirm the superiority of DeepSAVA in terms of attacking performance and efficiency. When compared to the baseline techniques, DeepSAVA exhibits the highest level of performance in generating adversarial videos for three distinct video classifiers. Remarkably, it achieves an impressive fooling rate ranging from 99.5% to 100% for the I3D model, with the perturbation of just a single frame. Additionally, DeepSAVA demonstrates favourable transferability across various time series models. The proposed adversarial training strategy is also empirically demonstrated with better performance on training robust video classifiers compared with the state-of-the-art adversarial training with projected gradient descent (PGD) adversary.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006792",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Computer vision",
      "Gene",
      "Logit",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Transferability"
    ],
    "authors": [
      {
        "surname": "Mu",
        "given_name": "Ronghui"
      },
      {
        "surname": "Marcolino",
        "given_name": "Leandro"
      },
      {
        "surname": "Ni",
        "given_name": "Qiang"
      },
      {
        "surname": "Ruan",
        "given_name": "Wenjie"
      }
    ]
  },
  {
    "title": "GT-LSTM: A spatio-temporal ensemble network for traffic flow prediction",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.016",
    "abstract": "Traffic flow prediction plays an instrumental role in modern intelligent transportation systems. Numerous existing studies utilize inter-embedded fusion routes to extract the intrinsic patterns of traffic flow with a single temporal learning approach, which relies heavily on constructing graphs and has low training efficiency. Different from existing studies, this paper proposes a spatio-temporal ensemble network that aims to leverage the strengths of different sequential capturing approaches to obtain the intrinsic dependencies of traffic flow. Specifically, we propose a novel model named graph temporal convolutional long short-term memory network (GT-LSTM), which mainly consists of features splicing and patterns capturing. In features splicing, the spatial dependencies of traffic flow are captured by employing self-adaptive graph convolutional network (GCN), and a non-inter-embedded approach is designed to integrate the spatial and temporal states. Further, the aggregated spatio-temporal states are fed into patterns capturing, which can effectively exploit the advantages of temporal convolutional network (TCN) and bidirectional long short-term memory network (Bi-LSTM) to extract the intrinsic patterns of traffic flow. Extensive experiments conducted on four real-world datasets demonstrate that the proposed network obtains excellent performance in both forecasting accuracy and training efficiency.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007207",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Convolutional neural network",
      "Data mining",
      "Deep learning",
      "Exploit",
      "Graph",
      "Leverage (statistics)",
      "Machine learning",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Luo",
        "given_name": "Yong"
      },
      {
        "surname": "Zheng",
        "given_name": "Jianying"
      },
      {
        "surname": "Wang",
        "given_name": "Xiang"
      },
      {
        "surname": "Tao",
        "given_name": "Yanyun"
      },
      {
        "surname": "Jiang",
        "given_name": "Xingxing"
      }
    ]
  },
  {
    "title": "An Online Support Vector Machine Algorithm for Dynamic Social Network Monitoring",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.024",
    "abstract": "Online monitoring of social networks offers exciting features for platforms, enabling both technical and behavioral analysis. Numerous studies have explored the adaptation of traditional quality control methods for detecting change points within social networks. However, the current research studies face limitations such as an overreliance on case-based attributes, high computational costs, poor scalability with large networks, and low sensitivity in fast change point detection. This paper proposes a novel algorithm for social network monitoring using One-Class Support Vector Machines (OC-SVMs) to address these limitations. Additionally, using both nodal and network-level attributes makes it versatile for diverse social network applications and effectively detecting network disturbances. The algorithm utilizes a well-defined training data dictionary with an updating procedure for evolutionary networks, enhancing memory and time efficiency by reducing the processing of input data. Extensive numerical experiments are conducted using an EpiCNet model to simulate interactions in an online social network, covering six change scenarios to evaluate the proposed methodology. The results show lower Average Run Length (ARL) and Expected Delay Detection (EDD), demonstrating the superior accuracy and effectiveness of the OC-SVM algorithm compared to alternative methods. Applying OC-SVM to the Enron Email network indicates its capability to identify change points, reflecting the tumultuous timeline that led to Enron's downfall. This further validates the substantial advancement of OC-SVM in social network monitoring and opens doors to broader real-world applications.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007293",
    "keywords": [
      "Adaptation (eye)",
      "Algorithm",
      "Archaeology",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Data mining",
      "Database",
      "History",
      "Machine learning",
      "Optics",
      "Physics",
      "Scalability",
      "Social media",
      "Social network (sociolinguistics)",
      "Support vector machine",
      "Timeline",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Karami",
        "given_name": "Arya"
      },
      {
        "surname": "Niaki",
        "given_name": "Seyed Taghi Akhavan"
      }
    ]
  },
  {
    "title": "ACVAE: A novel self-adversarial variational auto-encoder combined with contrast learning for time series anomaly detection",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.023",
    "abstract": "Deep generative models have advantages in modeling complex time series and are widely used in anomaly detection. Nevertheless, the existing deep generative approaches mainly concentrate on the investigation of models’ reconstruction capability rather than customizing a model suitable for anomaly detection. Meanwhile, VAE-based models suffer from posterior collapse, which can lead to a series of undesirable consequences, such as high false positive rate etc. Based on these considerations, in this paper, we propose a novel self-adversarial variational auto-encoder combined with contrast learning, short for ACVAE, to address these challenges. ACVAE consist of three parts 〈 T , E , G 〉 , wherein the transformation network T is employed to generate abnormal latent representations similar to those normal latent representations encoded by the encoder E , and the decoder G is used to distinguish the two representations. In the framework of this model, the normal reconstructions are considered as positive samples and abnormal reconstructions as negative samples, and the contrast learning is executed on the part E to measure the similarities between inputs and positive samples, dissimilarities between inputs and negative samples. Thus, an improved objective function is proposed by integrating two novel regularizers, one refers to adversarial mechanism and the other involves contrast learning, in which the encoder E and decoder G hold the capability to distinguish, and decoder G is constrained to mitigate the posterior collapse. We perform several experiments on five datasets, whose results show ACVAE outperforms state-of-the-art methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007281",
    "keywords": [
      "Adversarial system",
      "Algorithm",
      "Anomaly (physics)",
      "Anomaly detection",
      "Artificial intelligence",
      "Autoencoder",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Computer science",
      "Condensed matter physics",
      "Contrast (vision)",
      "Deep learning",
      "Encoder",
      "Gene",
      "Generative grammar",
      "Machine learning",
      "Operating system",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Physics",
      "Series (stratigraphy)",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Xiaoxia"
      },
      {
        "surname": "Shi",
        "given_name": "Shang"
      },
      {
        "surname": "Sun",
        "given_name": "HaiChao"
      },
      {
        "surname": "Chen",
        "given_name": "Degang"
      },
      {
        "surname": "Wang",
        "given_name": "Guoyin"
      },
      {
        "surname": "Wu",
        "given_name": "Kesheng"
      }
    ]
  },
  {
    "title": "Deep Isotonic Embedding Network: A flexible Monotonic Neural Network",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.026",
    "abstract": "Guaranteeing the monotonicity of a learned model is crucial to address concerns such as fairness, interpretability, and generalization. This paper develops a new monotonic neural network named Deep Isotonic Embedding Network (DIEN), which uses different modules to deal with monotonic and non-monotonic features respectively, and then combine outputs of these modules linearly to obtain the prediction result. A new embedding tool called Isotonic Embedding Unit is developed to process monotonic features and turn each one into an isotonic embedding vector. By converting non-monotonic features into a series of non-negative weight vectors and then combining them with isotonic embedding vectors that have special properties, we enable DIEN to guarantee monotonicity. Besides, we also introduce a module named Monotonic Feature Learning Network to capture complex dependencies between monotonic features. This module is a monotonic feedforward neural network with non-negative weights and can handle scenarios where there are few non-monotonic features or only monotonic features. In comparison to existing methods, DIEN does not require intricate structures like lattices or the use of additional verification techniques to ensure monotonicity. Additionally, the relationship between DIEN’s inputs and outputs is obvious and intuitive. Results from experiments on both synthetic and real-world datasets demonstrate DIEN’s superiority over existing methodologies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007311",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Embedding",
      "Estimator",
      "Generalization",
      "Interpretability",
      "Isotonic regression",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Monotonic function",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Zhao",
        "given_name": "Jiachi"
      },
      {
        "surname": "Zhang",
        "given_name": "Hongwen"
      },
      {
        "surname": "Wang",
        "given_name": "Yue"
      },
      {
        "surname": "Zhai",
        "given_name": "Yiteng"
      },
      {
        "surname": "Yang",
        "given_name": "Yao"
      }
    ]
  },
  {
    "title": "Large-Scale Cross-Modal Hashing with Unified Learning and Multi-Object Regional Correlation Reasoning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.12.018",
    "abstract": "To explore the rich information contained in multi-modal data and take into account efficiency, deep cross-modal hash retrieval (DCMHR) is a wise solution. But currently, most DCMHR methods have two key limitations, one is that the recommended classification of DCMHR models is conditioned only on the objects in different regions, respectively. Another flaw is that these methods either do not learn the unified hash codes in training or cannot design an efficient training process. To solve these two problems, this paper designs Large-Scale Cross-Modal Hashing with Unified Learning and Multi-Object Regional Correlation Reasoning (HUMOR). For the proposed related labels classified by ImgNet, HUMOR uses Multiple Instance Learning (MIL) to reason the correlation of these labels. When regional correlation reasoning is low, these labels will be through “reduce-add” to rectification from max-to-min (global precedence) or min-to-max (regional precedence). Then, HUMOR conducts unified learning on hash loss and classification loss, adopts the four-step iterative algorithm to optimize the unified hash codes, and reduces bias in the model. Experiments on two baseline datasets show that the average performance of this method is higher than most of the DCMHR methods. The results demonstrate the effectiveness and innovation of our method.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023007232",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Chemistry",
      "Computer science",
      "Computer security",
      "Correlation",
      "Geometry",
      "Hash function",
      "Machine learning",
      "Mathematics",
      "Modal",
      "Object (grammar)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Polymer chemistry",
      "Process (computing)",
      "Quantum mechanics",
      "Scale (ratio)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Bo"
      },
      {
        "surname": "Li",
        "given_name": "Zhixin"
      }
    ]
  },
  {
    "title": "Distributed deep reinforcement learning based on bi-objective framework for multi-robot formation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.063",
    "abstract": "Improving generalization ability in multi-robot formation can reduce repetitive training and calculation. In this paper, we study the multi-robot formation problem with the ability to generalize the target position. Since the generalization ability of neural network is directly proportional to spatial dimension, we adopt the strategy of using different networks to solve different objectives, so that the network learning can focus on the learning of one objective to obtain better performance. In addition, this paper presents a distributed deep reinforcement learning method based on soft actor–critic algorithm for solving multi-robot formation problem. At the same time, the formation evaluation assignment function is designed to adapt to distributed training. Compared with the original algorithm, the improved algorithm can get higher reward cumulative values. The experimental results show that the proposed algorithm can better maintain the desired formation in the moving process, and the rotation design in the reward function makes the multi-robot system have better flexibility in formation. The comparison of control signal curve shows that the proposed algorithm is more stable. At the end of the experiments, the universality of the proposed algorithm in formation maintenance and formation variations is demonstrated.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006901",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Flexibility (engineering)",
      "Generalization",
      "Mathematical analysis",
      "Mathematics",
      "Physics",
      "Quantum mechanics",
      "Reinforcement learning",
      "Robot",
      "Statistics",
      "Universality (dynamical systems)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Jinming"
      },
      {
        "surname": "Liu",
        "given_name": "Qingshan"
      },
      {
        "surname": "Chi",
        "given_name": "Guoyi"
      }
    ]
  },
  {
    "title": "SLAPP: Subgraph-level attention-based performance prediction for deep learning models",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.043",
    "abstract": "The intricacy of the Deep Learning (DL) landscape, brimming with a variety of models, applications, and platforms, poses considerable challenges for the optimal design, optimization, or selection of suitable DL models. One promising avenue to address this challenge is the development of accurate performance prediction methods. However, existing methods reveal critical limitations. Operator-level methods, proficient at predicting the performance of individual operators, often neglect broader graph features, which results in inaccuracies in full network performance predictions. On the contrary, graph-level methods excel in overall network prediction by leveraging these graph features but lack the ability to predict the performance of individual operators. To bridge these gaps, we propose SLAPP, a novel subgraph-level performance prediction method. Central to SLAPP is an innovative variant of Graph Neural Networks (GNNs) that we developed, named the Edge Aware Graph Attention Network (EAGAT). This specially designed GNN enables superior encoding of both node and edge features. Through this approach, SLAPP effectively captures both graph and operator features, thereby providing precise performance predictions for individual operators and entire networks. Moreover, we introduce a mixed loss design with dynamic weight adjustment to reconcile the predictive accuracy between individual operators and entire networks. In our experimental evaluation, SLAPP consistently outperforms traditional approaches in prediction accuracy, including the ability to handle unseen models effectively. Moreover, when compared to existing research, our method demonstrates a superior predictive performance across multiple DL models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006664",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Data mining",
      "Enhanced Data Rates for GSM Evolution",
      "Gene",
      "Graph",
      "Machine learning",
      "Operator (biology)",
      "Repressor",
      "Theoretical computer science",
      "Transcription factor"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Zhenyi"
      },
      {
        "surname": "Yang",
        "given_name": "Pengfei"
      },
      {
        "surname": "Hu",
        "given_name": "Linwei"
      },
      {
        "surname": "Zhang",
        "given_name": "Bowen"
      },
      {
        "surname": "Lin",
        "given_name": "Chengmin"
      },
      {
        "surname": "Lv",
        "given_name": "Wenkai"
      },
      {
        "surname": "Wang",
        "given_name": "Quan"
      }
    ]
  },
  {
    "title": "Markov chain stochastic DCA and applications in deep learning with PDEs regularization",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.032",
    "abstract": "This paper addresses a large class of nonsmooth nonconvex stochastic DC (difference-of-convex functions) programs where endogenous uncertainty is involved and i.i.d. (independent and identically distributed) samples are not available. Instead, we assume that it is only possible to access Markov chains whose sequences of distributions converge to the target distributions. This setting is legitimate as Markovian noise arises in many contexts including Bayesian inference, reinforcement learning, and stochastic optimization in high-dimensional or combinatorial spaces. We then design a stochastic algorithm named Markov chain stochastic DCA (MCSDCA) based on DCA (DC algorithm) - a well-known method for nonconvex optimization. We establish the convergence analysis in both asymptotic and nonasymptotic senses. The MCSDCA is then applied to deep learning via PDEs (partial differential equations) regularization, where two realizations of MCSDCA are constructed, namely MCSDCA-odLD and MCSDCA-udLD, based on overdamped and underdamped Langevin dynamics, respectively. Numerical experiments on time series prediction and image classification problems with a variety of neural network topologies show the merits of the proposed methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006561",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Independent and identically distributed random variables",
      "Machine learning",
      "Markov chain",
      "Markov process",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Partial differential equation",
      "Random variable",
      "Regularization (linguistics)",
      "Reinforcement learning",
      "Statistics",
      "Stochastic differential equation",
      "Stochastic optimization"
    ],
    "authors": [
      {
        "surname": "Luu",
        "given_name": "Hoang Phuc Hau"
      },
      {
        "surname": "Le",
        "given_name": "Hoai Minh"
      },
      {
        "surname": "Le Thi",
        "given_name": "Hoai An"
      }
    ]
  },
  {
    "title": "Star algorithm for neural network ensembling",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.020",
    "abstract": "Neural network ensembling is a common and robust way to increase model efficiency. In this paper, we propose a new neural network ensemble algorithm based on Audibert’s empirical star algorithm. We provide optimal theoretical minimax bound on the excess squared risk. Additionally, we empirically study this algorithm on regression and classification tasks and compare it to most popular ensembling methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006433",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Machine learning",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Minimax",
      "Pattern recognition (psychology)",
      "Star (game theory)"
    ],
    "authors": [
      {
        "surname": "Zinchenko",
        "given_name": "Sergey"
      },
      {
        "surname": "Lishudi",
        "given_name": "Dmitrii"
      }
    ]
  },
  {
    "title": "LDCNet: Lightweight dynamic convolution network for laparoscopic procedures image segmentation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.055",
    "abstract": "Medical image segmentation is fundamental for modern healthcare systems, especially for reducing the risk of surgery and treatment planning. Transanal total mesorectal excision (TaTME) has emerged as a recent focal point in laparoscopic research, representing a pivotal modality in the therapeutic arsenal for the treatment of colon & rectum cancers. Real-time instance segmentation of surgical imagery during TaTME procedures can serve as an invaluable tool in assisting surgeons, ultimately reducing surgical risks. The dynamic variations in size and shape of anatomical structures within intraoperative images pose a formidable challenge, rendering the precise instance segmentation of TaTME images a task of considerable complexity. Deep learning has exhibited its efficacy in Medical image segmentation. However, existing models have encountered challenges in concurrently achieving a satisfactory level of accuracy while maintaining manageable computational complexity in the context of TaTME data. To address this conundrum, we propose a lightweight dynamic convolution Network (LDCNet) that has the same superior segmentation performance as the state-of-the-art (SOTA) medical image segmentation network while running at the speed of the lightweight convolutional neural network. Experimental results demonstrate the promising performance of LDCNet, which consistently exceeds previous SOTA approaches. Codes are available at github.com/yinyiyang416/LDCNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006780",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Convolutional neural network",
      "Deep learning",
      "Image segmentation",
      "Paleontology",
      "Rendering (computer graphics)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Yin",
        "given_name": "Yiyang"
      },
      {
        "surname": "Luo",
        "given_name": "Shuangling"
      },
      {
        "surname": "Zhou",
        "given_name": "Jun"
      },
      {
        "surname": "Kang",
        "given_name": "Liang"
      },
      {
        "surname": "Chen",
        "given_name": "Calvin Yu-Chian"
      }
    ]
  },
  {
    "title": "Human–Object Interaction detection via Global Context and Pairwise-level Fusion Features Integration",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.002",
    "abstract": "Recent two-stage detector-based methods show superiority in Human-Object Interaction (HOI) detection along with the successful application of transformer. However, these methods are limited to extracting the global contextual features through instance-level attention without considering the perspective of human–object interaction pairs, and the fusion enhancement of interaction pair features lacks further exploration. The human–object interaction pairs guiding global context extraction relative to instance guiding global context extraction more fully utilize the semantics between human–object pairs, which helps HOI recognition. To this end, we propose a two-stage Global Context and Pairwise-level Fusion Features Integration Network (GFIN) for HOI detection. Specifically, the first stage employs an object detector for instance feature extraction. The second stage aims to capture the semantic-rich visual information through the proposed three modules, Global Contextual Feature Extraction Encoder (GCE), Pairwise Interaction Query Decoder (PID), and Human-Object Pairwise-level Attention Fusion Module (HOF). The GCE module intends to extract the global context memory by the proposed crossover-residual mechanism and then integrate it with the local instance memory from the DETR object detector. HOF utilizes the proposed pairwise-level attention mechanism to fuse and enhance the first stage’s multi-layer feature. PID outputs multi-label interaction recognition results with the input of the query sequence from HOF and the memory from GCE. Finally, comprehensive experiments conducted on HICO-DET and V-COCO datasets demonstrate that the proposed GFIN significantly outperforms the state-of-the-art methods. Code is available at https://github.com/ddwhzh/GFIN.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006251",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Feature (linguistics)",
      "Feature extraction",
      "Linguistics",
      "Object detection",
      "Pairwise comparison",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Haozhong"
      },
      {
        "surname": "Yu",
        "given_name": "Hua"
      },
      {
        "surname": "Zhang",
        "given_name": "Qiang"
      }
    ]
  },
  {
    "title": "Hierarchical attention network with progressive feature fusion for facial expression recognition",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.033",
    "abstract": "Facial expression recognition (FER) in the wild is challenging due to the disturbing factors including pose variation, occlusions, and illumination variation. The attention mechanism can relieve these issues by enhancing expression-relevant information and suppressing expression-irrelevant information. However, most methods utilize the same attention mechanism on feature tensors with varying spatial and channel sizes across different network layers, disregarding the dynamically changing sizes of these tensors. To solve this issue, this paper proposes a hierarchical attention network with progressive feature fusion for FER. Specifically, first, to aggregate diverse complementary features, a diverse feature extraction module based on several feature aggregation blocks is designed to exploit both local context and global context features, both low-level and high-level features, as well as the gradient features that are robust to illumination variation. Second, to effectively fuse the above diverse features, a hierarchical attention module (HAM) is designed to progressively enhance discriminative features from key parts of the facial images and suppress task-irrelevant features from disturbing facial regions. Extensive experiments show that our model achieves the best performance among existing FER methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006524",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Context (archaeology)",
      "Discriminative model",
      "Electrical engineering",
      "Engineering",
      "Facial expression",
      "Feature (linguistics)",
      "Feature extraction",
      "Fuse (electrical)",
      "Fusion",
      "Fusion mechanism",
      "Linguistics",
      "Lipid bilayer fusion",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy"
    ],
    "authors": [
      {
        "surname": "Tao",
        "given_name": "Huanjie"
      },
      {
        "surname": "Duan",
        "given_name": "Qianyue"
      }
    ]
  },
  {
    "title": "Dynamic-group-aware networks for multi-agent trajectory prediction with relational reasoning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.005",
    "abstract": "Demystifying the interactions among multiple agents from their past trajectories is fundamental to precise and interpretable trajectory prediction. However, previous works mainly consider static, pairwise interactions with limited relational reasoning. To more comprehensively model interactions and reason relations, we propose DynGroupNet, a dynamic-group-aware network, which (i) models time-varying interactions in highly dynamic scenes; (ii) captures both pairwise and group-wise interactions; and (iii) reasons both interaction strength and category without direct supervision. Based on DynGroupNet, we further design a prediction system to forecast socially plausible trajectories with dynamic relational reasoning. The proposed prediction system leverages the Gaussian mixture model, multiple sampling and prediction refinement to promote prediction diversity for multiple future possibilities capturing, training stability for efficient model learning and trajectory smoothness for more realistic predictions, respectively. The proposed complex interaction modeling of DynGroupNet, future diversity capturing, efficient model training and trajectory smoothing of prediction system together to promote more accurate and plausible future predictions. Extensive experiments show that: (1) DynGroupNet can capture time-varying group behaviors, infer time-varying interaction category and interaction strength during prediction; (2) DynGroupNet significantly outperforms the state-of-the-art trajectory prediction methods by 28.0%, 34.9%, 13.0% in FDE on the NBA, NFL and SDD datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006299",
    "keywords": [
      "Artificial intelligence",
      "Astronomy",
      "Computer science",
      "Computer vision",
      "Machine learning",
      "Pairwise comparison",
      "Physics",
      "Smoothing",
      "Stability (learning theory)",
      "Trajectory"
    ],
    "authors": [
      {
        "surname": "Xu",
        "given_name": "Chenxin"
      },
      {
        "surname": "Wei",
        "given_name": "Yuxi"
      },
      {
        "surname": "Tang",
        "given_name": "Bohan"
      },
      {
        "surname": "Yin",
        "given_name": "Sheng"
      },
      {
        "surname": "Zhang",
        "given_name": "Ya"
      },
      {
        "surname": "Chen",
        "given_name": "Siheng"
      },
      {
        "surname": "Wang",
        "given_name": "Yanfeng"
      }
    ]
  },
  {
    "title": "Graph embedding-based heterogeneous domain adaptation with domain-invariant feature learning and distributional order preserving",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.048",
    "abstract": "Heterogeneous domain adaptation (HDA) methods leverage prior knowledge from the source domain to train models for the target domain and address the differences in their feature spaces. However, incorrect alignment of categories and distribution structure disruption may be caused by unlabeled target samples during the domain alignment process for most existing methods, resulting in negative transfer. Additionally, the previous works rarely focus on the robustness and interpretability of the model. To address these issues, we propose a novel Graph embedding-based Heterogeneous domain-Invariant feature learning and Distributional order preserving framework (GHID). Specifically, a bidirectional robust cross-domain alignment graph embedding structure is proposed to globally align two domains, which learns the domain-invariant and discriminative features simultaneously. In addition, the interpretability of the proposed graph structures is demonstrated through two theoretical analyses, which can elucidate the correlation between important samples from a global perspective in heterogeneous domain alignment scenarios. Then, a heterogeneous discriminative distributional order preserving graph embedding structure is designed to preserve the original distribution relationship of each domain to prevent negative transfer. Moreover, the dynamic centroid strategy is incorporated into the graph structures to improve the robustness of the model. Comprehensive experimental results on four benchmarks demonstrate that the proposed method outperforms other state-of-the-art approaches in effectiveness.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006718",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Classifier (UML)",
      "Computer science",
      "Discriminative model",
      "Domain adaptation",
      "Embedding",
      "Gene",
      "Graph",
      "Graph embedding",
      "Interpretability",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Wenxu"
      },
      {
        "surname": "Li",
        "given_name": "Zhenbo"
      },
      {
        "surname": "Li",
        "given_name": "Weiran"
      }
    ]
  },
  {
    "title": "Foreground segmentation network using transposed convolutional neural networks and up sampling for multiscale feature encoding",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.015",
    "abstract": "Foreground segmentation algorithm aims to precisely separate moving objects from the background in various environments. However, the interference from darkness, dynamic background information, and camera jitter makes it still challenging to build a decent detection network. To solve these issues, a triplet CNN and Transposed Convolutional Neural Network (TCNN) are created by attaching a Features Pooling Module (FPM). TCNN process reduces the amount of multi-scale inputs to the network by fusing features into the Foreground Segmentation Network (FgSegNet) based FPM, which extracts multi-scale features from images and builds a strong feature pooling. Additionally, the up-sampling network is added to the proposed technique, which is used to up-sample the abstract image representation, so that its spatial dimensions match with the input image. The large context and long-range dependencies among pixels are acquired by TCNN and segmentation mask, in multiple scales using triplet CNN, to enhance the foreground segmentation of FgSegNet. The results, clearly show that FgSegNet surpasses other state-of-the-art algorithms on the CDnet2014 datasets, with an average F-Measure of 0.9804, precision of 0.9801, PWC as (0.0461), and recall as (0.9896). Moreover, the FgSegNet with up-sampling achieves the F-measure of 0.9804 which is higher when compared to the FgSegNet without up-sampling.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006378",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Filter (signal processing)",
      "Image segmentation",
      "Linguistics",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pixel",
      "Pooling",
      "Sampling (signal processing)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Gowda",
        "given_name": "Vishruth B."
      },
      {
        "surname": "Gopalakrishna",
        "given_name": "M.T."
      },
      {
        "surname": "Megha",
        "given_name": "J."
      },
      {
        "surname": "Mohankumar",
        "given_name": "Shilpa"
      }
    ]
  },
  {
    "title": "Multi-Adaptive Optimization for multi-task learning with deep neural networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.038",
    "abstract": "Multi-task learning is a promising paradigm to leverage task interrelations during the training of deep neural networks. A key challenge in the training of multi-task networks is to adequately balance the complementary supervisory signals of multiple tasks. In that regard, although several task-balancing approaches have been proposed, they are usually limited by the use of per-task weighting schemes and do not completely address the uneven contribution of the different tasks to the network training. In contrast to classical approaches, we propose a novel Multi-Adaptive Optimization (MAO) strategy that dynamically adjusts the contribution of each task to the training of each individual parameter in the network. This automatically produces a balanced learning across tasks and across parameters, throughout the whole training and for any number of tasks. To validate our proposal, we perform comparative experiments on real-world datasets for computer vision, considering different experimental settings. These experiments allow us to analyze the performance obtained in several multi-task scenarios along with the learning balance across tasks, network layers and training steps. The results demonstrate that MAO outperforms previous task-balancing alternatives. Additionally, the performed analyses provide insights that allow us to comprehend the advantages of this novel approach for multi-task learning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006615",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer security",
      "Economics",
      "Key (lock)",
      "Leverage (statistics)",
      "Machine learning",
      "Management",
      "Medicine",
      "Multi-task learning",
      "Radiology",
      "Task (project management)",
      "Weighting"
    ],
    "authors": [
      {
        "surname": "Hervella",
        "given_name": "Álvaro S."
      },
      {
        "surname": "Rouco",
        "given_name": "José"
      },
      {
        "surname": "Novo",
        "given_name": "Jorge"
      },
      {
        "surname": "Ortega",
        "given_name": "Marcos"
      }
    ]
  },
  {
    "title": "CVANet: Cascaded visual attention network for single image super-resolution",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.049",
    "abstract": "Deep convolutional neural networks (DCNNs) have exhibited excellent feature extraction and detail reconstruction capabilities for single image super-resolution (SISR). Nevertheless, most previous DCNN-based methods do not fully utilize the complementary strengths between feature maps, channels, and pixels. Therefore, it hinders the ability of DCNNs to represent abundant features. To tackle the aforementioned issues, we present a Cascaded Visual Attention Network for SISR called CVANet, which simulates the visual attention mechanism of the human eyes to focus on the reconstruction process of details. Specifically, we first designed a trainable feature attention module (FAM) for feature-level attention learning. Afterward, we introduce a channel attention module (CAM) to reinforce feature maps under channel-level attention learning. Meanwhile, we propose a pixel attention module (PAM) that adaptively selects representative features from the previous layers, which are utilized to generate a high-resolution image. Satisfactory, our CVANet can effectively improve the resolution of images by exploring the feature representation capabilities of different modules and the visual perception properties of the human eyes. Extensive experiments with different methods on four benchmarks demonstrate that our CVANet outperforms the state-of-the-art (SOTA) methods in subjective visual perception, PSNR, and SSIM.The code will be made available https://github.com/WilyZhao8/CVANet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802300672X",
    "keywords": [
      "Artificial intelligence",
      "Channel (broadcasting)",
      "Code (set theory)",
      "Computer network",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature learning",
      "Focus (optics)",
      "Image (mathematics)",
      "Law",
      "Linguistics",
      "Operating system",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pixel",
      "Political science",
      "Politics",
      "Process (computing)",
      "Programming language",
      "Representation (politics)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Weidong"
      },
      {
        "surname": "Zhao",
        "given_name": "Wenyi"
      },
      {
        "surname": "Li",
        "given_name": "Jia"
      },
      {
        "surname": "Zhuang",
        "given_name": "Peixian"
      },
      {
        "surname": "Sun",
        "given_name": "Haihan"
      },
      {
        "surname": "Xu",
        "given_name": "Yibo"
      },
      {
        "surname": "Li",
        "given_name": "Chongyi"
      }
    ]
  },
  {
    "title": "OnDev-LCT: On-Device Lightweight Convolutional Transformers towards federated learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.044",
    "abstract": "Federated learning (FL) has emerged as a promising approach to collaboratively train machine learning models across multiple edge devices while preserving privacy. The success of FL hinges on the efficiency of participating models and their ability to handle the unique challenges of distributed learning. While several variants of Vision Transformer (ViT) have shown great potential as alternatives to modern convolutional neural networks (CNNs) for centralized training, the unprecedented size and higher computational demands hinder their deployment on resource-constrained edge devices, challenging their widespread application in FL. Since client devices in FL typically have limited computing resources and communication bandwidth, models intended for such devices must strike a balance between model size, computational efficiency, and the ability to adapt to the diverse and non-IID data distributions encountered in FL. To address these challenges, we propose OnDev-LCT: Lightweight Convolutional Transformers for On-Device vision tasks with limited training data and resources. Our models incorporate image-specific inductive biases through the LCT tokenizer by leveraging efficient depthwise separable convolutions in residual linear bottleneck blocks to extract local features, while the multi-head self-attention (MHSA) mechanism in the LCT encoder implicitly facilitates capturing global representations of images. Extensive experiments on benchmark image datasets indicate that our models outperform existing lightweight vision models while having fewer parameters and lower computational demands, making them suitable for FL scenarios with data heterogeneity and communication bottlenecks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006676",
    "keywords": [
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Bottleneck",
      "Cloud computing",
      "Computer engineering",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Distributed computing",
      "Edge device",
      "Embedded system",
      "Encoder",
      "Geodesy",
      "Geography",
      "Machine learning",
      "Operating system",
      "Physics",
      "Quantum mechanics",
      "Software deployment",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Thwal",
        "given_name": "Chu Myaet"
      },
      {
        "surname": "Nguyen",
        "given_name": "Minh N.H."
      },
      {
        "surname": "Tun",
        "given_name": "Ye Lin"
      },
      {
        "surname": "Kim",
        "given_name": "Seong Tae"
      },
      {
        "surname": "Thai",
        "given_name": "My T."
      },
      {
        "surname": "Hong",
        "given_name": "Choong Seon"
      }
    ]
  },
  {
    "title": "IoUformer: Pseudo-IoU prediction with transformer for visual tracking",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.055",
    "abstract": "Siamese tracking has witnessed tremendous progress in tracking paradigm. However, its default box estimation pipeline still faces a crucial inconsistency issue, namely, the bounding box decided by its classification score is not always best overlapped with the ground truth, thus harming performance. To this end, we explore a novel simple tracking paradigm based on the intersection over union (IoU) value prediction. To first bypass this inconsistency issue, we propose a concise target state predictor termed IoUformer, which instead of default box estimation pipeline directly predicts the IoU values related to tracking performance metrics. In detail, it extends the long-range dependency modeling ability of transformer to jointly grasp target-aware interactions between target template and search region, and search sub-region interactions, thus neatly unifying global semantic interaction and target state prediction. Thanks to this joint strength, IoUformer can predict reliable IoU values near-linear with the ground truth, which paves a safe way for our new IoU-based siamese tracking paradigm. Since it is non-trivial to explore this paradigm with pleased efficacy and portability, we offer the respective network components and two alternative localization ways. Experimental results show that our IoUformer-based tracker achieves promising results with less training data. For its applicability, it still serves as a refinement module to consistently boost existing advanced trackers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006214",
    "keywords": [
      "Artificial intelligence",
      "BitTorrent tracker",
      "Computer science",
      "Eye tracking",
      "Ground truth",
      "Image (mathematics)",
      "Machine learning",
      "Minimum bounding box",
      "Pairwise comparison",
      "Physics",
      "Pipeline (software)",
      "Programming language",
      "Quantum mechanics",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Cai",
        "given_name": "Huayue"
      },
      {
        "surname": "Lan",
        "given_name": "Long"
      },
      {
        "surname": "Zhang",
        "given_name": "Jing"
      },
      {
        "surname": "Zhang",
        "given_name": "Xiang"
      },
      {
        "surname": "Zhan",
        "given_name": "Yibing"
      },
      {
        "surname": "Luo",
        "given_name": "Zhigang"
      }
    ]
  },
  {
    "title": "Mean square exponential stabilization analysis of stochastic neural networks with saturated impulsive input",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.026",
    "abstract": "The exponential stabilization of stochastic neural networks in mean square sense with saturated impulsive input is investigated in this paper. Firstly, the saturated term is handled by polyhedral representation method. When the impulsive sequence is determined by average impulsive interval, impulsive density and mode-dependent impulsive density, the sufficient conditions for stability are proposed, respectively. Then, the ellipsoid and the polyhedron are used to estimate the attractive domain, respectively. By transforming the estimation of the attractive domain into a convex optimization problem, a relatively optimum domain of attraction is obtained. Finally, a three-dimensional continuous time Hopfield neural network example is provided to illustrate the effectiveness and rationality of our proposed theoretical results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006494",
    "keywords": [
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Combinatorics",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Domain (mathematical analysis)",
      "Exponential stability",
      "Frequency domain",
      "Genetics",
      "Geometry",
      "Interval (graph theory)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Nonlinear system",
      "Physics",
      "Quantum mechanics",
      "Recurrent neural network",
      "Regular polygon",
      "Sequence (biology)",
      "Stability (learning theory)",
      "Stochastic neural network"
    ],
    "authors": [
      {
        "surname": "Deng",
        "given_name": "Hao"
      },
      {
        "surname": "Li",
        "given_name": "Chuandong"
      },
      {
        "surname": "Chang",
        "given_name": "Fei"
      },
      {
        "surname": "Wang",
        "given_name": "Yinuo"
      }
    ]
  },
  {
    "title": "A start–stop points CenterNet for wideband signals detection and time–frequency localization in spectrum sensing",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.013",
    "abstract": "Recently, deep learning (DL) based object detection methods have attracted significant attention for wideband multisignal detection, which has been viewed as an essential part in the field of cognitive radio spectrum sensing. However, the existing DL methods are difficult or very likely fail to detect discontinuous burst signals, not to mention the signals with wide, instantaneous, dynamic bandwidth, and multiple channels. To solve this problem, the present study proposes a scheme that combines the start–stop point signal features for wideband multi-signal detection, namely the Fast Spectrum-Size Self-Training network (FSSNet). Considering the horizontal rectangle form of a wideband signal in the time–frequency domain, we innovatively utilize the start–stop points of the two-dimensional (2D) Box to build the signal model. Specifically, We propose a fast Start–stop HeatMap where the proposed LPS-YXE simultaneously labels and divides the start–stop points positions in the X–Y axis of a single HeatMap. We attribute the method’s success in discontinuous signal detection to the multidimensional space transformation of HeatMap, which is used to locate the start–stop points and extract features separated from the signal regions of start–stop points. Furthermore, FSSNet can realize the 2D Box estimation of the wideband signal by regressing only a single variable, and thus with satisfactory detection speed. Simulation results verify the effectiveness and superiority of the proposed start–stop based wideband signal detection scheme with practical received signals. All our models and code are available at https://github.com/jn-z/SSNet2 .",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006366",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Bandwidth (computing)",
      "Cognitive radio",
      "Computer science",
      "Computer vision",
      "Detection theory",
      "Detector",
      "Electronic engineering",
      "Engineering",
      "Geometry",
      "Mathematics",
      "Programming language",
      "Rectangle",
      "SIGNAL (programming language)",
      "Telecommunications",
      "Time domain",
      "Wideband",
      "Wireless"
    ],
    "authors": [
      {
        "surname": "Cheng",
        "given_name": "Teng"
      },
      {
        "surname": "Sun",
        "given_name": "Lei"
      },
      {
        "surname": "Zhang",
        "given_name": "Junning"
      },
      {
        "surname": "Wang",
        "given_name": "Jinling"
      },
      {
        "surname": "Wei",
        "given_name": "Zhanyang"
      }
    ]
  },
  {
    "title": "Deep Kernel Principal Component Analysis for multi-level feature learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.045",
    "abstract": "Principal Component Analysis (PCA) and its nonlinear extension Kernel PCA (KPCA) are widely used across science and industry for data analysis and dimensionality reduction. Modern deep learning tools have achieved great empirical success, but a framework for deep principal component analysis is still lacking. Here we develop a deep kernel PCA methodology (DKPCA) to extract multiple levels of the most informative components of the data. Our scheme can effectively identify new hierarchical variables, called deep principal components, capturing the main characteristics of high-dimensional data through a simple and interpretable numerical optimization. We couple the principal components of multiple KPCA levels, theoretically showing that DKPCA creates both forward and backward dependency across levels, which has not been explored in kernel methods and yet is crucial to extract more informative features. Various experimental evaluations on multiple data types show that DKPCA finds more efficient and disentangled representations with higher explained variance in fewer principal components, compared to the shallow KPCA. We demonstrate that our method allows for effective hierarchical data exploration, with the ability to separate the key generative factors of the input data both for large datasets and when few training samples are available. Overall, DKPCA can facilitate the extraction of useful patterns from high-dimensional data by learning more informative features organized in different levels, giving diversified aspects to explore the variation factors in the data, while maintaining a simple mathematical formulation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802300669X",
    "keywords": [
      "Artificial intelligence",
      "Combinatorics",
      "Computer science",
      "Curse of dimensionality",
      "Data mining",
      "Deep learning",
      "Dimensionality reduction",
      "Kernel (algebra)",
      "Kernel method",
      "Kernel principal component analysis",
      "Machine learning",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Principal component analysis",
      "Support vector machine"
    ],
    "authors": [
      {
        "surname": "Tonin",
        "given_name": "Francesco"
      },
      {
        "surname": "Tao",
        "given_name": "Qinghua"
      },
      {
        "surname": "Patrinos",
        "given_name": "Panagiotis"
      },
      {
        "surname": "Suykens",
        "given_name": "Johan A.K."
      }
    ]
  },
  {
    "title": "Teacher–student complementary sample contrastive distillation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.036",
    "abstract": "Knowledge distillation (KD) is a widely adopted model compression technique for improving the performance of compact student models, by utilizing the “dark knowledge” of a large teacher model. However, previous studies have not adequately investigated the effectiveness of supervision from the teacher model, and overconfident predictions in the student model may degrade its performance. In this work, we propose a novel framework, Teacher–Student Complementary Sample Contrastive Distillation (TSCSCD), that alleviate these challenges. TSCSCD consists of three key components: Contrastive Sample Hardness (CSH), Supervision Signal Correction (SSC), and Student Self-Learning (SSL). Specifically, CSH evaluates the teacher’s supervision for each sample by comparing the predictions of two compact models, one distilled from the teacher and the other trained from scratch. SSC corrects weak supervision according to CSH, while SSL employs integrated learning among multi-classifiers to regularize overconfident predictions. Extensive experiments on four real-world datasets demonstrate that TSCSCD outperforms recent state-of-the-art knowledge distillation techniques.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006597",
    "keywords": [
      "Artificial intelligence",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Computer security",
      "Distillation",
      "Key (lock)",
      "Machine learning",
      "Sample (material)"
    ],
    "authors": [
      {
        "surname": "Bao",
        "given_name": "Zhiqiang"
      },
      {
        "surname": "Huang",
        "given_name": "Zhenhua"
      },
      {
        "surname": "Gou",
        "given_name": "Jianping"
      },
      {
        "surname": "Du",
        "given_name": "Lan"
      },
      {
        "surname": "Liu",
        "given_name": "Kang"
      },
      {
        "surname": "Zhou",
        "given_name": "Jingtao"
      },
      {
        "surname": "Chen",
        "given_name": "Yunwen"
      }
    ]
  },
  {
    "title": "Powerful-IoU: More straightforward and faster bounding box regression loss with a nonmonotonic focusing mechanism",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.041",
    "abstract": "Bounding box regression (BBR) is one of the core tasks in object detection, and the BBR loss function significantly impacts its performance. However, we have observed that existing IoU-based loss functions suffer from unreasonable penalty factors, leading to anchor boxes expanding during regression and significantly slowing down convergence. To address this issue, we intensively analyzed the reasons for anchor box enlargement. In response, we propose a Powerful-IoU (PIoU) loss function, which combines a target size-adaptive penalty factor and a gradient-adjusting function based on anchor box quality. The PIoU loss guides anchor boxes to regress along efficient paths, resulting in faster convergence than existing IoU-based losses. Additionally, we investigate the focusing mechanism and introduce a non-monotonic attention layer that was combined with PIoU to obtain a new loss function PIoU v2. PIoU v2 loss enhances the capability to focus on anchor boxes of medium quality. By incorporating PIoU v2 into popular object detectors such as YOLOv8 and DINO, we achieved an increase in average precision (AP) and improved performance compared to their original loss functions on the MS COCO and PASCAL VOC datasets, thus validating the effectiveness of our proposed improvement strategies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006640",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biology",
      "Bounding overwatch",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Evolutionary biology",
      "Function (biology)",
      "Image (mathematics)",
      "Mathematical analysis",
      "Mathematics",
      "Minimum bounding box",
      "Monotonic function",
      "Pascal (unit)",
      "Programming language",
      "Regression",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Can"
      },
      {
        "surname": "Wang",
        "given_name": "Kaige"
      },
      {
        "surname": "Li",
        "given_name": "Qing"
      },
      {
        "surname": "Zhao",
        "given_name": "Fazhan"
      },
      {
        "surname": "Zhao",
        "given_name": "Kun"
      },
      {
        "surname": "Ma",
        "given_name": "Hongtu"
      }
    ]
  },
  {
    "title": "Mental image reconstruction from human brain activity: Neural decoding of mental imagery via deep neural network-based Bayesian estimation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.024",
    "abstract": "Visual images observed by humans can be reconstructed from their brain activity. However, the visualization (externalization) of mental imagery is challenging. Only a few studies have reported successful visualization of mental imagery, and their visualizable images have been limited to specific domains such as human faces or alphabetical letters. Therefore, visualizing mental imagery for arbitrary natural images stands as a significant milestone. In this study, we achieved this by enhancing a previous method. Specifically, we demonstrated that the visual image reconstruction method proposed in the seminal study by Shen et al. (2019) heavily relied on low-level visual information decoded from the brain and could not efficiently utilize the semantic information that would be recruited during mental imagery. To address this limitation, we extended the previous method to a Bayesian estimation framework and introduced the assistance of semantic information into it. Our proposed framework successfully reconstructed both seen images (i.e., those observed by the human eye) and imagined images from brain activity. Quantitative evaluation showed that our framework could identify seen and imagined images highly accurately compared to the chance accuracy (seen: 90.7%, imagery: 75.6%, chance accuracy: 50.0%). In contrast, the previous method could only identify seen images (seen: 64.3%, imagery: 50.4%). These results suggest that our framework would provide a unique tool for directly investigating the subjective contents of the brain such as illusions, hallucinations, and dreams.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006470",
    "keywords": [
      "Artificial intelligence",
      "Cognition",
      "Cognitive psychology",
      "Computer science",
      "Computer vision",
      "Contrast (vision)",
      "Creative visualization",
      "Illusion",
      "Mental image",
      "Neuroscience",
      "Pattern recognition (psychology)",
      "Programming language",
      "Psychology",
      "Semantics (computer science)",
      "Visualization"
    ],
    "authors": [
      {
        "surname": "Koide-Majima",
        "given_name": "Naoko"
      },
      {
        "surname": "Nishimoto",
        "given_name": "Shinji"
      },
      {
        "surname": "Majima",
        "given_name": "Kei"
      }
    ]
  },
  {
    "title": "An off-policy multi-agent stochastic policy gradient algorithm for cooperative continuous control",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.046",
    "abstract": "Multi-agent reinforcement learning (MARL) algorithms based on trust regions (TR) have achieved significant success in numerous cooperative multi-agent tasks. These algorithms restrain the Kullback–Leibler (KL) divergence (i.e., TR constraint) between the current and new policies to avoid aggressive update steps and improve learning performance. However, the majority of existing TR-based MARL algorithms are on-policy, meaning that they require new data sampled by current policies for training and cannot utilize off-policy (or historical) data, leading to low sample efficiency. This study aims to enhance the data efficiency of TR-based learning methods. To achieve this, an approximation of the original objective function is designed. In addition, it is proven that as long as the update size of the policy (measured by the KL divergence) is restricted, optimizing the designed objective function using historical data can guarantee the monotonic improvement of the original target. Building on the designed objective, a practical off-policy multi-agent stochastic policy gradient algorithm is proposed within the framework of centralized training with decentralized execution (CTDE). Additionally, policy entropy is integrated into the reward to promote exploration, and consequently, improve stability. Comprehensive experiments are conducted on a representative benchmark for multi-agent MuJoCo (MAMuJoCo), which offers a range of challenging tasks in cooperative continuous multi-agent control. The results demonstrate that the proposed algorithm outperforms all other existing algorithms by a significant margin.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006688",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Biology",
      "Computer science",
      "Divergence (linguistics)",
      "Evolutionary biology",
      "Function (biology)",
      "Geodesy",
      "Geography",
      "Kullback–Leibler divergence",
      "Linguistics",
      "Machine learning",
      "Margin (machine learning)",
      "Mathematical optimization",
      "Mathematics",
      "Philosophy",
      "Reinforcement learning",
      "Stability (learning theory)"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Delin"
      },
      {
        "surname": "Tang",
        "given_name": "Lan"
      },
      {
        "surname": "Zhang",
        "given_name": "Xinggan"
      },
      {
        "surname": "Liang",
        "given_name": "Ying-chang"
      }
    ]
  },
  {
    "title": "MCNet: A multi-level context-aware network for the segmentation of adrenal gland in CT images",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.028",
    "abstract": "Accurate segmentation of the adrenal gland from abdominal computed tomography (CT) scans is a crucial step towards facilitating the computer-aided diagnosis of adrenal-related diseases such as essential hypertension and adrenal tumors. However, the small size of the adrenal gland, which occupies less than 1% of the abdominal CT slice, poses a significant challenge to accurate segmentation. To address this problem, we propose a novel multi-level context-aware network (MCNet) to segment adrenal glands in CT images. Our MCNet mainly consists of two components, i.e., the multi-level context aggregation (MCA) module and multi-level context guidance (MCG) module. Specifically, the MCA module employs multi-branch dilated convolutional layers to capture geometric information, which enables handling of changes in complex scenarios such as variations in the size and shape of objects. The MCG module, on the other hand, gathers valuable features from the shallow layer and leverages the complete utilization of feature information at different resolutions in various codec stages. Finally, we evaluate the performance of the MCNet on two CT datasets, including our clinical dataset (Ad-Seg) and a publicly available dataset known as Distorted Golden Standards (DGS), from different perspectives. Compared to ten other state-of-the-art segmentation methods, our MCNet achieves 71.34% and 75.29% of the best Dice similarity coefficient on the two datasets, respectively, which is at least 2.46% and 1.19% higher than other segmentation methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006512",
    "keywords": [
      "Adrenal gland",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Convolutional neural network",
      "Feature (linguistics)",
      "Linguistics",
      "Medicine",
      "Paleontology",
      "Pathology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Jinhao"
      },
      {
        "surname": "Li",
        "given_name": "Huying"
      },
      {
        "surname": "Zhang",
        "given_name": "Yuan"
      },
      {
        "surname": "Wang",
        "given_name": "Zhiqiang"
      },
      {
        "surname": "Zhu",
        "given_name": "Sheng"
      },
      {
        "surname": "Li",
        "given_name": "Xuanya"
      },
      {
        "surname": "Hu",
        "given_name": "Kai"
      },
      {
        "surname": "Gao",
        "given_name": "Xieping"
      }
    ]
  },
  {
    "title": "Theoretical limits on the speed of learning inverse models explain the rate of adaptation in arm reaching tasks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.049",
    "abstract": "An essential aspect of human motor learning is the formation of inverse models, which map desired actions to motor commands. Inverse models can be learned by adjusting parameters in neural circuits to minimize errors in the performance of motor tasks through gradient descent. However, the theory of gradient descent establishes limits on the learning speed. Specifically, the eigenvalues of the Hessian of the error surface around a minimum determine the maximum speed of learning in a task. Here, we use this theoretical framework to analyze the speed of learning in different inverse model learning architectures in a set of isometric arm-reaching tasks. We show theoretically that, in these tasks, the error surface and, thus the speed of learning, are determined by the shapes of the force manipulability ellipsoid of the arm and the distribution of targets in the task. In particular, rounder manipulability ellipsoids generate a rounder error surface, allowing for faster learning of the inverse model. Rounder target distributions have a similar effect. We tested these predictions experimentally in a quasi-isometric reaching task with a visuomotor transformation. The experimental results were consistent with our theoretical predictions. Furthermore, our analysis accounts for the speed of learning in previous experiments with incompatible and compatible virtual surgery tasks, and with visuomotor rotation tasks with different numbers of targets. By identifying aspects of a task that influence the speed of learning, our results provide theoretical principles for the design of motor tasks that allow for faster learning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006147",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Classical mechanics",
      "Computer science",
      "Economics",
      "Gene",
      "Geometry",
      "Gradient descent",
      "Inverse",
      "Inverse dynamics",
      "Isometric exercise",
      "Kinematics",
      "Management",
      "Mathematics",
      "Medicine",
      "Motor learning",
      "Neuroscience",
      "Physical therapy",
      "Physics",
      "Task (project management)",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Barradas",
        "given_name": "Victor R."
      },
      {
        "surname": "Koike",
        "given_name": "Yasuharu"
      },
      {
        "surname": "Schweighofer",
        "given_name": "Nicolas"
      }
    ]
  },
  {
    "title": "A comparative analysis of multi-biometrics performance in human and action recognition using silhouette thermal-face and skeletal data",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.016",
    "abstract": "Biometrics is a field that has been given importance in recent years and has been extensively studied. Biometrics can use physical and behavioural differences that are unique to individuals to recognize and identify them. Today, biometric information is used in many areas such as computer vision systems, entrance systems, security and recognition. In this study, a new biometrics database containing silhouette, thermal face and skeletal data based on the distance between the joints was created to be used in behavioural and physical biometrics studies. The fact that many cameras were used in previous studies increases both the processing intensity and the material cost. This study aimed to both increase the recognition performance and reduce material costs by adding thermal face data in addition to soft and behavioural biometrics with the optimum camera. The presented data set was created in accordance with both motion recognition and person identification. Various data loss scenarios and multi-biometrics approaches based on data fusion have been tried on the created data sets and the results have been given comparatively. In addition, the correlation coefficient of the motion frames method to obtain energy images from silhouette data was tested on this dataset and yielded high-accuracy results for both motion and person recognition.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023005701",
    "keywords": [
      "Artificial intelligence",
      "Biology",
      "Biometrics",
      "Botany",
      "Computer science",
      "Computer vision",
      "Data set",
      "Face (sociological concept)",
      "Facial recognition system",
      "Field (mathematics)",
      "Gait",
      "Identification (biology)",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physiology",
      "Pure mathematics",
      "Silhouette",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Kurban",
        "given_name": "Onur Can"
      },
      {
        "surname": "Yildirim",
        "given_name": "Tülay"
      }
    ]
  },
  {
    "title": "Boundary uncertainty aware network for automated polyp segmentation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.050",
    "abstract": "Recently, leveraging deep neural networks for automated colorectal polyp segmentation has emerged as a hot topic due to the favored advantages in evading the limitations of visual inspection, e.g., overwork and subjectivity. However, most existing methods do not pay enough attention to the uncertain areas of colonoscopy images and often provide unsatisfactory segmentation performance. In this paper, we propose a novel boundary uncertainty aware network (BUNet) for precise and robust colorectal polyp segmentation. Specifically, considering that polyps vary greatly in size and shape, we first adopt a pyramid vision transformer encoder to learn multi-scale feature representations. Then, a simple yet effective boundary exploration module (BEM) is proposed to explore boundary cues from the low-level features. To make the network focus on the ambiguous area where the prediction score is biased to neither the foreground nor the background, we further introduce a boundary uncertainty aware module (BUM) that explores error-prone regions from the high-level features with the assistance of boundary cues provided by the BEM. Through the top-down hybrid deep supervision, our BUNet implements coarse-to-fine polyp segmentation and finally localizes polyp regions precisely. Extensive experiments on five public datasets show that BUNet is superior to thirteen competing methods in terms of both effectiveness and generalization ability.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006731",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Boundary (topology)",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Encoder",
      "Feature (linguistics)",
      "Geometry",
      "Image (mathematics)",
      "Image segmentation",
      "Inpainting",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pyramid (geometry)",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Yue",
        "given_name": "Guanghui"
      },
      {
        "surname": "Zhuo",
        "given_name": "Guibin"
      },
      {
        "surname": "Yan",
        "given_name": "Weiqing"
      },
      {
        "surname": "Zhou",
        "given_name": "Tianwei"
      },
      {
        "surname": "Tang",
        "given_name": "Chang"
      },
      {
        "surname": "Yang",
        "given_name": "Peng"
      },
      {
        "surname": "Wang",
        "given_name": "Tianfu"
      }
    ]
  },
  {
    "title": "A versatile Wavelet-Enhanced CNN-Transformer for improved fluorescence microscopy image restoration",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.039",
    "abstract": "Fluorescence microscopes are indispensable tools for the life science research community. Nevertheless, the presence of optical component limitations, coupled with the maximum photon budget that the specimen can tolerate, inevitably leads to a decline in imaging quality and a lack of useful signals. Therefore, image restoration becomes essential for ensuring high-quality and accurate analyses. This paper presents the Wavelet-Enhanced Convolutional-Transformer (WECT), a novel deep learning technique developed specifically for the purpose of reducing noise in microscopy images and attaining super-resolution. Unlike traditional approaches, WECT integrates wavelet transform and inverse-transform for multi-resolution image decomposition and reconstruction, resulting in an expanded receptive field for the network without compromising information integrity. Subsequently, multiple consecutive parallel CNN-Transformer modules are utilized to collaboratively model local and global dependencies, thus facilitating the extraction of more comprehensive and diversified deep features. In addition, the incorporation of generative adversarial networks (GANs) into WECT enhances its capacity to generate high perceptual quality microscopic images. Extensive experiments have demonstrated that the WECT framework outperforms current state-of-the-art restoration methods on real fluorescence microscopy data under various imaging modalities and conditions, in terms of quantitative and qualitative analysis.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006627",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Deep learning",
      "Image (mathematics)",
      "Image processing",
      "Image quality",
      "Image restoration",
      "Microscopy",
      "Optics",
      "Pattern recognition (psychology)",
      "Physics",
      "Wavelet",
      "Wavelet transform"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Qinghua"
      },
      {
        "surname": "Li",
        "given_name": "Ziwei"
      },
      {
        "surname": "Zhang",
        "given_name": "Shuqi"
      },
      {
        "surname": "Chi",
        "given_name": "Nan"
      },
      {
        "surname": "Dai",
        "given_name": "Qionghai"
      }
    ]
  },
  {
    "title": "Feature-wise scaling and shifting: Improving the generalization capability of neural networks through capturing independent information of features",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.040",
    "abstract": "From the perspective of input features, information can be divided into independent information and correlation information. Current neural networks mainly concentrate on the capturing of correlation information through connection weight parameters supplemented by bias parameters. This paper introduces feature-wise scaling and shifting (FwSS) into neural networks for capturing independent information of features, and proposes a new neural network FwSSNet. In the network, a pair of scale and shift parameters is added before each input of each network layer, and bias is removed. The parameters are initialized as 1 and 0, respectively, and trained at separate learning rates, to guarantee the fully capturing of independence and correlation information. The learning rates of FwSS parameters depend on input data and the training speed ratios of adjacent FwSS and connection sublayers, meanwhile those of weight parameters remain unchanged as plain networks. Further, FwSS unifies the scaling and shifting operations in batch normalization (BN), and FwSSNet with BN is established through introducing a preprocessing layer. FwSS parameters except those in the last layer of the network can be simply trained at the same learning rate as weight parameters. Experiments show that FwSS is generally helpful in improving the generalization capability of both fully connected neural networks and deep convolutional neural networks, and FWSSNets achieve higher accuracies on UCI repository and CIFAR-10.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006639",
    "keywords": [
      "Anthropology",
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Computer science",
      "Convolutional neural network",
      "Correlation",
      "Data mining",
      "Data pre-processing",
      "Feature (linguistics)",
      "Generalization",
      "Geometry",
      "Layer (electronics)",
      "Linguistics",
      "Mathematical analysis",
      "Mathematics",
      "Normalization (sociology)",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Preprocessor",
      "Scaling",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Tongfeng"
      },
      {
        "surname": "Wang",
        "given_name": "Xiurui"
      },
      {
        "surname": "Li",
        "given_name": "Zhongnian"
      },
      {
        "surname": "Ding",
        "given_name": "Shifei"
      }
    ]
  },
  {
    "title": "AdjointBackMapV2: Precise reconstruction of arbitrary CNN unit’s activation via adjoint operators",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.009",
    "abstract": "Adjoint operators have been found to be effective in the exploration of CNN’s inner workings (Wan and Choe, 2022). However, the previous no-bias assumption restricted its generalization. We overcome the restriction via embedding input images into an extended normed space that includes bias in all CNN layers as part of the extended space and propose an adjoint-operator-based algorithm that maps high-level weights back to the extended input space for reconstructing an effective hypersurface. Such hypersurface can be computed for an arbitrary unit in the CNN, and we prove that this reconstructed hypersurface, when multiplied by the original input (through an inner product), will precisely replicate the output value of each unit. We show experimental results based on the CIFAR-10 and CIFAR-100 data sets where the proposed approach achieves near 0 activation value reconstruction error.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006329",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Embedding",
      "Gene",
      "Generalization",
      "Hypersurface",
      "Mathematical analysis",
      "Mathematics",
      "Mathematics education",
      "Operating system",
      "Operator (biology)",
      "Repressor",
      "Space (punctuation)",
      "Statistics",
      "Transcription factor",
      "Unit (ring theory)",
      "Unit sphere",
      "Value (mathematics)"
    ],
    "authors": [
      {
        "surname": "Wan",
        "given_name": "Qing"
      },
      {
        "surname": "Cheung",
        "given_name": "Siu Wun"
      },
      {
        "surname": "Choe",
        "given_name": "Yoonsuck"
      }
    ]
  },
  {
    "title": "LFighter: Defending against the label-flipping attack in federated learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.019",
    "abstract": "Federated learning (FL) provides autonomy and privacy by design to participating peers, who cooperatively build a machine learning (ML) model while keeping their private data in their devices. However, that same autonomy opens the door for malicious peers to poison the model by conducting either untargeted or targeted poisoning attacks. The label-flipping (LF) attack is a targeted poisoning attack where the attackers poison their training data by flipping the labels of some examples from one class (i.e., the source class) to another (i.e., the target class). Unfortunately, this attack is easy to perform and hard to detect, and it negatively impacts the performance of the global model. Existing defenses against LF are limited by assumptions on the distribution of the peers’ data and/or do not perform well with high-dimensional models. In this paper, we deeply investigate the LF attack behavior. We find that the contradicting objectives of attackers and honest peers on the source class examples are reflected on the parameter gradients corresponding to the neurons of the source and target classes in the output layer. This makes those gradients good discriminative features for the attack detection. Accordingly, we propose LFighter, a novel defense against the LF attack that first dynamically extracts those gradients from the peers’ local updates and then clusters the extracted gradients, analyzes the resulting clusters, and filters out potential bad updates before model aggregation. Extensive empirical analysis on three data sets shows the effectiveness of the proposed defense regardless of the data distribution or model dimensionality. Also, LFighter outperforms several state-of-the-art defenses by offering lower test error, higher overall accuracy, higher source class accuracy, lower attack success rate, and higher stability of the source class accuracy. Our code and data are available for reproducibility purposes at https://github.com/NajeebJebreel/LFighter.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006421",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Computer security",
      "Curse of dimensionality",
      "Discriminative model",
      "Machine learning"
    ],
    "authors": [
      {
        "surname": "Jebreel",
        "given_name": "Najeeb Moharram"
      },
      {
        "surname": "Domingo-Ferrer",
        "given_name": "Josep"
      },
      {
        "surname": "Sánchez",
        "given_name": "David"
      },
      {
        "surname": "Blanco-Justicia",
        "given_name": "Alberto"
      }
    ]
  },
  {
    "title": "Dynamic learning from adaptive neural control for full-state constrained strict-feedback nonlinear systems",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.064",
    "abstract": "This study focuses on the learning and control issues of strict-feedback systems with full-state constraints. To achieve learning capability under constraints, transformation mapping is utilized to convert the original system with full-state constraints into a quasi-pure-feedback unconstrained system. Utilizing the system transformation technique, only a single neural network (NN) is required to identify the unknown dynamics within the transformed system. Combining the dynamic surface control design, a novel adaptive neural control scheme is developed to ensure that all closed-loop signals are uniformly bounded, and every system state remains within the predefined constraint range. In addition, the precise convergence of NN weights is further transformed into an exponential stability problem for a category of linear time-varying systems under persistent excitation conditions. Subsequently, the converged NN weights are efficiently stored and utilized to create a learning controller to achieve better control performance while abiding by the full-state constraints. The viability of this control strategy is demonstrated via simulations.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006913",
    "keywords": [
      "Adaptive control",
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Biology",
      "Bounded function",
      "Chemistry",
      "Computer science",
      "Constraint (computer-aided design)",
      "Control (management)",
      "Control theory (sociology)",
      "Controller (irrigation)",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Gene",
      "Geometry",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Nonlinear system",
      "Physics",
      "Quantum mechanics",
      "Stability (learning theory)",
      "State (computer science)",
      "Transformation (genetics)"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Qinchen"
      },
      {
        "surname": "Zhang",
        "given_name": "Fukai"
      },
      {
        "surname": "Sun",
        "given_name": "Qinghua"
      },
      {
        "surname": "Wang",
        "given_name": "Cong"
      }
    ]
  },
  {
    "title": "CT-Net: Asymmetric compound branch Transformer for medical image segmentation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.034",
    "abstract": "The Transformer architecture has been widely applied in the field of image segmentation due to its powerful ability to capture long-range dependencies. However, its ability to capture local features is relatively weak and it requires a large amount of data for training. Medical image segmentation tasks, on the other hand, demand high requirements for local features and are often applied to small datasets. Therefore, existing Transformer networks show a significant decrease in performance when applied directly to this task. To address these issues, we have designed a new medical image segmentation architecture called CT-Net. It effectively extracts local and global representations using an asymmetric asynchronous branch parallel structure, while reducing unnecessary computational costs. In addition, we propose a high-density information fusion strategy that efficiently fuses the features of two branches using a fusion module of only 0.05M. This strategy ensures high portability and provides conditions for directly applying transfer learning to solve dataset dependency issues. Finally, we have designed a parameter-adjustable multi-perceptive loss function for this architecture to optimize the training process from both pixel-level and global perspectives. We have tested this network on 5 different tasks with 9 datasets, and compared to SwinUNet, CT-Net improves the IoU by 7.3% and 1.8% on Glas and MoNuSeg datasets respectively. Moreover, compared to SwinUNet, the average DSC on the Synapse dataset is improved by 3.5%.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006573",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Deep learning",
      "Image segmentation",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Segmentation",
      "Software portability",
      "Transformer",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Ning"
      },
      {
        "surname": "Yu",
        "given_name": "Long"
      },
      {
        "surname": "Zhang",
        "given_name": "Dezhi"
      },
      {
        "surname": "Wu",
        "given_name": "Weidong"
      },
      {
        "surname": "Tian",
        "given_name": "Shengwei"
      },
      {
        "surname": "Kang",
        "given_name": "Xiaojing"
      },
      {
        "surname": "Li",
        "given_name": "Min"
      }
    ]
  },
  {
    "title": "SVD-AE: An asymmetric autoencoder with SVD regularization for multivariate time series anomaly detection",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.023",
    "abstract": "Anomaly detection in multivariate time series is of critical importance in many real-world applications, such as system maintenance and Internet monitoring. In this article, we propose a novel unsupervised framework called SVD-AE to conduct anomaly detection in multivariate time series. The core idea is to fuse the strengths of both SVD and autoencoder to fully capture complex normal patterns in multivariate time series. An asymmetric autoencoder architecture is proposed, where two encoders are used to capture features in time and variable dimensions and a shared decoder is used to generate reconstructions based on latent representations from both dimensions. A new regularization based on singular value decomposition theory is designed to force each encoder to learn features in the corresponding axis with mathematical supports delivered. A specific loss component is further proposed to align Fourier coefficients of inputs and reconstructions. It can preserve details of original inputs, leading to enhanced feature learning capability of the model. Extensive experiments on three real world datasets demonstrate the proposed algorithm can achieve better performance on multivariate time series anomaly detection tasks under highly unbalanced scenarios compared with baseline algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006469",
    "keywords": [
      "Algorithm",
      "Anomaly detection",
      "Artificial intelligence",
      "Autoencoder",
      "Computer science",
      "Deep learning",
      "Encoder",
      "Machine learning",
      "Multivariate statistics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)",
      "Singular value decomposition"
    ],
    "authors": [
      {
        "surname": "Yao",
        "given_name": "Yueyue"
      },
      {
        "surname": "Ma",
        "given_name": "Jianghong"
      },
      {
        "surname": "Feng",
        "given_name": "Shanshan"
      },
      {
        "surname": "Ye",
        "given_name": "Yunming"
      }
    ]
  },
  {
    "title": "BrainGridNet: A two-branch depthwise CNN for decoding EEG-based multi-class motor imagery",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.037",
    "abstract": "Brain–computer interfaces (BCIs) based on motor imagery (MI) enable the disabled to interact with the world through brain signals. To meet demands of real-time, stable, and diverse interactions, it is crucial to develop lightweight networks that can accurately and reliably decode multi-class MI tasks. In this paper, we introduce BrainGridNet, a convolutional neural network (CNN) framework that integrates two intersecting depthwise CNN branches with 3D electroencephalography (EEG) data to decode a five-class MI task. The BrainGridNet attains competitive results in both the time and frequency domains, with superior performance in the frequency domain. As a result, an accuracy of 80.26 percent and a kappa value of 0.753 are achieved by BrainGridNet, surpassing the state-of-the-art (SOTA) model. Additionally, BrainGridNet shows optimal computational efficiency, excels in decoding the most challenging subject, and maintains robust accuracy despite the random loss of 16 electrode signals. Finally, the visualizations demonstrate that BrainGridNet learns discriminative features and identifies critical brain regions and frequency bands corresponding to each MI class. The convergence of BrainGridNet’s strong feature extraction capability, high decoding accuracy, steady decoding efficacy, and low computational costs renders it an appealing choice for facilitating the development of BCIs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006603",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Brain–computer interface",
      "Class (philosophy)",
      "Computer science",
      "Convolutional neural network",
      "Decoding methods",
      "Discriminative model",
      "Economics",
      "Electroencephalography",
      "Feature extraction",
      "Management",
      "Motor imagery",
      "Neural decoding",
      "Pattern recognition (psychology)",
      "Psychiatry",
      "Psychology",
      "Speech recognition",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xingfu"
      },
      {
        "surname": "Wang",
        "given_name": "Yu"
      },
      {
        "surname": "Qi",
        "given_name": "Wenxia"
      },
      {
        "surname": "Kong",
        "given_name": "Delin"
      },
      {
        "surname": "Wang",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Exploiting nonlinear dendritic adaptive computation in training deep Spiking Neural Networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.056",
    "abstract": "Inspired by the information transmission process in the brain, Spiking Neural Networks (SNNs) have gained considerable attention due to their event-driven nature. However, as the network structure grows complex, managing the spiking behavior within the network becomes challenging. Networks with excessively dense or sparse spikes fail to transmit sufficient information, inhibiting SNNs from exhibiting superior performance. Current SNNs linearly sum presynaptic information in postsynaptic neurons, overlooking the adaptive adjustment effect of dendrites on information processing. In this study, we introduce the Dendritic Spatial Gating Module (DSGM), which scales and translates the input, reducing the loss incurred when transforming the continuous membrane potential into discrete spikes. Simultaneously, by implementing the Dendritic Temporal Adjust Module (DTAM), dendrites assign different importance to inputs of different time steps, facilitating the establishment of the temporal dependency of spiking neurons and effectively integrating multi-step time information. The fusion of these two modules results in a more balanced spike representation within the network, significantly enhancing the neural network’s performance. This approach has achieved state-of-the-art performance on static image datasets, including CIFAR10 and CIFAR100, as well as event datasets like DVS-CIFAR10, DVS-Gesture, and N-Caltech101. It also demonstrates competitive performance compared to the current state-of-the-art on the ImageNet dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006202",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computation",
      "Computer science",
      "Machine learning",
      "Meteorology",
      "Models of neural computation",
      "Nonlinear system",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Spiking neural network",
      "Training (meteorology)"
    ],
    "authors": [
      {
        "surname": "Shen",
        "given_name": "Guobin"
      },
      {
        "surname": "Zhao",
        "given_name": "Dongcheng"
      },
      {
        "surname": "Zeng",
        "given_name": "Yi"
      }
    ]
  },
  {
    "title": "Genetic high-gain controller to improve the position perturbation attenuation and compact high-gain controller to improve the velocity perturbation attenuation in inverted pendulums",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.029",
    "abstract": "From the control theory, bigger observer and controller gains are utilized to improve the position perturbation attenuation, while smaller observer and controller gains are utilized to improve the velocity perturbation attenuation. Therefore, it would be interesting to suggest optimizers to find the best observer and controller gains to improve the position or velocity perturbation attenuation. In this investigation, a high-gain controller is suggested to obtain the perturbation attenuation, a genetic high-gain controller is suggested to improve the position perturbation attenuation, and a compact high-gain controller is suggested to improve velocity perturbation attenuation in inverted pendulums. The high-gain controller utilizes a high-gain observer and a high-gain estimator to obtain the state and perturbation estimation. The genetic high-gain controller utilizes a genetic optimizer to find the best observer and controller gains. The compact high-gain controller utilizes a compact optimizer to find the best observer and controller gains. The suggested high-gain controllers are compared with the simplex and bat controllers to improve the position or velocity perturbation attenuation in two inverted pendulums.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006536",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Attenuation",
      "Biology",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Controller (irrigation)",
      "Estimator",
      "Mathematics",
      "Observer (physics)",
      "Optics",
      "Perturbation (astronomy)",
      "Physics",
      "Quantum mechanics",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "de Jesús Rubio",
        "given_name": "José"
      },
      {
        "surname": "Hernandez",
        "given_name": "Mario Alberto"
      },
      {
        "surname": "Rosas",
        "given_name": "Francisco Javier"
      },
      {
        "surname": "Orozco",
        "given_name": "Eduardo"
      },
      {
        "surname": "Balcazar",
        "given_name": "Ricardo"
      },
      {
        "surname": "Pacheco",
        "given_name": "Jaime"
      }
    ]
  },
  {
    "title": "Learning deep representation and discriminative features for clustering of multi-layer networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.053",
    "abstract": "The multi-layer network consists of the interactions between different layers, where each layer of the network is depicted as a graph, providing a comprehensive way to model the underlying complex systems. The layer-specific modules of multi-layer networks are critical to understanding the structure and function of the system. However, existing methods fail to characterize and balance the connectivity and specificity of layer-specific modules in networks because of the complicated inter- and intra-coupling of various layers. To address the above issues, a joint learning graph clustering algorithm (DRDF) for detecting layer-specific modules in multi-layer networks is proposed, which simultaneously learns the deep representation and discriminative features. Specifically, DRDF learns the deep representation with deep nonnegative matrix factorization, where the high-order topology of the multi-layer network is gradually and precisely characterized. Moreover, it addresses the specificity of modules with discriminative feature learning, where the intra-class compactness and inter-class separation of pseudo-labels of clusters are explored as self-supervised information, thereby providing a more accurate method to explicitly model the specificity of the multi-layer network. Finally, DRDF balances the connectivity and specificity of layer-specific modules with joint learning, where the overall objective of the graph clustering algorithm and optimization rules are derived. The experiments on ten multi-layer networks showed that DRDF not only outperforms eight baselines on graph clustering but also enhances the robustness of algorithms.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006767",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Cluster analysis",
      "Computer science",
      "Discriminative model",
      "Feature learning",
      "Gene",
      "Graph",
      "Law",
      "Layer (electronics)",
      "Organic chemistry",
      "Pattern recognition (psychology)",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Robustness (evolution)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Wenming"
      },
      {
        "surname": "Ma",
        "given_name": "Xiaoke"
      },
      {
        "surname": "Wang",
        "given_name": "Quan"
      },
      {
        "surname": "Gong",
        "given_name": "Maoguo"
      },
      {
        "surname": "Gao",
        "given_name": "Quanxue"
      }
    ]
  },
  {
    "title": "A filter-augmented auto-encoder with learnable normalization for robust multivariate time series anomaly detection",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.047",
    "abstract": "While existing reconstruction-based multivariate time series (MTS) anomaly detection methods demonstrate advanced performance on many challenging real-world datasets, they generally assume the data only consists of normal samples when training models. However, real-world MTS data may contain significant noise and even be contaminated by anomalies. As a result, most existing approaches easily capture the pattern of the contaminated data, making identifying anomalies more difficult. Although a few studies have aimed to mitigate the interference of the noise and anomalies by introducing various regularizations, they still employ the objective of fully reconstructing the input data, impeding the model from learning an accurate profile of the MTS’s normal pattern. Moreover, it is difficult for existing methods to apply the most appropriate normalization schemes for each dataset in various complex scenarios, particularly for mixed-feature MTS. This paper proposes a filter-augmented auto-encoder with learnable normalization (NormFAAE) for robust MTS anomaly detection. Firstly, NormFAAE designs a deep hybrid normalization module. It is trained with the backbone end-to-end in the current training task to perform the optimal normalization scheme. Meanwhile, it integrates two learnable normalization sub-modules to deal with the mixed-feature MTS effectively. Secondly, NormFAAE proposes a filter-augmented auto-encoder with a dual-phase task. It separates the noise and anomalies from the input data by a deep filter module, which facilitates the model to only reconstruct the normal data, achieving a more robust latent representation of MTS. Experimental results demonstrate that NormFAAE outperforms 17 typical baselines on five real-world industrial datasets from diverse fields.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006706",
    "keywords": [
      "Anomaly detection",
      "Anthropology",
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Data mining",
      "Encoder",
      "Filter (signal processing)",
      "Normalization (sociology)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Yu",
        "given_name": "Jiahao"
      },
      {
        "surname": "Gao",
        "given_name": "Xin"
      },
      {
        "surname": "Li",
        "given_name": "Baofeng"
      },
      {
        "surname": "Zhai",
        "given_name": "Feng"
      },
      {
        "surname": "Lu",
        "given_name": "Jiansheng"
      },
      {
        "surname": "Xue",
        "given_name": "Bing"
      },
      {
        "surname": "Fu",
        "given_name": "Shiyuan"
      },
      {
        "surname": "Xiao",
        "given_name": "Chun"
      }
    ]
  },
  {
    "title": "BrainGridNet: A two-branch depthwise CNN for decoding EEG-based multi-class motor imagery",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.037",
    "abstract": "Brain–computer interfaces (BCIs) based on motor imagery (MI) enable the disabled to interact with the world through brain signals. To meet demands of real-time, stable, and diverse interactions, it is crucial to develop lightweight networks that can accurately and reliably decode multi-class MI tasks. In this paper, we introduce BrainGridNet, a convolutional neural network (CNN) framework that integrates two intersecting depthwise CNN branches with 3D electroencephalography (EEG) data to decode a five-class MI task. The BrainGridNet attains competitive results in both the time and frequency domains, with superior performance in the frequency domain. As a result, an accuracy of 80.26 percent and a kappa value of 0.753 are achieved by BrainGridNet, surpassing the state-of-the-art (SOTA) model. Additionally, BrainGridNet shows optimal computational efficiency, excels in decoding the most challenging subject, and maintains robust accuracy despite the random loss of 16 electrode signals. Finally, the visualizations demonstrate that BrainGridNet learns discriminative features and identifies critical brain regions and frequency bands corresponding to each MI class. The convergence of BrainGridNet’s strong feature extraction capability, high decoding accuracy, steady decoding efficacy, and low computational costs renders it an appealing choice for facilitating the development of BCIs.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006603",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Brain–computer interface",
      "Class (philosophy)",
      "Computer science",
      "Convolutional neural network",
      "Decoding methods",
      "Discriminative model",
      "Economics",
      "Electroencephalography",
      "Feature extraction",
      "Management",
      "Motor imagery",
      "Neural decoding",
      "Pattern recognition (psychology)",
      "Psychiatry",
      "Psychology",
      "Speech recognition",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Xingfu"
      },
      {
        "surname": "Wang",
        "given_name": "Yu"
      },
      {
        "surname": "Qi",
        "given_name": "Wenxia"
      },
      {
        "surname": "Kong",
        "given_name": "Delin"
      },
      {
        "surname": "Wang",
        "given_name": "Wei"
      }
    ]
  },
  {
    "title": "Exploiting nonlinear dendritic adaptive computation in training deep Spiking Neural Networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.056",
    "abstract": "Inspired by the information transmission process in the brain, Spiking Neural Networks (SNNs) have gained considerable attention due to their event-driven nature. However, as the network structure grows complex, managing the spiking behavior within the network becomes challenging. Networks with excessively dense or sparse spikes fail to transmit sufficient information, inhibiting SNNs from exhibiting superior performance. Current SNNs linearly sum presynaptic information in postsynaptic neurons, overlooking the adaptive adjustment effect of dendrites on information processing. In this study, we introduce the Dendritic Spatial Gating Module (DSGM), which scales and translates the input, reducing the loss incurred when transforming the continuous membrane potential into discrete spikes. Simultaneously, by implementing the Dendritic Temporal Adjust Module (DTAM), dendrites assign different importance to inputs of different time steps, facilitating the establishment of the temporal dependency of spiking neurons and effectively integrating multi-step time information. The fusion of these two modules results in a more balanced spike representation within the network, significantly enhancing the neural network’s performance. This approach has achieved state-of-the-art performance on static image datasets, including CIFAR10 and CIFAR100, as well as event datasets like DVS-CIFAR10, DVS-Gesture, and N-Caltech101. It also demonstrates competitive performance compared to the current state-of-the-art on the ImageNet dataset.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006202",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computation",
      "Computer science",
      "Machine learning",
      "Meteorology",
      "Models of neural computation",
      "Nonlinear system",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Spiking neural network",
      "Training (meteorology)"
    ],
    "authors": [
      {
        "surname": "Shen",
        "given_name": "Guobin"
      },
      {
        "surname": "Zhao",
        "given_name": "Dongcheng"
      },
      {
        "surname": "Zeng",
        "given_name": "Yi"
      }
    ]
  },
  {
    "title": "Combining external-latent attention for medical image segmentation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.046",
    "abstract": "The attention mechanism comes as a new entry point for improving the performance of medical image segmentation. How to reasonably assign weights is a key element of the attention mechanism, and the current popular schemes include the global squeezing and the non-local information interactions using self-attention (SA) operation. However, these approaches over-focus on external features and lack the exploitation of latent features. The global squeezing approach crudely represents the richness of contextual information by the global mean or maximum value, while non-local information interactions focus on the similarity of external features between different regions. Both ignore the fact that the contextual information is presented more in terms of the latent features like the frequency change within the data. To tackle above problems and make proper use of attention mechanisms in medical image segmentation, we propose an external-latent attention collaborative guided image segmentation network, named TransGuider. This network consists of three key components: 1) a latent attention module that uses an improved entropy quantification method to accurately explore and locate the distribution of latent contextual information. 2) an external self-attention module using sparse representation, which can preserve external global contextual information while reducing computational overhead by selecting representative feature description map for SA operation. 3) a multi-attention collaborative module to guide the network to continuously focus on the region of interest, refining the segmentation mask. Our experimental results on several benchmark medical image segmentation datasets show that TransGuider outperforms the state-of-the-art methods, and extensive ablation experiments demonstrate the effectiveness of the proposed components. Our code will be available at https://github.com/chasingone/TransGuider.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006081",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Data mining",
      "Feature (linguistics)",
      "Focus (optics)",
      "Image segmentation",
      "Key (lock)",
      "Linguistics",
      "Machine learning",
      "Optics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Enmin"
      },
      {
        "surname": "Zhan",
        "given_name": "Bangcheng"
      },
      {
        "surname": "Liu",
        "given_name": "Hong"
      }
    ]
  },
  {
    "title": "Boosting semi-supervised learning with Contrastive Complementary Labeling",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.052",
    "abstract": "Semi-supervised learning (SSL) approaches have achieved great success in leveraging a large amount of unlabeled data to learn deep models. Among them, one popular approach is pseudo-labeling which generates pseudo labels only for those unlabeled data with high-confidence predictions. As for the low-confidence ones, existing methods often simply discard them because these unreliable pseudo labels may mislead the model. Unlike existing methods, we highlight that these low-confidence data can be still beneficial to the training process. Specifically, although we cannot determine which class a low-confidence sample belongs to, we can assume that this sample should be very unlikely to belong to those classes with the lowest probabilities (often called complementary classes/labels). Inspired by this, we propose a novel Contrastive Complementary Labeling (CCL) method that constructs a large number of reliable negative pairs based on the complementary labels and adopts contrastive learning to make use of all the unlabeled data. Extensive experiments demonstrate that CCL significantly improves the performance on top of existing advanced methods and is particularly effective under the label-scarce settings. For example, CCL yields an improvement of 2.43% over FixMatch on CIFAR-10 only with 40 labeled data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006743",
    "keywords": [
      "Artificial intelligence",
      "Boosting (machine learning)",
      "Chemistry",
      "Chromatography",
      "Class (philosophy)",
      "Computer science",
      "Labeled data",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Sample (material)",
      "Semi-supervised learning",
      "Training set"
    ],
    "authors": [
      {
        "surname": "Deng",
        "given_name": "Qinyi"
      },
      {
        "surname": "Guo",
        "given_name": "Yong"
      },
      {
        "surname": "Yang",
        "given_name": "Zhibang"
      },
      {
        "surname": "Pan",
        "given_name": "Haolin"
      },
      {
        "surname": "Chen",
        "given_name": "Jian"
      }
    ]
  },
  {
    "title": "Grounding spatial relations in text-only language models",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.031",
    "abstract": "This paper shows that text-only Language Models (LM) can learn to ground spatial relations like left of or below if they are provided with explicit location information of objects and they are properly trained to leverage those locations. We perform experiments on a verbalized version of the Visual Spatial Reasoning (VSR) dataset, where images are coupled with textual statements which contain real or fake spatial relations between two objects of the image. We verbalize the images using an off-the-shelf object detector, adding location tokens to every object label to represent their bounding boxes in textual form. Given the small size of VSR, we do not observe any improvement when using locations, but pretraining the LM over a synthetic dataset automatically derived by us improves results significantly when using location tokens. We thus show that locations allow LMs to ground spatial relations, with our text-only LMs outperforming Vision-and-Language Models and setting the new state-of-the-art for the VSR dataset. Our analysis show that our text-only LMs can generalize beyond the relations seen in the synthetic dataset to some extent, learning also more useful information than that encoded in the spatial rules we used to create the synthetic dataset itself.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802300655X",
    "keywords": [
      "Artificial intelligence",
      "Bounding overwatch",
      "Computer science",
      "Ground truth",
      "Language model",
      "Leverage (statistics)",
      "Mathematics",
      "Natural language processing",
      "Object (grammar)",
      "Pattern recognition (psychology)",
      "Spatial analysis",
      "Spatial intelligence",
      "Spatial relation",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Azkune",
        "given_name": "Gorka"
      },
      {
        "surname": "Salaberria",
        "given_name": "Ander"
      },
      {
        "surname": "Agirre",
        "given_name": "Eneko"
      }
    ]
  },
  {
    "title": "Sampled-data controller scheme for multi-agent systems and its Application to circuit network",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.059",
    "abstract": "The objective of this study is to investigate the synchronization criteria under the sampled-data control method for multi-agent systems (MASs) with state quantization and time-varying delay. Currently, a looped Lyapunov–Krasovskii Functional (LKF) has been developed, which integrates information from the sampling interval to ensure that the leader system synchronizes with the follower system, resulting in a specific condition in the form of Linear Matrix Inequalities (LMIs). The LMIs can be easily solved using the LMI Control toolbox in Matlab. Finally, the proposed approach’s feasibility and effectiveness are demonstrated through numerical simulations and comparative results.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006822",
    "keywords": [
      "Agronomy",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Controller (irrigation)",
      "MATLAB",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Programming language",
      "Quantization (signal processing)",
      "Scheme (mathematics)",
      "State (computer science)",
      "Synchronization (alternating current)",
      "Toolbox"
    ],
    "authors": [
      {
        "surname": "Stephen",
        "given_name": "A."
      },
      {
        "surname": "Karthikeyan",
        "given_name": "R."
      },
      {
        "surname": "Sowmiya",
        "given_name": "C."
      },
      {
        "surname": "Raja",
        "given_name": "R."
      },
      {
        "surname": "Agarwal",
        "given_name": "Ravi P."
      }
    ]
  },
  {
    "title": "UTDNet: A unified triplet decoder network for multimodal salient object detection",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.051",
    "abstract": "Image Salient Object Detection (SOD) is a fundamental research topic in the area of computer vision. Recently, the multimodal information in RGB, Depth (D), and Thermal (T) modalities has been proven to be beneficial to the SOD. However, existing methods are only designed for RGB-D or RGB-T SOD, which may limit the utilization in various modalities, or just finetuned on specific datasets, which may bring about extra computation overhead. These defects can hinder the practical deployment of SOD in real-world applications. In this paper, we propose an end-to-end Unified Triplet Decoder Network, dubbed UTDNet, for both RGB-T and RGB-D SOD tasks. The intractable challenges for the unified multimodal SOD are mainly two-fold, i.e., (1) accurately detecting and segmenting salient objects, and (2) preferably via a single network that fits both RGB-T and RGB-D SOD. First, to deal with the former challenge, we propose the multi-scale feature extraction unit to enrich the discriminative contextual information, and the efficient fusion module to explore cross-modality complementary information. Then, the multimodal features are fed to the triplet decoder, where the hierarchical deep supervision loss further enable the network to capture distinctive saliency cues. Second, as to the latter challenge, we propose a simple yet effective continual learning method to unify multimodal SOD. Concretely, we sequentially train multimodal SOD tasks by applying Elastic Weight Consolidation (EWC) regularization with the hierarchical loss function to avoid catastrophic forgetting without inducing more parameters. Critically, the triplet decoder separates task-specific and task-invariant information, making the network easily adaptable to multimodal SOD tasks. Extensive comparisons with 26 recently proposed RGB-T and RGB-D SOD methods demonstrate the superiority of the proposed UTDNet.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006755",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Discriminative model",
      "Fusion",
      "Fusion mechanism",
      "Linguistics",
      "Lipid bilayer fusion",
      "Machine learning",
      "Modalities",
      "Pattern recognition (psychology)",
      "Philosophy",
      "RGB color model",
      "Social science",
      "Sociology"
    ],
    "authors": [
      {
        "surname": "Huo",
        "given_name": "Fushuo"
      },
      {
        "surname": "Liu",
        "given_name": "Ziming"
      },
      {
        "surname": "Guo",
        "given_name": "Jingcai"
      },
      {
        "surname": "Xu",
        "given_name": "Wenchao"
      },
      {
        "surname": "Guo",
        "given_name": "Song"
      }
    ]
  },
  {
    "title": "A novel fixed-time error-monitoring neural network for solving dynamic quaternion-valued Sylvester equations",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.058",
    "abstract": "This paper addresses the dynamic quaternion-valued Sylvester equation (DQSE) using the quaternion real representation and the neural network method. To transform the Sylvester equation in the quaternion field into an equivalent equation in the real field, three different real representation modes for the quaternion are adopted by considering the non-commutativity of quaternion multiplication. Based on the equivalent Sylvester equation in the real field, a novel recurrent neural network model with an integral design formula is proposed to solve the DQSE. The proposed model, referred to as the fixed-time error-monitoring neural network (FTEMNN), achieves fixed-time convergence through the action of a state-of-the-art nonlinear activation function. The fixed-time convergence of the FTEMNN model is theoretically analyzed. Two examples are presented to verify the performance of the FTEMNN model with a specific focus on fixed-time convergence. Furthermore, the chattering phenomenon of the FTEMNN model is discussed, and a saturation function scheme is designed. Finally, the practical value of the FTEMNN model is demonstrated through its application to image fusion denoising.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006810",
    "keywords": [
      "Algorithm",
      "Applied mathematics",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Eigenvalues and eigenvectors",
      "Field (mathematics)",
      "Geometry",
      "Mathematics",
      "Physics",
      "Pure mathematics",
      "Quantum mechanics",
      "Quaternion",
      "Sylvester equation"
    ],
    "authors": [
      {
        "surname": "Xiao",
        "given_name": "Lin"
      },
      {
        "surname": "Cao",
        "given_name": "Penglin"
      },
      {
        "surname": "Wang",
        "given_name": "Zidong"
      },
      {
        "surname": "Liu",
        "given_name": "Sai"
      }
    ]
  },
  {
    "title": "Classification of DBS microelectrode recordings using a residual neural network with attention in the temporal domain",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.021",
    "abstract": "During the Deep Brain Stimulation (DBS) surgery for Parkinson’s disease (PD), the main goal is to place the permanent stimulating electrode into an area of the brain that becomes pathologically hyperactive. This area, called Subthalamic Nucleus (STN), is small and located deep within the brain. Therefore, the main challenge is the precise localization of the STN region, considering various measurement errors and artifacts. In this paper, we have designed and developed a computer-aided decision support system for neurosurgical DBS surgery. The implementation of this system provides a novel method for calculating the expected position of the stimulating electrode based on the recordings of the electrical activity of brain tissue. The artificial neural network with attention is used to classify the microelectrode recordings and determine the final position of the stimulating electrode within the STN area. Experiments have verified the utility and efficiency of our system. The tests were carried out on many recordings collected during DBS surgeries, giving encouraging results. The experimental results demonstrate that deep learning methods extended with self-attention blocks compete with the other solutions. They provide significant robustness to recording artifacts and improve the accuracy of the stimulating electrode placement.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006457",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Computer science",
      "Domain (mathematical analysis)",
      "Electrode",
      "Mathematical analysis",
      "Mathematics",
      "Microelectrode",
      "Multielectrode array",
      "Neural engineering",
      "Pattern recognition (psychology)",
      "Physical chemistry",
      "Residual"
    ],
    "authors": [
      {
        "surname": "Ciecierski",
        "given_name": "K.A."
      },
      {
        "surname": "Mandat",
        "given_name": "T."
      }
    ]
  },
  {
    "title": "AdaSAM: Boosting sharpness-aware minimization with adaptive learning rate and momentum for training deep neural networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.044",
    "abstract": "Sharpness aware minimization (SAM) optimizer has been extensively explored as it can generalize better for training deep neural networks via introducing extra perturbation steps to flatten the landscape of deep learning models. Integrating SAM with adaptive learning rate and momentum acceleration, dubbed AdaSAM, has already been explored empirically to train large-scale deep neural networks without theoretical guarantee due to the triple difficulties in analyzing the coupled perturbation step, adaptive learning rate and momentum step. In this paper, we try to analyze the convergence rate of AdaSAM in the stochastic non-convex setting. We theoretically show that AdaSAM admits a O ( 1 / b T ) convergence rate, which achieves linear speedup property with respect to mini-batch size b . Specifically, to decouple the stochastic gradient steps with the adaptive learning rate and perturbed gradient, we introduce the delayed second-order momentum term to decompose them to make them independent while taking an expectation during the analysis. Then we bound them by showing the adaptive learning rate has a limited range, which makes our analysis feasible. To the best of our knowledge, we are the first to provide the non-trivial convergence rate of SAM with an adaptive learning rate and momentum acceleration. At last, we conduct several experiments on several NLP tasks and the synthetic task, which show that AdaSAM could achieve superior performance compared with SGD, AMSGrad, and SAM optimizers.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006068",
    "keywords": [
      "Acceleration",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Boosting (machine learning)",
      "Channel (broadcasting)",
      "Classical mechanics",
      "Computer network",
      "Computer science",
      "Deep learning",
      "Deep neural networks",
      "Economics",
      "Finance",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Minification",
      "Momentum (technical analysis)",
      "Operating system",
      "Perturbation (astronomy)",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Rate of convergence",
      "Speedup"
    ],
    "authors": [
      {
        "surname": "Sun",
        "given_name": "Hao"
      },
      {
        "surname": "Shen",
        "given_name": "Li"
      },
      {
        "surname": "Zhong",
        "given_name": "Qihuang"
      },
      {
        "surname": "Ding",
        "given_name": "Liang"
      },
      {
        "surname": "Chen",
        "given_name": "Shixiang"
      },
      {
        "surname": "Sun",
        "given_name": "Jingwei"
      },
      {
        "surname": "Li",
        "given_name": "Jing"
      },
      {
        "surname": "Sun",
        "given_name": "Guangzhong"
      },
      {
        "surname": "Tao",
        "given_name": "Dacheng"
      }
    ]
  },
  {
    "title": "Beyond the individual: An improved telecom fraud detection approach based on latent synergy graph learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.019",
    "abstract": "The development of telecom technology not only facilitates social interactions but also inevitably provides the breeding ground for telecom fraud crimes. However, telecom fraud detection is a challenging task as fraudsters tend to commit co-fraud and disguise themselves within the mass of benign ones. Previous approaches work by unearthing differences in calling sequential patterns between independent fraudsters, but they may ignore synergic fraud patterns and oversimplify fraudulent behaviors. Fortunately, graph-like data formed by traceable telecom interaction provides opportunities for graph neural network (GNN)-based telecom fraud detection methods. Therefore, we develop a latent synergy graph (LSG) learning-based telecom fraud detector, named LSG-FD, to model both sequential and interactive fraudulent behaviors. Specifically, LSG-FD introduces (1) a multi-view LSG extractor to reconstruct synergy relationship-oriented graphs from the meta-interaction graph based on second-order proximity assumption; (2) an LSTM-based calling behavior encoder to capture the sequential patterns from the perspective of local individuals; (3) a dual-channel based graph learning module to alleviate the disassortativity issue (caused by the camouflages of fraudsters) by incorporating the dual-channel frequency filters and the learnable controller to adaptively aggregate high- and low-frequency information from their neighbors; (4) an imbalance-resistant model trainer to remedy the graph imbalance issue by developing a label-aware sampler. Experiment results on the telecom fraud dataset and another two widely used fraud datasets have verified the effectiveness of our model.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023005737",
    "keywords": [
      "Artificial intelligence",
      "Commit",
      "Computer science",
      "Computer security",
      "Data science",
      "Database",
      "Deception",
      "Graph",
      "Machine learning",
      "Psychology",
      "Social psychology",
      "Telecommunications",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Junhang"
      },
      {
        "surname": "Hu",
        "given_name": "Ruimin"
      },
      {
        "surname": "Li",
        "given_name": "Dengshi"
      },
      {
        "surname": "Ren",
        "given_name": "Lingfei"
      },
      {
        "surname": "Huang",
        "given_name": "Zijun"
      },
      {
        "surname": "Zang",
        "given_name": "Yilong"
      }
    ]
  },
  {
    "title": "A comparative study of GNN and MLP based machine learning for the diagnosis of Alzheimer’s Disease involving data synthesis",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.040",
    "abstract": "Alzheimer’s Disease (AD) is a neurodegenerative disease that commonly occurs in older people. It is characterized by both cognitive and functional impairment. However, as AD has an unclear pathological cause, it can be hard to diagnose with confidence. This is even more so in the early stage of Mild Cognitive Impairment (MCI). This paper proposes a U-Net based Generative Adversarial Network (GAN) to synthesize fluorodeoxyglucose - positron emission tomography (FDG-PET) from magnetic resonance imaging - T1 weighted imaging (MRI-T1WI) for further usage in AD diagnosis including its early-stage MCI. The experiments have displayed promising results with Structural Similarity Index Measure (SSIM) reaching 0.9714. Furthermore, three types of classifiers are developed, i.e., one Multi-Layer Perceptron (MLP) based classifier, two Graph Neural Network (GNN) based classifiers where one is for graph classification and the other is for node classification. 10-fold cross-validation has been conducted on all trials of experiments for classifier comparison. The performance of these three types of classifiers has been compared with the different input modalities setting and data fusion strategies. The results have shown that GNN based node classifier surpasses the other two types of classifiers, and has achieved the state-of-the-art (SOTA) performance with the best accuracy at 90.18% for 3-class classification, namely AD, MCI and normal control (NC) with the synthesized fluorodeoxyglucose - positron emission tomography (FDG-PET) features fused at the input level. Moreover, involving synthesized FDG-PET as part of the input with proper data fusion strategies has also proved to enhance all three types of classifiers’ performance. This work provides support for the notion that machine learning-derived image analysis may be a useful approach to improving the diagnosis of AD.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006020",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Classifier (UML)",
      "Cognitive impairment",
      "Computer science",
      "Disease",
      "Machine learning",
      "Magnetic resonance imaging",
      "Medicine",
      "Multilayer perceptron",
      "Nuclear medicine",
      "Pathology",
      "Pattern recognition (psychology)",
      "Perceptron",
      "Positron emission tomography",
      "Radiology"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Ke"
      },
      {
        "surname": "Weng",
        "given_name": "Ying"
      },
      {
        "surname": "Hosseini",
        "given_name": "Akram A."
      },
      {
        "surname": "Dening",
        "given_name": "Tom"
      },
      {
        "surname": "Zuo",
        "given_name": "Guokun"
      },
      {
        "surname": "Zhang",
        "given_name": "Yiming"
      }
    ]
  },
  {
    "title": "Elucidating multifinal and equifinal pathways to developmental disorders by constructing real-world neurorobotic models",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.005",
    "abstract": "Vigorous research has been conducted to accumulate biological and theoretical knowledge about neurodevelopmental disorders, including molecular, neural, computational, and behavioral characteristics; however, these findings remain fragmentary and do not elucidate integrated mechanisms. An obstacle is the heterogeneity of developmental pathways causing clinical phenotypes. Additionally, in symptom formations, the primary causes and consequences of developmental learning processes are often indistinguishable. Herein, we review developmental neurorobotic experiments tackling problems related to the dynamic and complex properties of neurodevelopmental disorders. Specifically, we focus on neurorobotic models under predictive processing lens for the study of developmental disorders. By constructing neurorobotic models with predictive processing mechanisms of learning, perception, and action, we can simulate formations of integrated causal relationships among neurodynamical, computational, and behavioral characteristics in the robot agents while considering developmental learning processes. This framework has the potential to bind neurobiological hypotheses (excitation-inhibition imbalance and functional disconnection), computational accounts (unusual encoding of uncertainty), and clinical symptoms. Developmental neurorobotic approaches may serve as a complementary research framework for integrating fragmented knowledge and overcoming the heterogeneity of neurodevelopmental disorders.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023005580",
    "keywords": [
      "Artificial intelligence",
      "Cognitive psychology",
      "Cognitive science",
      "Computational model",
      "Computer science",
      "Disconnection",
      "Epistemology",
      "Law",
      "Mechanism (biology)",
      "Neuroscience",
      "Perception",
      "Philosophy",
      "Political science",
      "Psychology"
    ],
    "authors": [
      {
        "surname": "Idei",
        "given_name": "Hayato"
      },
      {
        "surname": "Yamashita",
        "given_name": "Yuichi"
      }
    ]
  },
  {
    "title": "Revisiting multi-view learning: A perspective of implicitly heterogeneous Graph Convolutional Network",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.052",
    "abstract": "Graph Convolutional Network (GCN) has become a hotspot in graph-based machine learning due to its powerful graph processing capability. Most of the existing GCN-based approaches are designed for single-view data. In numerous practical scenarios, data is expressed through multiple views, rather than a single view. The ability of GCN to model homogeneous graphs is indisputable, while it is insufficient in facing the heterophily property of multi-view data. In this paper, we revisit multi-view learning to propose an implicit heterogeneous graph convolutional network that efficiently captures the heterogeneity of multi-view data while exploiting the powerful feature aggregation capability of GCN. We automatically assign optimal importance to each view when constructing the meta-path graph. High-order cross-view meta-paths are explored based on the obtained graph, and a series of graph matrices are generated. Combining graph matrices with learnable global feature representation to obtain heterogeneous graph embeddings at various levels. Finally, in order to effectively utilize both local and global information, we introduce a graph-level attention mechanism at the meta-path level that allocates private information to each node individually. Extensive experimental results convincingly support the superior performance of the proposed method compared to other state-of-the-art approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006184",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Feature learning",
      "Graph",
      "Graph database",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Zou",
        "given_name": "Ying"
      },
      {
        "surname": "Fang",
        "given_name": "Zihan"
      },
      {
        "surname": "Wu",
        "given_name": "Zhihao"
      },
      {
        "surname": "Zheng",
        "given_name": "Chenghui"
      },
      {
        "surname": "Wang",
        "given_name": "Shiping"
      }
    ]
  },
  {
    "title": "Temporal shuffling for defending deep action recognition models against adversarial attacks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.033",
    "abstract": "Recently, video-based action recognition methods using convolutional neural networks (CNNs) achieve remarkable recognition performance. However, there is still lack of understanding about the generalization mechanism of action recognition models. In this paper, we suggest that action recognition models rely on the motion information less than expected, and thus they are robust to randomization of frame orders. Furthermore, we find that motion monotonicity remaining after randomization also contributes to such robustness. Based on this observation, we develop a novel defense method using temporal shuffling of input videos against adversarial attacks for action recognition models. Another observation enabling our defense method is that adversarial perturbations on videos are sensitive to temporal destruction. To the best of our knowledge, this is the first attempt to design a defense method without additional training for 3D CNN-based video action recognition models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023005889",
    "keywords": [
      "Action (physics)",
      "Action recognition",
      "Adversarial system",
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Class (philosophy)",
      "Computer science",
      "Convolutional neural network",
      "Gene",
      "Generalization",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "Robustness (evolution)",
      "Shuffling"
    ],
    "authors": [
      {
        "surname": "Hwang",
        "given_name": "Jaehui"
      },
      {
        "surname": "Zhang",
        "given_name": "Huan"
      },
      {
        "surname": "Choi",
        "given_name": "Jun-Ho"
      },
      {
        "surname": "Hsieh",
        "given_name": "Cho-Jui"
      },
      {
        "surname": "Lee",
        "given_name": "Jong-Seok"
      }
    ]
  },
  {
    "title": "To understand double descent, we need to understand VC theory",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.014",
    "abstract": "We analyze generalization performance of over-parameterized learning methods for classification, under VC-theoretical framework. Recently, practitioners in Deep Learning discovered ‘double descent’ phenomenon, when large networks can fit perfectly available training data, and at the same time, achieve good generalization for future (test) data. The current consensus view is that VC-theoretical results cannot account for good generalization performance of Deep Learning networks. In contrast, this paper shows that double descent can be explained by VC-theoretical concepts, such as VC-dimension and Structural Risk Minimization. We also present empirical results showing that double descent generalization curves can be accurately modeled using classical VC-generalization bounds. Proposed VC-theoretical analysis enables better understanding of generalization curves for data sets with different statistical characteristics, such as low vs high-dimensional data and noisy data. In addition, we analyze generalization performance of transfer learning using pre-trained Deep Learning networks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023005658",
    "keywords": [
      "Aerospace engineering",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Deep learning",
      "Descent (aeronautics)",
      "Dimension (graph theory)",
      "Empirical risk minimization",
      "Engineering",
      "Generalization",
      "Gradient descent",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Parameterized complexity",
      "Pure mathematics",
      "Transfer of learning",
      "VC dimension"
    ],
    "authors": [
      {
        "surname": "Cherkassky",
        "given_name": "Vladimir"
      },
      {
        "surname": "Lee",
        "given_name": "Eng Hock"
      }
    ]
  },
  {
    "title": "Saturation function-based continuous control on fixed-time synchronization of competitive neural networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.008",
    "abstract": "Currently, through proposing discontinuous control strategies with the signum function and discussing separately short-term memory (STM) and long-term memory (LTM) of competitive artificial neural networks (ANNs), the fixed-time (FXT) synchronization of competitive ANNs has been explored. Note that the method of separate analysis usually leads to complicated theoretical derivation and synchronization conditions, and the signum function inevitably causes the chattering to reduce the performance of the control schemes. To try to solve these challenging problems, the FXT synchronization issue is concerned in this paper for competitive ANNs by establishing a theorem of FXT stability with switching type and developing continuous control schemes based on a kind of saturation functions. Firstly, different from the traditional method of studying separately STM and LTM of competitive ANNs, the models of STM and LTM are compressed into a high-dimensional system so as to reduce the complexity of theoretical analysis. Additionally, as an important theoretical preliminary, a FXT stability theorem with switching differential conditions is established and some high-precision estimates for the convergence time are explicitly presented by means of several special functions. To achieve FXT synchronization of the addressed competitive ANNs, a type of continuous pure power-law control scheme is developed via introducing the saturation function instead of the signum function, and some synchronization criteria are further derived by the established FXT stability theorem. These theoretical results are further illustrated lastly via a numerical example and are applied to image encryption.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023005622",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Channel (broadcasting)",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Evolutionary biology",
      "Function (biology)",
      "Machine learning",
      "Mathematics",
      "Stability (learning theory)",
      "Synchronization (alternating current)",
      "Telecommunications"
    ],
    "authors": [
      {
        "surname": "Zheng",
        "given_name": "Caicai"
      },
      {
        "surname": "Hu",
        "given_name": "Cheng"
      },
      {
        "surname": "Yu",
        "given_name": "Juan"
      },
      {
        "surname": "Wen",
        "given_name": "Shiping"
      }
    ]
  },
  {
    "title": "An event-triggered collaborative neurodynamic approach to distributed global optimization",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.022",
    "abstract": "In this paper, we propose an event-triggered collaborative neurodynamic approach to distributed global optimization in the presence of nonconvexity. We design a projection neural network group consisting of multiple projection neural networks coupled via a communication network. We prove the convergence of the projection neural network group to Karush–Kuhn–Tucker points of a given global optimization problem. To reduce communication bandwidth consumption, we adopt an event-triggered mechanism to liaise with other neural networks in the group with the Zeno behavior being precluded. We employ multiple projection neural network groups for scattered searches and re-initialize their states using a meta-heuristic rule in the collaborative neurodynamic optimization framework. In addition, we apply the collaborative neurodynamic approach for distributed optimal chiller loading in a heating, ventilation, and air conditioning system.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023005762",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Event (particle physics)",
      "Global optimization",
      "Heuristic",
      "Mathematical optimization",
      "Mathematics",
      "Optimization problem",
      "Physics",
      "Projection (relational algebra)",
      "Quantum mechanics"
    ],
    "authors": [
      {
        "surname": "Xia",
        "given_name": "Zicong"
      },
      {
        "surname": "Liu",
        "given_name": "Yang"
      },
      {
        "surname": "Wang",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Corrigendum to ‘An exact mapping from ReLU networks to spiking neural networks’ [Neural Networks Volume 168 (2023) Pages 74-88]",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.057",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006226",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Deep neural networks",
      "Physics",
      "Thermodynamics",
      "Volume (thermodynamics)"
    ],
    "authors": [
      {
        "surname": "Stanojevic",
        "given_name": "Ana"
      },
      {
        "surname": "Woźniak",
        "given_name": "Stanisław"
      },
      {
        "surname": "Bellec",
        "given_name": "Guillaume"
      },
      {
        "surname": "Cherubini",
        "given_name": "Giovanni"
      },
      {
        "surname": "Pantazi",
        "given_name": "Angeliki"
      },
      {
        "surname": "Gerstner",
        "given_name": "Wulfram"
      }
    ]
  },
  {
    "title": "Adaptive class augmented prototype network for few-shot relation extraction",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.025",
    "abstract": "Relation extraction is one of the most essential tasks of knowledge construction, but it depends on a large amount of annotated data corpus. Few-shot relation extraction is proposed as a new paradigm, which is designed to learn new relationships between entities with merely a small number of annotated instances, effectively mitigating the cost of large-scale annotation and long-tail problems. To generalize to novel classes not included in the training set, existing approaches mainly focus on tuning pre-trained language models with relation instructions and developing class prototypes based on metric learning to extract relations. However, the learned representations are extremely sensitive to discrepancies in intra-class and inter-class relationships and hard to adaptively classify the relations due to biased class features and spurious correlations, such as similar relation classes having closer inter-class prototype representation. In this paper, we introduce an adaptive class augmented prototype network with instance-level and representation-level augmented mechanisms to strengthen the representation space. Specifically, we design the adaptive class augmentation mechanism to expand the representation of classes in instance-level augmentation, and class augmented representation learning with Bernoulli perturbation context attention to enhance the representation of class features in representation-level augmentation and explore adaptive debiased contrastive learning to train the model. Experimental results have been demonstrated on FewRel and NYT-25 under various few-shot settings, and the proposed model has improved accuracy and generalization, especially for cross-domain and different hard tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023005798",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Data mining",
      "Economics",
      "Feature learning",
      "Generalization",
      "Information extraction",
      "Law",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Metric (unit)",
      "Operations management",
      "Political science",
      "Politics",
      "Relation (database)",
      "Relationship extraction",
      "Representation (politics)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Rongzhen"
      },
      {
        "surname": "Zhong",
        "given_name": "Jiang"
      },
      {
        "surname": "Hu",
        "given_name": "Wenyue"
      },
      {
        "surname": "Dai",
        "given_name": "Qizhu"
      },
      {
        "surname": "Wang",
        "given_name": "Chen"
      },
      {
        "surname": "Wang",
        "given_name": "Wenzhu"
      },
      {
        "surname": "Li",
        "given_name": "Xue"
      }
    ]
  },
  {
    "title": "Learning a robust foundation model against clean-label data poisoning attacks at downstream tasks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.034",
    "abstract": "In the transfer learning paradigm, models that are pre-trained on large datasets are used as the foundation models for various downstream tasks. However, this paradigm exposes downstream practitioners to data poisoning threats, as attackers can inject malicious samples into the re-training datasets to manipulate the behavior of models in downstream tasks. In this work, we propose a defense strategy that significantly reduces the success rate of various data poisoning attacks in downstream tasks. Our defense aims to pre-train a robust foundation model by reducing adversarial feature distance and increasing inter-class feature distance. Experiments demonstrate the excellent defense performance of the proposed strategy towards state-of-the-art clean-label poisoning attacks in the transfer learning scenario.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023005890",
    "keywords": [
      "Adversarial system",
      "Archaeology",
      "Artificial intelligence",
      "Computer science",
      "Computer security",
      "Downstream (manufacturing)",
      "Engineering",
      "Feature (linguistics)",
      "Foundation (evidence)",
      "History",
      "Linguistics",
      "Machine learning",
      "Operations management",
      "Philosophy",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Ting"
      },
      {
        "surname": "Yan",
        "given_name": "Hanshu"
      },
      {
        "surname": "Han",
        "given_name": "Bo"
      },
      {
        "surname": "Liu",
        "given_name": "Lei"
      },
      {
        "surname": "Zhang",
        "given_name": "Jingfeng"
      }
    ]
  },
  {
    "title": "VC dimensions of group convolutional neural networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.012",
    "abstract": "We study the generalization capacity of group convolutional neural networks. We identify precise estimates for the VC dimensions of simple sets of group convolutional neural networks. In particular, we find that for infinite groups and appropriately chosen convolutional kernels, already two-parameter families of convolutional neural networks have an infinite VC dimension, despite being invariant to the action of an infinite group.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802300566X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Dimension (graph theory)",
      "Epistemology",
      "Generalization",
      "Group (periodic table)",
      "Invariant (physics)",
      "Mathematical analysis",
      "Mathematical physics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Pure mathematics",
      "Quantum mechanics",
      "Simple (philosophy)"
    ],
    "authors": [
      {
        "surname": "Petersen",
        "given_name": "Philipp Christian"
      },
      {
        "surname": "Sepliarskaia",
        "given_name": "Anna"
      }
    ]
  },
  {
    "title": "A benchmarking protocol for SAR colorization: From regression to deep learning approaches",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.058",
    "abstract": "Synthetic aperture radar (SAR) images are widely used in remote sensing. Interpreting SAR images can be challenging due to their intrinsic speckle noise and grayscale nature. To address this issue, SAR colorization has emerged as a research direction to colorize gray scale SAR images while preserving the original spatial information and radiometric information. However, this research field is still in its early stages, and many limitations can be highlighted. In this paper, we propose a full research line for supervised learning-based approaches to SAR colorization. Our approach includes a protocol for generating synthetic color SAR images, several baselines, and an effective method based on the conditional generative adversarial network (cGAN) for SAR colorization. We also propose numerical assessment metrics for the problem at hand. To our knowledge, this is the first attempt to propose a research line for SAR colorization that includes a protocol, a benchmark, and a complete performance evaluation. Our extensive tests demonstrate the effectiveness of our proposed cGAN-based network for SAR colorization. The code is available at https://github.com/shenkqtx/SAR-Colorization-Benchmarking-Protocol.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006238",
    "keywords": [
      "Alternative medicine",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Benchmarking",
      "Business",
      "Code (set theory)",
      "Computer science",
      "Computer vision",
      "Deep learning",
      "Geodesy",
      "Geology",
      "Grayscale",
      "Machine learning",
      "Marketing",
      "Medicine",
      "Pathology",
      "Pattern recognition (psychology)",
      "Pixel",
      "Programming language",
      "Protocol (science)",
      "Set (abstract data type)",
      "Speckle noise",
      "Speckle pattern",
      "Synthetic aperture radar"
    ],
    "authors": [
      {
        "surname": "Shen",
        "given_name": "Kangqing"
      },
      {
        "surname": "Vivone",
        "given_name": "Gemine"
      },
      {
        "surname": "Yang",
        "given_name": "Xiaoyuan"
      },
      {
        "surname": "Lolli",
        "given_name": "Simone"
      },
      {
        "surname": "Schmitt",
        "given_name": "Michael"
      }
    ]
  },
  {
    "title": "Non-fragile output-feedback control for time-delay neural networks with persistent dwell time switching: A system mode and time scheduler dual-dependent design",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.007",
    "abstract": "This paper is concerned with non-fragile output-feedback control for time-delay neural networks with persistent dwell time (PDT) switching in a continuous-time setting. The main purpose is to design an output-feedback controller subject to gain fluctuations, guaranteeing both asymptotic stability and L 2 -gain of the closed-loop control system. To achieve reduced conservatism, the controller is formulated to depend not only on the system mode but also on a time scheduler constructed based on the PDT switching rule and minimum time span. A criterion for the asymptotic stability and L 2 -gain analysis is established through the application of the Gronwall–Bellman inequality and mathematical induction. Then, a numerically tractable design approach for the desired controller is proposed, utilizing a four-section piecewise time-dependent Lyapunov–Krasovskii functional and several nonlinearity decoupling techniques. For comparative purposes, a simple case, independent of the time scheduler, is also investigated, and the corresponding controller design approach is presented. Finally, a simulation example is given to illustrate the effectiveness and superiority of the proposed system mode and time scheduler dual-dependent controller design approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006305",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Artificial neural network",
      "Clinical psychology",
      "Computer science",
      "Control (management)",
      "Control engineering",
      "Control theory (sociology)",
      "Dual (grammatical number)",
      "Dwell time",
      "Engineering",
      "Feedback control",
      "Literature",
      "Psychology"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Jianping"
      },
      {
        "surname": "Ma",
        "given_name": "Xiaofeng"
      },
      {
        "surname": "Yan",
        "given_name": "Zhilian"
      },
      {
        "surname": "Arik",
        "given_name": "Sabri"
      }
    ]
  },
  {
    "title": "Comprehensive mining of information in Weakly Supervised Semantic Segmentation: Saliency semantics and edge semantics",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.009",
    "abstract": "In the studies of Weakly Supervised Semantic Segmentation (WSSS) with image-level labels, there is an issue of incomplete semantic information, which we summarize as insufficient saliency semantic mining and neglected edge semantics. We proposes a two-stage framework, Saliency Semantic Full Mining-Edge Semantic Mining (SSFM-ESM), which views WSSS from the perspective of comprehensive information mining. In the first stage, we rely on SSFM to address the insufficient saliency semantic mining. The network learns feature representations consistent with salient regions via the proposed pixel-level class-agnostic distance loss. Then, the full saliency semantic information is mined by explicitly receiving pixel-level feedback. The initial pseudo-label with complete saliency semantic information can be obtained after the first stage. In the second stage, we focus on the mining of edge semantic information through the proposed edge semantic mining module. Specifically, we guide the initial pseudo-label avoid learning about false semantic information and obtain high-confidence edge semantics. The self-correction ability of the segmentation network is also fully utilized to obtain more edge semantic information. Extensive experiments are conducted on the PASCAL VOC 2012 and MS COCO 2014 datasets to verify the feasibility and superiority of this approach.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023005634",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Enhanced Data Rates for GSM Evolution",
      "Feature (linguistics)",
      "Information retrieval",
      "Linguistics",
      "Natural language processing",
      "Pascal (unit)",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Segmentation",
      "Semantic Web",
      "Semantic compression",
      "Semantic computing",
      "Semantic similarity",
      "Semantic technology",
      "Semantics (computer science)"
    ],
    "authors": [
      {
        "surname": "Wang",
        "given_name": "Shaohui"
      },
      {
        "surname": "Shao",
        "given_name": "Youjia"
      },
      {
        "surname": "Tian",
        "given_name": "Na"
      },
      {
        "surname": "Zhao",
        "given_name": "Wencang"
      }
    ]
  },
  {
    "title": "A framework for controllable Pareto front learning with completed scalarization functions and its applications",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.029",
    "abstract": "Pareto Front Learning (PFL) was recently introduced as an efficient method for approximating the entire Pareto front, the set of all optimal solutions to a Multi-Objective Optimization (MOO) problem. In the previous work, the mapping between a preference vector and a Pareto optimal solution is still ambiguous, rendering its results. This study demonstrates the convergence and completion aspects of solving MOO with pseudoconvex scalarization functions and combines them into Hypernetwork in order to offer a comprehensive framework for PFL, called Controllable Pareto Front Learning. Extensive experiments demonstrate that our approach is highly accurate and significantly less computationally expensive than prior methods in term of inference time.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802300583X",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Inference",
      "Mathematical optimization",
      "Mathematics",
      "Multi-objective optimization",
      "Pareto optimal",
      "Pareto principle",
      "Programming language",
      "Rendering (computer graphics)",
      "Set (abstract data type)"
    ],
    "authors": [
      {
        "surname": "Tuan",
        "given_name": "Tran Anh"
      },
      {
        "surname": "Hoang",
        "given_name": "Long P."
      },
      {
        "surname": "Le",
        "given_name": "Dung D."
      },
      {
        "surname": "Thang",
        "given_name": "Tran Ngoc"
      }
    ]
  },
  {
    "title": "Improving few-shot relation extraction through semantics-guided learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.053",
    "abstract": "Few-shot relation extraction (few-shot RE) aims to recognize relations between the entity pair in a given text by utilizing very few annotated instances. As a simple yet efficient approach, prototype network-based methods often directly incorporate relation information to enhance prototype representation or leverage contrastive learning to mitigate prediction confusion. Despite achieving good results, the above methods are still susceptible to false judgments of outlier samples and confusion of similar classes. To address these issues, we propose a novel Semantics-Guided Learning (SemGL) method that more effectively utilizes relation information to enhance both the representations of instances and prototypes for improving the performance of few-shot RE. First, SemGL employs the prompt encoder to encode various prompt templates of instances and relation information and obtains more accurate semantic representations of instances, instance prototypes, and concept prototypes via the prompt enhancement from large language models. Then, SemGL introduces a novel technique called relation graph learning, which leverages concept prototypes to cluster homogeneous instances together, emphasizing relation-specific features of concrete instances. Simultaneously, SemGL employs instance-level contrastive learning between instance prototypes and support instances to distinguish between intra-class instances and inter-class instances to promote shared features among intra-class instances. Additionally, prototype-level contrastive learning leverages concept prototypes to pull closer relation-specific features of the concept prototype and shared features of the instance prototype from the same relation. Finally, SemGL utilizes new relation prototypes that integrate interpretable features of concept prototypes and shared features of instance prototypes for prediction. Experimental results on two publicly available few-shot RE datasets demonstrate the effectiveness and efficiency of SemGL in introducing relation information, with particularly promising results for the domain adaptation challenge task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006196",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Data mining",
      "Encoder",
      "Law",
      "Leverage (statistics)",
      "Machine learning",
      "Natural language processing",
      "Operating system",
      "Political science",
      "Politics",
      "Programming language",
      "Relation (database)",
      "Relationship extraction",
      "Representation (politics)",
      "Semantics (computer science)"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Hui"
      },
      {
        "surname": "He",
        "given_name": "Yuting"
      },
      {
        "surname": "Chen",
        "given_name": "Yidong"
      },
      {
        "surname": "Bai",
        "given_name": "Yu"
      },
      {
        "surname": "Shi",
        "given_name": "Xiaodong"
      }
    ]
  },
  {
    "title": "Reduced-complexity Convolutional Neural Network in the compressed domain",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.020",
    "abstract": "Deep neural networks have achieved outstanding performance in computer vision tasks. Convolutional Neural Networks (CNNs) typically operate in the spatial domain with raw images, but in practice, images are usually stored and transmitted in their compressed representation where JPEG is one of the most widely used encoder. Also, these networks are computationally intensive and slow. This paper proposes performing the learning and inference processes in the compressed domain in order to reduce the computational complexity and improve the speed of popular CNNs. For this purpose, a novel graph-based frequency channel selection method is proposed to identify and select the most important frequency channels. The computational complexity is reduced by retaining the important frequency components and discarding the insignificant ones as well as eliminating the unnecessary layers of the network. Experimental results show that the modified ResNet-50 operating in the compressed domain is up to 70% faster than the spatial-based traditional ResNet-50 while resulting in similar classification accuracy. Moreover, this paper proposes a preprocessing step with partial encoding to improve the resilience to distortions caused by low-quality encoded images. Finally, we show that training a network with highly compressed data can achieve a good classification accuracy with up to 93% reduction in the storage requirements of the training data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023005749",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computational complexity theory",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Data compression",
      "Data mining",
      "Deep learning",
      "Encoding (memory)",
      "Frequency domain",
      "Geometry",
      "Inference",
      "JPEG",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Preprocessor",
      "Reduction (mathematics)"
    ],
    "authors": [
      {
        "surname": "Abdellatef",
        "given_name": "Hamdan"
      },
      {
        "surname": "Karam",
        "given_name": "Lina J."
      }
    ]
  },
  {
    "title": "Reduced-complexity Convolutional Neural Network in the compressed domain",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.020",
    "abstract": "Deep neural networks have achieved outstanding performance in computer vision tasks. Convolutional Neural Networks (CNNs) typically operate in the spatial domain with raw images, but in practice, images are usually stored and transmitted in their compressed representation where JPEG is one of the most widely used encoder. Also, these networks are computationally intensive and slow. This paper proposes performing the learning and inference processes in the compressed domain in order to reduce the computational complexity and improve the speed of popular CNNs. For this purpose, a novel graph-based frequency channel selection method is proposed to identify and select the most important frequency channels. The computational complexity is reduced by retaining the important frequency components and discarding the insignificant ones as well as eliminating the unnecessary layers of the network. Experimental results show that the modified ResNet-50 operating in the compressed domain is up to 70% faster than the spatial-based traditional ResNet-50 while resulting in similar classification accuracy. Moreover, this paper proposes a preprocessing step with partial encoding to improve the resilience to distortions caused by low-quality encoded images. Finally, we show that training a network with highly compressed data can achieve a good classification accuracy with up to 93% reduction in the storage requirements of the training data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023005749",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computational complexity theory",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Data compression",
      "Data mining",
      "Deep learning",
      "Encoding (memory)",
      "Frequency domain",
      "Geometry",
      "Inference",
      "JPEG",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Preprocessor",
      "Reduction (mathematics)"
    ],
    "authors": [
      {
        "surname": "Abdellatef",
        "given_name": "Hamdan"
      },
      {
        "surname": "Karam",
        "given_name": "Lina J."
      }
    ]
  },
  {
    "title": "Contextually enhanced ES-dRNN with dynamic attention for short-term load forecasting",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.017",
    "abstract": "In this paper, we propose a new short-term load forecasting (STLF) model based on contextually enhanced hybrid and hierarchical architecture combining exponential smoothing (ES) and a recurrent neural network (RNN). The model is composed of two simultaneously trained tracks: the context track and the main track. The context track introduces additional information to the main track. It is extracted from representative series and dynamically modulated to adjust to the individual series forecasted by the main track. The RNN architecture consists of multiple recurrent layers stacked with hierarchical dilations and equipped with recently proposed attentive dilated recurrent cells. These cells enable the model to capture short-term, long-term and seasonal dependencies across time series as well as to weight dynamically the input information. The model produces both point forecasts and predictive intervals. The experimental part of the work performed on 35 forecasting problems shows that the proposed model outperforms in terms of accuracy its predecessor as well as standard statistical models and state-of-the-art machine learning models.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006408",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Deep learning",
      "Exponential smoothing",
      "Geometry",
      "Machine learning",
      "Mathematics",
      "Operating system",
      "Paleontology",
      "Physics",
      "Point (geometry)",
      "Quantum mechanics",
      "Recurrent neural network",
      "Series (stratigraphy)",
      "Smoothing",
      "Term (time)",
      "Track (disk drive)"
    ],
    "authors": [
      {
        "surname": "Smyl",
        "given_name": "Slawek"
      },
      {
        "surname": "Dudek",
        "given_name": "Grzegorz"
      },
      {
        "surname": "Pełka",
        "given_name": "Paweł"
      }
    ]
  },
  {
    "title": "Improving few-shot relation extraction through semantics-guided learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.053",
    "abstract": "Few-shot relation extraction (few-shot RE) aims to recognize relations between the entity pair in a given text by utilizing very few annotated instances. As a simple yet efficient approach, prototype network-based methods often directly incorporate relation information to enhance prototype representation or leverage contrastive learning to mitigate prediction confusion. Despite achieving good results, the above methods are still susceptible to false judgments of outlier samples and confusion of similar classes. To address these issues, we propose a novel Semantics-Guided Learning (SemGL) method that more effectively utilizes relation information to enhance both the representations of instances and prototypes for improving the performance of few-shot RE. First, SemGL employs the prompt encoder to encode various prompt templates of instances and relation information and obtains more accurate semantic representations of instances, instance prototypes, and concept prototypes via the prompt enhancement from large language models. Then, SemGL introduces a novel technique called relation graph learning, which leverages concept prototypes to cluster homogeneous instances together, emphasizing relation-specific features of concrete instances. Simultaneously, SemGL employs instance-level contrastive learning between instance prototypes and support instances to distinguish between intra-class instances and inter-class instances to promote shared features among intra-class instances. Additionally, prototype-level contrastive learning leverages concept prototypes to pull closer relation-specific features of the concept prototype and shared features of the instance prototype from the same relation. Finally, SemGL utilizes new relation prototypes that integrate interpretable features of concept prototypes and shared features of instance prototypes for prediction. Experimental results on two publicly available few-shot RE datasets demonstrate the effectiveness and efficiency of SemGL in introducing relation information, with particularly promising results for the domain adaptation challenge task.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006196",
    "keywords": [
      "Artificial intelligence",
      "Class (philosophy)",
      "Computer science",
      "Data mining",
      "Encoder",
      "Law",
      "Leverage (statistics)",
      "Machine learning",
      "Natural language processing",
      "Operating system",
      "Political science",
      "Politics",
      "Programming language",
      "Relation (database)",
      "Relationship extraction",
      "Representation (politics)",
      "Semantics (computer science)"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Hui"
      },
      {
        "surname": "He",
        "given_name": "Yuting"
      },
      {
        "surname": "Chen",
        "given_name": "Yidong"
      },
      {
        "surname": "Bai",
        "given_name": "Yu"
      },
      {
        "surname": "Shi",
        "given_name": "Xiaodong"
      }
    ]
  },
  {
    "title": "Enhancement, integration, expansion: Activating representation of detailed features for occluded person re-identification",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.003",
    "abstract": "A proposed method, Enhancement, integration, and Expansion, aims to activate the representation of detailed features for occluded person re-identification. Region and context are two important and complementary features, and integrating them in an occluded environment can effectively improve the robustness of the model. Firstly, a self-enhancement module is designed. Based on the constructed multi-stream architecture, rich and meaningful feature interference is introduced in the feature extraction stage to enhance the model’s ability to perceive noise. Next, a collaborative integration module similar to cascading cross-attention is proposed. By studying the intrinsic interaction patterns of regional and contextual features, it adaptively fuses features across streams and enhances the diverse and complete representation of internal information. The module is not only robust to complex occlusions, but also mitigates the feature interference problem due to similar appearances or scenes. Finally, a matching expansion module that enhances feature discriminability and completeness is proposed. Providing more stable and accurate features for recognition. Compared with state-of-the-art methods on two occluded and holistic datasets, the proposed method is proved to be advanced and the effectiveness of the module is proved by extensive ablation studies.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006263",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Biology",
      "Chemistry",
      "Completeness (order theory)",
      "Computer science",
      "Context (archaeology)",
      "Feature (linguistics)",
      "Feature extraction",
      "Gene",
      "Image (mathematics)",
      "Law",
      "Linguistics",
      "Matching (statistics)",
      "Mathematical analysis",
      "Mathematics",
      "Noise (video)",
      "Paleontology",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Representation (politics)",
      "Robustness (evolution)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Ning",
        "given_name": "Enhao"
      },
      {
        "surname": "Wang",
        "given_name": "Yangfan"
      },
      {
        "surname": "Wang",
        "given_name": "Changshuo"
      },
      {
        "surname": "Zhang",
        "given_name": "Huang"
      },
      {
        "surname": "Ning",
        "given_name": "Xin"
      }
    ]
  },
  {
    "title": "Two-timescale projection neural networks in collaborative neurodynamic approaches to global optimization and distributed optimization",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.011",
    "abstract": "In this paper, we propose a two-timescale projection neural network (PNN) for solving optimization problems with nonconvex functions. We prove the convergence of the PNN with sufficiently different timescales to a local optimal solution. We develop a collaborative neurodynamic approach with multiple such PNNs to search for global optimal solutions. In addition, we develop a collaborative neurodynamic approach with multiple PNNs connected via a directed graph for distributed global optimization. We elaborate on four numerical examples to illustrate the characteristics of the approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023005671",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convergence (economics)",
      "Economic growth",
      "Economics",
      "Global optimization",
      "Graph",
      "Mathematical optimization",
      "Mathematics",
      "Optimization problem",
      "Projection (relational algebra)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Huang",
        "given_name": "Banghua"
      },
      {
        "surname": "Liu",
        "given_name": "Yang"
      },
      {
        "surname": "Jiang",
        "given_name": "Yun-Liang"
      },
      {
        "surname": "Wang",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Reduced-complexity Convolutional Neural Network in the compressed domain",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.020",
    "abstract": "Deep neural networks have achieved outstanding performance in computer vision tasks. Convolutional Neural Networks (CNNs) typically operate in the spatial domain with raw images, but in practice, images are usually stored and transmitted in their compressed representation where JPEG is one of the most widely used encoder. Also, these networks are computationally intensive and slow. This paper proposes performing the learning and inference processes in the compressed domain in order to reduce the computational complexity and improve the speed of popular CNNs. For this purpose, a novel graph-based frequency channel selection method is proposed to identify and select the most important frequency channels. The computational complexity is reduced by retaining the important frequency components and discarding the insignificant ones as well as eliminating the unnecessary layers of the network. Experimental results show that the modified ResNet-50 operating in the compressed domain is up to 70% faster than the spatial-based traditional ResNet-50 while resulting in similar classification accuracy. Moreover, this paper proposes a preprocessing step with partial encoding to improve the resilience to distortions caused by low-quality encoded images. Finally, we show that training a network with highly compressed data can achieve a good classification accuracy with up to 93% reduction in the storage requirements of the training data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023005749",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Computational complexity theory",
      "Computer science",
      "Computer vision",
      "Convolutional neural network",
      "Data compression",
      "Data mining",
      "Deep learning",
      "Encoding (memory)",
      "Frequency domain",
      "Geometry",
      "Inference",
      "JPEG",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Preprocessor",
      "Reduction (mathematics)"
    ],
    "authors": [
      {
        "surname": "Abdellatef",
        "given_name": "Hamdan"
      },
      {
        "surname": "Karam",
        "given_name": "Lina J."
      }
    ]
  },
  {
    "title": "Asymmetric double networks mutual teaching for unsupervised person Re-identification",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.001",
    "abstract": "Unsupervised person re-identification (Re-ID) has always been challenging in computer vision. It has received much attention from researchers because it does not require any labeled information and can be freely deployed to new scenarios. Most unsupervised person Re-ID research studies produce and optimize pseudo-labels by iterative clustering algorithms on a single network. However, these methods are easily affected by noisy labels and feature variations caused by camera shifts, which will limit the optimization of pseudo-labels. In this paper, we propose an Asymmetric Double Networks Mutual Teaching (ADNMT) architecture that uses two asymmetric networks to generate pseudo-labels for each other by clustering, and the pseudo-labels are updated and optimized by alternate training. Specifically, ADNMT contains two asymmetric networks. One network is a multiple granularity network, which extracts pedestrian features of multiple granularity that correspond to numerous classifiers, and the other network is a conventional backbone network, which extracts pedestrian features that correspond to a classifier. Furthermore, because the camera style changes seriously affect the generalization ability of the proposed model, this paper designs Similarity Compensation of Inter-Camera (SCIC) and Similarity Suppression of Intra-Camera (SSIC) according to the camera ID of the pedestrian images to optimize the similarity measure. Extensive experiments on multiple Re-ID benchmark datasets show that our proposed method achieves superior performance compared with the state‐of‐the‐art unsupervised person re-identification methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802300624X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Benchmark (surveying)",
      "Biology",
      "Botany",
      "Classifier (UML)",
      "Cluster analysis",
      "Computer science",
      "Generalization",
      "Geodesy",
      "Geography",
      "Granularity",
      "Identification (biology)",
      "Image (mathematics)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Similarity (geometry)",
      "Similarity measure",
      "Unsupervised learning"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Miaohui"
      },
      {
        "surname": "Li",
        "given_name": "Kaifang"
      },
      {
        "surname": "Ma",
        "given_name": "Jianxin"
      },
      {
        "surname": "Wang",
        "given_name": "Xile"
      }
    ]
  },
  {
    "title": "Automatic selection of spoken language biomarkers for dementia detection",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.018",
    "abstract": "This paper analyzes diverse features extracted from spoken language to select the most discriminative ones for dementia detection. We present a two-step feature selection (FS) approach: Step 1 utilizes filter methods to pre-screen features, and Step 2 uses a novel feature ranking (FR) method, referred to as dual dropout ranking (DDR), to rank the screened features and select spoken language biomarkers. The proposed DDR is based on a dual-net architecture that separates FS and dementia detection into two neural networks (namely, the operator and selector). The operator is trained on features obtained from the selector to reduce classification or regression loss. The selector is optimized to predict the operator’s performance based on automatic regularization. Results show that the approach significantly reduces feature dimensionality while identifying small feature subsets that achieve comparable or superior performance compared with the full, default feature set. The Python codes are available at https://github.com/kexquan/dual-dropout-ranking.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023005725",
    "keywords": [
      "Artificial intelligence",
      "Classifier (UML)",
      "Computer science",
      "Discriminative model",
      "Dropout (neural networks)",
      "Feature (linguistics)",
      "Feature extraction",
      "Feature selection",
      "Linguistics",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Ranking (information retrieval)",
      "Regularization (linguistics)",
      "Spoken language"
    ],
    "authors": [
      {
        "surname": "Ke",
        "given_name": "Xiaoquan"
      },
      {
        "surname": "Mak",
        "given_name": "Man Wai"
      },
      {
        "surname": "Meng",
        "given_name": "Helen M."
      }
    ]
  },
  {
    "title": "Deterministic learning-based neural identification and knowledge fusion",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.004",
    "abstract": "Recent deterministic learning methods have achieved locally-accurate identification of unknown system dynamics. However, the locally-accurate identification means that the neural networks can only capture the local dynamics knowledge along the system trajectory. In order to capture a broader knowledge region, this article investigates the knowledge fusion problem of deterministic learning, that is, the integration of different knowledge regions along different individual trajectories. Specifically, two kinds of knowledge fusion schemes are systematically introduced: an online fusion scheme and an offline fusion scheme. The online scheme can be viewed as an extension of distributed cooperative learning control to cooperative neural identification for sampled-data systems. By designing an auxiliary information transmission strategy to enable the neural network to receive information learned from other tasks while learning its own task, it is proven that the weights of all localized RBF networks exponentially converge to their common true/ideal values. The offline scheme can be regarded as a knowledge distillation strategy, in which the fused network is obtained by offline training through the knowledge learned from all individual system trajectories via deterministic learning. A novel weight fusion algorithm with low computational complexity is proposed based on the least squares solution under subspace constraints. Simulation studies show that the proposed fusion schemes can successfully integrate the knowledge regions of different individual trajectories while maintaining the learning performance, thereby greatly expanding the knowledge region learned from deterministic learning.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023005567",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biology",
      "Botany",
      "Computer science",
      "Identification (biology)",
      "Machine learning",
      "Mathematical analysis",
      "Mathematics",
      "Offline learning",
      "Online learning",
      "Scheme (mathematics)",
      "Subspace topology",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Weiming"
      },
      {
        "surname": "Hu",
        "given_name": "Jingtao"
      },
      {
        "surname": "Zhu",
        "given_name": "Zejian"
      },
      {
        "surname": "Zhang",
        "given_name": "Fukai"
      },
      {
        "surname": "Xu",
        "given_name": "Juanjuan"
      },
      {
        "surname": "Wang",
        "given_name": "Cong"
      }
    ]
  },
  {
    "title": "Filter pruning for convolutional neural networks in semantic image segmentation",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.010",
    "abstract": "The remarkable performance of Convolutional Neural Networks (CNNs) has increased their use in real-time systems and devices with limited resources. Hence, compacting these networks while preserving accuracy has become necessary, leading to multiple compression methods. However, the majority require intensive iterative procedures and do not delve into the influence of the used data. To overcome these issues, this paper presents several contributions, framed in the context of explainable Artificial Intelligence (xAI): (a) two filter pruning methods for CNNs, which remove the less significant convolutional kernels; (b) a fine-tuning strategy to recover generalization; (c) a layer pruning approach for U-Net; and (d) an explanation of the relationship between performance and the used data. Filter and feature maps information are used in the pruning process: Principal Component Analysis (PCA) is combined with a next-convolution influence-metric, while the latter and the mean standard deviation are used in an importance score distribution-based method. The developed strategies are generic, and therefore applicable to different models. Experiments demonstrating their effectiveness are conducted over distinct CNNs and datasets, focusing mainly on semantic segmentation (using U-Net, DeepLabv3+, SegNet, and VGG-16 as highly representative models). Pruned U-Net on agricultural benchmarks achieves 98.7% parameters and 97.5% FLOPs drop, with a 0.35% gain in accuracy. DeepLabv3+ and SegNet on CamVid reach 46.5% and 72.4% parameters reduction and a 51.9% and 83.6% FLOPs drop respectively, with almost no decrease in accuracy. VGG-16 on CIFAR-10 obtains up to 86.5% parameter and 82.2% FLOPs decrease with a 0.78% accuracy gain.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006330",
    "keywords": [
      "Agronomy",
      "Artificial intelligence",
      "Biology",
      "Computer science",
      "Computer vision",
      "Context (archaeology)",
      "Convolutional neural network",
      "FLOPS",
      "Feature (linguistics)",
      "Filter (signal processing)",
      "Linguistics",
      "Paleontology",
      "Parallel computing",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Pruning",
      "Segmentation"
    ],
    "authors": [
      {
        "surname": "López-González",
        "given_name": "Clara I."
      },
      {
        "surname": "Gascó",
        "given_name": "Esther"
      },
      {
        "surname": "Barrientos-Espillco",
        "given_name": "Fredy"
      },
      {
        "surname": "Besada-Portas",
        "given_name": "Eva"
      },
      {
        "surname": "Pajares",
        "given_name": "Gonzalo"
      }
    ]
  },
  {
    "title": "An efficient multi-scale learning method for image super-resolution networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.015",
    "abstract": "The image super-resolution (SR) operation holds multiple solutions with the one-to-many mapping from low-resolution (LR) to high-resolution (HR) space. However, the SR of different scales for the same image is usually regarded as independent tasks in the existing SR networks. Therefore, these networks are inflexible to effectively utilize feature learning experience and require much more computing time to recover HR images in higher resolutions. Recent arbitrary scale SR methods still cannot solve these problems. To efficiently and effectively recover HR images, this paper presents an efficient multi-scale learning method for image SR networks based on a novel self-generating (SG) mechanism. This method (briefly named SG-SR) utilizes the feature learning results of SR networks to generate upscale filters by using the novel SG upscale module, which is proposed to replace the traditional upscale module. For each scale factor, the SG upscale module provides the corresponding amount of the spatial weights to filter the LR tensor and then converts filtered tensors with the original tensor to corresponding HR images. The proposed method is evaluated through extensive experiments and compared with state-of-the-art (SOTA) methods on widely used benchmark datasets. The experimental results show that our method has superior performance compared with SOTA methods, and the SG upscale module can improve the performance of existing SR networks effectively. What is more, our module has a much less calculation cost than the other upscale modules.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023005695",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)",
      "Superresolution"
    ],
    "authors": [
      {
        "surname": "Ying",
        "given_name": "Wenyuan"
      },
      {
        "surname": "Dong",
        "given_name": "Tianyang"
      },
      {
        "surname": "Fan",
        "given_name": "Jing"
      }
    ]
  },
  {
    "title": "Robust underwater image enhancement with cascaded multi-level sub-networks and triple attention mechanism",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.008",
    "abstract": "With the growing exploration of marine resources, underwater image enhancement has gained significant attention. Recent advances in convolutional neural networks (CNN) have greatly impacted underwater image enhancement techniques. However, conventional CNN-based methods typically employ a single network structure, which may compromise robustness in challenging conditions. Additionally, commonly used UNet networks generally force fusion from low to high resolution for each layer, leading to inaccurate contextual information encoding. To address these issues, we propose a novel network called Cascaded Network with Multi-level Sub-networks (CNMS), which encompasses the following key components: (a) a cascade mechanism based on local modules and global networks for extracting feature representations with richer semantics and enhanced spatial precision, (b) information exchange between different resolution streams, and (c) a triple attention module for extracting attention-based features. CNMS selectively cascades multiple sub-networks through triple attention modules to extract distinct features from underwater images, bolstering the network’s robustness and improving generalization capabilities. Within the sub-network, we introduce a Multi-level Sub-network (MSN) that spans multiple resolution streams, combining contextual information from various scales while preserving the original underwater images’ high-resolution spatial details. Comprehensive experiments on multiple underwater datasets demonstrate that CNMS outperforms state-of-the-art methods in image enhancement tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006317",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Gene",
      "Geology",
      "Oceanography",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Dehuan"
      },
      {
        "surname": "Wu",
        "given_name": "Chenyu"
      },
      {
        "surname": "Zhou",
        "given_name": "Jingchun"
      },
      {
        "surname": "Zhang",
        "given_name": "Weishi"
      },
      {
        "surname": "Lin",
        "given_name": "Zifan"
      },
      {
        "surname": "Polat",
        "given_name": "Kemal"
      },
      {
        "surname": "Alenezi",
        "given_name": "Fayadh"
      }
    ]
  },
  {
    "title": "Event-triggered impulsive quasi-synchronization for BAM neural networks with reliable redundant channel",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.045",
    "abstract": "This work addresses the quasi-synchronization of delay master–slave BAM neural networks. To improve the utilization of channel bandwidth, a dynamic event-triggered impulsive mechanism is employed, in which data is transmitted only when a preset event-triggered mechanism or a forced impulse interval is satisfied. In addition, to guarantee the reliability of information transmission, a reliable redundant channel for BAM neural networks is adopted, whose transmission scheduling strategy is designed on the basis of the packet dropouts rate of the main communication channels. Further, an algorithm is employed to reduce the quasi-synchronization range of the error systems and the controllers are obtained. At last, a simulation result is shown to illustrate the effectiveness of the presented strategy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802300607X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Data transmission",
      "Impulse (physics)",
      "Network packet",
      "Physics",
      "Quantum mechanics",
      "Real-time computing",
      "Synchronization (alternating current)",
      "Telecommunications",
      "Transmission (telecommunications)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Yumei"
      },
      {
        "surname": "Lv",
        "given_name": "Weijun"
      },
      {
        "surname": "Tao",
        "given_name": "Jie"
      },
      {
        "surname": "Xu",
        "given_name": "Yong"
      },
      {
        "surname": "Huang",
        "given_name": "Tingwen"
      },
      {
        "surname": "Rutkowski",
        "given_name": "Leszek"
      }
    ]
  },
  {
    "title": "Grasping detection of dual manipulators based on Markov decision process with neural network",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.09.016",
    "abstract": "With the development of artificial intelligence, robots are widely used in various fields, grasping detection has been the focus of intelligent robot research. A dual manipulator grasping detection model based on Markov decision process is proposed to realize the stable grasping with complex multiple objects in this paper. Based on the principle of Markov decision process, the cross entropy convolutional neural network and full convolutional neural network are used to parameterize the grasping detection model of dual manipulators which are two-finger manipulator and vacuum sucker manipulator for multi-objective unknown objects. The data set generated in the simulated environment is used to train the two grasping detection networks. By comparing the grasping quality of the detection network output the best grasping by the two grasping methods, the network with better detection effect corresponding to the two grasping methods of two-finger and vacuum sucker is determined, and the dual manipulator grasping detection model is constructed in this paper. Robot grasping experiments are carried out, and the experimental results show that the proposed dual manipulator grasping detection method achieves 90.6% success rate, which is much higher than the other groups of experiments. The feasibility and superiority of the dual manipulator grasping detection method based on Markov decision process are verified.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023005075",
    "keywords": [
      "Art",
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Convolutional neural network",
      "Dual (grammatical number)",
      "Literature",
      "Operating system",
      "Process (computing)",
      "Robot"
    ],
    "authors": [
      {
        "surname": "Yun",
        "given_name": "Juntong"
      },
      {
        "surname": "Jiang",
        "given_name": "Du"
      },
      {
        "surname": "Huang",
        "given_name": "Li"
      },
      {
        "surname": "Tao",
        "given_name": "Bo"
      },
      {
        "surname": "Liao",
        "given_name": "Shangchun"
      },
      {
        "surname": "Liu",
        "given_name": "Ying"
      },
      {
        "surname": "Liu",
        "given_name": "Xin"
      },
      {
        "surname": "Li",
        "given_name": "Gongfa"
      },
      {
        "surname": "Chen",
        "given_name": "Disi"
      },
      {
        "surname": "Chen",
        "given_name": "Baojia"
      }
    ]
  },
  {
    "title": "An efficient multi-scale learning method for image super-resolution networks",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.015",
    "abstract": "The image super-resolution (SR) operation holds multiple solutions with the one-to-many mapping from low-resolution (LR) to high-resolution (HR) space. However, the SR of different scales for the same image is usually regarded as independent tasks in the existing SR networks. Therefore, these networks are inflexible to effectively utilize feature learning experience and require much more computing time to recover HR images in higher resolutions. Recent arbitrary scale SR methods still cannot solve these problems. To efficiently and effectively recover HR images, this paper presents an efficient multi-scale learning method for image SR networks based on a novel self-generating (SG) mechanism. This method (briefly named SG-SR) utilizes the feature learning results of SR networks to generate upscale filters by using the novel SG upscale module, which is proposed to replace the traditional upscale module. For each scale factor, the SG upscale module provides the corresponding amount of the spatial weights to filter the LR tensor and then converts filtered tensors with the original tensor to corresponding HR images. The proposed method is evaluated through extensive experiments and compared with state-of-the-art (SOTA) methods on widely used benchmark datasets. The experimental results show that our method has superior performance compared with SOTA methods, and the SG upscale module can improve the performance of existing SR networks effectively. What is more, our module has a much less calculation cost than the other upscale modules.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023005695",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Computer science",
      "Computer vision",
      "Image (mathematics)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Physics",
      "Quantum mechanics",
      "Scale (ratio)",
      "Superresolution"
    ],
    "authors": [
      {
        "surname": "Ying",
        "given_name": "Wenyuan"
      },
      {
        "surname": "Dong",
        "given_name": "Tianyang"
      },
      {
        "surname": "Fan",
        "given_name": "Jing"
      }
    ]
  },
  {
    "title": "Robust underwater image enhancement with cascaded multi-level sub-networks and triple attention mechanism",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.008",
    "abstract": "With the growing exploration of marine resources, underwater image enhancement has gained significant attention. Recent advances in convolutional neural networks (CNN) have greatly impacted underwater image enhancement techniques. However, conventional CNN-based methods typically employ a single network structure, which may compromise robustness in challenging conditions. Additionally, commonly used UNet networks generally force fusion from low to high resolution for each layer, leading to inaccurate contextual information encoding. To address these issues, we propose a novel network called Cascaded Network with Multi-level Sub-networks (CNMS), which encompasses the following key components: (a) a cascade mechanism based on local modules and global networks for extracting feature representations with richer semantics and enhanced spatial precision, (b) information exchange between different resolution streams, and (c) a triple attention module for extracting attention-based features. CNMS selectively cascades multiple sub-networks through triple attention modules to extract distinct features from underwater images, bolstering the network’s robustness and improving generalization capabilities. Within the sub-network, we introduce a Multi-level Sub-network (MSN) that spans multiple resolution streams, combining contextual information from various scales while preserving the original underwater images’ high-resolution spatial details. Comprehensive experiments on multiple underwater datasets demonstrate that CNMS outperforms state-of-the-art methods in image enhancement tasks.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006317",
    "keywords": [
      "Artificial intelligence",
      "Biochemistry",
      "Chemistry",
      "Computer science",
      "Convolutional neural network",
      "Data mining",
      "Gene",
      "Geology",
      "Oceanography",
      "Pattern recognition (psychology)",
      "Robustness (evolution)",
      "Underwater"
    ],
    "authors": [
      {
        "surname": "Zhang",
        "given_name": "Dehuan"
      },
      {
        "surname": "Wu",
        "given_name": "Chenyu"
      },
      {
        "surname": "Zhou",
        "given_name": "Jingchun"
      },
      {
        "surname": "Zhang",
        "given_name": "Weishi"
      },
      {
        "surname": "Lin",
        "given_name": "Zifan"
      },
      {
        "surname": "Polat",
        "given_name": "Kemal"
      },
      {
        "surname": "Alenezi",
        "given_name": "Fayadh"
      }
    ]
  },
  {
    "title": "Event-triggered impulsive quasi-synchronization for BAM neural networks with reliable redundant channel",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.045",
    "abstract": "This work addresses the quasi-synchronization of delay master–slave BAM neural networks. To improve the utilization of channel bandwidth, a dynamic event-triggered impulsive mechanism is employed, in which data is transmitted only when a preset event-triggered mechanism or a forced impulse interval is satisfied. In addition, to guarantee the reliability of information transmission, a reliable redundant channel for BAM neural networks is adopted, whose transmission scheduling strategy is designed on the basis of the packet dropouts rate of the main communication channels. Further, an algorithm is employed to reduce the quasi-synchronization range of the error systems and the controllers are obtained. At last, a simulation result is shown to illustrate the effectiveness of the presented strategy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S089360802300607X",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Control (management)",
      "Control theory (sociology)",
      "Data transmission",
      "Impulse (physics)",
      "Network packet",
      "Physics",
      "Quantum mechanics",
      "Real-time computing",
      "Synchronization (alternating current)",
      "Telecommunications",
      "Transmission (telecommunications)"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Yumei"
      },
      {
        "surname": "Lv",
        "given_name": "Weijun"
      },
      {
        "surname": "Tao",
        "given_name": "Jie"
      },
      {
        "surname": "Xu",
        "given_name": "Yong"
      },
      {
        "surname": "Huang",
        "given_name": "Tingwen"
      },
      {
        "surname": "Rutkowski",
        "given_name": "Leszek"
      }
    ]
  },
  {
    "title": "Neurodynamic approaches for multi-agent distributed optimization",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.025",
    "abstract": "This paper considers a class of multi-agent distributed convex optimization with a common set of constraints and provides several continuous-time neurodynamic approaches. In problem transformation, l 1 and l 2 penalty methods are used respectively to cast the linear consensus constraint into the objective function, which avoids introducing auxiliary variables and only involves information exchange among primal variables in the process of solving the problem. For nonsmooth cost functions, two differential inclusions with projection operator are proposed. Without convexity of the differential inclusions, the asymptotic behavior and convergence properties are explored. For smooth cost functions, by harnessing the smoothness of l 2 penalty function, finite- and fixed-time convergent algorithms are provided via a specifically designed average consensus estimator. Finally, several numerical examples in the multi-agent simulation environment are conducted to illustrate the effectiveness of the proposed neurodynamic approaches.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006482",
    "keywords": [
      "Algorithm",
      "Computer science",
      "Constraint (computer-aided design)",
      "Convergence (economics)",
      "Convex function",
      "Convex optimization",
      "Convexity",
      "Differential inclusion",
      "Economic growth",
      "Economics",
      "Estimator",
      "Financial economics",
      "Geometry",
      "Mathematical analysis",
      "Mathematical optimization",
      "Mathematics",
      "Optimization problem",
      "Projection (relational algebra)",
      "Regular polygon",
      "Smoothness",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Guo",
        "given_name": "Luyao"
      },
      {
        "surname": "Korovin",
        "given_name": "Iakov"
      },
      {
        "surname": "Gorbachev",
        "given_name": "Sergey"
      },
      {
        "surname": "Shi",
        "given_name": "Xinli"
      },
      {
        "surname": "Gorbacheva",
        "given_name": "Nadezhda"
      },
      {
        "surname": "Cao",
        "given_name": "Jinde"
      }
    ]
  },
  {
    "title": "Adaptive bias-variance trade-off in advantage estimator for actor–critic algorithms",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.023",
    "abstract": "Actor–critic methods are leading in many challenging continuous control tasks. Advantage estimators, the most common critics in the actor–critic framework, combine state values from bootstrapping value functions and sample returns. Different combinations balance the bias introduced by state values and the variance returned by samples to reduce estimation errors. The bias and variance constantly fluctuate throughout training, leading to different optimal combinations. However, existing advantage estimators usually use fixed combinations that fail to account for the trade-off between minimizing bias and variance to find the optimal estimate. Our previous work on adaptive advantage estimation (AAE) analyzed the sources of bias and variance and offered two indicators. This paper further explores the relationship between the indicators and their optimal combination through typical numerical experiments. These analyses develop a general form of adaptive combinations of state values and sample returns to achieve low estimation errors. Empirical results on simulated robotic locomotion tasks show that our proposed estimators achieve similar or superior performance compared to previous generalized advantage estimators (GAE).",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023005774",
    "keywords": [
      "Accounting",
      "Algorithm",
      "Bootstrapping (finance)",
      "Business",
      "Computer science",
      "Econometrics",
      "Estimator",
      "Mathematical optimization",
      "Mathematics",
      "Statistics",
      "Variance (accounting)"
    ],
    "authors": [
      {
        "surname": "Chen",
        "given_name": "Yurou"
      },
      {
        "surname": "Zhang",
        "given_name": "Fengyi"
      },
      {
        "surname": "Liu",
        "given_name": "Zhiyong"
      }
    ]
  },
  {
    "title": "Masked Kinematic Continuity-aware Hierarchical Attention Network for pose estimation in videos",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.038",
    "abstract": "Existing methods for estimating human poses from video content exploit the temporal features of the video sequences and have shown impressive results. However, most methods address spatiotemporal issues separately. They compromise on accuracy to reduce jitter, or require high-resolution images to deal with occlusion, preventing full consideration of temporal features. Unfortunately, these two issues are interrelated. For example, occlusion causes uncertainty between successive frames, leading to unsmoothed results. To address these issues, we propose the Masked Kinematic Continuity-aware Hierarchical Attention Network (M-HANet) as a novel framework that exploits masked kinematic keypoint features by extending our framework HANet framework. First, we randomly select and mask a keypoint to treat the masked keypoint as it is occluded, which allows us to make the network resilient to occlusion. We also use the velocity and acceleration of each individual keypoint to effectively capture temporal features. Second, the proposed hierarchical transformer encoder refines a 2D or 3D input pose derived from existing estimators by aggregating the masked continuity of the spatiotemporal dependencies of human motion. Finally, to facilitate collaborative optimization, we perform an online cross-supervision between the final pose from our decoder and the refined input pose produced by our encoder. We validate the effectiveness of our model demonstrating that our proposed approach improves PCK@0.05 by 14.1% and MPJPE by 8.7 mm compared to the existing method on a variety of tasks, including 2D and 3D pose estimation, body mesh recovery, and sparsely annotated multi-human pose estimation.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023005920",
    "keywords": [
      "Artificial intelligence",
      "Classical mechanics",
      "Computer science",
      "Computer security",
      "Computer vision",
      "Encoder",
      "Estimator",
      "Exploit",
      "Kinematics",
      "Mathematics",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Pose",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Jin",
        "given_name": "Kyung-Min"
      },
      {
        "surname": "Lee",
        "given_name": "Gun-Hee"
      },
      {
        "surname": "Nam",
        "given_name": "Woo-Jeoung"
      },
      {
        "surname": "Kang",
        "given_name": "Tae-Kyung"
      },
      {
        "surname": "Kim",
        "given_name": "Hyun-Woo"
      },
      {
        "surname": "Lee",
        "given_name": "Seong-Whan"
      }
    ]
  },
  {
    "title": "Enhancing Enterprise Credit Risk Assessment with Cascaded Multi-level Graph Representation Learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.050",
    "abstract": "The assessment of Enterprise Credit Risk (ECR) is a critical technique for investment decisions and financial regulation. Previous methods usually construct enterprise representations by credit-related indicators, such as liquidity and staff quality. However, indicators of many enterprises are not accessible, especially for the small- and medium-sized enterprises. To alleviate the indicator deficiency, graph learning based methods are proposed to enhance enterprise representation learning by the neighbor structure of enterprise graphs. However, existing methods usually only focus on pairwise relationships, and overlook the ubiquitous high-order relationships among enterprises, e.g., supply chain connecting multiple enterprises. To resolve this issue, we propose a Multi-Structure Cascaded Graph Neural Network framework (MS-CGNN) for ECR assessment. It enhances enterprise representation learning based on enterprise graph structures of different granularity, including knowledge graphs of pairwise relationships, homogeneous and heterogeneous hypergraphs of high-order relationships. To distinguish influences of different types of hyperedges, MS-CGNN redefine new type-dependent hyperedge weight matrices for heterogeneous hypergraph convolutions. Experimental results show that MS-CGNN achieves state-of-the-art performance on real-world ECR datasets.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006160",
    "keywords": [
      "Artificial intelligence",
      "Business",
      "Combinatorics",
      "Computer science",
      "Computer security",
      "Construct (python library)",
      "Discrete mathematics",
      "Enterprise private network",
      "Feature learning",
      "Granularity",
      "Graph",
      "Homogeneous",
      "Hypergraph",
      "Knowledge management",
      "Law",
      "Machine learning",
      "Marketing",
      "Mathematics",
      "Operating system",
      "Pairwise comparison",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Supply chain",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Song",
        "given_name": "Lingyun"
      },
      {
        "surname": "Li",
        "given_name": "Haodong"
      },
      {
        "surname": "Tan",
        "given_name": "Yacong"
      },
      {
        "surname": "Li",
        "given_name": "Zhanhuai"
      },
      {
        "surname": "Shang",
        "given_name": "Xuequn"
      }
    ]
  },
  {
    "title": "Nonparametric tensor ring decomposition with scalable amortized inference",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.031",
    "abstract": "Multi-dimensional data are common in many applications, such as videos and multi-variate time series. While tensor decomposition (TD) provides promising tools for analyzing such data, there still remains several limitations. First, traditional TDs assume multi-linear structures of the latent embeddings, which greatly limits their expressive power. Second, TDs cannot be straightforwardly applied to datasets with massive samples. To address these issues, we propose a nonparametric TD with amortized inference networks. Specifically, we establish a non-linear extension of tensor ring decomposition, using neural networks, to model complex latent structures. To jointly model the cross-sample correlations and physical structures, a matrix Gaussian process (GP) prior is imposed over the core tensors. From learning perspective, we develop a VAE-like amortized inference network to infer the posterior of core tensors corresponding to new tensor data, which enables TDs to be applied to large datasets. Our model can be also viewed as a kind of decomposition of VAE, which can additionally capture hidden tensor structure and enhance the expressiveness power. Finally, we derive an evidence lower bound such that a scalable optimization algorithm is developed. The advantages of our method have been evaluated extensively by data imputation on the Healing MNIST dataset and four multi-variate time series data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023005853",
    "keywords": [
      "Algorithm",
      "Amortized analysis",
      "Artificial intelligence",
      "Computer science",
      "Data structure",
      "Inference",
      "Mathematics",
      "Nonparametric statistics",
      "Programming language",
      "Pure mathematics",
      "Statistics",
      "Tensor (intrinsic definition)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Tao",
        "given_name": "Zerui"
      },
      {
        "surname": "Tanaka",
        "given_name": "Toshihisa"
      },
      {
        "surname": "Zhao",
        "given_name": "Qibin"
      }
    ]
  },
  {
    "title": "Nonparametric tensor ring decomposition with scalable amortized inference",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.031",
    "abstract": "Multi-dimensional data are common in many applications, such as videos and multi-variate time series. While tensor decomposition (TD) provides promising tools for analyzing such data, there still remains several limitations. First, traditional TDs assume multi-linear structures of the latent embeddings, which greatly limits their expressive power. Second, TDs cannot be straightforwardly applied to datasets with massive samples. To address these issues, we propose a nonparametric TD with amortized inference networks. Specifically, we establish a non-linear extension of tensor ring decomposition, using neural networks, to model complex latent structures. To jointly model the cross-sample correlations and physical structures, a matrix Gaussian process (GP) prior is imposed over the core tensors. From learning perspective, we develop a VAE-like amortized inference network to infer the posterior of core tensors corresponding to new tensor data, which enables TDs to be applied to large datasets. Our model can be also viewed as a kind of decomposition of VAE, which can additionally capture hidden tensor structure and enhance the expressiveness power. Finally, we derive an evidence lower bound such that a scalable optimization algorithm is developed. The advantages of our method have been evaluated extensively by data imputation on the Healing MNIST dataset and four multi-variate time series data.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023005853",
    "keywords": [
      "Algorithm",
      "Amortized analysis",
      "Artificial intelligence",
      "Computer science",
      "Data structure",
      "Inference",
      "Mathematics",
      "Nonparametric statistics",
      "Programming language",
      "Pure mathematics",
      "Statistics",
      "Tensor (intrinsic definition)",
      "Theoretical computer science"
    ],
    "authors": [
      {
        "surname": "Tao",
        "given_name": "Zerui"
      },
      {
        "surname": "Tanaka",
        "given_name": "Toshihisa"
      },
      {
        "surname": "Zhao",
        "given_name": "Qibin"
      }
    ]
  },
  {
    "title": "ComCo: Complementary supervised contrastive learning for complementary label learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.013",
    "abstract": "Complementary label learning (CLL) is an important problem that aims to reduce the cost of obtaining large-scale accurate datasets by only allowing each training sample to be equipped with labels the sample does not belong. Despite its promise, CLL remains a challenging task. Previous methods have proposed new loss functions or introduced deep learning-based models to CLL, but they mostly overlook the semantic information that may be implicit in the complementary labels. In this work, we propose a novel method, ComCo, which leverages a contrastive learning framework to assist CLL. Our method includes two key strategies: a positive selection strategy that identifies reliable positive samples and a negative selection strategy that skillfully integrates and leverages the information in the complementary labels to construct a negative set. These strategies bring ComCo closer to supervised contrastive learning. Empirically, ComCo significantly achieves better representation learning and outperforms the baseline models and the current state-of-the-art by up to 14.61% in CLL.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023005683",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Chemistry",
      "Chromatography",
      "Computer science",
      "Computer security",
      "Construct (python library)",
      "Deep learning",
      "Economics",
      "Feature learning",
      "Key (lock)",
      "Law",
      "Machine learning",
      "Management",
      "Political science",
      "Politics",
      "Programming language",
      "Representation (politics)",
      "Sample (material)",
      "Selection (genetic algorithm)",
      "Set (abstract data type)",
      "Supervised learning",
      "Task (project management)"
    ],
    "authors": [
      {
        "surname": "Jiang",
        "given_name": "Haoran"
      },
      {
        "surname": "Sun",
        "given_name": "Zhihao"
      },
      {
        "surname": "Tian",
        "given_name": "Yingjie"
      }
    ]
  },
  {
    "title": "Low-variance Forward Gradients using Direct Feedback Alignment and momentum",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.051",
    "abstract": "Supervised learning in deep neural networks is commonly performed using error backpropagation. However, the sequential propagation of errors during the backward pass limits its scalability and applicability to low-powered neuromorphic hardware. Therefore, there is growing interest in finding local alternatives to backpropagation. Recently proposed methods based on forward-mode automatic differentiation suffer from high variance in large deep neural networks, which affects convergence. In this paper, we propose the Forward Direct Feedback Alignment algorithm that combines Activity-Perturbed Forward Gradients with Direct Feedback Alignment and momentum. We provide both theoretical proofs and empirical evidence that our proposed method achieves lower variance than forward gradient techniques. In this way, our approach enables faster convergence and better performance when compared to other local alternatives to backpropagation and opens a new perspective for the development of online learning algorithms compatible with neuromorphic systems.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006172",
    "keywords": [
      "Accounting",
      "Algorithm",
      "Artificial intelligence",
      "Artificial neural network",
      "Backpropagation",
      "Business",
      "Computer science",
      "Control engineering",
      "Convergence (economics)",
      "Database",
      "Economic growth",
      "Economics",
      "Engineering",
      "Feed forward",
      "Machine learning",
      "Neuromorphic engineering",
      "Scalability",
      "Variance (accounting)"
    ],
    "authors": [
      {
        "surname": "Bacho",
        "given_name": "Florian"
      },
      {
        "surname": "Chu",
        "given_name": "Dominique"
      }
    ]
  },
  {
    "title": "Balanced influence maximization in social networks based on deep reinforcement learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.030",
    "abstract": "Balanced influence maximization aims to balance the influence maximization of multiple different entities in social networks and avoid the emergence of filter bubbles and echo chambers. Recently, an increasing number of studies have drawn attention to the study of balanced influence maximization in social networks and achieves success to some extent. However, most of them still have two major shortcomings. First, the previous works mainly focus on spreading the influence of multiple target entities to more users, ignoring the potential influence of the correlation between the target entities and other entities on information propagation in real social networks. Second, the existing methods require a large amount of diffusion sampling for influence estimation, making it difficult to apply to large social networks. To this end, we propose a Balanced Influence Maximization framework based on Deep Reinforcement Learning named BIM-DRL, which consists of two core components: an entity correlation evaluation module and a balanced seed node selection module. Specifically, in the entity correlation evaluation module, an entity correlation evaluation model based on the users’ historical behavior sequences is proposed, which can accurately evaluate the impact of entity correlation on information propagation. In the balanced seed node selection module, a balanced influence maximization model based on deep reinforcement learning is designed to train the parameters in the objective function, and then a set of seed nodes that maximize the balanced influence is found. Extensive experiments on six real-life network datasets demonstrate the superiority of the BIM-DRL over state-of-the-art methods on the metrics of balanced influence spread and balanced propagation accuracy.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023005841",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Computer vision",
      "Correlation",
      "Engineering",
      "Filter (signal processing)",
      "Geometry",
      "Machine learning",
      "Mathematical optimization",
      "Mathematics",
      "Maximization",
      "Node (physics)",
      "Programming language",
      "Reinforcement learning",
      "Set (abstract data type)",
      "Social media",
      "Social network (sociolinguistics)",
      "Structural engineering",
      "World Wide Web"
    ],
    "authors": [
      {
        "surname": "Yang",
        "given_name": "Shuxin"
      },
      {
        "surname": "Du",
        "given_name": "Quanming"
      },
      {
        "surname": "Zhu",
        "given_name": "Guixiang"
      },
      {
        "surname": "Cao",
        "given_name": "Jie"
      },
      {
        "surname": "Chen",
        "given_name": "Lei"
      },
      {
        "surname": "Qin",
        "given_name": "Weiping"
      },
      {
        "surname": "Wang",
        "given_name": "Youquan"
      }
    ]
  },
  {
    "title": "M-DDC: MRI based demyelinative diseases classification with U-Net segmentation and convolutional network",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.010",
    "abstract": "Childhood demyelinative diseases classification (DDC) with brain magnetic resonance imaging (MRI) is crucial to clinical diagnosis. But few attentions have been paid to DDC in the past. How to accurately differentiate pediatric-onset neuromyelitis optica spectrum disorder (NMOSD) from acute disseminated encephalomyelitis (ADEM) based on MRI is challenging in DDC. In this paper, a novel architecture M-DDC based on joint U-Net segmentation network and deep convolutional network is developed. The U-Net segmentation can provide pixel-level structure information, that helps the lesion areas location and size estimation. The classification branch in DDC can detect the regions of interest inside MRIs, including the white matter regions where lesions appear. The performance of the proposed method is evaluated on MRIs of 201 subjects recorded from the Children’s Hospital of Zhejiang University School of Medicine. The comparisons show that the proposed DDC achieves the highest accuracy of 99.19% and dice of 71.1% for ADEM and NMOSD classification and segmentation, respectively.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023005646",
    "keywords": [
      "Acute disseminated encephalomyelitis",
      "Artificial intelligence",
      "Computer science",
      "Convolutional neural network",
      "Magnetic resonance imaging",
      "Medicine",
      "Multiple sclerosis",
      "Neuromyelitis optica",
      "Pattern recognition (psychology)",
      "Psychiatry",
      "Radiology",
      "Segmentation",
      "White matter"
    ],
    "authors": [
      {
        "surname": "Zhou",
        "given_name": "Deyang"
      },
      {
        "surname": "Xu",
        "given_name": "Lu"
      },
      {
        "surname": "Wang",
        "given_name": "Tianlei"
      },
      {
        "surname": "Wei",
        "given_name": "Shaonong"
      },
      {
        "surname": "Gao",
        "given_name": "Feng"
      },
      {
        "surname": "Lai",
        "given_name": "Xiaoping"
      },
      {
        "surname": "Cao",
        "given_name": "Jiuwen"
      }
    ]
  },
  {
    "title": "Few-shot image generation with reverse contrastive learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.026",
    "abstract": "Generative models, such as Generative Adversarial Networks (GANs), have recently shown remarkable capabilities in various generation tasks. However, the success of these models heavily depends on the availability of a large-scale training dataset. When the size of the training dataset is limited, the quality and diversity of the generated results suffer from severe degradation. In this paper, we propose a novel approach, Reverse Contrastive Learning (RCL), to address the problem of high-quality and diverse image generation under few-shot settings. The success of RCL benefits from a two-sided, powerful regularization. Our proposed regularization is designed based on the correlation between generated samples, which can effectively utilize the latent feature information between different levels of samples. It does not require any auxiliary information or augmentation techniques. A series of qualitative and quantitative results show that our proposed method is superior to the existing State-Of-The-Art (SOTA) methods under the few-shot setting and is still competitive under the low-shot setting, showcasing the effectiveness of RCL. Code will be released upon acceptance at https://github.com/gouayao/RCL.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023005804",
    "keywords": [
      "Adversarial system",
      "Artificial intelligence",
      "Computer science",
      "Generative adversarial network",
      "Generative grammar",
      "Image (mathematics)",
      "Machine learning",
      "Pattern recognition (psychology)",
      "Regularization (linguistics)"
    ],
    "authors": [
      {
        "surname": "Gou",
        "given_name": "Yao"
      },
      {
        "surname": "Li",
        "given_name": "Min"
      },
      {
        "surname": "Zhang",
        "given_name": "Yusen"
      },
      {
        "surname": "He",
        "given_name": "Zhuzhen"
      },
      {
        "surname": "He",
        "given_name": "Yujie"
      }
    ]
  },
  {
    "title": "Denoising cosine similarity: A theory-driven approach for efficient representation learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.027",
    "abstract": "Representation learning has been increasing its impact on the research and practice of machine learning, since it enables to learn representations that can apply to various downstream tasks efficiently. However, recent works pay little attention to the fact that real-world datasets used during the stage of representation learning are commonly contaminated by noise, which can degrade the quality of learned representations. This paper tackles the problem to learn robust representations against noise in a raw dataset. To this end, inspired by recent works on denoising and the success of the cosine-similarity-based objective functions in representation learning, we propose the denoising Cosine-Similarity (dCS) loss. The dCS loss is a modified cosine-similarity loss and incorporates a denoising property, which is supported by both our theoretical and empirical findings. To make the dCS loss implementable, we also construct the estimators of the dCS loss with statistical guarantees. Finally, we empirically show the efficiency of the dCS loss over the baseline objective functions in vision and speech domains.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023005816",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Cosine similarity",
      "Epistemology",
      "Estimator",
      "Feature learning",
      "Image (mathematics)",
      "Law",
      "Machine learning",
      "Mathematics",
      "Noise (video)",
      "Noise reduction",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Property (philosophy)",
      "Representation (politics)",
      "Similarity (geometry)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Nakagawa",
        "given_name": "Takumi"
      },
      {
        "surname": "Sanada",
        "given_name": "Yutaro"
      },
      {
        "surname": "Waida",
        "given_name": "Hiroki"
      },
      {
        "surname": "Zhang",
        "given_name": "Yuhui"
      },
      {
        "surname": "Wada",
        "given_name": "Yuichiro"
      },
      {
        "surname": "Takanashi",
        "given_name": "Kōsaku"
      },
      {
        "surname": "Yamada",
        "given_name": "Tomonori"
      },
      {
        "surname": "Kanamori",
        "given_name": "Takafumi"
      }
    ]
  },
  {
    "title": "Denoising cosine similarity: A theory-driven approach for efficient representation learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.027",
    "abstract": "Representation learning has been increasing its impact on the research and practice of machine learning, since it enables to learn representations that can apply to various downstream tasks efficiently. However, recent works pay little attention to the fact that real-world datasets used during the stage of representation learning are commonly contaminated by noise, which can degrade the quality of learned representations. This paper tackles the problem to learn robust representations against noise in a raw dataset. To this end, inspired by recent works on denoising and the success of the cosine-similarity-based objective functions in representation learning, we propose the denoising Cosine-Similarity (dCS) loss. The dCS loss is a modified cosine-similarity loss and incorporates a denoising property, which is supported by both our theoretical and empirical findings. To make the dCS loss implementable, we also construct the estimators of the dCS loss with statistical guarantees. Finally, we empirically show the efficiency of the dCS loss over the baseline objective functions in vision and speech domains.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023005816",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Cosine similarity",
      "Epistemology",
      "Estimator",
      "Feature learning",
      "Image (mathematics)",
      "Law",
      "Machine learning",
      "Mathematics",
      "Noise (video)",
      "Noise reduction",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Political science",
      "Politics",
      "Property (philosophy)",
      "Representation (politics)",
      "Similarity (geometry)",
      "Statistics"
    ],
    "authors": [
      {
        "surname": "Nakagawa",
        "given_name": "Takumi"
      },
      {
        "surname": "Sanada",
        "given_name": "Yutaro"
      },
      {
        "surname": "Waida",
        "given_name": "Hiroki"
      },
      {
        "surname": "Zhang",
        "given_name": "Yuhui"
      },
      {
        "surname": "Wada",
        "given_name": "Yuichiro"
      },
      {
        "surname": "Takanashi",
        "given_name": "Kōsaku"
      },
      {
        "surname": "Yamada",
        "given_name": "Tomonori"
      },
      {
        "surname": "Kanamori",
        "given_name": "Takafumi"
      }
    ]
  },
  {
    "title": "GRAND: GAN-based software runtime anomaly detection method using trace information",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.036",
    "abstract": "Software runtime anomaly detection can detect manifestations (known as anomalies) caused by faults in complex systems before they lead to failure. Whereas most existing methods use external performance indicators, this study uses internal execution traces to reveal failures not only related to software performance issues but also functional errors. A neural network model called GRAND, which combines a variational autoencoder and a generative adversarial network, is proposed to mine anomalies in the execution trace. Cassandra, a widely used database system, was used as a representation to conduct the empirical study. The dataset was collected under a well-designed operational profile that contained 5180 time series, each containing more than ten million data points. GRAND achieved a higher detection performance than the other two SOTA baseline models, with a 99% F1-score compared with 93% and 87%. Ablation studies show that the workload information used in GRAND can determine whether the current internal status is consistent with the task, thus achieving a 16% improvement in the F1-score. The attention mechanism used for data fusion can achieve a 32% improvement in the F1-score.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023005919",
    "keywords": [
      "Anomaly detection",
      "Artificial intelligence",
      "Artificial neural network",
      "Autoencoder",
      "Baseline (sea)",
      "Computer science",
      "Data mining",
      "Economics",
      "Geology",
      "Linguistics",
      "Machine learning",
      "Management",
      "Oceanography",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Programming language",
      "Software",
      "Software bug",
      "TRACE (psycholinguistics)",
      "Task (project management)",
      "Workload"
    ],
    "authors": [
      {
        "surname": "Kong",
        "given_name": "Shiyi"
      },
      {
        "surname": "Ai",
        "given_name": "Jun"
      },
      {
        "surname": "Lu",
        "given_name": "Minyan"
      },
      {
        "surname": "Gong",
        "given_name": "Yiang"
      }
    ]
  },
  {
    "title": "Preassigned-time synchronization for complex-valued memristive neural networks with reaction–diffusion terms and Markov parameters",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.011",
    "abstract": "This study addresses the preassigned-time synchronization for complex-valued memristive neural networks with reaction–diffusion terms and Markov parameters. Employing a preassigned-time stable control strategy, two distinct controllers with varying power exponent parameters are designed to ensure that synchronization can be achieved within a predefined time frame. Unlike existing finite/fixed-time results, a priori specification of the settling time is addressed. Furthermore, Green’s formula and boundary conditions are efficiently applied to overcome potential symmetry loss. Additionally, the activation function’s constraint range is more lenient compared to existing constraints. Finally, the effectiveness of the presented methods are demonstrated through two examples.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006354",
    "keywords": [
      "A priori and a posteriori",
      "Artificial intelligence",
      "Artificial neural network",
      "Boundary (topology)",
      "Channel (broadcasting)",
      "Composite material",
      "Computer network",
      "Computer science",
      "Constraint (computer-aided design)",
      "Control (management)",
      "Control engineering",
      "Control theory (sociology)",
      "Diffusion",
      "Engineering",
      "Epistemology",
      "Geometry",
      "Machine learning",
      "Markov chain",
      "Materials science",
      "Mathematical analysis",
      "Mathematics",
      "Philosophy",
      "Physics",
      "Range (aeronautics)",
      "Settling time",
      "Step response",
      "Synchronization (alternating current)",
      "Thermodynamics"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Hongliang"
      },
      {
        "surname": "Cheng",
        "given_name": "Jun"
      },
      {
        "surname": "Cao",
        "given_name": "Jinde"
      },
      {
        "surname": "Katib",
        "given_name": "Iyad"
      }
    ]
  },
  {
    "title": "Multi-scale feature selection network for lightweight image super-resolution",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.043",
    "abstract": "Recently, many super-resolution (SR) methods based on convolutional neural networks (CNNs) have achieved superior performance by utilizing deep and heavy models, which may not be suitable for real-world low-budget devices. To address this issue, we propose a novel lightweight SR network called a multi-scale feature selection network (MFSN). As the basic building block of MFSN, the multi-scale feature selection block (MFSB) is presented to extract the rich multi-scale features from a coarse-to-fine receptive field level. For a better representation ability, a wide-activated residual unit is adopted in each branch of MFSB except the last one. In MFSB, the scale selection module (SSM) is designed to effectively fuse the features from two adjacent branches by adjusting receptive field sizes adaptively. Further, a comprehensive channel attention mechanism (CCAM) is integrated into SSM to learn the dynamic selection weight by considering the local and global inter-channel dependencies. Extensive experimental results illustrate that the proposed MFSN is superior to other lightweight methods.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006056",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Block (permutation group theory)",
      "Channel (broadcasting)",
      "Computer network",
      "Computer science",
      "Convolutional neural network",
      "Electrical engineering",
      "Engineering",
      "Feature (linguistics)",
      "Feature selection",
      "Field (mathematics)",
      "Fuse (electrical)",
      "Geometry",
      "Law",
      "Linguistics",
      "Mathematics",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Physics",
      "Political science",
      "Politics",
      "Pure mathematics",
      "Quantum mechanics",
      "Representation (politics)",
      "Residual",
      "Scale (ratio)",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Li",
        "given_name": "Minghong"
      },
      {
        "surname": "Zhao",
        "given_name": "Yuqian"
      },
      {
        "surname": "Zhang",
        "given_name": "Fan"
      },
      {
        "surname": "Luo",
        "given_name": "Biao"
      },
      {
        "surname": "Yang",
        "given_name": "Chunhua"
      },
      {
        "surname": "Gui",
        "given_name": "Weihua"
      },
      {
        "surname": "Chang",
        "given_name": "Kan"
      }
    ]
  },
  {
    "title": "Reproduced neuron-like excitability and bursting synchronization of memristive Josephson junctions loaded inductor",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.012",
    "abstract": "Employing electronic component including memristor and Josephson junction to mimic biological neuron or synapse has elicited intense research in recent years. Neurons described by nonlinear oscillators can exhibit complex electrical activities. Josephson junctions are excellent candidates for neuron-inspired components because of their physical properties with low energy costs and high efficiency. In this paper, we revisit a prior work on memristive Josephson junction (MJJ) to identify the dynamical mechanisms to mimic neuron-like excitability and spiking. The inductive memristive Josephson junction (L-MJJ) model is further developed by adding an inductor with internal resistor. It is found that the L-MJJ model can reproduce square-wave bursting of the classical neuronal model from the neurodynamics point of view. The coupling L-MJJ oscillators can achieve in-phase and antiphase bursting synchronization similar with nonlinear coupling neurons. From the framework of nonlinear dynamics theory, this work aspires to build effective bridge between superconducting physics and theoretical neuroscience. Obtained results confirm the potential feasibility of this junction in designing a neuron-inspired computation to explore dynamics of larger-scale neuromorphic network.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006342",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Biochemistry",
      "Biological neuron model",
      "Biology",
      "Bursting",
      "Chemistry",
      "Computer science",
      "Coupling (piping)",
      "Electrical Synapses",
      "Electrical engineering",
      "Engineering",
      "Gap junction",
      "Intracellular",
      "Josephson effect",
      "Materials science",
      "Memristor",
      "Metallurgy",
      "Neuromorphic engineering",
      "Neuroscience",
      "Nonlinear system",
      "Physics",
      "Quantum mechanics",
      "Resistor",
      "Superconductivity",
      "Synchronization (alternating current)",
      "Topology (electrical circuits)",
      "Voltage"
    ],
    "authors": [
      {
        "surname": "Wu",
        "given_name": "Fuqiang"
      },
      {
        "surname": "Meng",
        "given_name": "Hao"
      },
      {
        "surname": "Ma",
        "given_name": "Jun"
      }
    ]
  },
  {
    "title": "Maximum margin and global criterion based-recursive feature selection",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.037",
    "abstract": "In this research paper, we aim to investigate and address the limitations of recursive feature elimination (RFE) and its variants in high-dimensional feature selection tasks. We identify two main challenges associated with these methods. Firstly, the feature ranking criterion utilized in these approaches is inconsistent with the maximum-margin theory. Secondly, the computation of the criterion is performed locally, lacking the ability to measure the importance of features globally. To overcome these challenges, we propose a novel feature ranking criterion called Maximum Margin and Global (MMG) criterion. This criterion utilizes the classification margin to determine the importance of features and computes it globally, enabling a more accurate assessment of feature importance. Moreover, we introduce an optimal feature subset evaluation algorithm that leverages the MMG criterion to determine the best subset of features. To enhance the efficiency of the proposed algorithms, we provide two alpha seeding strategies that significantly reduce computational costs while maintaining high accuracy. These strategies offer a practical means to expedite the feature selection process. Through extensive experiments conducted on ten benchmark datasets, we demonstrate that our proposed algorithms outperform current state-of-the-art methods. Additionally, the alpha seeding strategies yield significant speedups, further enhancing the efficiency of the feature selection process.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023005877",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Benchmark (surveying)",
      "Computer science",
      "Data mining",
      "Feature (linguistics)",
      "Feature selection",
      "Geodesy",
      "Geography",
      "Linguistics",
      "Machine learning",
      "Margin (machine learning)",
      "Operating system",
      "Pattern recognition (psychology)",
      "Philosophy",
      "Process (computing)",
      "Ranking (information retrieval)",
      "Selection (genetic algorithm)"
    ],
    "authors": [
      {
        "surname": "Ding",
        "given_name": "Xiaojian"
      },
      {
        "surname": "Li",
        "given_name": "Yi"
      },
      {
        "surname": "Chen",
        "given_name": "Shilin"
      }
    ]
  },
  {
    "title": "A survey on few-shot class-incremental learning",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.039",
    "abstract": "Large deep learning models are impressive, but they struggle when real-time data is not available. Few-shot class-incremental learning (FSCIL) poses a significant challenge for deep neural networks to learn new tasks from just a few labeled samples without forgetting the previously learned ones. This setup can easily leads to catastrophic forgetting and overfitting problems, severely affecting model performance. Studying FSCIL helps overcome deep learning model limitations on data volume and acquisition time, while improving practicality and adaptability of machine learning models. This paper provides a comprehensive survey on FSCIL. Unlike previous surveys, we aim to synthesize few-shot learning and incremental learning, focusing on introducing FSCIL from two perspectives, while reviewing over 30 theoretical research studies and more than 20 applied research studies. From the theoretical perspective, we provide a novel categorization approach that divides the field into five subcategories, including traditional machine learning methods, meta learning-based methods, feature and feature space-based methods, replay-based methods, and dynamic network structure-based methods. We also evaluate the performance of recent theoretical research on benchmark datasets of FSCIL. From the application perspective, FSCIL has achieved impressive achievements in various fields of computer vision such as image classification, object detection, and image segmentation, as well as in natural language processing and graph. We summarize the important applications. Finally, we point out potential future research directions, including applications, problem setups, and theory development. Overall, this paper offers a comprehensive analysis of the latest advances in FSCIL from a methodological, performance, and application perspective.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006019",
    "keywords": [
      "Artificial intelligence",
      "Artificial neural network",
      "Categorization",
      "Computer science",
      "Deep learning",
      "Feature (linguistics)",
      "Field (mathematics)",
      "Forgetting",
      "Linguistics",
      "Machine learning",
      "Mathematics",
      "Overfitting",
      "Perspective (graphical)",
      "Philosophy",
      "Pure mathematics"
    ],
    "authors": [
      {
        "surname": "Tian",
        "given_name": "Songsong"
      },
      {
        "surname": "Li",
        "given_name": "Lusi"
      },
      {
        "surname": "Li",
        "given_name": "Weijun"
      },
      {
        "surname": "Ran",
        "given_name": "Hang"
      },
      {
        "surname": "Ning",
        "given_name": "Xin"
      },
      {
        "surname": "Tiwari",
        "given_name": "Prayag"
      }
    ]
  },
  {
    "title": "A surge of submissions",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.054",
    "abstract": "",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006779",
    "keywords": [
      "Artificial intelligence",
      "Computer science",
      "Geography",
      "Meteorology",
      "Surge"
    ],
    "authors": [
      {
        "surname": "Toyoizumi",
        "given_name": "Taro"
      },
      {
        "surname": "Wang",
        "given_name": "DeLiang"
      }
    ]
  },
  {
    "title": "Boosting fine-tuning via Conditional Online Knowledge Transfer",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.10.035",
    "abstract": "Fine-tuning is an effective technique to enhance network performance in scenarios with limited labeled data. To achieve this, recent methods exploit the knowledge mined in the source model (e.g., feature maps) to construct an extra regularization signal (RS), collaboratively supervising the target model along with target labels. However, these RSs are generated independently from the target information or are generated from the rough assistance of the target information, resulting in biased supervision different from the target task. In this paper, we propose a Conditional Online Knowledge Transfer (COKT) framework that finely utilizes the target information to construct robust and target-related RS. Specifically, we train a target-dominant RS branch that online supervises the target model in a knowledge distillation manner. The target information dominates the RS branch from three aspects: sample-wise conditional attention, residual feature fusion, and target task loss. With such a target-oriented framework, we can effectively exploit target-related prior knowledge of the source model. Extensive experiments demonstrate that COKT significantly outperforms the fine-tuning baselines, especially for dissimilar target tasks and small datasets. Moreover, different from most of the fine-tuning methods that are restricted to the vanilla fine-tuning scenario, COKT can be easily extended to cross-model and multi-model fine-tuning scenarios.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023005907",
    "keywords": [
      "Algorithm",
      "Artificial intelligence",
      "Boosting (machine learning)",
      "Computer science",
      "Computer security",
      "Construct (python library)",
      "Data mining",
      "Economics",
      "Exploit",
      "Fine-tuning",
      "Knowledge management",
      "Knowledge transfer",
      "Machine learning",
      "Management",
      "Operating system",
      "Pattern recognition (psychology)",
      "Physics",
      "Programming language",
      "Quantum mechanics",
      "RSS",
      "Regularization (linguistics)",
      "Residual",
      "Task (project management)",
      "Transfer of learning"
    ],
    "authors": [
      {
        "surname": "Liu",
        "given_name": "Zhiqiang"
      },
      {
        "surname": "Li",
        "given_name": "Yuhong"
      },
      {
        "surname": "Huang",
        "given_name": "Chengkai"
      },
      {
        "surname": "Luo",
        "given_name": "KunTing"
      },
      {
        "surname": "Liu",
        "given_name": "Yanxia"
      }
    ]
  },
  {
    "title": "A survey on cancer detection via convolutional neural networks: Current challenges and future directions",
    "journal": "Neural Networks",
    "year": "2024",
    "doi": "10.1016/j.neunet.2023.11.006",
    "abstract": "Cancer is a condition in which abnormal cells uncontrollably split and damage the body tissues. Hence, detecting cancer at an early stage is highly essential. Currently, medical images play an indispensable role in detecting various cancers; however, manual interpretation of these images by radiologists is observer-dependent, time-consuming, and tedious. An automatic decision-making process is thus an essential need for cancer detection and diagnosis. This paper presents a comprehensive survey on automated cancer detection in various human body organs, namely, the breast, lung, liver, prostate, brain, skin, and colon, using convolutional neural networks (CNN) and medical imaging techniques. It also includes a brief discussion about deep learning based on state-of-the-art cancer detection methods, their outcomes, and the possible medical imaging data used. Eventually, the description of the dataset used for cancer detection, the limitations of the existing solutions, future trends, and challenges in this domain are discussed. The utmost goal of this paper is to provide a piece of comprehensive and insightful information to researchers who have a keen interest in developing CNN-based models for cancer detection.",
    "link": "https://www.sciencedirect.com/science/article/pii/S0893608023006287",
    "keywords": [
      "Artificial intelligence",
      "Cancer",
      "Cancer detection",
      "Computer science",
      "Convolutional neural network",
      "Deep learning",
      "Internal medicine",
      "Machine learning",
      "Medical imaging",
      "Medicine",
      "Operating system",
      "Pattern recognition (psychology)",
      "Process (computing)"
    ],
    "authors": [
      {
        "surname": "Sharma",
        "given_name": "Pallabi"
      },
      {
        "surname": "Nayak",
        "given_name": "Deepak Ranjan"
      },
      {
        "surname": "Balabantaray",
        "given_name": "Bunil Kumar"
      },
      {
        "surname": "Tanveer",
        "given_name": "M."
      },
      {
        "surname": "Nayak",
        "given_name": "Rajashree"
      }
    ]
  }
]